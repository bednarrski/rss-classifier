{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from api import inoreader_api, inoreader_scraping\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "\n",
    "def rewrite(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape feeds starting from a concrete moment and store them in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=t_CG8hNj8aj6\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=8xuq3sUctGGH\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=rZqJsXROlADA\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=DqPQF0yrjOrx\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=po4gGSY1J9uX\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=QDDeL_9IdUjP\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=pUOKPZnUUiaR\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=iEqoZOQcN_C1\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=MCkRKbSJjLYH\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=_5jC8ZR9_uhI\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=9Pjwy7JaU6Ol\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=ADKZJQsu26Wo\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=YHyXWjta9pxk\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=JLq0HWiQntSF\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=zmad6t5J46nG\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=9ngxybAdlxWC\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=iiU8do5W_719\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=WDcqT3MnOQIx\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=KEpLGNM0flQj\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=C01OeBg2wm5n\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=wKLdyIWt9Hoe\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=KhpzjF72_ykG\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=u6oIk4orixIF\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=rpo5mkICmKyX\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=areDBdBFGjQ4\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=0g3Xsn73f_DH\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=CRPYWE1qHx3q\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=mJX59NcFqY8T\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=9UKecM9Ph4iu\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=n9_UrXiihxT5\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=zG7gu3Zs882e\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=WPaocdeoENKq\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=ycuHOqLkRW5m\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=oGhdC3qcZ3OW\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=gURC_PwAGWHZ\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Bdl3SB8RknUX\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=9Sah7yzmmoCe\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=QUtgahIjshdp\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=LdoTZQkbfqqF\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=pQ9nymigTX5q\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=3pAgul7tiJhy\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=r3ZzqM9wc4x4\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=K4ndnJOcKfGq\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=tYAb_yLM4emL\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Th3tw7I25Ftt\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=2olCkWUy7Fu0\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=lPMf50_mSysX\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=h4OWOaACaUFT\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=KDg9EEm9jo5d\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=zKSD_RStn_wS\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=NG_I08rEDrjz\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=WZ7i3w5JqDRM\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=ZWLsRckaZRht\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=4W_KxZchopdS\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Mo13OJSrntmr\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=EOdLerW1tj_K\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=otA_9ZfLYOW8\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=npKFO_0HZBug\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=ISNXPweFtnbh\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=2RkFJJQ49soz\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=TrP8dDh44UZb\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Eh94ED0oM1Sc\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=mHguoqZiaG9p\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=4IcCjWuSaXoP\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Mwnfu6YMda_R\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=bde9r7olUOmo\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=1rBrCyyaRCLU\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=OJsmXDTctHek\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=MtRpohAa23gZ\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=7TZ6F0_9qX_J\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=1c7o2fPcCwRn\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=QAzhO8tnlAp2\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=8A67z5Dg7Ue2\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Sattbge2toKu\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=S3oDZFZJG3pH\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=SMNqckQq9FOu\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=s4xrmsUtdCED\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=IpD9if95tX4j\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=MfwRrU6Yyxcr\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=3DDwdQN66WW8\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=YITQMxtbI22i\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=IR8lHUps5_wO\n",
      "200 OK\n",
      "1640\n"
     ]
    }
   ],
   "source": [
    "start_time = 1508112001\n",
    "feed = 'user/-/label/arXiv'\n",
    "file_name = 'articles_raw.json'\n",
    "\n",
    "number_scraped = inoreader_scraping.scrape(feed, start_time, file_name)\n",
    "print(number_scraped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the feeds from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(file_name, 'r')\n",
    "json_string = f.read()\n",
    "f.close\n",
    "\n",
    "json_object = json.loads(json_string)\n",
    "all_articles = json_object['all_articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_abstract_string(abs_dict):\n",
    "    abs_string = abs_dict['content']\n",
    "    abs_string = re.search('<p>(.*)</p>', abs_string, flags=re.DOTALL)\n",
    "    return abs_string.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_canonical_string(canonical_list):\n",
    "    url_dict = canonical_list[0]\n",
    "    url = url_dict['href']\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create preprocessed and cleaned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/piotr.bednarski/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/piotr.bednarski/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/piotr.bednarski/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "all_articles_pd = pd.DataFrame(all_articles)\n",
    "all_articles_pd = all_articles_pd[['canonical', 'author', 'categories', 'published', 'title', 'summary']]\n",
    "\n",
    "for i in range(len(all_articles_pd)):\n",
    "    all_articles_pd['summary'][i] = get_abstract_string(all_articles_pd['summary'][i])\n",
    "    all_articles_pd['canonical'][i] = get_canonical_string(all_articles_pd['canonical'][i])\n",
    "    \n",
    "all_articles_pd['read'] = False\n",
    "all_articles_pd['liked'] = False\n",
    "\n",
    "for i in range(len(all_articles_pd)):\n",
    "    if 'user/1005689817/state/com.google/read' in all_articles_pd['categories'][i]:\n",
    "        all_articles_pd['read'][i] = True\n",
    "    if 'user/1005689817/state/com.google/like' in all_articles_pd['categories'][i]:\n",
    "        all_articles_pd['liked'][i] = True\n",
    "\n",
    "del all_articles_pd['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all_articles_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_articles_pd = all_articles_pd[all_articles_pd['read'] == True]\n",
    "\n",
    "with open('old_articles_proto2.pickle', 'rb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    old_tagged_articles_pd = pickle.load(f)\n",
    "    \n",
    "tagged_articles_pd = pd.concat([tagged_articles_pd, old_tagged_articles_pd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_articles_unique_pd = tagged_articles_pd.sort_values(by=['canonical', 'liked'], ascending=False).reset_index(drop=True)\n",
    "to_drop = tagged_articles_unique_pd.duplicated(subset = ['canonical'], keep='first')\n",
    "to_drop = list(to_drop[to_drop == False].index.values)\n",
    "\n",
    "tagged_articles_unique_pd = tagged_articles_unique_pd[tagged_articles_unique_pd.index.isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2716\n",
      "2501\n"
     ]
    }
   ],
   "source": [
    "print(len(tagged_articles_pd))\n",
    "print(len(tagged_articles_unique_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ = list(tagged_articles_unique_pd['author']+' '+tagged_articles_unique_pd['title']+' '+tagged_articles_unique_pd['summary'])\n",
    "y_ = list(tagged_articles_unique_pd['liked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_shuffled = [i for i in range(len(X_))]\n",
    "random.shuffle(index_shuffled)\n",
    "X = []\n",
    "y = []\n",
    "for i in index_shuffled:\n",
    "    X.append(X_[i])\n",
    "    y.append(y_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('stopwords/english') as f:\n",
    "    stopwords = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def text_to_word_sequence(text,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True, split=\" \"):\n",
    "    if lower: text = text.lower()\n",
    "    if type(text) == str:\n",
    "        translate_table = {ord(c): ord(t) for c,t in zip(filters, split*len(filters)) }\n",
    "    else:\n",
    "        translate_table = maketrans(filters, split * len(filters))\n",
    "    text = text.translate(translate_table)\n",
    "    seq = text.split(split)\n",
    "    return [i for i in seq if i]\n",
    "\n",
    "print(len(X[4]))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    seq = text_to_word_sequence(X[i])\n",
    "    clean_seq = [word for word in seq if word not in stopwords]\n",
    "    X[i] = ' '.join(clean_seq)\n",
    "    \n",
    "print(len(X[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving or loading the data and LE and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "data_dict = {\n",
    "    'X' : X,\n",
    "    'y' : y\n",
    "}\n",
    "pickle.dump(data_dict,open(\"data.pickle\", \"wb\" ) )\n",
    "\n",
    "\n",
    "limit = int(0.75*len(X))\n",
    "print(limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "data_dict = pickle.load(open( \"data.pickle\", \"rb\" ) )\n",
    "X = data_dict['X']\n",
    "y = data_dict['y']\n",
    "limit = int(0.75*len(X))\n",
    "print(limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ = X\n",
    "y_ = y\n",
    "index_shuffled = [i for i in range(len(X_))]\n",
    "random.shuffle(index_shuffled)\n",
    "X = []\n",
    "y = []\n",
    "for i in index_shuffled:\n",
    "    X.append(X_[i])\n",
    "    y.append(y_[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF -> NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.84664536741214058, 0.9153791382943729)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "model_nb_tfidf = Pipeline([\n",
    "    (\"tfidf_vectorizer\", TfidfVectorizer()),\\\n",
    "    (\"nb\", BernoulliNB())\n",
    "])\n",
    "\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_nb_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_nb_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.846645367412 0.915379138294 1 626\n",
      "0.159744408946 % -  0\n",
      "\n",
      "\n",
      "Rejected 100.00% of wrong ones\n",
      "Accepted 1.03% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   8.30570688e-27],\n",
       "       [  1.00000000e+00,   9.85238929e-31],\n",
       "       [  1.00000000e+00,   1.72855324e-18],\n",
       "       [  1.00000000e+00,   3.32406901e-20],\n",
       "       [  1.00000000e+00,   6.48447509e-29]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_nb_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.15)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.10)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)/len(y_predicted)*100.0,\"% - \", int(sum(y_predicted)/len(y_predicted)*50.0))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVect -> NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.84504792332268375, 0.91601731601731617)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "model_nb = Pipeline([\n",
    "    #('vect', CountVectorizer(ngram_range=(1,3),stop_words='english')),\\\n",
    "    (\"count_vectorizer\", CountVectorizer(analyzer=rewrite)),\\\n",
    "    (\"nb\", BernoulliNB())\n",
    "])\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_nb.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.533546325879 0.46994372247 305 626\n",
      "48.7220447284 % -  24\n",
      "\n",
      "\n",
      "Rejected 52.74% of wrong ones\n",
      "Accepted 56.70% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.83709096,  0.16290904],\n",
       "       [ 0.89966041,  0.10033959],\n",
       "       [ 0.86289978,  0.13710022],\n",
       "       [ 0.82382712,  0.17617288],\n",
       "       [ 0.90048481,  0.09951519]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_nb.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.1296)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.1325)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF -> SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.85942492012779548, 0.90380838196886393)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "model_svc_tfidf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,3), min_df=1, max_df=1.0,stop_words='english')),\n",
    "    #('vect', CountVectorizer(ngram_range=(3,6),analyzer='char_wb', min_df=10, max_df=0.95,stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    (\"svc\", SVC(kernel='linear', probability=True, class_weight = 'balanced'))\n",
    "    #(\"svc\", SVC(kernel='linear', probability=True))\n",
    "])\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_svc_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_svc_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.543130990415 0.483110192189 331 626\n",
      "52.875399361 % -  26\n",
      "\n",
      "\n",
      "Rejected 50.85% of wrong ones\n",
      "Accepted 73.20% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.86538007,  0.13461993],\n",
       "       [ 0.95301607,  0.04698393],\n",
       "       [ 0.54068916,  0.45931084],\n",
       "       [ 0.48593804,  0.51406196],\n",
       "       [ 0.92803788,  0.07196212]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_svc_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.103)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]\n",
    "\n",
    "#(0.62077922077922076, 0.55417162908266104, 162, 385)\n",
    "#(42.077922077922075, '% - ', 21)\n",
    "#('Rejected good ones: ', 15, '/', 46)\n",
    "#('Accepted wrong ones:', 131, '/', 339)\n",
    "#('Rejected wrong ones: ', 208, '/', 339)\n",
    "#('Accepted good ones:', 31, '/', 46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-54.028753993610223"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/len(y_test)-sum(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.74281150159744413, 0.72492789072766561)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "model_svc_tfidf = Pipeline([\n",
    "    #('vect', CountVectorizer(ngram_range=(1,3), min_df=1, max_df=1.0,stop_words='english')),\n",
    "    ('vect', CountVectorizer(ngram_range=(3,6),analyzer='char_wb', min_df=10, max_df=0.95,stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    (\"svc\", SVC(kernel='linear', probability=True, class_weight = 'balanced'))\n",
    "    #(\"svc\", SVC(kernel='linear', probability=True))\n",
    "])\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_svc_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_svc_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.511182108626 0.450496232577 355 626\n",
      "56.7092651757 % -  28\n",
      "\n",
      "\n",
      "Rejected 46.69% of wrong ones\n",
      "Accepted 75.26% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.92043355,  0.07956645],\n",
       "       [ 0.91330185,  0.08669815],\n",
       "       [ 0.73624124,  0.26375876],\n",
       "       [ 0.6567993 ,  0.3432007 ],\n",
       "       [ 0.86044217,  0.13955783]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_svc_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.111)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]\n",
    "\n",
    "#(0.62077922077922076, 0.55417162908266104, 162, 385)\n",
    "#(42.077922077922075, '% - ', 21)\n",
    "#('Rejected good ones: ', 15, '/', 46)\n",
    "#('Accepted wrong ones:', 131, '/', 339)\n",
    "#('Rejected wrong ones: ', 208, '/', 339)\n",
    "#('Accepted good ones:', 31, '/', 46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count -> SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.86102236421725242, 0.91004330704814829)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "model_svc = Pipeline([\n",
    "    #('vect', CountVectorizer()),\n",
    "    ('vect', CountVectorizer(ngram_range=(1,3), min_df=1, max_df=1.0,stop_words='english')),\n",
    "    #('vect', CountVectorizer(ngram_range=(3,6),analyzer='char_wb', min_df=10, max_df=0.95,stop_words='english')),\n",
    "    #('tfidf', TfidfTransformer()),\n",
    "    (\"svc\", SVC(kernel='linear', probability=True, class_weight = 'balanced'))\n",
    "    #(\"svc\", SVC(kernel='linear', probability=True))\n",
    "])\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_svc.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_svc.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.506389776358 0.443629863022 346 626\n",
      "55.2715654952 % -  27\n",
      "\n",
      "\n",
      "Rejected 47.26% of wrong ones\n",
      "Accepted 69.07% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.87842511,  0.12157489],\n",
       "       [ 0.93458029,  0.06541971],\n",
       "       [ 0.66181316,  0.33818684],\n",
       "       [ 0.75052072,  0.24947928],\n",
       "       [ 0.88606537,  0.11393463]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_svc.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.11)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]\n",
    "\n",
    "#(0.62077922077922076, 0.55417162908266104, 162, 385)\n",
    "#(42.077922077922075, '% - ', 21)\n",
    "#('Rejected good ones: ', 15, '/', 46)\n",
    "#('Accepted wrong ones:', 131, '/', 339)\n",
    "#('Rejected wrong ones: ', 208, '/', 339)\n",
    "#('Accepted good ones:', 31, '/', 46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extree - pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'raz'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ['raz']\n",
    "'_'.join(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import models.embedding_matrix as embedding\n",
    "#reload(embedding)\n",
    "\n",
    "DATASETS_DIR = '../ml-research/datasets/'\n",
    "WIKI_DIR = DATASETS_DIR+'wiki.pl/'\n",
    "embeddings_file = WIKI_DIR+'wiki.pl.vec'\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "word2vec_pretrained = {}\n",
    "f = open(embeddings_file, encoding='utf-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = '_'.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    word2vec_pretrained[word] = coefs\n",
    "f.close()\n",
    "\n",
    "# train word2vec on all the texts - both training and test set\n",
    "# we're not using test labels, just texts so this is fine\n",
    "model = Word2Vec(X, size=100, window=5, min_count=5, workers=2)\n",
    "w2v_custom = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(next(iter(word2vec.items())))\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "# and a tf-idf version of the same\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(next(iter(word2vec.items())))\n",
    "        \n",
    "    def max_idf_func():\n",
    "        return self.max_idf\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        #tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf = TfidfVectorizer(analyzer=rewrite)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        self.max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            #lambda: max_idf, \n",
    "            self.max_idf_func, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extra Trees classifier is almost universally great, let's stack it with our embeddings\n",
    "\n",
    "etree_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=50))])\n",
    "etree_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"svc\", ExtraTreesClassifier(n_estimators=50))])\n",
    "svc_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"extra trees\", SVC(kernel=\"linear\", probability=True))])\n",
    "svc_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"svc\", SVC(kernel=\"linear\", probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.546325878594 0.483668515517 275 626\n",
      "43.9297124601 % -  21\n",
      "\n",
      "\n",
      "Rejected 56.33% of wrong ones\n",
      "Accepted 45.36% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.92,  0.08],\n",
       "       [ 0.76,  0.24],\n",
       "       [ 0.78,  0.22],\n",
       "       [ 0.62,  0.38],\n",
       "       [ 0.74,  0.26]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "etree_w2v.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = etree_w2v.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = etree_w2v.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.492012779553 0.426956064207 343 626\n",
      "54.7923322684 % -  27\n",
      "\n",
      "\n",
      "Rejected 46.69% of wrong ones\n",
      "Accepted 62.89% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.96,  0.04],\n",
       "       [ 0.78,  0.22],\n",
       "       [ 0.76,  0.24],\n",
       "       [ 0.74,  0.26],\n",
       "       [ 0.86,  0.14]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "etree_w2v_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = etree_w2v_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = etree_w2v_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.137)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26517571885 0.260382408155 523 626\n",
      "83.5463258786 % -  41\n",
      "\n",
      "\n",
      "Rejected 16.26% of wrong ones\n",
      "Accepted 82.47% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.85510646,  0.14489354],\n",
       "       [ 0.85092879,  0.14907121],\n",
       "       [ 0.86251459,  0.13748541],\n",
       "       [ 0.85062153,  0.14937847],\n",
       "       [ 0.8747115 ,  0.1252885 ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "svc_w2v.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = svc_w2v.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = svc_w2v.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.135)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.434504792332 0.37185657906 387 626\n",
      "61.821086262 % -  30\n",
      "\n",
      "\n",
      "Rejected 39.13% of wrong ones\n",
      "Accepted 67.01% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.86551962,  0.13448038],\n",
       "       [ 0.86960114,  0.13039886],\n",
       "       [ 0.85513103,  0.14486897],\n",
       "       [ 0.84606409,  0.15393591],\n",
       "       [ 0.89137907,  0.10862093]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "svc_w2v_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = svc_w2v_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = svc_w2v_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.145)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extree - Custom embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extra Trees classifier is almost universally great, let's stack it with our embeddings\n",
    "\n",
    "etree_w2v_custom = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v_custom)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=50))])\n",
    "etree_w2v_tfidf_custom = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v_custom)), \n",
    "                        (\"svc\", ExtraTreesClassifier(n_estimators=50))])\n",
    "svc_w2v_custom = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v_custom)), \n",
    "                        (\"extra trees\", SVC(kernel=\"linear\", probability=True))])\n",
    "svc_w2v_tfidf_custom = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v_custom)), \n",
    "                        (\"svc\", SVC(kernel=\"linear\", probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.576677316294 0.518277958272 270 626\n",
      "43.1309904153 % -  21\n",
      "\n",
      "\n",
      "Rejected 58.60% of wrong ones\n",
      "Accepted 52.58% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.98,  0.02],\n",
       "       [ 0.8 ,  0.2 ],\n",
       "       [ 0.92,  0.08],\n",
       "       [ 0.62,  0.38],\n",
       "       [ 0.8 ,  0.2 ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "etree_w2v_custom.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = etree_w2v_custom.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = etree_w2v_custom.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.477635782748 0.410896327504 344 626\n",
      "54.9520766773 % -  27\n",
      "\n",
      "\n",
      "Rejected 45.75% of wrong ones\n",
      "Accepted 58.76% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.98,  0.02],\n",
       "       [ 0.92,  0.08],\n",
       "       [ 0.86,  0.14],\n",
       "       [ 0.68,  0.32],\n",
       "       [ 0.78,  0.22]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "etree_w2v_tfidf_custom.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = etree_w2v_tfidf_custom.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = etree_w2v_tfidf_custom.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.137)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.311501597444 0.284468217531 490 626\n",
      "78.2747603834 % -  39\n",
      "\n",
      "\n",
      "Rejected 22.12% of wrong ones\n",
      "Accepted 80.41% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.83214848,  0.16785152],\n",
       "       [ 0.84460395,  0.15539605],\n",
       "       [ 0.83287225,  0.16712775],\n",
       "       [ 0.86248893,  0.13751107],\n",
       "       [ 0.86622438,  0.13377562]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "svc_w2v_custom.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = svc_w2v_custom.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = svc_w2v_custom.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.135)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.519169329073 0.455881625663 330 626\n",
      "52.7156549521 % -  26\n",
      "\n",
      "\n",
      "Rejected 49.53% of wrong ones\n",
      "Accepted 64.95% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.87600593,  0.12399407],\n",
       "       [ 0.86825217,  0.13174783],\n",
       "       [ 0.74844131,  0.25155869],\n",
       "       [ 0.80909423,  0.19090577],\n",
       "       [ 0.92030731,  0.07969269]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "svc_w2v_tfidf_custom.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = svc_w2v_tfidf_custom.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = svc_w2v_tfidf_custom.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.145)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression and SGD - both on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "logistic_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"logistic\", LogisticRegression())])\n",
    "logistic_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"logistic\", LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58785942492 0.530933297034 281 626\n",
      "44.8881789137 % -  22\n",
      "\n",
      "\n",
      "Rejected 58.22% of wrong ones\n",
      "Accepted 61.86% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.91300475,  0.08699525],\n",
       "       [ 0.876287  ,  0.123713  ],\n",
       "       [ 0.81748738,  0.18251262],\n",
       "       [ 0.8179039 ,  0.1820961 ],\n",
       "       [ 0.93249895,  0.06750105]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "logistic_w2v.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = logistic_w2v.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = logistic_w2v.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.586261980831 0.529121925201 282 626\n",
      "45.0479233227 % -  22\n",
      "\n",
      "\n",
      "Rejected 58.03% of wrong ones\n",
      "Accepted 61.86% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.91301352,  0.08698648],\n",
       "       [ 0.87565938,  0.12434062],\n",
       "       [ 0.81792818,  0.18207182],\n",
       "       [ 0.8166771 ,  0.1833229 ],\n",
       "       [ 0.93235949,  0.06764051]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "logistic_w2v_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = logistic_w2v_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = logistic_w2v_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "sgd_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"logistic\", SGDClassifier(loss = 'log'))])\n",
    "sgd_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"logistic\", SGDClassifier(loss = 'log'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.845047923323 0.890780343528 20 626\n",
      "3.19488817891 % -  1\n",
      "\n",
      "\n",
      "Rejected 98.11% of wrong ones\n",
      "Accepted 10.31% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  9.99925987e-01,   7.40129438e-05],\n",
       "       [  9.99048409e-01,   9.51590837e-04],\n",
       "       [  9.64826950e-01,   3.51730502e-02],\n",
       "       [  9.74513822e-01,   2.54861779e-02],\n",
       "       [  9.99986232e-01,   1.37679391e-05]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "sgd_w2v.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = sgd_w2v.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = sgd_w2v.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.846645367412 0.912465388711 3 626\n",
      "0.479233226837 % -  0\n",
      "\n",
      "\n",
      "Rejected 99.81% of wrong ones\n",
      "Accepted 2.06% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  9.99981792e-01,   1.82079612e-05],\n",
       "       [  9.99847635e-01,   1.52365436e-04],\n",
       "       [  9.95931515e-01,   4.06848476e-03],\n",
       "       [  9.97686029e-01,   2.31397114e-03],\n",
       "       [  9.99999189e-01,   8.10905966e-07]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "sgd_w2v_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = sgd_w2v_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = sgd_w2v_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Saving/loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def datetime_string():\n",
    "    localtime = time.localtime(time.time())\n",
    "    datetime_str = str(localtime.tm_year)\n",
    "    if(localtime.tm_mon < 10):\n",
    "        datetime_str += '0'\n",
    "    datetime_str += str(localtime.tm_mon)\n",
    "    if(localtime.tm_mday < 10):\n",
    "        datetime_str += '0'\n",
    "    datetime_str += str(localtime.tm_mday) + '_'\n",
    "    if(localtime.tm_hour < 10):\n",
    "        datetime_str += '0'\n",
    "    datetime_str += str(localtime.tm_hour)\n",
    "    if(localtime.tm_min < 10):\n",
    "        datetime_str += '0'\n",
    "    datetime_str += str(localtime.tm_min) + '_'\n",
    "    \n",
    "    return datetime_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "contents_dict = {\n",
    "    'model_svc_sklearn' : model_svc,\n",
    "    'model_svc_tfidf_sklearn' : model_svc_tfidf,\n",
    "    \n",
    "    'model_nb_sklearn' : model_nb,\n",
    "    'model_nb_tfidf_sklearn' : model_nb_tfidf,\n",
    "    \n",
    "    'etree_w2v_sklearn' : etree_w2v,\n",
    "    'etree_w2v_tfidf_sklearn' : etree_w2v_tfidf,\n",
    "    \n",
    "    'svc_w2v_sklearn' : svc_w2v,\n",
    "    'svc_w2v_tfidf_sklearn' : svc_w2v_tfidf,\n",
    "    \n",
    "    'etree_w2v_custom_sklearn' : etree_w2v_custom,\n",
    "    'etree_w2v_tfidf_custom_sklearn' : etree_w2v_tfidf_custom,\n",
    "    \n",
    "    'svc_w2v_custom_sklearn' : svc_w2v_custom,\n",
    "    'svc_w2v_tfidf_custom_sklearn' : svc_w2v_tfidf_custom,\n",
    "    \n",
    "    'logistic_w2v_sklearn' : logistic_w2v,\n",
    "    'logistic_w2v_tfidf_sklearn' : logistic_w2v_tfidf,\n",
    "    \n",
    "    'sgd_w2v_sklearn' : sgd_w2v,\n",
    "    'sgd_w2v_tfidf_sklearn' : sgd_w2v_tfidf\n",
    "}\n",
    "\n",
    "dat_str = datetime_string()\n",
    "with open(\"models/\"+dat_str+\"ensemble_models.pickle\", 'wb') as f:\n",
    "    pickle.dump(contents_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('models/'+dat_str+'ensemble_models.pickle', 'rb') as f:\n",
    "    contents_dict = pickle.load(f)\n",
    "\n",
    "model_svc = contents_dict['model_svc_sklearn']\n",
    "model_svc_tfidf = contents_dict['model_svc_tfidf_sklearn']\n",
    "\n",
    "model_nb = contents_dict['model_nb_sklearn']\n",
    "model_nb_tfidf = contents_dict['model_nb_tfidf_sklearn']\n",
    "\n",
    "etree_w2v = contents_dict['etree_w2v_sklearn']\n",
    "etree_w2v_tfidf = contents_dict['etree_w2v_tfidf_sklearn']\n",
    "\n",
    "svc_w2v = contents_dict['svc_w2v_sklearn']\n",
    "svc_w2v_tfidf = contents_dict['svc_w2v_tfidf_sklearn']\n",
    "\n",
    "etree_w2v_custom = contents_dict['etree_w2v_custom_sklearn']\n",
    "etree_w2v_tfidf_custom = contents_dict['etree_w2v_tfidf_custom_sklearn']\n",
    "\n",
    "svc_w2v_custom = contents_dict['svc_w2v_custom_sklearn']\n",
    "svc_w2v_tfidf_custom = contents_dict['svc_w2v_tfidf_custom_sklearn']\n",
    "\n",
    "logistic_w2v = contents_dict['logistic_w2v_sklearn']\n",
    "logistic_w2v_tfidf = contents_dict['logistic_w2v_tfidf_sklearn']\n",
    "\n",
    "sgd_w2v = contents_dict['sgd_w2v_sklearn']\n",
    "sgd_w2v_tfidf = contents_dict['sgd_w2v_tfidf_sklearn']\n",
    "\n",
    "#hyperparameters_cnn = contents_dict['hyperparameters_cnn']\n",
    "#hyperparameters_lstm = contents_dict['hyperparameters_lstm']\n",
    "\n",
    "#label_encoder = contents_dict['label_encoder']\n",
    "#tokenize = contents_dict['tokenizer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an ensembler - Without data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XE = X[limit:]\n",
    "yE = y[limit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.21574889e-01,   7.95664459e-02,   1.62909041e-01, ...,\n",
       "          8.69864809e-02,   7.40129438e-05,   1.82079612e-05],\n",
       "       [  6.54197136e-02,   8.66981483e-02,   1.00339593e-01, ...,\n",
       "          1.24340617e-01,   9.51590837e-04,   1.52365436e-04],\n",
       "       [  3.38186839e-01,   2.63758763e-01,   1.37100216e-01, ...,\n",
       "          1.82071820e-01,   3.51730502e-02,   4.06848476e-03],\n",
       "       ..., \n",
       "       [  1.93076372e-01,   1.20739214e-01,   1.16016954e-01, ...,\n",
       "          1.40714256e-01,   3.81290079e-03,   4.55353799e-04],\n",
       "       [  1.03825824e-01,   8.29335477e-02,   1.07106650e-01, ...,\n",
       "          8.67612935e-02,   8.51416976e-05,   1.59641229e-05],\n",
       "       [  1.01555166e-01,   1.58823533e-01,   1.40582320e-01, ...,\n",
       "          1.33796650e-01,   2.25872637e-03,   3.55119537e-04]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "XE_intermediate = np.column_stack((\n",
    "        model_svc.predict_proba(XE)[:,1],\n",
    "        model_svc_tfidf.predict_proba(XE)[:,1],\n",
    "    \n",
    "        model_nb.predict_proba(XE)[:,1],\n",
    "        model_nb_tfidf.predict_proba(XE)[:,1],\n",
    "\n",
    "        etree_w2v.predict_proba(XE)[:,1],\n",
    "        etree_w2v_tfidf.predict_proba(XE)[:,1],\n",
    "\n",
    "        svc_w2v.predict_proba(XE)[:,1],\n",
    "        svc_w2v_tfidf.predict_proba(XE)[:,1],\n",
    "\n",
    "        etree_w2v_custom.predict_proba(XE)[:,1],\n",
    "        etree_w2v_tfidf_custom.predict_proba(XE)[:,1],\n",
    "\n",
    "        svc_w2v_custom.predict_proba(XE)[:,1],\n",
    "        svc_w2v_tfidf_custom.predict_proba(XE)[:,1],\n",
    "\n",
    "        logistic_w2v.predict_proba(XE)[:,1],\n",
    "        logistic_w2v_tfidf.predict_proba(XE)[:,1],\n",
    "\n",
    "        sgd_w2v.predict_proba(XE)[:,1],\n",
    "        sgd_w2v_tfidf.predict_proba(XE)[:,1]\n",
    "    ))\n",
    "XE_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "limitE = int(0.8*len(XE_intermediate))\n",
    "print(limitE)\n",
    "\n",
    "X_train = XE_intermediate[:limitE]\n",
    "X_test = XE_intermediate[limitE:]\n",
    "\n",
    "y_train = yE[:limitE]\n",
    "y_test = yE[limitE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.563492063492 0.485714285714 63 126\n",
      "50.0 % -  25\n",
      "\n",
      "\n",
      "Rejected 53.57% of wrong ones\n",
      "Accepted 78.57% of good ones\n",
      "Median: 0.149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.87215598,  0.12784402],\n",
       "       [ 0.86738353,  0.13261647],\n",
       "       [ 0.8502556 ,  0.1497444 ],\n",
       "       [ 0.86755385,  0.13244615],\n",
       "       [ 0.7963666 ,  0.2036334 ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "logi = LogisticRegression()\n",
    "logi.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = logi.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = logi.predict_proba(X_test)\n",
    "y_predicted = (y_predicted_proba[:,1] > np.median(y_predicted_proba[:,1]))\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "print(\"Median: %.3f\" % np.median(y_predicted_proba[:,1]))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best ratings:\n",
    "Rejected 53.94% of wrong ones\n",
    "\n",
    "Accepted 77.05% of good ones\n",
    "\n",
    "Median: 0.043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "max_rej = 20.0\n",
    "max_acc = 20.0\n",
    "\n",
    "XE_intermediate_full = np.column_stack((\n",
    "        model_svc.predict_proba(XE)[:,1],\n",
    "        model_svc_tfidf.predict_proba(XE)[:,1],\n",
    "\n",
    "        model_nb.predict_proba(XE)[:,1],\n",
    "        model_nb_tfidf.predict_proba(XE)[:,1], #----------\n",
    "\n",
    "        etree_w2v.predict_proba(XE)[:,1],\n",
    "        etree_w2v_tfidf.predict_proba(XE)[:,1],\n",
    "\n",
    "        svc_w2v.predict_proba(XE)[:,1],\n",
    "        svc_w2v_tfidf.predict_proba(XE)[:,1], #----------\n",
    "\n",
    "        etree_w2v_custom.predict_proba(XE)[:,1],\n",
    "        etree_w2v_tfidf_custom.predict_proba(XE)[:,1],\n",
    "\n",
    "        svc_w2v_custom.predict_proba(XE)[:,1],\n",
    "        svc_w2v_tfidf_custom.predict_proba(XE)[:,1], #----------\n",
    "\n",
    "        logistic_w2v.predict_proba(XE)[:,1],\n",
    "        logistic_w2v_tfidf.predict_proba(XE)[:,1],\n",
    "\n",
    "        sgd_w2v.predict_proba(XE)[:,1],\n",
    "        sgd_w2v_tfidf.predict_proba(XE)[:,1]\n",
    "    ))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "1\n",
      "Rejected 73.21% of wrong ones\n",
      "Accepted 42.86% of good ones\n",
      "Median: 0.149\n",
      "\n",
      "\n",
      "[0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "2\n",
      "Rejected 75.00% of wrong ones\n",
      "Accepted 57.14% of good ones\n",
      "Median: 0.151\n",
      "\n",
      "\n",
      "[0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "3\n",
      "Rejected 75.00% of wrong ones\n",
      "Accepted 57.14% of good ones\n",
      "Median: 0.165\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]\n",
      "4\n",
      "Rejected 75.89% of wrong ones\n",
      "Accepted 64.29% of good ones\n",
      "Median: 0.166\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1]\n",
      "5\n",
      "Rejected 75.89% of wrong ones\n",
      "Accepted 64.29% of good ones\n",
      "Median: 0.168\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "6\n",
      "Rejected 77.68% of wrong ones\n",
      "Accepted 78.57% of good ones\n",
      "Median: 0.169\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
      "7\n",
      "Rejected 77.68% of wrong ones\n",
      "Accepted 78.57% of good ones\n",
      "Median: 0.169\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "8\n",
      "Rejected 77.68% of wrong ones\n",
      "Accepted 78.57% of good ones\n",
      "Median: 0.169\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]\n",
      "9\n",
      "Rejected 77.68% of wrong ones\n",
      "Accepted 78.57% of good ones\n",
      "Median: 0.169\n",
      "\n",
      "\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "10\n",
      "Rejected 77.68% of wrong ones\n",
      "Accepted 78.57% of good ones\n",
      "Median: 0.169\n",
      "\n",
      "\n",
      "[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "11\n",
      "Rejected 77.68% of wrong ones\n",
      "Accepted 78.57% of good ones\n",
      "Median: 0.169\n",
      "\n",
      "\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
      "12\n",
      "Rejected 77.68% of wrong ones\n",
      "Accepted 78.57% of good ones\n",
      "Median: 0.169\n",
      "\n",
      "\n",
      "[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]\n",
      "13\n",
      "Rejected 77.68% of wrong ones\n",
      "Accepted 78.57% of good ones\n",
      "Median: 0.169\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.0\n",
    "mask_list = []\n",
    "while(True):\n",
    "    threshold = threshold + 0.1\n",
    "    rand_vect = np.random.rand(16) > threshold\n",
    "    XE_intermediate = XE_intermediate_full[:,rand_vect]\n",
    "    if threshold > 0.85:\n",
    "        threshold = 0.0\n",
    "    if XE_intermediate.shape[1] < 1:\n",
    "        continue\n",
    "        \n",
    "    rand_vect_int = []\n",
    "    for i in range(len(rand_vect)):\n",
    "        if rand_vect[i] == True:\n",
    "            rand_vect_int.append(1)\n",
    "        else:\n",
    "            rand_vect_int.append(0)\n",
    "    if rand_vect_int in mask_list:\n",
    "        continue\n",
    "\n",
    "    limitE = int(0.8*len(XE_intermediate))\n",
    "    #print(limitE)\n",
    "\n",
    "    X_train = XE_intermediate[:limitE]\n",
    "    X_test = XE_intermediate[limitE:]\n",
    "\n",
    "    y_train = yE[:limitE]\n",
    "    y_test = yE[limitE:]\n",
    "\n",
    "    from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "    logi = LogisticRegression()\n",
    "    logi.fit(X_train, y_train)\n",
    "\n",
    "    y_predicted = logi.predict(X_test)\n",
    "    acc = accuracy_score(y_predicted, y_test)\n",
    "    f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "    y_predicted_proba = logi.predict_proba(X_test)\n",
    "    y_predicted = (y_predicted_proba[:,1] > np.percentile(y_predicted_proba[:,1],100-28.5))\n",
    "\n",
    "    acc = accuracy_score(y_predicted, y_test)\n",
    "    f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "    #print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "    #print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    rej = len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))\n",
    "    acc = len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)\n",
    "    \n",
    "    X_test_df = pd.DataFrame(X_test)\n",
    "    if (rej >= max_rej and acc >= max_acc):\n",
    "        max_rej = rej\n",
    "        \n",
    "        max_acc = acc\n",
    "        mask_list.append(rand_vect_int)\n",
    "        if len(mask_list) > 2000:\n",
    "            break\n",
    "        print(rand_vect_int)\n",
    "        print(len(mask_list))\n",
    "        print(\"Rejected %.2f%% of wrong ones\" % rej)\n",
    "        print(\"Accepted %.2f%% of good ones\" % acc)\n",
    "        print(\"Median: %.3f\\n\\n\" % np.median(y_predicted_proba[:,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0]\n",
    "41\n",
    "Rejected 72.32% of wrong ones\n",
    "Accepted 78.57% of good ones\n",
    "Median: 0.169\n",
    "\n",
    "\n",
    "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]\n",
    "42\n",
    "Rejected 72.32% of wrong ones\n",
    "Accepted 78.57% of good ones\n",
    "Median: 0.169\n",
    "\n",
    "percentile 33%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask_list_np = np.array([np.array(xi) for xi in mask_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask_list_np.sum(axis=0)/len(mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = mask_list_np.sum(axis=1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(a, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_vect = mask_list[-1]\n",
    "XE_intermediate = XE_intermediate_full[:,rand_vect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "limitE = int(0.8*len(XE_intermediate))\n",
    "print(limitE)\n",
    "\n",
    "X_train = XE_intermediate[:limitE]\n",
    "X_test = XE_intermediate[limitE:]\n",
    "\n",
    "y_train = yE[:limitE]\n",
    "y_test = yE[limitE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.714285714286 0.670867756136 32 126\n",
      "25.3968253968 % -  12\n",
      "\n",
      "\n",
      "Rejected 75.89% of wrong ones\n",
      "Accepted 35.71% of good ones\n",
      "Median: 0.134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.9003563 ,  0.0996437 ],\n",
       "       [ 0.87714897,  0.12285103],\n",
       "       [ 0.86583697,  0.13416303],\n",
       "       [ 0.87173636,  0.12826364],\n",
       "       [ 0.75371774,  0.24628226]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "logi = LogisticRegression()\n",
    "logi.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = logi.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = logi.predict_proba(X_test)\n",
    "y_predicted = (y_predicted_proba[:,1] > np.percentile(y_predicted_proba[:,1],100-33))\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "print(\"Median: %.3f\" % np.median(y_predicted_proba[:,1]))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
