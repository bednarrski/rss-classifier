{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from api import inoreader_api, inoreader_scraping\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape feeds starting from a concrete moment and store them in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=RPNqKG8HsllH\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=8zw66U74ix3t\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=legDeBDePL8T\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=9RhFPUmANlKO\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=PNPt0XMuhrKt\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=mZfWtjqsHQBc\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=2L4P8YDqIOM5\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=zonkfzuleBMu\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=si4IWb0WC106\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=gRgIMt0qWqU0\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=0OEECpK3fn1g\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=_TfkeKe0e3BE\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=HjwjHKepmaCk\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=WbmDdJQYzgnD\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=dBPH3jxM_H3z\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=AMnx4JOrurQn\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=ftJx6ZDPbZP7\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=elBJicD4tXno\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=WK4AMdBfHxs4\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=UUP0klB5ocNc\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=0fE9fB9iSY02\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=4bX3KBiHBbAw\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=WQ_OkgtIIzTw\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=orxW0xOoqSDX\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=QP2zqG_7zN2X\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=k63o61QbgmwF\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=RdHY0QWSP3rP\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=tgs7LGorKi0F\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=0_bfRC1XDzTq\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=BKEepmnjaNrr\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=QTS9GK_ozjyW\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=a9ZxNaC8l5IU\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=_NkZTDyJF9pI\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=gEs27kbsxKJf\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=plaJTBhaCWax\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=NzqWXDtZ9g41\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=6CShoe3dHqyp\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=M106HJiBTLQj\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=gyPYBlSX8qF6\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=CEc4_trQRWh2\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=u7stcbFnWSzz\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=lPIk1DCXHC3n\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=8Gx2pI4cnd5D\n",
      "(200, 'OK')\n",
      "860\n"
     ]
    }
   ],
   "source": [
    "start_time = 1508112001\n",
    "feed = 'user/-/label/arXiv'\n",
    "file_name = 'articles_raw.json'\n",
    "\n",
    "number_scraped = inoreader_scraping.scrape(feed, start_time, file_name)\n",
    "print(number_scraped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the feeds from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(file_name, 'r')\n",
    "json_string = f.read()\n",
    "f.close\n",
    "\n",
    "json_object = json.loads(json_string)\n",
    "all_articles = json_object['all_articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_abstract_string(abs_dict):\n",
    "    abs_string = abs_dict['content']\n",
    "    abs_string = re.search('<p>(.*)</p>', abs_string, flags=re.DOTALL)\n",
    "    return abs_string.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_canonical_string(canonical_list):\n",
    "    url_dict = canonical_list[0]\n",
    "    url = url_dict['href']\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create preprocessed and cleaned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/ipykernel/__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "all_articles_pd = pd.DataFrame(all_articles)\n",
    "all_articles_pd = all_articles_pd[['canonical', 'author', 'categories', 'published', 'title', 'summary']]\n",
    "\n",
    "for i in range(len(all_articles_pd)):\n",
    "    all_articles_pd['summary'][i] = get_abstract_string(all_articles_pd['summary'][i])\n",
    "    all_articles_pd['canonical'][i] = get_canonical_string(all_articles_pd['canonical'][i])\n",
    "    \n",
    "all_articles_pd['read'] = False\n",
    "all_articles_pd['liked'] = False\n",
    "\n",
    "for i in range(len(all_articles_pd)):\n",
    "    if 'user/1005689817/state/com.google/read' in all_articles_pd['categories'][i]:\n",
    "        all_articles_pd['read'][i] = True\n",
    "    if 'user/1005689817/state/com.google/like' in all_articles_pd['categories'][i]:\n",
    "        all_articles_pd['liked'][i] = True\n",
    "\n",
    "del all_articles_pd['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all_articles_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_articles_pd = all_articles_pd[all_articles_pd['read'] == True]\n",
    "\n",
    "with open('old_articles_proto2.pickle', 'rb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    old_tagged_articles_pd = pickle.load(f)\n",
    "    \n",
    "tagged_articles_pd = pd.concat([tagged_articles_pd, old_tagged_articles_pd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_articles_unique_pd = tagged_articles_pd.sort_values(by=['canonical', 'liked'], ascending=False).reset_index(drop=True)\n",
    "to_drop = tagged_articles_unique_pd.duplicated(subset = ['canonical'], keep='first')\n",
    "to_drop = list(to_drop[to_drop == False].index.values)\n",
    "\n",
    "tagged_articles_unique_pd = tagged_articles_unique_pd[tagged_articles_unique_pd.index.isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2156\n",
      "1997\n"
     ]
    }
   ],
   "source": [
    "print(len(tagged_articles_pd))\n",
    "print(len(tagged_articles_unique_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ = list(tagged_articles_unique_pd['author']+' '+tagged_articles_unique_pd['title']+' '+tagged_articles_unique_pd['summary'])\n",
    "y_ = list(tagged_articles_unique_pd['liked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_shuffled = [i for i in range(len(X_))]\n",
    "random.shuffle(index_shuffled)\n",
    "X = []\n",
    "y = []\n",
    "for i in index_shuffled:\n",
    "    X.append(X_[i])\n",
    "    y.append(y_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.preprocessing.text\n",
    "from string import maketrans\n",
    "\n",
    "def text_to_word_sequence(text,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True, split=\" \"):\n",
    "    if lower: text = text.lower()\n",
    "    if type(text) == unicode:\n",
    "        translate_table = {ord(c): ord(t) for c,t in zip(filters, split*len(filters)) }\n",
    "    else:\n",
    "        translate_table = maketrans(filters, split * len(filters))\n",
    "    text = text.translate(translate_table)\n",
    "    seq = text.split(split)\n",
    "    return [i for i in seq if i]\n",
    "    \n",
    "keras.preprocessing.text.text_to_word_sequence = text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('stopwords/english') as f:\n",
    "    stopwords = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658\n",
      "523\n"
     ]
    }
   ],
   "source": [
    "print(len(X[4]))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    seq = text_to_word_sequence(X[i])\n",
    "    clean_seq = [word for word in seq if word not in stopwords]\n",
    "    X[i] = ' '.join(clean_seq)\n",
    "    \n",
    "print(len(X[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.84499999999999997, 0.9159891598915989)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "model_nb_tfidf = Pipeline([\n",
    "    (\"tfidf_vectorizer\", TfidfVectorizer()),\\\n",
    "    (\"nb\", BernoulliNB())\n",
    "])\n",
    "\n",
    "limit = int(0.8*len(X))\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_nb_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_nb_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.84499999999999997, 0.9159891598915989, 0, 400)\n",
      "(0.0, '% - ', 0)\n",
      "\n",
      "\n",
      "('Rejected good ones: ', 62, '/', 62)\n",
      "('Accepted wrong ones:', 0, '/', 338)\n",
      "\n",
      "\n",
      "('Rejected wrong ones: ', 338, '/', 338)\n",
      "('Accepted good ones:', 0, '/', 62)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   1.97451476e-32],\n",
       "       [  1.00000000e+00,   2.15982820e-27],\n",
       "       [  1.00000000e+00,   1.45567847e-26],\n",
       "       [  1.00000000e+00,   9.28069391e-24],\n",
       "       [  1.00000000e+00,   1.29122415e-30],\n",
       "       [  1.00000000e+00,   3.68980531e-31],\n",
       "       [  1.00000000e+00,   1.44076672e-32],\n",
       "       [  1.00000000e+00,   6.14462879e-36],\n",
       "       [  1.00000000e+00,   3.73605272e-27],\n",
       "       [  1.00000000e+00,   2.09078447e-29],\n",
       "       [  1.00000000e+00,   7.28781746e-34],\n",
       "       [  1.00000000e+00,   6.47706825e-32],\n",
       "       [  1.00000000e+00,   4.89385962e-25],\n",
       "       [  1.00000000e+00,   1.28037260e-27],\n",
       "       [  1.00000000e+00,   6.26001717e-28],\n",
       "       [  1.00000000e+00,   1.47593448e-30],\n",
       "       [  1.00000000e+00,   6.76343405e-39],\n",
       "       [  1.00000000e+00,   2.94059614e-33],\n",
       "       [  1.00000000e+00,   5.72337714e-31],\n",
       "       [  1.00000000e+00,   1.29907326e-34]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_nb_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.15)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.15)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)/len(y_predicted)*100.0,\"% - \", int(sum(y_predicted)/len(y_predicted)*50.0))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (y_predicted == False)]), \"/\", sum(y_test))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (y_predicted == True)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == y_test) & (y_predicted == False)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == y_test) & (y_predicted == True)]), \"/\", sum(y_test))\n",
    "\n",
    "y_predicted_proba[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.82499999999999996, 0.84497354497354504)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "model_svc = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    (\"svc\", SVC(kernel='linear', probability=True, class_weight = 'balanced'))\n",
    "    #(\"svc\", SVC(kernel='linear', probability=True))\n",
    "])\n",
    "\n",
    "limit = int(0.8*len(X))\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_svc.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_svc.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_predicted)*100.0/len(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6875, 0.64851609128637688, 135, 400)\n",
      "(33.75, '% - ', 16)\n",
      "\n",
      "\n",
      "('Rejected good ones: ', 26, '/', 62)\n",
      "('Accepted wrong ones:', 99, '/', 338)\n",
      "\n",
      "\n",
      "('Rejected wrong ones: ', 239, '/', 338)\n",
      "('Accepted good ones:', 36, '/', 62)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.97338076,  0.02661924],\n",
       "       [ 0.7667114 ,  0.2332886 ],\n",
       "       [ 0.83590968,  0.16409032],\n",
       "       [ 0.77966527,  0.22033473],\n",
       "       [ 0.82370653,  0.17629347],\n",
       "       [ 0.93947894,  0.06052106],\n",
       "       [ 0.90484331,  0.09515669],\n",
       "       [ 0.96202216,  0.03797784],\n",
       "       [ 0.88106343,  0.11893657],\n",
       "       [ 0.86459522,  0.13540478],\n",
       "       [ 0.81555476,  0.18444524],\n",
       "       [ 0.74705481,  0.25294519],\n",
       "       [ 0.55774601,  0.44225399],\n",
       "       [ 0.72243058,  0.27756942],\n",
       "       [ 0.88776387,  0.11223613],\n",
       "       [ 0.8561867 ,  0.1438133 ],\n",
       "       [ 0.95899448,  0.04100552],\n",
       "       [ 0.89837069,  0.10162931],\n",
       "       [ 0.93964049,  0.06035951],\n",
       "       [ 0.91183766,  0.08816234]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_svc.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.15)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (y_predicted == False)]), \"/\", sum(y_test))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (y_predicted == True)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == y_test) & (y_predicted == False)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == y_test) & (y_predicted == True)]), \"/\", sum(y_test))\n",
    "\n",
    "y_predicted_proba[0:20]\n",
    "\n",
    "#(0.62077922077922076, 0.55417162908266104, 162, 385)\n",
    "#(42.077922077922075, '% - ', 21)\n",
    "#('Rejected good ones: ', 15, '/', 46)\n",
    "#('Accepted wrong ones:', 131, '/', 339)\n",
    "#('Rejected wrong ones: ', 208, '/', 339)\n",
    "#('Accepted good ones:', 31, '/', 46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Input, Embedding, SimpleRNN, Dense, Activation, TimeDistributed, Bidirectional, LSTM, GaussianNoise)\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model #Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASETS_DIR = '../ml-research/datasets/'\n",
    "GLOVE_DIR = DATASETS_DIR+'glove.6B/'\n",
    "WIKI_EN_DIR = DATASETS_DIR+'wiki.en/'\n",
    "#embeddings_file = os.path.join(GLOVE_DIR, 'glove.6B.300d.txt')\n",
    "embeddings_file = os.path.join(WIKI_EN_DIR, 'wiki.en.vec')\n",
    "\n",
    "# Word embeddings' constraints\n",
    "MAX_NB_WORDS = 20000  # Number of most common words for tokenizer\n",
    "EMBEDDING_DIM = 300   # Embeddings dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.preprocessing.text\n",
    "from string import maketrans\n",
    "\n",
    "def text_to_word_sequence(text,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True, split=\" \"):\n",
    "    if lower: text = text.lower()\n",
    "    if type(text) == unicode:\n",
    "        translate_table = {ord(c): ord(t) for c,t in zip(filters, split*len(filters)) }\n",
    "    else:\n",
    "        translate_table = maketrans(filters, split * len(filters))\n",
    "    text = text.translate(translate_table)\n",
    "    seq = text.split(split)\n",
    "    return [i for i in seq if i]\n",
    "    \n",
    "keras.preprocessing.text.text_to_word_sequence = text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22928 unique tokens.\n",
      "87\n",
      "22927\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing and creating word index\n",
    "\n",
    "additional_words = ['unk', 'num']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X+additional_words)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# inversing the word_index.\n",
    "index_word = dict((k,v) for v,k in word_index.items())\n",
    "\n",
    "# example\n",
    "print(word_index['adversarial'])\n",
    "print(word_index['unk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n"
     ]
    }
   ],
   "source": [
    "import models.embedding_matrix as embedding\n",
    "\n",
    "embedding_matrix = embedding.create_embedding_matrix(embeddings_file, MAX_NB_WORDS, EMBEDDING_DIM, word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Input, Embedding, SimpleRNN, Dense, Activation, TimeDistributed, Bidirectional,\n",
    "                          LSTM, GaussianNoise,Conv1D, MaxPooling1D, Flatten, Dropout)\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'conv_units': 64,\n",
    "    'hidden_units_1': 64,\n",
    "    'hidden_units_2': 32,\n",
    "    'dropout': 0.5,\n",
    "    'pooling' : 5,\n",
    "    'gauss_stddev' : 0.001,\n",
    "    'epochs' : 25,\n",
    "    'folds' : 5,\n",
    "    'nclasses' : 2,\n",
    "    'max_seq_len' : 300,\n",
    "    'learning_rate' : 0.001,\n",
    "    'stopwords' : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "X_sequences_len = []\n",
    "for item in X_sequences:\n",
    "    X_sequences_len.append(\n",
    "                            min( len(item), 1000\n",
    "                               ))\n",
    "    \n",
    "X_sequences_padded = pad_sequences(X_sequences, maxlen=hyperparameters['max_seq_len'])\n",
    "\n",
    "plt.hist(X_sequences_len, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "\n",
    "y_num = label_encoder.transform(y)\n",
    "y_matrix = to_categorical(y_num,hyperparameters['nclasses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(len(y)-sum(y))/sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import utils.evaluation as evaluation\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "class_weight = {0 : 1.,\n",
    "    1: (len(y)-sum(y))/sum(y)}\n",
    "\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "cm_summed = np.zeros((2,2))\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "for train_index, test_index in kf.split(X_sequences):\n",
    "    K.clear_session()\n",
    "    \n",
    "    X_sequences_padded = pad_sequences(X_sequences, maxlen=hyperparameters['max_seq_len'])\n",
    "\n",
    "    num_words = embedding_matrix.shape[0]\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=hyperparameters['max_seq_len'],\n",
    "                                trainable=False)\n",
    "\n",
    "    # train a 1D convnet with global maxpooling\n",
    "    sequence_input = Input(shape=(hyperparameters['max_seq_len'],), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    #x = GaussianNoise(hyperparameters['gauss_stddev'])(embedded_sequences)\n",
    "    x = embedded_sequences\n",
    "    x = Conv1D(2*hyperparameters['conv_units'], 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(hyperparameters['conv_units'], 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(hyperparameters['hidden_units_1'], activation='relu')(x)\n",
    "    x = Dropout(hyperparameters['dropout'])(x)\n",
    "    x = Dense(hyperparameters['hidden_units_2'], activation='relu')(x)\n",
    "    preds = Dense(hyperparameters['nclasses'], activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    \n",
    "    X_train, X_test = X_sequences_padded[train_index], X_sequences_padded[test_index]\n",
    "    y_train, y_test = y_matrix[train_index], y_matrix[test_index]\n",
    "\n",
    "    # train\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=128,\n",
    "              epochs=hyperparameters['epochs'],\n",
    "              class_weight=class_weight,\n",
    "              validation_data=(X_test, y_test))\n",
    "\n",
    "    # run\n",
    "    ground_truth = y_test.argmax(1)\n",
    "    predictions = [list(map(lambda x: x, model.predict_on_batch(np.asarray([x])).argmax(1)))[0] \\\n",
    "            for x in X_test]\n",
    "    \n",
    "    \n",
    "    acc = evaluation.accuracy(ground_truth, predictions)\n",
    "    f1 = evaluation.eval_f1_score(ground_truth, predictions)    \n",
    "    cm = evaluation.cm_matrix(ground_truth, predictions)\n",
    "    acc_list.append(acc)\n",
    "    f1_list.append(f1)\n",
    "    cm_summed = cm_summed + cm\n",
    "    print(acc, f1)\n",
    "    print(cm)\n",
    "    \n",
    "    #break\n",
    "    #del model\n",
    "    #K.clear_session()\n",
    "    \n",
    "print(np.asarray(acc).mean(), np.asarray(f1).mean())\n",
    "print(cm_summed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth = y_test.argmax(1)\n",
    "predictions = [list(map(lambda x: x, model.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "        for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_proba_np = np.asarray(predictions)\n",
    "#y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "y_predicted = (y_predicted_proba_np[:,1] > 0.0002)\n",
    "\n",
    "acc = accuracy_score(y_predicted, ground_truth)\n",
    "f1 = f1_score(y_predicted, ground_truth, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*30.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != ground_truth) & (y_predicted == False)]), \"/\", sum(ground_truth))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != ground_truth) & (y_predicted == True)]), \"/\", len(ground_truth)-sum(ground_truth))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == ground_truth) & (y_predicted == False)]), \"/\", len(ground_truth)-sum(ground_truth))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == ground_truth) & (y_predicted == True)]), \"/\", sum(ground_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# evaluate\n",
    "y_predicted = [list(map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x])).argmax(1)))[0] \\\n",
    "               for x in x_test]\n",
    "y_test = y[limit:]\n",
    "y_test = to_categorical(y_test,2)\n",
    "y_test = y_test.argmax(1)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted))\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Total good ones:\", sum(y_test))\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (np.asarray(y_predicted) == 0)]))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (np.asarray(y_predicted) == 1)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted_proba = [list(map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "               for x in x_test]\n",
    "y_predicted_proba_np = np.asarray(y_predicted_proba)\n",
    "#y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "y_predicted = (y_predicted_proba_np[:,1] > 0.2)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)/len(y_predicted)*100.0,\"% - \", int(sum(y_predicted)/len(y_predicted)*30.0))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (y_predicted == False)]), \"/\", sum(y_test))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (y_predicted == True)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == y_test) & (y_predicted == False)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == y_test) & (y_predicted == True)]), \"/\", sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "reload(runner)\n",
    "# Padding sequences\n",
    "x_train = pad_sequences(sequences_train, maxlen=hyperparameters['max_seq_len'])\n",
    "x_test = pad_sequences(sequences_test, maxlen=hyperparameters['max_seq_len'])\n",
    "\n",
    "dataset = x_train, y_train, x_test, y_test, embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "result_cnn, model_cnn = runner.build_train_run(dataset, le, hyperparameters_cnn, save=False, return_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "import numpy as np\n",
    "y_predicted = [list(map(lambda x: x, model_cnn.predict_on_batch(np.asarray([x])).argmax(1)))[0] \\\n",
    "               for x in x_test]\n",
    "y_test = y[limit:]\n",
    "y_test = to_categorical(y_test,hyperparameters['nclasses'])\n",
    "y_test = y_test.argmax(1)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted))\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Total good ones:\", sum(y_test))\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (np.asarray(y_predicted) == 0)]))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (np.asarray(y_predicted) == 1)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted_proba = [list(map(lambda x: x, model_cnn.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "               for x in x_test]\n",
    "y_predicted_proba_np = np.asarray(y_predicted_proba)\n",
    "\n",
    "#y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "y_predicted = (y_predicted_proba_np[:,1] > 0.0005)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)/len(y_predicted)*100.0,\"% - \", int(sum(y_predicted)/len(y_predicted)*30.0))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (y_predicted == False)]), \"/\", sum(y_test))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (y_predicted == True)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == y_test) & (y_predicted == False)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == y_test) & (y_predicted == True)]), \"/\", sum(y_test))\n",
    "\n",
    "y_predicted_proba[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted_proba = [list(map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "               for x in x_test]\n",
    "y_predicted_proba_np = np.asarray(y_predicted_proba)\n",
    "\n",
    "y_predicted_proba[0:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted_proba = [list(map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "               for x in x_test]\n",
    "y_predicted_proba_np = np.asarray(y_predicted_proba)\n",
    "#y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "y_predicted = (y_predicted_proba_np[:,1] > 0.2)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)/len(y_predicted)*100.0,\"% - \", int(sum(y_predicted)/len(y_predicted)*30.0))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (y_predicted == False)]), \"/\", sum(y_test))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (y_predicted == True)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == y_test) & (y_predicted == False)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == y_test) & (y_predicted == True)]), \"/\", sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
