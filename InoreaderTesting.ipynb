{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from api import inoreader_api, inoreader_scraping\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape feeds starting from a concrete moment and store them in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=puSBdNLy8hB3\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=dZdnWAkbhq8g\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=ShLUXm0iDw4d\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=IyXcsZCmjszf\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=E4E1U4faR_Jh\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=fMzIQQtUqKmD\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=gihZfeAH6Tbt\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=PU6jyS1_dBNH\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=zlBS8aBbiqNz\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Eta3aH_8fF8H\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=IGRhK6Aqg_8Y\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=1gez7Jxns0rU\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Qiyf8Iwq7fsY\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=4ZY2UJe81jSE\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=3dR3Dgrh2r_b\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=BKryINKSMyWr\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=2WypwCaPwCR0\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=JC6CxnpI5mHO\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=0ScL3TkrCCdu\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=B8bMbsO3xjtu\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=TIgOTgJ_QLnq\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=lR2oKMd2rGPd\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=14mBFedl5Djf\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=9h9TyWc6ybTQ\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=J8L9WF7MNiR8\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=oUOPCu3TcfT6\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=jnNNJXim4US2\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=k37DoIrgp21E\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=k8cR2QXkDc7i\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=dPqPjhWE5iXE\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=CS_LSFiCQ1Q9\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=s68_64YgwNpH\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=y4MIa563pTkq\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=aWfc8_ziNap0\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=aHTd4mTWfcHm\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=cRQERUfDw_2t\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=PzndbM3SCQKs\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=xeCx2nQDd0fi\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=1SFWAnQBUGcX\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=YCrZxM0aaiy0\n",
      "200 OK\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Z8U_9E7YStLY\n"
     ]
    }
   ],
   "source": [
    "start_time = 1508112001\n",
    "feed = 'user/-/label/arXiv'\n",
    "file_name = 'articles_raw.json'\n",
    "\n",
    "number_scraped = inoreader_scraping.scrape(feed, start_time, file_name)\n",
    "print(number_scraped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the feeds from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(file_name, 'r')\n",
    "json_string = f.read()\n",
    "f.close\n",
    "\n",
    "json_object = json.loads(json_string)\n",
    "all_articles = json_object['all_articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_abstract_string(abs_dict):\n",
    "    abs_string = abs_dict['content']\n",
    "    abs_string = re.search('<p>(.*)</p>', abs_string, flags=re.DOTALL)\n",
    "    return abs_string.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_canonical_string(canonical_list):\n",
    "    url_dict = canonical_list[0]\n",
    "    url = url_dict['href']\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create preprocessed and cleaned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_pd = pd.DataFrame(all_articles)\n",
    "all_articles_pd = all_articles_pd[['canonical', 'author', 'categories', 'published', 'title', 'summary']]\n",
    "\n",
    "for i in range(len(all_articles_pd)):\n",
    "    all_articles_pd['summary'][i] = get_abstract_string(all_articles_pd['summary'][i])\n",
    "    all_articles_pd['canonical'][i] = get_canonical_string(all_articles_pd['canonical'][i])\n",
    "    \n",
    "all_articles_pd['read'] = False\n",
    "all_articles_pd['liked'] = False\n",
    "\n",
    "for i in range(len(all_articles_pd)):\n",
    "    if 'user/1005689817/state/com.google/read' in all_articles_pd['categories'][i]:\n",
    "        all_articles_pd['read'][i] = True\n",
    "    if 'user/1005689817/state/com.google/like' in all_articles_pd['categories'][i]:\n",
    "        all_articles_pd['liked'][i] = True\n",
    "\n",
    "del all_articles_pd['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all_articles_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_articles_pd = all_articles_pd[all_articles_pd['read'] == True]\n",
    "\n",
    "with open('old_articles_proto2.pickle', 'rb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    old_tagged_articles_pd = pickle.load(f)\n",
    "    \n",
    "tagged_articles_pd = pd.concat([tagged_articles_pd, old_tagged_articles_pd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_articles_unique_pd = tagged_articles_pd.sort_values(by=['canonical', 'liked'], ascending=False).reset_index(drop=True)\n",
    "to_drop = tagged_articles_unique_pd.duplicated(subset = ['canonical'], keep='first')\n",
    "to_drop = list(to_drop[to_drop == False].index.values)\n",
    "\n",
    "tagged_articles_unique_pd = tagged_articles_unique_pd[tagged_articles_unique_pd.index.isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tagged_articles_pd))\n",
    "print(len(tagged_articles_unique_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ = list(tagged_articles_unique_pd['author']+' '+tagged_articles_unique_pd['title']+' '+tagged_articles_unique_pd['summary'])\n",
    "y_ = list(tagged_articles_unique_pd['liked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_shuffled = [i for i in range(len(X_))]\n",
    "random.shuffle(index_shuffled)\n",
    "X = []\n",
    "y = []\n",
    "for i in index_shuffled:\n",
    "    X.append(X_[i])\n",
    "    y.append(y_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.preprocessing.text\n",
    "from string import maketrans\n",
    "\n",
    "def text_to_word_sequence(text,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True, split=\" \"):\n",
    "    if lower: text = text.lower()\n",
    "    if type(text) == unicode:\n",
    "        translate_table = {ord(c): ord(t) for c,t in zip(filters, split*len(filters)) }\n",
    "    else:\n",
    "        translate_table = maketrans(filters, split * len(filters))\n",
    "    text = text.translate(translate_table)\n",
    "    seq = text.split(split)\n",
    "    return [i for i in seq if i]\n",
    "    \n",
    "keras.preprocessing.text.text_to_word_sequence = text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('stopwords/english') as f:\n",
    "    stopwords = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X[4]))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    seq = text_to_word_sequence(X[i])\n",
    "    clean_seq = [word for word in seq if word not in stopwords]\n",
    "    X[i] = ' '.join(clean_seq)\n",
    "    \n",
    "print(len(X[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "model_nb_tfidf = Pipeline([\n",
    "    (\"tfidf_vectorizer\", TfidfVectorizer()),\\\n",
    "    (\"nb\", BernoulliNB())\n",
    "])\n",
    "\n",
    "limit = int(0.8*len(X))\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_nb_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_nb_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_proba = model_nb_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.15)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.15)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)/len(y_predicted)*100.0,\"% - \", int(sum(y_predicted)/len(y_predicted)*50.0))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (y_predicted == False)]), \"/\", sum(y_test))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (y_predicted == True)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == y_test) & (y_predicted == False)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == y_test) & (y_predicted == True)]), \"/\", sum(y_test))\n",
    "\n",
    "y_predicted_proba[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "model_svc = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,3), min_df=1, max_df=1.0,stop_words='english')),\n",
    "    #('vect', CountVectorizer(ngram_range=(3,6),analyzer='char_wb', min_df=10, max_df=0.95,stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    (\"svc\", SVC(kernel='linear', probability=True, class_weight = 'balanced'))\n",
    "    #(\"svc\", SVC(kernel='linear', probability=True))\n",
    "])\n",
    "\n",
    "limit = int(0.8*len(X))\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_svc.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_svc.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_predicted)*100.0/len(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_proba = model_svc.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (y_predicted == False)]), \"/\", sum(y_test))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (y_predicted == True)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == y_test) & (y_predicted == False)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == y_test) & (y_predicted == True)]), \"/\", sum(y_test))\n",
    "\n",
    "y_predicted_proba[0:20]\n",
    "\n",
    "#(0.62077922077922076, 0.55417162908266104, 162, 385)\n",
    "#(42.077922077922075, '% - ', 21)\n",
    "#('Rejected good ones: ', 15, '/', 46)\n",
    "#('Accepted wrong ones:', 131, '/', 339)\n",
    "#('Rejected wrong ones: ', 208, '/', 339)\n",
    "#('Accepted good ones:', 31, '/', 46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Input, Embedding, SimpleRNN, Dense, Activation, TimeDistributed, Bidirectional, LSTM, GaussianNoise)\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model #Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASETS_DIR = '../ml-research/datasets/'\n",
    "GLOVE_DIR = DATASETS_DIR+'glove.6B/'\n",
    "WIKI_EN_DIR = DATASETS_DIR+'wiki.en/'\n",
    "#embeddings_file = os.path.join(GLOVE_DIR, 'glove.6B.300d.txt')\n",
    "embeddings_file = os.path.join(WIKI_EN_DIR, 'wiki.en.vec')\n",
    "\n",
    "# Word embeddings' constraints\n",
    "MAX_NB_WORDS = 20000  # Number of most common words for tokenizer\n",
    "EMBEDDING_DIM = 300   # Embeddings dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.preprocessing.text\n",
    "from string import maketrans\n",
    "\n",
    "def text_to_word_sequence(text,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True, split=\" \"):\n",
    "    if lower: text = text.lower()\n",
    "    if type(text) == unicode:\n",
    "        translate_table = {ord(c): ord(t) for c,t in zip(filters, split*len(filters)) }\n",
    "    else:\n",
    "        translate_table = maketrans(filters, split * len(filters))\n",
    "    text = text.translate(translate_table)\n",
    "    seq = text.split(split)\n",
    "    return [i for i in seq if i]\n",
    "    \n",
    "keras.preprocessing.text.text_to_word_sequence = text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24825 unique tokens.\n",
      "78\n",
      "24824\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing and creating word index\n",
    "\n",
    "additional_words = ['unk', 'num']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X+additional_words)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# inversing the word_index.\n",
    "index_word = dict((k,v) for v,k in word_index.items())\n",
    "\n",
    "# example\n",
    "print(word_index['adversarial'])\n",
    "print(word_index['unk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 2519371 word vectors.\n",
      "Creating Word Embeddings matrix...\n",
      "Word Embeddings matrix was successfuly created.\n"
     ]
    }
   ],
   "source": [
    "import models.embedding_matrix as embedding\n",
    "\n",
    "embedding_matrix = embedding.create_embedding_matrix(embeddings_file, MAX_NB_WORDS, EMBEDDING_DIM, word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Input, Embedding, SimpleRNN, Dense, Activation, TimeDistributed, Bidirectional,\n",
    "                          LSTM, GaussianNoise,Conv1D, MaxPooling1D, Flatten, Dropout)\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'conv_units': 128,\n",
    "    'hidden_units_1': 128,\n",
    "    'hidden_units_2': 64,\n",
    "    'dropout': 0.5,\n",
    "    'pooling' : 5,\n",
    "    'gauss_stddev' : 0.0002,\n",
    "    'epochs' : 35,\n",
    "    'folds' : 5,\n",
    "    'nclasses' : 2,\n",
    "    'max_seq_len' : 300,\n",
    "    'learning_rate' : 0.001,\n",
    "    'stopwords' : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa094daebd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "X_sequences_len = []\n",
    "for item in X_sequences:\n",
    "    X_sequences_len.append(\n",
    "                            min( len(item), 1000\n",
    "                               ))\n",
    "    \n",
    "X_sequences_padded = pad_sequences(X_sequences, maxlen=hyperparameters['max_seq_len'])\n",
    "\n",
    "plt.hist(X_sequences_len, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "\n",
    "y_num = label_encoder.transform(y)\n",
    "y_matrix = to_categorical(y_num,hyperparameters['nclasses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(len(y)-sum(y))/sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1832 samples, validate on 459 samples\n",
      "Epoch 1/35\n",
      "1832/1832 [==============================] - 27s - loss: 1.2102 - categorical_accuracy: 0.6163 - val_loss: 0.7033 - val_categorical_accuracy: 0.1220\n",
      "Epoch 2/35\n",
      "1832/1832 [==============================] - 27s - loss: 1.2031 - categorical_accuracy: 0.1474 - val_loss: 0.7266 - val_categorical_accuracy: 0.1220\n",
      "Epoch 3/35\n",
      "1832/1832 [==============================] - 27s - loss: 1.1953 - categorical_accuracy: 0.1648 - val_loss: 0.7482 - val_categorical_accuracy: 0.1285\n",
      "Epoch 4/35\n",
      "1832/1832 [==============================] - 27s - loss: 1.1730 - categorical_accuracy: 0.2997 - val_loss: 0.7475 - val_categorical_accuracy: 0.3769\n",
      "Epoch 5/35\n",
      "1832/1832 [==============================] - 27s - loss: 1.0717 - categorical_accuracy: 0.6015 - val_loss: 0.7382 - val_categorical_accuracy: 0.5686\n",
      "Epoch 6/35\n",
      " 384/1832 [=====>........................] - ETA: 19s - loss: 0.9343 - categorical_accuracy: 0.6823"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import utils.evaluation as evaluation\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "class_weight = {0 : 1.,\n",
    "    1: (len(y)-sum(y))/sum(y)}\n",
    "\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "cm_summed = np.zeros((2,2))\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "for train_index, test_index in kf.split(X_sequences):\n",
    "    K.clear_session()\n",
    "    \n",
    "    X_sequences_padded = pad_sequences(X_sequences, maxlen=hyperparameters['max_seq_len'])\n",
    "\n",
    "    num_words = embedding_matrix.shape[0]\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=hyperparameters['max_seq_len'],\n",
    "                                trainable=False)\n",
    "\n",
    "    # train a 1D convnet with global maxpooling\n",
    "    sequence_input = Input(shape=(hyperparameters['max_seq_len'],), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    #x = GaussianNoise(hyperparameters['gauss_stddev'])(embedded_sequences)\n",
    "    x = embedded_sequences\n",
    "    x = Conv1D(2*hyperparameters['conv_units'], 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(hyperparameters['conv_units'], 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(hyperparameters['hidden_units_1'], activation='relu')(x)\n",
    "    x = Dropout(hyperparameters['dropout'])(x)\n",
    "    x = Dense(hyperparameters['hidden_units_2'], activation='relu')(x)\n",
    "    preds = Dense(hyperparameters['nclasses'], activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    \n",
    "    X_train, X_test = X_sequences_padded[train_index], X_sequences_padded[test_index]\n",
    "    y_train, y_test = y_matrix[train_index], y_matrix[test_index]\n",
    "\n",
    "    # train\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=128,\n",
    "              epochs=hyperparameters['epochs'],\n",
    "              class_weight=class_weight,\n",
    "              validation_data=(X_test, y_test))\n",
    "\n",
    "    # run\n",
    "    ground_truth = y_test.argmax(1)\n",
    "    predictions = [list(map(lambda x: x, model.predict_on_batch(np.asarray([x])).argmax(1)))[0] \\\n",
    "            for x in X_test]\n",
    "    \n",
    "    \n",
    "    acc = evaluation.accuracy(ground_truth, predictions)\n",
    "    f1 = evaluation.eval_f1_score(ground_truth, predictions)    \n",
    "    cm = evaluation.cm_matrix(ground_truth, predictions)\n",
    "    acc_list.append(acc)\n",
    "    f1_list.append(f1)\n",
    "    cm_summed = cm_summed + cm\n",
    "    print(acc, f1)\n",
    "    print(cm)\n",
    "    \n",
    "    #break\n",
    "    #del model\n",
    "    #K.clear_session()\n",
    "    \n",
    "print(np.asarray(acc).mean(), np.asarray(f1).mean())\n",
    "print(cm_summed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth = y_test.argmax(1)\n",
    "predictions = [list(map(lambda x: x, model.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "        for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted_proba_np = np.asarray(predictions)\n",
    "#y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "y_predicted = (y_predicted_proba_np[:,1] > 0.00000001)\n",
    "\n",
    "acc = accuracy_score(y_predicted, ground_truth)\n",
    "f1 = f1_score(y_predicted, ground_truth, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*30.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != ground_truth) & (y_predicted == False)]), \"/\", sum(ground_truth))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != ground_truth) & (y_predicted == True)]), \"/\", len(ground_truth)-sum(ground_truth))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == ground_truth) & (y_predicted == False)]), \"/\", len(ground_truth)-sum(ground_truth))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == ground_truth) & (y_predicted == True)]), \"/\", sum(ground_truth))\n",
    "\n",
    "# 15% bezbolesnie odrzucic - 0.000008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# evaluate\n",
    "y_predicted = [list(map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x])).argmax(1)))[0] \\\n",
    "               for x in x_test]\n",
    "y_test = y[limit:]\n",
    "y_test = to_categorical(y_test,2)\n",
    "y_test = y_test.argmax(1)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted))\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Total good ones:\", sum(y_test))\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (np.asarray(y_predicted) == 0)]))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (np.asarray(y_predicted) == 1)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted_proba = [list(map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "               for x in x_test]\n",
    "y_predicted_proba_np = np.asarray(y_predicted_proba)\n",
    "#y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "y_predicted = (y_predicted_proba_np[:,1] > 0.2)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)/len(y_predicted)*100.0,\"% - \", int(sum(y_predicted)/len(y_predicted)*30.0))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (y_predicted == False)]), \"/\", sum(y_test))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (y_predicted == True)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == y_test) & (y_predicted == False)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == y_test) & (y_predicted == True)]), \"/\", sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "reload(runner)\n",
    "# Padding sequences\n",
    "x_train = pad_sequences(sequences_train, maxlen=hyperparameters['max_seq_len'])\n",
    "x_test = pad_sequences(sequences_test, maxlen=hyperparameters['max_seq_len'])\n",
    "\n",
    "dataset = x_train, y_train, x_test, y_test, embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "result_cnn, model_cnn = runner.build_train_run(dataset, le, hyperparameters_cnn, save=False, return_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "import numpy as np\n",
    "y_predicted = [list(map(lambda x: x, model_cnn.predict_on_batch(np.asarray([x])).argmax(1)))[0] \\\n",
    "               for x in x_test]\n",
    "y_test = y[limit:]\n",
    "y_test = to_categorical(y_test,hyperparameters['nclasses'])\n",
    "y_test = y_test.argmax(1)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted))\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Total good ones:\", sum(y_test))\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (np.asarray(y_predicted) == 0)]))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (np.asarray(y_predicted) == 1)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted_proba = [list(map(lambda x: x, model_cnn.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "               for x in x_test]\n",
    "y_predicted_proba_np = np.asarray(y_predicted_proba)\n",
    "\n",
    "#y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "y_predicted = (y_predicted_proba_np[:,1] > 0.0005)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)/len(y_predicted)*100.0,\"% - \", int(sum(y_predicted)/len(y_predicted)*30.0))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (y_predicted == False)]), \"/\", sum(y_test))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (y_predicted == True)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == y_test) & (y_predicted == False)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == y_test) & (y_predicted == True)]), \"/\", sum(y_test))\n",
    "\n",
    "y_predicted_proba[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted_proba = [list(map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "               for x in x_test]\n",
    "y_predicted_proba_np = np.asarray(y_predicted_proba)\n",
    "\n",
    "y_predicted_proba[0:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted_proba = [list(map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "               for x in x_test]\n",
    "y_predicted_proba_np = np.asarray(y_predicted_proba)\n",
    "#y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "y_predicted = (y_predicted_proba_np[:,1] > 0.2)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)/len(y_predicted)*100.0,\"% - \", int(sum(y_predicted)/len(y_predicted)*30.0))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (y_predicted == False)]), \"/\", sum(y_test))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (y_predicted == True)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == y_test) & (y_predicted == False)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == y_test) & (y_predicted == True)]), \"/\", sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
