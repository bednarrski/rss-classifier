{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from api import inoreader_api, inoreader_scraping\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape feeds starting from a concrete moment and store them in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Khc1e9o9MtOR\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=ew75EMAN5Ddx\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=4hTbTBeUt11h\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=lBBX8JIL9GKy\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=UUhsWuar3Qrf\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=CPaf5_UfDoco\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=QmGooxrYUSpY\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=HpgfKHdbMXej\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=DNRZU5IX0Tc_\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=lWZbadPMUZ7u\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=uGJgSIN2NQ9C\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=zwOOT6hY_RWq\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=QEYkOO4A3h8H\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=II4JwmSo3t_x\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=YASxzJ1PDGE7\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=KPWiH_47w7zo\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=I1JmT4I1ZwcM\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=qyrFd_cu4ohP\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=lgXP7Z9DyN5u\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=xMBku6fwmi9F\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Neeox4hoMQBb\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=E1xAL5k1HaNh\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=9gEgQo3rkdrx\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Z2NMgtHJO9r9\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=7RJ9zJTbpT_f\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=h5R0N0ZBr0y5\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=AU0sCu4rxhKn\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=2LMW5ABdEdSy\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=_B3owP__80Gh\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=QsLzqFNrk8A2\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=AUKrsGhLBiHt\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=63gan4eqGkQY\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=HrWoobLYRZMp\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Kk6WCX_1zQUm\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=3fKxelyy93mw\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=wHabJxZj8PuY\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=QoUdSKsBc45u\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=7L8Z7w3TXZDh\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Z_UHt_qyaRpO\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=16pP85WCpKNz\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=4S3Onj47ialw\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=_51GjNxXhBUS\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=hupcgj7xZhui\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=8XBlDxP3zFmA\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=SGCR1bamIIh1\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=7Ly_eRESRUqy\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=g7Tg6kI4ZQcn\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=LA_cbhpO3qlU\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=UXr6uIfi8xHl\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=YzO8jJktxnLD\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=QjruT2FXS1a9\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=YedAWuPSGNhQ\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=GSZ987wgqTFS\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Gi_I95p4zEU5\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=xdD4wtRr5KD7\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=t1J_EKEJW3BL\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=OctdExECFpmu\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=WG01KIzTEr1R\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=zPma8PrSM0A2\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=OXkz4fGE0ZNj\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=j0L0WEISNK9u\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=22Inr5ErDORO\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=hNkr1Ny3nbai\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=iI0C9XgMYTzq\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=6EnjqDUJ1PGX\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=ckkMb8dqHTIN\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=MX7s6a7LS7W2\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=3RwQME6aBRPA\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=BYSB_AfpOaCM\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=ROyldWxb92A2\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=P2YRmHLc4lKN\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=U3tFj7KfUl4U\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=LhA2uEWRZoXh\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=LsN6PT7d2XP0\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=skxBekouRnTQ\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=0G3GXF78mBbr\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=YFh_FiG9weHf\n",
      "(200, 'OK')\n",
      "1540\n"
     ]
    }
   ],
   "source": [
    "start_time = 1508112001\n",
    "feed = 'user/-/label/arXiv'\n",
    "file_name = 'articles_raw.json'\n",
    "\n",
    "number_scraped = inoreader_scraping.scrape(feed, start_time, file_name)\n",
    "print(number_scraped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the feeds from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(file_name, 'r')\n",
    "json_string = f.read()\n",
    "f.close\n",
    "\n",
    "json_object = json.loads(json_string)\n",
    "all_articles = json_object['all_articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_abstract_string(abs_dict):\n",
    "    abs_string = abs_dict['content']\n",
    "    abs_string = re.search('<p>(.*)</p>', abs_string, flags=re.DOTALL)\n",
    "    return abs_string.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_canonical_string(canonical_list):\n",
    "    url_dict = canonical_list[0]\n",
    "    url = url_dict['href']\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create preprocessed and cleaned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/ipykernel/__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "all_articles_pd = pd.DataFrame(all_articles)\n",
    "all_articles_pd = all_articles_pd[['canonical', 'author', 'categories', 'published', 'title', 'summary']]\n",
    "\n",
    "for i in range(len(all_articles_pd)):\n",
    "    all_articles_pd['summary'][i] = get_abstract_string(all_articles_pd['summary'][i])\n",
    "    all_articles_pd['canonical'][i] = get_canonical_string(all_articles_pd['canonical'][i])\n",
    "    \n",
    "all_articles_pd['read'] = False\n",
    "all_articles_pd['liked'] = False\n",
    "\n",
    "for i in range(len(all_articles_pd)):\n",
    "    if 'user/1005689817/state/com.google/read' in all_articles_pd['categories'][i]:\n",
    "        all_articles_pd['read'][i] = True\n",
    "    if 'user/1005689817/state/com.google/like' in all_articles_pd['categories'][i]:\n",
    "        all_articles_pd['liked'][i] = True\n",
    "\n",
    "del all_articles_pd['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all_articles_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_articles_pd = all_articles_pd[all_articles_pd['read'] == True]\n",
    "\n",
    "with open('old_articles_proto2.pickle', 'rb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    old_tagged_articles_pd = pickle.load(f)\n",
    "    \n",
    "tagged_articles_pd = pd.concat([tagged_articles_pd, old_tagged_articles_pd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_articles_unique_pd = tagged_articles_pd.sort_values(by=['canonical', 'liked'], ascending=False).reset_index(drop=True)\n",
    "to_drop = tagged_articles_unique_pd.duplicated(subset = ['canonical'], keep='first')\n",
    "to_drop = list(to_drop[to_drop == False].index.values)\n",
    "\n",
    "tagged_articles_unique_pd = tagged_articles_unique_pd[tagged_articles_unique_pd.index.isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2605\n",
      "2400\n"
     ]
    }
   ],
   "source": [
    "print(len(tagged_articles_pd))\n",
    "print(len(tagged_articles_unique_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ = list(tagged_articles_unique_pd['author']+' '+tagged_articles_unique_pd['title']+' '+tagged_articles_unique_pd['summary'])\n",
    "y_ = list(tagged_articles_unique_pd['liked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_shuffled = [i for i in range(len(X_))]\n",
    "random.shuffle(index_shuffled)\n",
    "X = []\n",
    "y = []\n",
    "for i in index_shuffled:\n",
    "    X.append(X_[i])\n",
    "    y.append(y_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.preprocessing.text\n",
    "from string import maketrans\n",
    "\n",
    "def text_to_word_sequence(text,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True, split=\" \"):\n",
    "    if lower: text = text.lower()\n",
    "    if type(text) == unicode:\n",
    "        translate_table = {ord(c): ord(t) for c,t in zip(filters, split*len(filters)) }\n",
    "    else:\n",
    "        translate_table = maketrans(filters, split * len(filters))\n",
    "    text = text.translate(translate_table)\n",
    "    seq = text.split(split)\n",
    "    return [i for i in seq if i]\n",
    "    \n",
    "keras.preprocessing.text.text_to_word_sequence = text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('stopwords/english') as f:\n",
    "    stopwords = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1141\n",
      "888\n"
     ]
    }
   ],
   "source": [
    "print(len(X[4]))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    seq = text_to_word_sequence(X[i])\n",
    "    clean_seq = [word for word in seq if word not in stopwords]\n",
    "    X[i] = ' '.join(clean_seq)\n",
    "    \n",
    "print(len(X[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920\n"
     ]
    }
   ],
   "source": [
    "\n",
    "limit = int(0.8*len(X))\n",
    "print(limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF -> NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.875, 0.93333333333333335)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "model_nb_tfidf = Pipeline([\n",
    "    (\"tfidf_vectorizer\", TfidfVectorizer()),\\\n",
    "    (\"nb\", BernoulliNB())\n",
    "])\n",
    "\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_nb_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_nb_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.875, 0.93333333333333335, 0, 480)\n",
      "(0.0, '% - ', 0)\n",
      "\n",
      "\n",
      "Rejected 100.00% of wrong ones\n",
      "Accepted 0.00% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   3.15265711e-22],\n",
       "       [  1.00000000e+00,   2.10592544e-28],\n",
       "       [  1.00000000e+00,   1.00164309e-28],\n",
       "       [  1.00000000e+00,   1.33199900e-28],\n",
       "       [  1.00000000e+00,   2.72014570e-23]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_nb_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.15)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.10)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)/len(y_predicted)*100.0,\"% - \", int(sum(y_predicted)/len(y_predicted)*50.0))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVect -> NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.875, 0.93333333333333335)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "model_nb = Pipeline([\n",
    "    #('vect', CountVectorizer(ngram_range=(1,3),stop_words='english')),\\\n",
    "    (\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)),\\\n",
    "    (\"nb\", BernoulliNB())\n",
    "])\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_nb.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.458333333333 0.379190385832 280 480\n",
      "58.3333333333 % -  29\n",
      "\n",
      "\n",
      "Rejected 42.86% of wrong ones\n",
      "Accepted 66.67% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.79166655,  0.20833345],\n",
       "       [ 0.92894937,  0.07105063],\n",
       "       [ 0.84815864,  0.15184136],\n",
       "       [ 0.89375578,  0.10624422],\n",
       "       [ 0.84866637,  0.15133363]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_nb.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.1296)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.1325)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print acc, f1, sum(y_predicted), len(y_predicted)\n",
    "print sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF -> SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.875, 0.92236119585112886)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "model_svc_tfidf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,3), min_df=1, max_df=1.0,stop_words='english')),\n",
    "    #('vect', CountVectorizer(ngram_range=(3,6),analyzer='char_wb', min_df=10, max_df=0.95,stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    (\"svc\", SVC(kernel='linear', probability=True, class_weight = 'balanced'))\n",
    "    #(\"svc\", SVC(kernel='linear', probability=True))\n",
    "])\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_svc_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_svc_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.55208333333333337, 0.48045048693926018, 251, 480)\n",
      "(52.291666666666664, '% - ', 26)\n",
      "\n",
      "\n",
      "Rejected 51.67% of wrong ones\n",
      "Accepted 80.00% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.97249655,  0.02750345],\n",
       "       [ 0.95833463,  0.04166537],\n",
       "       [ 0.88755331,  0.11244669],\n",
       "       [ 0.96457337,  0.03542663],\n",
       "       [ 0.91175798,  0.08824202]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_svc_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.103)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]\n",
    "\n",
    "#(0.62077922077922076, 0.55417162908266104, 162, 385)\n",
    "#(42.077922077922075, '% - ', 21)\n",
    "#('Rejected good ones: ', 15, '/', 46)\n",
    "#('Accepted wrong ones:', 131, '/', 339)\n",
    "#('Rejected wrong ones: ', 208, '/', 339)\n",
    "#('Accepted good ones:', 31, '/', 46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.791666666666664"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/len(y_test)-sum(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.78749999999999998, 0.77018518518518531)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "model_svc_tfidf = Pipeline([\n",
    "    #('vect', CountVectorizer(ngram_range=(1,3), min_df=1, max_df=1.0,stop_words='english')),\n",
    "    ('vect', CountVectorizer(ngram_range=(3,6),analyzer='char_wb', min_df=10, max_df=0.95,stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    (\"svc\", SVC(kernel='linear', probability=True, class_weight = 'balanced'))\n",
    "    #(\"svc\", SVC(kernel='linear', probability=True))\n",
    "])\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_svc_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_svc_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.48958333333333331, 0.41303953475006111, 273, 480)\n",
      "(56.875, '% - ', 28)\n",
      "\n",
      "\n",
      "Rejected 45.48% of wrong ones\n",
      "Accepted 73.33% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.96256645,  0.03743355],\n",
       "       [ 0.94595506,  0.05404494],\n",
       "       [ 0.88653152,  0.11346848],\n",
       "       [ 0.92424562,  0.07575438],\n",
       "       [ 0.91255095,  0.08744905]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_svc_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.111)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]\n",
    "\n",
    "#(0.62077922077922076, 0.55417162908266104, 162, 385)\n",
    "#(42.077922077922075, '% - ', 21)\n",
    "#('Rejected good ones: ', 15, '/', 46)\n",
    "#('Accepted wrong ones:', 131, '/', 339)\n",
    "#('Rejected wrong ones: ', 208, '/', 339)\n",
    "#('Accepted good ones:', 31, '/', 46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count -> SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.87708333333333333, 0.92530976937401521)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "model_svc = Pipeline([\n",
    "    #('vect', CountVectorizer()),\n",
    "    ('vect', CountVectorizer(ngram_range=(1,3), min_df=1, max_df=1.0,stop_words='english')),\n",
    "    #('vect', CountVectorizer(ngram_range=(3,6),analyzer='char_wb', min_df=10, max_df=0.95,stop_words='english')),\n",
    "    #('tfidf', TfidfTransformer()),\n",
    "    (\"svc\", SVC(kernel='linear', probability=True, class_weight = 'balanced'))\n",
    "    #(\"svc\", SVC(kernel='linear', probability=True))\n",
    "])\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_svc.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_svc.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.51666666666666672, 0.44157726331639385, 262, 480)\n",
      "(54.583333333333336, '% - ', 27)\n",
      "\n",
      "\n",
      "Rejected 48.33% of wrong ones\n",
      "Accepted 75.00% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.96184194,  0.03815806],\n",
       "       [ 0.91810116,  0.08189884],\n",
       "       [ 0.84512951,  0.15487049],\n",
       "       [ 0.91890093,  0.08109907],\n",
       "       [ 0.91697881,  0.08302119]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_svc.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.11)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]\n",
    "\n",
    "#(0.62077922077922076, 0.55417162908266104, 162, 385)\n",
    "#(42.077922077922075, '% - ', 21)\n",
    "#('Rejected good ones: ', 15, '/', 46)\n",
    "#('Accepted wrong ones:', 131, '/', 339)\n",
    "#('Rejected wrong ones: ', 208, '/', 339)\n",
    "#('Accepted good ones:', 31, '/', 46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Input, Embedding, SimpleRNN, Dense, Activation, TimeDistributed, Bidirectional, LSTM, GaussianNoise)\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model #Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASETS_DIR = '../ml-research/datasets/'\n",
    "GLOVE_DIR = DATASETS_DIR+'glove.6B/'\n",
    "WIKI_EN_DIR = DATASETS_DIR+'wiki.en/'\n",
    "#embeddings_file = os.path.join(GLOVE_DIR, 'glove.6B.300d.txt')\n",
    "embeddings_file = os.path.join(WIKI_EN_DIR, 'wiki.en.vec')\n",
    "\n",
    "# Word embeddings' constraints\n",
    "MAX_NB_WORDS = 20000  # Number of most common words for tokenizer\n",
    "EMBEDDING_DIM = 300   # Embeddings dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.preprocessing.text\n",
    "from string import maketrans\n",
    "\n",
    "def text_to_word_sequence(text,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True, split=\" \"):\n",
    "    if lower: text = text.lower()\n",
    "    if type(text) == unicode:\n",
    "        translate_table = {ord(c): ord(t) for c,t in zip(filters, split*len(filters)) }\n",
    "    else:\n",
    "        translate_table = maketrans(filters, split * len(filters))\n",
    "    text = text.translate(translate_table)\n",
    "    seq = text.split(split)\n",
    "    return [i for i in seq if i]\n",
    "    \n",
    "keras.preprocessing.text.text_to_word_sequence = text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25513 unique tokens.\n",
      "81\n",
      "25512\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing and creating word index\n",
    "\n",
    "additional_words = ['unk', 'num']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X+additional_words)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# inversing the word_index.\n",
    "index_word = dict((k,v) for v,k in word_index.items())\n",
    "\n",
    "# example\n",
    "print(word_index['adversarial'])\n",
    "print(word_index['unk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 2519371 word vectors.\n",
      "Creating Word Embeddings matrix...\n",
      "Word Embeddings matrix was successfuly created.\n"
     ]
    }
   ],
   "source": [
    "import models.embedding_matrix as embedding\n",
    "\n",
    "embedding_matrix = embedding.create_embedding_matrix(embeddings_file, MAX_NB_WORDS, EMBEDDING_DIM, word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Input, Embedding, SimpleRNN, Dense, Activation, TimeDistributed, Bidirectional,\n",
    "                          LSTM, GaussianNoise,Conv1D, MaxPooling1D, Flatten, Dropout)\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'conv_units': 128,\n",
    "    'hidden_units_1': 128,\n",
    "    'hidden_units_2': 64,\n",
    "    'dropout': 0.5,\n",
    "    'pooling' : 5,\n",
    "    'gauss_stddev' : 0.0002,\n",
    "    'epochs' : 15,\n",
    "    'folds' : 5,\n",
    "    'nclasses' : 2,\n",
    "    'max_seq_len' : 300,\n",
    "    'learning_rate' : 0.001,\n",
    "    'stopwords' : True\n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    "    'conv_units': 64,\n",
    "    'hidden_units_1': 64,\n",
    "    'hidden_units_2': 32,\n",
    "    'dropout': 0.5,\n",
    "    'pooling' : 5,\n",
    "    'gauss_stddev' : 0.0002,\n",
    "    'epochs' : 15,\n",
    "    'folds' : 5,\n",
    "    'nclasses' : 2,\n",
    "    'max_seq_len' : 300,\n",
    "    'learning_rate' : 0.001,\n",
    "    'stopwords' : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5df96a2390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "X_sequences_len = []\n",
    "for item in X_sequences:\n",
    "    X_sequences_len.append(\n",
    "                            min( len(item), 1000\n",
    "                               ))\n",
    "    \n",
    "X_sequences_padded = pad_sequences(X_sequences, maxlen=hyperparameters['max_seq_len'])\n",
    "\n",
    "plt.hist(X_sequences_len, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "\n",
    "y_num = label_encoder.transform(y)\n",
    "y_matrix = to_categorical(y_num,hyperparameters['nclasses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1920 samples, validate on 480 samples\n",
      "Epoch 1/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0998 - categorical_accuracy: 0.7672 - val_loss: 0.6707 - val_categorical_accuracy: 0.8750\n",
      "Epoch 2/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0969 - categorical_accuracy: 0.8484 - val_loss: 0.6347 - val_categorical_accuracy: 0.8750\n",
      "Epoch 3/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0721 - categorical_accuracy: 0.8474 - val_loss: 0.4943 - val_categorical_accuracy: 0.8750\n",
      "Epoch 4/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0767 - categorical_accuracy: 0.7130 - val_loss: 0.5319 - val_categorical_accuracy: 0.8750\n",
      "Epoch 5/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0754 - categorical_accuracy: 0.7031 - val_loss: 0.5468 - val_categorical_accuracy: 0.8208\n",
      "Epoch 6/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0536 - categorical_accuracy: 0.7750 - val_loss: 0.6271 - val_categorical_accuracy: 0.5854\n",
      "Epoch 7/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0293 - categorical_accuracy: 0.7281 - val_loss: 0.6704 - val_categorical_accuracy: 0.4354\n",
      "Epoch 8/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0155 - categorical_accuracy: 0.6594 - val_loss: 0.4953 - val_categorical_accuracy: 0.7562\n",
      "Epoch 9/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.9321 - categorical_accuracy: 0.7302 - val_loss: 0.7594 - val_categorical_accuracy: 0.4521\n",
      "Epoch 10/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.8764 - categorical_accuracy: 0.7323 - val_loss: 0.5444 - val_categorical_accuracy: 0.6813\n",
      "Epoch 11/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.7145 - categorical_accuracy: 0.8016 - val_loss: 0.7433 - val_categorical_accuracy: 0.5771\n",
      "Epoch 12/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.5899 - categorical_accuracy: 0.8422 - val_loss: 0.4590 - val_categorical_accuracy: 0.8042\n",
      "Epoch 13/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.3981 - categorical_accuracy: 0.9099 - val_loss: 1.0087 - val_categorical_accuracy: 0.5417\n",
      "Epoch 14/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.3434 - categorical_accuracy: 0.9167 - val_loss: 0.4865 - val_categorical_accuracy: 0.8208\n",
      "Epoch 15/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.1877 - categorical_accuracy: 0.9563 - val_loss: 0.5491 - val_categorical_accuracy: 0.8292\n",
      "(0.82916666666666672, 0.81217150433259144)\n",
      "[[390  30]\n",
      " [ 52   8]]\n",
      "Train on 1920 samples, validate on 480 samples\n",
      "Epoch 1/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0960 - categorical_accuracy: 0.7870 - val_loss: 0.6866 - val_categorical_accuracy: 0.8604\n",
      "Epoch 2/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0882 - categorical_accuracy: 0.8547 - val_loss: 0.6417 - val_categorical_accuracy: 0.8604\n",
      "Epoch 3/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0804 - categorical_accuracy: 0.8552 - val_loss: 0.6206 - val_categorical_accuracy: 0.8604\n",
      "Epoch 4/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0651 - categorical_accuracy: 0.8542 - val_loss: 0.6111 - val_categorical_accuracy: 0.8604\n",
      "Epoch 5/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0424 - categorical_accuracy: 0.8427 - val_loss: 0.5968 - val_categorical_accuracy: 0.7458\n",
      "Epoch 6/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0021 - categorical_accuracy: 0.7333 - val_loss: 0.5388 - val_categorical_accuracy: 0.7542\n",
      "Epoch 7/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.9263 - categorical_accuracy: 0.7385 - val_loss: 0.8983 - val_categorical_accuracy: 0.3833\n",
      "Epoch 8/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.8968 - categorical_accuracy: 0.7354 - val_loss: 0.4801 - val_categorical_accuracy: 0.7938\n",
      "Epoch 9/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.7298 - categorical_accuracy: 0.8125 - val_loss: 0.4954 - val_categorical_accuracy: 0.7542\n",
      "Epoch 10/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.5890 - categorical_accuracy: 0.8370 - val_loss: 0.4321 - val_categorical_accuracy: 0.8229\n",
      "Epoch 11/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.4447 - categorical_accuracy: 0.8984 - val_loss: 0.4793 - val_categorical_accuracy: 0.8021\n",
      "Epoch 12/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.2812 - categorical_accuracy: 0.9375 - val_loss: 0.6125 - val_categorical_accuracy: 0.7646\n",
      "Epoch 13/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.1416 - categorical_accuracy: 0.9719 - val_loss: 0.7128 - val_categorical_accuracy: 0.8583\n",
      "Epoch 14/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.1041 - categorical_accuracy: 0.9802 - val_loss: 0.7620 - val_categorical_accuracy: 0.7917\n",
      "Epoch 15/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.0794 - categorical_accuracy: 0.9839 - val_loss: 0.7702 - val_categorical_accuracy: 0.8229\n",
      "(0.82291666666666663, 0.80337698538783775)\n",
      "[[384  29]\n",
      " [ 56  11]]\n",
      "Train on 1920 samples, validate on 480 samples\n",
      "Epoch 1/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0875 - categorical_accuracy: 0.8161 - val_loss: 0.6127 - val_categorical_accuracy: 0.8604\n",
      "Epoch 2/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0694 - categorical_accuracy: 0.8536 - val_loss: 0.6282 - val_categorical_accuracy: 0.8604\n",
      "Epoch 3/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0657 - categorical_accuracy: 0.8104 - val_loss: 0.6529 - val_categorical_accuracy: 0.7208\n",
      "Epoch 4/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0483 - categorical_accuracy: 0.7755 - val_loss: 0.6470 - val_categorical_accuracy: 0.5708\n",
      "Epoch 5/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0085 - categorical_accuracy: 0.7115 - val_loss: 0.7138 - val_categorical_accuracy: 0.4896\n",
      "Epoch 6/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.9611 - categorical_accuracy: 0.7109 - val_loss: 0.5891 - val_categorical_accuracy: 0.6625\n",
      "Epoch 7/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.8961 - categorical_accuracy: 0.7646 - val_loss: 0.4703 - val_categorical_accuracy: 0.7937\n",
      "Epoch 8/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.7391 - categorical_accuracy: 0.7984 - val_loss: 0.4329 - val_categorical_accuracy: 0.8021\n",
      "Epoch 9/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.6071 - categorical_accuracy: 0.8484 - val_loss: 0.5325 - val_categorical_accuracy: 0.7250\n",
      "Epoch 10/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.4591 - categorical_accuracy: 0.8948 - val_loss: 0.4400 - val_categorical_accuracy: 0.8125\n",
      "Epoch 11/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.2914 - categorical_accuracy: 0.9521 - val_loss: 0.5632 - val_categorical_accuracy: 0.7771\n",
      "Epoch 12/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.1493 - categorical_accuracy: 0.9755 - val_loss: 0.5237 - val_categorical_accuracy: 0.8187\n",
      "Epoch 13/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.0522 - categorical_accuracy: 0.9958 - val_loss: 0.7693 - val_categorical_accuracy: 0.8063\n",
      "Epoch 14/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.0271 - categorical_accuracy: 0.9948 - val_loss: 0.7785 - val_categorical_accuracy: 0.8313\n",
      "Epoch 15/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.0719 - categorical_accuracy: 0.9859 - val_loss: 0.6343 - val_categorical_accuracy: 0.8271\n",
      "(0.82708333333333328, 0.81474294657405022)\n",
      "[[381  32]\n",
      " [ 51  16]]\n",
      "Train on 1920 samples, validate on 480 samples\n",
      "Epoch 1/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0725 - categorical_accuracy: 0.8078 - val_loss: 0.6131 - val_categorical_accuracy: 0.8396\n",
      "Epoch 2/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0573 - categorical_accuracy: 0.8578 - val_loss: 0.5828 - val_categorical_accuracy: 0.8396\n",
      "Epoch 3/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0186 - categorical_accuracy: 0.8609 - val_loss: 0.5492 - val_categorical_accuracy: 0.8417\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920/1920 [==============================] - 6s - loss: 0.9860 - categorical_accuracy: 0.8589 - val_loss: 0.5249 - val_categorical_accuracy: 0.8021\n",
      "Epoch 5/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.9367 - categorical_accuracy: 0.8516 - val_loss: 0.5683 - val_categorical_accuracy: 0.6813\n",
      "Epoch 6/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.8687 - categorical_accuracy: 0.7917 - val_loss: 0.5571 - val_categorical_accuracy: 0.6729\n",
      "Epoch 7/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.7642 - categorical_accuracy: 0.7943 - val_loss: 0.7191 - val_categorical_accuracy: 0.5021\n",
      "Epoch 8/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.7636 - categorical_accuracy: 0.7557 - val_loss: 0.5772 - val_categorical_accuracy: 0.6729\n",
      "Epoch 9/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.5403 - categorical_accuracy: 0.8635 - val_loss: 0.7149 - val_categorical_accuracy: 0.6542\n",
      "Epoch 10/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.3363 - categorical_accuracy: 0.9120 - val_loss: 0.7266 - val_categorical_accuracy: 0.7896\n",
      "Epoch 11/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.5251 - categorical_accuracy: 0.8490 - val_loss: 0.6931 - val_categorical_accuracy: 0.8104\n",
      "Epoch 12/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.2648 - categorical_accuracy: 0.9557 - val_loss: 0.6956 - val_categorical_accuracy: 0.7187\n",
      "Epoch 13/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.1530 - categorical_accuracy: 0.9682 - val_loss: 0.8858 - val_categorical_accuracy: 0.8000\n",
      "Epoch 14/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.0697 - categorical_accuracy: 0.9865 - val_loss: 0.9717 - val_categorical_accuracy: 0.8021\n",
      "Epoch 15/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.0488 - categorical_accuracy: 0.9917 - val_loss: 1.0951 - val_categorical_accuracy: 0.7875\n",
      "(0.78749999999999998, 0.78749999999999998)\n",
      "[[352  51]\n",
      " [ 51  26]]\n",
      "Train on 1920 samples, validate on 480 samples\n",
      "Epoch 1/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0781 - categorical_accuracy: 0.8401 - val_loss: 0.6152 - val_categorical_accuracy: 0.8500\n",
      "Epoch 2/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0594 - categorical_accuracy: 0.8589 - val_loss: 0.5794 - val_categorical_accuracy: 0.8500\n",
      "Epoch 3/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0292 - categorical_accuracy: 0.8182 - val_loss: 0.6331 - val_categorical_accuracy: 0.6667\n",
      "Epoch 4/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0169 - categorical_accuracy: 0.7672 - val_loss: 0.5577 - val_categorical_accuracy: 0.7521\n",
      "Epoch 5/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.9895 - categorical_accuracy: 0.7141 - val_loss: 0.6156 - val_categorical_accuracy: 0.5667\n",
      "Epoch 6/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.9193 - categorical_accuracy: 0.7401 - val_loss: 0.6590 - val_categorical_accuracy: 0.5458\n",
      "Epoch 7/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.8490 - categorical_accuracy: 0.7281 - val_loss: 0.4795 - val_categorical_accuracy: 0.7500\n",
      "Epoch 8/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.6885 - categorical_accuracy: 0.8047 - val_loss: 0.4540 - val_categorical_accuracy: 0.7958\n",
      "Epoch 9/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.5305 - categorical_accuracy: 0.8656 - val_loss: 0.5695 - val_categorical_accuracy: 0.7562\n",
      "Epoch 10/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.3508 - categorical_accuracy: 0.9052 - val_loss: 0.6241 - val_categorical_accuracy: 0.7583\n",
      "Epoch 11/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.3004 - categorical_accuracy: 0.9266 - val_loss: 0.5999 - val_categorical_accuracy: 0.7500\n",
      "Epoch 12/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.1429 - categorical_accuracy: 0.9646 - val_loss: 0.8200 - val_categorical_accuracy: 0.8250\n",
      "Epoch 13/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.1714 - categorical_accuracy: 0.9615 - val_loss: 0.8633 - val_categorical_accuracy: 0.7250\n",
      "Epoch 14/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.2206 - categorical_accuracy: 0.9448 - val_loss: 0.7381 - val_categorical_accuracy: 0.7542\n",
      "Epoch 15/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.0997 - categorical_accuracy: 0.9812 - val_loss: 0.8949 - val_categorical_accuracy: 0.8292\n",
      "(0.82916666666666672, 0.81600610352517722)\n",
      "[[378  30]\n",
      " [ 52  20]]\n",
      "(0.82916666666666672, 0.81600610352517722)\n",
      "[[ 1885.   172.]\n",
      " [  262.    81.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import utils.evaluation as evaluation\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "class_weight = {0 : 1.,\n",
    "    1: (len(y)-sum(y))/sum(y)}\n",
    "\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "cm_summed = np.zeros((2,2))\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "for train_index, test_index in kf.split(X_sequences):\n",
    "    K.clear_session()\n",
    "    \n",
    "    X_sequences_padded = pad_sequences(X_sequences, maxlen=hyperparameters['max_seq_len'])\n",
    "\n",
    "    num_words = embedding_matrix.shape[0]\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=hyperparameters['max_seq_len'],\n",
    "                                trainable=False)\n",
    "\n",
    "    # train a 1D convnet with global maxpooling\n",
    "    sequence_input = Input(shape=(hyperparameters['max_seq_len'],), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    #x = GaussianNoise(hyperparameters['gauss_stddev'])(embedded_sequences)\n",
    "    x = embedded_sequences\n",
    "    x = Conv1D(2*hyperparameters['conv_units'], 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(hyperparameters['conv_units'], 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(hyperparameters['hidden_units_1'], activation='relu')(x)\n",
    "    x = Dropout(hyperparameters['dropout'])(x)\n",
    "    x = Dense(hyperparameters['hidden_units_2'], activation='relu')(x)\n",
    "    preds = Dense(hyperparameters['nclasses'], activation='softmax')(x)\n",
    "\n",
    "    model_cnn = Model(sequence_input, preds)\n",
    "    model_cnn.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    \n",
    "    X_train, X_test = X_sequences_padded[train_index], X_sequences_padded[test_index]\n",
    "    y_train, y_test = y_matrix[train_index], y_matrix[test_index]\n",
    "\n",
    "    # train\n",
    "    model_cnn.fit(X_train, y_train,\n",
    "              batch_size=128,\n",
    "              epochs=hyperparameters['epochs'],\n",
    "              class_weight=class_weight,\n",
    "              validation_data=(X_test, y_test))\n",
    "\n",
    "    # run\n",
    "    ground_truth = y_test.argmax(1)\n",
    "    predictions = [list(map(lambda x: x, model_cnn.predict_on_batch(np.asarray([x])).argmax(1)))[0] \\\n",
    "            for x in X_test]\n",
    "    \n",
    "    \n",
    "    acc = evaluation.accuracy(ground_truth, predictions)\n",
    "    f1 = evaluation.eval_f1_score(ground_truth, predictions)    \n",
    "    cm = evaluation.cm_matrix(ground_truth, predictions)\n",
    "    acc_list.append(acc)\n",
    "    f1_list.append(f1)\n",
    "    cm_summed = cm_summed + cm\n",
    "    print(acc, f1)\n",
    "    print(cm)\n",
    "    \n",
    "print(np.asarray(acc).mean(), np.asarray(f1).mean())\n",
    "print(cm_summed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth = y_test.argmax(1)\n",
    "predictions = [list(map(lambda x: x, model_cnn.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "        for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5708333333333333, 0.51009396995376988, 230, 480)\n",
      "(47.916666666666664, '% - ', 23)\n",
      "\n",
      "\n",
      "Rejected 33.33% of wrong ones\n",
      "Accepted 44.61% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  9.84410644e-01,   1.55893471e-02],\n",
       "       [  9.87623096e-01,   1.23768579e-02],\n",
       "       [  1.25291245e-02,   9.87470925e-01],\n",
       "       [  9.99981284e-01,   1.87653804e-05],\n",
       "       [  9.98839557e-01,   1.16048940e-03]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba_np = np.asarray(predictions)\n",
    "#y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "\n",
    "acc = accuracy_score(y_predicted, ground_truth)\n",
    "f1 = f1_score(y_predicted, ground_truth, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test[:,0]) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test)[0])))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test[:,0]) & (y_predicted == True)])*100.0/sum(y_test)[0]))\n",
    "\n",
    "# 15% bezbolesnie odrzucic - 0.000008\n",
    "\n",
    "y_predicted_proba_np[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'conv_units': 128,\n",
    "    'hidden_units_1': 128,\n",
    "    'hidden_units_2': 64,\n",
    "    'dropout': 0.5,\n",
    "    'pooling' : 5,\n",
    "    'gauss_stddev' : 0.0002,\n",
    "    'epochs' : 15,\n",
    "    'folds' : 5,\n",
    "    'nclasses' : 2,\n",
    "    'max_seq_len' : 300,\n",
    "    'learning_rate' : 0.001,\n",
    "    'stopwords' : True\n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    "    'conv_units': 64,\n",
    "    'hidden_units_1': 64,\n",
    "    'hidden_units_2': 0,\n",
    "    'dropout': 0.5,\n",
    "    'pooling' : 5,\n",
    "    'gauss_stddev' : 0.0002,\n",
    "    'epochs' : 5,\n",
    "    'folds' : 5,\n",
    "    'nclasses' : 2,\n",
    "    'max_seq_len' : 300,\n",
    "    'learning_rate' : 0.001,\n",
    "    'stopwords' : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import utils.evaluation as evaluation\n",
    "from keras import metrics\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "class_weight = {0 : 1.,\n",
    "    1: (len(y)-sum(y))/sum(y)}\n",
    "\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "cm_summed = np.zeros((2,2))\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "for train_index, test_index in kf.split(X_sequences):\n",
    "    K.clear_session()\n",
    "\n",
    "    num_words = embedding_matrix.shape[0]\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "    \n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(Embedding(num_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False))\n",
    "\n",
    "    model_lstm.add(Bidirectional(LSTM(hyperparameters['conv_units'], activation='sigmoid', return_sequences=False)))\n",
    "    model_lstm.add(GaussianNoise(hyperparameters['gauss_stddev']))\n",
    "    model_lstm.add(Dropout(hyperparameters['dropout']))\n",
    "    model_lstm.add(Dense(units=hyperparameters['hidden_units_1']))\n",
    "    if hyperparameters['hidden_units_2'] > 0:\n",
    "        model_lstm.add(Dense(units=hyperparameters['hidden_units_2']))\n",
    "    model_lstm.add(Dense(units=hyperparameters['nclasses']))\n",
    "    model_lstm.add(Activation(\"softmax\"))\n",
    "\n",
    "    optimizer = RMSprop(lr=hyperparameters['learning_rate'])\n",
    "    model_lstm.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  #optimizer='adadelta',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    #print(model.summary())\n",
    "    \n",
    "    X_train, X_test = np.asarray(X_sequences)[train_index], np.asarray(X_sequences)[test_index]\n",
    "    y_train, y_test = y_matrix[train_index], y_matrix[test_index]\n",
    "\n",
    "    # train\n",
    "    nsentences = len(X_train)\n",
    "    best_f1 = -np.inf\n",
    "    for epoch in xrange(hyperparameters['epochs']):\n",
    "        # Shuffle datasets\n",
    "        #shuffle([x_train, y_train], 42)\n",
    "        tic = time.time()\n",
    "        for i in xrange(nsentences):\n",
    "            X_lstm = np.asarray([X_train[i]])\n",
    "            Y_lstm = y_train[i].reshape((1,hyperparameters['nclasses']))\n",
    "            if X_lstm.shape[1] == 1:\n",
    "                continue # Bug with X, Y of len 1\n",
    "            model_lstm.train_on_batch(X_lstm, Y_lstm)\n",
    "            print '[learning] epoch %i >> %2.2f%%'%(epoch,(i+1)*100./nsentences)+\\\n",
    "                    ' completed in %.2f (sec)\\r'%(time.time()-tic),\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # Evaluation // back into the real world : idx -> words\n",
    "        predictions = [map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x])).argmax(1))[0] \\\n",
    "            for x in X_test]\n",
    "        ground_truth = y_test.argmax(1)\n",
    "\n",
    "        f1_valid = evaluation.eval_f1_score(ground_truth, predictions)\n",
    "\n",
    "        if f1_valid > best_f1:\n",
    "            best_f1 = f1_valid\n",
    "            model_lstm.save_weights('best_model_lstm.h5', overwrite=True)\n",
    "            print 'NEW BEST: epoch', epoch, 'valid F1 = ', f1_valid, 'in', str(time.time()-tic), '(sec)',' '*30\n",
    "\n",
    "        # load best performing model\n",
    "        model_lstm.load_weights('best_model_lstm.h5')\n",
    "\n",
    "    # eval\n",
    "    ground_truth = y_test.argmax(1)\n",
    "    predictions = [list(map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x])).argmax(1)))[0] \\\n",
    "            for x in X_test]\n",
    "    \n",
    "    \n",
    "    acc = evaluation.accuracy(ground_truth, predictions)\n",
    "    f1 = evaluation.eval_f1_score(ground_truth, predictions)    \n",
    "    cm = evaluation.cm_matrix(ground_truth, predictions)\n",
    "    acc_list.append(acc)\n",
    "    f1_list.append(f1)\n",
    "    cm_summed = cm_summed + cm\n",
    "    print(acc, f1)\n",
    "    print(cm)\n",
    "    \n",
    "    #break\n",
    "    #del model\n",
    "    #K.clear_session()\n",
    "    \n",
    "print(np.asarray(acc).mean(), np.asarray(f1).mean())\n",
    "print(cm_summed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth = y_test.argmax(1)\n",
    "predictions = [list(map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "        for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_proba_np = np.asarray(predictions)\n",
    "#y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "y_predicted = (y_predicted_proba_np[:,1] > 0.4791)\n",
    "\n",
    "acc = accuracy_score(y_predicted, ground_truth)\n",
    "f1 = f1_score(y_predicted, ground_truth, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test[:,0]) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test)[0])))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test[:,0]) & (y_predicted == True)])*100.0/sum(y_test)[0]))\n",
    "\n",
    "# 15% bezbolesnie odrzucic - 0.000008\n",
    "\n",
    "y_predicted_proba_np[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extree - pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "import models.embedding_matrix as embedding\n",
    "reload(embedding)\n",
    "\n",
    "DATASETS_DIR = '../ml-research/datasets/'\n",
    "WIKI_DIR = DATASETS_DIR+'wiki.pl/'\n",
    "embeddings_file = WIKI_DIR+'wiki.pl.vec'\n",
    "\n",
    "word2vec_pretrained = embedding.create_embedding_dictionary(embeddings_file)\n",
    "\n",
    "# train word2vec on all the texts - both training and test set\n",
    "# we're not using test labels, just texts so this is fine\n",
    "model = Word2Vec(X, size=100, window=5, min_count=5, workers=2)\n",
    "w2v_custom = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "# and a tf-idf version of the same\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extra Trees classifier is almost universally great, let's stack it with our embeddings\n",
    "\n",
    "etree_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=50))])\n",
    "etree_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"svc\", ExtraTreesClassifier(n_estimators=50))])\n",
    "svc_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"extra trees\", SVC(kernel=\"linear\", probability=True))])\n",
    "svc_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"svc\", SVC(kernel=\"linear\", probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "etree_w2v.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = etree_w2v.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = etree_w2v.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "etree_w2v_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = etree_w2v_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = etree_w2v_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.137)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "svc_w2v.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = svc_w2v.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = svc_w2v.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.135)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "svc_w2v_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = svc_w2v_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = svc_w2v_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.145)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extree - Custom embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extra Trees classifier is almost universally great, let's stack it with our embeddings\n",
    "\n",
    "etree_w2v_custom = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v_custom)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=50))])\n",
    "etree_w2v_tfidf_custom = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v_custom)), \n",
    "                        (\"svc\", ExtraTreesClassifier(n_estimators=50))])\n",
    "svc_w2v_custom = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v_custom)), \n",
    "                        (\"extra trees\", SVC(kernel=\"linear\", probability=True))])\n",
    "svc_w2v_tfidf_custom = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v_custom)), \n",
    "                        (\"svc\", SVC(kernel=\"linear\", probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "etree_w2v_custom.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = etree_w2v_custom.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = etree_w2v_custom.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "etree_w2v_tfidf_custom.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = etree_w2v_tfidf_custom.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = etree_w2v_tfidf_custom.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.137)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "svc_w2v_custom.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = svc_w2v_custom.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = svc_w2v_custom.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.135)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "svc_w2v_tfidf_custom.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = svc_w2v_tfidf_custom.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = svc_w2v_tfidf_custom.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.145)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import types\n",
    "import tempfile\n",
    "import keras.models\n",
    "\n",
    "def make_keras_picklable():\n",
    "    def __getstate__(self):\n",
    "        model_str = \"\"\n",
    "        with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\n",
    "            keras.models.save_model(self, fd.name, overwrite=True)\n",
    "            model_str = fd.read()\n",
    "        d = { 'model_str': model_str }\n",
    "        return d\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\n",
    "            fd.write(state['model_str'])\n",
    "            fd.flush()\n",
    "            model = keras.models.load_model(fd.name)\n",
    "        self.__dict__ = model.__dict__\n",
    "\n",
    "\n",
    "    cls = keras.models.Model\n",
    "    cls.__getstate__ = __getstate__\n",
    "    cls.__setstate__ = __setstate__\n",
    "    \n",
    "\n",
    "make_keras_picklable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datetime_string():\n",
    "    localtime = time.localtime(time.time())\n",
    "    datetime_str = str(localtime.tm_year)\n",
    "    if(localtime.tm_mon < 10):\n",
    "        datetime_str += '0'\n",
    "    datetime_str += str(localtime.tm_mon)\n",
    "    if(localtime.tm_mday < 10):\n",
    "        datetime_str += '0'\n",
    "    datetime_str += str(localtime.tm_mday) + '_'\n",
    "    if(localtime.tm_hour < 10):\n",
    "        datetime_str += '0'\n",
    "    datetime_str += str(localtime.tm_hour)\n",
    "    if(localtime.tm_min < 10):\n",
    "        datetime_str += '0'\n",
    "    datetime_str += str(localtime.tm_min) + '_'\n",
    "    \n",
    "    return datetime_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models.model_runner import datetime_string\n",
    "import dill\n",
    "\n",
    "contents_dict = {\n",
    "    'model_cnn_keras' : model_cnn,\n",
    "    'model_lstm_keras' : model_lstm,\n",
    "    \n",
    "    'model_svc_sklearn' : model_svc,\n",
    "    'model_svc_tfidf_sklearn' : model_svc_tfidf,\n",
    "    \n",
    "    'model_nb_sklearn' : model_nb,\n",
    "    'model_nb_tfidf_sklearn' : model_nb_tfidf,\n",
    "    \n",
    "    'etree_w2v_sklearn' : etree_w2v,\n",
    "    'etree_w2v_tfidf_sklearn' : etree_w2v_tfidf,\n",
    "    \n",
    "    'svc_w2v_sklearn' : svc_w2v,\n",
    "    'svc_w2v_tfidf_sklearn' : svc_w2v_tfidf,\n",
    "    \n",
    "    'etree_w2v_custom_sklearn' : etree_w2v_custom,\n",
    "    'etree_w2v_tfidf_custom_sklearn' : etree_w2v_tfidf_custom,\n",
    "    \n",
    "    'svc_w2v_custom_sklearn' : svc_w2v_custom,\n",
    "    'svc_w2v_tfidf_custom_sklearn' : svc_w2v_tfidf_custom,\n",
    "    \n",
    "    'hyperparameters_cnn' : hyperparameters_cnn,\n",
    "    'hyperparameters_lstm' : hyperparameters_lstm,\n",
    "    \n",
    "    'label_encoder' : label_encoder,\n",
    "    'tokenizer' : tokenizer\n",
    "}\n",
    "   \n",
    "pickle.dump(contents_dict,open( \"results/\"+datetime_string()+\"ensemble_models.pickle\", \"wb\" ) )\n",
    "\n",
    "etree_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=50))])\n",
    "etree_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"svc\", ExtraTreesClassifier(n_estimators=50))])\n",
    "svc_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"extra trees\", SVC(kernel=\"linear\", probability=True))])\n",
    "svc_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"svc\", SVC(kernel=\"linear\", probability=True))])\n",
    "\n",
    "etree_w2v_custom = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v_custom)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=50))])\n",
    "etree_w2v_tfidf_custom = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v_custom)), \n",
    "                        (\"svc\", ExtraTreesClassifier(n_estimators=50))])\n",
    "svc_w2v_custom = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v_custom)), \n",
    "                        (\"extra trees\", SVC(kernel=\"linear\", probability=True))])\n",
    "svc_w2v_tfidf_custom = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v_custom)), \n",
    "                        (\"svc\", SVC(kernel=\"linear\", probability=True))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
