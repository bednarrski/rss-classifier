{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from api import inoreader_api, inoreader_scraping\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape feeds starting from a concrete moment and store them in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=rHZjQ582igTs\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=AzGTbQYwpKpn\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=UfamTHUNYlJQ\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=QDaon65eHcnb\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=M1F3EI0jWJEr\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=U8_DDKnwQFLP\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=1Ip54OMeKfPy\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=z7fm1iiaRfpf\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=FmBgeb1XNcrM\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=uX2Ap5YeTKDk\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=0y_oKEkTduTG\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=AT6HiC5a960m\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=x1u0DIPNx_dT\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=XHTg4qNsAPoz\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=4tdIChkwa6m0\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=HSHiZNLypXuY\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=AzO3LnQleG_Q\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=m4a5AwAfeiut\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=E0S_66J_lMBl\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=m65xLiyYb6rn\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=SbOfSOA6gjGJ\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=M5cyBPHWYxQb\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=YLwpfZSHUfjq\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=9CczPqRQqUzG\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=455uzM7sAD6K\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=11KjOq6YlEqu\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=nM10IOEBDHJo\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=6FLmHPMztuIW\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=7oWLOKFsdnx1\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=nODwhqi_hlNp\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=051R0lrCgWiO\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=bQ3iQYqK617U\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=zyC5_kxEK2dn\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=8_CzbpcYHRH5\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=fZxjwgmTjgMD\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=dkc7s3_bp_di\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=3gZU2yQBaIUw\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=f5Jf7NkclFCT\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=B6AtCad4WGHC\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=toT4IpGQkkU9\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=BKBCDpLG06RX\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=8BTcOkaalLbo\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=JNkKDCdbJ2dk\n",
      "(200, 'OK')\n",
      "860\n"
     ]
    }
   ],
   "source": [
    "start_time = 1508112001\n",
    "feed = 'user/-/label/arXiv'\n",
    "file_name = 'articles_raw.json'\n",
    "\n",
    "number_scraped = inoreader_scraping.scrape(feed, start_time, file_name)\n",
    "print(number_scraped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the feeds from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(file_name, 'r')\n",
    "json_string = f.read()\n",
    "f.close\n",
    "\n",
    "json_object = json.loads(json_string)\n",
    "all_articles = json_object['all_articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_abstract_string(abs_dict):\n",
    "    abs_string = abs_dict['content']\n",
    "    abs_string = re.search('<p>(.*)</p>', abs_string, flags=re.DOTALL)\n",
    "    return abs_string.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_canonical_string(canonical_list):\n",
    "    url_dict = canonical_list[0]\n",
    "    url = url_dict['href']\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create preprocessed and cleaned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/ipykernel/__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "all_articles_pd = pd.DataFrame(all_articles)\n",
    "all_articles_pd = all_articles_pd[['canonical', 'author', 'categories', 'published', 'title', 'summary']]\n",
    "\n",
    "for i in range(len(all_articles_pd)):\n",
    "    all_articles_pd['summary'][i] = get_abstract_string(all_articles_pd['summary'][i])\n",
    "    all_articles_pd['canonical'][i] = get_canonical_string(all_articles_pd['canonical'][i])\n",
    "    \n",
    "all_articles_pd['read'] = False\n",
    "all_articles_pd['liked'] = False\n",
    "\n",
    "for i in range(len(all_articles_pd)):\n",
    "    if 'user/1005689817/state/com.google/read' in all_articles_pd['categories'][i]:\n",
    "        all_articles_pd['read'][i] = True\n",
    "    if 'user/1005689817/state/com.google/like' in all_articles_pd['categories'][i]:\n",
    "        all_articles_pd['liked'][i] = True\n",
    "\n",
    "del all_articles_pd['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all_articles_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_articles_pd = all_articles_pd[all_articles_pd['read'] == True]\n",
    "\n",
    "with open('old_articles_proto2.pickle', 'rb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    old_tagged_articles_pd = pickle.load(f)\n",
    "    \n",
    "tagged_articles_pd = pd.concat([tagged_articles_pd, old_tagged_articles_pd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_articles_unique_pd = tagged_articles_pd.sort_values(by=['canonical', 'liked'], ascending=False).reset_index(drop=True)\n",
    "to_drop = tagged_articles_unique_pd.duplicated(subset = ['canonical'], keep='first')\n",
    "to_drop = list(to_drop[to_drop == False].index.values)\n",
    "\n",
    "tagged_articles_unique_pd = tagged_articles_unique_pd[tagged_articles_unique_pd.index.isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2192\n",
      "2029\n"
     ]
    }
   ],
   "source": [
    "print(len(tagged_articles_pd))\n",
    "print(len(tagged_articles_unique_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ = list(tagged_articles_unique_pd['author']+' '+tagged_articles_unique_pd['title']+' '+tagged_articles_unique_pd['summary'])\n",
    "y_ = list(tagged_articles_unique_pd['liked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_shuffled = [i for i in range(len(X_))]\n",
    "random.shuffle(index_shuffled)\n",
    "X = []\n",
    "y = []\n",
    "for i in index_shuffled:\n",
    "    X.append(X_[i])\n",
    "    y.append(y_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.preprocessing.text\n",
    "from string import maketrans\n",
    "\n",
    "def text_to_word_sequence(text,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True, split=\" \"):\n",
    "    if lower: text = text.lower()\n",
    "    if type(text) == unicode:\n",
    "        translate_table = {ord(c): ord(t) for c,t in zip(filters, split*len(filters)) }\n",
    "    else:\n",
    "        translate_table = maketrans(filters, split * len(filters))\n",
    "    text = text.translate(translate_table)\n",
    "    seq = text.split(split)\n",
    "    return [i for i in seq if i]\n",
    "    \n",
    "keras.preprocessing.text.text_to_word_sequence = text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('stopwords/english') as f:\n",
    "    stopwords = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "893\n",
      "683\n"
     ]
    }
   ],
   "source": [
    "print(len(X[4]))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    seq = text_to_word_sequence(X[i])\n",
    "    clean_seq = [word for word in seq if word not in stopwords]\n",
    "    X[i] = ' '.join(clean_seq)\n",
    "    \n",
    "print(len(X[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.88669950738916259, 0.93994778067885132)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "model_nb_tfidf = Pipeline([\n",
    "    (\"tfidf_vectorizer\", TfidfVectorizer()),\\\n",
    "    (\"nb\", BernoulliNB())\n",
    "])\n",
    "\n",
    "limit = int(0.8*len(X))\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_nb_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_nb_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.88669950738916259, 0.93994778067885132, 0, 406)\n",
      "(0.0, '% - ', 0)\n",
      "\n",
      "\n",
      "('Rejected good ones: ', 46, '/', 46)\n",
      "('Accepted wrong ones:', 0, '/', 360)\n",
      "\n",
      "\n",
      "('Rejected wrong ones: ', 360, '/', 360)\n",
      "('Accepted good ones:', 0, '/', 46)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   1.02955817e-27],\n",
       "       [  1.00000000e+00,   6.30777701e-30],\n",
       "       [  1.00000000e+00,   5.70110246e-29],\n",
       "       [  1.00000000e+00,   2.96748834e-25],\n",
       "       [  1.00000000e+00,   5.44886279e-34],\n",
       "       [  1.00000000e+00,   1.17075050e-21],\n",
       "       [  1.00000000e+00,   1.03236179e-25],\n",
       "       [  1.00000000e+00,   3.38034297e-27],\n",
       "       [  1.00000000e+00,   4.92712842e-27],\n",
       "       [  1.00000000e+00,   5.55070573e-30],\n",
       "       [  1.00000000e+00,   1.64270098e-25],\n",
       "       [  1.00000000e+00,   3.28509143e-28],\n",
       "       [  1.00000000e+00,   1.64008798e-23],\n",
       "       [  1.00000000e+00,   7.77876829e-23],\n",
       "       [  1.00000000e+00,   2.62401596e-34],\n",
       "       [  1.00000000e+00,   1.06384566e-22],\n",
       "       [  1.00000000e+00,   1.02830899e-27],\n",
       "       [  1.00000000e+00,   2.65577082e-24],\n",
       "       [  1.00000000e+00,   1.52213930e-28],\n",
       "       [  1.00000000e+00,   1.54139711e-22]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_nb_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.15)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.15)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)/len(y_predicted)*100.0,\"% - \", int(sum(y_predicted)/len(y_predicted)*50.0))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (y_predicted == False)]), \"/\", sum(y_test))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (y_predicted == True)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == y_test) & (y_predicted == False)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == y_test) & (y_predicted == True)]), \"/\", sum(y_test))\n",
    "\n",
    "y_predicted_proba[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.88177339901477836, 0.88540904309658419)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "model_svc = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    (\"svc\", SVC(kernel='linear', probability=True, class_weight = 'balanced'))\n",
    "    #(\"svc\", SVC(kernel='linear', probability=True))\n",
    "])\n",
    "\n",
    "limit = int(0.8*len(X))\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_svc.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_svc.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.8522167487684733"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_predicted)*100.0/len(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.66502463054187189, 0.60374231488906716, 164, 406)\n",
      "(40.39408866995074, '% - ', 20)\n",
      "\n",
      "\n",
      "('Rejected good ones: ', 9, '/', 46)\n",
      "('Accepted wrong ones:', 127, '/', 360)\n",
      "\n",
      "\n",
      "('Rejected wrong ones: ', 233, '/', 360)\n",
      "('Accepted good ones:', 37, '/', 46)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.83160791,  0.16839209],\n",
       "       [ 0.90376631,  0.09623369],\n",
       "       [ 0.70319026,  0.29680974],\n",
       "       [ 0.84286149,  0.15713851],\n",
       "       [ 0.94557018,  0.05442982],\n",
       "       [ 0.81690314,  0.18309686],\n",
       "       [ 0.79783034,  0.20216966],\n",
       "       [ 0.83107591,  0.16892409],\n",
       "       [ 0.89529812,  0.10470188],\n",
       "       [ 0.83194443,  0.16805557],\n",
       "       [ 0.89418885,  0.10581115],\n",
       "       [ 0.928681  ,  0.071319  ],\n",
       "       [ 0.86478599,  0.13521401],\n",
       "       [ 0.80600268,  0.19399732],\n",
       "       [ 0.95882427,  0.04117573],\n",
       "       [ 0.80979873,  0.19020127],\n",
       "       [ 0.90021698,  0.09978302],\n",
       "       [ 0.92225488,  0.07774512],\n",
       "       [ 0.79529416,  0.20470584],\n",
       "       [ 0.7356629 ,  0.2643371 ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_svc.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.15)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (y_predicted == False)]), \"/\", sum(y_test))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (y_predicted == True)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == y_test) & (y_predicted == False)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == y_test) & (y_predicted == True)]), \"/\", sum(y_test))\n",
    "\n",
    "y_predicted_proba[0:20]\n",
    "\n",
    "#(0.62077922077922076, 0.55417162908266104, 162, 385)\n",
    "#(42.077922077922075, '% - ', 21)\n",
    "#('Rejected good ones: ', 15, '/', 46)\n",
    "#('Accepted wrong ones:', 131, '/', 339)\n",
    "#('Rejected wrong ones: ', 208, '/', 339)\n",
    "#('Accepted good ones:', 31, '/', 46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Input, Embedding, SimpleRNN, Dense, Activation, TimeDistributed, Bidirectional, LSTM, GaussianNoise)\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model #Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASETS_DIR = '../ml-research/datasets/'\n",
    "GLOVE_DIR = DATASETS_DIR+'glove.6B/'\n",
    "WIKI_EN_DIR = DATASETS_DIR+'wiki.en/'\n",
    "#embeddings_file = os.path.join(GLOVE_DIR, 'glove.6B.300d.txt')\n",
    "embeddings_file = os.path.join(WIKI_EN_DIR, 'wiki.en.vec')\n",
    "\n",
    "# Word embeddings' constraints\n",
    "MAX_NB_WORDS = 20000  # Number of most common words for tokenizer\n",
    "EMBEDDING_DIM = 300   # Embeddings dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.preprocessing.text\n",
    "from string import maketrans\n",
    "\n",
    "def text_to_word_sequence(text,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True, split=\" \"):\n",
    "    if lower: text = text.lower()\n",
    "    if type(text) == unicode:\n",
    "        translate_table = {ord(c): ord(t) for c,t in zip(filters, split*len(filters)) }\n",
    "    else:\n",
    "        translate_table = maketrans(filters, split * len(filters))\n",
    "    text = text.translate(translate_table)\n",
    "    seq = text.split(split)\n",
    "    return [i for i in seq if i]\n",
    "    \n",
    "keras.preprocessing.text.text_to_word_sequence = text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23177 unique tokens.\n",
      "87\n",
      "23176\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing and creating word index\n",
    "\n",
    "additional_words = ['unk', 'num']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X+additional_words)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# inversing the word_index.\n",
    "index_word = dict((k,v) for v,k in word_index.items())\n",
    "\n",
    "# example\n",
    "print(word_index['adversarial'])\n",
    "print(word_index['unk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 2519371 word vectors.\n",
      "Creating Word Embeddings matrix...\n",
      "Word Embeddings matrix was successfuly created.\n"
     ]
    }
   ],
   "source": [
    "import models.embedding_matrix as embedding\n",
    "\n",
    "embedding_matrix = embedding.create_embedding_matrix(embeddings_file, MAX_NB_WORDS, EMBEDDING_DIM, word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Input, Embedding, SimpleRNN, Dense, Activation, TimeDistributed, Bidirectional,\n",
    "                          LSTM, GaussianNoise,Conv1D, MaxPooling1D, Flatten, Dropout)\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'conv_units': 128,\n",
    "    'hidden_units_1': 128,\n",
    "    'hidden_units_2': 64,\n",
    "    'dropout': 0.5,\n",
    "    'pooling' : 5,\n",
    "    'gauss_stddev' : 0.0002,\n",
    "    'epochs' : 35,\n",
    "    'folds' : 5,\n",
    "    'nclasses' : 2,\n",
    "    'max_seq_len' : 300,\n",
    "    'learning_rate' : 0.001,\n",
    "    'stopwords' : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB0syLToAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc3f9bffb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "X_sequences_len = []\n",
    "for item in X_sequences:\n",
    "    X_sequences_len.append(\n",
    "                            min( len(item), 1000\n",
    "                               ))\n",
    "    \n",
    "X_sequences_padded = pad_sequences(X_sequences, maxlen=hyperparameters['max_seq_len'])\n",
    "\n",
    "plt.hist(X_sequences_len, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "\n",
    "y_num = label_encoder.transform(y)\n",
    "y_matrix = to_categorical(y_num,hyperparameters['nclasses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(len(y)-sum(y))/sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1623 samples, validate on 406 samples\n",
      "Epoch 1/35\n",
      "1623/1623 [==============================] - 10s - loss: 1.1574 - categorical_accuracy: 0.6599 - val_loss: 0.6563 - val_categorical_accuracy: 0.8695\n",
      "Epoch 2/35\n",
      "1623/1623 [==============================] - 10s - loss: 1.1423 - categorical_accuracy: 0.8632 - val_loss: 0.5935 - val_categorical_accuracy: 0.8695\n",
      "Epoch 3/35\n",
      "1623/1623 [==============================] - 10s - loss: 1.1385 - categorical_accuracy: 0.7930 - val_loss: 0.6739 - val_categorical_accuracy: 0.5099\n",
      "Epoch 4/35\n",
      "1623/1623 [==============================] - 10s - loss: 1.1194 - categorical_accuracy: 0.6950 - val_loss: 0.5431 - val_categorical_accuracy: 0.8079\n",
      "Epoch 5/35\n",
      "1623/1623 [==============================] - 10s - loss: 1.0950 - categorical_accuracy: 0.7012 - val_loss: 0.6159 - val_categorical_accuracy: 0.5813\n",
      "Epoch 6/35\n",
      "1623/1623 [==============================] - 10s - loss: 1.0504 - categorical_accuracy: 0.6556 - val_loss: 0.6498 - val_categorical_accuracy: 0.5099\n",
      "Epoch 7/35\n",
      "1623/1623 [==============================] - 10s - loss: 1.0311 - categorical_accuracy: 0.6636 - val_loss: 0.6419 - val_categorical_accuracy: 0.5419\n",
      "Epoch 8/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.9126 - categorical_accuracy: 0.6907 - val_loss: 0.5310 - val_categorical_accuracy: 0.7192\n",
      "Epoch 9/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.7595 - categorical_accuracy: 0.7794 - val_loss: 0.6703 - val_categorical_accuracy: 0.6379\n",
      "Epoch 10/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.5005 - categorical_accuracy: 0.8620 - val_loss: 0.7587 - val_categorical_accuracy: 0.6995\n",
      "Epoch 11/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.2812 - categorical_accuracy: 0.9285 - val_loss: 0.7662 - val_categorical_accuracy: 0.7562\n",
      "Epoch 12/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.1602 - categorical_accuracy: 0.9612 - val_loss: 0.9043 - val_categorical_accuracy: 0.8424\n",
      "Epoch 13/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0891 - categorical_accuracy: 0.9846 - val_loss: 1.0763 - val_categorical_accuracy: 0.7463\n",
      "Epoch 14/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0648 - categorical_accuracy: 0.9920 - val_loss: 1.0513 - val_categorical_accuracy: 0.7635\n",
      "Epoch 15/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0764 - categorical_accuracy: 0.9852 - val_loss: 1.0076 - val_categorical_accuracy: 0.7709\n",
      "Epoch 16/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0313 - categorical_accuracy: 0.9926 - val_loss: 1.1458 - val_categorical_accuracy: 0.8547\n",
      "Epoch 17/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0145 - categorical_accuracy: 0.9982 - val_loss: 1.2479 - val_categorical_accuracy: 0.8498\n",
      "Epoch 18/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0067 - categorical_accuracy: 0.9988 - val_loss: 1.2937 - val_categorical_accuracy: 0.8522\n",
      "Epoch 19/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0034 - categorical_accuracy: 1.0000 - val_loss: 1.3569 - val_categorical_accuracy: 0.8202\n",
      "Epoch 20/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0028 - categorical_accuracy: 0.9994 - val_loss: 1.4960 - val_categorical_accuracy: 0.7562\n",
      "Epoch 21/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0038 - categorical_accuracy: 0.9994 - val_loss: 1.3885 - val_categorical_accuracy: 0.8571\n",
      "Epoch 22/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0017 - categorical_accuracy: 1.0000 - val_loss: 1.3817 - val_categorical_accuracy: 0.8054\n",
      "Epoch 23/35\n",
      "1623/1623 [==============================] - 10s - loss: 9.2619e-04 - categorical_accuracy: 1.0000 - val_loss: 1.3817 - val_categorical_accuracy: 0.8424\n",
      "Epoch 24/35\n",
      "1623/1623 [==============================] - 10s - loss: 6.6712e-04 - categorical_accuracy: 1.0000 - val_loss: 1.4159 - val_categorical_accuracy: 0.8498\n",
      "Epoch 25/35\n",
      "1623/1623 [==============================] - 10s - loss: 6.0101e-04 - categorical_accuracy: 1.0000 - val_loss: 1.4307 - val_categorical_accuracy: 0.8325\n",
      "Epoch 26/35\n",
      "1623/1623 [==============================] - 10s - loss: 3.5854e-04 - categorical_accuracy: 1.0000 - val_loss: 1.4592 - val_categorical_accuracy: 0.8251\n",
      "Epoch 27/35\n",
      "1623/1623 [==============================] - 10s - loss: 3.2036e-04 - categorical_accuracy: 1.0000 - val_loss: 1.4763 - val_categorical_accuracy: 0.8300\n",
      "Epoch 28/35\n",
      "1623/1623 [==============================] - 10s - loss: 2.2219e-04 - categorical_accuracy: 1.0000 - val_loss: 1.4967 - val_categorical_accuracy: 0.8325\n",
      "Epoch 29/35\n",
      "1623/1623 [==============================] - 10s - loss: 1.4499e-04 - categorical_accuracy: 1.0000 - val_loss: 1.5048 - val_categorical_accuracy: 0.8374\n",
      "Epoch 30/35\n",
      "1623/1623 [==============================] - 10s - loss: 2.7118e-04 - categorical_accuracy: 1.0000 - val_loss: 1.5192 - val_categorical_accuracy: 0.8325\n",
      "Epoch 31/35\n",
      "1623/1623 [==============================] - 10s - loss: 2.0947e-04 - categorical_accuracy: 1.0000 - val_loss: 1.5207 - val_categorical_accuracy: 0.8374\n",
      "Epoch 32/35\n",
      "1623/1623 [==============================] - 10s - loss: 1.4272e-04 - categorical_accuracy: 1.0000 - val_loss: 1.5213 - val_categorical_accuracy: 0.8424\n",
      "Epoch 33/35\n",
      "1623/1623 [==============================] - 10s - loss: 2.6261e-04 - categorical_accuracy: 1.0000 - val_loss: 1.5216 - val_categorical_accuracy: 0.8424\n",
      "Epoch 34/35\n",
      "1623/1623 [==============================] - 10s - loss: 2.5080e-04 - categorical_accuracy: 1.0000 - val_loss: 1.5271 - val_categorical_accuracy: 0.8276\n",
      "Epoch 35/35\n",
      "1623/1623 [==============================] - 10s - loss: 1.2969e-04 - categorical_accuracy: 1.0000 - val_loss: 1.5128 - val_categorical_accuracy: 0.8374\n",
      "(0.83743842364532017, 0.81860688572511231)\n",
      "[[331  22]\n",
      " [ 44   9]]\n",
      "Train on 1623 samples, validate on 406 samples\n",
      "Epoch 1/35\n",
      "1623/1623 [==============================] - 10s - loss: 1.1482 - categorical_accuracy: 0.5970 - val_loss: 0.6741 - val_categorical_accuracy: 0.8522\n",
      "Epoch 2/35\n",
      "1623/1623 [==============================] - 10s - loss: 1.1383 - categorical_accuracy: 0.8509 - val_loss: 0.6422 - val_categorical_accuracy: 0.8522\n",
      "Epoch 3/35\n",
      "1623/1623 [==============================] - 10s - loss: 1.1265 - categorical_accuracy: 0.8694 - val_loss: 0.6294 - val_categorical_accuracy: 0.8522\n",
      "Epoch 4/35\n",
      "1623/1623 [==============================] - 10s - loss: 1.1031 - categorical_accuracy: 0.8601 - val_loss: 0.6088 - val_categorical_accuracy: 0.8005\n",
      "Epoch 5/35\n",
      "1623/1623 [==============================] - 10s - loss: 1.0602 - categorical_accuracy: 0.8256 - val_loss: 0.5943 - val_categorical_accuracy: 0.7094\n",
      "Epoch 6/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.9974 - categorical_accuracy: 0.7807 - val_loss: 0.5492 - val_categorical_accuracy: 0.7783\n",
      "Epoch 7/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.8961 - categorical_accuracy: 0.8145 - val_loss: 0.5688 - val_categorical_accuracy: 0.7167\n",
      "Epoch 8/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.6907 - categorical_accuracy: 0.8404 - val_loss: 1.0637 - val_categorical_accuracy: 0.4236\n",
      "Epoch 9/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.5892 - categorical_accuracy: 0.8238 - val_loss: 0.9194 - val_categorical_accuracy: 0.5788\n",
      "Epoch 10/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.3560 - categorical_accuracy: 0.9094 - val_loss: 0.9531 - val_categorical_accuracy: 0.7266\n",
      "Epoch 11/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.1661 - categorical_accuracy: 0.9544 - val_loss: 1.1573 - val_categorical_accuracy: 0.8128\n",
      "Epoch 12/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.1000 - categorical_accuracy: 0.9784 - val_loss: 1.2935 - val_categorical_accuracy: 0.8103\n",
      "Epoch 13/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0730 - categorical_accuracy: 0.9815 - val_loss: 1.3725 - val_categorical_accuracy: 0.8030\n",
      "Epoch 14/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.2312 - categorical_accuracy: 0.9482 - val_loss: 1.4516 - val_categorical_accuracy: 0.8325\n",
      "Epoch 15/35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1623/1623 [==============================] - 10s - loss: 0.1194 - categorical_accuracy: 0.9710 - val_loss: 1.2183 - val_categorical_accuracy: 0.7734\n",
      "Epoch 16/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0530 - categorical_accuracy: 0.9895 - val_loss: 1.3227 - val_categorical_accuracy: 0.7906\n",
      "Epoch 17/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0314 - categorical_accuracy: 0.9957 - val_loss: 1.4912 - val_categorical_accuracy: 0.7217\n",
      "Epoch 18/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0456 - categorical_accuracy: 0.9883 - val_loss: 1.5546 - val_categorical_accuracy: 0.8153\n",
      "Epoch 19/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0560 - categorical_accuracy: 0.9963 - val_loss: 1.4079 - val_categorical_accuracy: 0.8054\n",
      "Epoch 20/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0238 - categorical_accuracy: 0.9957 - val_loss: 1.4875 - val_categorical_accuracy: 0.8350\n",
      "Epoch 21/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0181 - categorical_accuracy: 0.9975 - val_loss: 1.4623 - val_categorical_accuracy: 0.8005\n",
      "Epoch 22/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0116 - categorical_accuracy: 0.9982 - val_loss: 1.5185 - val_categorical_accuracy: 0.7611\n",
      "Epoch 23/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0042 - categorical_accuracy: 0.9994 - val_loss: 1.5818 - val_categorical_accuracy: 0.8153\n",
      "Epoch 24/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0027 - categorical_accuracy: 1.0000 - val_loss: 1.5957 - val_categorical_accuracy: 0.8030\n",
      "Epoch 25/35\n",
      "1623/1623 [==============================] - 10s - loss: 0.0013 - categorical_accuracy: 1.0000 - val_loss: 1.6257 - val_categorical_accuracy: 0.7857\n",
      "Epoch 26/35\n",
      "1623/1623 [==============================] - 10s - loss: 9.8261e-04 - categorical_accuracy: 1.0000 - val_loss: 1.6476 - val_categorical_accuracy: 0.7956\n",
      "Epoch 27/35\n",
      "1623/1623 [==============================] - 10s - loss: 9.4667e-04 - categorical_accuracy: 1.0000 - val_loss: 1.6658 - val_categorical_accuracy: 0.8079\n",
      "Epoch 28/35\n",
      "1623/1623 [==============================] - 10s - loss: 9.8413e-04 - categorical_accuracy: 1.0000 - val_loss: 1.6854 - val_categorical_accuracy: 0.8005\n",
      "Epoch 29/35\n",
      "1623/1623 [==============================] - 10s - loss: 6.4036e-04 - categorical_accuracy: 1.0000 - val_loss: 1.7010 - val_categorical_accuracy: 0.7956\n",
      "Epoch 30/35\n",
      "1623/1623 [==============================] - 10s - loss: 6.2156e-04 - categorical_accuracy: 1.0000 - val_loss: 1.7151 - val_categorical_accuracy: 0.8030\n",
      "Epoch 31/35\n",
      "1623/1623 [==============================] - 10s - loss: 6.8085e-04 - categorical_accuracy: 1.0000 - val_loss: 1.7386 - val_categorical_accuracy: 0.7980\n",
      "Epoch 32/35\n",
      "1623/1623 [==============================] - 10s - loss: 4.4046e-04 - categorical_accuracy: 1.0000 - val_loss: 1.7275 - val_categorical_accuracy: 0.8177\n",
      "Epoch 33/35\n",
      "1623/1623 [==============================] - 10s - loss: 6.9022e-04 - categorical_accuracy: 1.0000 - val_loss: 1.7287 - val_categorical_accuracy: 0.8177\n",
      "Epoch 34/35\n",
      "1623/1623 [==============================] - 10s - loss: 4.4392e-04 - categorical_accuracy: 1.0000 - val_loss: 1.7366 - val_categorical_accuracy: 0.8030\n",
      "Epoch 35/35\n",
      "1623/1623 [==============================] - 10s - loss: 6.4883e-04 - categorical_accuracy: 1.0000 - val_loss: 1.7539 - val_categorical_accuracy: 0.7980\n",
      "(0.79802955665024633, 0.78066917584546414)\n",
      "[[315  31]\n",
      " [ 51   9]]\n",
      "Train on 1623 samples, validate on 406 samples\n",
      "Epoch 1/35\n",
      "1623/1623 [==============================] - 10s - loss: 1.1751 - categorical_accuracy: 0.8453 - val_loss: 0.6787 - val_categorical_accuracy: 0.8867\n",
      "Epoch 2/35\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import utils.evaluation as evaluation\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "class_weight = {0 : 1.,\n",
    "    1: (len(y)-sum(y))/sum(y)}\n",
    "\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "cm_summed = np.zeros((2,2))\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "for train_index, test_index in kf.split(X_sequences):\n",
    "    K.clear_session()\n",
    "    \n",
    "    X_sequences_padded = pad_sequences(X_sequences, maxlen=hyperparameters['max_seq_len'])\n",
    "\n",
    "    num_words = embedding_matrix.shape[0]\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=hyperparameters['max_seq_len'],\n",
    "                                trainable=False)\n",
    "\n",
    "    # train a 1D convnet with global maxpooling\n",
    "    sequence_input = Input(shape=(hyperparameters['max_seq_len'],), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    #x = GaussianNoise(hyperparameters['gauss_stddev'])(embedded_sequences)\n",
    "    x = embedded_sequences\n",
    "    x = Conv1D(2*hyperparameters['conv_units'], 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(hyperparameters['conv_units'], 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(hyperparameters['hidden_units_1'], activation='relu')(x)\n",
    "    x = Dropout(hyperparameters['dropout'])(x)\n",
    "    x = Dense(hyperparameters['hidden_units_2'], activation='relu')(x)\n",
    "    preds = Dense(hyperparameters['nclasses'], activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    \n",
    "    X_train, X_test = X_sequences_padded[train_index], X_sequences_padded[test_index]\n",
    "    y_train, y_test = y_matrix[train_index], y_matrix[test_index]\n",
    "\n",
    "    # train\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=128,\n",
    "              epochs=hyperparameters['epochs'],\n",
    "              class_weight=class_weight,\n",
    "              validation_data=(X_test, y_test))\n",
    "\n",
    "    # run\n",
    "    ground_truth = y_test.argmax(1)\n",
    "    predictions = [list(map(lambda x: x, model.predict_on_batch(np.asarray([x])).argmax(1)))[0] \\\n",
    "            for x in X_test]\n",
    "    \n",
    "    \n",
    "    acc = evaluation.accuracy(ground_truth, predictions)\n",
    "    f1 = evaluation.eval_f1_score(ground_truth, predictions)    \n",
    "    cm = evaluation.cm_matrix(ground_truth, predictions)\n",
    "    acc_list.append(acc)\n",
    "    f1_list.append(f1)\n",
    "    cm_summed = cm_summed + cm\n",
    "    print(acc, f1)\n",
    "    print(cm)\n",
    "    \n",
    "    #break\n",
    "    #del model\n",
    "    #K.clear_session()\n",
    "    \n",
    "print(np.asarray(acc).mean(), np.asarray(f1).mean())\n",
    "print(cm_summed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth = y_test.argmax(1)\n",
    "predictions = [list(map(lambda x: x, model.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "        for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_proba_np = np.asarray(predictions)\n",
    "#y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "y_predicted = (y_predicted_proba_np[:,1] > 0.7)\n",
    "\n",
    "acc = accuracy_score(y_predicted, ground_truth)\n",
    "f1 = f1_score(y_predicted, ground_truth, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*30.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != ground_truth) & (y_predicted == False)]), \"/\", sum(ground_truth))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != ground_truth) & (y_predicted == True)]), \"/\", len(ground_truth)-sum(ground_truth))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == ground_truth) & (y_predicted == False)]), \"/\", len(ground_truth)-sum(ground_truth))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == ground_truth) & (y_predicted == True)]), \"/\", sum(ground_truth))\n",
    "\n",
    "# 15% bezbolesnie odrzucic - 0.000008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# evaluate\n",
    "y_predicted = [list(map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x])).argmax(1)))[0] \\\n",
    "               for x in x_test]\n",
    "y_test = y[limit:]\n",
    "y_test = to_categorical(y_test,2)\n",
    "y_test = y_test.argmax(1)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted))\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Total good ones:\", sum(y_test))\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (np.asarray(y_predicted) == 0)]))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (np.asarray(y_predicted) == 1)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted_proba = [list(map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "               for x in x_test]\n",
    "y_predicted_proba_np = np.asarray(y_predicted_proba)\n",
    "#y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "y_predicted = (y_predicted_proba_np[:,1] > 0.2)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)/len(y_predicted)*100.0,\"% - \", int(sum(y_predicted)/len(y_predicted)*30.0))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (y_predicted == False)]), \"/\", sum(y_test))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (y_predicted == True)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == y_test) & (y_predicted == False)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == y_test) & (y_predicted == True)]), \"/\", sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "reload(runner)\n",
    "# Padding sequences\n",
    "x_train = pad_sequences(sequences_train, maxlen=hyperparameters['max_seq_len'])\n",
    "x_test = pad_sequences(sequences_test, maxlen=hyperparameters['max_seq_len'])\n",
    "\n",
    "dataset = x_train, y_train, x_test, y_test, embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "result_cnn, model_cnn = runner.build_train_run(dataset, le, hyperparameters_cnn, save=False, return_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "import numpy as np\n",
    "y_predicted = [list(map(lambda x: x, model_cnn.predict_on_batch(np.asarray([x])).argmax(1)))[0] \\\n",
    "               for x in x_test]\n",
    "y_test = y[limit:]\n",
    "y_test = to_categorical(y_test,hyperparameters['nclasses'])\n",
    "y_test = y_test.argmax(1)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted))\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Total good ones:\", sum(y_test))\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (np.asarray(y_predicted) == 0)]))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (np.asarray(y_predicted) == 1)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted_proba = [list(map(lambda x: x, model_cnn.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "               for x in x_test]\n",
    "y_predicted_proba_np = np.asarray(y_predicted_proba)\n",
    "\n",
    "#y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "y_predicted = (y_predicted_proba_np[:,1] > 0.0005)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)/len(y_predicted)*100.0,\"% - \", int(sum(y_predicted)/len(y_predicted)*30.0))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (y_predicted == False)]), \"/\", sum(y_test))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (y_predicted == True)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == y_test) & (y_predicted == False)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == y_test) & (y_predicted == True)]), \"/\", sum(y_test))\n",
    "\n",
    "y_predicted_proba[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted_proba = [list(map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "               for x in x_test]\n",
    "y_predicted_proba_np = np.asarray(y_predicted_proba)\n",
    "\n",
    "y_predicted_proba[0:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted_proba = [list(map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "               for x in x_test]\n",
    "y_predicted_proba_np = np.asarray(y_predicted_proba)\n",
    "#y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "y_predicted = (y_predicted_proba_np[:,1] > 0.2)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)/len(y_predicted)*100.0,\"% - \", int(sum(y_predicted)/len(y_predicted)*30.0))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected good ones: \", len(X_test_df[(y_predicted != y_test) & (y_predicted == False)]), \"/\", sum(y_test))\n",
    "print(\"Accepted wrong ones:\", len(X_test_df[(y_predicted != y_test) & (y_predicted == True)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"\\n\")\n",
    "print(\"Rejected wrong ones: \", len(X_test_df[(y_predicted == y_test) & (y_predicted == False)]), \"/\", len(y_test)-sum(y_test))\n",
    "print(\"Accepted good ones:\", len(X_test_df[(y_predicted == y_test) & (y_predicted == True)]), \"/\", sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
