{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from api import inoreader_api, inoreader_scraping\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import keras.preprocessing.text\n",
    "from string import maketrans\n",
    "\n",
    "def text_to_word_sequence(text,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True, split=\" \"):\n",
    "    if lower: text = text.lower()\n",
    "    if type(text) == unicode:\n",
    "        translate_table = {ord(c): ord(t) for c,t in zip(filters, split*len(filters)) }\n",
    "    else:\n",
    "        translate_table = maketrans(filters, split * len(filters))\n",
    "    text = text.translate(translate_table)\n",
    "    seq = text.split(split)\n",
    "    return [i for i in seq if i]\n",
    "    \n",
    "keras.preprocessing.text.text_to_word_sequence = text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape feeds starting from a concrete moment and store them in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Eef1NM8nWr67\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=KfAjbRTlHolR\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=HhRSlcZXm8e_\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=ZYQWWgh1n6Jb\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=2PPCaCeI67Xh\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=bdhAxhj_rmEt\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Rfp5CBa0w_3b\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=KzMzLHNiKRqb\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=sgS761Nw5oCD\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=MzdspQ_jdQda\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=kgsGn6f0NgpP\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=DQS7SnON_lxY\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=8efjUwogrXFJ\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=4XZLU6SA1GYi\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=M2M0mTdoYXzJ\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=pCia6EiYFH7X\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=9qk3zyzgQgSM\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=EN26886eLJ5a\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=IDeDrYWQoDI7\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=B550ERZ4NsMy\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=wiNdufMjBqH4\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=O9Bt1FwgyY9C\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=iL9hqd59UBsa\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=9tKf88IOl8zC\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=mbQW8bzMDJOi\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=A71wkQW_HZX7\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=AhGxF8nXjlWU\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=10FL_4ld1fTE\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=W2Mi6gNCSIf5\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=HRWmJ6udLoJw\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=RjpqGWqFZTlr\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=l8_1LEiafDzh\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=g4XZ2BTdk1Ph\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=IcCtZ0wG0tzu\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=JbCtPYs2cj4f\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=8WCYBbn_To6g\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=ugGsm2ndlGH9\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=RpkFxrCcbMTx\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=ygM6yP6hyppX\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Pf5Xa7ZOxKKj\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=WqgwwSykKlg_\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=KIUNadwE6T4i\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=IJWzNikbEtJH\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=fYa3Jf8XWmEw\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=I27HYyAsuq0L\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=F2yXpCFAClhR\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=LHqgjyyWU3eY\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=nkoH4rGauiaf\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=2PJWhUCPCFzZ\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=XBRPPNAPw9bK\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=LgNc1a9yc_mj\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=kr6HhDnoytus\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=gjR5nSY3Qqcq\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=6a6yunGKQZNr\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=cuJ9ZcD5JilP\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=YdK4D1t4J0Gy\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=pdWb44eBiaTT\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=noDLymzDaBbx\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=blCqjygbhubD\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Agul5hJ24DWb\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=YjsHMSqEOwjX\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=HLD6O8mXNzu5\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=IFATLtDAT0OQ\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=_tpf4eWekurj\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=pz7BDKTrW3LJ\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=bEXsornLf_nZ\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=Y4jegkEIXuFx\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=IKNAbCXtKk_U\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=aTUzXBquCF1X\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=1j25G8s0XPpC\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=pp5Nzc8Tjajk\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=uGd6yAoU1Fx0\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=dNceAOxeSDwk\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=jiPW28HgXQ6g\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=hUG_SOX_9kq1\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=bL86OAJyD1EH\n",
      "(200, 'OK')\n",
      "\n",
      "Requesting: stream/contents/user/-/label/arXiv?c=SswM2pShISTT\n",
      "(200, 'OK')\n",
      "1540\n"
     ]
    }
   ],
   "source": [
    "start_time = 1508112001\n",
    "feed = 'user/-/label/arXiv'\n",
    "file_name = 'articles_raw.json'\n",
    "\n",
    "number_scraped = inoreader_scraping.scrape(feed, start_time, file_name)\n",
    "print(number_scraped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the feeds from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(file_name, 'r')\n",
    "json_string = f.read()\n",
    "f.close\n",
    "\n",
    "json_object = json.loads(json_string)\n",
    "all_articles = json_object['all_articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_abstract_string(abs_dict):\n",
    "    abs_string = abs_dict['content']\n",
    "    abs_string = re.search('<p>(.*)</p>', abs_string, flags=re.DOTALL)\n",
    "    return abs_string.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_canonical_string(canonical_list):\n",
    "    url_dict = canonical_list[0]\n",
    "    url = url_dict['href']\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create preprocessed and cleaned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/ipykernel/__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "all_articles_pd = pd.DataFrame(all_articles)\n",
    "all_articles_pd = all_articles_pd[['canonical', 'author', 'categories', 'published', 'title', 'summary']]\n",
    "\n",
    "for i in range(len(all_articles_pd)):\n",
    "    all_articles_pd['summary'][i] = get_abstract_string(all_articles_pd['summary'][i])\n",
    "    all_articles_pd['canonical'][i] = get_canonical_string(all_articles_pd['canonical'][i])\n",
    "    \n",
    "all_articles_pd['read'] = False\n",
    "all_articles_pd['liked'] = False\n",
    "\n",
    "for i in range(len(all_articles_pd)):\n",
    "    if 'user/1005689817/state/com.google/read' in all_articles_pd['categories'][i]:\n",
    "        all_articles_pd['read'][i] = True\n",
    "    if 'user/1005689817/state/com.google/like' in all_articles_pd['categories'][i]:\n",
    "        all_articles_pd['liked'][i] = True\n",
    "\n",
    "del all_articles_pd['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all_articles_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_articles_pd = all_articles_pd[all_articles_pd['read'] == True]\n",
    "\n",
    "with open('old_articles_proto2.pickle', 'rb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    old_tagged_articles_pd = pickle.load(f)\n",
    "    \n",
    "tagged_articles_pd = pd.concat([tagged_articles_pd, old_tagged_articles_pd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_articles_unique_pd = tagged_articles_pd.sort_values(by=['canonical', 'liked'], ascending=False).reset_index(drop=True)\n",
    "to_drop = tagged_articles_unique_pd.duplicated(subset = ['canonical'], keep='first')\n",
    "to_drop = list(to_drop[to_drop == False].index.values)\n",
    "\n",
    "tagged_articles_unique_pd = tagged_articles_unique_pd[tagged_articles_unique_pd.index.isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2605\n",
      "2400\n"
     ]
    }
   ],
   "source": [
    "print(len(tagged_articles_pd))\n",
    "print(len(tagged_articles_unique_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ = list(tagged_articles_unique_pd['author']+' '+tagged_articles_unique_pd['title']+' '+tagged_articles_unique_pd['summary'])\n",
    "y_ = list(tagged_articles_unique_pd['liked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_shuffled = [i for i in range(len(X_))]\n",
    "random.shuffle(index_shuffled)\n",
    "X = []\n",
    "y = []\n",
    "for i in index_shuffled:\n",
    "    X.append(X_[i])\n",
    "    y.append(y_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('stopwords/english') as f:\n",
    "    stopwords = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1314\n",
      "973\n"
     ]
    }
   ],
   "source": [
    "print(len(X[4]))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    seq = text_to_word_sequence(X[i])\n",
    "    clean_seq = [word for word in seq if word not in stopwords]\n",
    "    X[i] = ' '.join(clean_seq)\n",
    "    \n",
    "print(len(X[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving or loading the data and LE and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data_dict = {\n",
    "    'X' : X,\n",
    "    'y' : y\n",
    "}\n",
    "pickle.dump(data_dict,open(\"data.pickle\", \"wb\" ) )\n",
    "\n",
    "\n",
    "limit = int(0.75*len(X))\n",
    "print(limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "data_dict = pickle.load(open( \"data.pickle\", \"r\" ) )\n",
    "X = data_dict['X']\n",
    "y = data_dict['y']\n",
    "limit = int(0.75*len(X))\n",
    "print(limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF -> NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/piotr.bednarski/ml-env-27-gpu/lib/python2.7/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.86833333333333329, 0.92952720785013387)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "model_nb_tfidf = Pipeline([\n",
    "    (\"tfidf_vectorizer\", TfidfVectorizer()),\\\n",
    "    (\"nb\", BernoulliNB())\n",
    "])\n",
    "\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_nb_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_nb_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.86833333333333329, 0.92952720785013387, 0, 600)\n",
      "(0.0, '% - ', 0)\n",
      "\n",
      "\n",
      "Rejected 100.00% of wrong ones\n",
      "Accepted 0.00% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   3.36483172e-20],\n",
       "       [  1.00000000e+00,   2.23937741e-27],\n",
       "       [  1.00000000e+00,   2.85334486e-22],\n",
       "       [  1.00000000e+00,   3.03239846e-23],\n",
       "       [  1.00000000e+00,   1.21601865e-15]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_nb_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.15)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.10)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)/len(y_predicted)*100.0,\"% - \", int(sum(y_predicted)/len(y_predicted)*50.0))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVect -> NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.86833333333333329, 0.9263855514734276)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "model_nb = Pipeline([\n",
    "    #('vect', CountVectorizer(ngram_range=(1,3),stop_words='english')),\\\n",
    "    (\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)),\\\n",
    "    (\"nb\", BernoulliNB())\n",
    "])\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_nb.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.483333333333 0.406585462468 329 600\n",
      "54.8333333333 % -  27\n",
      "\n",
      "\n",
      "Rejected 46.26% of wrong ones\n",
      "Accepted 62.03% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.82088641,  0.17911359],\n",
       "       [ 0.89235893,  0.10764107],\n",
       "       [ 0.83130471,  0.16869529],\n",
       "       [ 0.87191453,  0.12808547],\n",
       "       [ 0.83807984,  0.16192016]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_nb.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.1296)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.1325)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print acc, f1, sum(y_predicted), len(y_predicted)\n",
    "print sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF -> SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.87666666666666671, 0.90398709944465527)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "model_svc_tfidf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,3), min_df=1, max_df=1.0,stop_words='english')),\n",
    "    #('vect', CountVectorizer(ngram_range=(3,6),analyzer='char_wb', min_df=10, max_df=0.95,stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    (\"svc\", SVC(kernel='linear', probability=True, class_weight = 'balanced'))\n",
    "    #(\"svc\", SVC(kernel='linear', probability=True))\n",
    "])\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_svc_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_svc_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.48333333333333334, 0.41519092988862588, 367, 600)\n",
      "(61.166666666666664, '% - ', 30)\n",
      "\n",
      "\n",
      "Rejected 42.61% of wrong ones\n",
      "Accepted 86.08% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.30123602,  0.69876398],\n",
       "       [ 0.8542823 ,  0.1457177 ],\n",
       "       [ 0.92327631,  0.07672369],\n",
       "       [ 0.86440089,  0.13559911],\n",
       "       [ 0.28126657,  0.71873343]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_svc_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.103)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]\n",
    "\n",
    "#(0.62077922077922076, 0.55417162908266104, 162, 385)\n",
    "#(42.077922077922075, '% - ', 21)\n",
    "#('Rejected good ones: ', 15, '/', 46)\n",
    "#('Accepted wrong ones:', 131, '/', 339)\n",
    "#('Rejected wrong ones: ', 208, '/', 339)\n",
    "#('Accepted good ones:', 31, '/', 46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-42.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/len(y_test)-sum(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.74333333333333329, 0.7152857142857143)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "model_svc_tfidf = Pipeline([\n",
    "    #('vect', CountVectorizer(ngram_range=(1,3), min_df=1, max_df=1.0,stop_words='english')),\n",
    "    ('vect', CountVectorizer(ngram_range=(3,6),analyzer='char_wb', min_df=10, max_df=0.95,stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    (\"svc\", SVC(kernel='linear', probability=True, class_weight = 'balanced'))\n",
    "    #(\"svc\", SVC(kernel='linear', probability=True))\n",
    "])\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_svc_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_svc_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.48166666666666669, 0.41097064560083896, 358, 600)\n",
      "(59.666666666666664, '% - ', 29)\n",
      "\n",
      "\n",
      "Rejected 43.38% of wrong ones\n",
      "Accepted 79.75% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.68131155,  0.31868845],\n",
       "       [ 0.85976017,  0.14023983],\n",
       "       [ 0.88189682,  0.11810318],\n",
       "       [ 0.87603488,  0.12396512],\n",
       "       [ 0.76284011,  0.23715989]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_svc_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.111)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]\n",
    "\n",
    "#(0.62077922077922076, 0.55417162908266104, 162, 385)\n",
    "#(42.077922077922075, '% - ', 21)\n",
    "#('Rejected good ones: ', 15, '/', 46)\n",
    "#('Accepted wrong ones:', 131, '/', 339)\n",
    "#('Rejected wrong ones: ', 208, '/', 339)\n",
    "#('Accepted good ones:', 31, '/', 46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count -> SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.875, 0.91084764139039731)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "model_svc = Pipeline([\n",
    "    #('vect', CountVectorizer()),\n",
    "    ('vect', CountVectorizer(ngram_range=(1,3), min_df=1, max_df=1.0,stop_words='english')),\n",
    "    #('vect', CountVectorizer(ngram_range=(3,6),analyzer='char_wb', min_df=10, max_df=0.95,stop_words='english')),\n",
    "    #('tfidf', TfidfTransformer()),\n",
    "    (\"svc\", SVC(kernel='linear', probability=True, class_weight = 'balanced'))\n",
    "    #(\"svc\", SVC(kernel='linear', probability=True))\n",
    "])\n",
    "\n",
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "model_svc.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model_svc.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.495, 0.42661286701648415, 360, 600)\n",
      "(60.0, '% - ', 30)\n",
      "\n",
      "\n",
      "Rejected 43.95% of wrong ones\n",
      "Accepted 86.08% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.75078058,  0.24921942],\n",
       "       [ 0.82732143,  0.17267857],\n",
       "       [ 0.89320943,  0.10679057],\n",
       "       [ 0.84763156,  0.15236844],\n",
       "       [ 0.57994713,  0.42005287]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba = model_svc.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.11)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]\n",
    "\n",
    "#(0.62077922077922076, 0.55417162908266104, 162, 385)\n",
    "#(42.077922077922075, '% - ', 21)\n",
    "#('Rejected good ones: ', 15, '/', 46)\n",
    "#('Accepted wrong ones:', 131, '/', 339)\n",
    "#('Rejected wrong ones: ', 208, '/', 339)\n",
    "#('Accepted good ones:', 31, '/', 46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Input, Embedding, SimpleRNN, Dense, Activation, TimeDistributed, Bidirectional, LSTM, GaussianNoise)\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model #Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASETS_DIR = '../ml-research/datasets/'\n",
    "GLOVE_DIR = DATASETS_DIR+'glove.6B/'\n",
    "WIKI_EN_DIR = DATASETS_DIR+'wiki.en/'\n",
    "#embeddings_file = os.path.join(GLOVE_DIR, 'glove.6B.300d.txt')\n",
    "embeddings_file = os.path.join(WIKI_EN_DIR, 'wiki.en.vec')\n",
    "\n",
    "# Word embeddings' constraints\n",
    "MAX_NB_WORDS = 20000  # Number of most common words for tokenizer\n",
    "EMBEDDING_DIM = 300   # Embeddings dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.preprocessing.text\n",
    "from string import maketrans\n",
    "\n",
    "def text_to_word_sequence(text,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True, split=\" \"):\n",
    "    if lower: text = text.lower()\n",
    "    if type(text) == unicode:\n",
    "        translate_table = {ord(c): ord(t) for c,t in zip(filters, split*len(filters)) }\n",
    "    else:\n",
    "        translate_table = maketrans(filters, split * len(filters))\n",
    "    text = text.translate(translate_table)\n",
    "    seq = text.split(split)\n",
    "    return [i for i in seq if i]\n",
    "    \n",
    "keras.preprocessing.text.text_to_word_sequence = text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25513 unique tokens.\n",
      "81\n",
      "25512\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing and creating word index\n",
    "\n",
    "additional_words = ['unk', 'num']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X+additional_words)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# inversing the word_index.\n",
    "index_word = dict((k,v) for v,k in word_index.items())\n",
    "\n",
    "# example\n",
    "print(word_index['adversarial'])\n",
    "print(word_index['unk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 2519371 word vectors.\n",
      "Creating Word Embeddings matrix...\n",
      "Word Embeddings matrix was successfuly created.\n"
     ]
    }
   ],
   "source": [
    "import models.embedding_matrix as embedding\n",
    "\n",
    "embedding_matrix = embedding.create_embedding_matrix(embeddings_file, MAX_NB_WORDS, EMBEDDING_DIM, word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Input, Embedding, SimpleRNN, Dense, Activation, TimeDistributed, Bidirectional,\n",
    "                          LSTM, GaussianNoise,Conv1D, MaxPooling1D, Flatten, Dropout)\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperparameters_cnn = {\n",
    "    'conv_units': 128,\n",
    "    'hidden_units_1': 128,\n",
    "    'hidden_units_2': 64,\n",
    "    'dropout': 0.5,\n",
    "    'pooling' : 5,\n",
    "    'gauss_stddev' : 0.0002,\n",
    "    'epochs' : 15,\n",
    "    'folds' : 5,\n",
    "    'nclasses' : 2,\n",
    "    'max_seq_len' : 300,\n",
    "    'learning_rate' : 0.001,\n",
    "    'stopwords' : True\n",
    "}\n",
    "\n",
    "hyperparameters_cnn = {\n",
    "    'conv_units': 64,\n",
    "    'hidden_units_1': 64,\n",
    "    'hidden_units_2': 32,\n",
    "    'dropout': 0.5,\n",
    "    'pooling' : 5,\n",
    "    'gauss_stddev' : 0.0002,\n",
    "    'epochs' : 15,\n",
    "    'folds' : 5,\n",
    "    'nclasses' : 2,\n",
    "    'max_seq_len' : 300,\n",
    "    'learning_rate' : 0.001,\n",
    "    'stopwords' : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncf8LRH9L4/MABTEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4732656bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "X_sequences_len = []\n",
    "for item in X_sequences:\n",
    "    X_sequences_len.append(\n",
    "                            min( len(item), 1000\n",
    "                               ))\n",
    "    \n",
    "X_sequences_padded = pad_sequences(X_sequences, maxlen=hyperparameters['max_seq_len'])\n",
    "\n",
    "plt.hist(X_sequences_len, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "\n",
    "y_num = label_encoder.transform(y)\n",
    "y_matrix = to_categorical(y_num,hyperparameters['nclasses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1920 samples, validate on 480 samples\n",
      "Epoch 1/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0997 - categorical_accuracy: 0.8036 - val_loss: 0.6698 - val_categorical_accuracy: 0.8687\n",
      "Epoch 2/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0844 - categorical_accuracy: 0.8474 - val_loss: 0.5261 - val_categorical_accuracy: 0.8687\n",
      "Epoch 3/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0726 - categorical_accuracy: 0.8479 - val_loss: 0.6188 - val_categorical_accuracy: 0.8687\n",
      "Epoch 4/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0609 - categorical_accuracy: 0.7995 - val_loss: 0.6361 - val_categorical_accuracy: 0.6875\n",
      "Epoch 5/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0492 - categorical_accuracy: 0.7458 - val_loss: 0.7183 - val_categorical_accuracy: 0.2979\n",
      "Epoch 6/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0401 - categorical_accuracy: 0.6740 - val_loss: 0.6277 - val_categorical_accuracy: 0.6083\n",
      "Epoch 7/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.9882 - categorical_accuracy: 0.7224 - val_loss: 0.6412 - val_categorical_accuracy: 0.5542\n",
      "Epoch 8/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.9200 - categorical_accuracy: 0.7089 - val_loss: 0.6480 - val_categorical_accuracy: 0.6229\n",
      "Epoch 9/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.8620 - categorical_accuracy: 0.7682 - val_loss: 0.5363 - val_categorical_accuracy: 0.7083\n",
      "Epoch 10/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.6864 - categorical_accuracy: 0.8120 - val_loss: 0.6030 - val_categorical_accuracy: 0.6583\n",
      "Epoch 11/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.4817 - categorical_accuracy: 0.8672 - val_loss: 0.5415 - val_categorical_accuracy: 0.7667\n",
      "Epoch 12/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.2852 - categorical_accuracy: 0.9349 - val_loss: 0.5971 - val_categorical_accuracy: 0.7667\n",
      "Epoch 13/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.1552 - categorical_accuracy: 0.9703 - val_loss: 0.6931 - val_categorical_accuracy: 0.7750\n",
      "Epoch 14/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.0645 - categorical_accuracy: 0.9875 - val_loss: 0.7497 - val_categorical_accuracy: 0.8417\n",
      "Epoch 15/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.0515 - categorical_accuracy: 0.9927 - val_loss: 0.8380 - val_categorical_accuracy: 0.8458\n",
      "(0.84583333333333333, 0.82442129629629646)\n",
      "[[395  22]\n",
      " [ 52  11]]\n",
      "Train on 1920 samples, validate on 480 samples\n",
      "Epoch 1/15\n",
      "1920/1920 [==============================] - 7s - loss: 1.0798 - categorical_accuracy: 0.8214 - val_loss: 0.6680 - val_categorical_accuracy: 0.8458\n",
      "Epoch 2/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0676 - categorical_accuracy: 0.8464 - val_loss: 0.6242 - val_categorical_accuracy: 0.8458\n",
      "Epoch 3/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0573 - categorical_accuracy: 0.8385 - val_loss: 0.5477 - val_categorical_accuracy: 0.8458\n",
      "Epoch 4/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0358 - categorical_accuracy: 0.8495 - val_loss: 0.5343 - val_categorical_accuracy: 0.8500\n",
      "Epoch 5/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0035 - categorical_accuracy: 0.8026 - val_loss: 0.4991 - val_categorical_accuracy: 0.8063\n",
      "Epoch 6/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.9756 - categorical_accuracy: 0.7474 - val_loss: 0.6362 - val_categorical_accuracy: 0.5667\n",
      "Epoch 7/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.8995 - categorical_accuracy: 0.7240 - val_loss: 0.8343 - val_categorical_accuracy: 0.3583\n",
      "Epoch 8/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.9029 - categorical_accuracy: 0.7219 - val_loss: 0.5233 - val_categorical_accuracy: 0.7188\n",
      "Epoch 9/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.7251 - categorical_accuracy: 0.7776 - val_loss: 0.5267 - val_categorical_accuracy: 0.7250\n",
      "Epoch 10/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.4841 - categorical_accuracy: 0.8667 - val_loss: 0.5671 - val_categorical_accuracy: 0.8000\n",
      "Epoch 11/15\n",
      "1920/1920 [==============================] - 8s - loss: 0.3214 - categorical_accuracy: 0.9208 - val_loss: 0.6256 - val_categorical_accuracy: 0.8083\n",
      "Epoch 12/15\n",
      "1920/1920 [==============================] - 8s - loss: 0.1649 - categorical_accuracy: 0.9620 - val_loss: 0.7221 - val_categorical_accuracy: 0.7875\n",
      "Epoch 13/15\n",
      "1920/1920 [==============================] - 8s - loss: 0.0945 - categorical_accuracy: 0.9792 - val_loss: 0.9176 - val_categorical_accuracy: 0.8104\n",
      "Epoch 14/15\n",
      "1920/1920 [==============================] - 8s - loss: 0.0597 - categorical_accuracy: 0.9896 - val_loss: 0.9899 - val_categorical_accuracy: 0.8187\n",
      "Epoch 15/15\n",
      "1920/1920 [==============================] - 8s - loss: 0.0291 - categorical_accuracy: 0.9943 - val_loss: 1.0611 - val_categorical_accuracy: 0.8292\n",
      "(0.82916666666666672, 0.8034777183600712)\n",
      "[[384  22]\n",
      " [ 60  14]]\n",
      "Train on 1920 samples, validate on 480 samples\n",
      "Epoch 1/15\n",
      "1920/1920 [==============================] - 8s - loss: 1.0769 - categorical_accuracy: 0.8167 - val_loss: 0.6507 - val_categorical_accuracy: 0.8438\n",
      "Epoch 2/15\n",
      "1920/1920 [==============================] - 8s - loss: 1.0616 - categorical_accuracy: 0.8594 - val_loss: 0.5529 - val_categorical_accuracy: 0.8438\n",
      "Epoch 3/15\n",
      "1920/1920 [==============================] - 8s - loss: 1.0525 - categorical_accuracy: 0.8599 - val_loss: 0.5487 - val_categorical_accuracy: 0.8438\n",
      "Epoch 4/15\n",
      "1920/1920 [==============================] - 8s - loss: 1.0284 - categorical_accuracy: 0.8365 - val_loss: 0.6318 - val_categorical_accuracy: 0.7417\n",
      "Epoch 5/15\n",
      "1920/1920 [==============================] - 8s - loss: 1.0216 - categorical_accuracy: 0.7839 - val_loss: 0.5961 - val_categorical_accuracy: 0.8354\n",
      "Epoch 6/15\n",
      "1920/1920 [==============================] - 8s - loss: 0.9822 - categorical_accuracy: 0.8396 - val_loss: 0.5737 - val_categorical_accuracy: 0.6958\n",
      "Epoch 7/15\n",
      "1920/1920 [==============================] - 8s - loss: 0.9176 - categorical_accuracy: 0.7870 - val_loss: 0.4357 - val_categorical_accuracy: 0.8417\n",
      "Epoch 8/15\n",
      "1920/1920 [==============================] - 7s - loss: 0.8307 - categorical_accuracy: 0.7792 - val_loss: 0.4617 - val_categorical_accuracy: 0.7687\n",
      "Epoch 9/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.6440 - categorical_accuracy: 0.8115 - val_loss: 0.6177 - val_categorical_accuracy: 0.6667\n",
      "Epoch 10/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.4949 - categorical_accuracy: 0.8599 - val_loss: 0.6198 - val_categorical_accuracy: 0.8146\n",
      "Epoch 11/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.3602 - categorical_accuracy: 0.9000 - val_loss: 0.6564 - val_categorical_accuracy: 0.7729\n",
      "Epoch 12/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.1813 - categorical_accuracy: 0.9589 - val_loss: 0.8079 - val_categorical_accuracy: 0.7771\n",
      "Epoch 13/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.1051 - categorical_accuracy: 0.9797 - val_loss: 0.9514 - val_categorical_accuracy: 0.8042\n",
      "Epoch 14/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.0577 - categorical_accuracy: 0.9875 - val_loss: 1.0137 - val_categorical_accuracy: 0.8000\n",
      "Epoch 15/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.0308 - categorical_accuracy: 0.9948 - val_loss: 1.0832 - val_categorical_accuracy: 0.8250\n",
      "(0.82499999999999996, 0.80459660211763762)\n",
      "[[379  26]\n",
      " [ 58  17]]\n",
      "Train on 1920 samples, validate on 480 samples\n",
      "Epoch 1/15\n",
      "1920/1920 [==============================] - 7s - loss: 1.0965 - categorical_accuracy: 0.7755 - val_loss: 0.6210 - val_categorical_accuracy: 0.8708\n",
      "Epoch 2/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0826 - categorical_accuracy: 0.8464 - val_loss: 0.5148 - val_categorical_accuracy: 0.8708\n",
      "Epoch 3/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0612 - categorical_accuracy: 0.8354 - val_loss: 0.6012 - val_categorical_accuracy: 0.7833\n",
      "Epoch 4/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0417 - categorical_accuracy: 0.7812 - val_loss: 0.5316 - val_categorical_accuracy: 0.7771\n",
      "Epoch 5/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0388 - categorical_accuracy: 0.7208 - val_loss: 0.4849 - val_categorical_accuracy: 0.8229\n",
      "Epoch 6/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0004 - categorical_accuracy: 0.7234 - val_loss: 0.5513 - val_categorical_accuracy: 0.6500\n",
      "Epoch 7/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.9411 - categorical_accuracy: 0.7240 - val_loss: 0.7836 - val_categorical_accuracy: 0.3854\n",
      "Epoch 8/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.8912 - categorical_accuracy: 0.7344 - val_loss: 0.5269 - val_categorical_accuracy: 0.7104\n",
      "Epoch 9/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.7597 - categorical_accuracy: 0.7870 - val_loss: 0.4790 - val_categorical_accuracy: 0.7854\n",
      "Epoch 10/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.6110 - categorical_accuracy: 0.8401 - val_loss: 0.5743 - val_categorical_accuracy: 0.7125\n",
      "Epoch 11/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.4385 - categorical_accuracy: 0.8948 - val_loss: 0.4880 - val_categorical_accuracy: 0.8333\n",
      "Epoch 12/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.2652 - categorical_accuracy: 0.9432 - val_loss: 0.8052 - val_categorical_accuracy: 0.6917\n",
      "Epoch 13/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.1957 - categorical_accuracy: 0.9661 - val_loss: 0.6584 - val_categorical_accuracy: 0.7812\n",
      "Epoch 14/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.1247 - categorical_accuracy: 0.9719 - val_loss: 0.7695 - val_categorical_accuracy: 0.8583\n",
      "Epoch 15/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.2376 - categorical_accuracy: 0.9427 - val_loss: 0.6335 - val_categorical_accuracy: 0.8646\n",
      "(0.86458333333333337, 0.83195164257924981)\n",
      "[[407  11]\n",
      " [ 54   8]]\n",
      "Train on 1920 samples, validate on 480 samples\n",
      "Epoch 1/15\n",
      "1920/1920 [==============================] - 7s - loss: 1.0831 - categorical_accuracy: 0.8380 - val_loss: 0.6526 - val_categorical_accuracy: 0.8562\n",
      "Epoch 2/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0600 - categorical_accuracy: 0.8573 - val_loss: 0.6161 - val_categorical_accuracy: 0.8562\n",
      "Epoch 3/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0372 - categorical_accuracy: 0.7984 - val_loss: 0.6260 - val_categorical_accuracy: 0.6667\n",
      "Epoch 4/15\n",
      "1920/1920 [==============================] - 6s - loss: 1.0204 - categorical_accuracy: 0.7417 - val_loss: 0.4847 - val_categorical_accuracy: 0.8479\n",
      "Epoch 5/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.9851 - categorical_accuracy: 0.7729 - val_loss: 0.6058 - val_categorical_accuracy: 0.6042\n",
      "Epoch 6/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.9049 - categorical_accuracy: 0.7115 - val_loss: 0.7097 - val_categorical_accuracy: 0.5333\n",
      "Epoch 7/15\n",
      "1920/1920 [==============================] - 7s - loss: 0.8490 - categorical_accuracy: 0.7286 - val_loss: 0.5068 - val_categorical_accuracy: 0.7417\n",
      "Epoch 8/15\n",
      "1920/1920 [==============================] - 7s - loss: 0.6819 - categorical_accuracy: 0.8448 - val_loss: 0.7034 - val_categorical_accuracy: 0.6250\n",
      "Epoch 9/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.4688 - categorical_accuracy: 0.8740 - val_loss: 0.5861 - val_categorical_accuracy: 0.8146\n",
      "Epoch 10/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.2515 - categorical_accuracy: 0.9469 - val_loss: 0.7112 - val_categorical_accuracy: 0.7604\n",
      "Epoch 11/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.1465 - categorical_accuracy: 0.9688 - val_loss: 0.7937 - val_categorical_accuracy: 0.7583\n",
      "Epoch 12/15\n",
      "1920/1920 [==============================] - 7s - loss: 0.2004 - categorical_accuracy: 0.9479 - val_loss: 0.8521 - val_categorical_accuracy: 0.8479\n",
      "Epoch 13/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.1844 - categorical_accuracy: 0.9677 - val_loss: 0.9921 - val_categorical_accuracy: 0.6396\n",
      "Epoch 14/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.1182 - categorical_accuracy: 0.9745 - val_loss: 0.8742 - val_categorical_accuracy: 0.8396\n",
      "Epoch 15/15\n",
      "1920/1920 [==============================] - 6s - loss: 0.0446 - categorical_accuracy: 0.9932 - val_loss: 0.9825 - val_categorical_accuracy: 0.8187\n",
      "(0.81874999999999998, 0.79165623477699354)\n",
      "[[385  26]\n",
      " [ 61   8]]\n",
      "(0.81874999999999998, 0.79165623477699354)\n",
      "[[ 1950.   107.]\n",
      " [  285.    58.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import utils.evaluation as evaluation\n",
    "from keras import metrics\n",
    "\n",
    "hyperparameters = hyperparameters_cnn\n",
    "class_weight = {0 : 1.,\n",
    "    1: (len(y)-sum(y))/sum(y)}\n",
    "\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "cm_summed = np.zeros((2,2))\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "for train_index, test_index in kf.split(X_sequences):\n",
    "    K.clear_session()\n",
    "    \n",
    "    X_sequences_padded = pad_sequences(X_sequences, maxlen=hyperparameters['max_seq_len'])\n",
    "\n",
    "    num_words = embedding_matrix.shape[0]\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=hyperparameters['max_seq_len'],\n",
    "                                trainable=False)\n",
    "\n",
    "    # train a 1D convnet with global maxpooling\n",
    "    sequence_input = Input(shape=(hyperparameters['max_seq_len'],), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    #x = GaussianNoise(hyperparameters['gauss_stddev'])(embedded_sequences)\n",
    "    x = embedded_sequences\n",
    "    x = Conv1D(2*hyperparameters['conv_units'], 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(hyperparameters['conv_units'], 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(hyperparameters['hidden_units_1'], activation='relu')(x)\n",
    "    x = Dropout(hyperparameters['dropout'])(x)\n",
    "    x = Dense(hyperparameters['hidden_units_2'], activation='relu')(x)\n",
    "    preds = Dense(hyperparameters['nclasses'], activation='softmax')(x)\n",
    "\n",
    "    model_cnn = Model(sequence_input, preds)\n",
    "    model_cnn.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    \n",
    "    X_train, X_test = X_sequences_padded[train_index], X_sequences_padded[test_index]\n",
    "    y_train, y_test = y_matrix[train_index], y_matrix[test_index]\n",
    "\n",
    "    # train\n",
    "    model_cnn.fit(X_train, y_train,\n",
    "              batch_size=128,\n",
    "              epochs=hyperparameters['epochs'],\n",
    "              class_weight=class_weight,\n",
    "              validation_data=(X_test, y_test))\n",
    "\n",
    "    # run\n",
    "    ground_truth = y_test.argmax(1)\n",
    "    predictions = [list(map(lambda x: x, model_cnn.predict_on_batch(np.asarray([x])).argmax(1)))[0] \\\n",
    "            for x in X_test]\n",
    "    \n",
    "    \n",
    "    acc = evaluation.accuracy(ground_truth, predictions)\n",
    "    f1 = evaluation.eval_f1_score(ground_truth, predictions)    \n",
    "    cm = evaluation.cm_matrix(ground_truth, predictions)\n",
    "    acc_list.append(acc)\n",
    "    f1_list.append(f1)\n",
    "    cm_summed = cm_summed + cm\n",
    "    print(acc, f1)\n",
    "    print(cm)\n",
    "    \n",
    "print(np.asarray(acc).mean(), np.asarray(f1).mean())\n",
    "print(cm_summed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth = y_test.argmax(1)\n",
    "predictions = [list(map(lambda x: x, model_cnn.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "        for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.57499999999999996, 0.51201967273395843, 225, 480)\n",
      "(46.875, '% - ', 23)\n",
      "\n",
      "\n",
      "Rejected 34.78% of wrong ones\n",
      "Accepted 43.80% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  9.99948740e-01,   5.12640072e-05],\n",
       "       [  9.98748541e-01,   1.25148252e-03],\n",
       "       [  9.99783337e-01,   2.16629545e-04],\n",
       "       [  9.99855757e-01,   1.44216669e-04],\n",
       "       [  9.94500160e-01,   5.49981929e-03]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba_np = np.asarray(predictions)\n",
    "#y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "\n",
    "acc = accuracy_score(y_predicted, ground_truth)\n",
    "f1 = f1_score(y_predicted, ground_truth, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test[:,0]) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test)[0])))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test[:,0]) & (y_predicted == True)])*100.0/sum(y_test)[0]))\n",
    "\n",
    "# 15% bezbolesnie odrzucic - 0.000008\n",
    "\n",
    "y_predicted_proba_np[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperparameters_lstm = {\n",
    "    'conv_units': 128,\n",
    "    'hidden_units_1': 128,\n",
    "    'hidden_units_2': 64,\n",
    "    'dropout': 0.5,\n",
    "    'pooling' : 5,\n",
    "    'gauss_stddev' : 0.0002,\n",
    "    'epochs' : 15,\n",
    "    'folds' : 5,\n",
    "    'nclasses' : 2,\n",
    "    'max_seq_len' : 300,\n",
    "    'learning_rate' : 0.001,\n",
    "    'stopwords' : True\n",
    "}\n",
    "\n",
    "hyperparameters_lstm = {\n",
    "    'conv_units': 64,\n",
    "    'hidden_units_1': 64,\n",
    "    'hidden_units_2': 0,\n",
    "    'dropout': 0.5,\n",
    "    'pooling' : 5,\n",
    "    'gauss_stddev' : 0.0002,\n",
    "    'epochs' : 2,\n",
    "    'folds' : 5,\n",
    "    'nclasses' : 2,\n",
    "    'max_seq_len' : 300,\n",
    "    'learning_rate' : 0.001,\n",
    "    'stopwords' : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW BEST: epoch 0 valid F1 =  0.801792364991 in 85.7773292065 (sec)                               \n",
      "(0.86458333333333337, 0.80179236499068907) 79.70 (sec)\n",
      "[[415   0]\n",
      " [ 65   0]]\n",
      "NEW BEST: epoch 0 valid F1 =  0.828613569322 in 85.8380498886 (sec)                               \n",
      "(0.8833333333333333, 0.82861356932153396)n 80.04 (sec)\n",
      "[[424   0]\n",
      " [ 56   0]]\n",
      "NEW BEST: epoch 0 valid F1 =  0.781081081081 in 85.3035309315 (sec)                               \n",
      "(0.84999999999999998, 0.7810810810810811)n 79.44 (sec)\n",
      "[[408   0]\n",
      " [ 72   0]]\n",
      "NEW BEST: epoch 0 valid F1 =  0.766369384673 in 86.890184164 (sec)                               \n",
      "(0.83958333333333335, 0.76636938467346172) 80.86 (sec)\n",
      "[[403   0]\n",
      " [ 77   0]]\n",
      "NEW BEST: epoch 0 valid F1 =  0.778133220594 in 85.2821340561 (sec)                               \n",
      "(0.84791666666666665, 0.77813322059376178) 79.51 (sec)\n",
      "[[407   0]\n",
      " [ 73   0]]\n",
      "(0.84791666666666665, 0.77813322059376178)\n",
      "[[ 2057.     0.]\n",
      " [  343.     0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import utils.evaluation as evaluation\n",
    "from keras import metrics\n",
    "import time\n",
    "import sys\n",
    "\n",
    "hyperparameters = hyperparameters_lstm\n",
    "class_weight = {0 : 1.,\n",
    "    1: (len(y)-sum(y))/sum(y)}\n",
    "\n",
    "acc_list = []\n",
    "f1_list = []\n",
    "cm_summed = np.zeros((2,2))\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "for train_index, test_index in kf.split(X_sequences):\n",
    "    K.clear_session()\n",
    "\n",
    "    num_words = embedding_matrix.shape[0]\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "    \n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(Embedding(num_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False))\n",
    "\n",
    "    model_lstm.add(Bidirectional(LSTM(hyperparameters['conv_units'], activation='sigmoid', return_sequences=False)))\n",
    "    model_lstm.add(GaussianNoise(hyperparameters['gauss_stddev']))\n",
    "    model_lstm.add(Dropout(hyperparameters['dropout']))\n",
    "    model_lstm.add(Dense(units=hyperparameters['hidden_units_1']))\n",
    "    if hyperparameters['hidden_units_2'] > 0:\n",
    "        model_lstm.add(Dense(units=hyperparameters['hidden_units_2']))\n",
    "    model_lstm.add(Dense(units=hyperparameters['nclasses']))\n",
    "    model_lstm.add(Activation(\"softmax\"))\n",
    "\n",
    "    optimizer = RMSprop(lr=hyperparameters['learning_rate'])\n",
    "    model_lstm.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  #optimizer='adadelta',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    #print(model.summary())\n",
    "    \n",
    "    X_train, X_test = np.asarray(X_sequences)[train_index], np.asarray(X_sequences)[test_index]\n",
    "    y_train, y_test = y_matrix[train_index], y_matrix[test_index]\n",
    "\n",
    "    # train\n",
    "    nsentences = len(X_train)\n",
    "    best_f1 = -np.inf\n",
    "    for epoch in xrange(hyperparameters['epochs']):\n",
    "        # Shuffle datasets\n",
    "        #shuffle([x_train, y_train], 42)\n",
    "        tic = time.time()\n",
    "        for i in xrange(nsentences):\n",
    "            X_lstm = np.asarray([X_train[i]])\n",
    "            Y_lstm = y_train[i].reshape((1,hyperparameters['nclasses']))\n",
    "            if X_lstm.shape[1] == 1:\n",
    "                continue # Bug with X, Y of len 1\n",
    "            model_lstm.train_on_batch(X_lstm, Y_lstm)\n",
    "            print '[learning] epoch %i >> %2.2f%%'%(epoch,(i+1)*100./nsentences)+\\\n",
    "                    ' completed in %.2f (sec)\\r'%(time.time()-tic),\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # Evaluation // back into the real world : idx -> words\n",
    "        predictions = [map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x])).argmax(1))[0] \\\n",
    "            for x in X_test]\n",
    "        ground_truth = y_test.argmax(1)\n",
    "\n",
    "        f1_valid = evaluation.eval_f1_score(ground_truth, predictions)\n",
    "\n",
    "        if f1_valid > best_f1:\n",
    "            best_f1 = f1_valid\n",
    "            model_lstm.save_weights('best_model_lstm.h5', overwrite=True)\n",
    "            print 'NEW BEST: epoch', epoch, 'valid F1 = ', f1_valid, 'in', str(time.time()-tic), '(sec)',' '*30\n",
    "\n",
    "        # load best performing model\n",
    "        model_lstm.load_weights('best_model_lstm.h5')\n",
    "\n",
    "    # eval\n",
    "    ground_truth = y_test.argmax(1)\n",
    "    predictions = [list(map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x])).argmax(1)))[0] \\\n",
    "            for x in X_test]\n",
    "    \n",
    "    \n",
    "    acc = evaluation.accuracy(ground_truth, predictions)\n",
    "    f1 = evaluation.eval_f1_score(ground_truth, predictions)    \n",
    "    cm = evaluation.cm_matrix(ground_truth, predictions)\n",
    "    acc_list.append(acc)\n",
    "    f1_list.append(f1)\n",
    "    cm_summed = cm_summed + cm\n",
    "    print(acc, f1)\n",
    "    print(cm)\n",
    "    \n",
    "    #break\n",
    "    #del model\n",
    "    #K.clear_session()\n",
    "    \n",
    "print(np.asarray(acc).mean(), np.asarray(f1).mean())\n",
    "print(cm_summed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth = y_test.argmax(1)\n",
    "predictions = [list(map(lambda x: x, model_lstm.predict_on_batch(np.asarray([x]))))[0] \\\n",
    "        for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.84791666666666665, 0.91770011273957153, 0, 480)\n",
      "(0.0, '% - ', 0)\n",
      "\n",
      "\n",
      "Rejected 100.00% of wrong ones\n",
      "Accepted 0.00% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.98317599,  0.01682401],\n",
       "       [ 0.97554839,  0.02445162],\n",
       "       [ 0.98144341,  0.01855657],\n",
       "       [ 0.98756611,  0.01243391],\n",
       "       [ 0.97481686,  0.02518315]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_proba_np = np.asarray(predictions)\n",
    "#y_predicted = (y_predicted_proba_np[:,1] > 0.001)\n",
    "y_predicted = (y_predicted_proba_np[:,1] > 0.4791)\n",
    "\n",
    "acc = accuracy_score(y_predicted, ground_truth)\n",
    "f1 = f1_score(y_predicted, ground_truth, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test[:,0]) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test)[0])))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test[:,0]) & (y_predicted == True)])*100.0/sum(y_test)[0]))\n",
    "\n",
    "# 15% bezbolesnie odrzucic - 0.000008\n",
    "\n",
    "y_predicted_proba_np[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extree - pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 1032578 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "import models.embedding_matrix as embedding\n",
    "reload(embedding)\n",
    "\n",
    "DATASETS_DIR = '../ml-research/datasets/'\n",
    "WIKI_DIR = DATASETS_DIR+'wiki.pl/'\n",
    "embeddings_file = WIKI_DIR+'wiki.pl.vec'\n",
    "\n",
    "word2vec_pretrained = embedding.create_embedding_dictionary(embeddings_file)\n",
    "\n",
    "# train word2vec on all the texts - both training and test set\n",
    "# we're not using test labels, just texts so this is fine\n",
    "model = Word2Vec(X, size=100, window=5, min_count=5, workers=2)\n",
    "w2v_custom = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "# and a tf-idf version of the same\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extra Trees classifier is almost universally great, let's stack it with our embeddings\n",
    "\n",
    "etree_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=50))])\n",
    "etree_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"svc\", ExtraTreesClassifier(n_estimators=50))])\n",
    "svc_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"extra trees\", SVC(kernel=\"linear\", probability=True))])\n",
    "svc_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"svc\", SVC(kernel=\"linear\", probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.56000000000000005, 0.49083845054647979, 299, 600)\n",
      "(49.833333333333336, '% - ', 24)\n",
      "\n",
      "\n",
      "Rejected 53.55% of wrong ones\n",
      "Accepted 72.15% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.78,  0.22],\n",
       "       [ 0.7 ,  0.3 ],\n",
       "       [ 0.84,  0.16],\n",
       "       [ 0.76,  0.24],\n",
       "       [ 0.84,  0.16]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "etree_w2v.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = etree_w2v.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = etree_w2v.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.47833333333333333, 0.40388570941762436, 344, 600)\n",
      "(57.333333333333336, '% - ', 28)\n",
      "\n",
      "\n",
      "Rejected 44.53% of wrong ones\n",
      "Accepted 69.62% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.72,  0.28],\n",
       "       [ 0.8 ,  0.2 ],\n",
       "       [ 0.84,  0.16],\n",
       "       [ 0.84,  0.16],\n",
       "       [ 0.88,  0.12]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "etree_w2v_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = etree_w2v_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = etree_w2v_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.137)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.35833333333333334, 0.30107561087142376, 432, 600)\n",
      "(72.0, '% - ', 36)\n",
      "\n",
      "\n",
      "Rejected 29.17% of wrong ones\n",
      "Accepted 79.75% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.81258768,  0.18741232],\n",
       "       [ 0.8378449 ,  0.1621551 ],\n",
       "       [ 0.849717  ,  0.150283  ],\n",
       "       [ 0.85650296,  0.14349704],\n",
       "       [ 0.82694067,  0.17305933]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "svc_w2v.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = svc_w2v.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = svc_w2v.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.135)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.34333333333333332, 0.29869747647305644, 457, 600)\n",
      "(76.166666666666671, '% - ', 38)\n",
      "\n",
      "\n",
      "Rejected 25.91% of wrong ones\n",
      "Accepted 89.87% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.84655737,  0.15344263],\n",
       "       [ 0.83030026,  0.16969974],\n",
       "       [ 0.84579444,  0.15420556],\n",
       "       [ 0.80816134,  0.19183866],\n",
       "       [ 0.84146273,  0.15853727]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "svc_w2v_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = svc_w2v_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = svc_w2v_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.145)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extree - Custom embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extra Trees classifier is almost universally great, let's stack it with our embeddings\n",
    "\n",
    "etree_w2v_custom = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v_custom)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=50))])\n",
    "etree_w2v_tfidf_custom = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v_custom)), \n",
    "                        (\"svc\", ExtraTreesClassifier(n_estimators=50))])\n",
    "svc_w2v_custom = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v_custom)), \n",
    "                        (\"extra trees\", SVC(kernel=\"linear\", probability=True))])\n",
    "svc_w2v_tfidf_custom = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v_custom)), \n",
    "                        (\"svc\", SVC(kernel=\"linear\", probability=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.58333333333333337, 0.51625128554446975, 275, 600)\n",
      "(45.833333333333336, '% - ', 22)\n",
      "\n",
      "\n",
      "Rejected 57.20% of wrong ones\n",
      "Accepted 65.82% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.84,  0.16],\n",
       "       [ 0.8 ,  0.2 ],\n",
       "       [ 0.8 ,  0.2 ],\n",
       "       [ 0.88,  0.12],\n",
       "       [ 0.84,  0.16]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "etree_w2v_custom.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = etree_w2v_custom.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = etree_w2v_custom.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.49166666666666664, 0.41747832092702014, 336, 600)\n",
      "(56.0, '% - ', 28)\n",
      "\n",
      "\n",
      "Rejected 46.07% of wrong ones\n",
      "Accepted 69.62% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.8 ,  0.2 ],\n",
       "       [ 0.78,  0.22],\n",
       "       [ 0.96,  0.04],\n",
       "       [ 0.8 ,  0.2 ],\n",
       "       [ 0.84,  0.16]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "etree_w2v_tfidf_custom.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = etree_w2v_tfidf_custom.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = etree_w2v_tfidf_custom.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.137)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.13333333333333333, 0.23265633653183243, 599, 600)\n",
      "(99.833333333333329, '% - ', 49)\n",
      "\n",
      "\n",
      "Rejected 0.19% of wrong ones\n",
      "Accepted 100.00% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.85100723,  0.14899277],\n",
       "       [ 0.85688099,  0.14311901],\n",
       "       [ 0.84538141,  0.15461859],\n",
       "       [ 0.848703  ,  0.151297  ],\n",
       "       [ 0.8506886 ,  0.1493114 ]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "svc_w2v_custom.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = svc_w2v_custom.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = svc_w2v_custom.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.135)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.51166666666666671, 0.4357106206676758, 306, 600)\n",
      "(51.0, '% - ', 25)\n",
      "\n",
      "\n",
      "Rejected 50.10% of wrong ones\n",
      "Accepted 58.23% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.83198247,  0.16801753],\n",
       "       [ 0.87050611,  0.12949389],\n",
       "       [ 0.85907289,  0.14092711],\n",
       "       [ 0.82871222,  0.17128778],\n",
       "       [ 0.82935238,  0.17064762]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "svc_w2v_tfidf_custom.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = svc_w2v_tfidf_custom.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = svc_w2v_tfidf_custom.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.145)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import types\n",
    "import tempfile\n",
    "import keras.models\n",
    "\n",
    "def make_keras_picklable():\n",
    "    def __getstate__(self):\n",
    "        model_str = \"\"\n",
    "        with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\n",
    "            keras.models.save_model(self, fd.name, overwrite=True)\n",
    "            model_str = fd.read()\n",
    "        d = { 'model_str': model_str }\n",
    "        return d\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\n",
    "            fd.write(state['model_str'])\n",
    "            fd.flush()\n",
    "            model = keras.models.load_model(fd.name)\n",
    "        self.__dict__ = model.__dict__\n",
    "\n",
    "\n",
    "    cls = keras.models.Model\n",
    "    cls.__getstate__ = __getstate__\n",
    "    cls.__setstate__ = __setstate__\n",
    "    \n",
    "\n",
    "make_keras_picklable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datetime_string():\n",
    "    localtime = time.localtime(time.time())\n",
    "    datetime_str = str(localtime.tm_year)\n",
    "    if(localtime.tm_mon < 10):\n",
    "        datetime_str += '0'\n",
    "    datetime_str += str(localtime.tm_mon)\n",
    "    if(localtime.tm_mday < 10):\n",
    "        datetime_str += '0'\n",
    "    datetime_str += str(localtime.tm_mday) + '_'\n",
    "    if(localtime.tm_hour < 10):\n",
    "        datetime_str += '0'\n",
    "    datetime_str += str(localtime.tm_hour)\n",
    "    if(localtime.tm_min < 10):\n",
    "        datetime_str += '0'\n",
    "    datetime_str += str(localtime.tm_min) + '_'\n",
    "    \n",
    "    return datetime_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "logistic_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"logistic\", LogisticRegression())])\n",
    "logistic_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"logistic\", LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.495, 0.42129813521602338, 336, 600)\n",
      "(56.0, '% - ', 28)\n",
      "\n",
      "\n",
      "Rejected 46.26% of wrong ones\n",
      "Accepted 70.89% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.83885018,  0.16114982],\n",
       "       [ 0.81801591,  0.18198409],\n",
       "       [ 0.85904388,  0.14095612],\n",
       "       [ 0.82870561,  0.17129439],\n",
       "       [ 0.85074211,  0.14925789]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "logistic_w2v.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = logistic_w2v.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = logistic_w2v.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.49333333333333335, 0.4195852956567242, 337, 600)\n",
      "(56.166666666666664, '% - ', 28)\n",
      "\n",
      "\n",
      "Rejected 46.07% of wrong ones\n",
      "Accepted 70.89% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.83842557,  0.16157443],\n",
       "       [ 0.81860965,  0.18139035],\n",
       "       [ 0.85839302,  0.14160698],\n",
       "       [ 0.8287643 ,  0.1712357 ],\n",
       "       [ 0.85128262,  0.14871738]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "logistic_w2v_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = logistic_w2v_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = logistic_w2v_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "sgd_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"logistic\", SGDClassifier(loss = 'log'))])\n",
    "sgd_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(word2vec_pretrained)), \n",
    "                        (\"logistic\", SGDClassifier(loss = 'log'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.87333333333333329, 0.9277272132291986, 3, 600)\n",
      "(0.5, '% - ', 0)\n",
      "\n",
      "\n",
      "Rejected 100.00% of wrong ones\n",
      "Accepted 3.80% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.99642056,  0.00357944],\n",
       "       [ 0.98454969,  0.01545031],\n",
       "       [ 0.99700074,  0.00299926],\n",
       "       [ 0.98753685,  0.01246315],\n",
       "       [ 0.99676083,  0.00323917]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "sgd_w2v.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = sgd_w2v.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = sgd_w2v.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.87333333333333329, 0.9277272132291986, 3, 600)\n",
      "(0.5, '% - ', 0)\n",
      "\n",
      "\n",
      "Rejected 100.00% of wrong ones\n",
      "Accepted 3.80% of good ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  9.97519168e-01,   2.48083187e-03],\n",
       "       [  9.92858107e-01,   7.14189321e-03],\n",
       "       [  9.99169359e-01,   8.30641396e-04],\n",
       "       [  9.97320416e-01,   2.67958437e-03],\n",
       "       [  9.98640203e-01,   1.35979702e-03]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:limit]\n",
    "X_test = X[limit:]\n",
    "\n",
    "y_train = y[:limit]\n",
    "y_test = y[limit:]\n",
    "\n",
    "sgd_w2v_tfidf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = sgd_w2v_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = sgd_w2v_tfidf.predict_proba(X_test)\n",
    "#y_predicted = (y_predicted_proba[:,1] > 0.12)\n",
    "y_predicted = (y_predicted_proba[:,1] > 0.14)\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import dill\n",
    "\n",
    "contents_dict = {\n",
    "    #'model_cnn_keras' : model_cnn,\n",
    "    #'model_lstm_keras' : model_lstm,\n",
    "    \n",
    "    'model_svc_sklearn' : model_svc,\n",
    "    'model_svc_tfidf_sklearn' : model_svc_tfidf,\n",
    "    \n",
    "    'model_nb_sklearn' : model_nb,\n",
    "    'model_nb_tfidf_sklearn' : model_nb_tfidf,\n",
    "    \n",
    "    'etree_w2v_sklearn' : etree_w2v,\n",
    "    'etree_w2v_tfidf_sklearn' : etree_w2v_tfidf,\n",
    "    \n",
    "    'svc_w2v_sklearn' : svc_w2v,\n",
    "    'svc_w2v_tfidf_sklearn' : svc_w2v_tfidf,\n",
    "    \n",
    "    'etree_w2v_custom_sklearn' : etree_w2v_custom,\n",
    "    'etree_w2v_tfidf_custom_sklearn' : etree_w2v_tfidf_custom,\n",
    "    \n",
    "    'svc_w2v_custom_sklearn' : svc_w2v_custom,\n",
    "    'svc_w2v_tfidf_custom_sklearn' : svc_w2v_tfidf_custom,\n",
    "    \n",
    "    'logistic_w2v_sklearn' : logistic_w2v,\n",
    "    'logistic_w2v_tfidf_sklearn' : logistic_w2v_tfidf,\n",
    "    \n",
    "    'sgd_w2v_sklearn' : sgd_w2v,\n",
    "    'sgd_w2v_tfidf_sklearn' : sgd_w2v_tfidf,\n",
    "    \n",
    "    'hyperparameters_cnn' : hyperparameters_cnn,\n",
    "    'hyperparameters_lstm' : hyperparameters_lstm,\n",
    "    \n",
    "    'label_encoder' : label_encoder,\n",
    "    'tokenizer' : tokenizer\n",
    "}\n",
    "   \n",
    "pickle.dump(contents_dict,open( \"models/\"+datetime_string()+\"ensemble_models.pickle\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contents_dict = pickle.load(open( \"models/20180125_1302_ensemble_models.pickle\", \"r\" ) )\n",
    "\n",
    "model_svc = contents_dict['model_svc_sklearn']\n",
    "model_svc_tfidf = contents_dict['model_svc_tfidf_sklearn']\n",
    "\n",
    "model_nb = contents_dict['model_nb_sklearn']\n",
    "model_nb_tfidf = contents_dict['model_nb_tfidf_sklearn']\n",
    "\n",
    "etree_w2v = contents_dict['etree_w2v_sklearn']\n",
    "etree_w2v_tfidf = contents_dict['etree_w2v_tfidf_sklearn']\n",
    "\n",
    "svc_w2v = contents_dict['svc_w2v_sklearn']\n",
    "svc_w2v_tfidf = contents_dict['svc_w2v_tfidf_sklearn']\n",
    "\n",
    "etree_w2v_custom = contents_dict['etree_w2v_custom_sklearn']\n",
    "etree_w2v_tfidf_custom = contents_dict['etree_w2v_tfidf_custom_sklearn']\n",
    "\n",
    "svc_w2v_custom = contents_dict['svc_w2v_custom_sklearn']\n",
    "svc_w2v_tfidf_custom = contents_dict['svc_w2v_tfidf_custom_sklearn']\n",
    "\n",
    "logistic_w2v = contents_dict['logistic_w2v_sklearn']\n",
    "logistic_w2v_tfidf = contents_dict['logistic_w2v_tfidf_sklearn']\n",
    "\n",
    "sgd_w2v = contents_dict['sgd_w2v_sklearn']\n",
    "sgd_w2v_tfidf = contents_dict['sgd_w2v_tfidf_sklearn']\n",
    "\n",
    "hyperparameters_cnn = contents_dict['hyperparameters_cnn']\n",
    "hyperparameters_lstm = contents_dict['hyperparameters_lstm']\n",
    "\n",
    "label_encoder = contents_dict['label_encoder']\n",
    "tokenize = contents_dict['tokenizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an ensembler - Without data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n"
     ]
    }
   ],
   "source": [
    "XE = X[limit:]\n",
    "yE = y[limit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24921942,  0.31868845,  0.17911359, ...,  0.16157443,\n",
       "         0.00357944,  0.00248083],\n",
       "       [ 0.17267857,  0.14023983,  0.10764107, ...,  0.18139035,\n",
       "         0.01545031,  0.00714189],\n",
       "       [ 0.10679057,  0.11810318,  0.16869529, ...,  0.14160698,\n",
       "         0.00299926,  0.00083064],\n",
       "       ..., \n",
       "       [ 0.13320165,  0.23170432,  0.16335656, ...,  0.15967144,\n",
       "         0.00474879,  0.00274219],\n",
       "       [ 0.4111404 ,  0.43813573,  0.20219167, ...,  0.20840976,\n",
       "         0.04140241,  0.0186639 ],\n",
       "       [ 0.09146169,  0.09744193,  0.12500477, ...,  0.17660965,\n",
       "         0.01188226,  0.00484308]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "XE_intermediate = np.column_stack((\n",
    "        model_svc.predict_proba(XE)[:,1],\n",
    "        model_svc_tfidf.predict_proba(XE)[:,1],\n",
    "    \n",
    "        model_nb.predict_proba(XE)[:,1],\n",
    "        model_nb_tfidf.predict_proba(XE)[:,1],\n",
    "\n",
    "        etree_w2v.predict_proba(XE)[:,1],\n",
    "        etree_w2v_tfidf.predict_proba(XE)[:,1],\n",
    "\n",
    "        svc_w2v.predict_proba(XE)[:,1],\n",
    "        svc_w2v_tfidf.predict_proba(XE)[:,1],\n",
    "\n",
    "        etree_w2v_custom.predict_proba(XE)[:,1],\n",
    "        etree_w2v_tfidf_custom.predict_proba(XE)[:,1],\n",
    "\n",
    "        svc_w2v_custom.predict_proba(XE)[:,1],\n",
    "        svc_w2v_tfidf_custom.predict_proba(XE)[:,1],\n",
    "\n",
    "        logistic_w2v.predict_proba(XE)[:,1],\n",
    "        logistic_w2v_tfidf.predict_proba(XE)[:,1],\n",
    "\n",
    "        sgd_w2v.predict_proba(XE)[:,1],\n",
    "        sgd_w2v_tfidf.predict_proba(XE)[:,1]\n",
    "    ))\n",
    "XE_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n"
     ]
    }
   ],
   "source": [
    "limitE = int(0.8*len(XE_intermediate))\n",
    "print(limitE)\n",
    "\n",
    "X_train = XE_intermediate[:limitE]\n",
    "X_test = XE_intermediate[limitE:]\n",
    "\n",
    "y_train = yE[:limitE]\n",
    "y_test = yE[limitE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.56874999999999998, 0.49909003372672772, 240, 480)\n",
      "(50.0, '% - ', 25)\n",
      "\n",
      "\n",
      "Rejected 53.94% of wrong ones\n",
      "Accepted 77.05% of good ones\n",
      "Median: 0.043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.94566751,  0.05433249],\n",
       "       [ 0.97973866,  0.02026134],\n",
       "       [ 0.95532316,  0.04467684],\n",
       "       [ 0.96781469,  0.03218531],\n",
       "       [ 0.96382419,  0.03617581]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "logi = LogisticRegression()\n",
    "logi.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = logi.predict(X_test)\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "y_predicted_proba = logi.predict_proba(X_test)\n",
    "y_predicted = (y_predicted_proba[:,1] > np.median(y_predicted_proba[:,1]))\n",
    "\n",
    "acc = accuracy_score(y_predicted, y_test)\n",
    "f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "print(\"\\n\")\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "print(\"Rejected %.2f%% of wrong ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))))\n",
    "print(\"Accepted %.2f%% of good ones\" % (len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)))\n",
    "print(\"Median: %.3f\" % np.median(y_predicted_proba[:,1]))\n",
    "\n",
    "y_predicted_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best ratings:\n",
    "Rejected 53.94% of wrong ones\n",
    "\n",
    "Accepted 77.05% of good ones\n",
    "\n",
    "Median: 0.043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True False  True  True  True  True  True  True  True  True\n",
      " False  True  True False]\n",
      "Rejected 53.94% of wrong ones\n",
      "Accepted 77.05% of good ones\n",
      "Median: 0.043\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "max_rej = 53.0\n",
    "max_acc = 75.0\n",
    "\n",
    "while(True):\n",
    "    rand_vect = np.random.rand(16) > 0.5\n",
    "    XE_intermediate = np.column_stack((\n",
    "            model_svc.predict_proba(XE)[:,1],\n",
    "            model_svc_tfidf.predict_proba(XE)[:,1],\n",
    "\n",
    "            model_nb.predict_proba(XE)[:,1],\n",
    "            model_nb_tfidf.predict_proba(XE)[:,1],\n",
    "\n",
    "            etree_w2v.predict_proba(XE)[:,1],\n",
    "            etree_w2v_tfidf.predict_proba(XE)[:,1],\n",
    "\n",
    "            svc_w2v.predict_proba(XE)[:,1],\n",
    "            svc_w2v_tfidf.predict_proba(XE)[:,1],\n",
    "\n",
    "            etree_w2v_custom.predict_proba(XE)[:,1],\n",
    "            etree_w2v_tfidf_custom.predict_proba(XE)[:,1],\n",
    "\n",
    "            svc_w2v_custom.predict_proba(XE)[:,1],\n",
    "            svc_w2v_tfidf_custom.predict_proba(XE)[:,1],\n",
    "\n",
    "            logistic_w2v.predict_proba(XE)[:,1],\n",
    "            logistic_w2v_tfidf.predict_proba(XE)[:,1],\n",
    "\n",
    "            sgd_w2v.predict_proba(XE)[:,1],\n",
    "            sgd_w2v_tfidf.predict_proba(XE)[:,1]\n",
    "        ))\n",
    "    XE_intermediate\n",
    "\n",
    "    limitE = int(0.8*len(XE_intermediate))\n",
    "    #print(limitE)\n",
    "\n",
    "    X_train = XE_intermediate[:limitE]\n",
    "    X_test = XE_intermediate[limitE:]\n",
    "\n",
    "    y_train = yE[:limitE]\n",
    "    y_test = yE[limitE:]\n",
    "\n",
    "    from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "    logi = LogisticRegression()\n",
    "    logi.fit(X_train, y_train)\n",
    "\n",
    "    y_predicted = logi.predict(X_test)\n",
    "    acc = accuracy_score(y_predicted, y_test)\n",
    "    f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "\n",
    "    y_predicted_proba = logi.predict_proba(X_test)\n",
    "    y_predicted = (y_predicted_proba[:,1] > np.median(y_predicted_proba[:,1]))\n",
    "\n",
    "    acc = accuracy_score(y_predicted, y_test)\n",
    "    f1 = f1_score(y_predicted, y_test, average='weighted')\n",
    "    #print(acc, f1, sum(y_predicted), len(y_predicted))\n",
    "    #print(sum(y_predicted)*100.0/len(y_predicted),\"% - \", int(sum(y_predicted)*50.0/len(y_predicted)))\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    rej = len(X_test_df[(y_predicted == y_test) & (y_predicted == False)])*100.0/(len(y_test)-sum(y_test))\n",
    "    acc = len(X_test_df[(y_predicted == y_test) & (y_predicted == True)])*100.0/sum(y_test)\n",
    "    \n",
    "    X_test_df = pd.DataFrame(X_test)\n",
    "    if (rej > max_rej and acc > max_acc):\n",
    "        max_rej = rej\n",
    "        max_acc = acc\n",
    "        print(rand_vect)\n",
    "        print(\"Rejected %.2f%% of wrong ones\" % rej)\n",
    "        print(\"Accepted %.2f%% of good ones\" % acc)\n",
    "        print(\"Median: %.3f\\n\\n\" % np.median(y_predicted_proba[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
