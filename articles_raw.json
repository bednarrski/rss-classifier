{"all_articles": [{"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166253", "id": "tag:google.com,2005:reader/item/00000003678215c9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multi-Layer Competitive-Cooperative Framework for Performance Enhancement of Differential Evolution. (arXiv:1801.10546v1 [cs.NE])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10546"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10546", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a733214a44db\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a733214a44db&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Differential Evolution (DE) is one of the most powerful optimizers in the \nevolutionary algorithm (EA) family. In recent years, many DE variants have been \nproposed to enhance performance. However, when compared with each other, \nsignificant differences in performances are seldomly observed. To meet this \nchallenge of a more significant improvement, this paper proposes a multi-layer \ncompetitive-cooperative (MLCC) framework to combine the advantages of multiple \nDEs. Existing multi-method strategies commonly use a multi-population based \nstructure, which classifies the entire population into several subpopulations \nand evolve individuals only in their corresponding subgroups. MLCC proposes to \nimplement a parallel structure with the entire population simultaneously \nmonitored by multiple DEs assigned in multiple layers. Each individual can \nstore, utilize and update its evolution information in different layers by \nusing a novel individual preference based layer selecting (IPLS) mechanism and \na computational resource allocation bias (RAB) mechanism. In IPLS, individuals \nonly connect to one favorite layer. While in RAB, high quality solutions are \nevolved by considering all the layers. In this way, the multiple layers work in \na competitive and cooperative manner. The proposed MLCC framework has been \nimplemented on several highly competitive DEs. Experimental studies show that \nMLCC variants significantly outperform the baseline DEs as well as several \nstate-of-the-art and up-to-date DEs on the CEC benchmark functions. \n</p>"}, "author": "Sheng Xin Zhang, Li Ming Zheng, Kit Sang Tang, Shao Yong Zheng, Wing Shing Chan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166252", "id": "tag:google.com,2005:reader/item/00000003678215d2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Reinforcement Learning for Programming Language Correction. (arXiv:1801.10467v1 [cs.AI])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10467"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10467", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Novice programmers often struggle with the formal syntax of programming \nlanguages. To assist them, we design a novel programming language correction \nframework amenable to reinforcement learning. The framework allows an agent to \nmimic human actions for text navigation and editing. We demonstrate that the \nagent can be trained through self-exploration directly from the raw input, that \nis, program text itself, without any knowledge of the formal syntax of the \nprogramming language. We leverage expert demonstrations for one tenth of the \ntraining data to accelerate training. The proposed technique is evaluated on \n6975 erroneous C programs with typographic errors, written by students during \nan introductory programming course. Our technique fixes 14% more programs and \n29% more compiler error messages relative to those fixed by a state-of-the-art \ntool, DeepFix, which uses a fully supervised neural machine translation \napproach. \n</p>"}, "author": "Rahul Gupta, Aditya Kanade, Shirish Shevade", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166251", "id": "tag:google.com,2005:reader/item/00000003678215dc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Fraternal Dropout. (arXiv:1711.00066v3 [stat.ML] UPDATED)", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00066"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00066", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent neural networks (RNNs) are important class of architectures among \nneural networks useful for language modeling and sequential prediction. \nHowever, optimizing RNNs is known to be harder compared to feed-forward neural \nnetworks. A number of techniques have been proposed in literature to address \nthis problem. In this paper we propose a simple technique called fraternal \ndropout that takes advantage of dropout to achieve this goal. Specifically, we \npropose to train two identical copies of an RNN (that share parameters) with \ndifferent dropout masks while minimizing the difference between their \n(pre-softmax) predictions. In this way our regularization encourages the \nrepresentations of RNNs to be invariant to dropout mask, thus being robust. We \nshow that our regularization term is upper bounded by the expectation-linear \ndropout objective which has been shown to address the gap due to the difference \nbetween the train and inference phases of dropout. We evaluate our model and \nachieve state-of-the-art results in sequence modeling tasks on two benchmark \ndatasets - Penn Treebank and Wikitext-2. We also show that our approach leads \nto performance improvement by a significant margin in image captioning \n(Microsoft COCO) and semi-supervised (CIFAR-10) tasks. \n</p>"}, "author": "Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166250", "id": "tag:google.com,2005:reader/item/00000003678215e9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Learning to Classify from Impure Samples. (arXiv:1801.10158v1 [hep-ph])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10158"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10158", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A persistent challenge in practical classification tasks is that labelled \ntraining sets are not always available. In particle physics, this challenge is \nsurmounted by the use of simulations. These simulations accurately reproduce \nmost features of data, but cannot be trusted to capture all of the complex \ncorrelations exploitable by modern machine learning methods. Recent work in \nweakly supervised learning has shown that simple, low-dimensional classifiers \ncan be trained using only the impure mixtures present in data. Here, we \ndemonstrate that complex, high-dimensional classifiers can also be trained on \nimpure mixtures using weak supervision techniques, with performance comparable \nto what could be achieved with pure samples. Using weak supervision will \ntherefore allow us to avoid relying exclusively on simulations for \nhigh-dimensional classification. This work opens the door to a new regime \nwhereby complex models are trained directly on data, providing direct access to \nprobe the underlying physics. \n</p>"}, "author": "Patrick T. Komiske, Eric M. Metodiev, Benjamin Nachman, Matthew D. Schwartz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166249", "id": "tag:google.com,2005:reader/item/00000003678215ef", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Kernel Distillation for Gaussian Processes. (arXiv:1801.10273v1 [stat.ML])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10273"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10273", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Gaussian processes (GPs) are flexible models that can capture complex \nstructure in large-scale dataset due to their non-parametric nature. However, \nthe usage of GPs in real-world application is limited due to their high \ncomputational cost at inference time. In this paper, we introduce a new \nframework, \\textit{kernel distillation}, for kernel matrix approximation. The \nidea adopts from knowledge distillation in deep learning community, where we \napproximate a fully trained teacher kernel matrix of size $n\\times n$ with a \nstudent kernel matrix. We combine inducing points method with sparse low-rank \napproximation in the distillation procedure. The distilled student kernel \nmatrix only cost $\\mathcal{O}(m^2)$ storage where $m$ is the number of inducing \npoints and $m \\ll n$. We also show that one application of kernel distillation \nis for fast GP prediction, where we demonstrate empirically that our \napproximation provide better balance between the prediction time and the \npredictive performance compared to the alternatives. \n</p>"}, "author": "Congzheng Song, Yiming Sun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166248", "id": "tag:google.com,2005:reader/item/00000003678215f4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Multi-view Learning to Rank. (arXiv:1801.10402v1 [cs.LG])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10402"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10402", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of learning to rank from multiple sources. Though \nmulti-view learning and learning to rank have been studied extensively leading \nto a wide range of applications, multi-view learning to rank as a synergy of \nboth topics has received little attention. The aim of the paper is to propose a \ncomposite ranking method while keeping a close correlation with the individual \nrankings simultaneously. We propose a multi-objective solution to ranking by \ncapturing the information of the feature mapping from both within each view as \nwell as across views using autoencoder-like networks. Moreover, a novel \nend-to-end solution is introduced to enhance the joint ranking with minimum \nview-specific ranking loss, so that we can achieve the maximum global view \nagreements within a single optimization process. The proposed method is \nvalidated on a wide variety of ranking problems, including university ranking, \nmulti-view lingual text ranking and image data ranking, providing superior \nresults. \n</p>"}, "author": "Guanqun Cao, Alexandros Iosifidis, Moncef Gabbouj, Vijay Raghavan, Raju Gottumukkala", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166247", "id": "tag:google.com,2005:reader/item/00000003678215fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Model compression for faster structural separation of macromolecules captured by Cellular Electron Cryo-Tomography. (arXiv:1801.10597v1 [q-bio.QM])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10597"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10597", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Electron Cryo-Tomography (ECT) enables 3D visualization of macromolecule \nstructure inside single cells. Macromolecule classification approaches based on \nconvolutional neural networks (CNN) were developed to separate millions of \nmacromolecules captured from ECT systematically. However, given the fast \naccumulation of ECT data, it will soon become necessary to use CNN models to \nefficiently and accurately separate substantially more macromolecules at the \nprediction stage, which requires additional computational costs. To speed up \nthe prediction, we compress classification models into compact neural networks \nwith little in accuracy for deployment. Specifically, we propose to perform \nmodel compression through knowledge distillation. Firstly, a complex teacher \nnetwork is trained to generate soft labels with better classification \nfeasibility followed by training of customized student networks with simple \narchitectures using the soft label to compress model complexity. Our tests \ndemonstrate that our compressed models significantly reduce the number of \nparameters and time cost while maintaining similar classification accuracy. \n</p>"}, "author": "Jialiang Guo, Bo Zhou, Xiangrui Zeng, Zachary Freyberg, Min Xu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166246", "id": "tag:google.com,2005:reader/item/0000000367821606", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Attacking Binarized Neural Networks. (arXiv:1711.00449v2 [cs.LG] UPDATED)", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00449"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00449", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks with low-precision weights and activations offer compelling \nefficiency advantages over their full-precision equivalents. The two most \nfrequently discussed benefits of quantization are reduced memory consumption, \nand a faster forward pass when implemented with efficient bitwise operations. \nWe propose a third benefit of very low-precision neural networks: improved \nrobustness against some adversarial attacks, and in the worst case, performance \nthat is on par with full-precision models. We focus on the very low-precision \ncase where weights and activations are both quantized to $\\pm$1, and note that \nstochastically quantizing weights in just one layer can sharply reduce the \nimpact of iterative attacks. We observe that non-scaled binary neural networks \nexhibit a similar effect to the original defensive distillation procedure that \nled to gradient masking, and a false notion of security. We address this by \nconducting both black-box and white-box experiments with binary models that do \nnot artificially mask gradients. \n</p>"}, "author": "Angus Galloway, Graham W. Taylor, Medhat Moussa", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893436", "id": "tag:google.com,2005:reader/item/0000000366c10d01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "ReNN: Rule-embedded Neural Networks. (arXiv:1801.09856v1 [cs.LG])", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09856"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09856", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The artificial neural network shows powerful ability of inference, but it is \nstill criticized for lack of interpretability and prerequisite needs of big \ndataset. This paper proposes the Rule-embedded Neural Network (ReNN) to \novercome the shortages. ReNN first makes local-based inferences to detect local \npatterns, and then uses rules based on domain knowledge about the local \npatterns to generate rule-modulated map. After that, ReNN makes global-based \ninferences that synthesizes the local patterns and the rule-modulated map. To \nsolve the optimization problem caused by rules, we use a two-stage optimization \nstrategy to train the ReNN model. By introducing rules into ReNN, we can \nstrengthen traditional neural networks with long-term dependencies which are \ndifficult to learn with limited empirical dataset, thus improving inference \naccuracy. The complexity of neural networks can be reduced since long-term \ndependencies are not modeled with neural connections, and thus the amount of \ndata needed to optimize the neural networks can be reduced. Besides, inferences \nfrom ReNN can be analyzed with both local patterns and rules, and thus have \nbetter interpretability. In this paper, ReNN has been validated with a \ntime-series detection problem. \n</p>"}, "author": "Hu Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893433", "id": "tag:google.com,2005:reader/item/0000000366c10d0c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Over-representation of Extreme Events in Decision-Making: A Rational Metacognitive Account. (arXiv:1801.09848v1 [q-bio.NC])", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09848"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09848", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a733214a4849\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a733214a4849&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The Availability bias, manifested in the over-representation of extreme \neventualities in decision-making, is a well-known cognitive bias, and is \ngenerally taken as evidence of human irrationality. In this work, we present \nthe first rational, metacognitive account of the Availability bias, formally \narticulated at Marr's algorithmic level of analysis. Concretely, we present a \nnormative, metacognitive model of how a cognitive system should over-represent \nextreme eventualities, depending on the amount of time available at its \ndisposal for decision-making. Our model also accounts for two well-known \nframing effects in human decision-making under risk---the fourfold pattern of \nrisk preferences in outcome probability (Tversky &amp; Kahneman, 1992) and in \noutcome magnitude (Markovitz, 1952)---thereby providing the first \nmetacognitively-rational basis for those effects. Empirical evidence, \nfurthermore, confirms an important prediction of our model. Surprisingly, our \nmodel is unimaginably robust with respect to its focal parameter. We discuss \nthe implications of our work for studies on human decision-making, and conclude \nby presenting a counterintuitive prediction of our model, which, if confirmed, \nwould have intriguing implications for human decision-making under risk. To our \nknowledge, our model is the first metacognitive, resource-rational process \nmodel of cognitive biases in decision-making. \n</p>"}, "author": "Ardavan S. Nobandegani, Kevin da Silva Castanheira, A. Ross Otto, Thomas R. Shultz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893432", "id": "tag:google.com,2005:reader/item/0000000366c10d15", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimal Number of Choices in Rating Contexts. (arXiv:1605.06588v6 [cs.AI] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1605.06588"}], "alternate": [{"href": "http://arxiv.org/abs/1605.06588", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In many settings people must give numerical scores to entities from a small \ndiscrete set. For instance, rating physical attractiveness from 1--5 on dating \nsites, or papers from 1--10 for conference reviewing. We study the problem of \nunderstanding when using a different number of options is optimal. For \nconcreteness we assume the true underlying scores are integers from 1--100. We \nconsider the case when scores are uniform random and Gaussian. We study when \nusing 2, 3, 4, 5, and 10 options is optimal in these models. One may expect \nthat using more options would always improve performance in this model, but we \nshow that this is not necessarily the case, and that using fewer choices---even \njust two---can surprisingly be optimal in certain situations. While in theory \nfor this setting it would be optimal to use all 100 options, in practice this \nis prohibitive, and it is preferable to utilize a smaller number of options due \nto humans' limited computational resources. Our results suggest that using a \nsmaller number of options than is typical could be optimal in certain \nsituations. This would have many potential applications, as settings requiring \nentities to be ranked by humans are ubiquitous. \n</p>"}, "author": "Sam Ganzfried, Farzana Yusuf", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893431", "id": "tag:google.com,2005:reader/item/0000000366c10d1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Towards a Quantum World Wide Web. (arXiv:1703.06642v2 [cs.AI] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.06642"}], "alternate": [{"href": "http://arxiv.org/abs/1703.06642", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We elaborate a quantum model for the meaning associated with corpora of \nwritten documents, like the pages forming the World Wide Web. To that end, we \nare guided by how physicists constructed quantum theory for microscopic \nentities, which unlike classical objects cannot be fully represented in our \nspatial theater. We suggest that a similar construction needs to be carried out \nby linguists and computational scientists, to capture the full meaning carried \nby collections of documental entities. More precisely, we show how to associate \na quantum-like 'entity of meaning' to a 'language entity formed by printed \ndocuments', considering the latter as the collection of traces that are left by \nthe former, in specific results of search actions that we describe as \nmeasurements. In other words, we offer a perspective where a collection of \ndocuments, like the Web, is described as the space of manifestation of a more \ncomplex entity - the QWeb - which is the object of our modeling, drawing its \ninspiration from previous studies on operational-realistic approaches to \nquantum physics and quantum modeling of human cognition and decision-making. We \nemphasize that a consistent QWeb model needs to account for the observed \ncorrelations between words appearing in printed documents, e.g., \nco-occurrences, as the latter would depend on the 'meaning connections' \nexisting between the concepts that are associated with these words. In that \nrespect, we show that both 'context and interference (quantum) effects' are \nrequired to explain the probabilities calculated by counting the relative \nnumber of documents containing certain words and co-ocurrrences of words. \n</p>"}, "author": "Diederik Aerts, Jonito Aerts Arguelles, Lester Beltran, Lyneth Beltran, Isaac Distrito, Massimiliano Sassoli de Bianchi, Sandro Sozzo, Tomas Veloz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893430", "id": "tag:google.com,2005:reader/item/0000000366c10d22", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Towards Synthesizing Complex Programs from Input-Output Examples. (arXiv:1706.01284v2 [cs.LG] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.01284"}], "alternate": [{"href": "http://arxiv.org/abs/1706.01284", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In recent years, deep learning techniques have been developed to improve the \nperformance of program synthesis from input-output examples. Albeit its \nsignificant progress, the programs that can be synthesized by state-of-the-art \napproaches are still simple in terms of their complexity. In this work, we move \na significant step forward along this direction by proposing a new class of \nchallenging tasks in the domain of program synthesis from input-output \nexamples: learning a context-free parser from pairs of input programs and their \nparse trees. We show that this class of tasks are much more challenging than \npreviously studied tasks, and the test accuracy of existing approaches is \nalmost 0%. \n</p> \n<p>We tackle the challenges by developing three novel techniques inspired by \nthree novel observations, which reveal the key ingredients of using deep \nlearning to synthesize a complex program. First, the use of a \nnon-differentiable machine is the key to effectively restrict the search space. \nThus our proposed approach learns a neural program operating a domain-specific \nnon-differentiable machine. Second, recursion is the key to achieve \ngeneralizability. Thus, we bake-in the notion of recursion in the design of our \nnon-differentiable machine. Third, reinforcement learning is the key to learn \nhow to operate the non-differentiable machine, but it is also hard to train the \nmodel effectively with existing reinforcement learning algorithms from a cold \nboot. We develop a novel two-phase reinforcement learning-based search \nalgorithm to overcome this issue. In our evaluation, we show that using our \nnovel approach, neural parsing programs can be learned to achieve 100% test \naccuracy on test inputs that are 500x longer than the training samples. \n</p>"}, "author": "Xinyun Chen, Chang Liu, Dawn Song", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893429", "id": "tag:google.com,2005:reader/item/0000000366c10d2c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Bayesian Neural Networks. (arXiv:1801.07710v2 [cs.LG] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.07710"}], "alternate": [{"href": "http://arxiv.org/abs/1801.07710", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper describes and discusses Bayesian Neural Network (BNN). The paper \nshowcases a few different applications of them for classification and \nregression problems. BNNs are comprised of a Probabilistic Model and a Neural \nNetwork. The intent of such a design is to combine the strengths of Neural \nNetworks and Stochastic modeling. Neural Networks exhibit continuous function \napproximator capabilities. Stochastic models allow direct specification of a \nmodel with known interaction between parameters to generate data. During the \nprediction phase, stochastic models generate a complete posterior distribution \nand produce probabilistic guarantees on the predictions. Thus BNNs are a unique \ncombination of neural network and stochastic models with the stochastic model \nforming the core of this integration. BNNs can then produce probabilistic \nguarantees on it's predictions and also generate the distribution of parameters \nthat it has learnt from the observations. That means, in the parameter space, \none can deduce the nature and shape of the neural network's learnt parameters. \nThese two characteristics makes them highly attractive to theoreticians as well \nas practitioners. Recently there has been a lot of activity in this area, with \nthe advent of numerous probabilistic programming libraries such as: PyMC3, \nEdward, Stan etc. Further this area is rapidly gaining ground as a standard \nmachine learning approach for numerous problems \n</p>"}, "author": "Vikram Mullachery, Aniruddh Khera, Amir Husain", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893428", "id": "tag:google.com,2005:reader/item/0000000366c10d3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Cross-type Biomedical Named Entity Recognition with Deep Multi-Task Learning. (arXiv:1801.09851v1 [cs.IR])", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09851"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09851", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Motivation: Biomedical named entity recognition (BioNER) is the most \nfundamental task in biomedical text mining. State-of-the-art BioNER systems \noften require handcrafted features specifically designed for each type of \nbiomedical entities. This feature generation process requires intensive labors \nfrom biomedical and linguistic experts, and makes it difficult to adapt these \nsystems to new biomedical entity types. Although recent studies explored using \nneural network models for BioNER to free experts from manual feature \ngeneration, these models still require substantial human efforts to annotate \nmassive training data. \n</p> \n<p>Results: We propose a multi-task learning framework for BioNER that is based \non neural network models to save human efforts. We build a global model by \ncollectively training multiple models that share parameters, each model \ncapturing the characteristics of a different biomedical entity type. In \nexperiments on five BioNER benchmark datasets covering four major biomedical \nentity types, our model outperforms state-of-the-art systems and other neural \nnetwork models by a large margin, even when only limited training data are \navailable. Further analysis shows that the large performance gains come from \nsharing character- and word-level information between different biomedical \nentities. The approach creates new opportunities for text-mining approaches to \nhelp biomedical scientists better exploit knowledge in biomedical literature. \n</p>"}, "author": "Xuan Wang, Yu Zhang, Xiang Ren, Yuhao Zhang, Marinka Zitnik, Jingbo Shang, Curtis Langlotz, Jiawei Han", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893427", "id": "tag:google.com,2005:reader/item/0000000366c10d4b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Cardiac Arrhythmia Detection from ECG Combining Convolutional and Long Short-Term Memory Networks. (arXiv:1801.10033v1 [eess.SP])", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10033"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10033", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Objectives: Atrial fibrillation (AF) is a common heart rhythm disorder \nassociated with deadly and debilitating consequences including heart failure, \nstroke, poor mental health, reduced quality of life and death. Having an \nautomatic system that diagnoses various types of cardiac arrhythmias would \nassist cardiologists to initiate appropriate preventive measures and to improve \nthe analysis of cardiac disease. To this end, this paper introduces a new \napproach to detect and classify automatically cardiac arrhythmias in \nelectrocardiograms (ECG) recordings. \n</p> \n<p>Methods: The proposed approach used a combination of Convolution Neural \nNetworks (CNNs) and a sequence of Long Short-Term Memory (LSTM) units, with \npooling, dropout and normalization techniques to improve their accuracy. The \nnetwork predicted a classification at every 18th input sample and we selected \nthe final prediction for classification. Results were cross-validated on the \nPhysionet Challenge 2017 training dataset, which contains 8,528 single lead ECG \nrecordings lasting from 9s to just over 60s. \n</p> \n<p>Results: Using the proposed structure and no explicit feature selection, \n10-fold stratified cross-validation gave an overall F-measure of 0.83.10-0.015 \non the held-out test data (mean-standard deviation over all folds) and 0.80 on \nthe hidden dataset of the Challenge entry server. \n</p>"}, "author": "Philip Warrick (1), Masun Nabhan Homsi (2) ((1) PeriGen. Inc., Montreal, Canada, (2) Simon Bolivar University, Caracas, Venezuela)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893426", "id": "tag:google.com,2005:reader/item/0000000366c10d54", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian Nonparametric Kernel-Learning. (arXiv:1506.08776v2 [stat.ML] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1506.08776"}], "alternate": [{"href": "http://arxiv.org/abs/1506.08776", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Kernel methods are ubiquitous tools in machine learning. However, there is \noften little reason for the common practice of selecting a kernel a priori. \nEven if a universal approximating kernel is selected, the quality of the finite \nsample estimator may be greatly affected by the choice of kernel. Furthermore, \nwhen directly applying kernel methods, one typically needs to compute a $N \n\\times N$ Gram matrix of pairwise kernel evaluations to work with a dataset of \n$N$ instances. The computation of this Gram matrix precludes the direct \napplication of kernel methods on large datasets, and makes kernel learning \nespecially difficult. In this paper we introduce Bayesian nonparmetric \nkernel-learning (BaNK), a generic, data-driven framework for scalable learning \nof kernels. BaNK places a nonparametric prior on the spectral distribution of \nrandom frequencies allowing it to both learn kernels and scale to large \ndatasets. We show that this framework can be used for large scale regression \nand classification tasks. Furthermore, we show that BaNK outperforms several \nother scalable approaches for kernel learning on a variety of real world \ndatasets. \n</p>"}, "author": "Junier Oliva, Avinava Dubey, Andrew G. Wilson, Barnabas Poczos, Jeff Schneider, Eric P. Xing", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893425", "id": "tag:google.com,2005:reader/item/0000000366c10d5b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Ensemble Adversarial Training: Attacks and Defenses. (arXiv:1705.07204v3 [stat.ML] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.07204"}], "alternate": [{"href": "http://arxiv.org/abs/1705.07204", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Adversarial examples are perturbed inputs designed to fool machine learning \nmodels. Adversarial training injects such examples into training data to \nincrease robustness. To scale this technique to large datasets, perturbations \nare crafted using fast single-step methods that maximize a linear approximation \nof the model's loss. We show that this form of adversarial training converges \nto a degenerate global minimum, wherein small curvature artifacts near the data \npoints obfuscate a linear approximation of the loss. The model thus learns to \ngenerate weak perturbations, rather than defend against strong ones. As a \nresult, we find that adversarial training remains vulnerable to black-box \nattacks, where we transfer perturbations computed on undefended models, as well \nas to a powerful novel single-step attack that escapes the non-smooth vicinity \nof the input data via a small random step. We further introduce Ensemble \nAdversarial Training, a technique that augments training data with \nperturbations transferred from other models. On ImageNet, Ensemble Adversarial \nTraining yields models with strong robustness to black-box attacks. In \nparticular, our most robust model won the first round of the NIPS 2017 \ncompetition on Defenses against Adversarial Attacks. \n</p>"}, "author": "Florian Tram&#xe8;r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick McDaniel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893424", "id": "tag:google.com,2005:reader/item/0000000366c10d5e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "SEARNN: Training RNNs with Global-Local Losses. (arXiv:1706.04499v2 [cs.LG] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.04499"}], "alternate": [{"href": "http://arxiv.org/abs/1706.04499", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose SEARNN, a novel training algorithm for recurrent neural networks \n(RNNs) inspired by the \"learning to search\" (L2S) approach to structured \nprediction. RNNs have been widely successful in structured prediction \napplications such as machine translation or parsing, and are commonly trained \nusing maximum likelihood estimation (MLE). Unfortunately, this training loss is \nnot always an appropriate surrogate for the test error: by only maximizing the \nground truth probability, it fails to exploit the wealth of information offered \nby structured losses. Further, it introduces discrepancies between training and \npredicting (such as exposure bias) that may hurt test performance. Instead, \nSEARNN leverages test-alike search space exploration to introduce global-local \nlosses that are closer to the test error. We first demonstrate improved \nperformance over MLE on two different tasks: OCR and spelling correction. Then, \nwe propose a subsampling strategy to enable SEARNN to scale to large vocabulary \nsizes. This allows us to validate the benefits of our approach on a machine \ntranslation task. \n</p>"}, "author": "R&#xe9;mi Leblond, Jean-Baptiste Alayrac, Anton Osokin, Simon Lacoste-Julien", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893423", "id": "tag:google.com,2005:reader/item/0000000366c10d62", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards Neural Phrase-based Machine Translation. (arXiv:1706.05565v5 [cs.CL] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.05565"}], "alternate": [{"href": "http://arxiv.org/abs/1706.05565", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a733214a4c10\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a733214a4c10&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our \nmethod explicitly models the phrase structures in output sequences using \nSleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence \nmodeling method. To mitigate the monotonic alignment requirement of SWAN, we \nintroduce a new layer to perform (soft) local reordering of input sequences. \nDifferent from existing neural machine translation (NMT) approaches, NPMT does \nnot use attention-based decoding mechanisms. Instead, it directly outputs \nphrases in a sequential order and can decode in linear time. Our experiments \nshow that NPMT achieves superior performances on IWSLT 2014 \nGerman-English/English-German and IWSLT 2015 English-Vietnamese machine \ntranslation tasks compared with strong NMT baselines. We also observe that our \nmethod produces meaningful phrases in output languages. \n</p>"}, "author": "Po-Sen Huang, Chong Wang, Sitao Huang, Dengyong Zhou, Li Deng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}]}