{"all_articles": [{"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Philip Bontrager, Wending Lin, Julian Togelius, Sebastian Risi", "title": "Deep Interactive Evolution. (arXiv:1801.08230v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.08230", "type": "text/html"}], "timestampUsec": "1516954931974852", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328e1288e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328e1288e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper describes an approach that combines generative adversarial \nnetworks (GANs) with interactive evolutionary computation (IEC). While GANs can \nbe trained to produce lifelike images, they are normally sampled randomly from \nthe learned distribution, providing limited control over the resulting output. \nOn the other hand, interactive evolution has shown promise in creating various \nartifacts such as images, music and 3D objects, but traditionally relies on a \nhand-designed evolvable representation of the target domain. The main insight \nin this paper is that a GAN trained on a specific target domain can act as a \ncompact and robust genotype-to-phenotype mapping (i.e. most produced phenotypes \ndo resemble valid domain artifacts). Once such a GAN is trained, the latent \nvector given as input to the GAN's generator network can be put under \nevolutionary control, allowing controllable and high-quality image generation. \nIn this paper, we demonstrate the advantage of this novel approach through a \nuser study in which participants were able to evolve images that strongly \nresemble specific target images. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954931975", "annotations": [], "published": 1516954932, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2efd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08230"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Richard Evans, Edward Grefenstette", "title": "Learning Explanatory Rules from Noisy Data. (arXiv:1711.04574v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.04574", "type": "text/html"}], "timestampUsec": "1516954931974851", "comments": [], "summary": {"content": "<p>Artificial Neural Networks are powerful function approximators capable of \nmodelling solutions to a wide variety of problems, both supervised and \nunsupervised. As their size and expressivity increases, so too does the \nvariance of the model, yielding a nearly ubiquitous overfitting problem. \nAlthough mitigated by a variety of model regularisation methods, the common \ncure is to seek large amounts of training data---which is not necessarily \neasily obtained---that sufficiently approximates the data distribution of the \ndomain we wish to test on. In contrast, logic programming methods such as \nInductive Logic Programming offer an extremely data-efficient process by which \nmodels can be trained to reason on symbolic domains. However, these methods are \nunable to deal with the variety of domains neural networks can be applied to: \nthey are not robust to noise in or mislabelling of inputs, and perhaps more \nimportantly, cannot be applied to non-symbolic domains where the data is \nambiguous, such as operating on raw pixels. In this paper, we propose a \nDifferentiable Inductive Logic framework, which can not only solve tasks which \ntraditional ILP systems are suited for, but shows a robustness to noise and \nerror in the training data which ILP cannot cope with. Furthermore, as it is \ntrained by backpropagation against a likelihood objective, it can be hybridised \nby connecting it with neural networks over ambiguous data in order to be \napplied to domains which ILP cannot address, while providing data efficiency \nand generalisation beyond what neural networks on their own can achieve. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954931975", "annotations": [], "published": 1516954932, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2f06", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.04574"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sergio Miguel Tom&#xe9;", "title": "Multi-optional Many-sorted Past Present Future structures and its description. (arXiv:1801.08212v1 [cs.LO])", "alternate": [{"href": "http://arxiv.org/abs/1801.08212", "type": "text/html"}], "timestampUsec": "1516954931974850", "comments": [], "summary": {"content": "<p>The cognitive theory of true conditions (CTTC) is a proposal to describe the \nmodel-theoretic semantics of symbolic cognitive architectures and design the \nimplementation of cognitive abilities. The CTTC is formulated mathematically \nusing the multi-optional many-sorted past present future(MMPPF) structures. \nThis article defines mathematically the MMPPF structures and the formal \nlanguages proposed to describe them by the CTTC. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954931975", "annotations": [], "published": 1516954932, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2f11", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08212"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kui Yu, Lin Liu, Jiuyong Li", "title": "Discovering Markov Blanket from Multiple interventional Datasets. (arXiv:1801.08295v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.08295", "type": "text/html"}], "timestampUsec": "1516954931974849", "comments": [], "summary": {"content": "<p>In this paper, we study the problem of discovering the Markov blanket (MB) of \na target variable from multiple interventional datasets. Datasets attained from \ninterventional experiments contain richer causal information than passively \nobserved data (observational data) for MB discovery. However, almost all \nexisting MB discovery methods are designed for finding MBs from a single \nobservational dataset. To identify MBs from multiple interventional datasets, \nwe face two challenges: (1) unknown intervention variables; (2) nonidentical \ndata distributions. To tackle the challenges, we theoretically analyze (a) \nunder what conditions we can find the correct MB of a target variable, and (b) \nunder what conditions we can identify the causes of the target variable via \ndiscovering its MB. Based on the theoretical analysis, we propose a new \nalgorithm for discovering MBs from multiple interventional datasets, and \npresent the conditions/assumptions which assure the correctness of the \nalgorithm. To our knowledge, this work is the first to present the theoretical \nanalyses about the conditions for MB discovery in multiple interventional \ndatasets and the algorithm to find the MBs in relation to the conditions. Using \nbenchmark Bayesian networks and real-world datasets, the experiments have \nvalidated the effectiveness and efficiency of the proposed algorithm in the \npaper. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954931975", "annotations": [], "published": 1516954932, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2f1a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08295"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "ihyung Moon, Hyochang Yang, Sungzoon Cho", "title": "Finding ReMO (Related Memory Object): A Simple Neural Architecture for Text based Reasoning. (arXiv:1801.08459v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.08459", "type": "text/html"}], "timestampUsec": "1516954931974848", "comments": [], "summary": {"content": "<p>To solve the text-based question and answering task that requires relational \nreasoning, it is necessary to memorize a large amount of information and find \nout the question relevant information from the memory. Most approaches were \nbased on external memory and four components proposed by Memory Network. The \ndistinctive component among them was the way of finding the necessary \ninformation and it contributes to the performance. Recently, a simple but \npowerful neural network module for reasoning called Relation Network (RN) has \nbeen introduced. We analyzed RN from the view of Memory Network, and realized \nthat its MLP component is able to reveal the complicate relation between \nquestion and object pair. Motivated from it, we introduce which uses MLP to \nfind out relevant information on Memory Network architecture. It shows new \nstate-of-the-art results in jointly trained bAbI-10k story-based question \nanswering tasks and bAbI dialog-based question answering tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954931975", "annotations": [], "published": 1516954932, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2f27", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08459"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yangyan Li, Rui Bu, Mingchao Sun, Baoquan Chen", "title": "PointCNN. (arXiv:1801.07791v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1801.07791", "type": "text/html"}], "timestampUsec": "1516954931974846", "comments": [], "summary": {"content": "<p>We present a simple and general framework for feature learning from point \ncloud. The key to the success of CNNs is the convolution operator that is \ncapable of leveraging spatially-local correlation in data represented densely \nin grids (e.g. images). However, point cloud are irregular and unordered, thus \na direct convolving of kernels against the features associated with the points \nwill result in deserting the shape information while being variant to the \norders. To address these problems, we propose to learn a X-transformation from \nthe input points, and then use it to simultaneously weight the input features \nassociated with the points and permute them into latent potentially canonical \norder, before the element-wise product and sum operations are applied. The \nproposed method is a generalization of typical CNNs into learning features from \npoint cloud, thus we call it PointCNN. Experiments show that PointCNN achieves \non par or better performance than state-of-the-art methods on multiple \nchallenging benchmark datasets and tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954931975", "annotations": [], "published": 1516954932, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2f31", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07791"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hongwei Wang, Fuzheng Zhang, Xing Xie, Minyi Guo", "title": "DKN: Deep Knowledge-Aware Network for News Recommendation. (arXiv:1801.08284v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.08284", "type": "text/html"}], "timestampUsec": "1516954931974845", "comments": [], "summary": {"content": "<p>Online news recommender systems aim to address the information explosion of \nnews and make personalized recommendation for users. In general, news language \nis highly condensed, full of knowledge entities and common sense. However, \nexisting methods are unaware of such external knowledge and cannot fully \ndiscover latent knowledge-level connections among news. The recommended results \nfor a user are consequently limited to simple patterns and cannot be extended \nreasonably. Moreover, news recommendation also faces the challenges of high \ntime-sensitivity of news and dynamic diversity of users' interests. To solve \nthe above problems, in this paper, we propose a deep knowledge-aware network \n(DKN) that incorporates knowledge graph representation into news \nrecommendation. DKN is a content-based deep recommendation framework for \nclick-through rate prediction. The key component of DKN is a multi-channel and \nword-entity-aligned knowledge-aware convolutional neural network (KCNN) that \nfuses semantic-level and knowledge-level representations of news. KCNN treats \nwords and entities as multiple channels, and explicitly keeps their alignment \nrelationship during convolution. In addition, to address users' diverse \ninterests, we also design an attention module in DKN to dynamically aggregate a \nuser's history with respect to current candidate news. Through extensive \nexperiments on a real online news platform, we demonstrate that DKN achieves \nsubstantial gains over state-of-the-art deep recommendation models. We also \nvalidate the efficacy of the usage of knowledge in DKN. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954931975", "annotations": [], "published": 1516954932, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2f3e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08284"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Carl Andersson, Niklas Wahlstr&#xf6;m, Thomas B. Sch&#xf6;n", "title": "Data-Driven Impulse Response Regularization via Deep Learning. (arXiv:1801.08383v1 [cs.SY])", "alternate": [{"href": "http://arxiv.org/abs/1801.08383", "type": "text/html"}], "timestampUsec": "1516954931974844", "comments": [], "summary": {"content": "<p>We consider the problem of impulse response estimation for stable linear \nsingle-input single-output systems. It is a well-studied problem where flexible \nnon-parametric models recently offered a leap in performance compared to the \nclassical finite-dimensional model structures. Inspired by this development and \nthe success of deep learning we propose a new flexible data-driven model. Our \nexperiments indicate that the new model is capable of exploiting even more of \nthe hidden patterns that are present in the input-output data as compared to \nthe non-parametric models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954931975", "annotations": [], "published": 1516954932, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2f49", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08383"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Eirikur Agustsson, Alexander Sage, Radu Timofte, Luc Van Gool", "title": "Optimal transport maps for distribution preserving operations on latent spaces of Generative Models. (arXiv:1711.01970v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.01970", "type": "text/html"}], "timestampUsec": "1516954931974842", "comments": [], "summary": {"content": "<p>Generative models such as Variational Auto Encoders (VAEs) and Generative \nAdversarial Networks (GANs) are typically trained for a fixed prior \ndistribution in the latent space, such as uniform or Gaussian. After a trained \nmodel is obtained, one can sample the Generator in various forms for \nexploration and understanding, such as interpolating between two samples, \nsampling in the vicinity of a sample or exploring differences between a pair of \nsamples applied to a third sample. In this paper, we show that the latent space \noperations used in the literature so far induce a distribution mismatch between \nthe resulting outputs and the prior distribution the model was trained on. To \naddress this, we propose to use distribution matching transport maps to ensure \nthat such latent space operations preserve the prior distribution, while \nminimally modifying the original operation. Our experimental results validate \nthat the proposed operations give higher quality samples compared to the \noriginal operations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954931975", "annotations": [], "published": 1516954932, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2f5c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.01970"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Seyed Hamid Reza Pasandideh, Soheyl Khalilpourazari", "title": "Sine Cosine Crow Search Algorithm: A powerful hybrid meta heuristic for global optimization. (arXiv:1801.08485v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.08485", "type": "text/html"}], "timestampUsec": "1516954929408895", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328e12a68\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328e12a68&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper presents a novel hybrid algorithm named Since Cosine Crow Search \nAlgorithm. To propose the SCCSA, two novel algorithms are considered including \nCrow Search Algorithm (CSA) and Since Cosine Algorithm (SCA). The advantages of \nthe two algorithms are considered and utilize to design an efficient hybrid \nalgorithm which can perform significantly better in various benchmark \nfunctions. The combination of concept and operators of the two algorithms \nenable the SCCSA to make an appropriate trade-off between exploration and \nexploitation abilities of the algorithm. To evaluate the performance of the \nproposed SCCSA, seven well-known benchmark functions are utilized. The results \nindicated that the proposed hybrid algorithm is able to provide very \ncompetitive solution comparing to other state-of-the-art meta heuristics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2d63", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08485"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Colm V. Gallagher, Kevin Leahy, Peter O&#x27;Donovan, Ken Bruton, Dominic T.J. O&#x27;Sullivan", "title": "Development and application of a machine learning supported methodology for measurement and verification (M&V) 2.0. (arXiv:1801.08175v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.08175", "type": "text/html"}], "timestampUsec": "1516954929408892", "comments": [], "summary": {"content": "<p>The foundations of all methodologies for the measurement and verification \n(M&amp;V) of energy savings are based on the same five key principles: accuracy, \ncompleteness, conservatism, consistency and transparency. The most widely \naccepted methodologies tend to generalise M&amp;V so as to ensure applicability \nacross the spectrum of energy conservation measures (ECM's). These do not \nprovide a rigid calculation procedure to follow. This paper aims to bridge the \ngap between high-level methodologies and the practical application of modelling \nalgorithms, with a focus on the industrial buildings sector. This is achieved \nwith the development of a novel, machine learning supported methodology for M&amp;V \n2.0 which enables accurate quantification of savings. \n</p> \n<p>A novel and computationally efficient feature selection algorithm and \npowerful machine learning regression algorithms are employed to maximise the \neffectiveness of available data. The baseline period energy consumption is \nmodelled using artificial neural networks, support vector machines, k-nearest \nneighbours and multiple ordinary least squares regression. Improved knowledge \ndiscovery and an expanded boundary of analysis allow more complex energy \nsystems be analysed, thus increasing the applicability of M&amp;V. A case study in \na large biomedical manufacturing facility is used to demonstrate the \nmethodology's ability to accurately quantify the savings under real-world \nconditions. The ECM was found to result in 604,527 kWh of energy savings with \n57% uncertainty at a confidence interval of 68%. 20 baseline energy models are \ndeveloped using an exhaustive approach with the optimal model being used to \nquantify savings. The range of savings estimated with each model are presented \nand the acceptability of uncertainty is reviewed. The case study demonstrates \nthe ability of the methodology to perform M&amp;V to an acceptable standard in \nchallenging circumstances. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2d83", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08175"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, Tamara L.Berg", "title": "MAttNet: Modular Attention Network for Referring Expression Comprehension. (arXiv:1801.08186v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.08186", "type": "text/html"}], "timestampUsec": "1516954929408891", "comments": [], "summary": {"content": "<p>In this paper, we address referring expression comprehension: localizing an \nimage region described by a natural language expression. While most recent work \ntreats expressions as a single unit, we propose to decompose them into three \nmodular components related to subject appearance, location, and relationship to \nother objects. This allows us to flexibly adapt to expressions containing \ndifferent types of information in an end-to-end framework. In our model, which \nwe call the Modular Attention Network (MAttNet), two types of attention are \nutilized: language-based attention that learns the module weights as well as \nthe word/phrase attention that each module should focus on; and visual \nattention that allows the subject and relationship modules to focus on relevant \nimage components. Module weights combine scores from all three modules \ndynamically to output an overall score. Experiments show that MAttNet \noutperforms previous state-of-art methods by a large margin on both \nbounding-box-level and pixel-level comprehension tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2d89", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08186"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Craig Sherstan, Brendan Bennett, Kenny Young, Dylan R. Ashley, Adam White, Martha White, Richard S. Sutton", "title": "Directly Estimating the Variance of the {\\lambda}-Return Using Temporal-Difference Methods. (arXiv:1801.08287v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.08287", "type": "text/html"}], "timestampUsec": "1516954929408890", "comments": [], "summary": {"content": "<p>This paper investigates estimating the variance of a temporal-difference \nlearning agent's update target. Most reinforcement learning methods use an \nestimate of the value function, which captures how good it is for the agent to \nbe in a particular state and is mathematically expressed as the expected sum of \ndiscounted future rewards (called the return). These values can be \nstraightforwardly estimated by averaging batches of returns using Monte Carlo \nmethods. However, if we wish to update the agent's value estimates during \nlearning--before terminal outcomes are observed--we must use a different \nestimation target called the {\\lambda}-return, which truncates the return with \nthe agent's own estimate of the value function. Temporal difference learning \nmethods estimate the expected {\\lambda}-return for each state, allowing these \nmethods to update online and incrementally, and in most cases achieve better \ngeneralization error and faster learning than Monte Carlo methods. Naturally \none could attempt to estimate higher-order moments of the {\\lambda}-return. \nThis paper is about estimating the variance of the {\\lambda}-return. Prior work \nhas shown that given estimates of the variance of the {\\lambda}-return, \nlearning systems can be constructed to (1) mitigate risk in action selection, \nand (2) automatically adapt the parameters of the learning process itself to \nimprove performance. Unfortunately, existing methods for estimating the \nvariance of the {\\lambda}-return are complex and not well understood \nempirically. We contribute a method for estimating the variance of the \n{\\lambda}-return directly using policy evaluation methods from reinforcement \nlearning. Our approach is significantly simpler than prior methods that \nindependently estimate the second moment of the {\\lambda}-return. Empirically \nour new approach behaves at least as well as existing approaches, but is \ngenerally more robust. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2d90", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08287"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Vaishak Belle", "title": "Probabilistic Planning by Probabilistic Programming. (arXiv:1801.08365v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.08365", "type": "text/html"}], "timestampUsec": "1516954929408889", "comments": [], "summary": {"content": "<p>Automated planning is a major topic of research in artificial intelligence, \nand enjoys a long and distinguished history. The classical paradigm assumes a \ndistinguished initial state, comprised of a set of facts, and is defined over a \nset of actions which change that state in one way or another. Planning in many \nreal-world settings, however, is much more involved: an agent's knowledge is \nalmost never simply a set of facts that are true, and actions that the agent \nintends to execute never operate the way they are supposed to. Thus, \nprobabilistic planning attempts to incorporate stochastic models directly into \nthe planning process. In this article, we briefly report on probabilistic \nplanning through the lens of probabilistic programming: a programming paradigm \nthat aims to ease the specification of structured probability distributions. In \nparticular, we provide an overview of the features of two systems, HYPE and \nALLEGRO, which emphasise different strengths of probabilistic programming that \nare particularly useful for complex modelling issues raised in probabilistic \nplanning. Among other things, with these systems, one can instantiate planning \nproblems with growing and shrinking state spaces, discrete and continuous \nprobability distributions, and non-unique prior distributions in a first-order \nsetting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2d9a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08365"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Housam Khalifa Bashier Babiker, Randy Goebel", "title": "Using KL-divergence to focus Deep Visual Explanation. (arXiv:1711.06431v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.06431", "type": "text/html"}], "timestampUsec": "1516954929408888", "comments": [], "summary": {"content": "<p>We present a method for explaining the image classification predictions of \ndeep convolution neural networks, by highlighting the pixels in the image which \ninfluence the final class prediction. Our method requires the identification of \na heuristic method to select parameters hypothesized to be most relevant in \nthis prediction, and here we use Kullback-Leibler divergence to provide this \nfocus. Overall, our approach helps in understanding and interpreting deep \nnetwork predictions and we hope contributes to a foundation for such \nunderstanding of deep learning networks. In this brief paper, our experiments \nevaluate the performance of two popular networks in this context of \ninterpretability. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2da4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.06431"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pin-Yu Chen, Baichuan Zhang, Mohammad Al Hasan", "title": "Incremental Eigenpair Computation for Graph Laplacian Matrices: Theory and Applications. (arXiv:1801.08196v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.08196", "type": "text/html"}], "timestampUsec": "1516954929408887", "comments": [], "summary": {"content": "<p>The smallest eigenvalues and the associated eigenvectors (i.e., eigenpairs) \nof a graph Laplacian matrix have been widely used in spectral clustering and \ncommunity detection. However, in real-life applications the number of clusters \nor communities (say, $K$) is generally unknown a-priori. Consequently, the \nmajority of the existing methods either choose $K$ heuristically or they repeat \nthe clustering method with different choices of $K$ and accept the best \nclustering result. The first option, more often, yields suboptimal result, \nwhile the second option is computationally expensive. In this work, we propose \nan incremental method for constructing the eigenspectrum of the graph Laplacian \nmatrix. This method leverages the eigenstructure of graph Laplacian matrix to \nobtain the $K$-th smallest eigenpair of the Laplacian matrix given a collection \nof all previously computed $K-1$ smallest eigenpairs. Our proposed method \nadapts the Laplacian matrix such that the batch eigenvalue decomposition \nproblem transforms into an efficient sequential leading eigenpair computation \nproblem. As a practical application, we consider user-guided spectral \nclustering. Specifically, we demonstrate that users can utilize the proposed \nincremental method for effective eigenpair computation and for determining the \ndesired number of clusters based on multiple clustering metrics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2dac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08196"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Rahul Mazumder, Diego F. Saldana, Haolei Weng", "title": "Matrix Completion with Nonconvex Regularization: Spectral Operators and Scalable Algorithms. (arXiv:1801.08227v1 [stat.CO])", "alternate": [{"href": "http://arxiv.org/abs/1801.08227", "type": "text/html"}], "timestampUsec": "1516954929408886", "comments": [], "summary": {"content": "<p>In this paper, we study the popularly dubbed matrix completion problem, where \nthe task is to \"fill in\" the unobserved entries of a matrix from a small subset \nof observed entries, under the assumption that the underlying matrix is of \nlow-rank. Our contributions herein, enhance our prior work on nuclear norm \nregularized problems for matrix completion (Mazumder et al., 2010) by \nincorporating a continuum of nonconvex penalty functions between the convex \nnuclear norm and nonconvex rank functions. Inspired by SOFT-IMPUTE (Mazumder et \nal., 2010; Hastie et al., 2016), we propose NC-IMPUTE- an EM-flavored \nalgorithmic framework for computing a family of nonconvex penalized matrix \ncompletion problems with warm-starts. We present a systematic study of the \nassociated spectral thresholding operators, which play an important role in the \noverall algorithm. We study convergence properties of the algorithm. Using \nstructured low-rank SVD computations, we demonstrate the computational \nscalability of our proposal for problems up to the Netflix size (approximately, \na $500,000 \\times 20, 000$ matrix with $10^8$ observed entries). We demonstrate \nthat on a wide range of synthetic and real data instances, our proposed \nnonconvex regularization framework leads to low-rank solutions with better \npredictive performance when compared to those obtained from nuclear norm \nproblems. Implementations of algorithms proposed herein, written in the R \nprogramming language, are made available on github. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2dbb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08227"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ishanu Chattopadhyay", "title": "A Hilbert Space of Stationary Ergodic Processes. (arXiv:1801.08256v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.08256", "type": "text/html"}], "timestampUsec": "1516954929408885", "comments": [], "summary": {"content": "<p>Identifying meaningful signal buried in noise is a problem of interest \narising in diverse scenarios of data-driven modeling. We present here a \ntheoretical framework for exploiting intrinsic geometry in data that resists \nnoise corruption, and might be identifiable under severe obfuscation. Our \napproach is based on uncovering a valid complete inner product on the space of \nergodic stationary finite valued processes, providing the latter with the \nstructure of a Hilbert space on the real field. This rigorous construction, \nbased on non-standard generalizations of the notions of sum and scalar \nmultiplication of finite dimensional probability vectors, allows us to \nmeaningfully talk about \"angles\" between data streams and data sources, and, \nmake precise the notion of orthogonal stochastic processes. In particular, the \nrelative angles appear to be preserved, and identifiable, under severe noise, \nand will be developed in future as the underlying principle for robust \nclassification, clustering and unsupervised featurization algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2dc0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08256"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yingxiang Yang, Jalal Etesami, Niao He, Negar Kiyavash", "title": "Nonparametric Hawkes Processes: Online Estimation and Generalization Bounds. (arXiv:1801.08273v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.08273", "type": "text/html"}], "timestampUsec": "1516954929408884", "comments": [], "summary": {"content": "<p>In this paper, we design a nonparametric online algorithm for estimating the \ntriggering functions of multivariate Hawkes processes. Unlike parametric \nestimation, where evolutionary dynamics can be exploited for fast computation \nof the gradient, and unlike typical function learning, where representer \ntheorem is readily applicable upon proper regularization of the objective \nfunction, nonparametric estimation faces the challenges of (i) inefficient \nevaluation of the gradient, (ii) lack of representer theorem, and (iii) \ncomputationally expensive projection necessary to guarantee positivity of the \ntriggering functions. In this paper, we offer solutions to the above \nchallenges, and design an online estimation algorithm named NPOLE-MHP that \noutputs estimations with a $\\mathcal{O}(1/T)$ regret, and a $\\mathcal{O}(1/T)$ \nstability. Furthermore, we design an algorithm, NPOLE-MMHP, for estimation of \nmultivariate marked Hawkes processes. We test the performance of NPOLE-MHP on \nvarious synthetic and real datasets, and demonstrate, under different \nevaluation metrics, that NPOLE-MHP performs as good as the optimal maximum \nlikelihood estimation (MLE), while having a run time as little as parametric \nonline algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2dd8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08273"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Antonin Leroux, Matthieu Boussard, Remi D&#xe8;s", "title": "Information gain ratio correction: Improving prediction with more balanced decision tree splits. (arXiv:1801.08310v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.08310", "type": "text/html"}], "timestampUsec": "1516954929408883", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328e12c4f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328e12c4f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Decision trees algorithms use a gain function to select the best split during \nthe tree's induction. This function is crucial to obtain trees with high \npredictive accuracy. Some gain functions can suffer from a bias when it \ncompares splits of different arities. Quinlan proposed a gain ratio in C4.5's \ninformation gain function to fix this bias. In this paper, we present an \nupdated version of the gain ratio that performs better as it tries to fix the \ngain ratio's bias for unbalanced trees and some splits with low predictive \ninterest. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2de9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08310"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Diego A. Mesa, Justin Tantiongloc, Marcela Mendoza, Todd P. Coleman", "title": "A Distributed Framework for the Construction of Transport Maps. (arXiv:1801.08454v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.08454", "type": "text/html"}], "timestampUsec": "1516954929408882", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328e58e3b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328e58e3b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The need to reason about uncertainty in large, complex, and multi-modal \ndatasets has become increasingly common across modern scientific environments. \nThe ability to transform samples from one distribution $P$ to another \ndistribution $Q$ enables the solution to many problems in machine learning \n(e.g. Bayesian inference, generative modeling) and has been actively pursued \nfrom theoretical, computational, and application perspectives across the fields \nof information theory, computer science, and biology. Performing such \ntransformations , in general, still comprises computational difficulties, \nespecially in high dimensions. Here, we consider the problem of computing such \n\"measure transport maps\" with efficient and parallelizable methods. Under the \nmild assumptions that $P$ need not be known but can be sampled from, that the \ndensity of $Q$ is known up to a proportionality constant, and that $Q$ is \nlog-concave, we provide a convex optimization problem pertaining to relative \nentropy minimization. We show how an empirical minimization formulation and \npolynomial chaos map parameterization can allow for learning a transport map \nbetween $P$ and $Q$ with distributed and scalable methods. We also leverage \nfindings from nonequilibrium thermodynamics to represent the transport map as a \ncomposition of simpler maps, each of which is learned sequentially with a \ntransport cost regularized version of the aforementioned problem formulation. \nWe provide examples of our framework within the context of Bayesian inference \nfor the Boston housing dataset, active learning for optimizing human computer \ninterfaces, density estimation for probabilistic sleep staging with EEG, and \ngenerative modeling for handwritten digit images from the MNIST dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2dfa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.08454"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yining Wang, Aarti Singh", "title": "Provably Correct Algorithms for Matrix Column Subset Selection with Selectively Sampled Data. (arXiv:1505.04343v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1505.04343", "type": "text/html"}], "timestampUsec": "1516954929408881", "comments": [], "summary": {"content": "<p>We consider the problem of matrix column subset selection, which selects a \nsubset of columns from an input matrix such that the input can be well \napproximated by the span of the selected columns. Column subset selection has \nbeen applied to numerous real-world data applications such as population \ngenetics summarization, electronic circuits testing and recommendation systems. \nIn many applications the complete data matrix is unavailable and one needs to \nselect representative columns by inspecting only a small portion of the input \nmatrix. In this paper we propose the first provably correct column subset \nselection algorithms for partially observed data matrices. Our proposed \nalgorithms exhibit different merits and limitations in terms of statistical \naccuracy, computational efficiency, sample complexity and sampling schemes, \nwhich provides a nice exploration of the tradeoff between these desired \nproperties for column subset selection. The proposed methods employ the idea of \nfeedback driven sampling and are inspired by several sampling schemes \npreviously introduced for low-rank matrix approximation tasks (Drineas et al., \n2008; Frieze et al., 2004; Deshpande and Vempala, 2006; Krishnamurthy and \nSingh, 2014). Our analysis shows that, under the assumption that the input data \nmatrix has incoherent rows but possibly coherent columns, all algorithms \nprovably converge to the best low-rank approximation of the original data as \nnumber of selected columns increases. Furthermore, two of the proposed \nalgorithms enjoy a relative error bound, which is preferred for column subset \nselection and matrix approximation purposes. We also demonstrate through both \ntheoretical and empirical analysis the power of feedback driven sampling \ncompared to uniform random sampling on input matrices with highly correlated \ncolumns. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2e08", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1505.04343"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yury Maximov, Massih-Reza Amini, Zaid Harchaoui", "title": "Rademacher Complexity Bounds for a Penalized Multiclass Semi-Supervised Algorithm. (arXiv:1607.00567v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1607.00567", "type": "text/html"}], "timestampUsec": "1516954929408880", "comments": [], "summary": {"content": "<p>We propose Rademacher complexity bounds for multiclass classifiers trained \nwith a two-step semi-supervised model. In the first step, the algorithm \npartitions the partially labeled data and then identifies dense clusters \ncontaining $\\kappa$ predominant classes using the labeled training examples \nsuch that the proportion of their non-predominant classes is below a fixed \nthreshold. In the second step, a classifier is trained by minimizing a margin \nempirical loss over the labeled training set and a penalization term measuring \nthe disability of the learner to predict the $\\kappa$ predominant classes of \nthe identified clusters. The resulting data-dependent generalization error \nbound involves the margin distribution of the classifier, the stability of the \nclustering technique used in the first step and Rademacher complexity terms \ncorresponding to partially labeled training data. Our theoretical result \nexhibit convergence rates extending those proposed in the literature for the \nbinary case, and experimental results on different multiclass classification \nproblems show empirical evidence that supports the theory. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2e0e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1607.00567"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michalis K. Titsias, Omiros Papaspiliopoulos", "title": "Auxiliary gradient-based sampling algorithms. (arXiv:1610.09641v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.09641", "type": "text/html"}], "timestampUsec": "1516954929408879", "comments": [], "summary": {"content": "<p>We introduce a new family of MCMC samplers that combine auxiliary variables, \nGibbs sampling and Taylor expansions of the target density. Our approach \npermits the marginalisation over the auxiliary variables yielding marginal \nsamplers, or the augmentation of the auxiliary variables, yielding auxiliary \nsamplers. The well-known Metropolis-adjusted Langevin algorithm (MALA) and \npreconditioned Crank-Nicolson Langevin (pCNL) algorithm are shown to be special \ncases. We prove that marginal samplers are superior in terms of asymptotic \nvariance and demonstrate cases where they are slower in computing time compared \nto auxiliary samplers. In the context of latent Gaussian models we propose new \nauxiliary and marginal samplers whose implementation requires a single tuning \nparameter, which can be found automatically during the transient phase. \nExtensive experimentation shows that the increase in efficiency (measured as \neffective sample size per unit of computing time) relative to (optimised \nimplementations of) pCNL, elliptical slice sampling and MALA ranges from \n10-fold in binary classification problems to 25-fold in log-Gaussian Cox \nprocesses to 100-fold in Gaussian process regression, and it is on par with \nRiemann manifold Hamiltonian Monte Carlo in an example where the latter has the \nsame complexity as the aforementioned algorithms. We explain this remarkable \nimprovement in terms of the way alternative samplers try to approximate the \neigenvalues of the target. We introduce a novel MCMC sampling scheme for \nhyperparameter learning that builds upon the auxiliary samplers. The MATLAB \ncode for reproducing the experiments in the article is publicly available and a \nSupplement to this article contains additional experiments and implementation \ndetails. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2e1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.09641"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xiao-Feng Gong, Qiu-Hua Lin, Feng-Yu Cong, Lieven De Lathauwer", "title": "Double Coupled Canonical Polyadic Decomposition for Joint Blind Source Separation. (arXiv:1612.09466v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1612.09466", "type": "text/html"}], "timestampUsec": "1516954929408878", "comments": [], "summary": {"content": "<p>Joint blind source separation (J-BSS) is an emerging data-driven technique \nfor multi-set data-fusion. In this paper, J-BSS is addressed from a tensorial \nperspective. We show how, by using second-order multi-set statistics in J-BSS, \na specific double coupled canonical polyadic decomposition (DC-CPD) problem can \nbe formulated. We propose an algebraic DC-CPD algorithm based on a coupled \nrank-1 detection mapping. This algorithm converts a possibly underdetermined \nDC-CPD to a set of overdetermined CPDs. The latter can be solved algebraically \nvia a generalized eigenvalue decomposition based scheme. Therefore, this \nalgorithm is deterministic and returns the exact solution in the noiseless \ncase. In the noisy case, it can be used to effectively initialize optimization \nbased DC-CPD algorithms. In addition, we obtain the determini- stic and generic \nuniqueness conditions for DC-CPD, which are shown to be more relaxed than their \nCPD counterpart. Experiment results are given to illustrate the superiority of \nDC-CPD over standard CPD based BSS methods and several existing J-BSS methods, \nwith regards to uniqueness and accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2e26", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.09466"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Vinay Kumar Verma, Piyush Rai", "title": "A Simple Exponential Family Framework for Zero-Shot Learning. (arXiv:1707.08040v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.08040", "type": "text/html"}], "timestampUsec": "1516954929408877", "comments": [], "summary": {"content": "<p>We present a simple generative framework for learning to predict previously \nunseen classes, based on estimating class-attribute-gated class-conditional \ndistributions. We model each class-conditional distribution as an exponential \nfamily distribution and the parameters of the distribution of each seen/unseen \nclass are defined as functions of the respective observed class attributes. \nThese functions can be learned using only the seen class data and can be used \nto predict the parameters of the class-conditional distribution of each unseen \nclass. Unlike most existing methods for zero-shot learning that represent \nclasses as fixed embeddings in some vector space, our generative model \nnaturally represents each class as a probability distribution. It is simple to \nimplement and also allows leveraging additional unlabeled data from unseen \nclasses to improve the estimates of their class-conditional distributions using \ntransductive/semi-supervised learning. Moreover, it extends seamlessly to \nfew-shot learning by easily updating these distributions when provided with a \nsmall number of additional labelled examples from unseen classes. Through a \ncomprehensive set of experiments on several benchmark data sets, we demonstrate \nthe efficacy of our framework. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2e3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.08040"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Marco Scutari", "title": "Dirichlet Bayesian Network Scores and the Maximum Relative Entropy Principle. (arXiv:1708.00689v4 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.00689", "type": "text/html"}], "timestampUsec": "1516954929408876", "comments": [], "summary": {"content": "<p>A classic approach for learning Bayesian networks from data is to identify a \nmaximum a posteriori (MAP) network structure. In the case of discrete Bayesian \nnetworks, MAP networks are selected by maximising one of several possible \nBayesian Dirichlet (BD) scores; the most famous is the Bayesian Dirichlet \nequivalent uniform (BDeu) score from Heckerman et al (1995). The key properties \nof BDeu arise from its uniform prior over the parameters of the network, which \nmakes structure learning computationally efficient; does not require the \nelicitation of prior knowledge from experts; and satisfies score equivalence. \n</p> \n<p>In this paper we will review the derivation and the properties of BD scores, \nand of BDeu in particular, and we will link them to the corresponding entropy \nestimates to study them from an information theoretic perspective. To this end, \nwe will work in the context of the foundational work of Giffin and Caticha \n(2007), who showed that Bayesian inference can be framed as a particular case \nof the maximum relative entropy principle. We will use this connection to show \nthat BDeu should not be used for structure learning from sparse data, since it \ncontradicts the maximum relative entropy principle; and that it is also \nproblematic from a more classic Bayesian model selection perspective, because \nit produces Bayes factors that are very sensitive to the value of its only \nhyperparameter. We will also show that these issues are in fact different \naspects of the same problem and a consequence of the distributional assumptions \nof the prior. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2e48", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.00689"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, Stephen Tu", "title": "On the Sample Complexity of the Linear Quadratic Regulator. (arXiv:1710.01688v2 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.01688", "type": "text/html"}], "timestampUsec": "1516954929408875", "comments": [], "summary": {"content": "<p>This paper addresses the optimal control problem known as the Linear \nQuadratic Regulator in the case when the dynamics are unknown. We propose a \nmulti-stage procedure, called Coarse-ID control, that estimates a model from a \nfew experimental trials, estimates the error in that model with respect to the \ntruth, and then designs a controller using both the model and uncertainty \nestimate. Our technique uses contemporary tools from random matrix theory to \nbound the error in the estimation procedure. We also employ a recently \ndeveloped approach to control synthesis called System Level Synthesis that \nenables robust control design by solving a convex optimization problem. We \nprovide end-to-end bounds on the relative error in control cost that are nearly \noptimal in the number of parameters and that highlight salient properties of \nthe system to be controlled such as closed-loop sensitivity and optimal control \nmagnitude. We show experimentally that the Coarse-ID approach enables efficient \ncomputation of a stabilizing controller in regimes where simple control schemes \nthat do not take the model uncertainty into account fail to stabilize the true \nsystem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2e58", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.01688"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Gerard Ben Arous, Song Mei, Andrea Montanari, Mihai Nica", "title": "The landscape of the spiked tensor model. (arXiv:1711.05424v2 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05424", "type": "text/html"}], "timestampUsec": "1516954929408874", "comments": [], "summary": {"content": "<p>We consider the problem of estimating a large rank-one tensor ${\\boldsymbol \nu}^{\\otimes k}\\in({\\mathbb R}^{n})^{\\otimes k}$, $k\\ge 3$ in Gaussian noise. \nEarlier work characterized a critical signal-to-noise ratio $\\lambda_{Bayes}= \nO(1)$ above which an ideal estimator achieves strictly positive correlation \nwith the unknown vector of interest. Remarkably no polynomial-time algorithm is \nknown that achieved this goal unless $\\lambda\\ge C n^{(k-2)/4}$ and even \npowerful semidefinite programming relaxations appear to fail for $1\\ll \n\\lambda\\ll n^{(k-2)/4}$. \n</p> \n<p>In order to elucidate this behavior, we consider the maximum likelihood \nestimator, which requires maximizing a degree-$k$ homogeneous polynomial over \nthe unit sphere in $n$ dimensions. We compute the expected number of critical \npoints and local maxima of this objective function and show that it is \nexponential in the dimensions $n$, and give exact formulas for the exponential \ngrowth rate. We show that (for $\\lambda$ larger than a constant) critical \npoints are either very close to the unknown vector ${\\boldsymbol u}$, or are \nconfined in a band of width $\\Theta(\\lambda^{-1/(k-1)})$ around the maximum \ncircle that is orthogonal to ${\\boldsymbol u}$. For local maxima, this band \nshrinks to be of size $\\Theta(\\lambda^{-1/(k-2)})$. These `uninformative' local \nmaxima are likely to cause the failure of optimization algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516954929409", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003637c2e63", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05424"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hokchhay Tann, Soheil Hashemi, Sherief Reda", "title": "Flexible Deep Neural Network Processing. (arXiv:1801.07353v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.07353", "type": "text/html"}], "timestampUsec": "1516772880595289", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328e5905d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328e5905d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The recent success of Deep Neural Networks (DNNs) has drastically improved \nthe state of the art for many application domains. While achieving high \naccuracy performance, deploying state-of-the-art DNNs is a challenge since they \ntypically require billions of expensive arithmetic computations. In addition, \nDNNs are typically deployed in ensemble to boost accuracy performance, which \nfurther exacerbates the system requirements. This computational overhead is an \nissue for many platforms, e.g. data centers and embedded systems, with tight \nlatency and energy budgets. In this article, we introduce flexible DNNs \nensemble processing technique, which achieves large reduction in average \ninference latency while incurring small to negligible accuracy drop. Our \ntechnique is flexible in that it allows for dynamic adaptation between quality \nof results (QoR) and execution runtime. We demonstrate the effectiveness of the \ntechnique on AlexNet and ResNet-50 using the ImageNet dataset. This technique \ncan also easily handle other types of networks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516772880595", "annotations": [], "published": 1516772881, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e4d11b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07353"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andrei Lissovoi, Pietro S. Oliveto, John Alasdair Warwicker", "title": "Hyper-heuristics Can Achieve Optimal Performance for Pseudo-Boolean Optimisation. (arXiv:1801.07546v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.07546", "type": "text/html"}], "timestampUsec": "1516772880595288", "comments": [], "summary": {"content": "<p>Selection hyper-heuristics are randomised search methodologies which choose \nand execute heuristics from a set of low-level heuristics. Recent research for \nthe LeadingOnes benchmark function has shown that the standard Simple Random, \nPermutation, Random Gradient, Greedy and Reinforcement Learning selection \nmechanisms show no effects of learning. The idea behind the learning mechanisms \nis to continue to exploit the currently selected heuristic as long as it is \nsuccessful. However, the probability that a promising heuristic is successful \nin the next step is relatively low when perturbing a reasonable solution to a \ncombinatorial optimisation problem. In this paper we generalise the `simple' \nselection-perturbation mechanisms so success can be measured over some fixed \nperiod of time tau, rather than in a single iteration. We present a benchmark \nfunction where it is necessary to learn to exploit a particular low-level \nheuristic, rigorously proving that it makes the difference between an efficient \nand an inefficient algorithm. For LeadingOnes we prove that the Generalised \nRandom Gradient, and the Generalised Greedy Gradient hyper-heuristics achieve \noptimal performance, while Generalised Greedy, although not as fast, still \noutperforms Random Local Search. The performance of the former two \nhyper-heuristics improves as the number of operators to choose from increases, \nwhile that of the Generalised Greedy hyper-heuristic does not. Experimental \nanalyses confirm these results for realistic problem sizes and shed some light \non the best choices of the parameter tau in various situations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516772880595", "annotations": [], "published": 1516772881, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e4d120", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07546"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Elie Aljalbout, Vladimir Golkov, Yawar Siddiqui, Daniel Cremers", "title": "Clustering with Deep Learning: Taxonomy and New Methods. (arXiv:1801.07648v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.07648", "type": "text/html"}], "timestampUsec": "1516772880595287", "comments": [], "summary": {"content": "<p>Clustering is a fundamental machine learning method. The quality of its \nresults is dependent on the data distribution. For this reason, deep neural \nnetworks can be used for learning better representations of the data. In this \npaper, we propose a systematic taxonomy for clustering with deep learning, in \naddition to a review of methods from the field. Based on our taxonomy, creating \nnew methods is more straightforward. We also propose a new approach which is \nbuilt on the taxonomy and surpasses some of the limitations of some previous \nwork. Our experimental evaluation on image datasets shows that the method \napproaches state-of-the-art clustering quality, and performs better in some \ncases. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516772880595", "annotations": [], "published": 1516772881, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e4d125", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07648"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech Gajewski, Andrea Gesmundo, Neil Houlsby, Wei Wang", "title": "Analyzing Language Learned by an Active Question Answering Agent. (arXiv:1801.07537v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1801.07537", "type": "text/html"}], "timestampUsec": "1516772880595286", "comments": [], "summary": {"content": "<p>We analyze the language learned by an agent trained with reinforcement \nlearning as a component of the ActiveQA system [Buck et al., 2017]. In \nActiveQA, question answering is framed as a reinforcement learning task in \nwhich an agent sits between the user and a black box question-answering system. \nThe agent learns to reformulate the user's questions to elicit the optimal \nanswers. It probes the system with many versions of a question that are \ngenerated via a sequence-to-sequence question reformulation model, then \naggregates the returned evidence to find the best answer. This process is an \ninstance of \\emph{machine-machine} communication. The question reformulation \nmodel must adapt its language to increase the quality of the answers returned, \nmatching the language of the question answering system. We find that the agent \ndoes not learn transformations that align with semantic intuitions but \ndiscovers through learning classical information retrieval techniques such as \ntf-idf re-weighting and stemming. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516772880595", "annotations": [], "published": 1516772881, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e4d12b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07537"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric P. Xing", "title": "Toward Controlled Generation of Text. (arXiv:1703.00955v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.00955", "type": "text/html"}], "timestampUsec": "1516772880595285", "comments": [], "summary": {"content": "<p>Generic generation and manipulation of text is challenging and has limited \nsuccess compared to recent deep generative modeling in visual domain. This \npaper aims at generating plausible natural language sentences, whose attributes \nare dynamically controlled by learning disentangled latent representations with \ndesignated semantics. We propose a new neural generative model which combines \nvariational auto-encoders and holistic attribute discriminators for effective \nimposition of semantic structures. With differentiable approximation to \ndiscrete text samples, explicit constraints on independent attribute controls, \nand efficient collaborative learning of generator and discriminators, our model \nlearns highly interpretable representations from even only word annotations, \nand produces realistic sentences with desired attributes. Quantitative \nevaluation validates the accuracy of sentence and attribute generation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516772880595", "annotations": [], "published": 1516772881, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e4d13c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.00955"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Robert Kosar, David W. Scott", "title": "The Hybrid Bootstrap: A Drop-in Replacement for Dropout. (arXiv:1801.07316v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.07316", "type": "text/html"}], "timestampUsec": "1516772880595284", "comments": [], "summary": {"content": "<p>Regularization is an important component of predictive model building. The \nhybrid bootstrap is a regularization technique that functions similarly to \ndropout except that features are resampled from other training points rather \nthan replaced with zeros. We show that the hybrid bootstrap offers superior \nperformance to dropout. We also present a sampling based technique to simplify \nhyperparameter choice. Next, we provide an alternative sampling technique for \nconvolutional neural networks. Finally, we demonstrate the efficacy of the \nhybrid bootstrap on non-image tasks using tree-based models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516772880595", "annotations": [], "published": 1516772881, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e4d14a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07316"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shang Yu, Chang-Jiang Huang, Jian-Shun Tang, Zhih-Ahn Jia, Yi-Tao Wang, Zhi-Jin Ke, Wei Liu, Xiao Liu, Zong-Quan Zhou, Ze-Di Cheng, Jin-Shi Xu, Yu-Chun Wu, Yuan-Yuan Zhao, Guo-Yong Xiang, Chuan-Feng Li, Guang-Can Guo, Gael Sent&#xed;s, Ramon Mu&#xf1;oz-Tapia", "title": "Experimentally detecting a quantum change point via Bayesian inference. (arXiv:1801.07508v1 [quant-ph])", "alternate": [{"href": "http://arxiv.org/abs/1801.07508", "type": "text/html"}], "timestampUsec": "1516772880595283", "comments": [], "summary": {"content": "<p>Detecting a change point is a crucial task in statistics that has been \nrecently extended to the quantum realm. A source state generator that emits a \nseries of single photons in a default state suffers an alteration at some point \nand starts to emit photons in a mutated state. The problem consists in \nidentifying the point where the change took place. In this work, we consider a \nlearning agent that applies Bayesian inference on experimental data to solve \nthis problem. This learning machine adjusts the measurement over each photon \naccording to the past experimental results finds the change position in an \nonline fashion. Our results show that the local-detection success probability \ncan be largely improved by using such a machine learning technique. This \nprotocol provides a tool for improvement in many applications where a sequence \nof identical quantum states is required. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516772880595", "annotations": [], "published": 1516772881, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e4d155", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07508"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Qimai Li, Zhichao Han, Xiao-Ming Wu", "title": "Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning. (arXiv:1801.07606v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.07606", "type": "text/html"}], "timestampUsec": "1516772880595282", "comments": [], "summary": {"content": "<p>Many interesting problems in machine learning are being revisited with new \ndeep learning tools. For graph-based semisupervised learning, a recent \nimportant development is graph convolutional networks (GCNs), which nicely \nintegrate local vertex features and graph topology in the convolutional layers. \nAlthough the GCN model compares favorably with other state-of-the-art methods, \nits mechanisms are not clear and it still requires a considerable amount of \nlabeled data for validation and model selection. In this paper, we develop \ndeeper insights into the GCN model and address its fundamental limits. First, \nwe show that the graph convolution of the GCN model is actually a special form \nof Laplacian smoothing, which is the key reason why GCNs work, but it also \nbrings potential concerns of over-smoothing with many convolutional layers. \nSecond, to overcome the limits of the GCN model with shallow architectures, we \npropose both co-training and self-training approaches to train GCNs. Our \napproaches significantly improve GCNs in learning with very few labels, and \nexempt them from requiring additional labels for validation. Extensive \nexperiments on benchmarks have verified our theory and proposals. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516772880595", "annotations": [], "published": 1516772881, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e4d15a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07606"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jian Shen, Yanru Qu, Weinan Zhang, Yong Yu", "title": "Wasserstein Distance Guided Representation Learning for Domain Adaptation. (arXiv:1707.01217v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.01217", "type": "text/html"}], "timestampUsec": "1516772880595281", "comments": [], "summary": {"content": "<p>Domain adaptation aims at generalizing a high-performance learner on a target \ndomain via utilizing the knowledge distilled from a source domain which has a \ndifferent but related data distribution. One solution to domain adaptation is \nto learn domain invariant feature representations while the learned \nrepresentations should also be discriminative in prediction. To learn such \nrepresentations, domain adaptation frameworks usually include a domain \ninvariant representation learning approach to measure and reduce the domain \ndiscrepancy, as well as a discriminator for classification. Inspired by \nWasserstein GAN, in this paper we propose a novel approach to learn domain \ninvariant feature representations, namely Wasserstein Distance Guided \nRepresentation Learning (WDGRL). WDGRL utilizes a neural network, denoted by \nthe domain critic, to estimate empirical Wasserstein distance between the \nsource and target samples and optimizes the feature extractor network to \nminimize the estimated Wasserstein distance in an adversarial manner. The \ntheoretical advantages of Wasserstein distance for domain adaptation lie in its \ngradient property and promising generalization bound. Empirical studies on \ncommon sentiment and image classification adaptation datasets demonstrate that \nour proposed WDGRL outperforms the state-of-the-art domain invariant \nrepresentation learning approaches. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516772880595", "annotations": [], "published": 1516772881, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e4d168", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.01217"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Shinichi Shirakawa, Yasushi Iwata, Youhei Akimoto", "title": "Dynamic Optimization of Neural Network Structures Using Probabilistic Modeling. (arXiv:1801.07650v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.07650", "type": "text/html"}], "timestampUsec": "1516771244219939", "comments": [], "summary": {"content": "<p>Deep neural networks (DNNs) are powerful machine learning models and have \nsucceeded in various artificial intelligence tasks. Although various \narchitectures and modules for the DNNs have been proposed, selecting and \ndesigning the appropriate network structure for a target problem is a \nchallenging task. In this paper, we propose a method to simultaneously optimize \nthe network structure and weight parameters during neural network training. We \nconsider a probability distribution that generates network structures, and \noptimize the parameters of the distribution instead of directly optimizing the \nnetwork structure. The proposed method can apply to the various network \nstructure optimization problems under the same framework. We apply the proposed \nmethod to several structure optimization problems such as selection of layers, \nselection of unit types, and selection of connections using the MNIST, \nCIFAR-10, and CIFAR-100 datasets. The experimental results show that the \nproposed method can find the appropriate and competitive network structures. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b613", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07650"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mauro Castelli, Ivo Gon&#xe7;alves, Luca Manzoni, Leonardo Vanneschi", "title": "Pruning Techniques for Mixed Ensembles of Genetic Programming Models. (arXiv:1801.07668v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.07668", "type": "text/html"}], "timestampUsec": "1516771244219938", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328e592c9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328e592c9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The objective of this paper is to define an effective strategy for building \nan ensemble of Genetic Programming (GP) models. Ensemble methods are widely \nused in machine learning due to their features: they average out biases, they \nreduce the variance and they usually generalize better than single models. \nDespite these advantages, building ensemble of GP models is not a \nwell-developed topic in the evolutionary computation community. To fill this \ngap, we propose a strategy that blends individuals produced by standard \nsyntax-based GP and individuals produced by geometric semantic genetic \nprogramming, one of the newest semantics-based method developed in GP. In fact, \nrecent literature showed that combining syntax and semantics could improve the \ngeneralization ability of a GP model. Additionally, to improve the diversity of \nthe GP models used to build up the ensemble, we propose different pruning \ncriteria that are based on correlation and entropy, a commonly used measure in \ninformation theory. Experimental results,obtained over different complex \nproblems, suggest that the pruning criteria based on correlation and entropy \ncould be effective in improving the generalization ability of the ensemble \nmodel and in reducing the computational burden required to build it. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b62a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07668"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Claudia Yan, Dipendra Misra, Andrew Bennnett, Aaron Walsman, Yonatan Bisk, Yoav Artzi", "title": "CHALET: Cornell House Agent Learning Environment. (arXiv:1801.07357v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.07357", "type": "text/html"}], "timestampUsec": "1516771244219937", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328eba01e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328eba01e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present CHALET, a 3D house simulator with support for navigation and \nmanipulation. CHALET includes 58 rooms and 10 house configuration, and allows \nto easily create new house and room layouts. CHALET supports a range of common \nhousehold activities, including moving objects, toggling appliances, and \nplacing objects inside closeable containers. The environment and actions \navailable are designed to create a challenging domain to train and evaluate \nautonomous agents, including for tasks that combine language, vision, and \nplanning in a dynamic environment. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b637", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07357"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Hugh Chen, Scott Lundberg, Su-In Lee", "title": "Hybrid Gradient Boosting Trees and NeuralNetworks for Forecasting Operating Room Data. (arXiv:1801.07384v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.07384", "type": "text/html"}], "timestampUsec": "1516771244219936", "comments": [], "summary": {"content": "<p>Time series data constitutes a distinct and growing problem in machine \nlearning. As the corpus of time series data grows larger, deep models that \nsimultaneously learn features and classify with these features can be \nintractable or suboptimal. In this paper, we present feature learning via long \nshort term memory (LSTM) networks and prediction via gradient boosting trees \n(XGB). Focusing on the consequential setting of electronic health record data, \nwe predict the occurrence of hypoxemia five minutes into the future based on \npast features. We make two observations: 1) long short term memory networks are \neffective at capturing long term dependencies based on a single feature and 2) \ngradient boosting trees are capable of tractably combining a large number of \nfeatures including static features like height and weight. With these \nobservations in mind, we generate features by performing \"supervised\" \nrepresentation learning with LSTM networks. Augmenting the original XGB model \nwith these features gives significantly better performance than either \nindividual method. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b642", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07384"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Wen-Jie Tseng, Jr-Chang Chen, I-Chen Wu, Tinghan Wei", "title": "Comparison Training for Computer Chinese Chess. (arXiv:1801.07411v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.07411", "type": "text/html"}], "timestampUsec": "1516771244219935", "comments": [], "summary": {"content": "<p>This paper describes the application of comparison training (CT) for \nautomatic feature weight tuning, with the final objective of improving the \nevaluation functions used in Chinese chess programs. First, we propose an \nn-tuple network to extract features, since n-tuple networks require very little \nexpert knowledge through its large numbers of features, while simulta-neously \nallowing easy access. Second, we propose a novel evalua-tion method that \nincorporates tapered eval into CT. Experiments show that with the same features \nand the same Chinese chess program, the automatically tuned comparison training \nfeature weights achieved a win rate of 86.58% against the weights that were \nhand-tuned. The above trained version was then improved by adding additional \nfeatures, most importantly n-tuple features. This improved version achieved a \nwin rate of 81.65% against the trained version without additional features. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b64f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07411"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ildefons Magrans de Abril, Ryota Kanai", "title": "Curiosity-driven reinforcement learning with homeostatic regulation. (arXiv:1801.07440v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.07440", "type": "text/html"}], "timestampUsec": "1516771244219934", "comments": [], "summary": {"content": "<p>We propose a curiosity reward based on information theory principles and \nconsistent with the animal instinct to maintain certain critical parameters \nwithin a bounded range. Our experimental validation shows the added value of \nthe additional homeostatic drive to enhance the overall information gain of a \nreinforcement learning agent interacting with a complex environment using \ncontinuous actions. Our method builds upon two ideas: i) To take advantage of a \nnew Bellman-like equation of information gain and ii) to simplify the \ncomputation of the local rewards by avoiding the approximation of complex \ndistributions over continuous states and actions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b65a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07440"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Timm Fitschen, Alexander Schlemmer, Daniel Hornung, Henrik tom W&#xf6;rden, Ulrich Parlitz, Stefan Luther", "title": "CaosDB - Research Data Management for Complex, Changing, and Automated Research Workflows. (arXiv:1801.07653v1 [cs.DB])", "alternate": [{"href": "http://arxiv.org/abs/1801.07653", "type": "text/html"}], "timestampUsec": "1516771244219933", "comments": [], "summary": {"content": "<p>Here we present CaosDB, a Research Data Management System (RDMS) designed to \nensure seamless integration of inhomogeneous data sources and repositories of \nlegacy data. Its primary purpose is the management of data from biomedical \nsciences, both from simulations and experiments during the complete research \ndata lifecycle. An RDMS for this domain faces particular challenges: Research \ndata arise in huge amounts, from a wide variety of sources, and traverse a \nhighly branched path of further processing. To be accepted by its users, an \nRDMS must be built around workflows of the scientists and practices and thus \nsupport changes in workflow and data structure. Nevertheless it should \nencourage and support the development and observation of standards and \nfurthermore facilitate the automation of data acquisition and processing with \nspecialized software. The storage data model of an RDMS must reflect these \ncomplexities with appropriate semantics and ontologies while offering simple \nmethods for finding, retrieving, and understanding relevant data. We show how \nCaosDB responds to these challenges and give an overview of the CaosDB Server, \nits data model and its easy-to-learn CaosDB Query Language. We briefly discuss \nthe status of the implementation, how we currently use CaosDB, and how we plan \nto use and extend it. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b663", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07653"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pablo Barros, German I. Parisi, Di Fu, Xun Liu, Stefan Wermter", "title": "Expectation Learning for Adaptive Crossmodal Stimuli Association. (arXiv:1801.07654v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.07654", "type": "text/html"}], "timestampUsec": "1516771244219932", "comments": [], "summary": {"content": "<p>The human brain is able to learn, generalize, and predict crossmodal stimuli. \nLearning by expectation fine-tunes crossmodal processing at different levels, \nthus enhancing our power of generalization and adaptation in highly dynamic \nenvironments. In this paper, we propose a deep neural architecture trained by \nusing expectation learning accounting for unsupervised learning tasks. Our \nlearning model exhibits a self-adaptable behavior, setting the first steps \ntowards the development of deep learning architectures for crossmodal stimuli \nassociation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b67e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07654"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "James W. Davis, Christopher Menart, Muhammad Akbar, Roman Ilin", "title": "A Classification Refinement Strategy for Semantic Segmentation. (arXiv:1801.07674v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.07674", "type": "text/html"}], "timestampUsec": "1516771244219931", "comments": [], "summary": {"content": "<p>Based on the observation that semantic segmentation errors are partially \npredictable, we propose a compact formulation using confusion statistics of the \ntrained classifier to refine (re-estimate) the initial pixel label hypotheses. \nThe proposed strategy is contingent upon computing the classifier confusion \nprobabilities for a given dataset and estimating a relevant prior on the object \nclasses present in the image to be classified. We provide a procedure to \nrobustly estimate the confusion probabilities and explore multiple prior \ndefinitions. Experiments are shown comparing performances on multiple \nchallenging datasets using different priors to improve a state-of-the-art \nsemantic segmentation classifier. This study demonstrates the potential to \nsignificantly improve semantic labeling and motivates future work for reliable \nlabel prior estimation from images. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b68e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07674"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Osamu Hirose", "title": "Dependent landmark drift: robust point set registration based on the Gaussian mixture model with a statistical shape model. (arXiv:1711.06588v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.06588", "type": "text/html"}], "timestampUsec": "1516771244219930", "comments": [], "summary": {"content": "<p>The goal of point set registration is to find point-by-point correspondences \nbetween point sets, each of which characterizes the shape of an object. Because \nlocal preservation of object geometry is assumed, prevalent algorithms in the \narea can often elegantly solve the problems without using geometric information \nspecific to the objects. This means that registration performance can be \nfurther improved by using prior knowledge of object geometry. In this paper, we \npropose a novel point set registration method using the Gaussian mixture model \nwith prior shape information encoded as a statistical shape model. Our \ntransformation model is defined as a combination of the similar transformation, \nmotion coherence, and the statistical shape model. Therefore, the proposed \nmethod works effectively if the target point set includes outliers and missing \nregions, or if it is rotated. The computational cost can be reduced to linear, \nand therefore the method is scalable to large point sets. The effectiveness of \nthe method will be verified through comparisons with existing algorithms using \ndatasets concerning human body shapes, hands, and faces. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b6ad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.06588"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ching-An Cheng, Byron Boots", "title": "Convergence of Value Aggregation for Imitation Learning. (arXiv:1801.07292v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.07292", "type": "text/html"}], "timestampUsec": "1516771244219929", "comments": [], "summary": {"content": "<p>Value aggregation is a general framework for solving imitation learning \nproblems. Based on the idea of data aggregation, it generates a policy sequence \nby iteratively interleaving policy optimization and evaluation in an online \nlearning setting. While the existence of a good policy in the policy sequence \ncan be guaranteed non-asymptotically, little is known about the convergence of \nthe sequence or the performance of the last policy. In this paper, we debunk \nthe common belief that value aggregation always produces a convergent policy \nsequence with improving performance. Moreover, we identify a critical stability \ncondition for convergence and provide a tight non-asymptotic bound on the \nperformance of the last policy. These new theoretical insights let us stabilize \nproblems with regularization, which removes the inconvenient process of \nidentifying the best policy in the policy sequence in stochastic problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b6b8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07292"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Lorin Crawford, Seth R. Flaxman, Daniel E. Runcie, Mike West", "title": "Predictor Variable Prioritization in Nonlinear Models: A Genetic Association Case Study. (arXiv:1801.07318v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1801.07318", "type": "text/html"}], "timestampUsec": "1516771244219928", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328eba251\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328eba251&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The central aim in this paper is to address variable selection questions in \nnonlinear and nonparametric regression. Motivated within the context of \nstatistical genetics, where nonlinear interactions are of particular interest, \nwe introduce a novel and interpretable way to summarize the relative importance \nof predictor variables. Methodologically, we develop the \"RelATive cEntrality\" \n(RATE) measure to prioritize candidate predictors that are not just marginally \nimportant, but whose associations also stem from significant covarying \nrelationships with other variables in the data. We focus on illustrating RATE \nthrough Bayesian Gaussian process regression; although, the methodological \ninnovations apply to other and more general methods. It is known that nonlinear \nmodels often exhibit greater predictive accuracy than linear models, \nparticularly for outcomes generated by complex architectures. With detailed \nsimulations and a botanical QTL mapping study, we show that applying RATE \nenables an explanation for this improved performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b6d4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07318"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tao Sun, Penghang Yin, Hao Jiang, Lizhi Cheng", "title": "On the complexity of convex inertial proximal algorithms. (arXiv:1801.07389v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1801.07389", "type": "text/html"}], "timestampUsec": "1516771244219927", "comments": [], "summary": {"content": "<p>The inertial proximal gradient algorithm is efficient for the composite \noptimization problem. Recently, the convergence of a special inertial proximal \ngradient algorithm under strong convexity has been also studied. In this paper, \nwe present more novel convergence complexity results, especially on the \nconvergence rates of the function values. The non-ergodic O(1/k) rate is proved \nfor inertial proximal gradient algorithm with constant stepzise when the \nobjective function is coercive. When the objective function fails to promise \ncoercivity, we prove the sublinear rate with diminishing inertial parameters. \nWhen the function satisfies some condition (which is much weaker than the \nstrong convexity), the linear convergence is proved with much larger and \ngeneral stepsize than previous literature. We also extend our results to the \nmulti-block version and present the computational complexity. Both cyclic and \nstochastic index selection strategies are considered. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b6eb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07389"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Chun-Na Li, Yuan-Hai Shao, Wei-Jie Chen, Nai-Yang Deng", "title": "Generalized two-dimensional linear discriminant analysis with regularization. (arXiv:1801.07426v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.07426", "type": "text/html"}], "timestampUsec": "1516771244219926", "comments": [], "summary": {"content": "<p>Recent advances show that two-dimensional linear discriminant analysis \n(2DLDA) is a successful matrix based dimensionality reduction method. However, \n2DLDA may encounter the singularity issue theoretically and the sensitivity to \noutliers. In this paper, a generalized Lp-norm 2DLDA framework with \nregularization for an arbitrary $p&gt;0$ is proposed, named G2DLDA. There are \nmainly two contributions of G2DLDA: one is G2DLDA model uses an arbitrary \nLp-norm to measure the between-class and within-class scatter, and hence a \nproper $p$ can be selected to achieve the robustness. The other one is that by \nintroducing an extra regularization term, G2DLDA achieves better generalization \nperformance, and solves the singularity problem. In addition, G2DLDA can be \nsolved through a series of convex problems with equality constraint, and it has \nclosed solution for each single problem. Its convergence can be guaranteed \ntheoretically when $1\\leq p\\leq2$. Preliminary experimental results on three \ncontaminated human face databases show the effectiveness of the proposed \nG2DLDA. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b6fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07426"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "J&#xf6;rg Herbel, Tomasz Kacprzak, Adam Amara, Alexandre Refregier, Aurelien Lucchi (ETH Zurich)", "title": "Fast Point Spread Function Modeling with Deep Learning. (arXiv:1801.07615v1 [astro-ph.IM])", "alternate": [{"href": "http://arxiv.org/abs/1801.07615", "type": "text/html"}], "timestampUsec": "1516771244219925", "comments": [], "summary": {"content": "<p>Modeling the Point Spread Function (PSF) of wide-field surveys is vital for \nmany astrophysical applications and cosmological probes including weak \ngravitational lensing. The PSF smears the image of any recorded object and \ntherefore needs to be taken into account when inferring properties of galaxies \nfrom astronomical images. In the case of cosmic shear, the PSF is one of the \ndominant sources of systematic errors and must be treated carefully to avoid \nbiases in cosmological parameters. Recently, forward modeling approaches to \ncalibrate shear measurements within the Monte-Carlo Control Loops ($MCCL$) \nframework have been developed. These methods typically require simulating a \nlarge amount of wide-field images, thus, the simulations need to be very fast \nyet have realistic properties in key features such as the PSF pattern. Hence, \nsuch forward modeling approaches require a very flexible PSF model, which is \nquick to evaluate and whose parameters can be estimated reliably from survey \ndata. We present a PSF model that meets these requirements based on a fast \ndeep-learning method to estimate its free parameters. We demonstrate our \napproach on publicly available SDSS data. We extract the most important \nfeatures of the SDSS sample via principal component analysis. Next, we \nconstruct our model based on perturbations of a fixed base profile, ensuring \nthat it captures these features. We then train a Convolutional Neural Network \nto estimate the free parameters of the model from noisy images of the PSF. This \nallows us to render a model image of each star, which we compare to the SDSS \nstars to evaluate the performance of our method. We find that our approach is \nable to accurately reproduce the SDSS PSF at the pixel level, which, due to the \nspeed of both the model evaluation and the parameter estimation, offers good \nprospects for incorporating our method into the $MCCL$ framework. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b70d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07615"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Hao Zhou, Garvesh Raskutti", "title": "Non-parametric sparse additive auto-regressive network models. (arXiv:1801.07644v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.07644", "type": "text/html"}], "timestampUsec": "1516771244219924", "comments": [], "summary": {"content": "<p>Consider a multi-variate time series $(X_t)_{t=0}^{T}$ where $X_t \\in \n\\mathbb{R}^d$ which may represent spike train responses for multiple neurons in \na brain, crime event data across multiple regions, and many others. An \nimportant challenge associated with these time series models is to estimate an \ninfluence network between the $d$ variables, especially when the number of \nvariables $d$ is large meaning we are in the high-dimensional setting. Prior \nwork has focused on parametric vector auto-regressive models. However, \nparametric approaches are somewhat restrictive in practice. In this paper, we \nuse the non-parametric sparse additive model (SpAM) framework to address this \nchallenge. Using a combination of $\\beta$ and $\\phi$-mixing properties of \nMarkov chains and empirical process techniques for reproducing kernel Hilbert \nspaces (RKHSs), we provide upper bounds on mean-squared error in terms of the \nsparsity $s$, logarithm of the dimension $\\log d$, number of time points $T$, \nand the smoothness of the RKHSs. Our rates are sharp up to logarithm factors in \nmany cases. We also provide numerical experiments that support our theoretical \nresults and display potential advantages of using our non-parametric SpAM \nframework for a Chicago crime dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b71f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07644"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yicheng He, Junfeng Liu, Lijun Cheng, Xia Ning", "title": "Drug Selection via Joint Push and Learning to Rank. (arXiv:1801.07691v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.07691", "type": "text/html"}], "timestampUsec": "1516771244219923", "comments": [], "summary": {"content": "<p>Selecting the right drugs for the right patients is a primary goal of \nprecision medicine. In this manuscript, we consider the problem of cancer drug \nselection in a learning-to-rank framework. We have formulated the cancer drug \nselection problem as to accurately predicting 1). the ranking positions of \nsensitive drugs and 2). the ranking orders among sensitive drugs in cancer cell \nlines based on their responses to cancer drugs. We have developed a new \nlearning-to-rank method, denoted as pLETORg , that predicts drug ranking \nstructures in each cell line via using drug latent vectors and cell line latent \nvectors. The pLETORg method learns such latent vectors through explicitly \nenforcing that, in the drug ranking list of each cell line, the sensitive drugs \nare pushed above insensitive drugs, and meanwhile the ranking orders among \nsensitive drugs are correct. Genomics information on cell lines is leveraged in \nlearning the latent vectors. Our experimental results on a benchmark cell \nline-drug response dataset demonstrate that the new pLETORg significantly \noutperforms the state-of-the-art method in prioritizing new sensitive drugs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b72b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07691"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Lenka Zdeborov&#xe1;, Florent Krzakala", "title": "Statistical physics of inference: Thresholds and algorithms. (arXiv:1511.02476v5 [cond-mat.stat-mech] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1511.02476", "type": "text/html"}], "timestampUsec": "1516771244219922", "comments": [], "summary": {"content": "<p>Many questions of fundamental interest in todays science can be formulated as \ninference problems: Some partial, or noisy, observations are performed over a \nset of variables and the goal is to recover, or infer, the values of the \nvariables based on the indirect information contained in the measurements. For \nsuch problems, the central scientific questions are: Under what conditions is \nthe information contained in the measurements sufficient for a satisfactory \ninference to be possible? What are the most efficient algorithms for this task? \nA growing body of work has shown that often we can understand and locate these \nfundamental barriers by thinking of them as phase transitions in the sense of \nstatistical physics. Moreover, it turned out that we can use the gained \nphysical insight to develop new promising algorithms. Connection between \ninference and statistical physics is currently witnessing an impressive \nrenaissance and we review here the current state-of-the-art, with a pedagogical \nfocus on the Ising model which formulated as an inference problem we call the \nplanted spin glass. In terms of applications we review two classes of problems: \n(i) inference of clusters on graphs and networks, with community detection as a \nspecial case and (ii) estimating a signal from its noisy linear measurements, \nwith compressed sensing as a case of sparse estimation. Our goal is to provide \na pedagogical review for researchers in physics and other fields interested in \nthis fascinating topic. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b735", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1511.02476"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Chia-Hsiang Lin, Ruiyuan Wu, Wing-Kin Ma, Chong-Yung Chi, Yue Wang", "title": "Maximum Volume Inscribed Ellipsoid: A New Simplex-Structured Matrix Factorization Framework via Facet Enumeration and Convex Optimization. (arXiv:1708.02883v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.02883", "type": "text/html"}], "timestampUsec": "1516771244219921", "comments": [], "summary": {"content": "<p>Consider a structured matrix factorization model where one factor is \nrestricted to have its columns lying in the unit simplex. This \nsimplex-structured matrix factorization (SSMF) model and the associated \nfactorization techniques have spurred much interest in research topics over \ndifferent areas, such as hyperspectral unmixing in remote sensing, topic \ndiscovery in machine learning, to name a few. In this paper we develop a new \ntheoretical SSMF framework whose idea is to study a maximum volume ellipsoid \ninscribed in the convex hull of the data points. This maximum volume inscribed \nellipsoid (MVIE) idea has not been attempted in prior literature, and we show a \nsufficient condition under which the MVIE framework guarantees exact recovery \nof the factors. The sufficient recovery condition we show for MVIE is much more \nrelaxed than that of separable non-negative matrix factorization (or pure-pixel \nsearch); coincidentally it is also identical to that of minimum volume \nenclosing simplex, which is known to be a powerful SSMF framework for \nnon-separable problem instances. We also show that MVIE can be practically \nimplemented by performing facet enumeration and then by solving a convex \noptimization problem. The potential of the MVIE framework is illustrated by \nnumerical results. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b748", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.02883"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xiaogang Su, Yaa Wonkye, Pei Wang, Xiangrong Yin", "title": "Weighted Orthogonal Components Regression Analysis. (arXiv:1709.04135v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04135", "type": "text/html"}], "timestampUsec": "1516771244219920", "comments": [], "summary": {"content": "<p>In the multiple linear regression setting, we propose a general framework, \ntermed weighted orthogonal components regression (WOCR), which encompasses many \nknown methods as special cases, including ridge regression and principal \ncomponents regression. WOCR makes use of the monotonicity inherent in \northogonal components to parameterize the weight function. The formulation \nallows for efficient determination of tuning parameters and hence is \ncomputationally advantageous. Moreover, WOCR offers insights for deriving new \nbetter variants. Specifically, we advocate weighting components based on their \ncorrelations with the response, which leads to enhanced predictive performance. \nBoth simulated studies and real data examples are provided to assess and \nillustrate the advantages of the proposed methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b75c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04135"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Can Karakus, Yifan Sun, Suhas Diggavi, Wotao Yin", "title": "Straggler Mitigation in Distributed Optimization Through Data Encoding. (arXiv:1711.04969v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.04969", "type": "text/html"}], "timestampUsec": "1516771244219919", "comments": [], "summary": {"content": "<p>Slow running or straggler tasks can significantly reduce computation speed in \ndistributed computation. Recently, coding-theory-inspired approaches have been \napplied to mitigate the effect of straggling, through embedding redundancy in \ncertain linear computational steps of the optimization algorithm, thus \ncompleting the computation without waiting for the stragglers. In this paper, \nwe propose an alternate approach where we embed the redundancy directly in the \ndata itself, and allow the computation to proceed completely oblivious to \nencoding. We propose several encoding schemes, and demonstrate that popular \nbatch algorithms, such as gradient descent and L-BFGS, applied in a \ncoding-oblivious manner, deterministically achieve sample path linear \nconvergence to an approximate solution of the original problem, using an \narbitrarily varying subset of the nodes at each iteration. Moreover, this \napproximation can be controlled by the amount of redundancy and the number of \nnodes used in each iteration. We provide experimental results demonstrating the \nadvantage of the approach over uncoded and data replication strategies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516771244220", "annotations": [], "published": 1516771244, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361e1b784", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.04969"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Benjamin Doerr", "title": "Probabilistic Tools for the Analysis of Randomized Optimization Heuristics. (arXiv:1801.06733v1 [cs.DS])", "alternate": [{"href": "http://arxiv.org/abs/1801.06733", "type": "text/html"}], "timestampUsec": "1516686209446273", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328eba454\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328eba454&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This chapter collects several probabilistic tools that proved to be useful in \nthe analysis of randomized search heuristics. This includes classic material \nlike Markov, Chebyshev and Chernoff inequalities, but also lesser known topics \nlike stochastic domination and coupling or Chernoff bounds for geometrically \ndistributed random variables and for negatively correlated random variables. \nAlmost all of the results presented here have appeared previously, some, \nhowever, only in recent conference publications. While the focus is on \ncollecting tools for the analysis of randomized search heuristics, many of \nthese may be useful as well in the analysis of classic randomized algorithms or \ndiscrete random structures. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361212162", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06733"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Botond Moln&#xe1;r, Melinda Varga, Zoltan Toroczkai, M&#xe1;ria Ercsey-Ravasz", "title": "A high-performance analog Max-SAT solver and its application to Ramsey numbers. (arXiv:1801.06620v1 [cs.CC])", "alternate": [{"href": "http://arxiv.org/abs/1801.06620", "type": "text/html"}], "timestampUsec": "1516686209446272", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328f0b450\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328f0b450&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We introduce a continuous-time analog solver for MaxSAT, a quintessential \nclass of NP-hard discrete optimization problems, where the task is to find a \ntruth assignment for a set of Boolean variables satisfying the maximum number \nof given logical constraints. We show that the scaling of an invariant of the \nsolver's dynamics, the escape rate, as function of the number of unsatisfied \nclauses can predict the global optimum value, often well before reaching the \ncorresponding state. We demonstrate the performance of the solver on hard \nMaxSAT competition problems. We then consider the two-color Ramsey number \n$R(m,m)$ problem, translate it to SAT, and apply our algorithm to the still \nunknown $R(5,5)$. We find edge colorings without monochromatic 5-cliques for \ncomplete graphs up to 42 vertices, while on 43 vertices we find colorings with \nonly two monochromatic 5-cliques, the best coloring found so far, supporting \nthe conjecture that $R(5,5) = 43$. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036121216c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06620"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Danping Liao, Siyu Chen, Yuntao Qian", "title": "Visualization of Hyperspectral Images Using Moving Least Squares. (arXiv:1801.06635v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.06635", "type": "text/html"}], "timestampUsec": "1516686209446271", "comments": [], "summary": {"content": "<p>Displaying the large number of bands in a hyper spectral image on a \ntrichromatic monitor has been an active research topic. The visualized image \nshall convey as much information as possible form the original data and \nfacilitate image interpretation. Most existing methods display HSIs in false \ncolors which contradict with human's experience and expectation. In this paper, \nwe propose a nonlinear approach to visualize an input HSI with natural colors \nby taking advantage of a corresponding RGB image. Our approach is based on \nMoving Least Squares, an interpolation scheme for reconstructing a surface from \na set of control points, which in our case is a set of matching pixels between \nthe HSI and the corresponding RGB image. Based on MLS, the proposed method \nsolves for each spectral signature a unique transformation so that the non \nlinear structure of the HSI can be preserved. The matching pixels between a \npair of HSI and RGB image can be reused to display other HSIs captured b the \nsame imaging sensor with natural colors. Experiments show that the output image \nof the proposed method no only have natural colors but also maintain the visual \ninformation necessary for human analysis. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361212175", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06635"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Babatunde Opeoluwa Akinkunmi, Moyin Florence Babalola", "title": "Knowledge Representation for High-Level Norms and Violation Inference in Logic Programming. (arXiv:1801.06740v1 [cs.MA])", "alternate": [{"href": "http://arxiv.org/abs/1801.06740", "type": "text/html"}], "timestampUsec": "1516686209446270", "comments": [], "summary": {"content": "<p>Most of the knowledge Representation formalisms developed for representing \nprescriptive norms can be categorized as either suitable for representing \neither low level or high level norms.We argue that low level norm \nrepresentations do not advance the cause of autonomy in agents in the sense \nthat it is not the agent itself that determines the normative position it \nshould be at a particular time, on the account of a more general rule. In other \nwords an agent on some external system for a nitty gritty prescriptions of its \nobligations and prohibitions. On the other hand, high level norms which have an \nexplicit description of a norm's precondition and have some form of \nimplication, do not as they exist in the literature do not support generalized \ninferences about violation like low level norm representations do. This paper \npresents a logical formalism for the representation of high level norms in open \nsocieties that enable violation inferences that detail the situation in which \nthe norm violation took place and the identity of the norm violation. Norms are \nformalized as logic programs whose heads specify what an agent is obliged or \npermitted to do when a situation arises and within what time constraint of the \nsituation.Each norm is also assigned an identity using some reification scheme. \nThe body of each logic program describes the nature of the situation in which \nthe agent is expected to act or desist from acting. This kind of violation is \nnovel in the literature. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361212176", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06740"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Weichang Wu, Junchi Yan, Xiaokang Yang, Hongyuan Zha", "title": "Decoupled Learning for Factorial Marked Temporal Point Processes. (arXiv:1801.06805v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.06805", "type": "text/html"}], "timestampUsec": "1516686209446269", "comments": [], "summary": {"content": "<p>This paper introduces the factorial marked temporal point process model and \npresents efficient learning methods. In conventional (multi-dimensional) marked \ntemporal point process models, event is often encoded by a single discrete \nvariable i.e. a marker. In this paper, we describe the factorial marked point \nprocesses whereby time-stamped event is factored into multiple markers. \nAccordingly the size of the infectivity matrix modeling the effect between \npairwise markers is in power order w.r.t. the number of the discrete marker \nspace. We propose a decoupled learning method with two learning procedures: i) \ndirectly solving the model based on two techniques: Alternating Direction \nMethod of Multipliers and Fast Iterative Shrinkage-Thresholding Algorithm; ii) \ninvolving a reformulation that transforms the original problem into a Logistic \nRegression model for more efficient learning. Moreover, a sparse group \nregularizer is added to identify the key profile features and event labels. \nEmpirical results on real world datasets demonstrate the efficiency of our \ndecoupled and reformulated method. The source code is available online. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036121217c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06805"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Liang Ding, Di Chang, Russell Malmberg, Aaron Martinez, David Robinson, Matthew Wicker, Hongfei Yan, Liming Cai", "title": "Efficient Learning of Optimal Markov Network Topology with k-Tree Modeling. (arXiv:1801.06900v1 [cs.DS])", "alternate": [{"href": "http://arxiv.org/abs/1801.06900", "type": "text/html"}], "timestampUsec": "1516686209446268", "comments": [], "summary": {"content": "<p>The seminal work of Chow and Liu (1968) shows that approximation of a finite \nprobabilistic system by Markov trees can achieve the minimum information loss \nwith the topology of a maximum spanning tree. Our current paper generalizes the \nresult to Markov networks of tree width $\\leq k$, for every fixed $k\\geq 2$. In \nparticular, we prove that approximation of a finite probabilistic system with \nsuch Markov networks has the minimum information loss when the network topology \nis achieved with a maximum spanning $k$-tree. While constructing a maximum \nspanning $k$-tree is intractable for even $k=2$, we show that polynomial \nalgorithms can be ensured by a sufficient condition accommodated by many \nmeaningful applications. In particular, we prove an efficient algorithm for \nlearning the optimal topology of higher order correlations among random \nvariables that belong to an underlying linear structure. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361212181", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06900"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Muhammad Abulaish, Jahiruddin", "title": "A Novel Weighted Distance Measure for Multi-Attributed Graph. (arXiv:1801.07150v1 [cs.SI])", "alternate": [{"href": "http://arxiv.org/abs/1801.07150", "type": "text/html"}], "timestampUsec": "1516686209446267", "comments": [], "summary": {"content": "<p>Due to exponential growth of complex data, graph structure has become \nincreasingly important to model various entities and their interactions, with \nmany interesting applications including, bioinformatics, social network \nanalysis, etc. Depending on the complexity of the data, the underlying graph \nmodel can be a simple directed/undirected and/or weighted/un-weighted graph to \na complex graph (aka multi-attributed graph) where vertices and edges are \nlabelled with multi-dimensional vectors. In this paper, we present a novel \nweighted distance measure based on weighted Euclidean norm which is defined as \na function of both vertex and edge attributes, and it can be used for various \ngraph analysis tasks including classification and cluster analysis. The \nproposed distance measure has flexibility to increase/decrease the weightage of \nedge labels while calculating the distance between vertex-pairs. We have also \nproposed a MAGDist algorithm, which reads multi-attributed graph stored in CSV \nfiles containing the list of vertex vectors and edge vectors, and calculates \nthe distance between each vertex-pair using the proposed weighted distance \nmeasure. Finally, we have proposed a multi-attributed similarity graph \ngeneration algorithm, MAGSim, which reads the output of MAGDist algorithm and \ngenerates a similarity graph that can be analysed using classification and \nclustering algorithms. The significance and accuracy of the proposed distance \nmeasure and algorithms is evaluated on Iris and Twitter data sets, and it is \nfound that the similarity graph generated by our proposed method yields better \nclustering results than the existing similarity graph generation methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361212185", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07150"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Laura Giordano, Valentina Gliozzi", "title": "Reasoning about multiple aspects in DLs: Semantics and Closure Construction. (arXiv:1801.07161v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.07161", "type": "text/html"}], "timestampUsec": "1516686209446266", "comments": [], "summary": {"content": "<p>Starting from the observation that rational closure has the undesirable \nproperty of being an \"all or nothing\" mechanism, we here propose a \nmultipreferential semantics, which enriches the preferential semantics \nunderlying rational closure in order to separately deal with the inheritance of \ndifferent properties in an ontology with exceptions. We provide a \nmultipreference closure mechanism which is sound with respect to the \nmultipreference semantics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361212188", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07161"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Junhong Lin, Volkan Cevher", "title": "Optimal Convergence for Distributed Learning with Stochastic Gradient Methods and Spectral-Regularization Algorithms. (arXiv:1801.07226v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.07226", "type": "text/html"}], "timestampUsec": "1516686209446265", "comments": [], "summary": {"content": "<p>We study generalization properties of distributed algorithms in the setting \nof nonparametric regression over a reproducing kernel Hilbert space (RKHS). We \nfirst investigate distributed stochastic gradient methods (SGM), with \nmini-batches and multi-passes over the data. We show that optimal \ngeneralization error bounds can be retained for distributed SGM provided that \nthe partition level is not too large. We then extend our results to \nspectral-regularization algorithms (SRA), including kernel ridge regression \n(KRR), kernel principal component analysis, and gradient methods. Our results \nare superior to the state-of-the-art theory. Particularly, our results show \nthat distributed SGM has a smaller theoretical computational complexity, \ncompared with distributed KRR and classic SGM. Moreover, even for \nnon-distributed SRA, they provide the first optimal, capacity-dependent \nconvergence rates, considering the case that the regression function may not be \nin the RKHS. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361212192", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07226"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mark Sh. Levin", "title": "Combinatorial framework for planning in geological exploration. (arXiv:1801.07229v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.07229", "type": "text/html"}], "timestampUsec": "1516686209446264", "comments": [], "summary": {"content": "<p>The paper describes combinatorial framework for planning of geological \nexploration for oil-gas fields. The suggested scheme of the geological \nexploration involves the following stages: (1) building of special 4-layer \ntree-like model (layer of geological exploration): productive layer, group of \nproductive layers, oil-gas field, oil-gas region (or group of the fields); (2) \ngenerations of local design (exploration) alternatives for each low-layer \ngeological objects: conservation, additional search, independent utilization, \njoint utilization; (3) multicriteria (i.e., multi-attribute) assessment of the \ndesign (exploration) alternatives and their interrelation (compatibility) and \nmapping if the obtained vector estimates into integrated ordinal scale; (4) \nhierarchical design ('bottom-up') of composite exploration plans for each \noil-gas field; (5) integration of the plans into region plans and (6) \naggregation of the region plans into a general exploration plan. Stages 2, 3, \n4, and 5 are based on hierarchical multicriteria morphological design (HMMD) \nmethod (assessment of ranking of alternatives, selection and composition of \nalternatives into composite alternatives). The composition problem is based on \nmorphological clique model. Aggregation of the obtained modular alternatives \n(stage 6) is based on detection of a alternatives 'kernel' and its extension by \naddition of elements (multiple choice model). In addition, the usage of \nmultiset estimates for alternatives is described as well. The alternative \nestimates are based on expert judgment. The suggested combinatorial planning \nmethodology is illustrated by numerical examples for geological exploration of \nYamal peninsula. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036121219c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07229"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ivan Brugere, Brian Gallagher, Tanya Y. Berger-Wolf", "title": "Network Structure Inference, A Survey: Motivations, Methods, and Applications. (arXiv:1610.00782v4 [cs.SI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.00782", "type": "text/html"}], "timestampUsec": "1516686209446263", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328f0b783\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328f0b783&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Networks represent relationships between entities in many complex systems, \nspanning from online social interactions to biological cell development and \nbrain connectivity. In many cases, relationships between entities are \nunambiguously known: are two users 'friends' in a social network? Do two \nresearchers collaborate on a published paper? Do two road segments in a \ntransportation system intersect? These are directly observable in the system in \nquestion. In most cases, relationship between nodes are not directly observable \nand must be inferred: does one gene regulate the expression of another? Do two \nanimals who physically co-locate have a social bond? Who infected whom in a \ndisease outbreak in a population? \n</p> \n<p>Existing approaches for inferring networks from data are found across many \napplication domains and use specialized knowledge to infer and measure the \nquality of inferred network for a specific task or hypothesis. However, current \nresearch lacks a rigorous methodology which employs standard statistical \nvalidation on inferred models. In this survey, we examine (1) how network \nrepresentations are constructed from underlying data, (2) the variety of \nquestions and tasks on these representations over several domains, and (3) \nvalidation strategies for measuring the inferred network's capability of \nanswering questions on the system of interest. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121a5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.00782"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Elad Hazan, Adam Klivans, Yang Yuan", "title": "Hyperparameter Optimization: A Spectral Approach. (arXiv:1706.00764v4 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.00764", "type": "text/html"}], "timestampUsec": "1516686209446262", "comments": [], "summary": {"content": "<p>We give a simple, fast algorithm for hyperparameter optimization inspired by \ntechniques from the analysis of Boolean functions. We focus on the \nhigh-dimensional regime where the canonical example is training a neural \nnetwork with a large number of hyperparameters. The algorithm --- an iterative \napplication of compressed sensing techniques for orthogonal polynomials --- \nrequires only uniform sampling of the hyperparameters and is thus easily \nparallelizable. \n</p> \n<p>Experiments for training deep neural networks on Cifar-10 show that compared \nto state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds \nsignificantly improved solutions, in some cases better than what is attainable \nby hand-tuning. In terms of overall running time (i.e., time required to sample \nvarious settings of hyperparameters plus additional computation time), we are \nat least an order of magnitude faster than Hyperband and Bayesian Optimization. \nWe also outperform Random Search 8x. \n</p> \n<p>Additionally, our method comes with provable guarantees and yields the first \nimprovements on the sample complexity of learning decision trees in over two \ndecades. In particular, we obtain the first quasi-polynomial time algorithm for \nlearning noisy decision trees with polynomial sample complexity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121a7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.00764"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Maziar Raissi", "title": "Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations. (arXiv:1801.06637v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.06637", "type": "text/html"}], "timestampUsec": "1516686209446260", "comments": [], "summary": {"content": "<p>A long-standing problem at the interface of artificial intelligence and \napplied mathematics is to devise an algorithm capable of achieving human level \nor even superhuman proficiency in transforming observed data into predictive \nmathematical models of the physical world. In the current era of abundance of \ndata and advanced machine learning capabilities, the natural question arises: \nHow can we automatically uncover the underlying laws of physics from \nhigh-dimensional data generated from experiments? In this work, we put forth a \ndeep learning approach for discovering nonlinear partial differential equations \nfrom scattered and potentially noisy observations in space and time. \nSpecifically, we approximate the unknown solution as well as the nonlinear \ndynamics by two deep neural networks. The first network acts as a prior on the \nunknown solution and essentially enables us to avoid numerical differentiations \nwhich are inherently ill-conditioned and unstable. The second network \nrepresents the nonlinear dynamics and helps us distill the mechanisms that \ngovern the evolution of a given spatiotemporal data-set. We test the \neffectiveness of our approach for several benchmark problems spanning a number \nof scientific domains and demonstrate how the proposed framework can help us \naccurately learn the underlying dynamics and forecast future states of the \nsystem. In particular, we study the Burgers', Korteweg-de Vries (KdV), \nKuramoto-Sivashinsky, nonlinear Schr\\\"{o}dinger, and Navier-Stokes equations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121ac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06637"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Junhong Lin, Volkan Cevher", "title": "Optimal Rates for Spectral-regularized Algorithms with Least-Squares Regression over Hilbert Spaces. (arXiv:1801.06720v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.06720", "type": "text/html"}], "timestampUsec": "1516686209446259", "comments": [], "summary": {"content": "<p>In this paper, we study regression problems over a separable Hilbert space \nwith the square loss, covering non-parametric regression over a reproducing \nkernel Hilbert space. We investigate a class of spectral-regularized \nalgorithms, including ridge regression, principal component analysis, and \ngradient methods. We prove optimal, high-probability convergence results in \nterms of variants of norms for the studied algorithms, considering a capacity \nassumption on the hypothesis space and a general source condition on the target \nfunction. Consequently, we obtain almost sure convergence results with optimal \nrates. Our results improve and generalize previous results, filling a \ntheoretical gap for the non-attainable cases. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121b3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06720"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Douglas Patterson, Melvin Hinich, Denisa Roberts", "title": "A Second Order Cumulant Spectrum Based Test for Strict Stationarity. (arXiv:1801.06727v1 [q-fin.ST])", "alternate": [{"href": "http://arxiv.org/abs/1801.06727", "type": "text/html"}], "timestampUsec": "1516686209446258", "comments": [], "summary": {"content": "<p>This article develops a statistical test for the null hypothesis of strict \nstationarity of a discrete time stochastic process. When the null hypothesis is \ntrue, the second order cumulant spectrum is zero at all the discrete Fourier \nfrequency pairs present in the principal domain of the cumulant spectrum. The \ntest uses a frame (window) averaged sample estimate of the second order \ncumulant spectrum to build a test statistic that has an asymptotic complex \nstandard normal distribution. We derive the test statistic, study the size and \npower properties of the test, and demonstrate its implementation with intraday \nstock market return data. The test has conservative size properties and good \npower to detect varying variance and unit root in the presence of varying \nvariance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121b8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06727"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xinhang Song, Luis Herranz, Shuqiang Jiang", "title": "Depth CNNs for RGB-D scene recognition: learning from scratch better than transferring from RGB-CNNs. (arXiv:1801.06797v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.06797", "type": "text/html"}], "timestampUsec": "1516686209446257", "comments": [], "summary": {"content": "<p>Scene recognition with RGB images has been extensively studied and has \nreached very remarkable recognition levels, thanks to convolutional neural \nnetworks (CNN) and large scene datasets. In contrast, current RGB-D scene data \nis much more limited, so often leverages RGB large datasets, by transferring \npretrained RGB CNN models and fine-tuning with the target RGB-D dataset. \nHowever, we show that this approach has the limitation of hardly reaching \nbottom layers, which is key to learn modality-specific features. In contrast, \nwe focus on the bottom layers, and propose an alternative strategy to learn \ndepth features combining local weakly supervised training from patches followed \nby global fine tuning with images. This strategy is capable of learning very \ndiscriminative depth-specific features with limited depth images, without \nresorting to Places-CNN. In addition we propose a modified CNN architecture to \nfurther match the complexity of the model and the amount of data available. For \nRGB-D scene recognition, depth and RGB features are combined by projecting them \nin a common space and further leaning a multilayer classifier, which is jointly \noptimized in an end-to-end network. Our framework achieves state-of-the-art \naccuracy on NYU2 and SUN RGB-D in both depth only and combined RGB-D data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121c2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06797"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Bruce Hajek, Suryanarayana Sankagiri", "title": "Preferential Attachment Graphs with Planted Communities. (arXiv:1801.06816v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.06816", "type": "text/html"}], "timestampUsec": "1516686209446256", "comments": [], "summary": {"content": "<p>A variation of the preferential attachment random graph model of Barab\\'{a}si \nand Albert is defined that incorporates planted communities. The graph is built \nprogressively, with new vertices attaching to the existing ones one-by-one. At \nevery step, the incoming vertex is randomly assigned a label, which represents \na community it belongs to. This vertex then chooses certain vertices as its \nneighbors, with the choice of each vertex being proportional to the degree of \nthe vertex multiplied by an affinity depending on the labels of the new vertex \nand a potential neighbor. It is shown that the fraction of half-edges attached \nto vertices with a given label converges almost surely for some classes of \naffinity matrices. In addition, the empirical degree distribution for the set \nof vertices with a given label converges to a heavy tailed distribution, such \nthat the tail decay parameter can be different for different communities. Our \nproof method may be of independent interest, both for the classical \nBarab\\'{a}si -Albert model and for other possible extensions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121ca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06816"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Bruce Hajek, Suryanarayana Sankagiri", "title": "Recovering a Hidden Community in a Preferential Attachment Graph. (arXiv:1801.06818v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.06818", "type": "text/html"}], "timestampUsec": "1516686209446255", "comments": [], "summary": {"content": "<p>A message passing algorithm (MP) is derived for recovering a dense subgraph \nwithin a graph generated by a variation of the Barab\\'{a}si-Albert preferential \nattachment model. The estimator is assumed to know the arrival times, or order \nof attachment, of the vertices. The derivation of the algorithm is based on \nbelief propagation under an independence assumption. Two precursors to the \nmessage passing algorithm are analyzed: the first is a degree thresholding (DT) \nalgorithm and the second is an algorithm based on the arrival times of the \nchildren (C) of a given vertex, where the children of a given vertex are the \nvertices that attached to it. C significantly outperforms DT, showing it is \nbeneficial to know the arrival times of the children, beyond simply knowing the \nnumber of them. It is shown that for a fixed fraction of vertices in the \ncommunity $\\rho$, fixed number of new edges per arriving vertex $m$, and fixed \naffinity between vertices in the community $\\beta$, the fraction of label \nerrors for either of the algorithms DT or C, or converges as $T\\to\\infty.$ \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121cf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06818"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Filippo Maria Bianchi, Lorenzo Livi, Alberto Ferrante, Jelena Milosevic, Miroslaw Malek", "title": "Time series kernel similarities for predicting Paroxysmal Atrial Fibrillation from ECGs. (arXiv:1801.06845v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.06845", "type": "text/html"}], "timestampUsec": "1516686209446254", "comments": [], "summary": {"content": "<p>We tackle the problem of classifying Electrocardiography (ECG) signals with \nthe aim of predicting the onset of Paroxysmal Atrial Fibrillation (PAF). Atrial \nfibrillation is the most common type of arrhythmia, but in many cases PAF \nepisodes are asymptomatic. Therefore, in order to help diagnosing PAF, it is \nimportant to be design a suitable procedure for detecting and, more \nimportantly, predicting PAF episodes. We propose a method for predicting PAF \nevents whose first step consists of a feature extraction procedure that \nrepresents each ECG as a multi-variate time series. Successively, we design a \nclassification framework based on kernel similarities for multi-variate time \nseries, capable of handling missing data. We consider different approaches to \nperform classification in the original space of the multi-variate time series \nand in an embedding space, defined by the kernel similarity measure. Our \nclassification results show state-of-the-art performance in terms of accuracy. \nFurthermore, we demonstrate the ability to predict, with high accuracy, the PAF \nonset up to 15 minutes in advance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121d5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06845"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yinhao Zhu, Nicholas Zabaras", "title": "Bayesian Deep Convolutional Encoder-Decoder Networks for Surrogate Modeling and Uncertainty Quantification. (arXiv:1801.06879v1 [physics.comp-ph])", "alternate": [{"href": "http://arxiv.org/abs/1801.06879", "type": "text/html"}], "timestampUsec": "1516686209446253", "comments": [], "summary": {"content": "<p>We are interested in the development of surrogate models for uncertainty \nquantification and propagation in problems governed by stochastic PDEs using a \ndeep convolutional encoder-decoder network in a similar fashion to approaches \nconsidered in deep learning for image-to-image regression tasks. Since normal \nneural networks are data intensive and cannot provide predictive uncertainty, \nwe propose a Bayesian approach to convolutional neural nets. A recently \nintroduced variational gradient descent algorithm based on Stein's method is \nscaled to deep convolutional networks to perform approximate Bayesian inference \non millions of uncertain network parameters. This approach achieves state of \nthe art performance in terms of predictive accuracy and uncertainty \nquantification in comparison to other approaches in Bayesian neural networks as \nwell as techniques that include Gaussian processes and ensemble methods even \nwhen the training data size is relatively small. To evaluate the performance of \nthis approach, we consider standard uncertainty quantification benchmark \nproblems including flow in heterogeneous media defined in terms of limited \ndata-driven permeability realizations. The performance of the surrogate model \ndeveloped is very good even though there is no underlying structure shared \nbetween the input (permeability) and output (flow/pressure) fields as is often \nthe case in the image-to-image regression models used in computer vision \nproblems. Studies are performed with an underlying stochastic input \ndimensionality up to $4,225$ where most other uncertainty quantification \nmethods fail. Uncertainty propagation tasks are considered and the predictive \noutput Bayesian statistics are compared to those obtained with Monte Carlo \nestimates. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06879"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Linbo Qiao, Tianyi Lin, Qi Qin, Xicheng Lu", "title": "On the Iteration Complexity Analysis of Stochastic Primal-Dual Hybrid Gradient Approach with High Probability. (arXiv:1801.06934v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.06934", "type": "text/html"}], "timestampUsec": "1516686209446252", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328f0b9d0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328f0b9d0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we propose a stochastic Primal-Dual Hybrid Gradient (PDHG) \napproach for solving a wide spectrum of regularized stochastic minimization \nproblems, where the regularization term is composite with a linear function. It \nhas been recognized that solving this kind of problem is challenging since the \nclosed-form solution of the proximal mapping associated with the regularization \nterm is not available due to the imposed linear composition, and the \nper-iteration cost of computing the full gradient of the expected objective \nfunction is extremely high when the number of input data samples is \nconsiderably large. \n</p> \n<p>Our new approach overcomes these issues by exploring the special structure of \nthe regularization term and sampling a few data points at each iteration. \nRather than analyzing the convergence in expectation, we provide the detailed \niteration complexity analysis for the cases of both uniformly and non-uniformly \naveraged iterates with high probability. This strongly supports the good \npractical performance of the proposed approach. Numerical experiments \ndemonstrate that the efficiency of stochastic PDHG, which outperforms other \ncompeting algorithms, as expected by the high-probability convergence analysis. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121da", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06934"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Stefan Feuerriegel, Julius Gordon", "title": "News-based forecasts of macroeconomic indicators: A semantic path model for interpretable predictions. (arXiv:1801.07047v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.07047", "type": "text/html"}], "timestampUsec": "1516686209446251", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328f4b8e2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328f4b8e2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The macroeconomic climate influences operations with regard to, e.g., raw \nmaterial prices, financing, supply chain utilization and demand quotas. In \norder to adapt to the economic environment, decision-makers across the public \nand private sectors require accurate forecasts of the economic outlook. \nExisting predictive frameworks base their forecasts primarily on time series \nanalysis, as well as the judgments of experts. As a consequence, current \napproaches are often biased and prone to error. In order to reduce forecast \nerrors, this paper presents an innovative methodology that extends lag \nvariables with unstructured data in the form of financial news: (1) we apply a \nvariety of models from machine learning to word counts as a high-dimensional \ninput. However, this approach suffers from low interpretability and \noverfitting, motivating the following remedies. (2) We follow the intuition \nthat the economic climate is driven by general sentiments and suggest a \nprojection of words onto latent semantic structures as a means of feature \nengineering. (3) We propose a semantic path model, together with estimation \ntechnique based on regularization, in order to yield full interpretability of \nthe forecasts. We demonstrate the predictive performance of our approach by \nutilizing 80,813 ad hoc announcements in order to make long-term forecasts of \nup to 24 months ahead regarding key macroeconomic indicators. Back-testing \nreveals a considerable reduction in forecast errors. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121dd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07047"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Satoshi Iso, Shotaro Shiba, Sumito Yokoo", "title": "Scale-invariant Feature Extraction of Neural Network and Renormalization Group Flow. (arXiv:1801.07172v1 [hep-th])", "alternate": [{"href": "http://arxiv.org/abs/1801.07172", "type": "text/html"}], "timestampUsec": "1516686209446250", "comments": [], "summary": {"content": "<p>Theoretical understanding of how deep neural network (DNN) extracts features \nfrom input images is still unclear, but it is widely believed that the \nextraction is performed hierarchically through a process of coarse-graining. It \nreminds us of the basic concept of renormalization group (RG) in statistical \nphysics. In order to explore possible relations between DNN and RG, we use the \nRestricted Boltzmann machine (RBM) applied to Ising model and construct a flow \nof model parameters (in particular, temperature) generated by the RBM. We show \nthat the unsupervised RBM trained by spin configurations at various \ntemperatures from $T=0$ to $T=6$ generates a flow along which the temperature \napproaches the critical value $T_c=2.27$. This behavior is opposite to the \ntypical RG flow of the Ising model. By analyzing various properties of the \nweight matrices of the trained RBM, we discuss why it flows towards $T_c$ and \nhow the RBM learns to extract features of spin configurations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121e2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07172"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sean Bayley, Davide Falessi", "title": "Optimizing Prediction Intervals by Tuning Random Forest via Meta-Validation. (arXiv:1801.07194v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.07194", "type": "text/html"}], "timestampUsec": "1516686209446249", "comments": [], "summary": {"content": "<p>Recent studies have shown that tuning prediction models increases prediction \naccuracy and that Random Forest can be used to construct prediction intervals. \nHowever, to our best knowledge, no study has investigated the need to, and the \nmanner in which one can, tune Random Forest for optimizing prediction intervals \n{ this paper aims to fill this gap. We explore a tuning approach that combines \nan effectively exhaustive search with a validation technique on a single Random \nForest parameter. This paper investigates which, out of eight validation \ntechniques, are beneficial for tuning, i.e., which automatically choose a \nRandom Forest configuration constructing prediction intervals that are reliable \nand with a smaller width than the default configuration. Additionally, we \npresent and validate three meta-validation techniques to determine which are \nbeneficial, i.e., those which automatically chose a beneficial validation \ntechnique. This study uses data from our industrial partner (Keymind Inc.) and \nthe Tukutuku Research Project, related to post-release defect prediction and \nWeb application effort estimation, respectively. Results from our study \nindicate that: i) the default configuration is frequently unreliable, ii) most \nof the validation techniques, including previously successfully adopted ones \nsuch as 50/50 holdout and bootstrap, are counterproductive in most of the \ncases, and iii) the 75/25 holdout meta-validation technique is always \nbeneficial; i.e., it avoids the likely counterproductive effects of validation \ntechniques. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121e4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07194"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Louis Faury, Flavian Vasile", "title": "Rover Descent: Learning to optimize by learning to navigate on prototypical loss surfaces. (arXiv:1801.07222v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.07222", "type": "text/html"}], "timestampUsec": "1516686209446248", "comments": [], "summary": {"content": "<p>Learning to optimize - the idea that we can learn from data algorithms that \noptimize a numerical criterion - has recently been at the heart of a growing \nnumber of research efforts. One of the most challenging issues within this \napproach is to learn a policy that is able to optimize over classes of \nfunctions that are fairly different from the ones that it was trained on. We \npropose a novel way of framing learning to optimize as a problem of learning a \ngood navigation policy on a partially observable loss surface. To this end, we \ndevelop Rover Descent, a solution that allows us to learn a fairly broad \noptimization policy from training on a small set of prototypical \ntwo-dimensional surfaces that encompasses the classically hard cases such as \nvalleys, plateaus, cliffs and saddles and by using strictly zero-order \ninformation. We show that, without having access to gradient or curvature \ninformation, we achieve state-of-the-art convergence speed on optimization \nproblems not presented at training time such as the Rosenbrock function and \nother hard cases in two dimensions. We extend our framework to optimize over \nhigh dimensional landscapes, while still handling only two-dimensional local \nlandscape information and show good preliminary results. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121e8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07222"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Quoc Tran-Dinh, Yen-Huan Li, Volkan Cevher", "title": "Composite convex minimization involving self-concordant-like cost functions. (arXiv:1502.01068v2 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1502.01068", "type": "text/html"}], "timestampUsec": "1516686209446247", "comments": [], "summary": {"content": "<p>The self-concordant-like property of a smooth convex function is a new \nanalytical structure that generalizes the self-concordant notion. While a wide \nvariety of important applications feature the self-concordant-like property, \nthis concept has heretofore remained unexploited in convex optimization. To \nthis end, we develop a variable metric framework of minimizing the sum of a \n\"simple\" convex function and a self-concordant-like function. We introduce a \nnew analytic step-size selection procedure and prove that the basic gradient \nalgorithm has improved convergence guarantees as compared to \"fast\" algorithms \nthat rely on the Lipschitz gradient property. Our numerical tests with \nreal-data sets shows that the practice indeed follows the theory. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121ed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1502.01068"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zhuang Ma, Xiaodong Li", "title": "Subspace Perspective on Canonical Correlation Analysis: Dimension Reduction and Minimax Rates. (arXiv:1605.03662v2 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1605.03662", "type": "text/html"}], "timestampUsec": "1516686209446246", "comments": [], "summary": {"content": "<p>Canonical correlation analysis (CCA) is a fundamental statistical tool for \nexploring the correlation structure between two sets of random variables. In \nthis paper, motivated by recent success of applying CCA to learn low \ndimensional representations of high dimensional objects, we propose to quantify \nthe estimation loss of CCA by the excess prediction loss defined through a \nprediction-after-dimension-reduction framework. Such framework suggests viewing \nCCA estimation as estimating the subspaces spanned by the canonical variates. \nInterestedly, the proposed error metrics derived from the excess prediction \nloss turn out to be closely related to the principal angles between the \nsubspaces spanned by the population and sample canonical variates respectively. \n</p> \n<p>We characterize the non-asymptotic minimax rates under the proposed metrics, \nespecially the dependency of the minimax rates on the key quantities including \nthe dimensions, the condition number of the covariance matrices, the canonical \ncorrelations and the eigen-gap, with minimal assumptions on the joint \ncovariance matrix. To the best of our knowledge, this is the first finite \nsample result that captures the effect of the canonical correlations on the \nminimax rates. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121f0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1605.03662"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xingguo Li, Junwei Lu, Raman Arora, Jarvis Haupt, Han Liu, Zhaoran Wang, Tuo Zhao", "title": "Symmetry, Saddle Points, and Global Optimization Landscape of Nonconvex Matrix Factorization. (arXiv:1612.09296v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1612.09296", "type": "text/html"}], "timestampUsec": "1516686209446245", "comments": [], "summary": {"content": "<p>We propose a general theory for studying the \\xl{landscape} of nonconvex \n\\xl{optimization} with underlying symmetric structures \\tz{for a class of \nmachine learning problems (e.g., low-rank matrix factorization, phase \nretrieval, and deep linear neural networks)}. In specific, we characterize the \nlocations of stationary points and the null space of Hessian matrices \\xl{of \nthe objective function} via the lens of invariant groups\\removed{for associated \noptimization problems, including low-rank matrix factorization, phase \nretrieval, and deep linear neural networks}. As a major motivating example, we \napply the proposed general theory to characterize the global \\xl{landscape} of \nthe \\xl{nonconvex optimization in} low-rank matrix factorization problem. In \nparticular, we illustrate how the rotational symmetry group gives rise to \ninfinitely many nonisolated strict saddle points and equivalent global minima \nof the objective function. By explicitly identifying all stationary points, we \ndivide the entire parameter space into three regions: ($\\cR_1$) the region \ncontaining the neighborhoods of all strict saddle points, where the objective \nhas negative curvatures; ($\\cR_2$) the region containing neighborhoods of all \nglobal minima, where the objective enjoys strong convexity along certain \ndirections; and ($\\cR_3$) the complement of the above regions, where the \ngradient has sufficiently large magnitudes. We further extend our result to the \nmatrix sensing problem. Such global landscape implies strong global convergence \nguarantees for popular iterative algorithms with arbitrary initial solutions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121f5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.09296"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Abraham Smith, Paul Bendich, John Harer, Alex Pieloch, Jay Hineman", "title": "Supervised Learning of Labeled Pointcloud Differences via Cover-Tree Entropy Reduction. (arXiv:1702.07959v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1702.07959", "type": "text/html"}], "timestampUsec": "1516686209446244", "comments": [], "summary": {"content": "<p>We introduce a new algorithm, called CDER, for supervised machine learning \nthat merges the multi-scale geometric properties of Cover Trees with the \ninformation-theoretic properties of entropy. CDER applies to a training set of \nlabeled pointclouds embedded in a common Euclidean space. If typical \npointclouds corresponding to distinct labels tend to differ at any scale in any \nsub-region, CDER can identify these differences in (typically) linear time, \ncreating a set of distributional coordinates which act as a feature extraction \nmechanism for supervised learning. We describe theoretical properties and \nimplementation details of CDER, and illustrate its benefits on several \nsynthetic examples. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003612121fd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1702.07959"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Shravan Vasishth, Lena A. J&#xe4;ger, Bruno Nicenboim", "title": "Feature overwriting as a finite mixture process: Evidence from comprehension data. (arXiv:1703.04081v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.04081", "type": "text/html"}], "timestampUsec": "1516686209446243", "comments": [], "summary": {"content": "<p>The ungrammatical sentence \"The key to the cabinets are on the table\" is \nknown to lead to an illusion of grammaticality. As discussed in the \nmeta-analysis by Jaeger et al., 2017, faster reading times are observed at the \nverb are in the agreement-attraction sentence above compared to the equally \nungrammatical sentence \"The key to the cabinet are on the table\". One \nexplanation for this facilitation effect is the feature percolation account: \nthe plural feature on cabinets percolates up to the head noun key, leading to \nthe illusion. An alternative account is in terms of cue-based retrieval (Lewis \n&amp; Vasishth, 2005), which assumes that the non-subject noun cabinets is \nmisretrieved due to a partial feature-match when a dependency completion \nprocess at the auxiliary initiates a memory access for a subject with plural \nmarking. We present evidence for yet another explanation for the observed \nfacilitation. Because the second sentence has two nouns with identical number, \nit is possible that these are, in some proportion of trials, more difficult to \nkeep distinct, leading to slower reading times at the verb in the first \nsentence above; this is the feature overwriting account of Nairne, 1990. We \nshow that the feature overwriting proposal can be implemented as a finite \nmixture process. We reanalysed ten published data-sets, fitting hierarchical \nBayesian mixture models to these data assuming a two-mixture distribution. We \nshow that in nine out of the ten studies, a mixture distribution corresponding \nto feature overwriting furnishes a superior fit over both the feature \npercolation and the cue-based retrieval accounts. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361212201", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.04081"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Stephan Mandt, Matthew D. Hoffman, David M. Blei", "title": "Stochastic Gradient Descent as Approximate Bayesian Inference. (arXiv:1704.04289v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.04289", "type": "text/html"}], "timestampUsec": "1516686209446242", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328f4bb66\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328f4bb66&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Stochastic Gradient Descent with a constant learning rate (constant SGD) \nsimulates a Markov chain with a stationary distribution. With this perspective, \nwe derive several new results. (1) We show that constant SGD can be used as an \napproximate Bayesian posterior inference algorithm. Specifically, we show how \nto adjust the tuning parameters of constant SGD to best match the stationary \ndistribution to a posterior, minimizing the Kullback-Leibler divergence between \nthese two distributions. (2) We demonstrate that constant SGD gives rise to a \nnew variational EM algorithm that optimizes hyperparameters in complex \nprobabilistic models. (3) We also propose SGD with momentum for sampling and \nshow how to adjust the damping coefficient accordingly. (4) We analyze MCMC \nalgorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we \nquantify the approximation errors due to finite learning rates. Finally (5), we \nuse the stochastic process perspective to give a short proof of why Polyak \naveraging is optimal. Based on this idea, we propose a scalable approximate \nMCMC algorithm, the Averaged Stochastic Gradient Sampler. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361212209", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.04289"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Razieh Nabi, Ilya Shpitser", "title": "Fair Inference On Outcomes. (arXiv:1705.10378v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.10378", "type": "text/html"}], "timestampUsec": "1516686209446241", "comments": [], "summary": {"content": "<p>In this paper, we consider the problem of fair statistical inference \ninvolving outcome variables. Examples include classification and regression \nproblems, and estimating treatment effects in randomized trials or \nobservational data. The issue of fairness arises in such problems where some \ncovariates or treatments are \"sensitive,\" in the sense of having potential of \ncreating discrimination. In this paper, we argue that the presence of \ndiscrimination can be formalized in a sensible way as the presence of an effect \nof a sensitive covariate on the outcome along certain causal pathways, a view \nwhich generalizes (Pearl, 2009). A fair outcome model can then be learned by \nsolving a constrained optimization problem. We discuss a number of \ncomplications that arise in classical statistical inference due to this view \nand provide workarounds based on recent work in causal and semi-parametric \ninference. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361212210", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.10378"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Niki Kilbertus, Mateo Rojas-Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, Bernhard Sch&#xf6;lkopf", "title": "Avoiding Discrimination through Causal Reasoning. (arXiv:1706.02744v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.02744", "type": "text/html"}], "timestampUsec": "1516686209446240", "comments": [], "summary": {"content": "<p>Recent work on fairness in machine learning has focused on various \nstatistical discrimination criteria and how they trade off. Most of these \ncriteria are observational: They depend only on the joint distribution of \npredictor, protected attribute, features, and outcome. While convenient to work \nwith, observational criteria have severe inherent limitations that prevent them \nfrom resolving matters of fairness conclusively. \n</p> \n<p>Going beyond observational criteria, we frame the problem of discrimination \nbased on protected attributes in the language of causal reasoning. This \nviewpoint shifts attention from \"What is the right fairness criterion?\" to \n\"What do we want to assume about the causal data generating process?\" Through \nthe lens of causality, we make several contributions. First, we crisply \narticulate why and when observational criteria fail, thus formalizing what was \nbefore a matter of opinion. Second, our approach exposes previously ignored \nsubtleties and why they are fundamental to the problem. Finally, we put forward \nnatural causal non-discrimination criteria and develop algorithms that satisfy \nthem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361212214", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.02744"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sudeepa Roy, Cynthia Rudin, Alexander Volfovsky, Tianyu Wang", "title": "FLAME: A Fast Large-scale Almost Matching Exactly Approach to Causal Inference. (arXiv:1707.06315v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.06315", "type": "text/html"}], "timestampUsec": "1516686209446239", "comments": [], "summary": {"content": "<p>A classical problem in causal inference is that of matching, where treatment \nunits need to be matched to control units. Some of the main challenges in \ndeveloping matching methods arise from the tension among (i) inclusion of as \nmany covariates as possible in defining the matched groups, (ii) having matched \ngroups with enough treated and control units for a valid estimate of Average \nTreatment Effect (ATE) in each group, and (iii) computing the matched pairs \nefficiently for large datasets. In this paper we propose a fast method for \napproximate and exact matching in causal analysis called FLAME (Fast \nLarge-scale Almost Matching Exactly). We define an optimization objective for \nmatch quality, which gives preferences to matching on covariates that can be \nuseful for predicting the outcome while encouraging as many matches as \npossible. FLAME aims to optimize our match quality measure, leveraging \ntechniques that are natural for query processing in the area of database \nmanagement. We provide two implementations of FLAME using SQL queries and \nbit-vector techniques. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361212216", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.06315"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pavel Gurevich, Hannes Stuke", "title": "Learning uncertainty in regression tasks by artificial neural networks. (arXiv:1707.07287v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.07287", "type": "text/html"}], "timestampUsec": "1516686209446238", "comments": [], "summary": {"content": "<p>We suggest a general approach to quantification of different forms of \nuncertainty in regression tasks performed by artificial neural networks. It is \nbased on the simultaneous training of two neural networks with a joint loss \nfunction. One of the networks performs predictions and the other simultaneously \nquantifies the uncertainty of predictions by estimating the locally averaged \nloss of the first one. Unlike in many classical uncertainty quantification \nmethods, the targets are not assumed to be sampled from a probability \ndistribution of an a priori given form. We analyze how the hyperparameters \naffect the learning process and, additionally, show that our method even allows \nfor better predictions compared to standard neural networks without uncertainty \ncounterparts. Finally, we show that particular cases of our approach include \nmaximization of log-likelihood, assuming Gaussian or Laplace noise. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036121221b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.07287"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Wei Chen, Mark Fuge", "title": "Active Expansion Sampling for Learning Feasible Domains in an Unbounded Input Space. (arXiv:1708.07888v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.07888", "type": "text/html"}], "timestampUsec": "1516686209446237", "comments": [], "summary": {"content": "<p>Many engineering problems require identifying feasible domains under implicit \nconstraints. One example is finding acceptable car body styling designs based \non constraints like aesthetics and functionality. Current active-learning based \nmethods learn feasible domains for bounded input spaces. However, we usually \nlack prior knowledge about how to set those input variable bounds. Bounds that \nare too small will fail to cover all feasible domains; while bounds that are \ntoo large will waste query budget. To avoid this problem, we introduce Active \nExpansion Sampling (AES), a method that identifies (possibly disconnected) \nfeasible domains over an unbounded input space. AES progressively expands our \nknowledge of the input space, and uses successive exploitation and exploration \nstages to switch between learning the decision boundary and searching for new \nfeasible domains. We show that AES has a misclassification loss guarantee \nwithin the explored region, independent of the number of iterations or labeled \nsamples. Thus it can be used for real-time prediction of samples' feasibility \nwithin the explored region. We evaluate AES on three test examples and compare \nAES with two adaptive sampling methods -- the Neighborhood-Voronoi algorithm \nand the straddle heuristic -- that operate over fixed input variable bounds. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361212222", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.07888"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ritabrata Dutta, Antonietta Mira, Jukka-Pekka Onnela", "title": "Bayesian Inference of Spreading Processes on Networks. (arXiv:1709.08862v2 [stat.AP] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.08862", "type": "text/html"}], "timestampUsec": "1516686209446236", "comments": [], "summary": {"content": "<p>Infectious diseases are studied to understand their spreading mechanisms, to \nevaluate control strategies and to predict the risk and course of future \noutbreaks. Because people only interact with a small number of individuals, and \nbecause the structure of these interactions matters for spreading processes, \nthe pairwise relationships between individuals in a population can be usefully \nrepresented by a network. Although the underlying processes of transmission are \ndifferent, the network approach can be used to study the spread of pathogens in \na contact network or the spread of rumors in an online social network. We study \nsimulated simple and complex epidemics on synthetic networks and on two \nempirical networks, a social / contact network in an Indian village and an \nonline social network in the U.S. Our goal is to learn simultaneously about the \nspreading process parameters and the source node (first infected node) of the \nepidemic, given a fixed and known network structure, and observations about \nstate of nodes at several points in time. Our inference scheme is based on \napproximate Bayesian computation (ABC), an inference technique for complex \nmodels with likelihood functions that are either expensive to evaluate or \nanalytically intractable. ABC enables us to adopt a Bayesian approach to the \nproblem despite the posterior distribution being very complex. Our method is \nagnostic about the topology of the network and the nature of the spreading \nprocess. It generally performs well and, somewhat counter-intuitively, the \ninference problem appears to be easier on more heterogeneous network \ntopologies, which enhances its future applicability to real-world settings \nwhere few networks have homogeneous topologies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036121222c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.08862"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Srikrishna Varadarajan, Muktabh Mayank Srivastava, Monika Grewal, Pulkit Kumar", "title": "Anatomical labeling of brain CT scan anomalies using multi-context nearest neighbor relation networks. (arXiv:1710.09180v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.09180", "type": "text/html"}], "timestampUsec": "1516686209446235", "comments": [], "summary": {"content": "<p>This work is an endeavor to develop a deep learning methodology for automated \nanatomical labeling of a given region of interest (ROI) in brain computed \ntomography (CT) scans. We combine both local and global context to obtain a \nrepresentation of the ROI. We then use Relation Networks (RNs) to predict the \ncorresponding anatomy of the ROI based on its relationship score for each \nclass. Further, we propose a novel strategy employing nearest neighbors \napproach for training RNs. We train RNs to learn the relationship of the target \nROI with the joint representation of its nearest neighbors in each class \ninstead of all data-points in each class. The proposed strategy leads to better \ntraining of RNs along with increased performance as compared to training \nbaseline RN network. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036121223b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.09180"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ching-An Cheng, Byron Boots", "title": "Variational Inference for Gaussian Process Models with Linear Complexity. (arXiv:1711.10127v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10127", "type": "text/html"}], "timestampUsec": "1516686209446234", "comments": [], "summary": {"content": "<p>Large-scale Gaussian process inference has long faced practical challenges \ndue to time and space complexity that is superlinear in dataset size. While \nsparse variational Gaussian process models are capable of learning from \nlarge-scale data, standard strategies for sparsifying the model can prevent the \napproximation of complex functions. In this work, we propose a novel \nvariational Gaussian process model that decouples the representation of mean \nand covariance functions in reproducing kernel Hilbert space. We show that this \nnew parametrization generalizes previous models. Furthermore, it yields a \nvariational inference problem that can be solved by stochastic gradient ascent \nwith time and space complexity that is only linear in the number of mean \nfunction parameters, regardless of the choice of kernels, likelihoods, and \ninducing points. This strategy makes the adoption of large-scale expressive \nGaussian process models possible. We run several experiments on regression \ntasks and show that this decoupled approach greatly outperforms previous sparse \nvariational Gaussian process inference procedures. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361212245", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10127"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Giambattista Parascandolo, Mateo Rojas-Carulla, Niki Kilbertus, Bernhard Sch&#xf6;lkopf", "title": "Learning Independent Causal Mechanisms. (arXiv:1712.00961v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00961", "type": "text/html"}], "timestampUsec": "1516686209446233", "comments": [], "summary": {"content": "<p>Independent causal mechanisms are a central concept in the study of causality \nwith implications for machine learning tasks. In this work we develop an \nalgorithm to recover a set of (inverse) independent mechanisms relating a \ndistribution transformed by the mechanisms to a reference distribution. The \napproach is fully unsupervised and based on a set of experts that compete for \ndata to specialize and extract the mechanisms. We test and analyze the proposed \nmethod on a series of experiments based on image transformations. Each expert \nsuccessfully maps a subset of the transformed data to the original domain, and \nthe learned mechanisms generalize to other domains. We discuss implications for \ndomain transfer and links to recent trends in generative modeling. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000361212248", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00961"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Salman Salloum, Yulin He, Joshua Zhexue Huang, Xiaoliang Zhang, Tamer Z. Emara, Chenghao Wei, Heping He", "title": "A Random Sample Partition Data Model for Big Data Analysis. (arXiv:1712.04146v2 [cs.DC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.04146", "type": "text/html"}], "timestampUsec": "1516686209446232", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328f4be5f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328f4be5f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Big data sets must be carefully partitioned into statistically similar data \nsubsets that can be used as representative samples for big data analysis tasks. \nIn this paper, we propose the random sample partition (RSP) data model to \nrepresent a big data set as a set of non-overlapping data subsets, called RSP \ndata blocks, where each RSP data block has a probability distribution similar \nto the whole big data set. Under this data model, efficient block level \nsampling is used to randomly select RSP data blocks, replacing expensive record \nlevel sampling to select sample data from a big distributed data set on a \ncomputing cluster. We show how RSP data blocks can be employed to estimate \nstatistics of a big data set and build models which are equivalent to those \nbuilt from the whole big data set. In this approach, analysis of a big data set \nbecomes analysis of few RSP data blocks which have been generated in advance on \nthe computing cluster. Therefore, the new method for data analysis based on RSP \ndata blocks is scalable to big data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686209446", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036121224e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04146"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yu Shi, Fangqiu Han, Xinran He, Carl Yang, Jie Luo, Jiawei Han", "title": "mvn2vec: Preservation and Collaboration in Multi-View Network Embedding. (arXiv:1801.06597v1 [cs.SI])", "alternate": [{"href": "http://arxiv.org/abs/1801.06597", "type": "text/html"}], "timestampUsec": "1516686007583912", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328f8a43c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328f8a43c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Multi-view networks are ubiquitous in real-world applications. In order to \nextract knowledge or business value, it is of interest to transform such \nnetworks into representations that are easily machine-actionable. Meanwhile, \nnetwork embedding has emerged as an effective approach to generate distributed \nnetwork representations. Therefore, we are motivated to study the problem of \nmulti-view network embedding, with a focus on the characteristics that are \nspecific and important in embedding this type of networks. In our practice of \nembedding real-world multi-view networks, we identify two such characteristics, \nwhich we refer to as preservation and collaboration. We then explore the \nfeasibility of achieving better embedding quality by simultaneously modeling \npreservation and collaboration, and propose the mvn2vec algorithms. With \nexperiments on a series of synthetic datasets, an internal Snapchat dataset, \nand two public datasets, we further confirm the presence and importance of \npreservation and collaboration. These experiments also demonstrate that better \nembedding can be obtained by simultaneously modeling the two characteristics, \nwhile not over-complicating the model or requiring additional supervision. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686007584", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036120be0c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06597"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Liangzhen Lai, Naveen Suda, Vikas Chandra", "title": "CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs. (arXiv:1801.06601v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.06601", "type": "text/html"}], "timestampUsec": "1516686007583911", "comments": [], "summary": {"content": "<p>Deep Neural Networks are becoming increasingly popular in always-on IoT edge \ndevices performing data analytics right at the source, reducing latency as well \nas energy consumption for data communication. This paper presents CMSIS-NN, \nefficient kernels developed to maximize the performance and minimize the memory \nfootprint of neural network (NN) applications on Arm Cortex-M processors \ntargeted for intelligent IoT edge devices. Neural network inference based on \nCMSIS-NN kernels achieves 4.6X improvement in runtime/throughput and 4.9X \nimprovement in energy efficiency. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686007584", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036120be16", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06601"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hongxin Wang, Jigen Peng, Shigang Yue", "title": "A Directionally Selective Small Target Motion Detecting Visual Neural Network in Cluttered Backgrounds. (arXiv:1801.06687v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.06687", "type": "text/html"}], "timestampUsec": "1516686007583910", "comments": [], "summary": {"content": "<p>Discriminating targets moving against a cluttered background is a huge \nchallenge, let alone detecting a target as small as one or a few pixels and \ntracking it in flight. In the fly's visual system, a class of specific neurons, \ncalled small target motion detectors (STMDs), have been identified as showing \nexquisite selectivity for small target motion. Some of the STMDs have also \ndemonstrated directional selectivity which means these STMDs respond strongly \nonly to their preferred motion direction. Directional selectivity is an \nimportant property of these STMD neurons which could contribute to tracking \nsmall targets such as mates in flight. However, little has been done on \nsystematically modeling these directional selective STMD neurons. In this \npaper, we propose a directional selective STMD-based neural network (DSTMD) for \nsmall target detection in a cluttered background. In the proposed neural \nnetwork, a new correlation mechanism is introduced for direction selectivity \nvia correlating signals relayed from two pixels. Then, a lateral inhibition \nmechanism is implemented on the spatial field for size selectivity of STMD \nneurons. Extensive experiments showed that the proposed neural network not only \nis in accord with current biological findings, i.e. showing directional \npreferences, but also worked reliably in detecting small targets against \ncluttered backgrounds. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686007584", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036120be24", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06687"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Iulian V. Serban, Chinnadhurai Sankar, Mathieu Germain, Saizheng Zhang, Zhouhan Lin, Sandeep Subramanian, Taesup Kim, Michael Pieper, Sarath Chandar, Nan Rosemary Ke, Sai Rajeswar, Alexandre de Brebisson, Jose M. R. Sotelo, Dendi Suhubdy, Vincent Michalski, Alexandre Nguyen, Joelle Pineau, Yoshua Bengio", "title": "A Deep Reinforcement Learning Chatbot (Short Version). (arXiv:1801.06700v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1801.06700", "type": "text/html"}], "timestampUsec": "1516686007583909", "comments": [], "summary": {"content": "<p>We present MILABOT: a deep reinforcement learning chatbot developed by the \nMontreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize \ncompetition. MILABOT is capable of conversing with humans on popular small talk \ntopics through both speech and text. The system consists of an ensemble of \nnatural language generation and retrieval models, including neural network and \ntemplate-based models. By applying reinforcement learning to crowdsourced data \nand real-world user interactions, the system has been trained to select an \nappropriate response from the models in its ensemble. The system has been \nevaluated through A/B testing with real-world users, where it performed \nsignificantly better than other systems. The results highlight the potential of \ncoupling ensemble systems with deep reinforcement learning as a fruitful path \nfor developing real-world, open-domain conversational agents. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686007584", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036120be3a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06700"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hongxin Wang, Jigen Peng, Shigang Yue", "title": "An Improved LPTC Neural Model for Background Motion Direction Estimation. (arXiv:1801.06976v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.06976", "type": "text/html"}], "timestampUsec": "1516686007583908", "comments": [], "summary": {"content": "<p>A class of specialized neurons, called lobula plate tangential cells (LPTCs) \nhas been shown to respond strongly to wide-field motion. The classic model, \nelementary motion detector (EMD) and its improved model, two-quadrant detector \n(TQD) have been proposed to simulate LPTCs. Although EMD and TQD can percept \nbackground motion, their outputs are so cluttered that it is difficult to \ndiscriminate actual motion direction of the background. In this paper, we \npropose a max operation mechanism to model a newly-found transmedullary neuron \nTm9 whose physiological properties do not map onto EMD and TQD. This proposed \nmax operation mechanism is able to improve the detection performance of TQD in \ncluttered background by filtering out irrelevant motion signals. We will \ndemonstrate the functionality of this proposed mechanism in wide-field motion \nperception. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686007584", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036120be48", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06976"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Esra&#x27;a Alkafaween, Ahmad B. A. Hassanat", "title": "Improving TSP Solutions Using GA with a New Hybrid Mutation Based on Knowledge and Randomness. (arXiv:1801.07233v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.07233", "type": "text/html"}], "timestampUsec": "1516686007583907", "comments": [], "summary": {"content": "<p>Genetic algorithm (GA) is an efficient tool for solving optimization problems \nby evolving solutions, as it mimics the Darwinian theory of natural evolution. \nThe mutation operator is one of the key success factors in GA, as it is \nconsidered the exploration operator of GA. Various mutation operators exist to \nsolve hard combinatorial problems such as the TSP. In this paper, we propose a \nhybrid mutation operator called \"IRGIBNNM\", this mutation is a combination of \ntwo existing mutations, a knowledge-based mutation, and a random-based \nmutation. We also improve the existing \"select best mutation\" strategy using \nthe proposed mutation. We conducted several experiments on twelve benchmark \nSymmetric traveling salesman problem (STSP) instances. The results of our \nexperiments show the efficiency of the proposed mutation, particularly when we \nuse it with some other mutations. Keyword: Knowledge-based mutation, Inversion \nmutation, Slide mutation, RGIBNNM, SBM. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686007584", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036120be63", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07233"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Dietmar Volz", "title": "A Note on Topology Preservation in Classification, and the Construction of a Universal Neuron Grid. (arXiv:1308.1603v3 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1308.1603", "type": "text/html"}], "timestampUsec": "1516686007583906", "comments": [], "summary": {"content": "<p>It will be shown that according to theorems of K. Menger, every neuron grid \nif identified with a curve is able to preserve the adopted qualitative \nstructure of a data space. Furthermore, if this identification is made, the \nneuron grid structure can always be mapped to a subset of a universal neuron \ngrid which is constructable in three space dimensions. Conclusions will be \ndrawn for established neuron grid types as well as neural fields. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686007584", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036120be80", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1308.1603"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Necati Alp Muyesser, Kyle Dunovan, Timothy Verstynen", "title": "Learning model-based strategies in simple environments with hierarchical q-networks. (arXiv:1801.06689v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.06689", "type": "text/html"}], "timestampUsec": "1516686007583904", "comments": [], "summary": {"content": "<p>Recent advances in deep learning have allowed artificial agents to rival \nhuman-level performance on a wide range of complex tasks; however, the ability \nof these networks to learn generalizable strategies remains a pressing \nchallenge. This critical limitation is due in part to two factors: the opaque \ninformation representation in deep neural networks and the complexity of the \ntask environments in which they are typically deployed. Here we propose a novel \nHierarchical Q-Network (HQN) motivated by theories of the hierarchical \norganization of the human prefrontal cortex, that attempts to identify lower \ndimensional patterns in the value landscape that can be exploited to construct \nan internal model of rules in simple environments. We draw on combinatorial \ngames, where there exists a single optimal strategy for winning that \ngeneralizes across other features of the game, to probe the strategy \ngeneralization of the HQN and other reinforcement learning (RL) agents using \nvariations of Wythoff's game. Traditional RL approaches failed to reach \nsatisfactory performance on variants of Wythoff's Game; however, the HQN \nlearned heuristic-like strategies that generalized across changes in board \nconfiguration. More importantly, the HQN allowed for transparent inspection of \nthe agent's internal model of the game following training. Our results show how \na biologically inspired hierarchical learner can facilitate learning abstract \nrules to promote robust and flexible action policies in simplified training \nenvironments with clearly delineated optimal strategies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686007584", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036120be9f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06689"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Fred Hohman, Minsuk Kahng, Robert Pienta, Duen Horng Chau", "title": "Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers. (arXiv:1801.06889v1 [cs.HC])", "alternate": [{"href": "http://arxiv.org/abs/1801.06889", "type": "text/html"}], "timestampUsec": "1516686007583903", "comments": [], "summary": {"content": "<p>Deep learning has recently seen rapid development and significant attention \ndue to its state-of-the-art performance on previously-thought hard problems. \nHowever, because of the innate complexity and nonlinear structure of deep \nneural networks, the underlying decision making processes for why these models \nare achieving such high performance are challenging and sometimes mystifying to \ninterpret. As deep learning spreads across domains, it is of paramount \nimportance that we equip users of deep learning with tools for understanding \nwhen a model works correctly, when it fails, and ultimately how to improve its \nperformance. Standardized toolkits for building neural networks have helped \ndemocratize deep learning; visual analytics systems have now been developed to \nsupport model explanation, interpretation, debugging, and improvement. We \npresent a survey of the role of visual analytics in deep learning research, \nnoting its short yet impactful history and summarize the state-of-the-art using \na human-centered interrogative framework, focusing on the Five W's and How \n(Why, Who, What, How, When, and Where), to thoroughly summarize deep learning \nvisual analytics research. We conclude by highlighting research directions and \nopen research problems. This survey helps new researchers and practitioners in \nboth visual analytics and deep learning to quickly learn key aspects of this \nyoung and rapidly growing body of research, whose impact spans a diverse range \nof domains. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686007584", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036120beaa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06889"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Girish Joshi, Girish Chowdhary", "title": "Cross-Domain Transfer in Reinforcement Learning using Target Apprentice. (arXiv:1801.06920v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.06920", "type": "text/html"}], "timestampUsec": "1516686007583902", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328f8aa00\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328f8aa00&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we present a new approach to Transfer Learning (TL) in \nReinforcement Learning (RL) for cross-domain tasks. Many of the available \ntechniques approach the transfer architecture as a method of speeding up the \ntarget task learning. We propose to adapt and reuse the mapped source task \noptimal-policy directly in related domains. We show the optimal policy from a \nrelated source task can be near optimal in target domain provided an adaptive \npolicy accounts for the model error between target and source. The main benefit \nof this policy augmentation is generalizing policies across multiple related \ndomains without having to re-learn the new tasks. Our results show that this \narchitecture leads to better sample efficiency in the transfer, reducing sample \ncomplexity of target task learning to target apprentice learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686007584", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036120beae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06920"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Feng Li, Sibo Yang, Huanhuan Huang, Wei Wu", "title": "Extreme Learning Machine with Local Connections. (arXiv:1801.06975v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.06975", "type": "text/html"}], "timestampUsec": "1516686007583901", "comments": [], "summary": {"content": "<p>This paper is concerned with the sparsification of the input-hidden weights \nof ELM (Extreme Learning Machine). For ordinary feedforward neural networks, \nthe sparsification is usually done by introducing certain regularization \ntechnique into the learning process of the network. But this strategy can not \nbe applied for ELM, since the input-hidden weights of ELM are supposed to be \nrandomly chosen rather than to be learned. To this end, we propose a modified \nELM, called ELM-LC (ELM with local connections), which is designed for the \nsparsification of the input-hidden weights as follows: The hidden nodes and the \ninput nodes are divided respectively into several corresponding groups, and an \ninput node group is fully connected with its corresponding hidden node group, \nbut is not connected with any other hidden node group. As in the usual ELM, the \nhidden-input weights are randomly given, and the hidden-output weights are \nobtained through a least square learning. In the numerical simulations on some \nbenchmark problems, the new ELM-CL behaves better than the traditional ELM. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686007584", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036120beb4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06975"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chao Yan, Bo Li, Yevgeniy Vorobeychik, Aron Laszka, Daniel Fabbri, Bradley Malin", "title": "Get Your Workload in Order: Game Theoretic Prioritization of Database Auditing. (arXiv:1801.07215v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.07215", "type": "text/html"}], "timestampUsec": "1516686007583900", "comments": [], "summary": {"content": "<p>For enhancing the privacy protections of databases, where the increasing \namount of detailed personal data is stored and processed, multiple mechanisms \nhave been developed, such as audit logging and alert triggers, which notify \nadministrators about suspicious activities; however, the two main limitations \nin common are: 1) the volume of such alerts is often substantially greater than \nthe capabilities of resource-constrained organizations, and 2) strategic \nattackers may disguise their actions or carefully choosing which records they \ntouch, making incompetent the statistical detection models. For solving them, \nwe introduce a novel approach to database auditing that explicitly accounts for \nadversarial behavior by 1) prioritizing the order in which types of alerts are \ninvestigated and 2) providing an upper bound on how much resource to allocate \nfor each type. We model the interaction between a database auditor and \npotential attackers as a Stackelberg game in which the auditor chooses an \nauditing policy and attackers choose which records to target. A corresponding \napproach combining linear programming, column generation, and heuristic search \nis proposed to derive an auditing policy. For testing the policy-searching \nperformance, a publicly available credit card application dataset are adopted, \non which it shows that our methods produce high-quality mixed strategies as \ndatabase audit policies, and our general approach significantly outperforms \nnon-game-theoretic baselines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686007584", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036120beba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07215"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston", "title": "Personalizing Dialogue Agents: I have a dog, do you have pets too?. (arXiv:1801.07243v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.07243", "type": "text/html"}], "timestampUsec": "1516686007583899", "comments": [], "summary": {"content": "<p>Chit-chat models are known to have several problems: they lack specificity, \ndo not display a consistent personality and are often not very captivating. In \nthis work we present the task of making chit-chat more engaging by conditioning \non profile information. We collect data and train models to (i) condition on \ntheir given profile information; and (ii) information about the person they are \ntalking to, resulting in improved dialogues, as measured by next utterance \nprediction. Since (ii) is initially unknown our model is trained to engage its \npartner with personal topics, and we show the resulting dialogue can be used to \npredict profile information about the interlocutors. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686007584", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036120bec4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07243"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Garrett Warnell, Nicholas Waytowich, Vernon Lawhern, Peter Stone", "title": "Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces. (arXiv:1709.10163v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.10163", "type": "text/html"}], "timestampUsec": "1516686007583898", "comments": [], "summary": {"content": "<p>While recent advances in deep reinforcement learning have allowed autonomous \nlearning agents to succeed at a variety of complex tasks, existing algorithms \ngenerally require a lot of training data. One way to increase the speed at \nwhich agents are able to learn to perform tasks is by leveraging the input of \nhuman trainers. Although such input can take many forms, real-time, \nscalar-valued feedback is especially useful in situations where it proves \ndifficult or impossible for humans to provide expert demonstrations. Previous \napproaches have shown the usefulness of human input provided in this fashion \n(e.g., the TAMER framework), but they have thus far not considered \nhigh-dimensional state spaces or employed the use of deep learning. In this \npaper, we do both: we propose Deep TAMER, an extension of the TAMER framework \nthat leverages the representational power of deep neural networks in order to \nlearn complex tasks in just a short amount of time with a human trainer. We \ndemonstrate Deep TAMER's success by using it and just 15 minutes of \nhuman-provided feedback to train an agent that performs better than humans on \nthe Atari game of Bowling - a task that has proven difficult for even \nstate-of-the-art reinforcement learning methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686007584", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036120bec9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.10163"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexandre Gilotte, Cl&#xe9;ment Calauz&#xe8;nes, Thomas Nedelec, Alexandre Abraham, Simon Doll&#xe9;", "title": "Offline A/B testing for Recommender Systems. (arXiv:1801.07030v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.07030", "type": "text/html"}], "timestampUsec": "1516686007583895", "comments": [], "summary": {"content": "<p>Before A/B testing online a new version of a recommender system, it is usual \nto perform some offline evaluations on historical data. We focus on evaluation \nmethods that compute an estimator of the potential uplift in revenue that could \ngenerate this new technology. It helps to iterate faster and to avoid losing \nmoney by detecting poor policies. These estimators are known as counterfactual \nor off-policy estimators. We show that traditional counterfactual estimators \nsuch as capped importance sampling and normalised importance sampling are \nexperimentally not having satisfying bias-variance compromises in the context \nof personalised product recommendation for online advertising. We propose two \nvariants of counterfactual estimates with different modelling of the bias that \nprove to be accurate in real-world conditions. We provide a benchmark of these \nestimators by showing their correlation with business metrics observed by \nrunning online A/B tests on a commercial recommender system. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686007584", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036120bed8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07030"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Eric Alcaide", "title": "E-swish: Adjusting Activations to Different Network Depths. (arXiv:1801.07145v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.07145", "type": "text/html"}], "timestampUsec": "1516686007583894", "comments": [], "summary": {"content": "<p>Activation functions have a notorious impact on neural networks on both \ntraining and testing the models against the desired problem. Currently, the \nmost used activation function is the Rectified Linear Unit (ReLU). This paper \nintroduces a new and novel activation function, closely related with the new \nactivation $Swish = x * sigmoid(x)$ (Ramachandran et al., 2017) which \ngeneralizes it. We call the new activation $E-swish = \\beta x * sigmoid(x)$. We \nshow that E-swish outperforms many other well-known activations including both \nReLU and Swish. For example, using E-swish provided 1.5% and 4.6% accuracy \nimprovements on Cifar10 and Cifar100 respectively for the WRN 10-2 when \ncompared to ReLU and 0.35% and 0.6% respectively when compared to Swish. The \ncode to reproduce all our experiments can be found at \nhttps://github.com/EricAlcaide/E-swish \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516686007584", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000036120bedb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.07145"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Leonard Johard, Lukas Breitwieser, Alberto Di Meglio, Marco Manca, Manuel Mazzara, Max Talanov", "title": "The BioDynaMo Project: a platform for computer simulations of biological dynamics. (arXiv:1608.01818v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1608.01818", "type": "text/html"}], "timestampUsec": "1516599274251400", "comments": [], "summary": {"content": "<p>This paper is a brief update on developments in the BioDynaMo project, a new \nplatform for computer simulations for biological research. We will discuss the \nnew capabilities of the simulator, important new concepts simulation \nmethodology as well as its numerous applications to the computational biology \nand nanoscience communities. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516599274251", "annotations": [], "published": 1516599275, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605f7c6b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1608.01818"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shaika Chowdhury, Chenwei Zhang, Philip S. Yu", "title": "Multi-Task Pharmacovigilance Mining from Social Media Posts. (arXiv:1801.06294v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.06294", "type": "text/html"}], "timestampUsec": "1516599274251399", "comments": [], "summary": {"content": "<p>Social media has grown to be a crucial information source for \npharmacovigilance studies where an increasing number of people post adverse \nreactions to medical drugs that are previously unreported. Aiming to \neffectively monitor various aspects of Adverse Drug Reactions (ADRs) from \ndiversely expressed social medical posts, we propose a multi-task neural \nnetwork framework that learns several tasks associated with ADR monitoring with \ndifferent levels of supervisions collectively. Besides being able to correctly \nclassify ADR posts and accurately extract ADR mentions from online posts, the \nproposed framework is also able to further understand reasons for which the \ndrug is being taken, known as 'indication', from the given social media post. A \ncoverage-based attention mechanism is adopted in our framework to help the \nmodel properly identify 'phrasal' ADRs and Indications that are attentive to \nmultiple words in a post. Our framework is applicable in situations where \nlimited parallel data for different pharmacovigilance tasks are available.We \nevaluate the proposed framework on real-world Twitter datasets, where the \nproposed model outperforms the state-of-the-art alternatives of each individual \ntask consistently. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516599274251", "annotations": [], "published": 1516599275, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605f7c7a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06294"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "He-Liang Huang, Xi-Lin Wang, Peter P. Rohde, Yi-Han Luo, You-Wei Zhao, Chang Liu, Li Li, Nai-Le Liu, Chao-Yang Lu, Jian-Wei Pan", "title": "Demonstration of Topological Data Analysis on a Quantum Processor. (arXiv:1801.06316v1 [quant-ph])", "alternate": [{"href": "http://arxiv.org/abs/1801.06316", "type": "text/html"}], "timestampUsec": "1516599274251398", "comments": [], "summary": {"content": "<p>Topological data analysis offers a robust way to extract useful information \nfrom noisy, unstructured data by identifying its underlying structure. \nRecently, an efficient quantum algorithm was proposed [Lloyd, Garnerone, \nZanardi, Nat. Commun. 7, 10138 (2016)] for calculating Betti numbers of data \npoints -- topological features that count the number of topological holes of \nvarious dimensions in a scatterplot. Here, we implement a proof-of-principle \ndemonstration of this quantum algorithm by employing a six-photon quantum \nprocessor to successfully analyze the topological features of Betti numbers of \na network including three data points, providing new insights into data \nanalysis in the era of quantum computing. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516599274251", "annotations": [], "published": 1516599275, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605f7c81", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06316"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Linyuan Gong, Ruyi Ji", "title": "What Does a TextCNN Learn?. (arXiv:1801.06287v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.06287", "type": "text/html"}], "timestampUsec": "1516599274251395", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328f8afaf\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328f8afaf&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>TextCNN, the convolutional neural network for text, is a useful deep learning \nalgorithm for sentence classification tasks such as sentiment analysis and \nquestion classification. However, neural networks have long been known as black \nboxes because interpreting them is a challenging task. Researchers have \ndeveloped several tools to understand a CNN for image classification by deep \nvisualization, but research about deep TextCNNs is still insufficient. In this \npaper, we are trying to understand what a TextCNN learns on two classical NLP \ndatasets. Our work focuses on functions of different convolutional kernels and \ncorrelations between convolutional kernels. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516599274251", "annotations": [], "published": 1516599275, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605f7c96", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06287"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rie Johnson, Tong Zhang", "title": "Composite Functional Gradient Learning of Generative Adversarial Models. (arXiv:1801.06309v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.06309", "type": "text/html"}], "timestampUsec": "1516599274251394", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328fd0026\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328fd0026&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Generative adversarial networks (GAN) have become popular for generating data \nthat mimic observations by learning a suitable variable transformation from a \nrandom variable. However, empirically, GAN is known to suffer from instability. \nAlso, the theory provided based on the minimax optimization formulation of GAN \ncannot explain the widely-used practical procedure that uses the so-called logd \ntrick. This paper provides a different theoretical foundation for generative \nadversarial methods which does not rely on the minimax formulation. We show \nthat with a strong discriminator, it is possible to learn a good variable \ntransformation via functional gradient learning, which updates the functional \ndefinition of a generator model, instead of updating only the model parameters \nas in GAN. The theory guarantees that the learned generator improves the \nKL-divergence between the probability distributions of real data and generated \ndata after each functional gradient step, until the KL-divergence converges to \nzero. This new point of view leads to enhanced stable procedures for training \ngenerative models that can utilize arbitrary learning algorithms. It also gives \na new theoretical insight into the original GAN procedure both with and without \nthe logd trick. Empirical results are shown on image generation to illustrate \nthe effectiveness of our new method. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516599274251", "annotations": [], "published": 1516599275, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605f7c9c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06309"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nikolaus Mayer, Eddy Ilg, Philipp Fischer, Caner Hazirbas, Daniel Cremers, Alexey Dosovitskiy, Thomas Brox", "title": "What Makes Good Synthetic Training Data for Learning Disparity and Optical Flow Estimation?. (arXiv:1801.06397v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.06397", "type": "text/html"}], "timestampUsec": "1516599274251393", "comments": [], "summary": {"content": "<p>The finding that very large networks can be trained efficiently and reliably \nhas led to a paradigm shift in computer vision from engineered solutions to \nlearning formulations. As a result, the research challenge shifts from devising \nalgorithms to creating suitable and abundant training data for supervised \nlearning. How to efficiently create such training data? The dominant data \nacquisition method in visual recognition is based on web data and manual \nannotation. Yet, for many computer vision problems, such as stereo or optical \nflow estimation, this approach is not feasible because humans cannot manually \nenter a pixel-accurate flow field. In this paper, we promote the use of \nsynthetically generated data for the purpose of training deep networks on such \ntasks.We suggest multiple ways to generate such data and evaluate the influence \nof dataset properties on the performance and generalization properties of the \nresulting networks. We also demonstrate the benefit of learning schedules that \nuse different types of data at selected stages of the training process. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516599274251", "annotations": [], "published": 1516599275, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605f7ca5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06397"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yuhao Zhu, Matthew Mattina, Paul Whatmough", "title": "Mobile Machine Learning Hardware at ARM: A Systems-on-Chip (SoC) Perspective. (arXiv:1801.06274v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.06274", "type": "text/html"}], "timestampUsec": "1516598532242793", "comments": [], "summary": {"content": "<p>Machine learning is playing an increasingly significant role in emerging \nmobile application domains such as AR/VR, ADAS, etc. Accordingly, hardware \narchitects have designed customized hardware for machine learning algorithms, \nespecially neural networks, to improve compute efficiency. However, machine \nlearning is typically just one processing stage in complex end-to-end \napplications, which involve multiple components in a mobile Systems-on-a-chip \n(SoC). Focusing on just ML accelerators loses bigger optimization opportunity \nat the system (SoC) level. This paper argues that hardware architects should \nexpand the optimization scope to the entire SoC. We demonstrate one particular \ncase-study in the domain of continuous computer vision where camera sensor, \nimage signal processor (ISP), memory, and NN accelerator are synergistically \nco-designed to achieve optimal system-level efficiency. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a2c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06274"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ido Freeman, Lutz Roese-Koerner, Anton Kummert", "title": "EffNet: An Efficient Structure for Convolutional Neural Networks. (arXiv:1801.06434v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.06434", "type": "text/html"}], "timestampUsec": "1516598532242792", "comments": [], "summary": {"content": "<p>With the ever increasing application of Convolutional Neural Networks to \ncostumer products the need emerges for models which can efficiently run on \nembedded, mobile hardware. Slimmer models have therefore become a hot research \ntopic with multiple different approaches which vary from binary networks to \nrevised convolution layers. We offer our contribution to the latter and propose \na novel convolution block which significantly reduces the computational burden \nwhile surpassing the current state-of-the-art. Our model, dubbed EffNet, is \noptimised for models which are slim to begin with and is created to tackle \nissues in existing models such as MobileNet and ShuffleNet. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a30", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06434"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Matei Mancas, Christian Frisson, Jo&#xeb;lle Tilmanne, Nicolas d&#x27;Alessandro, Petr Barborka, Furkan Bayansar, Francisco Bernard, Rebecca Fiebrink, Alexis Heloir, Edgar Hemery, Sohaib Laraba, Alexis Moinet, Fabrizio Nunnari, Thierry Ravet, Lo&#xef;c Reboursi&#xe8;re, Alvaro Sarasua, Micka&#xeb;l Tits, No&#xe9; Tits, Fran&#xe7;ois Zaj&#xe9;ga, Paolo Alborno, Ksenia Kolykhalova, Emma Frid, Damiano Malafronte, Lisanne Huis in&#x27;t Veld, H&#xfc;seyin Cakmak, Kevin El Haddad, Nicolas Riche, Julien Leroy, Pierre Marighetto, Bekir Berker T&#xfc;rker, Hossein Khaki, Roberto Pulisci, Emer Gilmartin, Fasih Haider, K&#xfc;bra Cengiz, Martin Sulir, Ilaria Torre, Shabbir Marzban, Ramazan Yaz&#x131;c&#x131;, Furkan Burak B&#xe2;gc&#x131;, Vedat Gazi K&#x131;l&#x131;, Hilal Sezer, Sena B&#xfc;sra Yenge, et al. (31 additional authors not shown)", "title": "Proceedings of eNTERFACE 2015 Workshop on Intelligent Interfaces. (arXiv:1801.06349v1 [cs.HC])", "alternate": [{"href": "http://arxiv.org/abs/1801.06349", "type": "text/html"}], "timestampUsec": "1516598532242791", "comments": [], "summary": {"content": "<p>The 11th Summer Workshop on Multimodal Interfaces eNTERFACE 2015 was hosted \nby the Numediart Institute of Creative Technologies of the University of Mons \nfrom August 10th to September 2015. During the four weeks, students and \nresearchers from all over the world came together in the Numediart Institute of \nthe University of Mons to work on eight selected projects structured around \nintelligent interfaces. Eight projects were selected and their reports are \nshown here. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a32", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06349"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Chen Liang, Jianbo Ye, Han Zhao, Bart Pursel, C. Lee Giles", "title": "Active Learning of Strict Partial Orders: A Case Study on Concept Prerequisite Relations. (arXiv:1801.06481v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.06481", "type": "text/html"}], "timestampUsec": "1516598532242790", "comments": [], "summary": {"content": "<p>Strict partial order is a mathematical structure commonly seen in relational \ndata. One obstacle to extracting such type of relations at scale is the lack of \nlarge-scale labels for building effective data-driven solutions. We develop an \nactive learning framework for mining such relations subject to a strict order. \nOur approach incorporates relational reasoning not only in finding new \nunlabeled pairs whose labels can be deduced from an existing label set, but \nalso in devising new query strategies that consider the relational structure of \nlabels. Our experiments on concept prerequisite relations show our proposed \nframework can substantially improve the classification performance with the \nsame query budget compared to other baseline approaches. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a34", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06481"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Doo Re Song, Chuanyu Yang, Christopher McGreavy, Zhibin Li", "title": "Recurrent Network-based Deterministic Policy Gradient for Solving Bipedal Walking Challenge on Rugged Terrains. (arXiv:1710.02896v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.02896", "type": "text/html"}], "timestampUsec": "1516598532242788", "comments": [], "summary": {"content": "<p>This paper presents the learning algorithm based on the Recurrent \nNetwork-based Deterministic Policy Gradient. The Long-Short Term Memory is \nutilized to enable the Partially Observed Markov Decision Process framework. \nThe novelty are improvements of LSTM networks: update of multi-step temporal \ndifference, removal of backpropagation through time on actor, initialisation of \nhidden state using past trajectory scanning, and injection of external \nexperiences learned by other agents. Our methods benefit the reinforcement \nlearning agent on inferring the desirable action by referring the trajectories \nof both past observations and actions. The proposed algorithm was implemented \nto solve the Bipedal-Walker challenge in OpenAI virtual environment where only \npartial state information is available. The validation on the extremely rugged \nterrain demonstrates the effectiveness of the proposed algorithm by achieving a \nnew record of highest rewards in the challenge. The autonomous behaviors \ngenerated by our agent are highly adaptive to a variety of obstacles as shown \nin the simulation results. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a36", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.02896"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Qizhe Xie, Guokun Lai, Zihang Dai, Eduard Hovy", "title": "Large-scale Cloze Test Dataset Designed by Teachers. (arXiv:1711.03225v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.03225", "type": "text/html"}], "timestampUsec": "1516598532242787", "comments": [], "summary": {"content": "<p>Cloze test is widely adopted in language exams to evaluate students' language \nproficiency. In this paper, we propose the first large-scale human-designed \ncloze test dataset CLOTH, in which the questions were used in middle-school and \nhigh-school language exams. With the missing blanks carefully created by \nteachers and candidate choices purposely designed to be confusing, CLOTH \nrequires a deeper language understanding and a wider attention span than \nprevious automatically generated cloze datasets. We show humans outperform \ndedicated designed baseline models by a significant margin, even when the model \nis trained on sufficiently large external data. We investigate the source of \nthe performance gap, trace model deficiencies to some distinct properties of \nCLOTH, and identify the limited ability of comprehending a long-term context to \nbe the key bottleneck. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a38", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.03225"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "William G. P. Mayner, William Marshall, Larissa Albantakis, Graham Findlay, Robert Marchman, Giulio Tononi", "title": "PyPhi: A toolbox for integrated information theory. (arXiv:1712.09644v2 [q-bio.NC] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1712.09644", "type": "text/html"}], "timestampUsec": "1516598532242784", "comments": [], "summary": {"content": "<p>Integrated information theory provides a mathematical framework to fully \ncharacterize the cause-effect structure of a physical system. Here, we \nintroduce PyPhi, a Python software package that implements this framework for \ncausal analysis and unfolds the full cause-effect structure of discrete \ndynamical systems of binary elements. The software allows users to easily study \nthese structures, serves as an up-to-date reference implementation of the \nformalisms of integrated information theory, and has been applied in research \non complexity, emergence, and certain biological questions. We first provide an \noverview of the main algorithm and demonstrate PyPhi's functionality in the \ncourse of analyzing an example system, and then describe details of the \nalgorithm's design and implementation. \n</p> \n<p>PyPhi can be installed with Python's package manager via the command 'pip \ninstall pyphi' on Linux and macOS systems equipped with Python 3.4 or higher. \nPyPhi is open-source and licensed under the GPLv3; the source code is hosted on \nGitHub at https://github.com/wmayner/pyphi . Comprehensive and \ncontinually-updated documentation is available at https://pyphi.readthedocs.io/ \n. The pyphi-users mailing list can be joined at \nhttps://groups.google.com/forum/#!forum/pyphi-users . A web-based graphical \ninterface to the software is available at \n<a href=\"http://integratedinformationtheory.org/calculate.html\">this http URL</a> . \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a3f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09644"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yunchuan Kong, Tianwei Yu", "title": "A graph-embedded deep feedforward network for disease outcome classification and feature selection using gene expression data. (arXiv:1801.06202v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.06202", "type": "text/html"}], "timestampUsec": "1516598532242783", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328fd025e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328fd025e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Gene expression data represents a unique challenge in predictive model \nbuilding, because of the small number of samples $(n)$ compared to the huge \namount of features $(p)$. This \"$n&lt;&lt;p$\" property has hampered application of \ndeep learning techniques for disease outcome classification. Sparse learning by \nincorporating external gene network information could be a potential solution \nto this issue. Still, the problem is very challenging because (1) there are \ntens of thousands of features and only hundreds of training samples, (2) the \nscale-free structure of the gene network is unfriendly to the setup of \nconvolutional neural networks. To address these issues and build a robust \nclassification model, we propose the Graph-Embedded Deep Feedforward Networks \n(GEDFN), to integrate external relational information of features into the deep \nneural network architecture. The method is able to achieve sparse connection \nbetween network layers to prevent overfitting. To validate the method's \ncapability, we conducted both simulation experiments and a real data analysis \nusing a breast cancer RNA-seq dataset from The Cancer Genome Atlas (TCGA). The \nresulting high classification accuracy and easily interpretable feature \nselection results suggest the method is a useful addition to the current \nclassification models and feature selection procedures. The method is available \nat https://github.com/yunchuankong/NetworkNeuralNetwork. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a44", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06202"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Brian Trippe, Richard Turner", "title": "Overpruning in Variational Bayesian Neural Networks. (arXiv:1801.06230v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.06230", "type": "text/html"}], "timestampUsec": "1516598532242782", "comments": [], "summary": {"content": "<p>The motivations for using variational inference (VI) in neural networks \ndiffer significantly from those in latent variable models. This has a \ncounter-intuitive consequence; more expressive variational approximations can \nprovide significantly worse predictions as compared to those with less \nexpressive families. In this work we make two contributions. First, we identify \na cause of this performance gap, variational over-pruning. Second, we introduce \na theoretically grounded explanation for this phenomenon. Our perspective sheds \nlight on several related published results and provides intuition into the \ndesign of effective variational approximations of neural networks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06230"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Rico Krueger, Akshay Vij, Taha H. Rashidi", "title": "A Dirichlet Process Mixture Model of Discrete Choice. (arXiv:1801.06296v1 [stat.AP])", "alternate": [{"href": "http://arxiv.org/abs/1801.06296", "type": "text/html"}], "timestampUsec": "1516598532242781", "comments": [], "summary": {"content": "<p>We present a mixed multinomial logit (MNL) model, which leverages the \ntruncated stick-breaking process representation of the Dirichlet process as a \nflexible nonparametric mixing distribution. The proposed model is a Dirichlet \nprocess mixture model and accommodates discrete representations of \nheterogeneity, like a latent class MNL model. Yet, unlike a latent class MNL \nmodel, the proposed discrete choice model does not require the analyst to fix \nthe number of mixture components prior to estimation, as the complexity of the \ndiscrete mixing distribution is inferred from the evidence. For posterior \ninference in the proposed Dirichlet process mixture model of discrete choice, \nwe derive an expectation maximisation algorithm. In a simulation study, we \ndemonstrate that the proposed model framework can flexibly capture \ndifferently-shaped taste parameter distributions. Furthermore, we empirically \nvalidate the model framework in a case study on motorists' route choice \npreferences and find that the proposed Dirichlet process mixture model of \ndiscrete choice outperforms a latent class MNL model and mixed MNL models with \ncommon parametric mixing distributions in terms of both in-sample fit and \nout-of-sample predictive ability. Compared to extant modelling approaches, the \nproposed discrete choice model substantially abbreviates specification \nsearches, as it relies on less restrictive parametric assumptions and does not \nrequire the analyst to specify the complexity of the discrete mixing \ndistribution prior to estimation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a50", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06296"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Thierry Moreau, Anton Lokhmotov, Grigori Fursin", "title": "Introducing ReQuEST: an Open Platform for Reproducible and Quality-Efficient Systems-ML Tournaments. (arXiv:1801.06378v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.06378", "type": "text/html"}], "timestampUsec": "1516598532242780", "comments": [], "summary": {"content": "<p>Co-designing efficient machine learning based systems across the whole \nhardware/software stack to trade off speed, accuracy, energy and costs is \nbecoming extremely complex and time consuming. Researchers often struggle to \nevaluate and compare different published works across rapidly evolving software \nframeworks, heterogeneous hardware platforms, compilers, libraries, algorithms, \ndata sets, models, and environments. \n</p> \n<p>We present our community effort to develop an open co-design tournament \nplatform with an online public scoreboard. It will gradually incorporate best \nresearch practices while providing a common way for multidisciplinary \nresearchers to optimize and compare the quality vs. efficiency Pareto \noptimality of various workloads on diverse and complete hardware/software \nsystems. We want to leverage the open-source Collective Knowledge framework and \nthe ACM artifact evaluation methodology to validate and share the complete \nmachine learning system implementations in a standardized, portable, and \nreproducible fashion. We plan to hold regular multi-objective optimization and \nco-design tournaments for emerging workloads such as deep learning, starting \nwith ASPLOS'18 (ACM conference on Architectural Support for Programming \nLanguages and Operating Systems - the premier forum for multidisciplinary \nsystems research spanning computer architecture and hardware, programming \nlanguages and compilers, operating systems and networking) to build a public \nrepository of the most efficient machine learning algorithms and systems which \ncan be easily customized, reused and built upon. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a54", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06378"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Rafael Pinot", "title": "Minimum spanning tree release under differential privacy constraints. (arXiv:1801.06423v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1801.06423", "type": "text/html"}], "timestampUsec": "1516598532242779", "comments": [], "summary": {"content": "<p>We investigate the problem of nodes clustering under privacy constraints when \nrepresenting a dataset as a graph. Our contribution is threefold. First we \nformally define the concept of differential privacy for structured databases \nsuch as graphs, and give an alternative definition based on a new neighborhood \nnotion between graphs. This definition is adapted to particular frameworks that \ncan be met in various application fields such as genomics, world wide web, \npopulation survey, etc. Second, we introduce a new algorithm to tackle the \nissue of privately releasing an approximated minimum spanning tree topology for \na simple-undirected-weighted graph. It provides a simple way of producing the \ntopology of a private almost minimum spanning tree which outperforms, in most \ncases, the state of the art \"Laplace mechanism\" in terms of \nweight-approximation error. \n</p> \n<p>Finally, we propose a theoretically motivated method combining a sanitizing \nmechanism (such as Laplace or our new algorithm) with a Minimum Spanning Tree \n(MST)-based clustering algorithm. It provides an accurate method for nodes \nclustering in a graph while keeping the sensitive information contained in the \nedges weights of the private graph. We provide some theoretical results on the \nrobustness of an almost minimum spanning tree construction for Laplace \nsanitizing mechanisms. These results exhibit which conditions the graph weights \nshould respect in order to consider that the nodes form well separated clusters \nboth for Laplace and our algorithm as sanitizing mechanism. The method has been \nexperimentally evaluated on simulated data, and preliminary results show the \ngood behavior of the algorithm while identifying well separated clusters. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a59", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06423"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mehdi Bahri, Yannis Panagakis, Stefanos Zafeiriou", "title": "Robust Kronecker Component Analysis. (arXiv:1801.06432v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.06432", "type": "text/html"}], "timestampUsec": "1516598532242778", "comments": [], "summary": {"content": "<p>Dictionary learning and component analysis models are fundamental in learning \ncompact representations that are relevant to a given task (feature extraction, \ndimensionality reduction, denoising, etc.). The model complexity is encoded by \nmeans of specific structure, such as sparsity, low-rankness, or nonnegativity. \nUnfortunately, approaches like K-SVD - that learn dictionaries for sparse \ncoding via Singular Value Decomposition (SVD) - are hard to scale to \nhigh-volume and high-dimensional visual data, and fragile in the presence of \noutliers. Conversely, robust component analysis methods such as the Robust \nPrinciple Component Analysis (RPCA) are able to recover low-complexity (e.g., \nlow-rank) representations from data corrupted with noise of unknown magnitude \nand support, but do not provide a dictionary that respects the structure of the \ndata (e.g., images), and also involve expensive computations. In this paper, we \npropose a novel Kronecker-decomposable component analysis model, coined as \nRobust Kronecker Component Analysis (RKCA), that combines ideas from sparse \ndictionary learning and robust component analysis. RKCA has several appealing \nproperties, including robustness to gross corruption; it can be used for \nlow-rank modeling, and leverages separability to solve significantly smaller \nproblems. We design an efficient learning algorithm by drawing links with a \nrestricted form of tensor factorization, and analyze its optimality and \nlow-rankness properties. The effectiveness of the proposed approach is \ndemonstrated on real-world applications, namely background subtraction and \nimage denoising and completion, by performing a thorough comparison with the \ncurrent state of the art. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a5c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06432"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pankaj Pansari, Chris Russell, M.Pawan Kumar", "title": "Worst-case Optimal Submodular Extensions for Marginal Estimation. (arXiv:1801.06490v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.06490", "type": "text/html"}], "timestampUsec": "1516598532242777", "comments": [], "summary": {"content": "<p>Submodular extensions of an energy function can be used to efficiently \ncompute approximate marginals via variational inference. The accuracy of the \nmarginals depends crucially on the quality of the submodular extension. To \nidentify the best possible extension, we show an equivalence between the \nsubmodular extensions of the energy and the objective functions of linear \nprogramming (LP) relaxations for the corresponding MAP estimation problem. This \nallows us to (i) establish the worst-case optimality of the submodular \nextension for Potts model used in the literature; (ii) identify the worst-case \noptimal submodular extension for the more general class of metric labeling; and \n(iii) efficiently compute the marginals for the widely used dense CRF model \nwith the help of a recently proposed Gaussian filtering method. Using synthetic \nand real data, we show that our approach provides comparable upper bounds on \nthe log-partition function to those obtained using tree-reweighted message \npassing (TRW) in cases where the latter is computationally feasible. \nImportantly, unlike TRW, our approach provides the first practical algorithm to \ncompute an upper bound on the dense CRF model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a5e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06490"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alexandre Attia, Sharone Dayan", "title": "Global overview of Imitation Learning. (arXiv:1801.06503v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.06503", "type": "text/html"}], "timestampUsec": "1516598532242776", "comments": [], "summary": {"content": "<p>Imitation Learning is a sequential task where the learner tries to mimic an \nexpert's action in order to achieve the best performance. Several algorithms \nhave been proposed recently for this task. In this project, we aim at proposing \na wide review of these algorithms, presenting their main features and comparing \nthem on their performance and their regret bounds. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a62", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06503"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alexandre Attia, Sharone Dayan", "title": "Detecting and counting tiny faces. (arXiv:1801.06504v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.06504", "type": "text/html"}], "timestampUsec": "1516598532242775", "comments": [], "summary": {"content": "<p>Finding Tiny Faces by Hu and Ramanan - and released at CVPR 2017 - proposes a \nnovel approach to find small objects in an image. Our contribution consists in \ndeeply understanding the choices of the paper together with applying and \nextending a similar method to a real world subject which is the counting of \npeople in a public demonstration. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a65", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06504"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Guoyin Li, Ting Kei Pong", "title": "Calculus of the exponent of Kurdyka-{\\L}ojasiewicz inequality and its applications to linear convergence of first-order methods. (arXiv:1602.02915v5 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1602.02915", "type": "text/html"}], "timestampUsec": "1516598532242774", "comments": [], "summary": {"content": "<p>In this paper, we study the Kurdyka-{\\L}ojasiewicz (KL) exponent, an \nimportant quantity for analyzing the convergence rate of first-order methods. \nSpecifically, we develop various calculus rules to deduce the KL exponent of \nnew (possibly nonconvex and nonsmooth) functions formed from functions with \nknown KL exponents. In addition, we show that the well-studied Luo-Tseng error \nbound together with a mild assumption on the separation of stationary values \nimplies that the KL exponent is $\\frac12$. The Luo-Tseng error bound is known \nto hold for a large class of concrete structured optimization problems, and \nthus we deduce the KL exponent of a large class of functions whose exponents \nwere previously unknown. Building upon this and the calculus rules, we are then \nable to show that for many convex or nonconvex optimization models for \napplications such as sparse recovery, their objective function's KL exponent is \n$\\frac12$. This includes the least squares problem with smoothly clipped \nabsolute deviation (SCAD) regularization or minimax concave penalty (MCP) \nregularization and the logistic regression problem with $\\ell_1$ \nregularization. Since many existing local convergence rate analysis for \nfirst-order methods in the nonconvex scenario relies on the KL exponent, our \nresults enable us to obtain explicit convergence rate for various first-order \nmethods when they are applied to a large variety of practical optimization \nmodels. Finally, we further illustrate how our results can be applied to \nestablishing local linear convergence of the proximal gradient algorithm and \nthe inertial proximal algorithm with constant step-sizes for some specific \nmodels that arise in sparse recovery. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a6a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1602.02915"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Runze Tang, Michael Ketcha, Alexandra Badea, Evan D. Calabrese, Daniel S. Margulies, Joshua T. Vogelstein, Carey E. Priebe, Daniel L. Sussman", "title": "Connectome Smoothing via Low-rank Approximations. (arXiv:1609.01672v2 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1609.01672", "type": "text/html"}], "timestampUsec": "1516598532242773", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b328fd044b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b328fd044b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In statistical connectomics, the quantitative study of brain networks, \nestimating the mean of a population of graphs based on a sample is a core \nproblem. Often, this problem is especially difficult because the sample or \ncohort size is relatively small, sometimes even a single subject. While using \nthe element-wise sample mean of the adjacency matrices is a common approach, \nthis method does not exploit any underlying structural properties of the \ngraphs. We propose using a low-rank method which incorporates tools for \ndimension selection and diagonal augmentation to smooth the estimates and \nimprove performance over the naive methodology for small sample sizes. \nTheoretical results for the stochastic blockmodel show that this method offers \nmajor improvements when there are many vertices. Similarly, we demonstrate that \nthe low-rank methods outperform the standard sample mean for a variety of \nindependent edge distributions as well as human connectome data derived from \nmagnetic resonance imaging, especially when sample sizes are small. Moreover, \nthe low-rank methods yield \"eigen-connectomes\", which correlate with the \nlobe-structure of the human brain and superstructures of the mouse brain. These \nresults indicate that low-rank methods are an important part of the tool box \nfor researchers studying populations of graphs in general, and statistical \nconnectomics in particular. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a6b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1609.01672"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tiago P. Peixoto", "title": "Nonparametric Bayesian inference of the microcanonical stochastic block model. (arXiv:1610.02703v3 [physics.data-an] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.02703", "type": "text/html"}], "timestampUsec": "1516598532242772", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3290301a5\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3290301a5&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A principled approach to characterize the hidden structure of networks is to \nformulate generative models, and then infer their parameters from data. When \nthe desired structure is composed of modules or \"communities\", a suitable \nchoice for this task is the stochastic block model (SBM), where nodes are \ndivided into groups, and the placement of edges is conditioned on the group \nmemberships. Here, we present a nonparametric Bayesian method to infer the \nmodular structure of empirical networks, including the number of modules and \ntheir hierarchical organization. We focus on a microcanonical variant of the \nSBM, where the structure is imposed via hard constraints, i.e. the generated \nnetworks are not allowed to violate the patterns imposed by the model. We show \nhow this simple model variation allows simultaneously for two important \nimprovements over more traditional inference approaches: 1. Deeper Bayesian \nhierarchies, with noninformative priors replaced by sequences of priors and \nhyperpriors, that not only remove limitations that seriously degrade the \ninference on large networks, but also reveal structures at multiple scales; 2. \nA very efficient inference algorithm that scales well not only for networks \nwith a large number of nodes and edges, but also with an unlimited number of \nmodules. We show also how this approach can be used to sample modular \nhierarchies from the posterior distribution, as well as to perform model \nselection. We discuss and analyze the differences between sampling from the \nposterior and simply finding the single parameter estimate that maximizes it. \nFurthermore, we expose a direct equivalence between our microcanonical approach \nand alternative derivations based on the canonical SBM. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a78", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.02703"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Maria-Florina Balcan, Yingyu Liang, David P. Woodruff, Hongyang Zhang", "title": "Matrix Completion and Related Problems via Strong Duality. (arXiv:1704.08683v3 [cs.DS] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.08683", "type": "text/html"}], "timestampUsec": "1516598532242771", "comments": [], "summary": {"content": "<p>This work studies the strong duality of non-convex matrix factorization \nproblems: we show that under certain dual conditions, these problems and its \ndual have the same optimum. This has been well understood for convex \noptimization, but little was known for non-convex problems. We propose a novel \nanalytical framework and show that under certain dual conditions, the optimal \nsolution of the matrix factorization program is the same as its bi-dual and \nthus the global optimality of the non-convex program can be achieved by solving \nits bi-dual which is convex. These dual conditions are satisfied by a wide \nclass of matrix factorization problems, although matrix factorization problems \nare hard to solve in full generality. This analytical framework may be of \nindependent interest to non-convex optimization more broadly. \n</p> \n<p>We apply our framework to two prototypical matrix factorization problems: \nmatrix completion and robust Principal Component Analysis (PCA). These are \nexamples of efficiently recovering a hidden matrix given limited reliable \nobservations of it. Our framework shows that exact recoverability and strong \nduality hold with nearly-optimal sample complexity guarantees for matrix \ncompletion and robust PCA. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a7d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.08683"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tiago P. Peixoto", "title": "Bayesian stochastic blockmodeling. (arXiv:1705.10225v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.10225", "type": "text/html"}], "timestampUsec": "1516598532242770", "comments": [], "summary": {"content": "<p>This chapter provides a self-contained introduction to the use of Bayesian \ninference to extract large-scale modular structures from network data, based on \nthe stochastic block model (SBM), as well as its degree-corrected and \noverlapping generalizations. We focus on nonparametric formulations that allow \ntheir inference in a manner that prevents overfitting, and enables model \nselection. We discuss aspects of the choice of priors, in particular how to \navoid underfitting via increased Bayesian hierarchies, and we contrast the task \nof sampling network partitions from the posterior distribution with finding the \nsingle point estimate that maximizes it, while describing efficient algorithms \nto perform either one. We also show how inferring the SBM can be used to \npredict missing and spurious links, and shed light on the fundamental \nlimitations of the detectability of modular structures in networks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a7f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.10225"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Greg Ver Steeg, Aram Galstyan", "title": "Low Complexity Gaussian Latent Factor Models and a Blessing of Dimensionality. (arXiv:1706.03353v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.03353", "type": "text/html"}], "timestampUsec": "1516598532242769", "comments": [], "summary": {"content": "<p>Learning the structure of graphical models from data usually incurs a heavy \ncurse of dimensionality that renders this problem intractable in many \nreal-world situations. The rare cases where the curse becomes a blessing \nprovide insight into the limits of the efficiently computable and augment the \nscarce options for treating very under-sampled, high-dimensional data. We study \na special class of Gaussian latent factor models where each (non-iid) observed \nvariable depends on at most one of a set of latent variables. We derive \ninformation-theoretic lower bounds on the sample complexity for structure \nrecovery that suggest complexity actually decreases as the dimensionality \nincreases. Contrary to this prediction, we observe that existing structure \nrecovery methods deteriorate with increasing dimension. Therefore, we design a \nnew approach to learning Gaussian latent factor models that benefits from \ndimensionality. Our approach relies on an unconstrained information-theoretic \nobjective whose global optima correspond to structured latent factor generative \nmodels. In addition to improved structure recovery, we also show that we are \nable to outperform state-of-the-art approaches for covariance estimation on \nboth synthetic and real data in the very under-sampled, high-dimensional \nregime. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516598532243", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003605e2a83", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.03353"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yuhuang Hu, Adrian Huber, Jithendar Anumula, Shih-Chii Liu", "title": "Overcoming the vanishing gradient problem in plain recurrent networks. (arXiv:1801.06105v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.06105", "type": "text/html"}], "timestampUsec": "1516343194817832", "comments": [], "summary": {"content": "<p>Plain recurrent networks greatly suffer from the vanishing gradient problem \nwhile Gated Neural Networks (GNNs) such as Long-short Term Memory (LSTM) and \nGated Recurrent Unit (GRU) deliver promising results in many sequence learning \ntasks through sophisticated network designs. This paper shows how we can \naddress this problem in a plain recurrent network by analyzing the gating \nmechanisms in GNNs. We propose a novel network called the Recurrent Identity \nNetwork (RIN) which allows a plain recurrent network to overcome the vanishing \ngradient problem while training very deep models without the use of gates. We \ncompare this model with IRNNs and LSTMs on multiple sequence modeling \nbenchmarks. The RINs demonstrate competitive performance and converge faster in \nall tasks. Notably, small RIN models produce 12%--67% higher accuracy on the \nSequential and Permuted MNIST datasets and reach state-of-the-art performance \non the bAbI question answering dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516343194818", "annotations": [], "published": 1516343195, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e93daea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06105"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, Kam-Fai Wong", "title": "Integrating planning for task-completion dialogue policy learning. (arXiv:1801.06176v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1801.06176", "type": "text/html"}], "timestampUsec": "1516343194817831", "comments": [], "summary": {"content": "<p>Training a task-completion dialogue agent with real users via reinforcement \nlearning (RL) could be prohibitively expensive, because it requires many \ninteractions with users. One alternative is to resort to a user simulator, \nwhile the discrepancy of between simulated and real users makes the learned \npolicy unreliable in practice. This paper addresses these challenges by \nintegrating planning into the dialogue policy learning based on Dyna-Q \nframework, and provides a more sample-efficient approach to learn the dialogue \npolices. The proposed agent consists of a planner trained on-line with limited \nreal user experience that can generate large amounts of simulated experience to \nsupplement with limited real user experience, and a policy model trained on \nthese hybrid experiences. The effectiveness of our approach is validated on a \nmovie-booking task in both a simulation setting and a human-in-the-loop \nsetting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516343194818", "annotations": [], "published": 1516343195, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e93db22", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06176"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Bo Wu, Wen-Huang Cheng, Yongdong Zhang, Tao Mei", "title": "Time Matters: Multi-scale Temporalization of Social Media Popularity. (arXiv:1801.05853v1 [cs.SI])", "alternate": [{"href": "http://arxiv.org/abs/1801.05853", "type": "text/html"}], "timestampUsec": "1516343194817830", "comments": [], "summary": {"content": "<p>The evolution of social media popularity exhibits rich temporality, i.e., \npopularities change over time at various levels of temporal granularity. This \nis influenced by temporal variations of public attentions or user activities. \nFor example, popularity patterns of street snap on Flickr are observed to \ndepict distinctive fashion styles at specific time scales, such as season-based \nperiodic fluctuations for Trench Coat or one-off peak in days for Evening \nDress. However, this fact is often overlooked by existing research of \npopularity modeling. We present the first study to incorporate multiple \ntime-scale dynamics into predicting online popularity. We propose a novel \ncomputational framework in the paper, named Multi-scale Temporalization, for \nestimating popularity based on multi-scale decomposition and structural \nreconstruction in a tensor space of user, post, and time by joint low-rank \nconstraints. By considering the noise caused by context inconsistency, we \ndesign a data rearrangement step based on context aggregation as preprocessing \nto enhance contextual relevance of neighboring data in the tensor space. As a \nresult, our approach can leverage multiple levels of temporal characteristics \nand reduce the noise of data decomposition to improve modeling effectiveness. \nWe evaluate our approach on two large-scale Flickr image datasets with over 1.8 \nmillion photos in total, for the task of popularity prediction. The results \nshow that our approach significantly outperforms state-of-the-art popularity \nprediction techniques, with a relative improvement of 10.9%-47.5% in terms of \nprediction accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516343194818", "annotations": [], "published": 1516343195, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e93db49", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05853"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Vinod Kumar Chauhan, Kalpana Dahiya, Anuj Sharma", "title": "Faster Algorithms for Large-scale Machine Learning using Simple Sampling Techniques. (arXiv:1801.05931v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.05931", "type": "text/html"}], "timestampUsec": "1516343194817829", "comments": [], "summary": {"content": "<p>Now a days, the major challenge in machine learning is the `Big~Data' \nchallenge. The big data problems due to large number of data points or large \nnumber of features in each data point, or both, the training of models have \nbecome very slow. The training time has two major components: Time to access \nthe data and time to process the data. In this paper, we have proposed one \npossible solution to handle the big data problems in machine learning. The \nfocus is on reducing the training time through reducing data access time by \nproposing systematic sampling and cyclic/sequential sampling to select \nmini-batches from the dataset. To prove the effectiveness of proposed sampling \ntechniques, we have used Empirical Risk Minimization, which is commonly used \nmachine learning problem, for strongly convex and smooth case. The problem has \nbeen solved using SAG, SAGA, SVRG, SAAG-II and MBSGD (Mini-batched SGD), each \nusing two step determination techniques, namely, constant step size and \nbacktracking line search method. Theoretical results prove the same convergence \nfor systematic sampling, cyclic sampling and the widely used random sampling \ntechnique, in expectation. Experimental results with bench marked datasets \nprove the efficacy of the proposed sampling techniques. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516343194818", "annotations": [], "published": 1516343195, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e93db82", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05931"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gino Brunner, Yuyi Wang, Roger Wattenhofer, Michael Weigelt", "title": "Natural Language Multitasking: Analyzing and Improving Syntactic Saliency of Hidden Representations. (arXiv:1801.06024v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1801.06024", "type": "text/html"}], "timestampUsec": "1516343194817828", "comments": [], "summary": {"content": "<p>We train multi-task autoencoders on linguistic tasks and analyze the learned \nhidden sentence representations. The representations change significantly when \ntranslation and part-of-speech decoders are added. The more decoders a model \nemploys, the better it clusters sentences according to their syntactic \nsimilarity, as the representation space becomes less entangled. We explore the \nstructure of the representation space by interpolating between sentences, which \nyields interesting pseudo-English sentences, many of which have recognizable \nsyntactic structure. Lastly, we point out an interesting property of our \nmodels: The difference-vector between two sentences can be added to change a \nthird sentence with similar features in a meaningful way. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516343194818", "annotations": [], "published": 1516343195, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e93db97", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06024"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Catherine F. Higham, Desmond J. Higham", "title": "Deep Learning: An Introduction for Applied Mathematicians. (arXiv:1801.05894v1 [math.HO])", "alternate": [{"href": "http://arxiv.org/abs/1801.05894", "type": "text/html"}], "timestampUsec": "1516343194817827", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329030400\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329030400&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Multilayered artificial neural networks are becoming a pervasive tool in a \nhost of application fields. At the heart of this deep learning revolution are \nfamiliar concepts from applied and computational mathematics; notably, in \ncalculus, approximation theory, optimization and linear algebra. This article \nprovides a very brief introduction to the basic ideas that underlie deep \nlearning from an applied mathematics perspective. Our target audience includes \npostgraduate and final year undergraduate students in mathematics who are keen \nto learn about the area. The article may also be useful for instructors in \nmathematics who wish to enliven their classes with references to the \napplication of deep learning techniques. We focus on three fundamental \nquestions: what is a deep neural network? how is a network trained? what is the \nstochastic gradient method? We illustrate the ideas with a short MATLAB code \nthat sets up and trains a network. We also show the use of state-of-the art \nsoftware on a large scale image classification problem. We finish with \nreferences to the current literature. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516343194818", "annotations": [], "published": 1516343195, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e93dbb2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05894"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mustafa Hajij, Nata&#x161;a Jonoska, Denys Kukushkin, Masahico Saito", "title": "Graph Based Analysis for Gene Segment Organization In a Scrambled Genome. (arXiv:1801.05922v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.05922", "type": "text/html"}], "timestampUsec": "1516343194817826", "comments": [], "summary": {"content": "<p>DNA rearrangement processes recombine gene segments that are organized on the \nchromosome in a variety of ways. The segments can overlap, interleave or one \nmay be a subsegment of another. We use directed graphs to represent segment \norganizations on a given locus where contigs containing rearranged segments \nrepresent vertices and the edges correspond to the segment relationships. Using \ngraph properties we associate a point in a higher dimensional Euclidean space \nto each graph such that cluster formations and analysis can be performed with \nmethods from topological data analysis. The method is applied to a recently \nsequenced model organism \\textit{Oxytricha trifallax}, a species of ciliate \nwith highly scrambled genome that undergoes massive rearrangement process after \nconjugation. The analysis shows some emerging star-like graph structures \nindicating that segments of a single gene can interleave, or even contain all \nof the segments from fifteen or more other genes in between its segments. We \nalso observe that as many as six genes can have their segments mutually \ninterleaving or overlapping. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516343194818", "annotations": [], "published": 1516343195, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e93dbe2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05922"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ferdi Kara, Hakan Kaya, Okan Erkaymaz, Ertan Ozturk", "title": "Prediction of the Optimal Threshold Value in DF Relay Selection Schemes Based on Artificial Neural Networks. (arXiv:1801.05984v1 [eess.SP])", "alternate": [{"href": "http://arxiv.org/abs/1801.05984", "type": "text/html"}], "timestampUsec": "1516338583663314", "comments": [], "summary": {"content": "<p>In wireless communications, the cooperative communication (CC) technology \npromises performance gains compared to traditional Single-Input Single Output \n(SISO) techniques. Therefore, the CC technique is one of the nominees for 5G \nnetworks. In the Decode-and-Forward (DF) relaying scheme which is one of the CC \ntechniques, determination of the threshold value at the relay has a key role \nfor the system performance and power usage. In this paper, we propose \nprediction of the optimal threshold values for the best relay selection scheme \nin cooperative communications, based on Artificial Neural Networks (ANNs) for \nthe first time in literature. The average link qualities and number of relays \nhave been used as inputs in the prediction of optimal threshold values using \nArtificial Neural Networks (ANNs): Multi-Layer Perceptron (MLP) and Radial \nBasis Function (RBF) networks. The MLP network has better performance from the \nRBF network on the prediction of optimal threshold value when the same number \nof neurons is used at the hidden layer for both networks. Besides, the optimal \nthreshold values obtained using ANNs are verified by the optimal threshold \nvalues obtained numerically using the closed form expression derived for the \nsystem. The results show that the optimal threshold values obtained by ANNs on \nthe best relay selection scheme provide a minimum Bit-Error-Rate (BER) because \nof the reduction of the probability that error propagation may occur. Also, for \nthe same BER performance goal, prediction of optimal threshold values provides \n2dB less power usage, which is great gain in terms of green communicationBER \nperformance goal, prediction of optimal threshold values provides 2dB less \npower usage, which is great gain in terms of green communication. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b76f3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05984"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pieter Gijsbers, Joaquin Vanschoren, Randal S. Olson", "title": "Layered TPOT: Speeding up Tree-based Pipeline Optimization. (arXiv:1801.06007v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.06007", "type": "text/html"}], "timestampUsec": "1516338583663313", "comments": [], "summary": {"content": "<p>With the demand for machine learning increasing, so does the demand for tools \nwhich make it easier to use. Automated machine learning (AutoML) tools have \nbeen developed to address this need, such as the Tree-Based Pipeline \nOptimization Tool (TPOT) which uses genetic programming to build optimal \npipelines. We introduce Layered TPOT, a modification to TPOT which aims to \ncreate pipelines equally good as the original, but in significantly less time. \nThis approach evaluates candidate pipelines on increasingly large subsets of \nthe data according to their fitness, using a modified evolutionary algorithm to \nallow for separate competition between pipelines trained on different sample \nsizes. Empirical evaluation shows that, on sufficiently large datasets, Layered \nTPOT indeed finds better models faster. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b76f8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06007"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Lindsey Kuper, Guy Katz, Justin Gottschlich, Kyle Julian, Clark Barrett, Mykel Kochenderfer", "title": "Toward Scalable Verification for Safety-Critical Deep Networks. (arXiv:1801.05950v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.05950", "type": "text/html"}], "timestampUsec": "1516338583663311", "comments": [], "summary": {"content": "<p>The increasing use of deep neural networks for safety-critical applications, \nsuch as autonomous driving and flight control, raises concerns about their \nsafety and reliability. Formal verification can address these concerns by \nguaranteeing that a deep learning system operates as intended, but the \nstate-of-the-art is limited to small systems. In this work-in-progress report \nwe give an overview of our work on mitigating this difficulty, by pursuing two \ncomplementary directions: devising scalable verification techniques, and \nidentifying design choices that result in deep learning systems that are more \namenable to verification. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b76fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05950"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sam Ganzfried, Farzana Yusuf", "title": "Optimal Weighting for Exam Composition. (arXiv:1801.06043v1 [cs.CY])", "alternate": [{"href": "http://arxiv.org/abs/1801.06043", "type": "text/html"}], "timestampUsec": "1516338583663310", "comments": [], "summary": {"content": "<p>A problem faced by many instructors is that of designing exams that \naccurately assess the abilities of the students. Typically these exams are \nprepared several days in advance, and generic question scores are used based on \nrough approximation of the question difficulty and length. For example, for a \nrecent class taught by the author, there were 30 multiple choice questions \nworth 3 points, 15 true/false with explanation questions worth 4 points, and 5 \nanalytical exercises worth 10 points. We describe a novel framework where \nalgorithms from machine learning are used to modify the exam question weights \nin order to optimize the exam scores, using the overall class grade as a proxy \nfor a student's true ability. We show that significant error reduction can be \nobtained by our approach over standard weighting schemes, and we make several \nnew observations regarding the properties of the \"good\" and \"bad\" exam \nquestions that can have impact on the design of improved future evaluation \nmethods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b7706", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06043"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xiaocheng Li, Huaiyang Zhong, Margaret L. Brandeau", "title": "Quantile Markov Decision Process. (arXiv:1711.05788v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05788", "type": "text/html"}], "timestampUsec": "1516338583663309", "comments": [], "summary": {"content": "<p>In this paper, we consider the problem of optimizing the quantiles of the \ncumulative rewards of Markov Decision Processes (MDP), to which we refers as \nQuantile Markov Decision Processes (QMDP). Traditionally, the goal of a Markov \nDecision Process (MDP) is to maximize expected cumulative reward over a defined \nhorizon (possibly to be infinite). In many applications, however, a decision \nmaker may be interested in optimizing a specific quantile of the cumulative \nreward instead of its expectation. Our framework of QMDP provides analytical \nresults characterizing the optimal QMDP solution and presents the algorithm for \nsolving the QMDP. We provide analytical results characterizing the optimal QMDP \nsolution and present the algorithms for solving the QMDP. We illustrate the \nmodel with two experiments: a grid game and a HIV optimal treatment experiment. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b7709", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05788"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Richard G. Everitt", "title": "Bootstrapped synthetic likelihood. (arXiv:1711.05825v2 [stat.CO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05825", "type": "text/html"}], "timestampUsec": "1516338583663308", "comments": [], "summary": {"content": "<p>Approximate Bayesian computation (ABC) and synthetic likelihood (SL) \ntechniques have enabled the use of Bayesian inference for models that may be \nsimulated, but for which the likelihood cannot be evaluated pointwise at values \nof an unknown parameter $\\theta$. The main idea in ABC and SL is to, for \ndifferent values of $\\theta$ (usually chosen using a Monte Carlo algorithm), \nbuild estimates of the likelihood based on simulations from the model \nconditional on $\\theta$. The quality of these estimates determines the \nefficiency of an ABC/SL algorithm. In standard ABC/SL, the only means to \nimprove an estimated likelihood at $\\theta$ is to simulate more times from the \nmodel conditional on $\\theta$, which is infeasible in cases where the simulator \nis computationally expensive. In this paper we describe how to use \nbootstrapping as a means for improving SL estimates whilst using fewer \nsimulations from the model, and also investigate its use in ABC. Further, we \ninvestigate the use of the bag of little bootstraps as a means for applying \nthis approach to large datasets, yielding Monte Carlo algorithms that \naccurately approximate posterior distributions whilst only simulating \nsubsamples of the full data. Examples of the approach applied to i.i.d., \ntemporal and spatial data are given. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b770e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05825"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Daokun Zhang, Jie Yin, Xingquan Zhu, Chengqi Zhang", "title": "Network Representation Learning: A Survey. (arXiv:1801.05852v1 [cs.SI])", "alternate": [{"href": "http://arxiv.org/abs/1801.05852", "type": "text/html"}], "timestampUsec": "1516338583663305", "comments": [], "summary": {"content": "<p>With the widespread use of information technologies, information networks \nhave increasingly become popular to capture complex relationships across \nvarious disciplines, such as social networks, citation networks, \ntelecommunication networks, and biological networks. Analyzing these networks \nsheds light on different aspects of social life such as the structure of \nsociety, information diffusion, and different patterns of communication. \nHowever, the large scale of information networks often makes network analytic \ntasks computationally expensive and intractable. Recently, network \nrepresentation learning has been proposed as a new learning paradigm that \nembeds network vertices into a low-dimensional vector space, by preserving \nnetwork topology structure, vertex content, and other side information. This \nfacilitates the original network to be easily handled in the new vector space \nfor further analysis. In this survey, we perform a thorough review of the \ncurrent literature on network representation learning in the field of data \nmining and machine learning. We propose a new categorization to analyze and \nsummarize state-of-the-art network representation learning techniques according \nto the methodology they employ and the network information they preserve. \nFinally, to facilitate research on this topic, we summarize benchmark datasets \nand evaluation methodologies, and discuss open issues and future research \ndirections in this field. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b7714", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05852"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Benjamin Mirabelli, Dan Kushnir", "title": "Active Community Detection: A Maximum Likelihood Approach. (arXiv:1801.05856v1 [cs.SI])", "alternate": [{"href": "http://arxiv.org/abs/1801.05856", "type": "text/html"}], "timestampUsec": "1516338583663304", "comments": [], "summary": {"content": "<p>We propose novel semi-supervised and active learning algorithms for the \nproblem of community detection on networks. The algorithms are based on \noptimizing the likelihood function of the community assignments given a graph \nand an estimate of the statistical model that generated it. The optimization \nframework is inspired by prior work on the unsupervised community detection \nproblem in Stochastic Block Models (SBM) using Semi-Definite Programming (SDP). \nIn this paper we provide the next steps in the evolution of learning \ncommunities in this context which involves a constrained semi-definite \nprogramming algorithm, and a newly presented active learning algorithm. The \nactive learner intelligently queries nodes that are expected to maximize the \nchange in the model likelihood. Experimental results show that this active \nlearning algorithm outperforms the random-selection semi-supervised version of \nthe same algorithm as well as other state-of-the-art active learning \nalgorithms. Our algorithms significantly improved performance is demonstrated \non both real-world and SBM-generated networks even when the SBM has a signal to \nnoise ratio (SNR) below the known unsupervised detectability threshold. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b7719", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05856"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Koulik Khamaru, Rahul Mazumder", "title": "Computation of the Maximum Likelihood estimator in low-rank Factor Analysis. (arXiv:1801.05935v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1801.05935", "type": "text/html"}], "timestampUsec": "1516338583663303", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3290305ec\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3290305ec&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Factor analysis, a classical multivariate statistical technique is popularly \nused as a fundamental tool for dimensionality reduction in statistics, \neconometrics and data science. Estimation is often carried out via the Maximum \nLikelihood (ML) principle, which seeks to maximize the likelihood under the \nassumption that the positive definite covariance matrix can be decomposed as \nthe sum of a low rank positive semidefinite matrix and a diagonal matrix with \nnonnegative entries. This leads to a challenging rank constrained nonconvex \noptimization problem. We reformulate the low rank ML Factor Analysis problem as \na nonlinear nonsmooth semidefinite optimization problem, study various \nstructural properties of this reformulation and propose fast and scalable \nalgorithms based on difference of convex (DC) optimization. Our approach has \ncomputational guarantees, gracefully scales to large problems, is applicable to \nsituations where the sample covariance matrix is rank deficient and adapts to \nvariants of the ML problem with additional constraints on the problem \nparameters. Our numerical experiments demonstrate the significant usefulness of \nour approach over existing state-of-the-art approaches. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b771c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05935"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jeremy Howard, Sebastian Ruder", "title": "Fine-tuned Language Models for Text Classification. (arXiv:1801.06146v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1801.06146", "type": "text/html"}], "timestampUsec": "1516338583663302", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329075864\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329075864&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Transfer learning has revolutionized computer vision, but existing approaches \nin NLP still require task-specific modifications and training from scratch. We \npropose Fine-tuned Language Models (FitLaM), an effective transfer learning \nmethod that can be applied to any task in NLP, and introduce techniques that \nare key for fine-tuning a state-of-the-art language model. Our method \nsignificantly outperforms the state-of-the-art on five text classification \ntasks, reducing the error by 18-24% on the majority of datasets. We open-source \nour pretrained models and code to enable adoption by the community. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b7720", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06146"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Brendan D. Tracey, David H. Wolpert", "title": "Upgrading from Gaussian Processes to Student's-T Processes. (arXiv:1801.06147v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.06147", "type": "text/html"}], "timestampUsec": "1516338583663301", "comments": [], "summary": {"content": "<p>Gaussian process priors are commonly used in aerospace design for performing \nBayesian optimization. Nonetheless, Gaussian processes suffer two significant \ndrawbacks: outliers are a priori assumed unlikely, and the posterior variance \nconditioned on observed data depends only on the locations of those data, not \nthe associated sample values. Student's-T processes are a generalization of \nGaussian processes, founded on the Student's-T distribution instead of the \nGaussian distribution. Student's-T processes maintain the primary advantages of \nGaussian processes (kernel function, analytic update rule) with additional \nbenefits beyond Gaussian processes. The Student's-T distribution has higher \nKurtosis than a Gaussian distribution and so outliers are much more likely, and \nthe posterior variance increases or decreases depending on the variance of \nobserved data sample values. Here, we describe Student's-T processes, and \ndiscuss their advantages in the context of aerospace optimization. We show how \nto construct a Student's-T process using a kernel function and how to update \nthe process given new samples. We provide a clear derivation of \noptimization-relevant quantities such as expected improvement, and contrast \nwith the related computations for Gaussian processes. Finally, we compare the \nperformance of Student's-T processes against Gaussian process on canonical test \nproblems in Bayesian optimization, and apply the Student's-T process to the \noptimization of an aerostructural design problem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b7728", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06147"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Lam M. Nguyen, Nam H. Nguyen, Dzung T. Phan, Jayant R. Kalagnanam, Katya Scheinberg", "title": "When Does Stochastic Gradient Algorithm Work Well?. (arXiv:1801.06159v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.06159", "type": "text/html"}], "timestampUsec": "1516338583663300", "comments": [], "summary": {"content": "<p>In this paper, we consider a general stochastic optimization problem which is \noften at the core of supervised learning, such as deep learning and linear \nclassification. We consider a standard stochastic gradient descent (SGD) method \nwith a fixed, large step size and propose a novel assumption on the objective \nfunction, under which this method has the improved convergence rates (to a \nneighborhood of the optimal solutions). We then empirically demonstrate that \nthese assumptions hold for logistic regression and standard deep neural \nnetworks on classical data sets. Thus our analysis helps to explain when \nefficient behavior can be expected from the SGD method in training \nclassification models and deep neural networks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b772a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.06159"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tianyi Lin, Shiqian Ma, Shuzhong Zhang", "title": "Global Convergence of Unmodified 3-Block ADMM for a Class of Convex Minimization Problems. (arXiv:1505.04252v4 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1505.04252", "type": "text/html"}], "timestampUsec": "1516338583663299", "comments": [], "summary": {"content": "<p>The alternating direction method of multipliers (ADMM) has been successfully \napplied to solve structured convex optimization problems due to its superior \npractical performance. The convergence properties of the 2-block ADMM have been \nstudied extensively in the literature. Specifically, it has been proven that \nthe 2-block ADMM globally converges for any penalty parameter $\\gamma&gt;0$. In \nthis sense, the 2-block ADMM allows the parameter to be free, i.e., there is no \nneed to restrict the value for the parameter when implementing this algorithm \nin order to ensure convergence. However, for the 3-block ADMM, Chen \\etal \n\\cite{Chen-admm-failure-2013} recently constructed a counter-example showing \nthat it can diverge if no further condition is imposed. The existing results on \nstudying further sufficient conditions on guaranteeing the convergence of the \n3-block ADMM usually require $\\gamma$ to be smaller than a certain bound, which \nis usually either difficult to compute or too small to make it a practical \nalgorithm. In this paper, we show that the 3-block ADMM still globally \nconverges with any penalty parameter $\\gamma&gt;0$ if the third function $f_3$ in \nthe objective is smooth and strongly convex, and its condition number is in \n$[1,1.0798)$, besides some other mild conditions. This requirement covers an \nimportant class of problems to be called regularized least squares \ndecomposition (RLSD) in this paper. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b772e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1505.04252"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Bo Jiang, Tianyi Lin, Shiqian Ma, Shuzhong Zhang", "title": "Structured Nonconvex and Nonsmooth Optimization: Algorithms and Iteration Complexity Analysis. (arXiv:1605.02408v5 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1605.02408", "type": "text/html"}], "timestampUsec": "1516338583663298", "comments": [], "summary": {"content": "<p>Nonconvex and nonsmooth optimization problems are frequently encountered in \nmuch of statistics, business, science and engineering, but they are not yet \nwidely recognized as a technology in the sense of scalability. A reason for \nthis relatively low degree of popularity is the lack of a well developed system \nof theory and algorithms to support the applications, as is the case for its \nconvex counterpart. This paper aims to take one step in the direction of \ndisciplined nonconvex and nonsmooth optimization. In particular, we consider in \nthis paper some constrained nonconvex optimization models in block decision \nvariables, with or without coupled affine constraints. In the case of without \ncoupled constraints, we show a sublinear rate of convergence to an \n$\\epsilon$-stationary solution in the form of variational inequality for a \ngeneralized conditional gradient method, where the convergence rate is shown to \nbe dependent on the H\\\"olderian continuity of the gradient of the smooth part \nof the objective. For the model with coupled affine constraints, we introduce \ncorresponding $\\epsilon$-stationarity conditions, and apply two proximal-type \nvariants of the ADMM to solve such a model, assuming the proximal ADMM updates \ncan be implemented for all the block variables except for the last block, for \nwhich either a gradient step or a majorization-minimization step is \nimplemented. We show an iteration complexity bound of $O(1/\\epsilon^2)$ to \nreach an $\\epsilon$-stationary solution for both algorithms. Moreover, we show \nthat the same iteration complexity of a proximal BCD method follows \nimmediately. Numerical results are provided to illustrate the efficacy of the \nproposed algorithms for tensor robust PCA. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b7734", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1605.02408"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Nishant Subramani", "title": "PAG2ADMG: An Algorithm for the Complete Causal Enumeration of a Markov Equivalence Class. (arXiv:1612.00099v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1612.00099", "type": "text/html"}], "timestampUsec": "1516338583663297", "comments": [], "summary": {"content": "<p>Causal graphs, such as directed acyclic graphs (DAGs) and partial ancestral \ngraphs (PAGs), represent causal relationships among variables in a model. \nMethods exist for learning DAGs and PAGs from data and for converting DAGs to \nPAGs. However, these methods are significantly limited in that they only output \na single causal graph consistent with the independencies and dependencies (the \nMarkov equivalence class $M$) estimated from the data. This is problematic and \ninsufficient because many distinct graphs may be consistent with $M$. A data \nmodeler may wish to select among these numerous consistent graphs using domain \nknowledge or other model selection algorithms. Enumeration of the set of \nconsistent graphs is the bottleneck. In this paper, we present a method that \nmakes this desired enumeration possible. We introduce PAG2ADMG, the first \nalgorithm for enumerating all causal graphs consistent with $M$. PAG2ADMG \nconverts a given PAG into the complete set of acyclic directed mixed graphs \n(ADMGs) consistent with $M$. We prove the correctness of the approach and \ndemonstrate its efficiency relative to brute-force enumeration. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b7748", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.00099"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alina Beygelzimer, Francesco Orabona, Chicheng Zhang", "title": "Efficient Online Bandit Multiclass Learning with $\\tilde{O}(\\sqrt{T})$ Regret. (arXiv:1702.07958v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1702.07958", "type": "text/html"}], "timestampUsec": "1516338583663296", "comments": [], "summary": {"content": "<p>We present an efficient second-order algorithm with \n$\\tilde{O}(\\frac{1}{\\eta}\\sqrt{T})$ regret for the bandit online multiclass \nproblem. The regret bound holds simultaneously with respect to a family of loss \nfunctions parameterized by $\\eta$, for a range of $\\eta$ restricted by the norm \nof the competitor. The family of loss functions ranges from hinge loss \n($\\eta=0$) to squared hinge loss ($\\eta=1$). This provides a solution to the \nopen problem of (J. Abernethy and A. Rakhlin. An efficient bandit algorithm for \n$\\sqrt{T}$-regret in online multiclass prediction? In COLT, 2009). We test our \nalgorithm experimentally, showing that it also performs favorably against \nearlier algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b7756", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1702.07958"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zexun Chen, Bo Wang, Alexander N. Gorban", "title": "Multivariate Gaussian and Student$-t$ Process Regression for Multi-output Prediction. (arXiv:1703.04455v5 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.04455", "type": "text/html"}], "timestampUsec": "1516338583663295", "comments": [], "summary": {"content": "<p>Gaussian process for vector-valued function model has been shown to be a \nuseful method for multi-output prediction. The existing method for this model \nis to re-formulate the matrix-variate Gaussian distribution as a multivariate \nnormal distribution. Although it is effective in many cases, re-formulation is \nnot always workable and difficult to extend because not all matrix-variate \ndistributions can be transformed to related multivariate distributions, such as \nthe case for matrix-variate Student$-t$ distribution. In this paper, we propose \na new derivation of multivariate Gaussian process regression (MV-GPR), where \nthe model settings, derivations and computations are all directly performed in \nmatrix form, rather than vectorizing the matrices as done in the existing \nmethods. Furthermore, we introduce the multivariate Student$-t$ process and \nthen derive a new method, multivariate Student$-t$ process regression (MV-TPR) \nfor multi-output prediction. Both MV-GPR and MV-TPR have closed-form \nexpressions for the marginal likelihoods and predictive distributions. The \nusefulness of the proposed methods is illustrated through several simulated \nexamples. In particular, we verify empirically that MV-TPR has superiority for \nthe datasets considered, including air quality prediction and bike rent \nprediction. At last, the proposed methods are shown to produce profitable \ninvestment strategies in the stock markets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b775b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.04455"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tiago P. Peixoto", "title": "Nonparametric weighted stochastic block models. (arXiv:1708.01432v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.01432", "type": "text/html"}], "timestampUsec": "1516338583663294", "comments": [], "summary": {"content": "<p>We present a Bayesian formulation of weighted stochastic block models that \ncan be used to infer the large-scale modular structure of weighted networks, \nincluding their hierarchical organization. Our method is nonparametric, and \nthus does not require the prior knowledge of the number of groups or other \ndimensions of the model, which are instead inferred from data. We give a \ncomprehensive treatment of different kinds of edge weights (i.e. continuous or \ndiscrete, signed or unsigned, bounded or unbounded), as well as arbitrary \nweight transformations, and describe an unsupervised model selection approach \nto choose the best network description. We illustrate the application of our \nmethod to a variety of empirical weighted networks, such as global migrations, \nvoting patterns in congress, and neural connections in the human brain. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b7760", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.01432"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ali Pesaranghader, Herna Viktor, Eric Paquet", "title": "McDiarmid Drift Detection Methods for Evolving Data Streams. (arXiv:1710.02030v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.02030", "type": "text/html"}], "timestampUsec": "1516338583663293", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329075a5b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329075a5b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Increasingly, Internet of Things (IoT) domains, such as sensor networks, \nsmart cities, and social networks, generate vast amounts of data. Such data are \nnot only unbounded and rapidly evolving. Rather, the content thereof \ndynamically evolves over time, often in unforeseen ways. These variations are \ndue to so-called concept drifts, caused by changes in the underlying data \ngeneration mechanisms. In a classification setting, concept drift causes the \npreviously learned models to become inaccurate, unsafe and even unusable. \nAccordingly, concept drifts need to be detected, and handled, as soon as \npossible. In medical applications and emergency response settings, for example, \nchange in behaviours should be detected in near real-time, to avoid potential \nloss of life. To this end, we introduce the McDiarmid Drift Detection Method \n(MDDM), which utilizes McDiarmid's inequality in order to detect concept drift. \nThe MDDM approach proceeds by sliding a window over prediction results, and \nassociate window entries with weights. Higher weights are assigned to the most \nrecent entries, in order to emphasize their importance. As instances are \nprocessed, the detection algorithm compares a weighted mean of elements inside \nthe sliding window with the maximum weighted mean observed so far. A \nsignificant difference between the two weighted means, upper-bounded by the \nMcDiarmid inequality, implies a concept drift. Our extensive experimentation \nagainst synthetic and real-world data streams show that our novel method \noutperforms the state-of-the-art. Specifically, MDDM yields shorter detection \ndelays as well as lower false negative rates, while maintaining high \nclassification accuracies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b7763", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.02030"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Uiwon Hwang, Sungwoon Choi, Sungroh Yoon", "title": "Disease Prediction from Electronic Health Records Using Generative Adversarial Networks. (arXiv:1711.04126v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.04126", "type": "text/html"}], "timestampUsec": "1516338583663292", "comments": [], "summary": {"content": "<p>Electronic health records (EHRs) have contributed to the computerization of \npatient records so that they can be used not only for efficient and systematic \nmedical services, but also for research on data science. In this paper, we \ncompared the disease prediction performance of generative adversarial networks \n(GANs) and conventional learning algorithms in combination with missing value \nprediction methods. As a result, the highest accuracy of 98.05% was obtained \nusing a stacked autoencoder as the missing value prediction method and an \nauxiliary classifier GANs (AC-GANs) as the disease predicting method. Our \nresults show that the combination of the stacked autoencoder and the AC-GANs \nsignificantly outperforms existing algorithms for the problem of disease \nprediction in which missing values and class imbalance exist. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516338583663", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035e8b7767", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.04126"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "J Gerard Wolff", "title": "Solutions to problems with deep learning. (arXiv:1801.05457v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.05457", "type": "text/html"}], "timestampUsec": "1516256207359070", "comments": [], "summary": {"content": "<p>Despite the several successes of deep learning systems, there are concerns \nabout their limitations, discussed most recently by Gary Marcus. This paper \ndiscusses Marcus's concerns and some others, together with solutions to several \nof these problems provided by the \"P theory of intelligence\" and its \nrealisation in the \"SP computer model\". The main advantages of the SP system \nare: relatively small requirements for data and the ability to learn from a \nsingle experience; the ability to model both hierarchical and non-hierarchical \nstructures; strengths in several kinds of reasoning, including `commonsense' \nreasoning; transparency in the representation of knowledge, and the provision \nof an audit trail for all processing; the likelihood that the SP system could \nnot be fooled into bizarre or eccentric recognition of stimuli, as deep \nlearning systems can be; the SP system provides a robust solution to the \nproblem of `catastrophic forgetting' in deep learning systems; the SP system \nprovides a theoretically-coherent solution to the problems of correcting over- \nand under-generalisations in learning, and learning correct structures despite \nerrors in data; unlike most research on deep learning, the SP programme of \nresearch draws extensively on research on human learning, perception, and \ncognition; and the SP programme of research has an overarching theory, \nsupported by evidence, something that is largely missing from research on deep \nlearning. In general, the SP system provides a much firmer foundation than deep \nlearning for the development of artificial general intelligence. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516256207359", "annotations": [], "published": 1516256208, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dd12221", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05457"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jory Schossau, Larissa Albantakis, Arend Hintze", "title": "The Role of Conditional Independence in the Evolution of Intelligent Systems. (arXiv:1801.05462v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.05462", "type": "text/html"}], "timestampUsec": "1516256207359069", "comments": [], "summary": {"content": "<p>Systems are typically made from simple components regardless of their \ncomplexity. While the function of each part is easily understood, higher order \nfunctions are emergent properties and are notoriously difficult to explain. In \nnetworked systems, both digital and biological, each component receives inputs, \nperforms a simple computation, and creates an output. When these components \nhave multiple outputs, we intuitively assume that the outputs are causally \ndependent on the inputs but are themselves independent of each other given the \nstate of their shared input. However, this intuition can be violated for \ncomponents with probabilistic logic, as these typically cannot be decomposed \ninto separate logic gates with one output each. This violation of conditional \nindependence on the past system state is equivalent to instantaneous \ninteraction --- the idea is that some information between the outputs is not \ncoming from the inputs and thus must have been created instantaneously. Here we \ncompare evolved artificial neural systems with and without instantaneous \ninteraction across several task environments. We show that systems without \ninstantaneous interactions evolve faster, to higher final levels of \nperformance, and require fewer logic components to create a densely connected \ncognitive machinery. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516256207359", "annotations": [], "published": 1516256208, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dd1222c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05462"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ursula Challita, Walid Saad, Christian Bettstetter", "title": "Cellular-Connected UAVs over 5G: Deep Reinforcement Learning for Interference Management. (arXiv:1801.05500v1 [cs.IT])", "alternate": [{"href": "http://arxiv.org/abs/1801.05500", "type": "text/html"}], "timestampUsec": "1516256207359068", "comments": [], "summary": {"content": "<p>In this paper, an interference-aware path planning scheme for a network of \ncellular-connected unmanned aerial vehicles (UAVs) is proposed. In particular, \neach UAV aims at achieving a tradeoff between maximizing energy efficiency and \nminimizing both wireless latency and the interference level caused on the \nground network along its path. The problem is cast as a dynamic game among \nUAVs. To solve this game, a deep reinforcement learning algorithm, based on \necho state network (ESN) cells, is proposed. The introduced deep ESN \narchitecture is trained to allow each UAV to map each observation of the \nnetwork state to an action, with the goal of minimizing a sequence of \ntime-dependent utility functions. Each UAV uses ESN to learn its optimal path, \ntransmission power level, and cell association vector at different locations \nalong its path. The proposed algorithm is shown to reach a subgame perfect Nash \nequilibrium (SPNE) upon convergence. Moreover, an upper and lower bound for the \naltitude of the UAVs is derived thus reducing the computational complexity of \nthe proposed algorithm. Simulation results show that the proposed scheme \nachieves better wireless latency per UAV and rate per ground user (UE) while \nrequiring a number of steps that is comparable to a heuristic baseline that \nconsiders moving via the shortest distance towards the corresponding \ndestinations. The results also show that the optimal altitude of the UAVs \nvaries based on the ground network density and the UE data rate requirements \nand plays a vital role in minimizing the interference level on the ground UEs \nas well as the wireless transmission delay of the UAV. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516256207359", "annotations": [], "published": 1516256208, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dd12235", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05500"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sungwoon Choi, Heonseok Ha, Uiwon Hwang, Chanju Kim, Jung-Woo Ha, Sungroh Yoon", "title": "Reinforcement Learning based Recommender System using Biclustering Technique. (arXiv:1801.05532v1 [cs.IR])", "alternate": [{"href": "http://arxiv.org/abs/1801.05532", "type": "text/html"}], "timestampUsec": "1516256207359067", "comments": [], "summary": {"content": "<p>A recommender system aims to recommend items that a user is interested in \namong many items. The need for the recommender system has been expanded by the \ninformation explosion. Various approaches have been suggested for providing \nmeaningful recommendations to users. One of the proposed approaches is to \nconsider a recommender system as a Markov decision process (MDP) problem and \ntry to solve it using reinforcement learning (RL). However, existing RL-based \nmethods have an obvious drawback. To solve an MDP in a recommender system, they \nencountered a problem with the large number of discrete actions that bring RL \nto a larger class of problems. In this paper, we propose a novel RL-based \nrecommender system. We formulate a recommender system as a gridworld game by \nusing a biclustering technique that can reduce the state and action space \nsignificantly. Using biclustering not only reduces space but also improves the \nrecommendation quality effectively handling the cold-start problem. In \naddition, our approach can provide users with some explanation why the system \nrecommends certain items. Lastly, we examine the proposed algorithm on a \nreal-world dataset and achieve a better performance than the widely used \nrecommendation algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516256207359", "annotations": [], "published": 1516256208, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dd1223a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05532"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lei Shu, Hu Xu, Bing Liu", "title": "Unseen Class Discovery in Open-world Classification. (arXiv:1801.05609v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.05609", "type": "text/html"}], "timestampUsec": "1516256207359066", "comments": [], "summary": {"content": "<p>This paper concerns open-world classification, where the classifier not only \nneeds to classify test examples into seen classes that have appeared in \ntraining but also reject examples from unseen or novel classes that have not \nappeared in training. Specifically, this paper focuses on discovering the \nhidden unseen classes of the rejected examples. Clearly, without prior \nknowledge this is difficult. However, we do have the data from the seen \ntraining classes, which can tell us what kind of similarity/difference is \nexpected for examples from the same class or from different classes. It is \nreasonable to assume that this knowledge can be transferred to the rejected \nexamples and used to discover the hidden unseen classes in them. This paper \naims to solve this problem. It first proposes a joint open classification model \nwith a sub-model for classifying whether a pair of examples belongs to the same \nor different classes. This sub-model can serve as a distance function for \nclustering to discover the hidden classes of the rejected examples. \nExperimental results show that the proposed model is highly promising. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516256207359", "annotations": [], "published": 1516256208, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dd12245", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05609"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gary Marcus", "title": "Innateness, AlphaZero, and Artificial Intelligence. (arXiv:1801.05667v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.05667", "type": "text/html"}], "timestampUsec": "1516256207359065", "comments": [], "summary": {"content": "<p>The concept of innateness is rarely discussed in the context of artificial \nintelligence. When it is discussed, or hinted at, it is often the context of \ntrying to reduce the amount of innate machinery in a given system. In this \npaper, I consider as a test case a recent series of papers by Silver et al \n(Silver et al., 2017a) on AlphaGo and its successors that have been presented \nas an argument that a \"even in the most challenging of domains: it is possible \nto train to superhuman level, without human examples or guidance\", \"starting \ntabula rasa.\" \n</p> \n<p>I argue that these claims are overstated, for multiple reasons. I close by \narguing that artificial intelligence needs greater attention to innateness, and \nI point to some proposals about what that innateness might look like. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516256207359", "annotations": [], "published": 1516256208, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dd12249", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05667"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ryan Lowe, Michael Noseworthy, Iulian V. Serban, Nicolas Angelard-Gontier, Yoshua Bengio, Joelle Pineau", "title": "Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses. (arXiv:1708.07149v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.07149", "type": "text/html"}], "timestampUsec": "1516256207359064", "comments": [], "summary": {"content": "<p>Automatically evaluating the quality of dialogue responses for unstructured \ndomains is a challenging problem. Unfortunately, existing automatic evaluation \nmetrics are biased and correlate very poorly with human judgements of response \nquality. Yet having an accurate automatic evaluation procedure is crucial for \ndialogue research, as it allows rapid prototyping and testing of new models \nwith fewer expensive human evaluations. In response to this challenge, we \nformulate automatic dialogue evaluation as a learning problem. We present an \nevaluation model (ADEM) that learns to predict human-like scores to input \nresponses, using a new dataset of human response scores. We show that the ADEM \nmodel's predictions correlate significantly, and at a level much higher than \nword-overlap metrics such as BLEU, with human judgements at both the utterance \nand system-level. We also show that ADEM can generalize to evaluating dialogue \nmodels unseen during training, an important step for automatic dialogue \nevaluation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516256207359", "annotations": [], "published": 1516256208, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dd1224f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.07149"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Stephane Fotso", "title": "Deep Neural Networks for Survival Analysis Based on a Multi-Task Framework. (arXiv:1801.05512v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.05512", "type": "text/html"}], "timestampUsec": "1516256207359063", "comments": [], "summary": {"content": "<p>Survival analysis/time-to-event models are extremely useful as they can help \ncompanies predict when a customer will buy a product, churn or default on a \nloan, and therefore help them improve their ROI. In this paper, we introduce a \nnew method to calculate survival functions using the Multi-Task Logistic \nRegression (MTLR) model as its base and a deep learning architecture as its \ncore. Based on the Concordance index (C-index) and Brier score, this method \noutperforms the MTLR in all the experiments disclosed in this paper as well as \nthe Cox Proportional Hazard (CoxPH) model when nonlinear dependencies are \nfound. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516256207359", "annotations": [], "published": 1516256208, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dd12256", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05512"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yoonho Lee, Seungjin Choi", "title": "Meta-Learning with Adaptive Layerwise Metric and Subspace. (arXiv:1801.05558v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.05558", "type": "text/html"}], "timestampUsec": "1516256207359062", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329075c50\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329075c50&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Recent advances in meta-learning demonstrate that deep representations \ncombined with the gradient descent method have sufficient capacity to \napproximate any learning algorithm. A promising approach is the model-agnostic \nmeta-learning (MAML) which embeds gradient descent into the meta-learner. It \noptimizes for the initial parameters of the learner to warm-start the gradient \ndescent updates, such that new tasks can be solved using a small number of \nexamples. In this paper we elaborate the gradient-based meta-learning, \ndeveloping two new schemes. First, we present a feedforward neural network, \nreferred to as T-net, where the linear transformation between two adjacent \nlayers is decomposed as T W such that W is learned by task-specific learners \nand the transformation T, which is shared across tasks, is meta-learned to \nspeed up the convergence of gradient updates for task-specific learners. \nSecond, we present MT-net where gradient updates in the T-net are guided by a \nbinary mask M that is meta-learned, restricting the updates to be performed in \na subspace. Empirical results demonstrate that our method is less sensitive to \nthe choice of initial learning rates than existing meta-learning methods, and \nachieves the state-of-the-art or comparable performance on few-shot \nclassification and regression tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516256207359", "annotations": [], "published": 1516256208, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dd1225a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05558"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pavel Izmailov, Alexander Novikov, Dmitry Kropotov", "title": "Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition. (arXiv:1710.07324v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.07324", "type": "text/html"}], "timestampUsec": "1516256207359061", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3290ba678\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3290ba678&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a method (TT-GP) for approximate inference in Gaussian Process \n(GP) models. We build on previous scalable GP research including stochastic \nvariational inference based on inducing inputs, kernel interpolation, and \nstructure exploiting algebra. The key idea of our method is to use Tensor Train \ndecomposition for variational parameters, which allows us to train GPs with \nbillions of inducing inputs and achieve state-of-the-art results on several \nbenchmarks. Further, our approach allows for training kernels based on deep \nneural networks without any modifications to the underlying GP model. A neural \nnetwork learns a multidimensional embedding for the data, which is used by the \nGP to make the final prediction. We train GP and neural network parameters \nend-to-end without pretraining, through maximization of GP marginal likelihood. \nWe show the efficiency of the proposed approach on several regression and \nclassification benchmark datasets including MNIST, CIFAR-10, and Airline. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516256207359", "annotations": [], "published": 1516256208, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dd1225c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.07324"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, Igor Mordatch", "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments. (arXiv:1706.02275v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.02275", "type": "text/html"}], "timestampUsec": "1516253547490916", "comments": [], "summary": {"content": "<p>We explore deep reinforcement learning methods for multi-agent domains. We \nbegin by analyzing the difficulty of traditional algorithms in the multi-agent \ncase: Q-learning is challenged by an inherent non-stationarity of the \nenvironment, while policy gradient suffers from a variance that increases as \nthe number of agents grows. We then present an adaptation of actor-critic \nmethods that considers action policies of other agents and is able to \nsuccessfully learn policies that require complex multi-agent coordination. \nAdditionally, we introduce a training regimen utilizing an ensemble of policies \nfor each agent that leads to more robust multi-agent policies. We show the \nstrength of our approach compared to existing methods in cooperative as well as \ncompetitive scenarios, where agent populations are able to discover various \nphysical and informational coordination strategies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc0089", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.02275"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alessandro Bay, Biswa Sengupta", "title": "StackSeq2Seq: Dual Encoder Seq2Seq Recurrent Networks. (arXiv:1710.04211v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.04211", "type": "text/html"}], "timestampUsec": "1516253547490915", "comments": [], "summary": {"content": "<p>A widely studied non-deterministic polynomial time (NP) hard problem lies in \nfinding a route between the two nodes of a graph. Often meta-heuristics \nalgorithms such as $A^{*}$ are employed on graphs with a large number of nodes. \nHere, we propose a deep recurrent neural network architecture based on the \nSequence-2-Sequence (Seq2Seq) model, widely used, for instance in text \ntranslation. Particularly, we illustrate that utilising a context vector that \nhas been learned from two different recurrent networks enables increased \naccuracies in learning the shortest route of a graph. Additionally, we show \nthat one can boost the performance of the Seq2Seq network by smoothing the loss \nfunction using a homotopy continuation of the decoder's loss function. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc008d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.04211"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jiaming Song, Yuhuai Wu", "title": "An Empirical Analysis of Proximal Policy Optimization with Kronecker-factored Natural Gradients. (arXiv:1801.05566v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.05566", "type": "text/html"}], "timestampUsec": "1516253547490912", "comments": [], "summary": {"content": "<p>In this technical report, we consider an approach that combines the PPO \nobjective and K-FAC natural gradient optimization, for which we call PPOKFAC. \nWe perform a range of empirical analysis on various aspects of the algorithm, \nsuch as sample complexity, training speed, and sensitivity to batch size and \ntraining epochs. We observe that PPOKFAC is able to outperform PPO in terms of \nsample complexity and speed in a range of MuJoCo environments, while being \nscalable in terms of batch size. In spite of this, it seems that adding more \nepochs is not necessarily helpful for sample efficiency, and PPOKFAC seems to \nbe worse than its A2C counterpart, ACKTR. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc009f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05566"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ankur Sharma, Felix Martin Schuhknecht, Jens Dittrich", "title": "The Case for Automatic Database Administration using Deep Reinforcement Learning. (arXiv:1801.05643v1 [cs.DB])", "alternate": [{"href": "http://arxiv.org/abs/1801.05643", "type": "text/html"}], "timestampUsec": "1516253547490911", "comments": [], "summary": {"content": "<p>Like any large software system, a full-fledged DBMS offers an overwhelming \namount of configuration knobs. These range from static initialisation \nparameters like buffer sizes, degree of concurrency, or level of replication to \ncomplex runtime decisions like creating a secondary index on a particular \ncolumn or reorganising the physical layout of the store. To simplify the \nconfiguration, industry grade DBMSs are usually shipped with various advisory \ntools, that provide recommendations for given workloads and machines. However, \nreality shows that the actual configuration, tuning, and maintenance is usually \nstill done by a human administrator, relying on intuition and experience. \nRecent work on deep reinforcement learning has shown very promising results in \nsolving problems, that require such a sense of intuition. For instance, it has \nbeen applied very successfully in learning how to play complicated games with \nenormous search spaces. Motivated by these achievements, in this work we \nexplore how deep reinforcement learning can be used to administer a DBMS. \nFirst, we will describe how deep reinforcement learning can be used to \nautomatically tune an arbitrary software system like a DBMS by defining a \nproblem environment. Second, we showcase our concept of NoDBA at the concrete \nexample of index selection and evaluate how well it recommends indexes for \ngiven workloads. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc00ae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05643"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Olivier Cailloux, Yves Meinard", "title": "A formal framework for deliberated judgment. (arXiv:1801.05644v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.05644", "type": "text/html"}], "timestampUsec": "1516253547490910", "comments": [], "summary": {"content": "<p>While the philosophical literature has extensively studied how decisions \nrelate to arguments, reasons and justifications, decision theory almost \nentirely ignores the latter notions and rather focuses on preference and \nbelief. In this article, we argue that decision theory can largely benefit from \nexplicitly taking into account the stance that decision-makers take towards \narguments and counter-arguments. To that end, we elaborate a formal framework \naiming to integrate the role of arguments and argumentation in decision theory \nand decision aid. We start from a decision situation, where an individual \nrequests decision support. In this context, we formally define, as a \ncommendable basis for decision-aid, this individual's deliberated judgment, \npopularized by Rawls. We explain how models of deliberated judgment can be \nvalidated empirically. We then identify conditions upon which the existence of \na valid model can be taken for granted, and analyze how these conditions can be \nrelaxed. We then explore the significance of our proposed framework for \ndecision aiding practice. We argue that our concept of deliberated judgment \nowes its normative credentials both to its normative foundations (the idea of \nrationality based on arguments) and to its reference to empirical reality (the \nstance that real, empirical individuals hold towards arguments and \ncounter-arguments, on due reflection). We then highlight that our framework \nopens promising avenues for future research involving both philosophical and \ndecision theoretic approaches, as well as empirical implementations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc00bb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05644"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516338584, "author": "Fuyuan Xiao", "title": "A Generalized Dempster--Shafer Evidence Theory. (arXiv:1801.05707v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1801.05707", "type": "text/html"}], "timestampUsec": "1516253547490909", "comments": [], "summary": {"content": "<p>Dempster-Shafer evidence theory has been widely used in various fields of \napplications. Besides, it has been proven that the quantum theory has powerful \ncapabilities of solving the decision making problems. However, due to the \ninconsistency of the expression, the classical Dempster-Shafer evidence theory \nmodelled by real numbers can not be integrated directly with the quantum theory \nmodelled by complex numbers. The main contribution in this study is that, \nunlike the existing evidence theory, a mass function in the generalized \nDempster-Shafer evidence theory is modelled by a complex number, called as a \ncomplex mass function. When the complex mass function is degenerated from \ncomplex numbers to real numbers, the generalized Dempster's combination rule \ndegenerates to the classical evidence theory. This generalized Dempster-Shafer \nevidence theory provides a promising way to model and handle more uncertain \ninformation. Numerical examples are illustrated to show the efficiency of the \ngeneralized Dempster-Shafer evidence theory. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516338584, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc00c0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05707"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zhiyuan Xu, Jian Tang, Jingsong Meng, Weiyi Zhang, Yanzhi Wang, Chi Harold Liu, Dejun Yang", "title": "Experience-driven Networking: A Deep Reinforcement Learning based Approach. (arXiv:1801.05757v1 [cs.NI])", "alternate": [{"href": "http://arxiv.org/abs/1801.05757", "type": "text/html"}], "timestampUsec": "1516253547490908", "comments": [], "summary": {"content": "<p>Modern communication networks have become very complicated and highly \ndynamic, which makes them hard to model, predict and control. In this paper, we \ndevelop a novel experience-driven approach that can learn to well control a \ncommunication network from its own experience rather than an accurate \nmathematical model, just as a human learns a new skill (such as driving, \nswimming, etc). Specifically, we, for the first time, propose to leverage \nemerging Deep Reinforcement Learning (DRL) for enabling model-free control in \ncommunication networks; and present a novel and highly effective DRL-based \ncontrol framework, DRL-TE, for a fundamental networking problem: Traffic \nEngineering (TE). The proposed framework maximizes a widely-used utility \nfunction by jointly learning network environment and its dynamics, and making \ndecisions under the guidance of powerful Deep Neural Networks (DNNs). We \npropose two new techniques, TE-aware exploration and actor-critic-based \nprioritized experience replay, to optimize the general DRL framework \nparticularly for TE. To validate and evaluate the proposed framework, we \nimplemented it in ns-3, and tested it comprehensively with both representative \nand randomly generated network topologies. Extensive packet-level simulation \nresults show that 1) compared to several widely-used baseline methods, DRL-TE \nsignificantly reduces end-to-end delay and consistently improves the network \nutility, while offering better or comparable throughput; 2) DRL-TE is robust to \nnetwork changes; and 3) DRL-TE consistently outperforms a state-ofthe-art DRL \nmethod (for continuous control), Deep Deterministic Policy Gradient (DDPG), \nwhich, however, does not offer satisfying performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc00cb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05757"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jose M. Pe&#xf1;a", "title": "Unifying DAGs and UGs. (arXiv:1708.08722v5 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.08722", "type": "text/html"}], "timestampUsec": "1516253547490907", "comments": [], "summary": {"content": "<p>We introduce a new class of graphical models that generalizes \nLauritzen-Wermuth-Frydenberg chain graphs by relaxing the semi-directed \nacyclity constraint so that only directed cycles are forbidden. Moreover, up to \ntwo edges are allowed between any pair of nodes. Specifically, we present \nlocal, pairwise and global Markov properties for the new graphical models and \nprove their equivalence. We also present an equivalent factorization property. \nFinally, we develop an exact algorithm for learning the new models from data \nvia answer set programming, and we report preliminary results. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc00d4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.08722"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Guolei Sun, Xiangliang Zhang", "title": "Graph Embedding with Rich Information through Heterogeneous Network. (arXiv:1710.06879v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.06879", "type": "text/html"}], "timestampUsec": "1516253547490906", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3290baa3c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3290baa3c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Graph embedding has attracted increasing attention due to its critical \napplication in social network analysis. Most existing algorithms for graph \nembedding only rely on the typology information and fail to use the copious \ninformation in nodes as well as edges. As a result, their performance for many \ntasks may not be satisfactory. In this paper, we proposed a novel and general \nframework of representation learning for graph with rich text information \nthrough constructing a bipartite heterogeneous network. Specially, we designed \na biased random walk to explore the constructed heterogeneous network with the \nnotion of flexible neighborhood. The efficacy of our method is demonstrated by \nextensive comparison experiments with several baselines on various datasets. It \nimproves the Micro-F1 and Macro-F1 of node classification by 10% and 7% on Cora \ndataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc00e1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.06879"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "W. James Murdoch, Peter J. Liu, Bin Yu", "title": "Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs. (arXiv:1801.05453v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1801.05453", "type": "text/html"}], "timestampUsec": "1516253547490904", "comments": [], "summary": {"content": "<p>The driving force behind the recent success of LSTMs has been their ability \nto learn complex and non-linear relationships. Consequently, our inability to \ndescribe these relationships has led to LSTMs being characterized as black \nboxes. To this end, we introduce contextual decomposition (CD), an \ninterpretation algorithm for analysing individual predictions made by standard \nLSTMs, without any changes to the underlying model. By decomposing the output \nof a LSTM, CD captures the contributions of combinations of words or variables \nto the final prediction of an LSTM. On the task of sentiment analysis with the \nYelp and SST data sets, we show that CD is able to reliably identify words and \nphrases of contrasting sentiment, and how they are combined to yield the LSTM's \nfinal prediction. Using the phrase-level labels in SST, we also demonstrate \nthat CD is able to successfully extract positive and negative negations from an \nLSTM, something which has not previously been done. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc00e9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05453"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Fady Medhat, David Chesmore, John Robinson", "title": "Automatic Classification of Music Genre using Masked Conditional Neural Networks. (arXiv:1801.05504v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1801.05504", "type": "text/html"}], "timestampUsec": "1516253547490903", "comments": [], "summary": {"content": "<p>Neural network based architectures used for sound recognition are usually \nadapted from other application domains such as image recognition, which may not \nharness the time-frequency representation of a signal. The ConditionaL Neural \nNetworks (CLNN) and its extension the Masked ConditionaL Neural Networks \n(MCLNN) are designed for multidimensional temporal signal recognition. The CLNN \nis trained over a window of frames to preserve the inter-frame relation, and \nthe MCLNN enforces a systematic sparseness over the network's links that mimics \na filterbank-like behavior. The masking operation induces the network to learn \nin frequency bands, which decreases the network susceptibility to \nfrequency-shifts in time-frequency representations. Additionally, the mask \nallows an exploration of a range of feature combinations concurrently analogous \nto the manual handcrafting of the optimum collection of features for a \nrecognition task. MCLNN have achieved competitive performance on the Ballroom \nmusic dataset compared to several hand-crafted attempts and outperformed models \nbased on state-of-the-art Convolutional Neural Networks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc00f5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05504"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ying Lu, Liming Chen, Alexandre Saidi, Xianfeng Gu", "title": "Brenier approach for optimal transportation between a quasi-discrete measure and a discrete measure. (arXiv:1801.05574v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.05574", "type": "text/html"}], "timestampUsec": "1516253547490902", "comments": [], "summary": {"content": "<p>Correctly estimating the discrepancy between two data distributions has \nalways been an important task in Machine Learning. Recently, Cuturi proposed \nthe Sinkhorn distance which makes use of an approximate Optimal Transport cost \nbetween two distributions as a distance to describe distribution discrepancy. \nAlthough it has been successfully adopted in various machine learning \napplications (e.g. in Natural Language Processing and Computer Vision) since \nthen, the Sinkhorn distance also suffers from two unnegligible limitations. The \nfirst one is that the Sinkhorn distance only gives an approximation of the real \nWasserstein distance, the second one is the `divide by zero' problem which \noften occurs during matrix scaling when setting the entropy regularization \ncoefficient to a small value. In this paper, we introduce a new Brenier \napproach for calculating a more accurate Wasserstein distance between two \ndiscrete distributions, this approach successfully avoids the two limitations \nshown above for Sinkhorn distance and gives an alternative way for estimating \ndistribution discrepancy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc0106", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05574"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Franck Iutzeler (1), Jerome Malick (1) ((1) DAO)", "title": "On the Proximal Gradient Algorithm with Alternated Inertia. (arXiv:1801.05589v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1801.05589", "type": "text/html"}], "timestampUsec": "1516253547490901", "comments": [], "summary": {"content": "<p>In this paper, we investigate the attractive properties of the proximal \ngradient algorithm with inertia. Notably, we show that using alternated inertia \nyields monotonically decreasing functional values, which contrasts with usual \naccelerated proximal gradient methods. We also provide convergence rates for \nthe algorithm with alternated inertia based on local geometric properties of \nthe objective function. The results are put into perspective by discussions on \nseveral extensions and illustrations on common regularized problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc0114", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05589"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Stephan Cl&#xe9;men&#xe7;on, Mastane Achab", "title": "Ranking Data with Continuous Labels through Oriented Recursive Partitions. (arXiv:1801.05772v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.05772", "type": "text/html"}], "timestampUsec": "1516253547490900", "comments": [], "summary": {"content": "<p>We formulate a supervised learning problem, referred to as continuous \nranking, where a continuous real-valued label Y is assigned to an observable \nr.v. X taking its values in a feature space $\\mathcal{X}$ and the goal is to \norder all possible observations x in $\\mathcal{X}$ by means of a scoring \nfunction $s:\\mathcal{X}\\rightarrow \\mathbb{R}$ so that s(X) and Y tend to \nincrease or decrease together with highest probability. This problem \ngeneralizes bi/multi-partite ranking to a certain extent and the task of \nfinding optimal scoring functions s(x) can be naturally cast as optimization of \na dedicated functional criterion, called the IROC curve here, or as \nmaximization of the Kendall ${\\tau}$ related to the pair (s(X), Y ). From the \ntheoretical side, we describe the optimal elements of this problem and provide \nstatistical guarantees for empirical Kendall ${\\tau}$ maximization under \nappropriate conditions for the class of scoring function candidates. We also \npropose a recursive statistical learning algorithm tailored to empirical IROC \ncurve optimization and producing a piecewise constant scoring function that is \nfully described by an oriented binary tree. Preliminary numerical experiments \nhighlight the difference in nature between regression and continuous ranking \nand provide strong empirical evidence of the performance of empirical \noptimizers of the criteria proposed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc0119", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05772"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Lucas Theis, Iryna Korshunova, Alykhan Tejani, Ferenc Husz&#xe1;r", "title": "Faster gaze prediction with dense networks and Fisher pruning. (arXiv:1801.05787v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.05787", "type": "text/html"}], "timestampUsec": "1516253547490899", "comments": [], "summary": {"content": "<p>Predicting human fixations from images has recently seen large improvements \nby leveraging deep representations which were pretrained for object \nrecognition. However, as we show in this paper, these networks are highly \noverparameterized for the task of fixation prediction. We first present a \nsimple yet principled greedy pruning method which we call Fisher pruning. \nThrough a combination of knowledge distillation and Fisher pruning, we obtain \nmuch more runtime-efficient architectures for saliency prediction, achieving a \n10x speedup for the same AUC performance as a state of the art network on the \nCAT2000 dataset. Speeding up single-image gaze prediction is important for many \nreal-world applications, but it is also a crucial step in the development of \nvideo saliency models, where the amount of data to be processed is \nsubstantially larger. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc011e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05787"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Stratis Ioannidis, Andrea Montanari", "title": "Learning Combinations of Sigmoids Through Gradient Estimation. (arXiv:1708.06678v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.06678", "type": "text/html"}], "timestampUsec": "1516253547490897", "comments": [], "summary": {"content": "<p>We develop a new approach to learn the parameters of regression models with \nhidden variables. In a nutshell, we estimate the gradient of the regression \nfunction at a set of random points, and cluster the estimated gradients. The \ncenters of the clusters are used as estimates for the parameters of hidden \nunits. We justify this approach by studying a toy model, whereby the regression \nfunction is a linear combination of sigmoids. We prove that indeed the \nestimated gradients concentrate around the parameter vectors of the hidden \nunits, and provide non-asymptotic bounds on the number of required samples. To \nthe best of our knowledge, no comparable guarantees have been proven for linear \ncombinations of sigmoids. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc012a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.06678"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Romain Lopez, Jeffrey Regier, Michael Cole, Michael Jordan, Nir Yosef", "title": "A deep generative model for gene expression profiles from single-cell RNA sequencing. (arXiv:1709.02082v4 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.02082", "type": "text/html"}], "timestampUsec": "1516253547490896", "comments": [], "summary": {"content": "<p>We propose a probabilistic model for interpreting gene expression levels that \nare observed through single-cell RNA sequencing. In the model, each cell has a \nlow-dimensional latent representation. Additional latent variables account for \ntechnical effects that may erroneously set some observations of gene expression \nlevels to zero. Conditional distributions are specified by neural networks, \ngiving the proposed model enough flexibility to fit the data well. We use \nvariational inference and stochastic optimization to approximate the posterior \ndistribution. The inference procedure scales to over one million cells, whereas \ncompeting algorithms do not. Even for smaller datasets, for several tasks, the \nproposed procedure outperforms state-of-the-art methods like ZIFA and \nZINB-WaVE. We also extend our framework to account for batch effects and other \nconfounding factors, and propose a Bayesian hypothesis test for differential \nexpression that outperforms DESeq2. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc012f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.02082"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein", "title": "Deep Neural Networks as Gaussian Processes. (arXiv:1711.00165v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.00165", "type": "text/html"}], "timestampUsec": "1516253547490895", "comments": [], "summary": {"content": "<p>A deep fully-connected neural network with an i.i.d. prior over its \nparameters is equivalent to a Gaussian process (GP) in the limit of infinite \nnetwork width. This correspondence enables exact Bayesian inference for neural \nnetworks on regression tasks by means of straightforward matrix computations. \nFor single hidden-layer networks, the covariance function of this GP has long \nbeen known. Recently, kernel functions for multi-layer random neural networks \nhave been developed, but only outside of a Bayesian framework. As such, \nprevious work has not identified the correspondence between using these kernels \nas the covariance function for a GP and performing fully Bayesian prediction \nwith a deep neural network. In this work, we derive this correspondence and \ndevelop a computationally efficient pipeline to compute the covariance \nfunctions. We then use the resulting GP to perform Bayesian inference for deep \nneural networks on MNIST and CIFAR-10. We find that the GP-based predictions \nare competitive and can outperform neural networks trained with stochastic \ngradient descent. We observe that the trained neural network accuracy \napproaches that of the corresponding GP-based computation with increasing layer \nwidth, and that the GP uncertainty is strongly correlated with prediction \nerror. We connect our observations to the recent development of signal \npropagation in random neural networks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc0135", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.00165"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Irina Gaynanova, Tianying Wang", "title": "Sparse quadratic classification rules via linear dimension reduction. (arXiv:1711.04817v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.04817", "type": "text/html"}], "timestampUsec": "1516253547490894", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3290badf7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3290badf7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We consider the problem of high-dimensional classification between the two \ngroups with unequal covariance matrices. Rather than estimating the full \nquadratic discriminant rule, we propose to perform simultaneous variable \nselection and linear dimension reduction on original data, with the subsequent \napplication of quadratic discriminant analysis on the reduced space. In \ncontrast to quadratic discriminant analysis, the proposed framework doesn't \nrequire estimation of precision matrices and scales linearly with the number of \nmeasurements, making it especially attractive for the use on high-dimensional \ndatasets. We support the methodology with theoretical guarantees on variable \nselection consistency, and empirical comparison with competing approaches. We \napply the method to gene expression data of breast cancer patients, and confirm \nthe crucial importance of ESR1 gene in differentiating estrogen receptor \nstatus. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc013d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.04817"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kerem C. Tezcan, Christian F. Baumgartner, Ender Konukoglu", "title": "MR image reconstruction using deep density priors. (arXiv:1711.11386v3 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.11386", "type": "text/html"}], "timestampUsec": "1516253547490893", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329126c17\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329126c17&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Purpose: MR image reconstruction exploits regularization to compensate for \nmissing k-space data. In this work, we propose to learn the probability \ndistribution of MR image patches with neural networks and use this distribution \nas prior information constraining images during reconstruction, effectively \nemploying it as regularization. \n</p> \n<p>Methods: We use variational autoencoders (VAE) to learn the distribution of \nMR image patches, which models the high-dimensional distribution by a latent \nparameter model of lower dimensions in a non-linear fashion. The proposed \nalgorithm uses the learned prior in a Maximum-A-Posteriori estimation \nformulation. We evaluate the proposed reconstruction method with T1 weighted \nimages and also apply our method on images with white matter lesions. \n</p> \n<p>Results: Visual evaluation of the samples showed that the VAE algorithm can \napproximate the distribution of MR patches well. The proposed reconstruction \nalgorithm using the VAE prior produced high quality reconstructions. The \nalgorithm achieved normalized RMSE, CNR and CN values of 2.77\\%, 0.43, 0.11; \n4.29\\%, 0.43, 0.11, 6.36\\%, 0.47, 0.11 and 10.00\\%, 0.42, 0.10 for \nundersampling ratios of 2, 3, 4 and 5, respectively, where it outperformed most \nof the alternative methods. In the experiments on images with white matter \nlesions, the method faithfully reconstructed the lesions. \n</p> \n<p>Conclusion: We introduced a novel method for MR reconstruction, which takes a \nnew perspective on regularization by using priors learned by neural networks. \nResults suggest the method compares favorably against the other evaluated \nmethods and can reconstruct lesions as well. \n</p> \n<p>Keywords: Reconstruction, MRI, prior probability, MAP estimation, machine \nlearning, variational inference, deep learning \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc0146", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11386"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Guo Yu, Jacob Bien", "title": "Estimating the error variance in a high-dimensional linear model. (arXiv:1712.02412v2 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02412", "type": "text/html"}], "timestampUsec": "1516253547490892", "comments": [], "summary": {"content": "<p>The lasso has been studied extensively as a tool for estimating the \ncoefficient vector in the high-dimensional linear model; however, considerably \nless is known about estimating the error variance. Indeed, most well-known \ntheoretical properties of the lasso, including recent advances in selective \ninference with the lasso, are established under the assumption that the \nunderlying error variance is known. Yet the error variance in practice is, of \ncourse, unknown. In this paper, we propose the natural lasso estimator for the \nerror variance, which maximizes a penalized likelihood objective. A key aspect \nof the natural lasso is that the likelihood is expressed in terms of the \nnatural parameterization of the multiparameter exponential family of a Gaussian \nwith unknown mean and variance. The result is a remarkably simple estimator \nwith provably good performance in terms of mean squared error. These \ntheoretical results do not require placing any assumptions on the design matrix \nor the true regression coefficients. We also propose a companion estimator, \ncalled the organic lasso, which theoretically does not require tuning of the \nregularization parameter. Both estimators do well compared to preexisting \nmethods, especially in settings where successful recovery of the true support \nof the coefficient vector is hard. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516253547491", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035dcc014b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02412"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shumeet Baluja", "title": "Empirical Explorations in Training Networks with Discrete Activations. (arXiv:1801.05156v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.05156", "type": "text/html"}], "timestampUsec": "1516169304791576", "comments": [], "summary": {"content": "<p>We present extensive experiments training and testing hidden units in deep \nnetworks that emit only a predefined, static, number of discretized values. \nThese units provide benefits in real-world deployment in systems in which \nmemory and/or computation may be limited. Additionally, they are particularly \nwell suited for use in large recurrent network models that require the \nmaintenance of large amounts of internal state in memory. Surprisingly, we find \nthat despite reducing the number of values that can be represented in the \noutput activations from $2^{32}-2^{64}$ to between 64 and 256, there is little \nto no degradation in network performance across a variety of different \nsettings. We investigate simple classification and regression tasks, as well as \nmemorization and compression problems. We compare the results with more \nstandard activations, such as tanh and relu. Unlike previous discretization \nstudies which often concentrate only on binary units, we examine the effects of \nvarying the number of allowed activation levels. Compared to existing \napproaches for discretization, the approach presented here is both conceptually \nand programatically simple, has no stochastic component, and allows the \ntraining, testing, and usage phases to be treated in exactly the same manner. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516169304792", "annotations": [], "published": 1516169305, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d0be2a4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05156"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, Anders S&#xf8;gaard", "title": "Learning what to share between loosely related tasks. (arXiv:1705.08142v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08142", "type": "text/html"}], "timestampUsec": "1516169304791575", "comments": [], "summary": {"content": "<p>Multi-task learning is motivated by the observation that humans bring to bear \nwhat they know about related problems when solving new ones. Similarly, deep \nneural networks can profit from related tasks by sharing parameters with other \nnetworks. However, humans do not consciously decide to transfer knowledge \nbetween tasks. In Natural Language Processing (NLP), it is hard to predict if \nsharing will lead to improvements, particularly if tasks are only loosely \nrelated. To overcome this, we introduce Sluice Networks, a general framework \nfor multi-task learning where trainable parameters control the amount of \nsharing. Our framework generalizes previous proposals in enabling sharing of \nall combinations of subspaces, layers, and skip connections. We perform \nexperiments on three task pairs, and across seven different domains, using data \nfrom OntoNotes 5.0, and achieve up to 15% average error reductions over common \napproaches to multi-task learning. We show that a) label entropy is predictive \nof gains in sluice networks, confirming findings for hard parameter sharing and \nb) while sluice networks easily fit noise, they are robust across domains in \npractice. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516169304792", "annotations": [], "published": 1516169305, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d0be2b1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08142"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Feng-Lin Li, Minghui Qiu, Haiqing Chen, Xiongwei Wang, Xing Gao, Jun Huang, Juwei Ren, Zhongzhou Zhao, Weipeng Zhao, Lei Wang, Guwei Jin, Wei Chu", "title": "AliMe Assist: An Intelligent Assistant for Creating an Innovative E-commerce Experience. (arXiv:1801.05032v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1801.05032", "type": "text/html"}], "timestampUsec": "1516169304791574", "comments": [], "summary": {"content": "<p>We present AliMe Assist, an intelligent assistant designed for creating an \ninnovative online shopping experience in E-commerce. Based on question \nanswering (QA), AliMe Assist offers assistance service, customer service, and \nchatting service. It is able to take voice and text input, incorporate context \nto QA, and support multi-round interaction. Currently, it serves millions of \ncustomer questions per day and is able to address 85% of them. In this paper, \nwe demonstrate the system, present the underlying techniques, and share our \nexperience in dealing with real-world QA in the E-commerce field. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516169304792", "annotations": [], "published": 1516169305, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d0be2bd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05032"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sina Mohseni, Eric D. Ragan", "title": "A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning. (arXiv:1801.05075v1 [cs.HC])", "alternate": [{"href": "http://arxiv.org/abs/1801.05075", "type": "text/html"}], "timestampUsec": "1516169304791573", "comments": [], "summary": {"content": "<p>In order for people to be able to trust and take advantage of the results of \nadvanced machine learning and artificial intelligence solutions for real \ndecision making, people need to be able to understand the machine rationale for \ngiven output. Research in explain artificial intelligence (XAI) addresses the \naim, but there is a need for evaluation of human relevance and \nunderstandability of explanations. Our work contributes a novel methodology for \nevaluating the quality or human interpretability of explanations for machine \nlearning models. We present an evaluation benchmark for instance explanations \nfrom text and image classifiers. The explanation meta-data in this benchmark is \ngenerated from user annotations of image and text samples. We describe the \nbenchmark and demonstrate its utility by a quantitative evaluation on \nexplanations generated from a recent machine learning algorithm. This research \ndemonstrates how human-grounded evaluation could be used as a measure to \nqualify local machine-learning explanations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516169304792", "annotations": [], "published": 1516169305, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d0be2c3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05075"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chandra Khatri, Michael Hunter, Bistra Dilkina, Richard Fujimoto, Kari Watkins", "title": "Real-time Road Traffic Information Detection Through Social Media. (arXiv:1801.05088v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1801.05088", "type": "text/html"}], "timestampUsec": "1516169304791572", "comments": [], "summary": {"content": "<p>In current study, a mechanism to extract traffic related information such as \ncongestion and incidents from textual data from the internet is proposed. The \ncurrent source of data is Twitter. As the data being considered is extremely \nlarge in size automated models are developed to stream, download, and mine the \ndata in real-time. Furthermore, if any tweet has traffic related information \nthen the models should be able to infer and extract this data. \n</p> \n<p>Currently, the data is collected only for United States and a total of \n120,000 geo-tagged traffic related tweets are extracted, while six million \ngeo-tagged non-traffic related tweets are retrieved and classification models \nare trained. Furthermore, this data is used for various kinds of spatial and \ntemporal analysis. A mechanism to calculate level of traffic congestion, \nsafety, and traffic perception for cities in U.S. is proposed. Traffic \ncongestion and safety rankings for the various urban areas are obtained and \nthen they are statistically validated with existing widely adopted rankings. \nTraffic perception depicts the attitude and perception of people towards the \ntraffic. \n</p> \n<p>It is also seen that traffic related data when visualized spatially and \ntemporally provides the same pattern as the actual traffic flows for various \nurban areas. When visualized at the city level, it is clearly visible that the \nflow of tweets is similar to flow of vehicles and that the traffic related \ntweets are representative of traffic within the cities. With all the findings \nin current study, it is shown that significant amount of traffic related \ninformation can be extracted from Twitter and other sources on internet. \nFurthermore, Twitter and these data sources are freely available and are not \nbound by spatial and temporal limitations. That is, wherever there is a user \nthere is a potential for data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516169304792", "annotations": [], "published": 1516169305, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d0be2c8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05088"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kamil Bennani-Smires, Claudiu Musat, Andreea Hossmann, Michael Baeriswyl", "title": "GitGraph - Architecture Search Space Creation through Frequent Computational Subgraph Mining. (arXiv:1801.05159v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.05159", "type": "text/html"}], "timestampUsec": "1516169304791571", "comments": [], "summary": {"content": "<p>The dramatic success of deep neural networks across multiple application \nareas often relies on experts painstakingly designing a network architecture \nspecific to each task. To simplify this process and make it more accessible, an \nemerging research effort seeks to automate the design of neural network \narchitectures, using e.g. evolutionary algorithms or reinforcement learning or \nsimple search in a constrained space of neural modules. \n</p> \n<p>Considering the typical size of the search space (e.g. $10^{10}$ candidates \nfor a $10$-layer network) and the cost of evaluating a single candidate, \ncurrent architecture search methods are very restricted. They either rely on \nstatic pre-built modules to be recombined for the task at hand, or they define \na static hand-crafted framework within which they can generate new \narchitectures from the simplest possible operations. \n</p> \n<p>In this paper, we relax these restrictions, by capitalizing on the collective \nwisdom contained in the plethora of neural networks published in online code \nrepositories. Concretely, we (a) extract and publish GitGraph, a corpus of \nneural architectures and their descriptions; (b) we create problem-specific \nneural architecture search spaces, implemented as a textual search mechanism \nover GitGraph; (c) we propose a method of identifying unique common subgraphs \nwithin the architectures solving each problem (e.g., image processing, \nreinforcement learning), that can then serve as modules in the newly created \nproblem specific neural search space. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516169304792", "annotations": [], "published": 1516169305, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d0be2d5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05159"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mohammad Javad Shafiee, Brendan Chwyl, Francis Li, Rongyan Chen, Michelle Karg, Christian Scharfenberger, Alexander Wong", "title": "StressedNets: Efficient Feature Representations via Stress-induced Evolutionary Synthesis of Deep Neural Networks. (arXiv:1801.05387v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.05387", "type": "text/html"}], "timestampUsec": "1516169304791570", "comments": [], "summary": {"content": "<p>The computational complexity of leveraging deep neural networks for \nextracting deep feature representations is a significant barrier to its \nwidespread adoption, particularly for use in embedded devices. One particularly \npromising strategy to addressing the complexity issue is the notion of \nevolutionary synthesis of deep neural networks, which was demonstrated to \nsuccessfully produce highly efficient deep neural networks while retaining \nmodeling performance. Here, we further extend upon the evolutionary synthesis \nstrategy for achieving efficient feature extraction via the introduction of a \nstress-induced evolutionary synthesis framework, where stress signals are \nimposed upon the synapses of a deep neural network during training to induce \nstress and steer the synthesis process towards the production of more efficient \ndeep neural networks over successive generations and improved model fidelity at \na greater efficiency. The proposed stress-induced evolutionary synthesis \napproach is evaluated on a variety of different deep neural network \narchitectures (LeNet5, AlexNet, and YOLOv2) on different tasks (object \nclassification and object detection) to synthesize efficient StressedNets over \nmultiple generations. Experimental results demonstrate the efficacy of the \nproposed framework to synthesize StressedNets with significant improvement in \nnetwork architecture efficiency (e.g., 40x for AlexNet and 33x for YOLOv2) and \nspeed improvements (e.g., 5.5x inference speed-up for YOLOv2 on an Nvidia Tegra \nX1 mobile processor). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516169304792", "annotations": [], "published": 1516169305, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d0be2dd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05387"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xinyuan Zhang, Ricardo Henao, Zhe Gan, Yitong Li, Lawrence Carin", "title": "Multi-Label Learning from Medical Plain Text with Convolutional Residual Models. (arXiv:1801.05062v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.05062", "type": "text/html"}], "timestampUsec": "1516169304791569", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329126e25\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329126e25&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Predicting diagnoses from Electronic Health Records (EHRs) is an important \nmedical application of multi-label learning. We propose a convolutional \nresidual model for multi-label classification from doctor notes in EHR data. A \ngiven patient may have multiple diagnoses, and therefore multi-label learning \nis required. We employ a Convolutional Neural Network (CNN) to encode plain \ntext into a fixed-length sentence embedding vector. Since diagnoses are \ntypically correlated, a deep residual network is employed on top of the CNN \nencoder, to capture label (diagnosis) dependencies and incorporate information \ndirectly from the encoded sentence vector. A real EHR dataset is considered, \nand we compare the proposed model with several well-known baselines, to predict \ndiagnoses based on doctor notes. Experimental results demonstrate the \nsuperiority of the proposed convolutional residual model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516169304792", "annotations": [], "published": 1516169305, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d0be2eb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05062"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiang Li, Shuo Chen, Xiaolin Hu, Jian Yang", "title": "Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift. (arXiv:1801.05134v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.05134", "type": "text/html"}], "timestampUsec": "1516169304791568", "comments": [], "summary": {"content": "<p>This paper first answers the question \"why do the two most powerful \ntechniques Dropout and Batch Normalization (BN) often lead to a worse \nperformance when they are combined together?\" in both theoretical and \nstatistical aspects. Theoretically, we find that Dropout would shift the \nvariance of a specific neural unit when we transfer the state of that network \nfrom train to test. However, BN would maintain its statistical variance, which \nis accumulated from the entire learning procedure, in the test phase. The \ninconsistency of that variance (we name this scheme as \"variance shift\") causes \nthe unstable numerical behavior in inference that leads to more erroneous \npredictions finally, when applying Dropout before BN. Thorough experiments on \nDenseNet, ResNet, ResNeXt and Wide ResNet confirm our findings. According to \nthe uncovered mechanism, we next explore several strategies that modifies \nDropout and try to overcome the limitations of their combination by avoiding \nthe variance shift risks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516169304792", "annotations": [], "published": 1516169305, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d0be2f6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05134"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1516256208, "author": "Josh Gardner, Christopher Brooks, Juan Miguel L. Andres, Ryan Baker", "title": "MORF: A Framework for MOOC Predictive Modeling and Replication At Scale. (arXiv:1801.05236v2 [cs.SE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1801.05236", "type": "text/html"}], "timestampUsec": "1516169304791567", "comments": [], "summary": {"content": "<p>The MOOC Replication Framework (MORF) is a novel software system for feature \nextraction, model training/testing, and evaluation of predictive dropout models \nin Massive Open Online Courses (MOOCs). MORF makes large-scale replication of \ncomplex machine-learned models tractable and accessible for researchers, and \nenables public research on privacy-protected data. It does so by focusing on \nthe high-level operations of an extract-train-test-evaluate workflow, and \nenables researchers to encapsulate their implementations in portable, fully \nreproducible software containers which are executed on data with a known \nschema. MORF's workflow allows researchers to use data in analysis without \nproviding them access to the underlying data directly, preserving privacy and \ndata security. During execution, containers are sandboxed for security and data \nleakage and parallelized for efficiency, allowing researchers to create and \ntest new models rapidly, on large-scale multi-institutional datasets that were \npreviously inaccessible to most researchers. MORF is provided both as a Python \nAPI (the MORF Software), for institutions to use on their own MOOC data) or in \na platform-as-a-service (PaaS) model with a web API and a high-performance \ncomputing environment (the MORF Platform). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516169304792", "annotations": [], "published": 1516256208, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d0be303", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05236"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Neil Mallinar, Corbin Rosset", "title": "Deep Canonically Correlated LSTMs. (arXiv:1801.05407v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.05407", "type": "text/html"}], "timestampUsec": "1516169304791566", "comments": [], "summary": {"content": "<p>We examine Deep Canonically Correlated LSTMs as a way to learn nonlinear \ntransformations of variable length sequences and embed them into a correlated, \nfixed dimensional space. We use LSTMs to transform multi-view time-series data \nnon-linearly while learning temporal relationships within the data. We then \nperform correlation analysis on the outputs of these neural networks to find a \ncorrelated subspace through which we get our final representation via \nprojection. This work follows from previous work done on Deep Canonical \nCorrelation (DCCA), in which deep feed-forward neural networks were used to \nlearn nonlinear transformations of data while maximizing correlation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516169304792", "annotations": [], "published": 1516169305, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d0be30e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05407"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pratik Chaudhari, Stefano Soatto", "title": "Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. (arXiv:1710.11029v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.11029", "type": "text/html"}], "timestampUsec": "1516169304791565", "comments": [], "summary": {"content": "<p>Stochastic gradient descent (SGD) is widely believed to perform implicit \nregularization when used to train deep neural networks, but the precise manner \nin which this occurs has thus far been elusive. We prove that SGD minimizes an \naverage potential over the posterior distribution of weights along with an \nentropic regularization term. This potential is however not the original loss \nfunction in general. So SGD does perform variational inference, but for a \ndifferent loss than the one used to compute the gradients. Even more \nsurprisingly, SGD does not even converge in the classical sense: we show that \nthe most likely trajectories of SGD for deep networks do not behave like \nBrownian motion around critical points. Instead, they resemble closed loops \nwith deterministic components. We prove that such \"out-of-equilibrium\" behavior \nis a consequence of highly non-isotropic gradient noise in SGD; the covariance \nmatrix of mini-batch gradients for deep networks has a rank as small as 1% of \nits dimension. We provide extensive empirical validation of these claims, \nproven in the appendix. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516169304792", "annotations": [], "published": 1516169305, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d0be315", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.11029"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Boyang Deng, Qing Liu, Siyuan Qiao, Alan Yuille", "title": "Unleashing the Potential of CNNs for Interpretable Few-Shot Learning. (arXiv:1711.08277v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08277", "type": "text/html"}], "timestampUsec": "1516169304791564", "comments": [], "summary": {"content": "<p>Convolutional neural networks (CNNs) have been generally acknowledged as one \nof the driving forces for the advancement of computer vision. Despite their \npromising performances on many tasks, CNNs still face major obstacles on the \nroad to achieving ideal machine intelligence. One is that CNNs are complex and \nhard to interpret. Another is that standard CNNs require large amounts of \nannotated data, which is sometimes very hard to obtain, and it is desirable to \nbe able to learn them from few examples. In this work, we address these \nlimitations of CNNs by developing novel, simple, and interpretable models for \nfew-shot learning. Our models are based on the idea of encoding objects in \nterms of visual concepts, which are interpretable visual cues represented by \nthe feature vectors within CNNs. We first adapt the learning of visual concepts \nto the few-shot setting and then uncover two key properties of feature encoding \nusing visual concepts, which we call category sensitivity and spatial pattern. \nMotivated by these properties, we present two intuitive models for the problem \nof few-shot learning. Experiments show that our models achieve competitive \nperformances, while being much more flexible and interpretable than alternative \nstate-of-the-art few-shot learning methods. We conclude that using visual \nconcepts helps expose the natural capability of CNNs for few-shot learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516169304792", "annotations": [], "published": 1516169305, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d0be31a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08277"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Chi Zhang, Kai Qiao, Linyuan Wang, Li Tong, Ying Zeng, Bin Yan", "title": "Constraint-free Natural Image Reconstruction from fMRI Signals Based on Convolutional Neural Network. (arXiv:1801.05151v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.05151", "type": "text/html"}], "timestampUsec": "1516165575381208", "comments": [], "summary": {"content": "<p>In recent years, research on decoding brain activity based on functional \nmagnetic resonance imaging (fMRI) has made remarkable achievements. However, \nconstraint-free natural image reconstruction from brain activity is still a \nchallenge. The existing methods simplified the problem by using semantic prior \ninformation or just reconstructing simple images such as letters and digitals. \nWithout semantic prior information, we present a novel method to reconstruct \nnature images from fMRI signals of human visual cortex based on the computation \nmodel of convolutional neural network (CNN). Firstly, we extracted the units \noutput of viewed natural images in each layer of a pre-trained CNN as CNN \nfeatures. Secondly, we transformed image reconstruction from fMRI signals into \nthe problem of CNN feature visualizations by training a sparse linear \nregression to map from the fMRI patterns to CNN features. By iteratively \noptimization to find the matched image, whose CNN unit features become most \nsimilar to those predicted from the brain activity, we finally achieved the \npromising results for the challenging constraint-free natural image \nreconstruction. As there was no use of semantic prior information of the \nstimuli when training decoding model, any category of images (not constraint by \nthe training set) could be reconstructed theoretically. We found that the \nreconstructed images resembled the natural stimuli, especially in position and \nshape. The experimental results suggest that hierarchical visual features can \neffectively express the visual perception process of human brain. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e056", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05151"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Paolo Cremonesi, Chiara Francalanci, Alessandro Poli, Roberto Pagano, Luca Mazzoni, Alberto Maggioni, Mehdi Elahi", "title": "Social Network based Short-Term Stock Trading System. (arXiv:1801.05295v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.05295", "type": "text/html"}], "timestampUsec": "1516165575381207", "comments": [], "summary": {"content": "<p>This paper proposes a novel adaptive algorithm for the automated short-term \ntrading of financial instrument. The algorithm adopts a semantic sentiment \nanalysis technique to inspect the Twitter posts and to use them to predict the \nbehaviour of the stock market. Indeed, the algorithm is specifically developed \nto take advantage of both the sentiment and the past values of a certain \nfinancial instrument in order to choose the best investment decision. This \nallows the algorithm to ensure the maximization of the obtainable profits by \ntrading on the stock market. We have conducted an investment simulation and \ncompared the performance of our proposed with a well-known benchmark (DJTATO \nindex) and the optimal results, in which an investor knows in advance the \nfuture price of a product. The result shows that our approach outperforms the \nbenchmark and achieves the performance score close to the optimal result. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e05d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05295"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Hoang Thanh Lam, Tran Ngoc Minh, Mathieu Sinn, Beat Buesser, Martin Wistuba", "title": "Learning Features For Relational Data. (arXiv:1801.05372v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.05372", "type": "text/html"}], "timestampUsec": "1516165575381206", "comments": [], "summary": {"content": "<p>Feature engineering is one of the most important but tedious tasks in data \nscience projects. This work studies automation of feature learning for \nrelational data. We first theoretically proved that learning relevant features \nfrom relational data for a given predictive analytics problem is NP-hard. \nHowever, it is possible to empirically show that an efficient rule based \napproach predefining transformations as a priori based on heuristics can \nextract very useful features from relational data. Indeed, the proposed \napproach outperformed the state of the art solutions with a significant margin. \nWe further introduce a deep neural network which automatically learns \nappropriate transformations of relational data into a representation that \npredicts the target variable well instead of being predefined as a priori by \nusers. In an extensive experiment with Kaggle competitions, the proposed \nmethods could win late medals. To the best of our knowledge, this is the first \ntime an automation system could win medals in Kaggle competitions with complex \nrelational data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e060", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05372"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Wei-Han Lee, Jorge Ortiz, Bongjun Ko, Ruby Lee", "title": "Time Series Segmentation through Automatic Feature Learning. (arXiv:1801.05394v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.05394", "type": "text/html"}], "timestampUsec": "1516165575381205", "comments": [], "summary": {"content": "<p>Internet of things (IoT) applications have become increasingly popular in \nrecent years, with applications ranging from building energy monitoring to \npersonal health tracking and activity recognition. In order to leverage these \ndata, automatic knowledge extraction - whereby we map from observations to \ninterpretable states and transitions - must be done at scale. As such, we have \nseen many recent IoT data sets include annotations with a human expert \nspecifying states, recorded as a set of boundaries and associated labels in a \ndata sequence. These data can be used to build automatic labeling algorithms \nthat produce labels as an expert would. Here, we refer to human-specified \nboundaries as breakpoints. Traditional changepoint detection methods only look \nfor statistically-detectable boundaries that are defined as abrupt variations \nin the generative parameters of a data sequence. However, we observe that \nbreakpoints occur on more subtle boundaries that are non-trivial to detect with \nthese statistical methods. In this work, we propose a new unsupervised \napproach, based on deep learning, that outperforms existing techniques and \nlearns the more subtle, breakpoint boundaries with a high accuracy. Through \nextensive experiments on various real-world data sets - including \nhuman-activity sensing data, speech signals, and electroencephalogram (EEG) \nactivity traces - we demonstrate the effectiveness of our algorithm for \npractical applications. Furthermore, we show that our approach achieves \nsignificantly better performance than previous methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e065", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05394"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Hao Wang, Berk Ustun, Flavio P. Calmon", "title": "On the Direction of Discrimination: An Information-Theoretic Analysis of Disparate Impact in Machine Learning. (arXiv:1801.05398v1 [cs.IT])", "alternate": [{"href": "http://arxiv.org/abs/1801.05398", "type": "text/html"}], "timestampUsec": "1516165575381204", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32912700c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32912700c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In the context of machine learning, disparate impact refers to a form of \nsystematic discrimination whereby the output distribution of a model depends on \nthe value of a sensitive attribute (e.g., race or gender). In this paper, we \npresent an information-theoretic framework to analyze the disparate impact of a \nbinary classification model. We view the model as a fixed channel, and quantify \ndisparate impact as the divergence in output distributions over two groups. We \nthen aim to find a \\textit{correction function} that can be used to perturb the \ninput distributions of each group in order to align their output distributions. \nWe present an optimization problem that can be solved to obtain a correction \nfunction that will make the output distributions statistically \nindistinguishable. We derive closed-form expression for the correction function \nthat can be used to compute it efficiently. We illustrate the use of the \ncorrection function for a recidivism prediction application derived from the \nProPublica COMPAS dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e067", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05398"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jacob W. Crandall", "title": "Robust Learning for Repeated Stochastic Games via Meta-Gaming. (arXiv:1409.8498v2 [cs.GT] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1409.8498", "type": "text/html"}], "timestampUsec": "1516165575381203", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329187f5f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329187f5f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In repeated stochastic games (RSGs), an agent must quickly adapt to the \nbehavior of previously unknown associates, who may themselves be learning. This \nmachine-learning problem is particularly challenging due, in part, to the \npresence of multiple (even infinite) equilibria and inherently large strategy \nspaces. In this paper, we introduce a method to reduce the strategy space of \ntwo-player general-sum RSGs to a handful of expert strategies. This process, \ncalled Mega, effectually reduces an RSG to a bandit problem. We show that the \nresulting strategy space preserves several important properties of the original \nRSG, thus enabling a learner to produce robust strategies within a reasonably \nsmall number of interactions. To better establish strengths and weaknesses of \nthis approach, we empirically evaluate the resulting learning system against \nother algorithms in three different RSGs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e06b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1409.8498"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jacob W. Crandall, Mayada Oudah, Tennom, Fatimah Ishowo-Oloko, Sherief Abdallah, Jean-Fran&#xe7;ois Bonnefon, Manuel Cebrian, Azim Shariff, Michael A. Goodrich, Iyad Rahwan", "title": "Cooperating with Machines. (arXiv:1703.06207v4 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.06207", "type": "text/html"}], "timestampUsec": "1516165575381202", "comments": [], "summary": {"content": "<p>Since Alan Turing envisioned Artificial Intelligence (AI) [1], a major \ndriving force behind technical progress has been competition with human \ncognition. Historical milestones have been frequently associated with computers \nmatching or outperforming humans in difficult cognitive tasks (e.g. face \nrecognition [2], personality classification [3], driving cars [4], or playing \nvideo games [5]), or defeating humans in strategic zero-sum encounters (e.g. \nChess [6], Checkers [7], Jeopardy! [8], Poker [9], or Go [10]). In contrast, \nless attention has been given to developing autonomous machines that establish \nmutually cooperative relationships with people who may not share the machine's \npreferences. A main challenge has been that human cooperation does not require \nsheer computational power, but rather relies on intuition [11], cultural norms \n[12], emotions and signals [13, 14, 15, 16], and pre-evolved dispositions \ntoward cooperation [17], common-sense mechanisms that are difficult to encode \nin machines for arbitrary contexts. Here, we combine a state-of-the-art \nmachine-learning algorithm with novel mechanisms for generating and acting on \nsignals to produce a new learning algorithm that cooperates with people and \nother machines at levels that rival human cooperation in a variety of \ntwo-player repeated stochastic games. This is the first general-purpose \nalgorithm that is capable, given a description of a previously unseen game \nenvironment, of learning to cooperate with people within short timescales in \nscenarios previously unanticipated by algorithm designers. This is achieved \nwithout complex opponent modeling or higher-order theories of mind, thus \nshowing that flexible, fast, and general human-machine cooperation is \ncomputationally achievable using a non-trivial, but ultimately simple, set of \nalgorithmic mechanisms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e06e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.06207"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michele Colledanchise, Petter &#xd6;gren", "title": "Behavior Trees in Robotics and AI: An Introduction. (arXiv:1709.00084v3 [cs.RO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.00084", "type": "text/html"}], "timestampUsec": "1516165575381201", "comments": [], "summary": {"content": "<p>A Behavior Tree (BT) is a way to structure the switching between different \ntasks in an autonomous agent, such as a robot or a virtual entity in a computer \ngame. BTs are a very efficient way of creating complex systems that are both \nmodular and reactive. These properties are crucial in many applications, which \nhas led to the spread of BT from computer game programming to many branches of \nAI and Robotics. In this book, we will first give an introduction to BTs, then \nwe describe how BTs relate to, and in many cases generalize, earlier switching \nstructures. These ideas are then used as a foundation for a set of efficient \nand easy to use design principles. Properties such as safety, robustness, and \nefficiency are important for an autonomous system, and we describe a set of \ntools for formally analyzing these using a state space description of BTs. With \nthe new analysis tools, we can formalize the descriptions of how BTs generalize \nearlier approaches. We also show the use of BTs in automated planning and \nmachine learning. Finally, we describe an extended set of tools to capture the \nbehavior of Stochastic BTs, where the outcomes of actions are described by \nprobabilities. These tools enable the computation of both success probabilities \nand time to completion. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e076", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.00084"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jose Bento, Surjyendu Ray", "title": "On the Complexity of the Weighted Fussed Lasso. (arXiv:1801.04987v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.04987", "type": "text/html"}], "timestampUsec": "1516165575381198", "comments": [], "summary": {"content": "<p>The solution path of the 1D fused lasso for an $n$-dimensional input is \npiecewise linear with $\\mathcal{O}(n)$ segments (Hoefling et al. 2010 and \nTibshirani et al 2011). However, existing proofs of this bound do not hold for \nthe weighted fused lasso. At the same time, results for the generalized lasso, \nof which the weighted fused lasso is a special case, allow $\\Omega(3^n)$ \nsegments (Mairal et al. 2012). In this paper, we prove that the number of \nsegments in the solution path of the weighted fused lasso is \n$\\mathcal{O}(n^3)$, and for some instances $\\Omega(n^2)$. We also give a new, \nvery simple, proof of the $\\mathcal{O}(n)$ bound for the fused lasso. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e07a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04987"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Qi Liu, Anindya Bhadra, William S. Cleveland", "title": "Divide and Recombine for Large and Complex Data: Model Likelihood Functions using MCMC. (arXiv:1801.05007v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1801.05007", "type": "text/html"}], "timestampUsec": "1516165575381197", "comments": [], "summary": {"content": "<p>In Divide &amp; Recombine (D&amp;R), big data are divided into subsets, each analytic \nmethod is applied to subsets, and the outputs are recombined. This enables deep \nanalysis and practical computational performance. An innovate D\\&amp;R procedure is \nproposed to compute likelihood functions of data-model (DM) parameters for big \ndata. The likelihood-model (LM) is a parametric probability density function of \nthe DM parameters. The density parameters are estimated by fitting the density \nto MCMC draws from each subset DM likelihood function, and then the fitted \ndensities are recombined. The procedure is illustrated using normal and \nskew-normal LMs for the logistic regression DM. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e07e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05007"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Maryam Fazel, Rong Ge, Sham M. Kakade, Mehran Mesbahi", "title": "Global Convergence of Policy Gradient Methods for Linearized Control Problems. (arXiv:1801.05039v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.05039", "type": "text/html"}], "timestampUsec": "1516165575381196", "comments": [], "summary": {"content": "<p>Direct policy gradient methods for reinforcement learning and continuous \ncontrol problems are a popular approach for a variety of reasons: 1) they are \neasy to implement without explicit knowledge of the underlying model 2) they \nare an \"end-to-end\" approach, directly optimizing the performance metric of \ninterest 3) they inherently allow for richly parameterized policies. A notable \ndrawback is that even in the most basic continuous control problem (that of \nlinear quadratic regulators), these methods must solve a non-convex \noptimization problem, where little is understood about their efficiency from \nboth computational and statistical perspectives. In contrast, system \nidentification and model based planning in optimal control theory have a much \nmore solid theoretical footing, where much is known with regards to their \ncomputational and statistical properties. This work bridges this gap showing \nthat (model free) policy gradient methods globally converge to the optimal \nsolution and are efficient (polynomially so in relevant problem dependent \nquantities) with regards to their sample and computational complexities. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e081", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05039"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Edward Raff, Charles Nicholas", "title": "Toward Metric Indexes for Incremental Insertion and Querying. (arXiv:1801.05055v1 [cs.DS])", "alternate": [{"href": "http://arxiv.org/abs/1801.05055", "type": "text/html"}], "timestampUsec": "1516165575381195", "comments": [], "summary": {"content": "<p>In this work we explore the use of metric index structures, which accelerate \nnearest neighbor queries, in the scenario where we need to interleave \ninsertions and queries during deployment. This use-case is inspired by a \nreal-life need in malware analysis triage, and is surprisingly understudied. \nExisting literature tends to either focus on only final query efficiency, often \ndoes not support incremental insertion, or does not support arbitrary distance \nmetrics. We modify and improve three algorithms to support our scenario of \nincremental insertion and querying with arbitrary metrics, and evaluate them on \nmultiple datasets and distance metrics while varying the value of $k$ for the \ndesired number of nearest neighbors. In doing so we determine that our improved \nVantage-Point tree of Minimum-Variance performs best for this scenario. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e087", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05055"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kazi Nazmul Haque, Mohammad Abu Yousuf, Rajib Rana", "title": "Image denoising and restoration with CNN-LSTM Encoder Decoder with Direct Attention. (arXiv:1801.05141v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.05141", "type": "text/html"}], "timestampUsec": "1516165575381194", "comments": [], "summary": {"content": "<p>Image denoising is always a challenging task in the field of computer vision \nand image processing. In this paper, we have proposed an encoder-decoder model \nwith direct attention, which is capable of denoising and reconstruct highly \ncorrupted images. Our model consists of an encoder and a decoder, where the \nencoder is a convolutional neural network and decoder is a multilayer Long \nShort-Term memory network. In the proposed model, the encoder reads an image \nand catches the abstraction of that image in a vector, where decoder takes that \nvector as well as the corrupted image to reconstruct a clean image. We have \ntrained our model on MNIST handwritten digit database after making lower half \nof every image as black as well as adding noise top of that. After a massive \ndestruction of the images where it is hard for a human to understand the \ncontent of those images, our model can retrieve that image with minimal error. \nOur proposed model has been compared with convolutional encoder-decoder, where \nour model has performed better at generating missing part of the images than \nconvolutional autoencoder. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e08f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05141"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Thomas M&#xf6;llenhoff, Zhenzhang Ye, Tao Wu, Daniel Cremers", "title": "Combinatorial Preconditioners for Proximal Algorithms on Graphs. (arXiv:1801.05413v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1801.05413", "type": "text/html"}], "timestampUsec": "1516165575381193", "comments": [], "summary": {"content": "<p>We present a novel preconditioning technique for proximal optimization \nmethods that relies on graph algorithms to construct effective preconditioners. \nSuch combinatorial preconditioners arise from partitioning the graph into \nforests. We prove that certain decompositions lead to a theoretically optimal \ncondition number. We also show how ideal decompositions can be realized using \nmatroid partitioning and propose efficient greedy variants thereof for \nlarge-scale problems. Coupled with specialized solvers for the resulting scaled \nproximal subproblems, the preconditioned algorithm achieves competitive \nperformance in machine learning and vision applications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e09b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.05413"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jean Daunizeau", "title": "The variational Laplace approach to approximate Bayesian inference. (arXiv:1703.02089v2 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.02089", "type": "text/html"}], "timestampUsec": "1516165575381191", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329188260\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329188260&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Variational approaches to approximate Bayesian inference provide very \nefficient means of performing parameter estimation and model selection. Among \nthese, so-called variational-Laplace or VL schemes rely on Gaussian \napproximations to posterior densities on model parameters. In this note, we \nreview the main variants of VL approaches, that follow from considering \nnonlinear models of continuous and/or categorical data. En passant, we also \nderive a few novel theoretical results that complete the portfolio of existing \nanalyses of variational Bayesian approaches, including investigations of their \nasymptotic convergence. We also suggest practical ways of extending existing VL \napproaches to hierarchical generative models that include (e.g., precision) \nhyperparameters. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e0a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.02089"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mario Lucic, Matthew Faulkner, Andreas Krause, Dan Feldman", "title": "Training Gaussian Mixture Models at Scale via Coresets. (arXiv:1703.08110v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.08110", "type": "text/html"}], "timestampUsec": "1516165575381190", "comments": [], "summary": {"content": "<p>How can we train a statistical mixture model on a massive data set? In this \nwork we show how to construct coresets for mixtures of Gaussians. A coreset is \na weighted subset of the data, which guarantees that models fitting the coreset \nalso provide a good fit for the original data set. We show that, perhaps \nsurprisingly, Gaussian mixtures admit coresets of size polynomial in dimension \nand the number of mixture components, while being independent of the data set \nsize. Hence, one can harness computationally intensive algorithms to compute a \ngood approximation on a significantly smaller data set. More importantly, such \ncoresets can be efficiently constructed both in distributed and streaming \nsettings and do not impose restrictions on the data generating process. Our \nresults rely on a novel reduction of statistical estimation to problems in \ncomputational geometry and new combinatorial complexity results for mixtures of \nGaussians. Empirical evaluation on several real-world datasets suggests that \nour coreset-based approach enables significant reduction in training-time with \nnegligible approximation error. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e0ae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.08110"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Luciana Ferrer", "title": "Joint Probabilistic Linear Discriminant Analysis. (arXiv:1704.02346v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.02346", "type": "text/html"}], "timestampUsec": "1516165575381189", "comments": [], "summary": {"content": "<p>Standard probabilistic linear discriminant analysis (PLDA) for speaker \nrecognition assumes that the sample's features (usually, i-vectors) are given \nby a sum of three terms: a term that depends on the speaker identity, a term \nthat models the within-speaker variability and is assumed independent across \nsamples, and a final term that models any remaining variability and is also \nindependent across samples. In this work, we propose a generalization of this \nmodel where the within-speaker variability is not necessarily assumed \nindependent across samples but dependent on another discrete variable. This \nvariable, which we call the channel variable as in the standard PLDA approach, \ncould be, for example, a discrete category for the channel characteristics, the \nlanguage spoken by the speaker, the type of speech in the sample \n(conversational, monologue, read), etc. The value of this variable is assumed \nto be known during training but not during testing. Scoring is performed, as in \nstandard PLDA, by computing a likelihood ratio between the null hypothesis that \nthe two sides of a trial belong to the same speaker versus the alternative \nhypothesis that the two sides belong to different speakers. The two likelihoods \nare computed by marginalizing over two hypothesis about the channels in both \nsides of a trial: that they are the same and that they are different. This way, \nwe expect that the new model will be better at coping with same-channel versus \ndifferent-channel trials than standard PLDA, since knowledge about the channel \n(or language, or speech style) is used during training and implicitly \nconsidered during scoring. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e0b4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.02346"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Paola Cerchiello, Giancarlo Nicola, Samuel Ronnqvist, Peter Sarlin", "title": "Deep learning bank distress from news and numerical financial data. (arXiv:1706.09627v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.09627", "type": "text/html"}], "timestampUsec": "1516165575381188", "comments": [], "summary": {"content": "<p>In this paper we focus our attention on the exploitation of the information \ncontained in financial news to enhance the performance of a classifier of bank \ndistress. Such information should be analyzed and inserted into the predictive \nmodel in the most efficient way and this task deals with all the issues related \nto text analysis and specifically analysis of news media. Among the different \nmodels proposed for such purpose, we investigate one of the possible deep \nlearning approaches, based on a doc2vec representation of the textual data, a \nkind of neural network able to map the sequential and symbolic text input onto \na reduced latent semantic space. Afterwards, a second supervised neural network \nis trained combining news data with standard financial figures to classify \nbanks whether in distressed or tranquil states, based on a small set of known \ndistress events. Then the final aim is not only the improvement of the \npredictive performance of the classifier but also to assess the importance of \nnews data in the classification process. Does news data really bring more \nuseful information not contained in standard financial variables? Our results \nseem to confirm such hypothesis. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e0b6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.09627"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Hansi Jiang, Haoyu Wang, Wenhao Hu, Deovrat Kakde, Arin Chaudhuri", "title": "Fast Incremental SVDD Learning Algorithm with the Gaussian Kernel. (arXiv:1709.00139v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.00139", "type": "text/html"}], "timestampUsec": "1516165575381187", "comments": [], "summary": {"content": "<p>Support vector data description (SVDD) is a machine learning technique that \nis used for single-class classification and outlier detection. The idea of SVDD \nis to find a set of support vectors that defines a boundary around data. When \ndealing with online or large data, existing batch SVDD methods have to be rerun \nin each iteration. We propose an incremental learning algorithm for SVDD that \nuses the Gaussian kernel. This algorithm builds on the observation that all \nsupport vectors on the boundary have the same distance to the center of sphere \nin a higher-dimensional feature space as mapped by the Gaussian kernel \nfunction. Each iteration only involves the existing support vectors and the new \ndata point. The algorithm is based solely on matrix manipulations; the support \nvectors and their corresponding Lagrange multiplier $\\alpha_i$'s are \nautomatically selected and determined in each iteration. It can be seen that \nthe complexity of our algorithm in each iteration is only $O(k^2)$, where $k$ \nis the number of support vectors. Our experimental results on some real data \nsets show that our incremental algorithm achieves similar F-1 scores with much \nless running time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516165575381", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035d04e0bc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.00139"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Liangzhen Lai, Naveen Suda, Vikas Chandra", "title": "Not All Ops Are Created Equal!. (arXiv:1801.04326v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.04326", "type": "text/html"}], "timestampUsec": "1516082463659103", "comments": [], "summary": {"content": "<p>Efficient and compact neural network models are essential for enabling the \ndeployment on mobile and embedded devices. In this work, we point out that \ntypical design metrics for gauging the efficiency of neural network \narchitectures -- total number of operations and parameters -- are not \nsufficient. These metrics may not accurately correlate with the actual \ndeployment metrics such as energy and memory footprint. We show that throughput \nand energy varies by up to 5X across different neural network operation types \non an off-the-shelf Arm Cortex-M7 microcontroller. Furthermore, we show that \nthe memory required for activation data also need to be considered, apart from \nthe model parameters, for network architecture exploration studies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516082463659", "annotations": [], "published": 1516082464, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c48b9a5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04326"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jiannan Zhao, Cheng Hu, Chun Zhang, Zhihua Wang, Shigang Yue", "title": "A Bio-inspired Collision Detecotr for Small Quadcopter. (arXiv:1801.04530v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.04530", "type": "text/html"}], "timestampUsec": "1516082463659102", "comments": [], "summary": {"content": "<p>Sense and avoid capability enables insects to fly versatilely and robustly in \ndynamic complex environment. Their biological principles are so practical and \nefficient that inspired we human imitating them in our flying machines. In this \npaper, we studied a novel bio-inspired collision detector and its application \non a quadcopter. The detector is inspired from LGMD neurons in the locusts, and \nmodeled into an STM32F407 MCU. Compared to other collision detecting methods \napplied on quadcopters, we focused on enhancing the collision selectivity in a \nbio-inspired way that can considerably increase the computing efficiency during \nan obstacle detecting task even in complex dynamic environment. We designed the \nquadcopter's responding operation imminent collisions and tested this \nbio-inspired system in an indoor arena. The observed results from the \nexperiments demonstrated that the LGMD collision detector is feasible to work \nas a vision module for the quadcopter's collision avoidance task. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516082463659", "annotations": [], "published": 1516082464, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c48b9b7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04530"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lars Mescheder", "title": "On the convergence properties of GAN training. (arXiv:1801.04406v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.04406", "type": "text/html"}], "timestampUsec": "1516082463659101", "comments": [], "summary": {"content": "<p>Recent work has shown local convergence of GAN training for absolutely \ncontinuous data and generator distributions. In this note we show that the \nrequirement of absolute continuity is necessary: we describe a simple yet \nprototypical counterexample showing that in the more realistic case of \ndistributions that are not absolutely continuous, unregularized GAN training is \ngenerally not convergent. Furthermore, we discuss recent regularization \nstrategies that were proposed to stabilize GAN training. Our analysis shows \nthat while GAN training with instance noise or gradient penalties converges, \nWasserstein-GANs and Wasserstein-GANs-GP with a finite number of discriminator \nupdates per generator update do in general not converge to the equilibrium \npoint. We explain these results and show that both instance noise and gradient \npenalties constitute solutions to the problem of purely imaginary eigenvalues \nof the Jacobian of the gradient vector field. Based on our analysis, we also \npropose a simplified gradient penalty with the same effects on local \nconvergence as more complicated penalties. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516082463659", "annotations": [], "published": 1516082464, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c48b9c0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04406"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1516686009, "author": "Aaron Hertzmann", "title": "Can Computers Create Art?. (arXiv:1801.04486v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1801.04486", "type": "text/html"}], "timestampUsec": "1516082463659100", "comments": [], "summary": {"content": "<p>This paper discusses whether computers, using Artifical Intelligence (AI), \ncould create art. The first part concerns AI-based tools for assisting with art \nmaking. The history of technologies that automated aspects of art is covered, \nincluding photography and animation. In each case, we see initial fears and \ndenial of the technology, followed by a blossoming of new creative and \nprofessional opportunities for artists. The hype and reality of Artificial \nIntelligence (AI) tools for art making is discussed, together with predictions \nabout how AI tools will be used. The second part speculates about whether it \ncould ever happen that AI systems could conceive of artwork, and be credited \nwith authorship of an artwork. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516082463659", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c48b9ce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04486"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Matej Hoffmann, Rolf Pfeifer", "title": "Robots as Powerful Allies for the Study of Embodied Cognition from the Bottom Up. (arXiv:1801.04819v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.04819", "type": "text/html"}], "timestampUsec": "1516082463659099", "comments": [], "summary": {"content": "<p>A large body of compelling evidence has been accumulated demonstrating that \nembodiment - the agent's physical setup, including its shape, materials, \nsensors and actuators - is constitutive for any form of cognition and as a \nconsequence, models of cognition need to be embodied. In contrast to methods \nfrom empirical sciences to study cognition, robots can be freely manipulated \nand virtually all key variables of their embodiment and control programs can be \nsystematically varied. As such, they provide an extremely powerful tool of \ninvestigation. We present a robotic bottom-up or developmental approach, \nfocusing on three stages: (a) low-level behaviors like walking and reflexes, \n(b) learning regularities in sensorimotor spaces, and (c) human-like cognition. \nWe also show that robotic based research is not only a productive path to \ndeepening our understanding of cognition, but that robots can strongly benefit \nfrom human-like cognition in order to become more autonomous, robust, \nresilient, and safe. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516082463659", "annotations": [], "published": 1516082464, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c48b9da", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04819"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pararth Shah, Dilek Hakkani-T&#xfc;r, Gokhan T&#xfc;r, Abhinav Rastogi, Ankur Bapna, Neha Nayak, Larry Heck", "title": "Building a Conversational Agent Overnight with Dialogue Self-Play. (arXiv:1801.04871v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.04871", "type": "text/html"}], "timestampUsec": "1516082463659098", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329188539\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329188539&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose Machines Talking To Machines (M2M), a framework combining \nautomation and crowdsourcing to rapidly bootstrap end-to-end dialogue agents \nfor goal-oriented dialogues in arbitrary domains. M2M scales to new tasks with \njust a task schema and an API client from the dialogue system developer, but it \nis also customizable to cater to task-specific interactions. Compared to the \nWizard-of-Oz approach for data collection, M2M achieves greater diversity and \ncoverage of salient dialogue flows while maintaining the naturalness of \nindividual utterances. In the first phase, a simulated user bot and a \ndomain-agnostic system bot converse to exhaustively generate dialogue \n\"outlines\", i.e. sequences of template utterances and their semantic parses. In \nthe second phase, crowd workers provide contextual rewrites of the dialogues to \nmake the utterances more natural while preserving their meaning. The entire \nprocess can finish within a few hours. We propose a new corpus of 3,000 \ndialogues spanning 2 domains collected with M2M, and present comparisons with \npopular dialogue datasets on the quality and diversity of the surface forms and \ndialogue flows. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516082463659", "annotations": [], "published": 1516082464, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c48b9e2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04871"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Iuliia Kotseruba, John K. Tsotsos", "title": "A Review of 40 Years of Cognitive Architecture Research: Core Cognitive Abilities and Practical Applications. (arXiv:1610.08602v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.08602", "type": "text/html"}], "timestampUsec": "1516082463659097", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3291da261\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3291da261&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper we present a broad overview of the last 40 years of research on \ncognitive architectures. Although the number of existing architectures is \nnearing several hundred, most of the existing surveys do not reflect this \ngrowth and focus on a handful of well-established architectures. Thus, in this \nsurvey we wanted to shift the focus towards a more inclusive and high-level \noverview of the research on cognitive architectures. Our final set of 84 \narchitectures includes 49 that are still actively developed, and borrow from a \ndiverse set of disciplines, spanning areas from psychoanalysis to neuroscience. \nTo keep the length of this paper within reasonable limits we discuss only the \ncore cognitive abilities, such as perception, attention mechanisms, action \nselection, memory, learning and reasoning. In order to assess the breadth of \npractical applications of cognitive architectures we gathered information on \nover 900 practical projects implemented using the cognitive architectures in \nour list. We use various visualization techniques to highlight overall trends \nin the development of the field. In addition to summarizing the current \nstate-of-the-art in the cognitive architecture research, this survey describes \na variety of methods and ideas that have been tried and their relative success \nin modeling human cognitive abilities, as well as which aspects of cognitive \nbehavior need more research with respect to their mechanistic counterparts and \nthus can further inform how cognitive science might progress. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516082463659", "annotations": [], "published": 1516082464, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c48b9e7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.08602"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Brandon Amos, J. Zico Kolter", "title": "OptNet: Differentiable Optimization as a Layer in Neural Networks. (arXiv:1703.00443v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.00443", "type": "text/html"}], "timestampUsec": "1516082463659096", "comments": [], "summary": {"content": "<p>This paper presents OptNet, a network architecture that integrates \noptimization problems (here, specifically in the form of quadratic programs) as \nindividual layers in larger end-to-end trainable deep networks. These layers \nencode constraints and complex dependencies between the hidden states that \ntraditional convolutional and fully-connected layers often cannot capture. In \nthis paper, we explore the foundations for such an architecture: we show how \ntechniques from sensitivity analysis, bilevel optimization, and implicit \ndifferentiation can be used to exactly differentiate through these layers and \nwith respect to layer parameters; we develop a highly efficient solver for \nthese layers that exploits fast GPU-based batch solves within a primal-dual \ninterior point method, and which provides backpropagation gradients with \nvirtually no additional cost on top of the solve; and we highlight the \napplication of these approaches in several problems. In one notable example, we \nshow that the method is capable of learning to play mini-Sudoku (4x4) given \njust input and output games, with no a priori information about the rules of \nthe game; this highlights the ability of our architecture to learn hard \nconstraints better than other neural architectures. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516082463659", "annotations": [], "published": 1516082464, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c48b9ee", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.00443"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Peng Su, Xiao-Rong Ding, Yuan-Ting Zhang, Jing Liu, Fen Miao, Ni Zhao", "title": "Long-term Blood Pressure Prediction with Deep Recurrent Neural Networks. (arXiv:1705.04524v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.04524", "type": "text/html"}], "timestampUsec": "1516082463659095", "comments": [], "summary": {"content": "<p>Existing methods for arterial blood pressure (BP) estimation directly map the \ninput physiological signals to output BP values without explicitly modeling the \nunderlying temporal dependencies in BP dynamics. As a result, these models \nsuffer from accuracy decay over a long time and thus require frequent \ncalibration. In this work, we address this issue by formulating BP estimation \nas a sequence prediction problem in which both the input and target are \ntemporal sequences. We propose a novel deep recurrent neural network (RNN) \nconsisting of multilayered Long Short-Term Memory (LSTM) networks, which are \nincorporated with (1) a bidirectional structure to access larger-scale context \ninformation of input sequence, and (2) residual connections to allow gradients \nin deep RNN to propagate more effectively. The proposed deep RNN model was \ntested on a static BP dataset, and it achieved root mean square error (RMSE) of \n3.90 and 2.66 mmHg for systolic BP (SBP) and diastolic BP (DBP) prediction \nrespectively, surpassing the accuracy of traditional BP prediction models. On a \nmulti-day BP dataset, the deep RNN achieved RMSE of 3.84, 5.25, 5.80 and 5.81 \nmmHg for the 1st day, 2nd day, 4th day and 6th month after the 1st day SBP \nprediction, and 1.80, 4.78, 5.0, 5.21 mmHg for corresponding DBP prediction, \nrespectively, which outperforms all previous models with notable improvement. \nThe experimental results suggest that modeling the temporal dependencies in BP \ndynamics significantly improves the long-term BP prediction accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516082463659", "annotations": [], "published": 1516082464, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c48b9f7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.04524"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Vaishnavh Nagarajan, J. Zico Kolter", "title": "Gradient descent GAN optimization is locally stable. (arXiv:1706.04156v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.04156", "type": "text/html"}], "timestampUsec": "1516082463659094", "comments": [], "summary": {"content": "<p>Despite the growing prominence of generative adversarial networks (GANs), \noptimization in GANs is still a poorly understood topic. In this paper, we \nanalyze the \"gradient descent\" form of GAN optimization i.e., the natural \nsetting where we simultaneously take small gradient steps in both generator and \ndiscriminator parameters. We show that even though GAN optimization does not \ncorrespond to a convex-concave game (even for simple parameterizations), under \nproper conditions, equilibrium points of this optimization procedure are still \n\\emph{locally asymptotically stable} for the traditional GAN formulation. On \nthe other hand, we show that the recently proposed Wasserstein GAN can have \nnon-convergent limit cycles near equilibrium. Motivated by this stability \nanalysis, we propose an additional regularization term for gradient descent GAN \nupdates, which \\emph{is} able to guarantee local stability for both the WGAN \nand the traditional GAN, and also shows practical promise in speeding up \nconvergence and addressing mode collapse. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516082463659", "annotations": [], "published": 1516082464, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c48ba04", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.04156"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Prerna Agarwal, Richa Verma, Angshul Majumdar", "title": "Indian Regional Movie Dataset for Recommender Systems. (arXiv:1801.02203v1 [cs.IR] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1801.02203", "type": "text/html"}], "timestampUsec": "1516082463659091", "comments": [], "summary": {"content": "<p>Indian regional movie dataset is the first database of regional Indian \nmovies, users and their ratings. It consists of movies belonging to 18 \ndifferent Indian regional languages and metadata of users with varying \ndemographics. Through this dataset, the diversity of Indian regional cinema and \nits huge viewership is captured. We analyze the dataset that contains roughly \n10K ratings of 919 users and 2,851 movies using some supervised and \nunsupervised collaborative filtering techniques like Probabilistic Matrix \nFactorization, Matrix Completion, Blind Compressed Sensing etc. The dataset \nconsists of metadata information of users like age, occupation, home state and \nknown languages. It also consists of metadata of movies like genre, language, \nrelease year and cast. India has a wide base of viewers which is evident by the \nlarge number of movies released every year and the huge box-office revenue. \nThis dataset can be used for designing recommendation systems for Indian users \nand regional movies, which do not, yet, exist. The dataset can be downloaded \nfrom \\href{https://goo.gl/EmTPv6}{https://goo.gl/EmTPv6}. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516082463659", "annotations": [], "published": 1516082464, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c48ba0b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02203"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Bo Luo, Yannan Liu, Lingxiao Wei, Qiang Xu", "title": "Towards Imperceptible and Robust Adversarial Example Attacks against Neural Networks. (arXiv:1801.04693v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.04693", "type": "text/html"}], "timestampUsec": "1516082463659090", "comments": [], "summary": {"content": "<p>Machine learning systems based on deep neural networks, being able to produce \nstate-of-the-art results on various perception tasks, have gained mainstream \nadoption in many applications. However, they are shown to be vulnerable to \nadversarial example attack, which generates malicious output by adding slight \nperturbations to the input. Previous adversarial example crafting methods, \nhowever, use simple metrics to evaluate the distances between the original \nexamples and the adversarial ones, which could be easily detected by human \neyes. In addition, these attacks are often not robust due to the inevitable \nnoises and deviation in the physical world. In this work, we present a new \nadversarial example attack crafting method, which takes the human perceptual \nsystem into consideration and maximizes the noise tolerance of the crafted \nadversarial example. Experimental results demonstrate the efficacy of the \nproposed technique. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516082463659", "annotations": [], "published": 1516082464, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c48ba26", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04693"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhinus Marzi, Soorya Gopalakrishnan, Upamanyu Madhow, Ramtin Pedarsani", "title": "Sparsity-based Defense against Adversarial Attacks on Linear Classifiers. (arXiv:1801.04695v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.04695", "type": "text/html"}], "timestampUsec": "1516082463659089", "comments": [], "summary": {"content": "<p>Deep neural networks represent the state of the art in machine learning in a \ngrowing number of fields, including vision, speech and natural language \nprocessing. However, recent work raises important questions about the \nrobustness of such architectures, by showing that it is possible to induce \nclassification errors through tiny, almost imperceptible, perturbations. \nVulnerability to such \"adversarial attacks\", or \"adversarial examples\", has \nbeen conjectured to be due to the excessive linearity of deep networks. In this \npaper, we study this phenomenon in the setting of a linear classifier, and show \nthat it is possible to exploit sparsity in natural data to combat \n$\\ell_{\\infty}$-bounded adversarial perturbations. Specifically, we demonstrate \nthe efficacy of a sparsifying front end via an ensemble averaged analysis, and \nexperimental results for the MNIST handwritten digit database. To the best of \nour knowledge, this is the first work to provide a theoretically rigorous \nframework for defense against adversarial attacks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516082463659", "annotations": [], "published": 1516082464, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c48ba35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04695"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Benjamin Doerr", "title": "Better Runtime Guarantees Via Stochastic Domination. (arXiv:1801.04487v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.04487", "type": "text/html"}], "timestampUsec": "1516080167763011", "comments": [], "summary": {"content": "<p>Apart from few exceptions, the mathematical runtime analysis of evolutionary \nalgorithms is mostly concerned with expected runtimes. In this work, we argue \nthat stochastic domination is a notion that should be used more frequently in \nthis area. Stochastic domination allows to formulate much more informative \nperformance guarantees than the expectation alone, it allows to decouple the \nalgorithm analysis into the true algorithmic part of detecting a domination \nstatement and probability theoretic part of deriving the desired probabilistic \nguarantees from this statement, and it allows simpler and more natural proofs. \n</p> \n<p>As particular results, we prove a fitness level theorem which shows that the \nruntime is dominated by a sum of independent geometric random variables, we \nprove tail bounds for several classic problems, and we give a short and natural \nproof for Witt's result that the runtime of any $(\\mu,p)$ mutation-based \nalgorithm on any function with unique optimum is subdominated by the runtime of \na variant of the (1+1) evolutionary algorithm on the OneMax function. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c444812", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04487"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kai Zoschke, Maurice G&#xfc;ttler, Lars B&#xf6;ttcher, Andreas Gr&#xfc;bl, Dan Husmann, Johannes Schemmel, Karlheinz Meier, Oswin Ehrmann", "title": "Full Wafer Redistribution and Wafer Embedding as Key Technologies for a Multi-Scale Neuromorphic Hardware Cluster. (arXiv:1801.04734v1 [cs.ET])", "alternate": [{"href": "http://arxiv.org/abs/1801.04734", "type": "text/html"}], "timestampUsec": "1516080167763010", "comments": [], "summary": {"content": "<p>Together with the Kirchhoff-Institute for Physics(KIP) the Fraunhofer IZM has \ndeveloped a full wafer redistribution and embedding technology as base for a \nlarge-scale neuromorphic hardware system. The paper will give an overview of \nthe neuromorphic computing platform at the KIP and the associated hardware \nrequirements which drove the described technological developments. In the first \nphase of the project standard redistribution technologies from wafer level \npackaging were adapted to enable a high density reticle-to-reticle routing on \n200mm CMOS wafers. Neighboring reticles were interconnected across the scribe \nlines with an 8{\\mu}m pitch routing based on semi-additive copper \nmetallization. Passivation by photo sensitive benzocyclobutene was used to \nenable a second intra-reticle routing layer. Final IO pads with flash gold were \ngenerated on top of each reticle. With that concept neuromorphic systems based \non full wafers could be assembled and tested. The fabricated high density \ninter-reticle routing revealed a very high yield of larger than 99.9%. In order \nto allow an upscaling of the system size to a large number of wafers with \nfeasible effort a full wafer embedding concept for printed circuit boards was \ndeveloped and proven in the second phase of the project. The wafers were \nthinned to 250{\\mu}m and laminated with additional prepreg layers and copper \nfoils into a core material. After lamination of the PCB panel the reticle IOs \nof the embedded wafer were accessed by micro via drilling, copper \nelectroplating, lithography and subtractive etching of the PCB wiring \nstructure. The created wiring with 50um line width enabled an access of the \nreticle IOs on the embedded wafer as well as a board level routing. The panels \nwith the embedded wafers were subsequently stressed with up to 1000 thermal \ncycles between 0C and 100C and have shown no severe failure formation over the \ncycle time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c44481c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04734"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Juan Biondi, Gerardo Fernandez, Silvia Castro, Osvaldo Agamennoni", "title": "Eye-Movement behavior identification for AD diagnosis. (arXiv:1702.00837v3 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1702.00837", "type": "text/html"}], "timestampUsec": "1516080167763009", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3291da5cf\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3291da5cf&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In the present work, we develop a deep-learning approach for differentiating \nthe eye-movement behavior of people with neurodegenerative diseases over \nhealthy control subjects during reading well-defined sentences. We define an \ninformation compaction of the eye-tracking data of subjects without and with \nprobable Alzheimer's disease when reading a set of well-defined, previously \nvalidated, sentences including high-, low-predictable sentences, and proverbs. \nUsing this information we train a set of denoising sparse-autoencoders and \nbuild a deep neural network with these and a softmax classifier. Our results \nare very promising and show that these models may help to understand the \ndynamics of eye movement behavior and its relationship with underlying \nneuropsychological correlates. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c444821", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1702.00837"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Saifuddin Hitawala", "title": "Comparative Study on Generative Adversarial Networks. (arXiv:1801.04271v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.04271", "type": "text/html"}], "timestampUsec": "1516080167763007", "comments": [], "summary": {"content": "<p>In recent years, there have been tremendous advancements in the field of \nmachine learning. These advancements have been made through both academic as \nwell as industrial research. Lately, a fair amount of research has been \ndedicated to the usage of generative models in the field of computer vision and \nimage classification. These generative models have been popularized through a \nnew framework called Generative Adversarial Networks. Moreover, many modified \nversions of this framework have been proposed in the last two years. We study \nthe original model proposed by Goodfellow et al. as well as modifications over \nthe original model and provide a comparative analysis of these models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c444824", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04271"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sajan Patel, Brent Griffin, Kristofer Kusano, Jason J. Corso", "title": "Predicting Future Lane Changes of Other Highway Vehicles using RNN-based Deep Models. (arXiv:1801.04340v1 [cs.RO])", "alternate": [{"href": "http://arxiv.org/abs/1801.04340", "type": "text/html"}], "timestampUsec": "1516080167763006", "comments": [], "summary": {"content": "<p>In the event of sensor failure, it is necessary for autonomous vehicles to \nsafely execute emergency maneuvers while avoiding other vehicles on the road. \nIn order to accomplish this, the sensor-failed vehicle must predict the future \nsemantic behaviors of other drivers, such as lane changes, as well as their \nfuture trajectories given a small window of past sensor observations. We \naddress the first issue of semantic behavior prediction in this paper, by \nintroducing a prediction framework that leverages the power of recurrent neural \nnetworks (RNNs) and graphical models. Our prediction goal is to predict the \nfuture categorical driving intent, for lane changes, of neighboring vehicles up \nto three seconds into the future given as little as a one-second window of past \nLIDAR, GPS, inertial, and map data. \n</p> \n<p>We collect real-world data containing over 500,000 samples of highway driving \nusing an autonomous Toyota vehicle. We propose a pair of models that leverage \nRNNs: first, a monolithic RNN model that tries to directly map inputs to future \nbehavior through a long-short-term-memory network. Second, we propose a \ncomposite RNN model by adopting the methodology of Structural Recurrent Neural \nNetworks (RNNs) to learn factor functions and take advantage of both the \nhigh-level structure of graphical models and the sequence modeling power of \nRNNs, which we expect to afford more transparent modeling and activity than the \nmonolithic RNN. To demonstrate our approach, we validate our models using \nauthentic interstate highway driving to predict the future lane change \nmaneuvers of other vehicles neighboring our autonomous vehicle. We find that \nboth RNN models outperform baselines, and they outperform each other in certain \nconditions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c444828", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04340"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Forough Arabshahi, Sameer Singh, Animashree Anandkumar", "title": "Combining Symbolic and Function Evaluation Expressions In Neural Programs. (arXiv:1801.04342v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.04342", "type": "text/html"}], "timestampUsec": "1516080167763005", "comments": [], "summary": {"content": "<p>Neural programming involves training neural networks to learn programs from \ndata. Previous works have failed to achieve good generalization performance, \nespecially on programs with high complexity or on large domains. This is \nbecause they mostly rely either on black-box function evaluations that do not \ncapture the structure of the program, or on detailed execution traces that are \nexpensive to obtain, and hence the training data has poor coverage of the \ndomain under consideration. We present a novel framework that utilizes \nblack-box function evaluations, in conjunction with symbolic expressions that \nintegrate relationships between the given functions. We employ tree LSTMs to \nincorporate the structure of the symbolic expression trees. We use tree \nencoding for numbers present in function evaluation data, based on their \ndecimal representation. We present an evaluation benchmark for this task to \ndemonstrate our proposed model combines symbolic reasoning and function \nevaluation in a fruitful manner, obtaining high accuracies in our experiments. \nOur framework generalizes significantly better to expressions of higher depth \nand is able to fill partial equations with valid completions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c44482f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04342"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Nathalia Moraes do Nascimento, Carlos Jose Pereira de Lucena", "title": "Engineering Cooperative Smart Things based on Embodied Cognition. (arXiv:1801.04345v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.04345", "type": "text/html"}], "timestampUsec": "1516080167763004", "comments": [], "summary": {"content": "<p>The goal of the Internet of Things (IoT) is to transform any thing around us, \nsuch as a trash can or a street light, into a smart thing. A smart thing has \nthe ability of sensing, processing, communicating and/or actuating. In order to \nachieve the goal of a smart IoT application, such as minimizing waste \ntransportation costs or reducing energy consumption, the smart things in the \napplication scenario must cooperate with each other without a centralized \ncontrol. Inspired by known approaches to design swarm of cooperative and \nautonomous robots, we modeled our smart things based on the embodied cognition \nconcept. Each smart thing is a physical agent with a body composed of a \nmicrocontroller, sensors and actuators, and a brain that is represented by an \nartificial neural network. This type of agent is commonly called an embodied \nagent. The behavior of these embodied agents is autonomously configured through \nan evolutionary algorithm that is triggered according to the application \nperformance. To illustrate, we have designed three homogeneous prototypes for \nsmart street lights based on an evolved network. This application has shown \nthat the proposed approach results in a feasible way of modeling decentralized \nsmart things with self-developed and cooperative capabilities. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c444835", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04345"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Richard Kim, Max Kleiman-Weiner, Andres Abeliuk, Edmond Awad, Sohan Dsouza, Josh Tenenbaum, Iyad Rahwan", "title": "A Computational Model of Commonsense Moral Decision Making. (arXiv:1801.04346v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.04346", "type": "text/html"}], "timestampUsec": "1516080167763003", "comments": [], "summary": {"content": "<p>We introduce a new computational model of moral decision making, drawing on a \nrecent theory of commonsense moral learning via social dynamics. Our model \ndescribes moral dilemmas as a utility function that computes trade-offs in \nvalues over abstract moral dimensions, which provide interpretable parameter \nvalues when implemented in machine-led ethical decision-making. Moreover, \ncharacterizing the social structures of individuals and groups as a \nhierarchical Bayesian model, we show that a useful description of an \nindividual's moral values - as well as a group's shared values - can be \ninferred from a limited amount of observed data. Finally, we apply and evaluate \nour approach to data from the Moral Machine, a web application that collects \nhuman judgments on moral dilemmas involving autonomous vehicles. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c444838", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04346"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "AmirEmad Ghassami, Sajad Khodadadian, Negar Kiyavash", "title": "Fairness in Supervised Learning: An Information Theoretic Approach. (arXiv:1801.04378v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.04378", "type": "text/html"}], "timestampUsec": "1516080167763002", "comments": [], "summary": {"content": "<p>Automated decision making systems are increasingly being used in real-world \napplications. In these systems for the most part, the decision rules are \nderived by minimizing the training error on the available historical data. \nTherefore, if there is a bias related to a sensitive attribute such as gender, \nrace, religion, etc. in the data, say, due to cultural/historical \ndiscriminatory practices against a certain demographic, the system could \ncontinue discrimination in decisions by including the said bias in its decision \nrule. We present an information theoretic framework for designing fair \npredictors from data, which aim to prevent discrimination against a specified \nsensitive attribute in a supervised learning setting. We use equalized odds as \nthe criterion for discrimination, which demands that the prediction should be \nindependent of the protected attribute conditioned on the actual label. To \nensure fairness and generalization simultaneously, we compress the data to an \nauxiliary variable, which is used for the prediction task. This auxiliary \nvariable is chosen such that it is decontaminated from the discriminatory \nattribute in the sense of equalized odds. The final predictor is obtained by \napplying a Bayesian decision rule to the auxiliary variable. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c44483f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04378"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon Song, Zenglin Xu, Tim Kraska", "title": "SuperNeurons: Dynamic GPU Memory Management for Training Deep Neural Networks. (arXiv:1801.04380v1 [cs.DC])", "alternate": [{"href": "http://arxiv.org/abs/1801.04380", "type": "text/html"}], "timestampUsec": "1516080167763001", "comments": [], "summary": {"content": "<p>Going deeper and wider in neural architectures improves the accuracy, while \nthe limited GPU DRAM places an undesired restriction on the network design \ndomain. Deep Learning (DL) practitioners either need change to less desired \nnetwork architectures, or nontrivially dissect a network across multiGPUs. \nThese distract DL practitioners from concentrating on their original machine \nlearning tasks. We present SuperNeurons: a dynamic GPU memory scheduling \nruntime to enable the network training far beyond the GPU DRAM capacity. \nSuperNeurons features 3 memory optimizations, \\textit{Liveness Analysis}, \n\\textit{Unified Tensor Pool}, and \\textit{Cost-Aware Recomputation}, all \ntogether they effectively reduce the network-wide peak memory usage down to the \nmaximal memory usage among layers. We also address the performance issues in \nthose memory saving techniques. Given the limited GPU DRAM, SuperNeurons not \nonly provisions the necessary memory for the training, but also dynamically \nallocates the memory for convolution workspaces to achieve the high \nperformance. Evaluations against Caffe, Torch, MXNet and TensorFlow have \ndemonstrated that SuperNeurons trains at least 3.2432 deeper network than \ncurrent ones with the leading performance. Particularly, SuperNeurons can train \nResNet2500 that has $10^4$ basic network layers on a 12GB K40c. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c444842", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04380"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Dipan K. Pal, Marios Savvides", "title": "Non-Parametric Transformation Networks. (arXiv:1801.04520v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.04520", "type": "text/html"}], "timestampUsec": "1516080167763000", "comments": [], "summary": {"content": "<p>ConvNets have been very effective in many applications where it is required \nto learn invariances to within-class nuisance transformations. However, through \ntheir architecture, ConvNets only enforce invariance to translation. In this \npaper, we introduce a new class of convolutional architectures called \nNon-Parametric Transformation Networks (NPTNs) which can learn general \ninvariances and symmetries directly from data. NPTNs are a direct and natural \ngeneralization of ConvNets and can be optimized directly using gradient \ndescent. They make no assumption regarding structure of the invariances present \nin the data and in that aspect are very flexible and powerful. We also model \nConvNets and NPTNs under a unified framework called Transformation Networks \nwhich establishes the natural connection between the two. We demonstrate the \nefficacy of NPTNs on natural data such as MNIST and CIFAR 10 where it \noutperforms ConvNet baselines with the same number of parameters. We show it is \neffective in learning invariances unknown apriori directly from data from \nscratch. Finally, we apply NPTNs to Capsule Networks and show that they enable \nthem to perform even better. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c444843", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04520"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Colin de Vrieze, Shane Barratt, Daniel Tsai, Anant Sahai", "title": "Cooperative Multi-Agent Reinforcement Learning for Low-Level Wireless Communication. (arXiv:1801.04541v1 [eess.SP])", "alternate": [{"href": "http://arxiv.org/abs/1801.04541", "type": "text/html"}], "timestampUsec": "1516080167762999", "comments": [], "summary": {"content": "<p>Traditional radio systems are strictly co-designed on the lower levels of the \nOSI stack for compatibility and efficiency. Although this has enabled the \nsuccess of radio communications, it has also introduced lengthy standardization \nprocesses and imposed static allocation of the radio spectrum. Various \ninitiatives have been undertaken by the research community to tackle the \nproblem of artificial spectrum scarcity by both making frequency allocation \nmore dynamic and building flexible radios to replace the static ones. There is \nreason to believe that just as computer vision and control have been overhauled \nby the introduction of machine learning, wireless communication can also be \nimproved by utilizing similar techniques to increase the flexibility of \nwireless networks. In this work, we pose the problem of discovering low-level \nwireless communication schemes ex-nihilo between two agents in a fully \ndecentralized fashion as a reinforcement learning problem. Our proposed \napproach uses policy gradients to learn an optimal bi-directional communication \nscheme and shows surprisingly sophisticated and intelligent learning behavior. \nWe present the results of extensive experiments and an analysis of the fidelity \nof our approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c444847", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04541"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Konstantin B&#xf6;ttinger, Patrice Godefroid, Rishabh Singh", "title": "Deep Reinforcement Fuzzing. (arXiv:1801.04589v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.04589", "type": "text/html"}], "timestampUsec": "1516080167762998", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3291da894\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3291da894&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Fuzzing is the process of finding security vulnerabilities in \ninput-processing code by repeatedly testing the code with modified inputs. In \nthis paper, we formalize fuzzing as a reinforcement learning problem using the \nconcept of Markov decision processes. This in turn allows us to apply \nstate-of-the-art deep Q-learning algorithms that optimize rewards, which we \ndefine from runtime properties of the program under test. By observing the \nrewards caused by mutating with a specific set of actions performed on an \ninitial program input, the fuzzing agent learns a policy that can next generate \nnew higher-reward inputs. We have implemented this new approach, and \npreliminary empirical evidence shows that reinforcement fuzzing can outperform \nbaseline random fuzzing. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c444852", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04589"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mehdi S. M. Sajjadi, Raviteja Vemulapalli, Matthew Brown", "title": "Frame-Recurrent Video Super-Resolution. (arXiv:1801.04590v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.04590", "type": "text/html"}], "timestampUsec": "1516080167762997", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329250a6d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329250a6d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Recent advances in video super-resolution have shown that convolutional \nneural networks combined with motion compensation are able to merge information \nfrom multiple low-resolution (LR) frames to generate high-quality images. \nCurrent state-of-the-art methods process a batch of LR frames to generate a \nsingle high-resolution (HR) frame and run this scheme in a sliding window \nfashion over the entire video, effectively treating the problem as a large \nnumber of separate multi-frame super-resolution tasks. This approach has two \nmain weaknesses: 1) Each input frame is processed and warped multiple times, \nincreasing the computational cost, and 2) each output frame is estimated \nindependently conditioned on the input frames, limiting the system's ability to \nproduce temporally consistent results. \n</p> \n<p>In this work, we propose an end-to-end trainable frame-recurrent video \nsuper-resolution framework that uses the previously inferred HR estimate to \nsuper-resolve the subsequent frame. This naturally encourages temporally \nconsistent results and reduces the computational cost by warping only one image \nin each step. Furthermore, due to its recurrent nature, the proposed method has \nthe ability to assimilate a large number of previous frames without increased \ncomputational demands. Extensive evaluations and comparisons with previous \nmethods validate the strengths of our approach and demonstrate that the \nproposed framework is able to significantly outperform the current state of the \nart. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c444859", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04590"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zi Wang, Dali Wang, Chengcheng Li, Yichi Xu, Husheng Li, Zhirong Bao", "title": "Deep Reinforcement Learning of Cell Movement in the Early Stage of C. elegans Embryogenesis. (arXiv:1801.04600v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.04600", "type": "text/html"}], "timestampUsec": "1516080167762996", "comments": [], "summary": {"content": "<p>Cell movement in the early phase of C. elegans development is regulated by a \nhighly complex process in which a set of rules and connections are formulated \nat distinct scales. Previous efforts have demonstrated that agent-based, \nmulti-scale modeling systems can integrate physical and biological rules and \nprovide new avenues to study developmental systems. However, the application of \nthese systems to model cell movement is still challenging and requires a \ncomprehensive understanding of regulation networks at the right scales. Recent \ndevelopments in deep learning and reinforcement learning provide an \nunprecedented opportunity to explore cell movement using 3D time-lapse \nmicroscopy images. We presented a deep reinforcement learning approach within \nan agent-based modeling system to characterize cell movement in the embryonic \ndevelopment of C. elegans. We tested our model through two scenarios within \nreal developmental processes: the anterior movement of the Cpaaa cell via \nintercalation and the restoration of the superficial left-right symmetry. Our \nmodeling system overcame the local optimization problems encountered by \ntraditional rule-based, agent-based modeling by using greedy algorithms. It \nalso overcame the computational challenges in the action selection which has \nbeen plagued by the traditional tabular-based reinforcement learning approach. \nOur system can automatically explore the cell movement path by using live \nmicroscopy images and it can provide a unique capability to model cell movement \nscenarios where regulatory mechanisms are not well studied. In addition, our \nsystem can be used to explore potential paths of a cell under different \nregulatory mechanisms or to facilitate new hypotheses for explaining certain \ncell movement behaviors. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c44485f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04600"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Vatsal Mahajan", "title": "Top k Memory Candidates in Memory Networks for Common Sense Reasoning. (arXiv:1801.04622v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.04622", "type": "text/html"}], "timestampUsec": "1516080167762995", "comments": [], "summary": {"content": "<p>Successful completion of reasoning task requires the agent to have relevant \nprior knowledge or some given context of the world dynamics. Usually, the \ninformation provided to the system for a reasoning task is just the query or \nsome supporting story, which is often not enough for common reasoning tasks. \nThe goal here is that, if the information provided along the question is not \nsufficient to correctly answer the question, the model should choose k most \nrelevant documents that can aid its inference process. In this work, the model \ndynamically selects top k most relevant memory candidates that can be used to \nsuccessfully solve reasoning tasks. Experiments were conducted on a subset of \nWinograd Schema Challenge (WSC) problems to show that the proposed model has \nthe potential for commonsense reasoning. The WSC is a test of machine \nintelligence, designed to be an improvement on the Turing test. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c444865", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04622"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ao Zhang, Nan Li, Jian Pu, Jun Wang, Junchi Yan, Hongyuan Zha", "title": "tau-FPL: Tolerance-Constrained Learning in Linear Time. (arXiv:1801.04701v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.04701", "type": "text/html"}], "timestampUsec": "1516080167762994", "comments": [], "summary": {"content": "<p>Learning a classifier with control on the false-positive rate plays a \ncritical role in many machine learning applications. Existing approaches either \nintroduce prior knowledge dependent label cost or tune parameters based on \ntraditional classifiers, which lack consistency in methodology because they do \nnot strictly adhere to the false-positive rate constraint. In this paper, we \npropose a novel scoring-thresholding approach, tau-False Positive Learning \n(tau-FPL) to address this problem. We show the scoring problem which takes the \nfalse-positive rate tolerance into accounts can be efficiently solved in linear \ntime, also an out-of-bootstrap thresholding method can transform the learned \nranking function into a low false-positive classifier. Both theoretical \nanalysis and experimental results show superior performance of the proposed \ntau-FPL over existing approaches. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c44486d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04701"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Shayegan Omidshafiei, Dong-Ki Kim, Jason Pazis, Jonathan P. How", "title": "Crossmodal Attentive Skill Learner. (arXiv:1711.10314v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10314", "type": "text/html"}], "timestampUsec": "1516080167762993", "comments": [], "summary": {"content": "<p>This paper presents the Crossmodal Attentive Skill Learner (CASL), integrated \nwith the recently-introduced Asynchronous Advantage Option-Critic (A2OC) \narchitecture [Harb et al., 2017] to enable hierarchical reinforcement learning \nacross multiple sensory inputs. We provide concrete examples where the approach \nnot only improves performance in a single task, but accelerates transfer to new \ntasks. We demonstrate the attention mechanism anticipates and identifies useful \nlatent features, while filtering irrelevant sensor modalities during execution. \nWe modify the Arcade Learning Environment [Bellemare et al., 2013] to support \naudio queries, and conduct evaluations of crossmodal learning in the Atari 2600 \ngame Amidar. Finally, building on the recent work of Babaeizadeh et al. [2017], \nwe open-source a fast hybrid CPU-GPU implementation of CASL. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c444872", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10314"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mehdi S. M. Sajjadi, Bernhard Sch&#xf6;lkopf, Michael Hirsch", "title": "EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis. (arXiv:1612.07919v2 [cs.CV] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1612.07919", "type": "text/html"}], "timestampUsec": "1516080167762990", "comments": [], "summary": {"content": "<p>Single image super-resolution is the task of inferring a high-resolution \nimage from a single low-resolution input. Traditionally, the performance of \nalgorithms for this task is measured using pixel-wise reconstruction measures \nsuch as peak signal-to-noise ratio (PSNR) which have been shown to correlate \npoorly with the human perception of image quality. As a result, algorithms \nminimizing these metrics tend to produce over-smoothed images that lack \nhigh-frequency textures and do not look natural despite yielding high PSNR \nvalues. \n</p> \n<p>We propose a novel application of automated texture synthesis in combination \nwith a perceptual loss focusing on creating realistic textures rather than \noptimizing for a pixel-accurate reproduction of ground truth images during \ntraining. By using feed-forward fully convolutional neural networks in an \nadversarial training setting, we achieve a significant boost in image quality \nat high magnification ratios. Extensive experiments on a number of datasets \nshow the effectiveness of our approach, yielding state-of-the-art results in \nboth quantitative and qualitative benchmarks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c44487d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.07919"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Saad Mohamad, Abdelhamid Bouchachia, Moamar Sayed-Mouchaweh", "title": "Asynchronous Stochastic Variational Inference. (arXiv:1801.04289v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.04289", "type": "text/html"}], "timestampUsec": "1516080167762989", "comments": [], "summary": {"content": "<p>Stochastic variational inference (SVI) employs stochastic optimization to \nscale up Bayesian computation to massive data. Since SVI is at its core a \nstochastic gradient-based algorithm, horizontal parallelism can be harnessed to \nallow larger scale inference. We propose a lock-free parallel implementation \nfor SVI which allows distributed computations over multiple slaves in an \nasynchronous style. We show that our implementation leads to linear speed-up \nwhile guaranteeing an asymptotic ergodic convergence rate $O(1/\\sqrt(T)$ ) \ngiven that the number of slaves is bounded by $\\sqrt(T)$ ($T$ is the total \nnumber of iterations). The implementation is done in a high-performance \ncomputing (HPC) environment using message passing interface (MPI) for python \n(MPI4py). The extensive empirical evaluation shows that our parallel SVI is \nlossless, performing comparably well to its counterpart serial SVI with linear \nspeed-up. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c44488c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04289"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ankit Pensia, Varun Jog, Po-Ling Loh", "title": "Generalization Error Bounds for Noisy, Iterative Algorithms. (arXiv:1801.04295v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.04295", "type": "text/html"}], "timestampUsec": "1516080167762988", "comments": [], "summary": {"content": "<p>In statistical learning theory, generalization error is used to quantify the \ndegree to which a supervised machine learning algorithm may overfit to training \ndata. Recent work [Xu and Raginsky (2017)] has established a bound on the \ngeneralization error of empirical risk minimization based on the mutual \ninformation $I(S;W)$ between the algorithm input $S$ and the algorithm output \n$W$, when the loss function is sub-Gaussian. We leverage these results to \nderive generalization error bounds for a broad class of iterative algorithms \nthat are characterized by bounded, noisy updates with Markovian structure. Our \nbounds are very general and are applicable to numerous settings of interest, \nincluding stochastic gradient Langevin dynamics (SGLD) and variants of the \nstochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm. Furthermore, our \nerror bounds hold for any output function computed over the path of iterates, \nincluding the last iterate of the algorithm or the average of subsets of \niterates, and also allow for non-uniform sampling of data in successive updates \nof the algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c444892", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04295"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jason M. Klusowski, Yihong Wu", "title": "Estimating the Number of Connected Components in a Graph via Subgraph Sampling. (arXiv:1801.04339v1 [math.ST])", "alternate": [{"href": "http://arxiv.org/abs/1801.04339", "type": "text/html"}], "timestampUsec": "1516080167762987", "comments": [], "summary": {"content": "<p>Learning properties of large graphs from samples has been an important \nproblem in statistical network analysis since the early work of Goodman \n\\cite{Goodman1949} and Frank \\cite{Frank1978}. We revisit a problem formulated \nby Frank \\cite{Frank1978} of estimating the number of connected components in a \nlarge graph based on the subgraph sampling model, in which we randomly sample a \nsubset of the vertices and observe the induced subgraph. The key question is \nwhether accurate estimation is achievable in the \\emph{sublinear} regime where \nonly a vanishing fraction of the vertices are sampled. We show that it is \nimpossible if the parent graph is allowed to contain high-degree vertices or \nlong induced cycles. For the class of chordal graphs, where induced cycles of \nlength four or above are forbidden, we characterize the optimal sample \ncomplexity within constant factors and construct linear-time estimators that \nprovably achieve these bounds. This significantly expands the scope of previous \nresults which have focused on unbiased estimators and special classes of graphs \nsuch as forests or cliques. \n</p> \n<p>Both the construction and the analysis of the proposed methodology rely on \ncombinatorial properties of chordal graphs and identities of induced subgraph \ncounts. They, in turn, also play a key role in proving minimax lower bounds \nbased on construction of random instances of graphs with matching structures of \nsmall subgraphs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c444899", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04339"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Fazle Karim, Somshubra Majumdar, Houshang Darabi, Samuel Harford", "title": "Multivariate LSTM-FCNs for Time Series Classification. (arXiv:1801.04503v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.04503", "type": "text/html"}], "timestampUsec": "1516080167762986", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329250d64\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329250d64&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Over the past decade, multivariate time series classification has been \nreceiving a lot of attention. We propose augmenting the existing univariate \ntime series classification models, LSTM-FCN and ALSTM-FCN with a squeeze and \nexcitation block to further improve performance. Our proposed models outperform \nmost of the state of the art models while requiring minimum preprocessing. The \nproposed models work efficiently on various complex multivariate time series \nclassification tasks such as activity recognition or action recognition. \nFurthermore, the proposed models are highly efficient at test time and small \nenough to deploy on memory constrained systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c44489f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04503"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Chenglong Dai, Jia Wu, Dechang Pi, Lin Cui", "title": "Brain EEG Time Series Selection: A Novel Graph-Based Approach for Classification. (arXiv:1801.04510v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.04510", "type": "text/html"}], "timestampUsec": "1516080167762985", "comments": [], "summary": {"content": "<p>Brain Electroencephalography (EEG) classification is widely applied to \nanalyze cerebral diseases in recent years. Unfortunately, invalid/noisy EEGs \ndegrade the diagnosis performance and most previously developed methods ignore \nthe necessity of EEG selection for classification. To this end, this paper \nproposes a novel maximum weight clique-based EEG selection approach, named \nmwcEEGs, to map EEG selection to searching maximum similarity-weighted cliques \nfrom an improved Fr\\'{e}chet distance-weighted undirected EEG graph \nsimultaneously considering edge weights and vertex weights. Our mwcEEGs \nimproves the classification performance by selecting intra-clique pairwise \nsimilar and inter-clique discriminative EEGs with similarity threshold \n$\\delta$. Experimental results demonstrate the algorithm effectiveness compared \nwith the state-of-the-art time series selection algorithms on real-world EEG \ndatasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c4448a4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04510"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Elad Hoffer, Itay Hubara, Daniel Soudry", "title": "Fix your classifier: the marginal value of training the last weight layer. (arXiv:1801.04540v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.04540", "type": "text/html"}], "timestampUsec": "1516080167762984", "comments": [], "summary": {"content": "<p>Neural networks are commonly used as models for classification for a wide \nvariety of tasks. Typically, a learned affine transformation is placed at the \nend of such models, yielding a per-class value used for classification. This \nclassifier can have a vast number of parameters, which grows linearly with the \nnumber of possible classes, thus requiring increasingly more resources. \n</p> \n<p>In this work we argue that this classifier can be fixed, up to a global scale \nconstant, with little or no loss of accuracy for most tasks, allowing memory \nand computational benefits. Moreover, we show that by initializing the \nclassifier with a Hadamard matrix we can speed up inference as well. We discuss \nthe implications for current understanding of neural network models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c4448a8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04540"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Quan Hoang", "title": "Predicting Movie Genres Based on Plot Summaries. (arXiv:1801.04813v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1801.04813", "type": "text/html"}], "timestampUsec": "1516080167762983", "comments": [], "summary": {"content": "<p>This project explores several Machine Learning methods to predict movie \ngenres based on plot summaries. Naive Bayes, Word2Vec+XGBoost and Recurrent \nNeural Networks are used for text classification, while K-binary \ntransformation, rank method and probabilistic classification with learned \nprobability threshold are employed for the multi-label problem involved in the \ngenre tagging task.Experiments with more than 250,000 movies show that \nemploying the Gated Recurrent Units (GRU) neural networks for the probabilistic \nclassification with learned probability threshold approach achieves the best \nresult on the test set. The model attains a Jaccard Index of 50.0%, a F-score \nof 0.56, and a hit rate of 80.5%. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c4448b3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04813"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Hao Peng, Xiaoli Bai", "title": "Improving Orbit Prediction Accuracy through Supervised Machine Learning. (arXiv:1801.04856v1 [astro-ph.EP])", "alternate": [{"href": "http://arxiv.org/abs/1801.04856", "type": "text/html"}], "timestampUsec": "1516080167762982", "comments": [], "summary": {"content": "<p>Due to the lack of information such as the space environment condition and \nresident space objects' (RSOs') body characteristics, current orbit predictions \nthat are solely grounded on physics-based models may fail to achieve required \naccuracy for collision avoidance and have led to satellite collisions already. \nThis paper presents a methodology to predict RSOs' trajectories with higher \naccuracy than that of the current methods. Inspired by the machine learning \n(ML) theory through which the models are learned based on large amounts of \nobserved data and the prediction is conducted without explicitly modeling space \nobjects and space environment, the proposed ML approach integrates \nphysics-based orbit prediction algorithms with a learning-based process that \nfocuses on reducing the prediction errors. Using a simulation-based space \ncatalog environment as the test bed, the paper demonstrates three types of \ngeneralization capability for the proposed ML approach: 1) the ML model can be \nused to improve the same RSO's orbit information that is not available during \nthe learning process but shares the same time interval as the training data; 2) \nthe ML model can be used to improve predictions of the same RSO at future \nepochs; and 3) the ML model based on a RSO can be applied to other RSOs that \nshare some common features. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c4448ba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04856"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Quoc Tran-Dinh, Volkan Cevher", "title": "Smooth Alternating Direction Methods for Nonsmooth Constrained Convex Optimization. (arXiv:1507.03734v3 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1507.03734", "type": "text/html"}], "timestampUsec": "1516080167762981", "comments": [], "summary": {"content": "<p>We propose two new alternating direction methods to solve \"fully\" nonsmooth \nconstrained convex problems. Our algorithms have the best known worst-case \niteration-complexity guarantee under mild assumptions for both the objective \nresidual and feasibility gap. Through theoretical analysis, we show how to \nupdate all the algorithmic parameters automatically with clear impact on the \nconvergence performance. We also provide a representative numerical example \nshowing the advantages of our methods over the classical alternating direction \nmethods using a well-known feasibility problem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c4448c0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1507.03734"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Bruce Hajek, Yihong Wu, Jiaming Xu", "title": "Recovering a Hidden Community Beyond the Kesten-Stigum Limit in $O(|E| \\log^*|V|)$ Time. (arXiv:1510.02786v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1510.02786", "type": "text/html"}], "timestampUsec": "1516080167762980", "comments": [], "summary": {"content": "<p>Community detection is considered for a stochastic block model graph of n \nvertices, with K vertices in the planted community, edge probability p for \npairs of vertices both in the community, and edge probability q for other pairs \nof vertices. \n</p> \n<p>The main focus of the paper is on weak recovery of the community based on the \ngraph G, with o(K) misclassified vertices on average, in the sublinear regime \n$n^{1-o(1)} \\leq K \\leq o(n).$ A critical parameter is the effective \nsignal-to-noise ratio $\\lambda=K^2(p-q)^2/((n-K)q)$, with $\\lambda=1$ \ncorresponding to the Kesten-Stigum threshold. We show that a belief propagation \nalgorithm achieves weak recovery if $\\lambda&gt;1/e$, beyond the Kesten-Stigum \nthreshold by a factor of $1/e.$ The belief propagation algorithm only needs to \nrun for $\\log^\\ast n+O(1) $ iterations, with the total time complexity $O(|E| \n\\log^*n)$, where $\\log^*n$ is the iterated logarithm of $n.$ Conversely, if \n$\\lambda \\leq 1/e$, no local algorithm can asymptotically outperform trivial \nrandom guessing. Furthermore, a linear message-passing algorithm that \ncorresponds to applying power iteration to the non-backtracking matrix of the \ngraph is shown to attain weak recovery if and only if $\\lambda&gt;1$. In addition, \nthe belief propagation algorithm can be combined with a linear-time voting \nprocedure to achieve the information limit of exact recovery (correctly \nclassify all vertices with high probability) for all $K \\ge \\frac{n}{\\log n} \n\\left( \\rho_{\\rm BP} +o(1) \\right),$ where $\\rho_{\\rm BP}$ is a function of \n$p/q$. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c4448ca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1510.02786"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Botao Hao, Will Wei Sun, Yufeng Liu, Guang Cheng", "title": "Simultaneous Clustering and Estimation of Heterogeneous Graphical Models. (arXiv:1611.09391v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.09391", "type": "text/html"}], "timestampUsec": "1516080167762979", "comments": [], "summary": {"content": "<p>We consider joint estimation of multiple graphical models arising from \nheterogeneous and high-dimensional observations. Unlike most previous \napproaches which assume that the cluster structure is given in advance, an \nappealing feature of our method is to learn cluster structure while estimating \nheterogeneous graphical models. This is achieved via a high dimensional version \nof Expectation Conditional Maximization (ECM) algorithm (Meng and Rubin, 1993). \nA joint graphical lasso penalty is imposed on the conditional maximization step \nto extract both homogeneity and heterogeneity components across all clusters. \nOur algorithm is computationally efficient due to fast sparse learning routines \nand can be implemented without unsupervised learning knowledge. The superior \nperformance of our method is demonstrated by extensive experiments and its \napplication to a Glioblastoma cancer dataset reveals some new insights in \nunderstanding the Glioblastoma cancer. In theory, a non-asymptotic error bound \nis established for the output directly from our high dimensional ECM algorithm, \nand it consists of two quantities: statistical error (statistical accuracy) and \noptimization error (computational complexity). Such a result gives a \ntheoretical guideline in terminating our ECM iterations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c4448e0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.09391"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Oktay Gunluk, Jayant Kalagnanam, Matt Menickelly, Katya Scheinberg", "title": "Optimal Generalized Decision Trees via Integer Programming. (arXiv:1612.03225v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1612.03225", "type": "text/html"}], "timestampUsec": "1516080167762978", "comments": [], "summary": {"content": "<p>Decision trees have been a very popular class of predictive models for \ndecades due to their interpretability and good performance on categorical \nfeatures. However, they are not always robust and tend to overfit the data. \nAdditionally, if allowed to grow large, they lose interpretability. In this \npaper, we present a novel mixed integer programming formulation to construct \noptimal decision trees of specified size. We take special structure of \ncategorical features into account and allow combinatorial decisions (based on \nsubsets of values of such a feature) at each node. We show that very good \naccuracy can be achieved with small trees using moderately-sized training sets. \nThe optimization problems we solve are easily tractable with modern solvers. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c4448f4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.03225"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zachary Charles, Amin Jalali, Rebecca Willett", "title": "Subspace Clustering with Missing and Corrupted Data. (arXiv:1707.02461v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.02461", "type": "text/html"}], "timestampUsec": "1516080167762977", "comments": [], "summary": {"content": "<p>Given full or partial information about a collection of points that lie close \nto a union of several subspaces, subspace clustering refers to the process of \nclustering the points according to their subspace and identifying the \nsubspaces. One popular approach, sparse subspace clustering (SSC), represents \neach sample as a weighted combination of the other samples, with weights of \nminimal $\\ell_1$ norm, and then uses those learned weights to cluster the \nsamples. SSC is stable in settings where each sample is contaminated by a \nrelatively small amount of noise. However, when there is a significant amount \nof additive noise, or a considerable number of entries are missing, theoretical \nguarantees are scarce. In this paper, we study a robust variant of SSC and \nestablish clustering guarantees in the presence of corrupted or missing data. \nWe give explicit bounds on amount of noise and missing data that the algorithm \ncan tolerate, both in deterministic settings and in a random generative model. \nNotably, our approach provides guarantees for higher tolerance to noise and \nmissing data than existing analyses for this method. By design, the results \nhold even when we do not know the locations of the missing data; e.g., as in \npresence-only data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c444903", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.02461"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yazhen Wang", "title": "Asymptotic Analysis via Stochastic Differential Equations of Gradient Descent Algorithms in Statistical and Computational Paradigms. (arXiv:1711.09514v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09514", "type": "text/html"}], "timestampUsec": "1516080167762976", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329251038\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329251038&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper investigates asymptotic behaviors of gradient descent algorithms \n(particularly accelerated gradient descent and stochastic gradient descent) in \nthe context of stochastic optimization arose in statistics and machine learning \nwhere objective functions are estimated from available data. We show that these \nalgorithms can be modeled by continuous-time ordinary or stochastic \ndifferential equations, and their asymptotic dynamic evolutions and \ndistributions are governed by some linear ordinary or stochastic differential \nequations, as the data size goes to infinity. We illustrate that our study can \nprovide a novel unified framework for a joint computational and statistical \nasymptotic analysis on dynamic behaviors of these algorithms with the time (or \nthe number of iterations in the algorithms) and large sample behaviors of the \nstatistical decision rules (like estimators and classifiers) that the \nalgorithms are applied to compute, where the statistical decision rules are the \nlimits of the random sequences generated from these iterative algorithms as the \nnumber of iterations goes to infinity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1516080167763", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035c44490f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09514"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Edward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F. Stewart, Jimeng Sun", "title": "Generating Multi-label Discrete Patient Records using Generative Adversarial Networks. (arXiv:1703.06490v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.06490", "type": "text/html"}], "timestampUsec": "1515995659414901", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3292a11cf\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3292a11cf&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Access to electronic health record (EHR) data has motivated computational \nadvances in medical research. However, various concerns, particularly over \nprivacy, can limit access to and collaborative use of EHR data. Sharing \nsynthetic EHR data could mitigate risk. In this paper, we propose a new \napproach, medical Generative Adversarial Network (medGAN), to generate \nrealistic synthetic patient records. Based on input real patient records, \nmedGAN can generate high-dimensional discrete variables (e.g., binary and count \nfeatures) via a combination of an autoencoder and generative adversarial \nnetworks. We also propose minibatch averaging to efficiently avoid mode \ncollapse, and increase the learning efficiency with batch normalization and \nshortcut connections. To demonstrate feasibility, we showed that medGAN \ngenerates synthetic patient records that achieve comparable performance to real \ndata on many experiments including distribution statistics, predictive modeling \ntasks and a medical expert review. We also empirically observe a limited \nprivacy risk in both identity and attribute disclosure using medGAN. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515995659415", "annotations": [], "published": 1515995660, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8f2627", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.06490"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Judea Pearl", "title": "Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution. (arXiv:1801.04016v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.04016", "type": "text/html"}], "timestampUsec": "1515995659414900", "comments": [], "summary": {"content": "<p>Current machine learning systems operate, almost exclusively, in a \nstatistical, or model-free mode, which entails severe theoretical limits on \ntheir power and performance. Such systems cannot reason about interventions and \nretrospection and, therefore, cannot serve as the basis for strong AI. To \nachieve human level intelligence, learning machines need the guidance of a \nmodel of reality, similar to the ones used in causal inference tasks. To \ndemonstrate the essential role of such models, I will present a summary of \nseven tasks which are beyond reach of current machine learning systems and \nwhich have been accomplished using the tools of causal modeling. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515995659415", "annotations": [], "published": 1515995660, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8f2634", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04016"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Akram Erraqabi, Aristide Baratin, Yoshua Bengio, Simon Lacoste-Julien", "title": "A3T: Adversarially Augmented Adversarial Training. (arXiv:1801.04055v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.04055", "type": "text/html"}], "timestampUsec": "1515995659414899", "comments": [], "summary": {"content": "<p>Recent research showed that deep neural networks are highly sensitive to \nso-called adversarial perturbations, which are tiny perturbations of the input \ndata purposely designed to fool a machine learning classifier. Most \nclassification models, including deep learning models, are highly vulnerable to \nadversarial attacks. In this work, we investigate a procedure to improve \nadversarial robustness of deep neural networks through enforcing representation \ninvariance. The idea is to train the classifier jointly with a discriminator \nattached to one of its hidden layer and trained to filter the adversarial \nnoise. We perform preliminary experiments to test the viability of the approach \nand to compare it to other standard adversarial training methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515995659415", "annotations": [], "published": 1515995660, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8f263e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04055"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ishmael Belghazi, Sai Rajeswar, Aristide Baratin, R Devon Hjelm, Aaron Courville", "title": "MINE: Mutual Information Neural Estimation. (arXiv:1801.04062v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.04062", "type": "text/html"}], "timestampUsec": "1515995659414898", "comments": [], "summary": {"content": "<p>We argue that the estimation of the mutual information between high \ndimensional continuous random variables is achievable by gradient descent over \nneural networks. This paper presents a Mutual Information Neural Estimator \n(MINE) that is linearly scalable in dimensionality as well as in sample size. \nMINE is back-propable and we prove that it is strongly consistent. We \nillustrate a handful of applications in which MINE is succesfully applied to \nenhance the property of generative models in both unsupervised and supervised \nsettings. We apply our framework to estimate the information bottleneck, and \napply it in tasks related to supervised classification problems. Our results \ndemonstrate substantial added flexibility and improvement in these settings. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515995659415", "annotations": [], "published": 1515995660, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8f2644", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04062"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ali Batuhan Yard&#x131;m, Victor Kristof, Lucas Maystre, Matthias Grossglauser", "title": "Can Who-Edits-What Predict Edit Survival?. (arXiv:1801.04159v1 [stat.AP])", "alternate": [{"href": "http://arxiv.org/abs/1801.04159", "type": "text/html"}], "timestampUsec": "1515995659414897", "comments": [], "summary": {"content": "<p>The Internet has enabled the emergence of massive online collaborative \nprojects. As the number of contributors to these projects grows, it becomes \nincreasingly important to understand and predict whether the edits that users \nmake will eventually impact the project positively. Existing solutions either \nrely on a user reputation system or consist of a highly-specialized predictor \ntailored to a specific peer-production system. In this work, we explore a \ndifferent point in the solution space, which does not involve any content-based \nfeature of the edits. To this end, we formulate a statistical model of edit \noutcomes. We view each edit as a game between the editor and the component of \nthe project. We posit that the probability of a positive outcome is a function \nof the editor's skill, of the difficulty of editing the component and of a \nuser-component interaction term. Our model is broadly applicable, as it only \nrequires observing data about who makes an edit, what the edit affects and \nwhether the edit survives or not. Then, we consider Wikipedia and the Linux \nkernel, two examples of large-scale collaborative projects, and we seek to \nunderstand whether this simple model can effectively predict edit survival: in \nboth cases, we provide a positive answer. Our approach significantly \noutperforms those based solely on user reputation and bridges the gap with \nspecialized predictors that use content-based features. Furthermore, inspecting \nthe model parameters enables us to discover interesting structure in the data. \nOur method is simple to implement, computationally inexpensive, and it produces \ninterpretable results; as such, we believe that it is a valuable tool to \nanalyze collaborative systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515995659415", "annotations": [], "published": 1515995660, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8f2649", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04159"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Felix Horger, Tobias W&#xfc;rfl, Vincent Christlein, Andreas Maier", "title": "Deep Learning for Sampling from Arbitrary Probability Distributions. (arXiv:1801.04211v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.04211", "type": "text/html"}], "timestampUsec": "1515995659414896", "comments": [], "summary": {"content": "<p>This paper proposes a fully connected neural network model to map samples \nfrom a uniform distribution to samples of any explicitly known probability \ndensity function. During the training, the Jensen-Shannon divergence between \nthe distribution of the model's output and the target distribution is \nminimized. We experimentally demonstrate that our model converges towards the \ndesired state. It provides an alternative to existing sampling methods such as \ninversion sampling, rejection sampling, Gaussian mixture models and \nMarkov-Chain-Monte-Carlo. Our model has high sampling efficiency and is easily \napplied to any probability distribution, without the need of further analytical \nor numerical calculations. It can produce correlated samples, such that the \noutput distribution converges faster towards the target than for independent \nsamples. But it is also able to produce independent samples, if single values \nare fed into the network and the input values are independent as well. We focus \non one-dimensional sampling, but additionally illustrate a two-dimensional \nexample with a target distribution of dependent variables. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515995659415", "annotations": [], "published": 1515995660, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8f264e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04211"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Dmitriy Serdyuk, Nan Rosemary Ke, Alessandro Sordoni, Adam Trischler, Chris Pal, Yoshua Bengio", "title": "Twin Networks: Matching the Future for Sequence Generation. (arXiv:1708.06742v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.06742", "type": "text/html"}], "timestampUsec": "1515995659414895", "comments": [], "summary": {"content": "<p>We propose a simple technique for encouraging generative RNNs to plan ahead. \nWe train a \"backward\" recurrent network to generate a given sequence in reverse \norder, and we encourage states of the forward model to predict cotemporal \nstates of the backward model. The backward network is used only during \ntraining, and plays no role during sampling or inference. We hypothesize that \nour approach eases modeling of long-term dependencies by implicitly forcing the \nforward states to hold information about the longer-term future (as contained \nin the backward states). We show empirically that our approach achieves 9% \nrelative improvement for a speech recognition task, and achieves significant \nimprovement on a COCO caption generation task. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515995659415", "annotations": [], "published": 1515995660, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8f2655", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.06742"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Haiping Huang", "title": "Mean-field theory of input dimensionality reduction in unsupervised deep neural networks. (arXiv:1710.01467v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.01467", "type": "text/html"}], "timestampUsec": "1515995659414893", "comments": [], "summary": {"content": "<p>Deep neural networks as powerful tools are widely used in various domains. \nHowever, the nature of computations at each layer of the deep networks is far \nfrom being well understood. Increasing the interpretability of deep neural \nnetworks is thus important. Here, we construct a mean-field framework to \nunderstand how compact representations are developed across layers, not only in \ndeterministic random deep networks but also in generative deep networks where \nnetwork parameters are learned from input data. Our theory shows that the deep \ncomputation implements a dimensionality reduction while maintaining a finite \nlevel of weak correlations between neurons for possible feature extraction. \nThis work may pave the way for understanding how a sensory hierarchy works in \ngeneral. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515995659415", "annotations": [], "published": 1515995660, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8f2664", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.01467"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Lionel Pibre, Pasquet J&#xe9;r&#xf4;me, Dino Ienco, Marc Chaumont", "title": "Deep learning is a good steganalysis tool when embedding key is reused for different images, even if there is a cover source-mismatch. (arXiv:1511.04855v2 [cs.MM] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1511.04855", "type": "text/html"}], "timestampUsec": "1515994816955133", "comments": [], "summary": {"content": "<p>Since the BOSS competition, in 2010, most steganalysis approaches use a \nlearning methodology involving two steps: feature extraction, such as the Rich \nModels (RM), for the image representation, and use of the Ensemble Classifier \n(EC) for the learning step. In 2015, Qian et al. have shown that the use of a \ndeep learning approach that jointly learns and computes the features, is very \npromising for the steganalysis. In this paper, we follow-up the study of Qian \net al., and show that, due to intrinsic joint minimization, the results \nobtained from a Convolutional Neural Network (CNN) or a Fully Connected Neural \nNetwork (FNN), if well parameterized, surpass the conventional use of a RM with \nan EC. First, numerous experiments were conducted in order to find the best \" \nshape \" of the CNN. Second, experiments were carried out in the clairvoyant \nscenario in order to compare the CNN and FNN to an RM with an EC. The results \nshow more than 16% reduction in the classification error with our CNN or FNN. \nThird, experiments were also performed in a cover-source mismatch setting. The \nresults show that the CNN and FNN are naturally robust to the mismatch problem. \nIn Addition to the experiments, we provide discussions on the internal \nmechanisms of a CNN, and weave links with some previously stated ideas, in \norder to understand the impressive results we obtained. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da91f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1511.04855"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Robin Tibor Schirrmeister, Lukas Gemein, Katharina Eggensperger, Frank Hutter, Tonio Ball", "title": "Deep learning with convolutional neural networks for decoding and visualization of EEG pathology. (arXiv:1708.08012v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.08012", "type": "text/html"}], "timestampUsec": "1515994816955132", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3292a1506\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3292a1506&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We apply convolutional neural networks (ConvNets) to the task of \ndistinguishing pathological from normal EEG recordings in the Temple University \nHospital EEG Abnormal Corpus. We use two basic, shallow and deep ConvNet \narchitectures recently shown to decode task-related information from EEG at \nleast as well as established algorithms designed for this purpose. In decoding \nEEG pathology, both ConvNets reached substantially better accuracies (about 6% \nbetter, ~85% vs. ~79%) than the only published result for this dataset, and \nwere still better when using only 1 minute of each recording for training and \nonly six seconds of each recording for testing. We used automated methods to \noptimize architectural hyperparameters and found intriguingly different ConvNet \narchitectures, e.g., with max pooling as the only nonlinearity. Visualizations \nof the ConvNet decoding behavior showed that they used spectral power changes \nin the delta (0-4 Hz) and theta (4-8 Hz) frequency range, possibly alongside \nother features, consistent with expectations derived from spectral analysis of \nthe EEG data and from the textual medical reports. Analysis of the textual \nmedical reports also highlighted the potential for accuracy increases by \nintegrating contextual information, such as the age of subjects. In summary, \nthe ConvNets and visualization techniques used in this study constitute a next \nstep towards clinically useful automated EEG diagnosis and establish a new \nbaseline for future work on this topic. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da925", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.08012"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Lucas Bechberger, Kai-Uwe K&#xfc;hnberger", "title": "Formalized Conceptual Spaces with a Geometric Representation of Correlations. (arXiv:1801.03929v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.03929", "type": "text/html"}], "timestampUsec": "1515994816955130", "comments": [], "summary": {"content": "<p>The highly influential framework of conceptual spaces provides a geometric \nway of representing knowledge. Instances are represented by points in a \nsimilarity space and concepts are represented by convex regions in this space. \nAfter pointing out a problem with the convexity requirement, we propose a \nformalization of conceptual spaces based on fuzzy star-shaped sets. Our \nformalization uses a parametric definition of concepts and extends the original \nframework by adding means to represent correlations between different domains \nin a geometric way. Moreover, we define various operations for our \nformalization, both for creating new concepts from old ones and for measuring \nrelations between concepts. We present an illustrative toy-example and sketch a \nresearch project on concept formation that is based on both our formalization \nand its implementation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da930", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03929"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Glen Berseth, Michiel van de Panne", "title": "Model-Based Action Exploration. (arXiv:1801.03954v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.03954", "type": "text/html"}], "timestampUsec": "1515994816955129", "comments": [], "summary": {"content": "<p>Deep reinforcement learning has great stride in solving challenging motion \ncontrol tasks. \n</p> \n<p>Recently there has been a significant amount of work on methods to exploit \nthe data gathered during training, but less work is done on good methods for \ngenerating data to learn from. \n</p> \n<p>For continuous actions domains, the typical method for generating exploratory \nactions is by sampling from a Gaussian distribution centred around the mean of \na policy. \n</p> \n<p>Although these methods can find an optimal policy, in practise, they do not \nscale well, and solving environments with many actions dimensions becomes \nimpractical. \n</p> \n<p>We consider learning a forward dynamics model to predict the result, \n($s_{t+1}$), of taking a particular action, ($a$), given a specific observation \nof the state, ($s_{t}$). \n</p> \n<p>With a model such as this we, can perform what comes more naturally to \nbiological systems that have already collect experience, we perform internal \npredictions of outcomes and endeavour to try actions we believe have a \nreasonable chance of success. \n</p> \n<p>This method greatly reduces the space of exploratory actions, increasing \nlearning speed and enables higher quality solutions to difficult problems, such \nas robotic locomotion. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da936", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03954"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Eisa Alanazi, Malek Mouhoub, Sandra Zilles", "title": "Interactive Learning of Acyclic Conditional Preference Networks. (arXiv:1801.03968v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.03968", "type": "text/html"}], "timestampUsec": "1515994816955128", "comments": [], "summary": {"content": "<p>Learning of user preferences, as represented by, for example, Conditional \nPreference Networks (CP-nets), has become a core issue in AI research. Recent \nstudies investigate learning of CP-nets from randomly chosen examples or from \nmembership and equivalence queries. To assess the optimality of learning \nalgorithms as well as to better understand the combinatorial structure of \nclasses of CP-nets, it is helpful to calculate certain learning-theoretic \ninformation complexity parameters. This paper determines bounds on or exact \nvalues of some of the most central information complexity parameters, namely \nthe VC dimension, the (recursive) teaching dimension, the self-directed \nlearning complexity, and the optimal mistake bound, for classes of acyclic \nCP-nets. We further provide an algorithm that learns tree-structured CP-nets \nfrom membership queries. Using our results on complexity parameters, we assess \nthe optimality of our algorithm as well as that of another query learning \nalgorithm for acyclic CP-nets presented in the literature. Our algorithm is \nnear-optimal, and can, under certain assumptions be adapted to the case when \nthe membership oracle is faulty. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da938", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03968"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mufti Mahmud, M. Shamim Kaiser, M. Mostafizur Rahman, M. Arifur Rahman, Antesar Shabut, Shamim Al-Mamun, Amir Hussain", "title": "A Brain-Inspired Trust Management Model to Assure Security in a Cloud based IoT Framework for Neuroscience Applications. (arXiv:1801.03984v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1801.03984", "type": "text/html"}], "timestampUsec": "1515994816955127", "comments": [], "summary": {"content": "<p>Rapid popularity of Internet of Things (IoT) and cloud computing permits \nneuroscientists to collect multilevel and multichannel brain data to better \nunderstand brain functions, diagnose diseases, and devise treatments. To ensure \nsecure and reliable data communication between end-to-end (E2E) devices \nsupported by current IoT and cloud infrastructure, trust management is needed \nat the IoT and user ends. This paper introduces a Neuro-Fuzzy based \nBrain-inspired trust management model (TMM) to secure IoT devices and relay \nnodes, and to ensure data reliability. The proposed TMM utilizes node \nbehavioral trust and data trust estimated using Adaptive Neuro-Fuzzy Inference \nSystem and weighted-additive methods respectively to assess the nodes \ntrustworthiness. In contrast to the existing fuzzy based TMMs, the NS2 \nsimulation results confirm the robustness and accuracy of the proposed TMM in \nidentifying malicious nodes in the communication network. With the growing \nusage of cloud based IoT frameworks in Neuroscience research, integrating the \nproposed TMM into the existing infrastructure will assure secure and reliable \ndata communication among the E2E devices. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da93f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03984"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Min Chen, Stefanos Nikolaidis, Harold Soh, David Hsu, Siddhartha Srinivasa", "title": "Planning with Trust for Human-Robot Collaboration. (arXiv:1801.04099v1 [cs.RO])", "alternate": [{"href": "http://arxiv.org/abs/1801.04099", "type": "text/html"}], "timestampUsec": "1515994816955126", "comments": [], "summary": {"content": "<p>Trust is essential for human-robot collaboration and user adoption of \nautonomous systems, such as robot assistants. This paper introduces a \ncomputational model which integrates trust into robot decision-making. \nSpecifically, we learn from data a partially observable Markov decision process \n(POMDP) with human trust as a latent variable. The trust-POMDP model provides a \nprincipled approach for the robot to (i) infer the trust of a human teammate \nthrough interaction, (ii) reason about the effect of its own actions on human \nbehaviors, and (iii) choose actions that maximize team performance over the \nlong term. We validated the model through human subject experiments on a \ntable-clearing task in simulation (201 participants) and with a real robot (20 \nparticipants). The results show that the trust-POMDP improves human-robot team \nperformance in this task. They further suggest that maximizing trust in itself \nmay not improve team performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da945", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04099"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jonas Rothfuss, Fabio Ferreira, Eren Erdal Aksoy, You Zhou, Tamim Asfour", "title": "Deep Episodic Memory: Encoding, Recalling, and Predicting Episodic Experiences for Robot Action Execution. (arXiv:1801.04134v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.04134", "type": "text/html"}], "timestampUsec": "1515994816955125", "comments": [], "summary": {"content": "<p>We present a novel deep neural network architecture for representing robot \nexperiences in an episodic-like memory which facilitates encoding, recalling, \nand predicting action experiences. Our proposed unsupervised deep episodic \nmemory model 1) encodes observed actions in a latent vector space and, based on \nthis latent encoding, 2) infers action categories, 3) reconstructs original \nframes, and 4) predicts future frames. We evaluate the proposed model on two \ndifferent large-scale action datasets. Results show that conceptually similar \nactions are mapped into the same region of the latent vector space. Results \nshow that conceptual similarity of videos is reflected by the proximity of \ntheir vector representations in the latent space.Based on this contribution, we \nintroduce an action matching and retrieval mechanism and evaluate its \nperformance and generalization capability on a real humanoid robot in an action \nexecution scenario. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da94b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04134"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Andrey Chistyakov", "title": "Multilayered Model of Speech. (arXiv:1801.04170v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.04170", "type": "text/html"}], "timestampUsec": "1515994816955124", "comments": [], "summary": {"content": "<p>Human speech is the most important part of General Artificial Intelligence \nand subject of much research. The hypothesis proposed in this article provides \nexplanation of difficulties that modern science tackles in the field of human \nbrain simulation. The hypothesis is based on the author's conviction that the \nbrain of any given person has different ability to process and store \ninformation. Therefore, the approaches that are currently used to create \nGeneral Artificial Intelligence have to be altered. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da94e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04170"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michael Crosscombe, Jonathan Lawry", "title": "A Model of Multi-Agent Consensus for Vague and Uncertain Beliefs. (arXiv:1612.03433v2 [cs.MA] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1612.03433", "type": "text/html"}], "timestampUsec": "1515994816955123", "comments": [], "summary": {"content": "<p>Consensus formation is investigated for multi-agent systems in which agents' \nbeliefs are both vague and uncertain. Vagueness is represented by a third truth \nstate meaning \\emph{borderline}. This is combined with a probabilistic model of \nuncertainty. A belief combination operator is then proposed which exploits \nborderline truth values to enable agents with conflicting beliefs to reach a \ncompromise. A number of simulation experiments are carried out in which agents \napply this operator in pairwise interactions, under the bounded confidence \nrestriction that the two agents' beliefs must be sufficiently consistent with \neach other before agreement can be reached. As well as studying the consensus \noperator in isolation we also investigate scenarios in which agents are \ninfluenced either directly or indirectly by the state of the world. For the \nformer we conduct simulations which combine consensus formation with belief \nupdating based on evidence. For the latter we investigate the effect of \nassuming that the closer an agent's beliefs are to the truth the more visible \nthey are in the consensus building process. In all cases applying the consensus \noperators results in the population converging to a single shared belief which \nis both crisp and certain. Furthermore, simulations which combine consensus \nformation with evidential updating converge faster to a shared opinion which is \ncloser to the actual state of the world than those in which beliefs are only \nchanged as a result of directly receiving new evidence. Finally, if agent \ninteractions are guided by belief quality measured as similarity to the true \nstate of the world, then applying the consensus operator alone results in the \npopulation converging to a high quality shared belief. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da952", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.03433"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Josh Warren, Jeff Lipkowitz, Vadim Sokolov", "title": "Clusters of Driving Behavior from Observational Smartphone Data. (arXiv:1710.04502v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.04502", "type": "text/html"}], "timestampUsec": "1515994816955122", "comments": [], "summary": {"content": "<p>Understanding driving behaviors is essential for improving safety and \nmobility of our transportation systems. Data is usually collected via \nsimulator-based studies or naturalistic driving studies. Those techniques allow \nfor understanding relations between demographics, road conditions and safety. \nOn the other hand, they are very costly and time consuming. Thanks to the \nubiquity of smartphones, we have an opportunity to substantially complement \nmore traditional data collection techniques with data extracted from phone \nsensors, such as GPS, accelerometer gyroscope and camera. We developed \nstatistical models that provided insight into driver behavior in the San \nFrancisco metro area based on tens of thousands of driver logs. We used novel \ndata sources to support our work. We used cell phone sensor data drawn from \nfive hundred drivers in San Francisco to understand the speed of traffic across \nthe city as well as the maneuvers of drivers in different areas. Specifically, \nwe clustered drivers based on their driving behavior. We looked at driver norms \nby street and flagged driving behaviors that deviated from the norm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da958", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.04502"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Daniel Levy, Matthew D. Hoffman, Jascha Sohl-Dickstein", "title": "Generalizing Hamiltonian Monte Carlo with Neural Networks. (arXiv:1711.09268v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09268", "type": "text/html"}], "timestampUsec": "1515994816955121", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3292a1865\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3292a1865&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present a general-purpose method to train Markov chain Monte Carlo \nkernels, parameterized by deep neural networks, that converge and mix quickly \nto their target distribution. Our method generalizes Hamiltonian Monte Carlo \nand is trained to maximize expected squared jumped distance, a proxy for mixing \nspeed. We demonstrate large empirical gains on a collection of simple but \nchallenging distributions, for instance achieving a 106x improvement in \neffective sample size in one case, and mixing when standard HMC makes no \nmeasurable progress in a second. Finally, we show quantitative and qualitative \ngains on a real-world task: latent-variable generative modeling. We release an \nopen source TensorFlow implementation of the algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da95c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09268"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Osonde Osoba, Bart Kosko", "title": "Noisy Expectation-Maximization: Applications and Generalizations. (arXiv:1801.04053v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.04053", "type": "text/html"}], "timestampUsec": "1515994816955120", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3292f066d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3292f066d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present a noise-injected version of the Expectation-Maximization (EM) \nalgorithm: the Noisy Expectation Maximization (NEM) algorithm. The NEM \nalgorithm uses noise to speed up the convergence of the EM algorithm. The NEM \ntheorem shows that injected noise speeds up the average convergence of the EM \nalgorithm to a local maximum of the likelihood surface if a positivity \ncondition holds. The generalized form of the noisy expectation-maximization \n(NEM) algorithm allow for arbitrary modes of noise injection including adding \nand multiplying noise to the data. \n</p> \n<p>We demonstrate these noise benefits on EM algorithms for the Gaussian mixture \nmodel (GMM) with both additive and multiplicative NEM noise injection. A \nseparate theorem (not presented here) shows that the noise benefit for \nindependent identically distributed additive noise decreases with sample size \nin mixture models. This theorem implies that the noise benefit is most \npronounced if the data is sparse. Injecting blind noise only slowed \nconvergence. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da964", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04053"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "A. Vafaei Sadr, M. Farhang, S. M. S. Movahed, B. Bassett, M. Kunz", "title": "Cosmic String Detection with Tree-Based Machine Learning. (arXiv:1801.04140v1 [astro-ph.CO])", "alternate": [{"href": "http://arxiv.org/abs/1801.04140", "type": "text/html"}], "timestampUsec": "1515994816955119", "comments": [], "summary": {"content": "<p>We explore the use of random forest and gradient boosting, two powerful \ntree-based machine learning algorithms, for the detection of cosmic strings in \nmaps of the cosmic microwave background (CMB), through their unique \nGott-Kaiser-Stebbins effect on the temperature anisotropies.The information in \nthe maps is compressed into feature vectors before being passed to the learning \nunits. The feature vectors contain various statistical measures of processed \nCMB maps that boost the cosmic string detectability. Our proposed classifiers, \nafter training, give results improved over or similar to the claimed \ndetectability levels of the existing methods for string tension, $G\\mu$. They \ncan make $3\\sigma$ detection of strings with $G\\mu \\gtrsim 2.1\\times 10^{-10}$ \nfor noise-free, $0.9'$-resolution CMB observations. The minimum detectable \ntension increases to $G\\mu \\gtrsim 3.0\\times 10^{-8}$ for a more realistic, CMB \nS4-like (II) strategy, still a significant improvement over the previous \nresults. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da967", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04140"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516598533, "author": "Xiaoyue Xi, Fran&#xe7;ois-Xavier Briol, Mark Girolami", "title": "Bayesian Quadrature for Multiple Related Integrals. (arXiv:1801.04153v2 [stat.CO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1801.04153", "type": "text/html"}], "timestampUsec": "1515994816955118", "comments": [], "summary": {"content": "<p>Bayesian probabilistic numerical methods are a set of tools providing \nposterior distributions on the output of numerical methods. The use of these \nmethods is usually motivated by the fact that they can represent our \nuncertainty due to incomplete/finite information about the continuous \nmathematical problem being approximated. In this paper, we demonstrate that \nthis paradigm can provide additional advantages, such as the possibility of \ntransferring information between several numerical methods. This allows users \nto represent uncertainty in a more faithful manner and, as a by-product, \nprovide increased numerical efficiency. We propose the first such numerical \nmethod by extending the well-known Bayesian quadrature algorithm to the case \nwhere we are interested in computing the integral of several related functions. \nWe then demonstrate its efficiency in the context of multi-fidelity models for \ncomplex engineering systems, as well as a problem of global illumination in \ncomputer graphics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da96c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04153"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mor Absa Loum (LM-Orsay), Marie-Anne Poursat (LM-Orsay), Abdourahmane Sow, Amadou Sall, Cheikh Loucoubar (G4-IPD), Elisabeth Gassiat (LM-Orsay)", "title": "Multinomial logistic model for coinfection diagnosis between arbovirus and malaria in Kedougou. (arXiv:1801.04212v1 [stat.AP])", "alternate": [{"href": "http://arxiv.org/abs/1801.04212", "type": "text/html"}], "timestampUsec": "1515994816955117", "comments": [], "summary": {"content": "<p>In tropical regions, populations continue to suffer morbidity and mortality \nfrom malaria and arboviral diseases. In Kedougou (Senegal), these illnesses are \nall endemic due to the climate and its geographical position. The \nco-circulation of malaria parasites and arboviruses can explain the observation \nof coinfected cases. Indeed there is strong resemblance in symptoms between \nthese diseases making problematic targeted medical care of coinfected cases. \nThis is due to the fact that the origin of illness is not obviously known. Some \ncases could be immunized against one or the other of the pathogens, immunity \ntypically acquired with factors like age and exposure as usual for endemic \narea. Then, coinfection needs to be better diagnosed. Using data collected from \npatients in Kedougou region, from 2009 to 2013, we adjusted a multinomial \nlogistic model and selected relevant variables in explaining coinfection \nstatus. We observed specific sets of variables explaining each of the diseases \nexclusively and the coinfection. We tested the independence between arboviral \nand malaria infections and derived coinfection probabilities from the model \nfitting. In case of a coinfection probability greater than a threshold value to \nbe calibrated on the data, duration of illness above 3 days and age above 10 \nyears-old are mostly indicative of arboviral disease while body temperature \nhigher than 40{\\textdegree}C and presence of nausea or vomiting symptoms during \nthe rainy season are mostly indicative of malaria disease. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da971", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.04212"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Aur&#xe9;lie Fischer (UPD7), Lucie Montuelle (UPD7), Mathilde Mougeot (UPD7), Dominique Picard (UPD7)", "title": "Statistical learning for wind power : a modeling and stability study towards forecasting. (arXiv:1610.01000v2 [stat.AP] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.01000", "type": "text/html"}], "timestampUsec": "1515994816955116", "comments": [], "summary": {"content": "<p>We focus on wind power modeling using machine learning techniques. We show on \nreal data provided by the wind energy company Ma{\\\"i}a Eolis, that parametric \nmodels, even following closely the physical equation relating wind production \nto wind speed are outperformed by intelligent learning algorithms. In \nparticular, the CART-Bagging algorithm gives very stable and promising results. \nBesides, as a step towards forecast, we quantify the impact of using \ndeteriorated wind measures on the performances. We show also on this \napplication that the default methodology to select a subset of predictors \nprovided in the standard random forest package can be refined, especially when \nthere exists among the predictors one variable which has a major impact. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da975", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.01000"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter", "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. (arXiv:1706.08500v6 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.08500", "type": "text/html"}], "timestampUsec": "1515994816955115", "comments": [], "summary": {"content": "<p>Generative Adversarial Networks (GANs) excel at creating realistic images \nwith complex models for which maximum likelihood is infeasible. However, the \nconvergence of GAN training has still not been proved. We propose a two \ntime-scale update rule (TTUR) for training GANs with stochastic gradient \ndescent on arbitrary GAN loss functions. TTUR has an individual learning rate \nfor both the discriminator and the generator. Using the theory of stochastic \napproximation, we prove that the TTUR converges under mild assumptions to a \nstationary local Nash equilibrium. The convergence carries over to the popular \nAdam optimization, for which we prove that it follows the dynamics of a heavy \nball with friction and thus prefers flat minima in the objective landscape. For \nthe evaluation of the performance of GANs at image generation, we introduce the \n\"Fr\\'echet Inception Distance\" (FID) which captures the similarity of generated \nimages to real ones better than the Inception Score. In experiments, TTUR \nimproves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) \noutperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN \nBedrooms, and the One Billion Word Benchmark. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da97b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.08500"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Chenchao Zhao, Jun S. Song", "title": "Quantum transport senses community structure in networks. (arXiv:1711.04979v2 [quant-ph] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.04979", "type": "text/html"}], "timestampUsec": "1515994816955114", "comments": [], "summary": {"content": "<p>Quantum time evolution exhibits rich physics, attributable to the interplay \nbetween the density and phase of a wave function. However, unlike classical \nheat diffusion, the wave nature of quantum mechanics has not yet been \nextensively explored in modern data analysis. We propose that the Laplace \ntransform of quantum transport (QT) can be used to construct an ensemble of \nmaps from a given complex network to a circle $S^1$, such that closely-related \nnodes on the network are grouped into sharply concentrated clusters on $S^1$. \nThe resulting QT clustering (QTC) algorithm is as powerful as the \nstate-of-the-art spectral clustering in discerning complex geometric patterns \nand more robust when clusters show strong density variations or heterogeneity \nin size. The observed phenomenon of QTC can be interpreted as a collective \nbehavior of the microscopic nodes that evolve as macroscopic cluster orbitals \nin an effective tight-binding model recapitulating the network. Python source \ncode implementing the algorithm and examples are available at \nhttps://github.com/jssong-lab/QTC. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da97e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.04979"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Abdul-Saboor Sheikh, Kashif Rasul, Andreas Merentitis, Urs Bergmann", "title": "Stochastic Maximum Likelihood Optimization via Hypernetworks. (arXiv:1712.01141v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.01141", "type": "text/html"}], "timestampUsec": "1515994816955113", "comments": [], "summary": {"content": "<p>This work explores maximum likelihood optimization of neural networks through \nhypernetworks. A hypernetwork initializes the weights of another network, which \nin turn can be employed for typical functional tasks such as regression and \nclassification. We optimize hypernetworks to directly maximize the conditional \nlikelihood of target variables given input. Using this approach we obtain \ncompetitive empirical results on regression and classification benchmarks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515994816955", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035b8da985", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01141"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ahmad B. A. Hassanat, Esra&#x27;a Alkafaween, Nedal A. Al-Nawaiseh, Mohammad A. Abbadi, Mouhammd Alkasassbeh, Mahmoud B. Alhasanat", "title": "Enhancing Genetic Algorithms using Multi Mutations. (arXiv:1602.08313v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1602.08313", "type": "text/html"}], "timestampUsec": "1515735584908028", "comments": [], "summary": {"content": "<p>Mutation is one of the most important stages of the genetic algorithm because \nof its impact on the exploration of global optima, and to overcome premature \nconvergence. There are many types of mutation, and the problem lies in \nselection of the appropriate type, where the decision becomes more difficult \nand needs more trial and error. This paper investigates the use of more than \none mutation operator to enhance the performance of genetic algorithms. Novel \nmutation operators are proposed, in addition to two selection strategies for \nthe mutation operators, one of which is based on selecting the best mutation \noperator and the other randomly selects any operator. Several experiments on \nsome Travelling Salesman Problems (TSP) were conducted to evaluate the proposed \nmethods, and these were compared to the well-known exchange mutation and \nrearrangement mutation. The results show the importance of some of the proposed \nmethods, in addition to the significant enhancement of the genetic algorithm's \nperformance, particularly when using more than one mutation operator. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb2332", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1602.08313"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516080169, "author": "Marco F. Cusumano-Towner, Vikash K. Mansinghka", "title": "Using probabilistic programs as proposals. (arXiv:1801.03612v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1801.03612", "type": "text/html"}], "timestampUsec": "1515735584908026", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3292f0a4e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3292f0a4e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Monte Carlo inference has asymptotic guarantees, but can be slow when using \ngeneric proposals. Handcrafted proposals that rely on user knowledge about the \nposterior distribution can be efficient, but are difficult to derive and \nimplement. This paper proposes to let users express their posterior knowledge \nin the form of proposal programs, which are samplers written in probabilistic \nprogramming languages. One strategy for writing good proposal programs is to \ncombine domain-specific heuristic algorithms with neural network models. The \nheuristics identify high probability regions, and the neural networks model the \nposterior uncertainty around the outputs of the algorithm. Proposal programs \ncan be used as proposal distributions in importance sampling and \nMetropolis-Hastings samplers without sacrificing asymptotic consistency, and \ncan be optimized offline using inference compilation. Support for optimizing \nand using proposal programs is easily implemented in a sampling-based \nprobabilistic programming runtime. The paper illustrates the proposed technique \nwith a proposal program that combines RANSAC and neural networks to accelerate \ninference in a Bayesian linear regression with outliers model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb2333", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03612"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516165576, "author": "Mohnish Dubey, Debayan Banerjee, Debanjan Chaudhuri, Jens Lehmann", "title": "EARL: Joint Entity and Relation Linking for Question Answering over Knowledge Graphs. (arXiv:1801.03825v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1801.03825", "type": "text/html"}], "timestampUsec": "1515735584908025", "comments": [], "summary": {"content": "<p>In order to answer natural language questions over knowledge graphs, most \nprocessing pipelines involve entity and relation linking. Traditionally, entity \nlinking and relation linking has been performed either as dependent sequential \ntasks or independent parallel tasks. In this paper, we propose a framework \ncalled \"EARL\", which performs entity linking and relation linking as a joint \nsingle task. EARL uses a graph connection based solution to the problem. We \nmodel the linking task as an instance of the Generalised Travelling Salesman \nProblem (GTSP) and use GTSP approximate algorithm solutions. We later develop \nEARL which uses a pair-wise graph-distance based solution to the problem.The \nsystem determines the best semantic connection between all keywords of the \nquestion by referring to a knowledge graph. This is achieved by exploiting the \n\"connection density\" between entity candidates and relation candidates. The \n\"connection density\" based solution performs at par with the approximate GTSP \nsolution.We have empirically evaluated the framework on a dataset with 5000 \nquestions. Our system surpasses state-of-the-art scores for entity linking task \nby reporting an accuracy of 0.65 to 0.40 from the next best entity linker. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1516165575, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb2334", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03825"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ferdinando Fioretto, Enrico Pontelli, William Yeoh", "title": "Distributed Constraint Optimization Problems and Applications: A Survey. (arXiv:1602.06347v4 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1602.06347", "type": "text/html"}], "timestampUsec": "1515735584908024", "comments": [], "summary": {"content": "<p>The field of Multi-Agent System (MAS) is an active area of research within \nArtificial Intelligence, with an increasingly important impact in industrial \nand other real-world applications. Within a MAS, autonomous agents interact to \npursue personal interests and/or to achieve common objectives. Distributed \nConstraint Optimization Problems (DCOPs) have emerged as one of the prominent \nagent architectures to govern the agents' autonomous behavior, where both \nalgorithms and communication models are driven by the structure of the specific \nproblem. During the last decade, several extensions to the DCOP model have \nenabled them to support MAS in complex, real-time, and uncertain environments. \nThis survey aims at providing an overview of the DCOP model, giving a \nclassification of its multiple extensions and addressing both resolution \nmethods and applications that find a natural mapping within each class of \nDCOPs. The proposed classification suggests several future perspectives for \nDCOP extensions, and identifies challenges in the design of efficient \nresolution algorithms, possibly through the adaptation of strategies from \ndifferent areas. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb233a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1602.06347"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Virgile Landeiro, Aron Culotta", "title": "Controlling for Unobserved Confounds in Classification Using Correlational Constraints. (arXiv:1703.01671v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.01671", "type": "text/html"}], "timestampUsec": "1515735584908023", "comments": [], "summary": {"content": "<p>As statistical classifiers become integrated into real-world applications, it \nis important to consider not only their accuracy but also their robustness to \nchanges in the data distribution. In this paper, we consider the case where \nthere is an unobserved confounding variable $z$ that influences both the \nfeatures $\\mathbf{x}$ and the class variable $y$. When the influence of $z$ \nchanges from training to testing data, we find that the classifier accuracy can \ndegrade rapidly. In our approach, we assume that we can predict the value of \n$z$ at training time with some error. The prediction for $z$ is then fed to \nPearl's back-door adjustment to build our model. Because of the attenuation \nbias caused by measurement error in $z$, standard approaches to controlling for \n$z$ are ineffective. In response, we propose a method to properly control for \nthe influence of $z$ by first estimating its relationship with the class \nvariable $y$, then updating predictions for $z$ to match that estimated \nrelationship. By adjusting the influence of $z$, we show that we can build a \nmodel that exceeds competing baselines on accuracy as well as on robustness \nover a range of confounding relationships. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb2341", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.01671"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ivan Brugere, Tanya Y. Berger-Wolf", "title": "Network Model Selection Using Task-Focused Minimum Description Length. (arXiv:1710.05207v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.05207", "type": "text/html"}], "timestampUsec": "1515735584908022", "comments": [], "summary": {"content": "<p>Networks are fundamental models for data used in practically every \napplication domain. In most instances, several implicit or explicit choices \nabout the network definition impact the translation of underlying data to a \nnetwork representation, and the subsequent question(s) about the underlying \nsystem being represented. Users of downstream network data may not even be \naware of these choices or their impacts. We propose a task-focused network \nmodel selection methodology which addresses several key challenges. Our \napproach constructs network models from underlying data and uses minimum \ndescription length (MDL) criteria for selection. Our methodology measures \nefficiency, a general and comparable measure of the network's performance of a \nlocal (i.e. node-level) predictive task of interest. Selection on efficiency \nfavors parsimonious (e.g. sparse) models to avoid overfitting and can be \napplied across arbitrary tasks and representations. We show stability, \nsensitivity, and significance testing in our methodology. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb2347", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.05207"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mahardhika Pratama, Eric Dimla, Edwin Lughofer, Witold Pedrycz, Tegoeh Tjahjowidowo", "title": "Online Tool Condition Monitoring Based on Parsimonious Ensemble+. (arXiv:1711.01843v1 [cs.LG] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1711.01843", "type": "text/html"}], "timestampUsec": "1515735584908021", "comments": [], "summary": {"content": "<p>Accurate diagnosis of tool wear in metal turning process remains an open \nchallenge for both scientists and industrial practitioners because of \ninhomogeneities in workpiece material, nonstationary machining settings to suit \nproduction requirements, and nonlinear relations between measured variables and \ntool wear. Common methodologies for tool condition monitoring still rely on \nbatch approaches which cannot cope with a fast sampling rate of metal cutting \nprocess. Furthermore they require a retraining process to be completed from \nscratch when dealing with a new set of machining parameters. This paper \npresents an online tool condition monitoring approach based on Parsimonious \nEnsemble+, pENsemble+. The unique feature of pENsemble+ lies in its highly \nflexible principle where both ensemble structure and base-classifier structure \ncan automatically grow and shrink on the fly based on the characteristics of \ndata streams. Moreover, the online feature selection scenario is integrated to \nactively sample relevant input attributes. The paper presents advancement of a \nnewly developed ensemble learning algorithm, pENsemble+, where online active \nlearning scenario is incorporated to reduce operator labelling effort. The \nensemble merging scenario is proposed which allows reduction of ensemble \ncomplexity while retaining its diversity. Experimental studies utilising \nreal-world manufacturing data streams and comparisons with well known \nalgorithms were carried out. Furthermore, the efficacy of pENsemble was \nexamined using benchmark concept drift data streams. It has been found that \npENsemble+ incurs low structural complexity and results in a significant \nreduction of operator labelling effort. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb234a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.01843"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jon Kleinberg, Manish Raghavan", "title": "Selection Problems in the Presence of Implicit Bias. (arXiv:1801.03533v1 [cs.CY])", "alternate": [{"href": "http://arxiv.org/abs/1801.03533", "type": "text/html"}], "timestampUsec": "1515735584908019", "comments": [], "summary": {"content": "<p>Over the past two decades, the notion of implicit bias has come to serve as \nan important component in our understanding of discrimination in activities \nsuch as hiring, promotion, and school admissions. Research on implicit bias \nposits that when people evaluate others -- for example, in a hiring context -- \ntheir unconscious biases about membership in particular groups can have an \neffect on their decision-making, even when they have no deliberate intention to \ndiscriminate against members of these groups. A growing body of experimental \nwork has pointed to the effect that implicit bias can have in producing adverse \noutcomes. \n</p> \n<p>Here we propose a theoretical model for studying the effects of implicit bias \non selection decisions, and a way of analyzing possible procedural remedies for \nimplicit bias within this model. A canonical situation represented by our model \nis a hiring setting: a recruiting committee is trying to choose a set of \nfinalists to interview among the applicants for a job, evaluating these \napplicants based on their future potential, but their estimates of potential \nare skewed by implicit bias against members of one group. In this model, we \nshow that measures such as the Rooney Rule, a requirement that at least one of \nthe finalists be chosen from the affected group, can not only improve the \nrepresentation of this affected group, but also lead to higher payoffs in \nabsolute terms for the organization performing the recruiting. However, \nidentifying the conditions under which such measures can lead to improved \npayoffs involves subtle trade-offs between the extent of the bias and the \nunderlying distribution of applicant characteristics, leading to novel \ntheoretical questions about order statistics in the presence of probabilistic \nside information. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb2351", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03533"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Chris Cremer, Xuechen Li, David Duvenaud", "title": "Inference Suboptimality in Variational Autoencoders. (arXiv:1801.03558v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.03558", "type": "text/html"}], "timestampUsec": "1515735584908018", "comments": [], "summary": {"content": "<p>Amortized inference has led to efficient approximate inference for large \ndatasets. The quality of posterior inference is largely determined by two \nfactors: a) the ability of the variational distribution to model the true \nposterior and b) the capacity of the recognition network to generalize \ninference over all datapoints. We analyze approximate inference in variational \nautoencoders in terms of these factors. We find that suboptimal inference is \noften due to amortizing inference rather than the limited complexity of the \napproximating distribution. We show that this is due partly to the generator \nlearning to accommodate the choice of approximation. Furthermore, we show that \nthe parameters used to increase the expressiveness of the approximation play a \nrole in generalizing inference rather than simply improving the complexity of \nthe approximation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb2356", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03558"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Saeid Haghighatshoar, Mahdi Barzegar Khalilsarai, Giuseppe Caire", "title": "Multi-Band Covariance Interpolation with Applications in Massive MIMO. (arXiv:1801.03714v1 [cs.IT])", "alternate": [{"href": "http://arxiv.org/abs/1801.03714", "type": "text/html"}], "timestampUsec": "1515735584908017", "comments": [], "summary": {"content": "<p>In this paper, we study the problem of multi-band (frequency-variant) \ncovariance interpolation with a particular emphasis towards massive MIMO \napplications. In a massive MIMO system, the communication between each BS with \n$M \\gg 1$ antennas and each single-antenna user occurs through a collection of \nscatterers in the environment, where the channel vector of each user at BS \nantennas consists in a weighted linear combination of the array responses of \nthe scatterers, where each scatterer has its own angle of arrival (AoA) and \ncomplex channel gain. The array response at a given AoA depends on the \nwavelength of the incoming planar wave and is naturally frequency dependent. \nThis results in a frequency-dependent distortion where the second order \nstatistics, i.e., the covariance matrix, of the channel vectors varies with \nfrequency. In this paper, we show that although this effect is generally \nnegligible for a small number of antennas $M$, it results in a considerable \ndistortion of the covariance matrix and especially its dominant signal subspace \nin the massive MIMO regime where $M \\to \\infty$, and can generally incur a \nserious degradation of the performance especially in frequency division \nduplexing (FDD) massive MIMO systems where the uplink (UL) and the downlink \n(DL) communication occur over different frequency bands. We propose a novel \nUL-DL covariance interpolation technique that is able to recover the covariance \nmatrix in the DL from an estimate of the covariance matrix in the UL under a \nmild reciprocity condition on the angular power spread function (PSF) of the \nusers. We analyze the performance of our proposed scheme mathematically and \nprove its robustness under a sufficiently large spatial oversampling of the \narray. We also propose several simple off-the-shelf algorithms for UL-DL \ncovariance interpolation and evaluate their performance via numerical \nsimulations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb235a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03714"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Dirk A. Lorenz, Quoc Tran-Dinh", "title": "Non-stationary Douglas-Rachford and alternating direction method of multipliers: adaptive stepsizes and convergence. (arXiv:1801.03765v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1801.03765", "type": "text/html"}], "timestampUsec": "1515735584908016", "comments": [], "summary": {"content": "<p>We revisit the classical Douglas-Rachford (DR) method for finding a zero of \nthe sum of two maximal monotone operators. Since the practical performance of \nthe DR method crucially depends on the stepsizes, we aim at developing an \nadaptive stepsize rule. To that end, we take a closer look at a linear case of \nthe problem and use our findings to develop a stepsize strategy that eliminates \nthe need for stepsize tuning. We analyze a general non-stationary DR scheme and \nprove its convergence for a convergent sequence of stepsizes with summable \nincrements. This, in turn, proves the convergence of the method with the new \nadaptive stepsize rule. We also derive the related non-stationary alternating \ndirection method of multipliers (ADMM) from such a non-stationary DR method. We \nillustrate the efficiency of the proposed methods on several numerical \nexamples. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb235d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03765"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tak-Shing T. Chan, Yi-Hsuan Yang", "title": "Polar $n$-Complex and $n$-Bicomplex Singular Value Decomposition and Principal Component Pursuit. (arXiv:1801.03773v1 [eess.SP])", "alternate": [{"href": "http://arxiv.org/abs/1801.03773", "type": "text/html"}], "timestampUsec": "1515735584908015", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3292f0e18\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3292f0e18&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Informed by recent work on tensor singular value decomposition and circulant \nalgebra matrices, this paper presents a new theoretical bridge that unifies the \nhypercomplex and tensor-based approaches to singular value decomposition and \nrobust principal component analysis. We begin our work by extending the \nprincipal component pursuit to Olariu's polar $n$-complex numbers as well as \ntheir bicomplex counterparts. In so doing, we have derived the polar \n$n$-complex and $n$-bicomplex proximity operators for both the $\\ell_1$- and \ntrace-norm regularizers, which can be used by proximal optimization methods \nsuch as the alternating direction method of multipliers. Experimental results \non two sets of audio data show that our algebraically-informed formulation \noutperforms tensor robust principal component analysis. We conclude with the \nmessage that an informed definition of the trace norm can bridge the gap \nbetween the hypercomplex and tensor-based approaches. Our approach can be seen \nas a general methodology for generating other principal component pursuit \nalgorithms with proper algebraic structures. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb2360", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03773"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tak-Shing T. Chan, Yi-Hsuan Yang", "title": "Informed Group-Sparse Representation for Singing Voice Separation. (arXiv:1801.03815v1 [eess.AS])", "alternate": [{"href": "http://arxiv.org/abs/1801.03815", "type": "text/html"}], "timestampUsec": "1515735584908014", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32935a8d1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32935a8d1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Singing voice separation attempts to separate the vocal and instrumental \nparts of a music recording, which is a fundamental problem in music information \nretrieval. Recent work on singing voice separation has shown that the low-rank \nrepresentation and informed separation approaches are both able to improve \nseparation quality. However, low-rank optimizations are computationally \ninefficient due to the use of singular value decompositions. Therefore, in this \npaper, we propose a new linear-time algorithm called informed group-sparse \nrepresentation, and use it to separate the vocals from music using pitch \nannotations as side information. Experimental results on the iKala dataset \nconfirm the efficacy of our approach, suggesting that the music accompaniment \nfollows a group-sparse structure given a pre-trained instrumental dictionary. \nWe also show how our work can be easily extended to accommodate multiple \ndictionaries using the DSD100 dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb2365", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03815"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tak-Shing T. Chan, Yi-Hsuan Yang", "title": "Complex and Quaternionic Principal Component Pursuit and Its Application to Audio Separation. (arXiv:1801.03816v1 [eess.SP])", "alternate": [{"href": "http://arxiv.org/abs/1801.03816", "type": "text/html"}], "timestampUsec": "1515735584908013", "comments": [], "summary": {"content": "<p>Recently, the principal component pursuit has received increasing attention \nin signal processing research ranging from source separation to video \nsurveillance. So far, all existing formulations are real-valued and lack the \nconcept of phase, which is inherent in inputs such as complex spectrograms or \ncolor images. Thus, in this letter, we extend principal component pursuit to \nthe complex and quaternionic cases to account for the missing phase \ninformation. Specifically, we present both complex and quaternionic proximity \noperators for the $\\ell_1$- and trace-norm regularizers. These operators can be \nused in conjunction with proximal minimization methods such as the inexact \naugmented Lagrange multiplier algorithm. The new algorithms are then applied to \nthe singing voice separation problem, which aims to separate the singing voice \nfrom the instrumental accompaniment. Results on the iKala and MSD100 datasets \nconfirmed the usefulness of phase information in principal component pursuit. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb236d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03816"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Christopher K. I. Williams, Charlie Nash", "title": "Autoencoders and Probabilistic Inference with Missing Data: An Exact Solution for The Factor Analysis Case. (arXiv:1801.03851v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.03851", "type": "text/html"}], "timestampUsec": "1515735584908012", "comments": [], "summary": {"content": "<p>Latent variable models can be used to probabilistically \"fill-in\" missing \ndata entries. The variational autoencoder architecture (Kingma and Welling, \n2014; Rezende et al., 2014) includes a \"recognition\" or \"encoder\" network that \ninfers the latent variables given the data variables. However, it is not clear \nhow to handle missing data variables in this network. We show how to calculate \nexactly the latent posterior distribution for the factor analysis (FA) model in \nthe presence of missing data, and note that this solution exhibits a \nnon-trivial dependence on the pattern of missingness. Experiments compare the \neffectiveness of various approaches to filling in the missing data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb2373", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03851"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yizhe Zhang, Xiangyu Wang, Changyou Chen, Ricardo Henao, Kai Fan, Lawrence Carin", "title": "Towards Unifying Hamiltonian Monte Carlo and Slice Sampling. (arXiv:1602.07800v5 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1602.07800", "type": "text/html"}], "timestampUsec": "1515735584908011", "comments": [], "summary": {"content": "<p>We unify slice sampling and Hamiltonian Monte Carlo (HMC) sampling, \ndemonstrating their connection via the Hamiltonian-Jacobi equation from \nHamiltonian mechanics. This insight enables extension of HMC and slice sampling \nto a broader family of samplers, called Monomial Gamma Samplers (MGS). We \nprovide a theoretical analysis of the mixing performance of such samplers, \nproving that in the limit of a single parameter, the MGS draws decorrelated \nsamples from the desired target distribution. We further show that as this \nparameter tends toward this limit, performance gains are achieved at a cost of \nincreasing numerical difficulty and some practical convergence issues. Our \ntheoretical results are validated with synthetic data and real-world \napplications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb2376", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1602.07800"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Faris B. Mismar, Brian L. Evans", "title": "Improving Downlink Coordinated Multipoint Performance in Heterogeneous Networks. (arXiv:1608.08306v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1608.08306", "type": "text/html"}], "timestampUsec": "1515735584908010", "comments": [], "summary": {"content": "<p>We propose a novel method for practical Joint Processing downlink coordinated \nmultipoint (DL CoMP) implementation in LTE/LTE-A systems using supervised \nmachine learning. DL CoMP has not been thoroughly studied in previous work \nalthough cluster formation and interference mitigation have been studied \nextensively. In this paper, we attempt to improve the cell edge data rate \nserved by a heterogeneous network cluster by means of dynamically changing the \nDL SINR threshold at which the DL CoMP feature is triggered. We do so by using \na support vector machine (SVM) classifier. The simulation results show a cell \nedge user throughput improvement of 33.3% for pico cells and more than \nfour-fold improvement in user throughput in the cluster. This has resulted from \na reduction in the downlink block error rate (DL BLER) and an improvement in \nthe spectral efficiency due to the informed triggering of the multiple radio \nstreams as part of DL CoMP. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb2380", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1608.08306"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "George Papamakarios, Theo Pavlakou, Iain Murray", "title": "Masked Autoregressive Flow for Density Estimation. (arXiv:1705.07057v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.07057", "type": "text/html"}], "timestampUsec": "1515735584908009", "comments": [], "summary": {"content": "<p>Autoregressive models are among the best performing neural density \nestimators. We describe an approach for increasing the flexibility of an \nautoregressive model, based on modelling the random numbers that the model uses \ninternally when generating data. By constructing a stack of autoregressive \nmodels, each modelling the random numbers of the next model in the stack, we \nobtain a type of normalizing flow suitable for density estimation, which we \ncall Masked Autoregressive Flow. This type of flow is closely related to \nInverse Autoregressive Flow and is a generalization of Real NVP. Masked \nAutoregressive Flow achieves state-of-the-art performance in a range of \ngeneral-purpose density estimation tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb2385", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.07057"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Anders Bredahl Kock, Martin Thyrsgaard", "title": "Optimal sequential treatment allocation. (arXiv:1705.09952v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.09952", "type": "text/html"}], "timestampUsec": "1515735584908008", "comments": [], "summary": {"content": "<p>In a treatment allocation problem the individuals to be treated often arrive \ngradually. Initially, when the first treatments are made, little is known about \nthe effect of the treatments but as more treatments are assigned the policy \nmaker learns about their effects by observing outcomes. Thus, there is a \ntradeoff between exploring the available treatments to learn about their merits \nand exploiting the best treatment, i.e. administering it as often as possible, \nin order to maximise the cumulative welfare of all the assignments made. \nFurthermore, a policy maker may not only be interested in the expected effect \nof the treatment but also its riskiness. Thus, we allow the welfare function to \ndepend on the first and second moments of the distribution of treatment \noutcomes. We propose a dynamic treatment policy which attains the minimax \noptimal regret relative to the unknown best treatment in this dynamic setting. \nWe allow for the data to arrive in batches as, say, unemployment programs only \nstart once a month or blood samples are only send to the laboratory for \ninvestigation in batches. Furthermore, we show that the minimax optimality does \nnot come at the price of overly aggressive experimentation as we provide upper \nbounds on the expected number of times any suboptimal treatment is assigned. We \nalso consider the case where the outcome of a treatment is only observed with \ndelay as it may take time for the treatment to work. Thus, a doctor faces a \ntradeoff between getting imprecise information quickly by making the \nmeasurement soon after the treatment is given or getting precise information \nlater at the expense of less information for the individuals who are treated in \nthe meantime. Finally, using Danish register data, we show how our treatment \npolicy can be used to assign unemployed to active labor market policy programs \nin order to maximise the probability of ending the unemployment spell. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb238c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.09952"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yizhe Zhang, Changyou Chen, Zhe Gan, Ricardo Henao, Lawrence Carin", "title": "Stochastic Gradient Monomial Gamma Sampler. (arXiv:1706.01498v2 [stat.ML] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1706.01498", "type": "text/html"}], "timestampUsec": "1515735584908006", "comments": [], "summary": {"content": "<p>Recent advances in stochastic gradient techniques have made it possible to \nestimate posterior distributions from large datasets via Markov Chain Monte \nCarlo (MCMC). However, when the target posterior is multimodal, mixing \nperformance is often poor. This results in inadequate exploration of the \nposterior distribution. A framework is proposed to improve the sampling \nefficiency of stochastic gradient MCMC, based on Hamiltonian Monte Carlo. A \ngeneralized kinetic function is leveraged, delivering superior stationary \nmixing, especially for multimodal distributions. Techniques are also discussed \nto overcome the practical issues introduced by this generalization. It is shown \nthat the proposed approach is better at exploring complex multimodal posterior \ndistributions, as demonstrated on multiple applications and in comparison with \nother stochastic gradient MCMC methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb2390", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.01498"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Chen-Yu Wei, Haipeng Luo", "title": "More Adaptive Algorithms for Adversarial Bandits. (arXiv:1801.03265v1 [cs.LG] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1801.03265", "type": "text/html"}], "timestampUsec": "1515735584908005", "comments": [], "summary": {"content": "<p>We develop a novel and generic algorithm for the adversarial multi-armed \nbandit problem (or more generally the combinatorial semi-bandit problem). When \ninstantiated differently, our algorithm achieves various new data-dependent \nregret bounds improving previous works. Examples include: 1) a regret bound \ndepending on the variance of only the best arm; 2) a regret bound depending on \nthe first-order path-length of only the best arm; 3) a regret bound depending \non the sum of first-order path-lengths of all arms as well as an important \nnegative term, which together lead to faster convergence rates for some normal \nform games with partial feedback; 4) a regret bound that simultaneously implies \nsmall regret when the best arm has small loss and logarithmic regret when there \nexists an arm whose expected loss is always smaller than those of others by a \nfixed gap (e.g. the classic i.i.d. setting). In some cases, such as the last \ntwo results, our algorithm is completely parameter-free. \n</p> \n<p>The main idea of our algorithm is to apply the optimism and adaptivity \ntechniques to the well-known Online Mirror Descent framework with a special \nlog-barrier regularizer. The challenges are to come up with appropriate \noptimistic predictions and correction terms in this framework. Some of our \nresults also crucially rely on using a sophisticated increasing learning rate \nschedule. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515735584908", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359bb2393", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03265"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Fernando Fernandes Neto", "title": "Generative Models for Stochastic Processes Using Convolutional Neural Networks. (arXiv:1801.03523v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.03523", "type": "text/html"}], "timestampUsec": "1515734114179905", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32935ab5e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32935ab5e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The present paper aims to demonstrate the usage of Convolutional Neural \nNetworks as a generative model for stochastic processes, enabling researchers \nfrom a wide range of fields (such as quantitative finance and physics) to \ndevelop a general tool for forecasts and simulations without the need to \nidentify/assume a specific system structure or estimate its parameters. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515734114180", "annotations": [], "published": 1515734114, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359b85c5a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03523"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Vincent Dumoulin, Francesco Visin", "title": "A guide to convolution arithmetic for deep learning. (arXiv:1603.07285v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1603.07285", "type": "text/html"}], "timestampUsec": "1515734114179904", "comments": [], "summary": {"content": "<p>We introduce a guide to help deep learning practitioners understand and \nmanipulate convolutional neural network architectures. The guide clarifies the \nrelationship between various properties (input shape, kernel shape, zero \npadding, strides and output shape) of convolutional, pooling and transposed \nconvolutional layers, as well as the relationship between convolutional and \ntransposed convolutional layers. Relationships are derived for various cases, \nand are illustrated in order to make them intuitive. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515734114180", "annotations": [], "published": 1515734114, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359b85c6c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1603.07285"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Daniel A. Abolafia, Mohammad Norouzi, Quoc V. Le", "title": "Neural Program Synthesis with Priority Queue Training. (arXiv:1801.03526v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.03526", "type": "text/html"}], "timestampUsec": "1515734114179903", "comments": [], "summary": {"content": "<p>We consider the task of program synthesis in the presence of a reward \nfunction over the output of programs, where the goal is to find programs with \nmaximal rewards. We employ an iterative optimization scheme, where we train an \nRNN on a dataset of K best programs from a priority queue of the generated \nprograms so far. Then, we synthesize new programs and add them to the priority \nqueue by sampling from the RNN. We benchmark our algorithm, called priority \nqueue training (or PQT), against genetic algorithm and reinforcement learning \nbaselines on a simple but expressive Turing complete programming language \ncalled BF. Our experimental results show that our simple PQT algorithm \nsignificantly outperforms the baselines. By adding a program length penalty to \nthe reward function, we are able to synthesize short, human readable programs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515734114180", "annotations": [], "published": 1515734114, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359b85c75", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03526"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ashwin Ram, Rohit Prasad, Chandra Khatri, Anu Venkatesh, Raefer Gabriel, Qing Liu, Jeff Nunn, Behnam Hedayatnia, Ming Cheng, Ashish Nagar, Eric King, Kate Bland, Amanda Wartick, Yi Pan, Han Song, Sk Jayadevan, Gene Hwang, Art Pettigrue", "title": "Conversational AI: The Science Behind the Alexa Prize. (arXiv:1801.03604v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.03604", "type": "text/html"}], "timestampUsec": "1515734114179902", "comments": [], "summary": {"content": "<p>Conversational agents are exploding in popularity. However, much work remains \nin the area of social conversation as well as free-form conversation over a \nbroad range of domains and topics. To advance the state of the art in \nconversational AI, Amazon launched the Alexa Prize, a 2.5-million-dollar \nuniversity competition where sixteen selected university teams were challenged \nto build conversational agents, known as socialbots, to converse coherently and \nengagingly with humans on popular topics such as Sports, Politics, \nEntertainment, Fashion and Technology for 20 minutes. The Alexa Prize offers \nthe academic community a unique opportunity to perform research with a live \nsystem used by millions of users. The competition provided university teams \nwith real user conversational data at scale, along with the user-provided \nratings and feedback augmented with annotations by the Alexa team. This enabled \nteams to effectively iterate and make improvements throughout the competition \nwhile being evaluated in real-time through live user interactions. To build \ntheir socialbots, university teams combined state-of-the-art techniques with \nnovel strategies in the areas of Natural Language Understanding, Context \nModeling, Dialog Management, Response Generation, and Knowledge Acquisition. To \nsupport the efforts of participating teams, the Alexa Prize team made \nsignificant scientific and engineering investments to build and improve \nConversational Speech Recognition, Topic Tracking, Dialog Evaluation, Voice \nUser Experience, and tools for traffic management and scalability. This paper \noutlines the advances created by the university teams as well as the Alexa \nPrize team to achieve the common goal of solving the problem of Conversational \nAI. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515734114180", "annotations": [], "published": 1515734114, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359b85c7e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03604"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Fenfei Guo, Angeliki Metallinou, Chandra Khatri, Anirudh Raju, Anu Venkatesh, Ashwin Ram", "title": "Topic-based Evaluation for Conversational Bots. (arXiv:1801.03622v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1801.03622", "type": "text/html"}], "timestampUsec": "1515734114179901", "comments": [], "summary": {"content": "<p>Dialog evaluation is a challenging problem, especially for non task-oriented \ndialogs where conversational success is not well-defined. We propose to \nevaluate dialog quality using topic-based metrics that describe the ability of \na conversational bot to sustain coherent and engaging conversations on a topic, \nand the diversity of topics that a bot can handle. To detect conversation \ntopics per utterance, we adopt Deep Average Networks (DAN) and train a topic \nclassifier on a variety of question and query data categorized into multiple \ntopics. We propose a novel extension to DAN by adding a topic-word attention \ntable that allows the system to jointly capture topic keywords in an utterance \nand perform topic classification. We compare our proposed topic based metrics \nwith the ratings provided by users and show that our metrics both correlate \nwith and complement human judgment. Our analysis is performed on tens of \nthousands of real human-bot dialogs from the Alexa Prize competition and \nhighlights user expectations for conversational bots. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515734114180", "annotations": [], "published": 1515734114, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359b85c84", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03622"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anu Venkatesh, Chandra Khatri, Ashwin Ram, Fenfei Guo, Raefer Gabriel, Ashish Nagar, Rohit Prasad, Ming Cheng, Behnam Hedayatnia, Angeliki Metallinou, Rahul Goel, Shaohua Yang, Anirudh Raju", "title": "On Evaluating and Comparing Conversational Agents. (arXiv:1801.03625v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1801.03625", "type": "text/html"}], "timestampUsec": "1515734114179900", "comments": [], "summary": {"content": "<p>Conversational agents are exploding in popularity. However, much work remains \nin the area of non goal-oriented conversations, despite significant growth in \nresearch interest over recent years. To advance the state of the art in \nconversational AI, Amazon launched the Alexa Prize, a 2.5-million dollar \nuniversity competition where sixteen selected university teams built \nconversational agents to deliver the best social conversational experience. \nAlexa Prize provided the academic community with the unique opportunity to \nperform research with a live system used by millions of users. The subjectivity \nassociated with evaluating conversations is key element underlying the \nchallenge of building non-goal oriented dialogue systems. In this paper, we \npropose a comprehensive evaluation strategy with multiple metrics designed to \nreduce subjectivity by selecting metrics which correlate well with human \njudgement. The proposed metrics provide granular analysis of the conversational \nagents, which is not captured in human ratings. We show that these metrics can \nbe used as a reasonable proxy for human judgment. We provide a mechanism to \nunify the metrics for selecting the top performing agents, which has also been \napplied throughout the Alexa Prize competition. To our knowledge, to date it is \nthe largest setting for evaluating agents with millions of conversations and \nhundreds of thousands of ratings from users. We believe that this work is a \nstep towards an automatic evaluation process for conversational AIs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515734114180", "annotations": [], "published": 1515734114, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359b85c8a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03625"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Stuart Armstrong", "title": "Counterfactual equivalence for POMDPs, and underlying deterministic environments. (arXiv:1801.03737v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.03737", "type": "text/html"}], "timestampUsec": "1515734114179899", "comments": [], "summary": {"content": "<p>Partially Observable Markov Decision Processes (POMDPs) are rich environments \noften used in machine learning. But the issue of information and causal \nstructures in POMDPs has been relatively little studied. This paper presents \nthe concepts of equivalent and counterfactually equivalent POMDPs, where agents \ncannot distinguish which environment they are in though any observations and \nactions. It shows that any POMDP is counterfactually equivalent, for any finite \nnumber of turns, to a deterministic POMDP with all uncertainty concentrated \ninto the initial state. This allows a better understanding of POMDP \nuncertainty, information, and learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515734114180", "annotations": [], "published": 1515734114, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359b85c8d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03737"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mahardhika Pratama, Witold Pedrycz, Edwin Lughofer", "title": "Evolving Ensemble Fuzzy Classifier. (arXiv:1705.06460v1 [cs.LG] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1705.06460", "type": "text/html"}], "timestampUsec": "1515734114179897", "comments": [], "summary": {"content": "<p>The concept of ensemble learning offers a promising avenue in learning from \ndata streams under complex environments because it addresses the bias and \nvariance dilemma better than its single model counterpart and features a \nreconfigurable structure, which is well suited to the given context. While \nvarious extensions of ensemble learning for mining non-stationary data streams \ncan be found in the literature, most of them are crafted under a static base \nclassifier and revisits preceding samples in the sliding window for a \nretraining step. This feature causes computationally prohibitive complexity and \nis not flexible enough to cope with rapidly changing environments. Their \ncomplexities are often demanding because it involves a large collection of \noffline classifiers due to the absence of structural complexities reduction \nmechanisms and lack of an online feature selection mechanism. A novel evolving \nensemble classifier, namely Parsimonious Ensemble pENsemble, is proposed in \nthis paper. pENsemble differs from existing architectures in the fact that it \nis built upon an evolving classifier from data streams, termed Parsimonious \nClassifier pClass. pENsemble is equipped by an ensemble pruning mechanism, \nwhich estimates a localized generalization error of a base classifier. A \ndynamic online feature selection scenario is integrated into the pENsemble. \nThis method allows for dynamic selection and deselection of input features on \nthe fly. pENsemble adopts a dynamic ensemble structure to output a final \nclassification decision where it features a novel drift detection scenario to \ngrow the ensemble structure. The efficacy of the pENsemble has been numerically \ndemonstrated through rigorous numerical studies with dynamic and evolving data \nstreams where it delivers the most encouraging performance in attaining a \ntradeoff between accuracy and complexity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515734114180", "annotations": [], "published": 1515734114, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359b85c98", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.06460"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Boris Hanin", "title": "Which Neural Net Architectures Give Rise To Exploding and Vanishing Gradients?. (arXiv:1801.03744v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.03744", "type": "text/html"}], "timestampUsec": "1515734114179896", "comments": [], "summary": {"content": "<p>We give a rigorous analysis of the statistical behavior of gradients in \nrandomly initialized feed-forward networks with ReLU activations. Our results \nshow that a fully connected depth $d$ ReLU net with hidden layer widths $n_j$ \nwill have exploding and vanishing gradients if and only if $\\sum_{j=1}^{d-1} \n1/n_j$ is large. The point of view of this article is that whether a given \nneural net will have exploding/vanishing gradients is a function mainly of the \narchitecture of the net, and hence can be tested at initialization. Our results \nimply that a fully connected network that produces manageable gradients at \ninitialization must have many hidden layers that are about as wide as the \nnetwork is deep. This work is related to the mean field theory approach to \nrandom neural nets. From this point of view, we give a rigorous computation of \nthe $1/n_j$ corrections to the propagation of gradients at the so-called edge \nof chaos. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515734114180", "annotations": [], "published": 1515734114, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359b85ca2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03744"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "R&#xe9;mi Leblond, Fabian Pederegosa, Simon Lacoste-Julien", "title": "Improved asynchronous parallel optimization analysis for stochastic incremental methods. (arXiv:1801.03749v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1801.03749", "type": "text/html"}], "timestampUsec": "1515734114179895", "comments": [], "summary": {"content": "<p>As datasets continue to increase in size and multi-core computer \narchitectures are developed, asynchronous parallel optimization algorithms \nbecome more and more essential to the field of Machine Learning. Unfortunately, \nconducting the theoretical analysis asynchronous methods is difficult, notably \ndue to the introduction of delay and inconsistency in inherently sequential \nalgorithms. Handling these issues often requires resorting to simplifying but \nunrealistic assumptions. Through a novel perspective, we revisit and clarify a \nsubtle but important technical issue present in a large fraction of the recent \nconvergence rate proofs for asynchronous parallel optimization algorithms, and \npropose a simplification of the recently introduced \"perturbed iterate\" \nframework that resolves it. We demonstrate the usefulness of our new framework \nby analyzing three distinct asynchronous parallel incremental optimization \nalgorithms: Hogwild (asynchronous SGD), KROMAGNON (asynchronous SVRG) and \nASAGA, a novel asynchronous parallel version of the incremental gradient \nalgorithm SAGA that enjoys fast linear convergence rates. We are able to both \nremove problematic assumptions and obtain better theoretical results. Notably, \nwe prove that ASAGA and KROMAGNON can obtain a theoretical linear speedup on \nmulti-core systems even without sparsity assumptions. We present results of an \nimplementation on a 40-core architecture illustrating the practical speedups as \nwell as the hardware overhead. Finally, we investigate the overlap constant, an \nill-understood but central quantity for the theoretical analysis of \nasynchronous parallel algorithms. We find that it encompasses much more \ncomplexity than suggested in previous work, and often is order-of-magnitude \nbigger than traditionally thought. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515734114180", "annotations": [], "published": 1515734114, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000359b85cbf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03749"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pierrick Bruneau, Philippe Pinheiro, Yoann Didry", "title": "Data-driven forecasting of solar irradiance. (arXiv:1801.03373v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.03373", "type": "text/html"}], "timestampUsec": "1515649356890255", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32935ae62\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32935ae62&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper describes a flexible approach to short term prediction of \nmeteorological variables. In particular, we focus on the prediction of the \nsolar irradiance one hour ahead, a task that has high practical value when \noptimizing solar energy resources. As D\\'efi EGC 2018 provides us with time \nseries data for multiple sensors (e.g. solar irradiance, temperature, \nhygrometry), recorded every minute for two years and 5 geographical sites from \nLa R\\'eunion island, we test the value of using recently observed data as input \nfor prediction models, as well as the performance of models across sites. After \ndescribing our data cleaning and normalization process, we combine a variable \nselection step based on AutoRegressive Integrated Moving Average (ARIMA) \nmodels, to using general purpose regression techniques such as neural networks \nand regression trees. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f7323b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03373"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Chen Wang, Suzhen Wang, Fuyan Shi, Zaixiang Wang", "title": "Robust Propensity Score Computation Method based on Machine Learning with Label-corrupted Data. (arXiv:1801.03132v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1801.03132", "type": "text/html"}], "timestampUsec": "1515649356890253", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3293acfe1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3293acfe1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In biostatistics, propensity score is a common approach to analyze the \nimbalance of covariate and process confounding covariates to eliminate \ndifferences between groups. While there are an abundant amount of methods to \ncompute propensity score, a common issue of them is the corrupted labels in the \ndataset. For example, the data collected from the patients could contain \nsamples that are treated mistakenly, and the computing methods could \nincorporate them as a misleading information. In this paper, we propose a \nMachine Learning-based method to handle the problem. Specifically, we utilize \nthe fact that the majority of sample should be labeled with the correct \ninstance and design an approach to first cluster the data with spectral \nclustering and then sample a new dataset with a distribution processed from the \nclustering results. The propensity score is computed by Xgboost, and a \nmathematical justification of our method is provided in this paper. The \nexperimental results illustrate that xgboost propensity scores computing with \nthe data processed by our method could outperform the same method with original \ndata, and the advantages of our method increases as we add some artificial \ncorruptions to the dataset. Meanwhile, the implementation of xgboost to compute \npropensity score for multiple treatments is also a pioneering work in the area. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f73244", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03132"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ben Parr", "title": "Deep In-GPU Experience Replay. (arXiv:1801.03138v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.03138", "type": "text/html"}], "timestampUsec": "1515649356890252", "comments": [], "summary": {"content": "<p>Experience replay allows a reinforcement learning agent to train on samples \nfrom a large amount of the most recent experiences. A simple in-RAM experience \nreplay stores these most recent experiences in a list in RAM, and then copies \nsampled batches to the GPU for training. I moved this list to the GPU, thus \ncreating an in-GPU experience replay, and a training step that no longer has \ninputs copied from the CPU. I trained an agent to play Super Smash Bros. Melee, \nusing internal game memory values as inputs and outputting controller button \npresses. A single state in Melee contains 27 floats, so the full experience \nreplay fits on a single GPU. For a batch size of 128, the in-GPU experience \nreplay trained twice as fast as the in-RAM experience replay. As far as I know, \nthis is the first in-GPU implementation of experience replay. Finally, I note a \nfew ideas for fitting the experience replay inside the GPU when the environment \nstate requires more memory. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f7324a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03138"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mathew Monfort, Bolei Zhou, Sarah Adel Bargal, Alex Andonian, Tom Yan, Kandan Ramakrishnan, Lisa Brown, Quanfu Fan, Dan Gutfruend, Carl Vondrick, Aude Oliva", "title": "Moments in Time Dataset: one million videos for event understanding. (arXiv:1801.03150v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.03150", "type": "text/html"}], "timestampUsec": "1515649356890251", "comments": [], "summary": {"content": "<p>We present the Moments in Time Dataset, a large-scale human-annotated \ncollection of one million short videos corresponding to dynamic events \nunfolding within three seconds. Modeling the spatial-audio-temporal dynamics \neven for actions occurring in 3 second videos poses many challenges: meaningful \nevents do not include only people, but also objects, animals, and natural \nphenomena; visual and auditory events can be symmetrical or not in time \n(\"opening\" means \"closing\" in reverse order), and transient or sustained. We \ndescribe the annotation process of our dataset (each video is tagged with one \naction or activity label among 339 different classes), analyze its scale and \ndiversity in comparison to other large-scale video datasets for action \nrecognition, and report results of several baseline models addressing \nseparately and jointly three modalities: spatial, temporal and auditory. The \nMoments in Time dataset designed to have a large coverage and diversity of \nevents in both visual and auditory modalities, can serve as a new challenge to \ndevelop models that scale to the level of complexity and abstract reasoning \nthat a human processes on a daily basis. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f7324d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03150"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Martin Mose Bentzen, Felix Lindner", "title": "A Formalization of Kant's Second Formulation of the Categorical Imperative. (arXiv:1801.03160v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.03160", "type": "text/html"}], "timestampUsec": "1515649356890250", "comments": [], "summary": {"content": "<p>We present a formalization and computational implementation of the second \nformulation of Kant's categorical imperative. This ethical principle requires \nan agent to never treat someone merely as a means but always also as an end. \nHere we interpret this principle in terms of how persons are causally affected \nby actions. We introduce Kantian causal agency models in which moral patients, \nactions, goals, and causal influence are represented, and we show how to \nformalize several readings of Kant's categorical imperative that correspond to \nKant's concept of strict and wide duties towards oneself and others. Stricter \nversions handle cases where an action directly causally affects oneself or \nothers, whereas the wide version maximizes the number of persons being treated \nas an end. We discuss limitations of our formalization by pointing to one of \nKant's cases that the machinery cannot handle in a satisfying way. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f73256", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03160"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Justin Gottschlich", "title": "Paranom: A Parallel Anomaly Dataset Generator. (arXiv:1801.03164v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.03164", "type": "text/html"}], "timestampUsec": "1515649356890249", "comments": [], "summary": {"content": "<p>In this paper, we present Paranom, a parallel anomaly dataset generator. We \ndiscuss its design and provide brief experimental results demonstrating its \nusefulness in improving the classification correctness of LSTM-AD, a \nstate-of-the-art anomaly detection model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f73262", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03164"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516712761, "author": "Tae Jun Lee, Justin Gottschlich, Nesime Tatbul, Eric Metcalf, Stan Zdonik", "title": "Greenhouse: A Zero-Positive Machine Learning System for Time-Series Anomaly Detection. (arXiv:1801.03168v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1801.03168", "type": "text/html"}], "timestampUsec": "1515649356890248", "comments": [], "summary": {"content": "<p>This short paper describes our ongoing research on Greenhouse - a \nzero-positive machine learning system for time-series anomaly detection. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1516712761, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f73266", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03168"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mohammadreza Esfandiari, Senjuti Basu Roy, Sihem Amer-Yahia", "title": "Eliciting Worker Preference for Task Completion. (arXiv:1801.03233v1 [cs.DB])", "alternate": [{"href": "http://arxiv.org/abs/1801.03233", "type": "text/html"}], "timestampUsec": "1515649356890247", "comments": [], "summary": {"content": "<p>Current crowdsourcing platforms provide little support for worker feedback. \nWorkers are sometimes invited to post free text describing their experience and \npreferences in completing tasks. They can also use forums such as Turker \nNation1 to exchange preferences on tasks and requesters. In fact, crowdsourcing \nplatforms rely heavily on observing workers and inferring their preferences \nimplicitly. In this work, we believe that asking workers to indicate their \npreferences explicitly improve their experience in task completion and hence, \nthe quality of their contributions. Explicit elicitation can indeed help to \nbuild more accurate worker models for task completion that captures the \nevolving nature of worker preferences. We design a worker model whose accuracy \nis improved iteratively by requesting preferences for task factors such as \nrequired skills, task payment, and task relevance. We propose a generic \nframework, develop efficient solutions in realistic scenarios, and run \nextensive experiments that show the benefit of explicit preference elicitation \nover implicit ones with statistical significance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f7326b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03233"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "M. Sadegh Riazi, Christian Weinert, Oleksandr Tkachenko, Ebrahim M. Songhori, Thomas Schneider, Farinaz Koushanfar", "title": "Chameleon: A Hybrid Secure Computation Framework for Machine Learning Applications. (arXiv:1801.03239v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1801.03239", "type": "text/html"}], "timestampUsec": "1515649356890246", "comments": [], "summary": {"content": "<p>We present Chameleon, a novel hybrid (mixed-protocol) framework for secure \nfunction evaluation (SFE) which enables two parties to jointly compute a \nfunction without disclosing their private inputs. Chameleon combines the best \naspects of generic SFE protocols with the ones that are based upon additive \nsecret sharing. In particular, the framework performs linear operations in the \nring $\\mathbb{Z}_{2^l}$ using additively secret shared values and nonlinear \noperations using Yao's Garbled Circuits or the Goldreich-Micali-Wigderson \nprotocol. Chameleon departs from the common assumption of additive or linear \nsecret sharing models where three or more parties need to communicate in the \nonline phase: the framework allows two parties with private inputs to \ncommunicate in the online phase under the assumption of a third node generating \ncorrelated randomness in an offline phase. Almost all of the heavy \ncryptographic operations are precomputed in an offline phase which \nsubstantially reduces the communication overhead. Chameleon is both scalable \nand significantly more efficient than the ABY framework (NDSS'15) it is based \non. Our framework supports signed fixed-point numbers. In particular, \nChameleon's vector dot product of signed fixed-point numbers improves the \nefficiency of mining and classification of encrypted data for algorithms based \nupon heavy matrix multiplications. Our evaluation of Chameleon on a 5 layer \nconvolutional deep neural network shows 133x and 4.2x faster executions than \nMicrosoft CryptoNets (ICML'16) and MiniONN (CCS'17), respectively. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f7326f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03239"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kamil Ciosek, Shimon Whiteson", "title": "Expected Policy Gradients for Reinforcement Learning. (arXiv:1801.03326v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.03326", "type": "text/html"}], "timestampUsec": "1515649356890245", "comments": [], "summary": {"content": "<p>We propose expected policy gradients (EPG), which unify stochastic policy \ngradients (SPG) and deterministic policy gradients (DPG) for reinforcement \nlearning. Inspired by expected sarsa, EPG integrates (or sums) across actions \nwhen estimating the gradient, instead of relying only on the action in the \nsampled trajectory. For continuous action spaces, we first derive a practical \nresult for Gaussian policies and quadric critics and then extend it to an \nanalytical method for the universal case, covering a broad class of actors and \ncritics, including Gaussian, exponential families, and reparameterised policies \nwith bounded support. For Gaussian policies, we show that it is optimal to \nexplore using covariance proportional to the matrix exponential of the scaled \nHessian of the critic with respect to the actions. EPG also provides a general \nframework for reasoning about policy gradient methods, which we use to \nestablish a new general policy gradient theorem, of which the stochastic and \ndeterministic policy gradient theorems are special cases. Furthermore, we prove \nthat EPG reduces the variance of the gradient estimates without requiring \ndeterministic policies and with little computational overhead. Finally, we show \nthat EPG outperforms existing approaches on six challenging domains involving \nthe simulated control of physical systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f73277", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03326"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Craig Innes, Alex Lascarides, Stefano V Albrecht, Subramanian Ramamoorthy, Benjamin Rosman", "title": "Reasoning about Unforeseen Possibilities During Policy Learning. (arXiv:1801.03331v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.03331", "type": "text/html"}], "timestampUsec": "1515649356890244", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3293ad43b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3293ad43b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Methods for learning optimal policies in autonomous agents often assume that \nthe way the domain is conceptualised---its possible states and actions and \ntheir causal structure---is known in advance and does not change during \nlearning. This is an unrealistic assumption in many scenarios, because new \nevidence can reveal important information about what is possible, possibilities \nthat the agent was not aware existed prior to learning. We present a model of \nan agent which both discovers and learns to exploit unforeseen possibilities \nusing two sources of evidence: direct interaction with the world and \ncommunication with a domain expert. We use a combination of probabilistic and \nsymbolic reasoning to estimate all components of the decision problem, \nincluding its set of random variables and their causal dependencies. Agent \nsimulations show that the agent converges on optimal polices even when it \nstarts out unaware of factors that are critical to behaving optimally. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f7327b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03331"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Wilmer Bandres, Blai Bonet, Hector Geffner", "title": "Planning with Pixels in (Almost) Real Time. (arXiv:1801.03354v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.03354", "type": "text/html"}], "timestampUsec": "1515649356890243", "comments": [], "summary": {"content": "<p>Recently, width-based planning methods have been shown to yield \nstate-of-the-art results in the Atari 2600 video games. For this, the states \nwere associated with the (RAM) memory states of the simulator. In this work, we \nconsider the same planning problem but using the screen instead. By using the \nsame visual inputs, the planning results can be compared with those of humans \nand learning methods. We show that the planning approach, out of the box and \nwithout training, results in scores that compare well with those obtained by \nhumans and learning methods, and moreover, by developing an episodic, rollout \nversion of the IW(k) algorithm, we show that such scores can be obtained in \nalmost real time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f7327f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03354"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "L&#xe1;szl&#xf3; Csat&#xf3;", "title": "Axiomatizations of inconsistency indices for triads. (arXiv:1801.03355v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.03355", "type": "text/html"}], "timestampUsec": "1515649356890242", "comments": [], "summary": {"content": "<p>Pairwise comparison matrices often exhibit inconsistency, therefore, a number \nof indices has been introduced to measure their deviation from a consistent \nmatrix. Since inconsistency first emerges in the case of three alternatives, \nseveral inconsistency indices are based on triads. Recently, a set of axioms \nhas been proposed, and is required to be satisfied by any reasonable \ninconsistency index. We illustrate by an example that this set seems to be not \nexhaustive, hence expand it by adding two new properties. We consider all \naxioms on the set of triads, and choose the logically independent ones. \nFinally, it is proved that they characterize the inconsistency ranking induced \nby the Koczkodaj inconsistency index on this domain. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f73283", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03355"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ruth Fong, Andrea Vedaldi", "title": "Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks. (arXiv:1801.03454v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.03454", "type": "text/html"}], "timestampUsec": "1515649356890241", "comments": [], "summary": {"content": "<p>In an effort to understand the meaning of the intermediate representations \ncaptured by deep networks, recent papers have tried to associate specific \nsemantic concepts to individual neural network filter responses, where \ninteresting correlations are often found, largely by focusing on extremal \nfilter responses. In this paper, we show that this approach can favor \neasy-to-interpret cases that are not necessarily representative of the average \nbehavior of a representation. \n</p> \n<p>A more realistic but harder-to-study hypothesis is that semantic \nrepresentations are distributed, and thus filters must be studied in \nconjunction. In order to investigate this idea while enabling systematic \nvisualization and quantification of multiple filter responses, we introduce the \nNet2Vec framework, in which semantic concepts are mapped to vectorial \nembeddings based on corresponding filter responses. By studying such \nembeddings, we are able to show that 1., in most cases, multiple filters are \nrequired to code for a concept, that 2., often filters are not concept specific \nand help encode multiple concepts, and that 3., compared to single filter \nactivations, filter embeddings are able to better characterize the meaning of a \nrepresentation and its relationship to other concepts. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f73288", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03454"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xuelin Qian, Yanwei Fu, Wenxuan Wang, Tao Xiang, Yang Wu, Yu-Gang Jiang, Xiangyang Xue", "title": "Pose-Normalized Image Generation for Person Re-identification. (arXiv:1712.02225v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02225", "type": "text/html"}], "timestampUsec": "1515649356890239", "comments": [], "summary": {"content": "<p>Person Re-identification (re-id) faces two major challenges: the lack of \ncross-view paired training data and learning discriminative identity-sensitive \nand view-invariant features in the presence of large pose variations. In this \nwork, we address both problems by proposing a novel deep person image \ngeneration model for synthesizing realistic person images conditional on pose. \nThe model is based on a generative adversarial network (GAN) and used \nspecifically for pose normalization in re-id, thus termed pose-normalization \nGAN (PN-GAN). With the synthesized images, we can learn a new type of deep \nre-id feature free of the influence of pose variations. We show that this \nfeature is strong on its own and highly complementary to features learned with \nthe original images. Importantly, we now have a model that generalizes to any \nnew re-id dataset without the need for collecting any training data for model \nfine-tuning, thus making a deep re-id model truly scalable. Extensive \nexperiments on five benchmarks show that our model outperforms the \nstate-of-the-art models, often significantly. In particular, the features \nlearned on Market-1501 can achieve a Rank-1 accuracy of 68.67% on VIPeR without \nany model fine-tuning, beating almost all existing models fine-tuned on the \ndataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f7328b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02225"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Igor Gitman, Deepak Dilipkumar, Ben Parr", "title": "Convergence Analysis of Gradient Descent Algorithms with Proportional Updates. (arXiv:1801.03137v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.03137", "type": "text/html"}], "timestampUsec": "1515649356890237", "comments": [], "summary": {"content": "<p>The rise of deep learning in recent years has brought with it increasingly \nclever optimization methods to deal with complex, non-linear loss functions. \nThese methods are often designed with convex optimization in mind, but have \nbeen shown to work well in practice even for the highly non-convex optimization \nassociated with neural networks. However, one significant drawback of these \nmethods when they are applied to deep learning is that the magnitude of the \nupdate step is sometimes disproportionate to the magnitude of the weights (much \nsmaller or larger), leading to training instabilities such as vanishing and \nexploding gradients. An idea to combat this issue is gradient descent with \nproportional updates. Gradient descent with proportional updates was introduced \nin 2017. It was independently developed by You et al (Layer-wise Adaptive Rate \nScaling (LARS) algorithm) and by Abu-El-Haija (PercentDelta algorithm). The \nbasic idea of both of these algorithms is to make each step of the gradient \ndescent proportional to the current weight norm and independent of the gradient \nmagnitude. It is common in the context of new optimization methods to prove \nconvergence or derive regret bounds under the assumption of Lipschitz \ncontinuity and convexity. However, even though LARS and PercentDelta were shown \nto work well in practice, there is no theoretical analysis of the convergence \nproperties of these algorithms. Thus it is not clear if the idea of gradient \ndescent with proportional updates is used in the optimal way, or if it could be \nimproved by using a different norm or specific learning rate schedule, for \nexample. Moreover, it is not clear if these algorithms can be extended to other \nproblems, besides neural networks. We attempt to answer these questions by \nestablishing the theoretical analysis of gradient descent with proportional \nupdates, and verifying this analysis with empirical examples. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f7328f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03137"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "S. Rao Jammalamadaka, Jinwen Qiu, Ning Ning", "title": "Multivariate Bayesian Structural Time Series Model. (arXiv:1801.03222v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.03222", "type": "text/html"}], "timestampUsec": "1515649356890236", "comments": [], "summary": {"content": "<p>This paper deals with inference and prediction for multiple correlated time \nseries, where one has also the choice of using a candidate pool of \ncontemporaneous predictors for each target series. Starting with a structural \nmodel for the time-series, Bayesian tools are used for model fitting, \nprediction, and feature selection, thus extending some recent work along these \nlines for the univariate case. The Bayesian paradigm in this multivariate \nsetting helps the model avoid overfitting as well as capture correlations among \nthe multiple time series with the various state components. The model provides \nneeded flexibility to choose a different set of components and available \npredictors for each target series. The cyclical component in the model can \nhandle large variations in the short term, which may be caused by external \nshocks. We run extensive simulations to investigate properties such as \nestimation accuracy and performance in forecasting. We then run an empirical \nstudy with one-step-ahead prediction on the max log return of a portfolio of \nstocks that involve four leading financial institutions. Both the simulation \nstudies and the extensive empirical study confirm that this multivariate model \noutperforms three other benchmark models, viz. a model that treats each target \nseries as independent, the autoregressive integrated moving average model with \nregression (ARIMAX), and the multivariate ARIMAX (MARIMAX) model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f73292", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03222"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ruoyu Li, Sheng Wang, Feiyun Zhu, Junzhou Huang", "title": "Adaptive Graph Convolutional Neural Networks. (arXiv:1801.03226v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.03226", "type": "text/html"}], "timestampUsec": "1515649356890235", "comments": [], "summary": {"content": "<p>Graph Convolutional Neural Networks (Graph CNNs) are generalizations of \nclassical CNNs to handle graph data such as molecular data, point could and \nsocial networks. Current filters in graph CNNs are built for fixed and shared \ngraph structure. However, for most real data, the graph structures varies in \nboth size and connectivity. The paper proposes a generalized and flexible graph \nCNN taking data of arbitrary graph structure as input. In that way a \ntask-driven adaptive graph is learned for each graph data while training. To \nefficiently learn the graph, a distance metric learning is proposed. Extensive \nexperiments on nine graph-structured datasets have demonstrated the superior \nperformance improvement on both convergence speed and predictive accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f73299", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03226"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mikhail Belkin", "title": "Approximation beats concentration? An approximation view on inference with smooth radial kernels. (arXiv:1801.03437v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.03437", "type": "text/html"}], "timestampUsec": "1515649356890234", "comments": [], "summary": {"content": "<p>Positive definite kernels and their associated Reproducing Kernel Hilbert \nSpaces provide a mathematically compelling and practically competitive \nframework for learning from data. \n</p> \n<p>In this paper we take the approximation theory point of view to explore \nvarious aspects of smooth kernels related to their inferential properties. We \nanalyze eigenvalue decay of kernels operators and matrices, properties of \neigenfunctions/eigenvectors and \"Fourier\" coefficients of functions in the \nkernel space restricted to a discrete set of data points. We also investigate \nthe fitting capacity of kernels, giving explicit bounds on the fat shattering \ndimension of the balls in Reproducing Kernel Hilbert spaces. Interestingly, the \nsame properties that make kernels very effective approximators for functions in \ntheir \"native\" kernel space, also limit their capacity to represent arbitrary \nfunctions. We discuss various implications, including those for gradient \ndescent type methods. \n</p> \n<p>It is important to note that most of our bounds are measure independent. \nMoreover, at least in moderate dimension, the bounds for eigenvalues are much \ntighter than the bounds which can be obtained from the usual matrix \nconcentration results. For example, we see that the eigenvalues of kernel \nmatrices show nearly exponential decay with constants depending only on the \nkernel and the domain. We call this \"approximation beats concentration\" \nphenomenon as even when the data are sampled from a probability distribution, \nsome of their aspects are better understood in terms of approximation theory. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f7329a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03437"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Manuel Valera, Zhengyang Guo, Priscilla Kelly, Sean Matz, Adrian Cantu, Allon G. Percus, Jeffrey D. Hyman, Gowri Srinivasan, Hari S. Viswanathan", "title": "Machine learning for graph-based representations of three-dimensional discrete fracture networks. (arXiv:1705.09866v2 [physics.geo-ph] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.09866", "type": "text/html"}], "timestampUsec": "1515649356890233", "comments": [], "summary": {"content": "<p>Structural and topological information play a key role in modeling flow and \ntransport through fractured rock in the subsurface. Discrete fracture network \n(DFN) computational suites such as dfnWorks are designed to simulate flow and \ntransport in such porous media. Flow and transport calculations reveal that a \nsmall backbone of fractures exists, where most flow and transport occurs. \nRestricting the flowing fracture network to this backbone provides a \nsignificant reduction in the network's effective size. However, the particle \ntracking simulations needed to determine the reduction are computationally \nintensive. Such methods may be impractical for large systems or for robust \nuncertainty quantification of fracture networks, where thousands of forward \nsimulations are needed to bound system behavior. \n</p> \n<p>In this paper, we develop an alternative network reduction approach to \ncharacterizing transport in DFNs, by combining graph theoretical and machine \nlearning methods. We consider a graph representation where nodes signify \nfractures and edges denote their intersections. Using random forest and support \nvector machines, we rapidly identify a subnetwork that captures the flow \npatterns of the full DFN, based primarily on node centrality features in the \ngraph. Our supervised learning techniques train on particle-tracking backbone \npaths found by dfnWorks, but run in negligible time compared to those \nsimulations. We find that our predictions can reduce the network to \napproximately 20% of its original size, while still generating breakthrough \ncurves consistent with those of the original network. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f7329e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.09866"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Angshuman Roy, Alok Goswami, C. A. Murthy", "title": "Multivariate Dependency Measure based on Copula and Gaussian Kernel. (arXiv:1708.07485v2 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.07485", "type": "text/html"}], "timestampUsec": "1515649356890231", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3293ad8e5\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3293ad8e5&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a new multivariate dependency measure. It is obtained by \nconsidering a Gaussian kernel based distance between the copula transform of \nthe given d-dimensional distribution and the uniform copula and then \nappropriately normalizing it. The resulting measure is shown to satisfy a \nnumber of desirable properties. A nonparametric estimate is proposed for this \ndependency measure and its properties (finite sample as well as asymptotic) are \nderived. Some comparative studies of the proposed dependency measure estimate \nwith some widely used dependency measure estimates on artificial datasets are \nincluded. A non-parametric test of independence between two or more random \nvariables based on this measure is proposed. A comparison of the proposed test \nwith some existing nonparametric multivariate test for independence is \npresented. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f732a0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.07485"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Aman Sinha, Hongseok Namkoong, John Duchi", "title": "Certifiable Distributional Robustness with Principled Adversarial Training. (arXiv:1710.10571v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.10571", "type": "text/html"}], "timestampUsec": "1515649356890230", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32940a41d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32940a41d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Neural networks are vulnerable to adversarial examples and researchers have \nproposed many heuristic attack and defense mechanisms. We take the principled \nview of distributionally robust optimization, which guarantees performance \nunder adversarial input perturbations. By considering a Lagrangian penalty \nformulation of perturbation of the underlying data distribution in a \nWasserstein ball, we provide a training procedure that augments model parameter \nupdates with worst-case perturbations of training data. For smooth losses, our \nprocedure provably achieves moderate levels of robustness with little \ncomputational or statistical cost relative to empirical risk minimization. \nFurthermore, our statistical guarantees allow us to efficiently certify \nrobustness for the population loss. For imperceptible perturbations, our method \nmatches or outperforms heuristic approaches. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515649356890", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f732aa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.10571"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Artit Wangperawong, Kettip Kriangchaivech, Austin Lanari, Supui Lam, Panthong Wangperawong", "title": "Comparing heterogeneous entities using artificial neural networks of trainable weighted structural components and machine-learned activation functions. (arXiv:1801.03143v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.03143", "type": "text/html"}], "timestampUsec": "1515646940354238", "comments": [], "summary": {"content": "<p>To compare entities of differing types and structural components, the \nartificial neural network paradigm was used to cross-compare structural \ncomponents between heterogeneous documents. Trainable weighted structural \ncomponents were input into machine-learned activation functions of the neurons. \nThe model was used for matching news articles and videos, where the inputs and \nactivation functions respectively consisted of term vectors and cosine \nsimilarity measures between the weighted structural components. The model was \ntested with different weights, achieving as high as 59.2% accuracy for matching \nvideos to news articles. A mobile application user interface for recommending \nrelated videos for news articles was developed to demonstrate consumer value, \nincluding its potential usefulness for cross-selling products from unrelated \ncategories. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515646940354", "annotations": [], "published": 1515646941, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f28ba8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03143"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ludovic Trottier, Philippe Gigu&#xe8;re, Brahim Chaib-draa", "title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks. (arXiv:1605.09332v4 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1605.09332", "type": "text/html"}], "timestampUsec": "1515646940354237", "comments": [], "summary": {"content": "<p>Object recognition is an important task for improving the ability of visual \nsystems to perform complex scene understanding. Recently, the Exponential \nLinear Unit (ELU) has been proposed as a key component for managing bias shift \nin Convolutional Neural Networks (CNNs), but defines a parameter that must be \nset by hand. In this paper, we propose learning a parameterization of ELU in \norder to learn the proper activation shape at each layer in the CNNs. Our \nresults on the MNIST, CIFAR-10/100 and ImageNet datasets using the NiN, \nOverfeat, All-CNN and ResNet networks indicate that our proposed Parametric ELU \n(PELU) has better performances than the non-parametric ELU. We have observed as \nmuch as a 7.28% relative error improvement on ImageNet with the NiN network, \nwith only 0.0003% parameter increase. Our visual examination of the non-linear \nbehaviors adopted by Vgg using PELU shows that the network took advantage of \nthe added flexibility by learning different activations at different layers. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515646940354", "annotations": [], "published": 1515646941, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f28bae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1605.09332"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Haiping Huang", "title": "Role of zero synapses in unsupervised feature learning. (arXiv:1703.07943v4 [q-bio.NC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.07943", "type": "text/html"}], "timestampUsec": "1515646940354236", "comments": [], "summary": {"content": "<p>Synapses in real neural circuits can take discrete values, including zero \n(silent or potential) synapses. The computational role of zero synapses in \nunsupervised feature learning of unlabeled noisy data is still unclear, thus it \nis important to understand how the sparseness of synaptic activity is shaped \nduring learning and its relationship with receptive field formation. Here, we \nformulate this kind of sparse feature learning by a statistical mechanics \napproach. We find that learning decreases the fraction of zero synapses, and \nwhen the fraction decreases rapidly around a critical data size, an \nintrinsically structured receptive field starts to develop. Further increasing \nthe data size refines the receptive field, while a very small fraction of zero \nsynapses remain to act as contour detectors. This phenomenon is discovered not \nonly in learning a handwritten digits dataset, but also in learning retinal \nneural activity measured in a natural-movie-stimuli experiment. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515646940354", "annotations": [], "published": 1515646941, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f28bb1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.07943"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hendrik Richter", "title": "Visual art inspired by the collective feeding behavior of sand-bubbler crabs. (arXiv:1709.00410v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.00410", "type": "text/html"}], "timestampUsec": "1515646940354235", "comments": [], "summary": {"content": "<p>Sand--bubblers are crabs of the genera Dotilla and Scopimera which are known \nto produce remarkable patterns and structures at tropical beaches. From these \npattern-making abilities, we may draw inspiration for digital visual art. A \nsimple mathematical model is proposed and an algorithm is designed that may \ncreate such sand-bubbler patterns artificially. In addition, design parameters \nto modify the patterns are identified and analyzed by computational aesthetic \nmeasures. Finally, an extension of the algorithm is discussed that may enable \ncontrolling and guiding generative evolution of the art-making process. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515646940354", "annotations": [], "published": 1515646941, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f28bb5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.00410"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tae Jun Lee, Justin Gottschlich, Nesime Tatbul, Eric Metcalf, Stan Zdonik", "title": "Precision and Recall for Range-Based Anomaly Detection. (arXiv:1801.03175v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.03175", "type": "text/html"}], "timestampUsec": "1515646940354234", "comments": [], "summary": {"content": "<p>Classical anomaly detection is principally concerned with point-based \nanomalies, anomalies that occur at a single data point. In this paper, we \npresent a new mathematical model to express range-based anomalies, anomalies \nthat occur over a range (or period) of time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515646940354", "annotations": [], "published": 1515646941, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f28bb8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03175"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gil Keren, Maximilian Schmitt, Thomas Kehrenberg, Bj&#xf6;rn Schuller", "title": "Weakly Supervised One-Shot Detection with Attention Siamese Networks. (arXiv:1801.03329v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.03329", "type": "text/html"}], "timestampUsec": "1515646940354232", "comments": [], "summary": {"content": "<p>We consider the task of weakly supervised one-shot detection. In this task, \nwe attempt to perform a detection task over a set of unseen classes, when \ntraining only using weak binary labels that indicate the existence of a class \ninstance in a given example. The model is conditioned on a single exemplar of \nan unseen class and a target example that may or may not contain an instance of \nthe same class as the exemplar. A similarity map is computed by using a Siamese \nneural network to map the exemplar and regions of the target example to a \nlatent representation space and then computing cosine similarity scores between \nrepresentations. An attention mechanism weights different regions in the target \nexample, and enables learning of the one-shot detection task using the weaker \nlabels alone. The model can be applied to detection tasks from different \ndomains, including computer vision object detection. We evaluate our attention \nSiamese networks on a one-shot detection task from the audio domain, where it \ndetects audio keywords in spoken utterances. Our model considerably outperforms \na baseline approach and yields a 42.6% average precision for detection across \n10 unseen classes. Moreover, architectural developments from computer vision \nobject detection models such as a region proposal network can be incorporated \ninto the model architecture, and results show that performance is expected to \nimprove by doing so. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515646940354", "annotations": [], "published": 1515646941, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f28bbc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03329"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nima Dehmamy, Neda Rohani, Aggelos Katsaggelos", "title": "Deriving optimal weights in deep neural networks. (arXiv:1703.04757v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.04757", "type": "text/html"}], "timestampUsec": "1515646940354231", "comments": [], "summary": {"content": "<p>Training deep neural networks generally requires massive amounts of data and \nis very computation intensive. We show here that it may be possible to \ncircumvent the expensive gradient descent procedure and derive the parameters \nof a neural network directly from properties of the training data. We show \nthat, near convergence, the gradient descent equations for layers close to the \ninput can be linearized and become stochastic equations with noise related to \nthe covariance of data for each class. We derive the distribution of solutions \nto these equations and discover that it is related to a \"supervised principal \ncomponent analysis.\" We implement these results on image datasets MNIST, \nCIFAR10 and CIFAR100 and find that, indeed, pretrained layers using our \nfindings performs comparable or superior to neural networks of the same size \nand architecture trained with gradient descent. Moreover, our pretrained layers \ncan often be calculated using a fraction of the training data, owing to the \nquick convergence of the covariance matrix. Thus, our findings indicate that we \ncan cut the training time both by requiring only a fraction of the data used \nfor gradient descent, and by eliminating layers in the costly backpropagation \nstep of the training. Additionally, these findings partially elucidate the \ninner workings of deep neural networks and allow us to mathematically calculate \noptimal solutions for some stages of classification problems, thus \nsignificantly boosting our ability to solve such problems efficiently. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515646940354", "annotations": [], "published": 1515646941, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000358f28bc1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.04757"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Eliya Nachmani, Yaron Bachar, Elad Marciano, David Burshtein, Yair Be&#x27;ery", "title": "Near Maximum Likelihood Decoding with Deep Learning. (arXiv:1801.02726v1 [cs.IT])", "alternate": [{"href": "http://arxiv.org/abs/1801.02726", "type": "text/html"}], "timestampUsec": "1515563090982598", "comments": [], "summary": {"content": "<p>A novel and efficient neural decoder algorithm is proposed. The proposed \ndecoder is based on the neural Belief Propagation algorithm and the \nAutomorphism Group. By combining neural belief propagation with permutations \nfrom the Automorphism Group we achieve near maximum likelihood performance for \nHigh Density Parity Check codes. Moreover, the proposed decoder significantly \nimproves the decoding complexity, compared to our earlier work on the topic. We \nalso investigate the training process and show how it can be accelerated. \nSimulations of the hessian and the condition number show why the learning \nprocess is accelerated. We demonstrate the decoding algorithm for various \nlinear block codes of length up to 63 bits. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b77d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02726"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xinyu Wu, Vishal Saxena", "title": "Dendritic-Inspired Processing Enables Bio-Plausible STDP in Compound Binary Synapses. (arXiv:1801.02797v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.02797", "type": "text/html"}], "timestampUsec": "1515563090982597", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32940a765\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32940a765&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Brain-inspired learning mechanisms, e.g. spike timing dependent plasticity \n(STDP), enable agile and fast on-the-fly adaptation capability in a spiking \nneural network. When incorporating emerging nanoscale resistive non-volatile \nmemory (NVM) devices, with ultra-low power consumption and high-density \nintegration capability, a spiking neural network hardware would result in \nseveral orders of magnitude reduction in energy consumption at a very small \nform factor and potentially herald autonomous learning machines. However, \nactual memory devices have shown to be intrinsically binary with stochastic \nswitching, and thus impede the realization of ideal STDP with continuous analog \nvalues. In this work, a dendritic-inspired processing architecture is proposed \nin addition to novel CMOS neuron circuits. The utilization of spike \nattenuations and delays transforms the traditionally undesired stochastic \nbehavior of binary NVMs into a useful leverage that enables \nbiologically-plausible STDP learning. As a result, this work paves a pathway to \nadopt practical binary emerging NVM devices in brain-inspired neuromorphic \ncomputing. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b784", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02797"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Lex Fridman, Benedikt Jenik, Jack Terwilliger", "title": "DeepTraffic: Driving Fast through Dense Traffic with Deep Reinforcement Learning. (arXiv:1801.02805v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.02805", "type": "text/html"}], "timestampUsec": "1515563090982596", "comments": [], "summary": {"content": "<p>We present a micro-traffic simulation (named \"DeepTraffic\") where the \nperception, control, and planning systems for one of the cars are all handled \nby a single neural network as part of a model-free, off-policy reinforcement \nlearning process. The primary goal of DeepTraffic is to make the hands-on study \nof deep reinforcement learning accessible to thousands of students, educators, \nand researchers in order to inspire and fuel the exploration and evaluation of \nDQN variants and hyperparameter configurations through large-scale, open \ncompetition. This paper investigates the crowd-sourced hyperparameter tuning of \nthe policy network that resulted from the first iteration of the DeepTraffic \ncompetition where thousands of participants actively searched through the \nhyperparameter space with the objective of their neural network submission to \nmake it onto the top-10 leaderboard. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b78a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02805"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516954929, "author": "Esra&#x27;a O Alkafaween", "title": "Novel Methods for Enhancing the Performance of Genetic Algorithms. (arXiv:1801.02827v3 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1801.02827", "type": "text/html"}], "timestampUsec": "1515563090982595", "comments": [], "summary": {"content": "<p>In this thesis we propose new methods for crossover operator namely: cut on \nworst gene (COWGC), cut on worst L+R gene (COWLRGC) and Collision Crossovers. \nAnd also we propose several types of mutation operator such as: worst gene with \nrandom gene mutation (WGWRGM) , worst LR gene with random gene mutation \n(WLRGWRGM), worst gene with worst gene mutation (WGWWGM), worst gene with \nnearest neighbour mutation (WGWNNM), worst gene with the worst around the \nnearest neighbour mutation (WGWWNNM), worst gene inserted beside nearest \nneighbour mutation (WGIBNNM), random gene inserted beside nearest neighbour \nmutation (RGIBNNM), Swap worst gene locally mutation (SWGLM), Insert best \nrandom gene before worst gene mutation (IBRGBWGM) and Insert best random gene \nbefore random gene mutation (IBRGBRGM). In addition to proposing four selection \nstrategies, namely: select any crossover (SAC), select any mutation (SAM), \nselect best crossover (SBC) and select best mutation (SBM). The first two are \nbased on selection of the best crossover and mutation operator respectively, \nand the other two strategies randomly select any operator. So we investigate \nthe use of more than one crossover/mutation operator (based on the proposed \nstrategies) to enhance the performance of genetic algorithms. Our experiments, \nconducted on several Travelling Salesman Problems (TSP), show the superiority \nof some of the proposed methods in crossover and mutation over some of the \nwell-known crossover and mutation operators described in the literature. In \naddition, using any of the four strategies (SAC, SAM, SBC and SBM), found to be \nbetter than using one crossover/mutation operator in general, because those \nallow the GA to avoid local optima, or the so-called premature convergence. \nKeywords: GAs, Collision crossover, Multi crossovers, Multi mutations, TSP. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b790", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02827"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516080170, "author": "Han Xiao", "title": "Convexification of Neural Graph. (arXiv:1801.02901v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1801.02901", "type": "text/html"}], "timestampUsec": "1515563090982594", "comments": [], "summary": {"content": "<p>Traditionally, most complex intelligence architectures are extremely \nnon-convex, which could not be well performed by convex optimization. However, \nthis paper decomposes complex structures into three types of nodes: operators, \nalgorithms and functions. Iteratively, propagating from node to node along \nedge, we prove that \"regarding the tree-structured neural graph, it is nearly \nconvex in each variable, when the other variables are fixed.\" In fact, the \nnon-convex properties stem from circles and functions, which could be \ntransformed to be convex with our proposed \\textit{\\textbf{scale mechanism}}. \nExperimentally, we justify our theoretical analysis by two practical \napplications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b795", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02901"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Robert J. Rovetto", "title": "An Ontology for Satellite Databases. (arXiv:1801.02940v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.02940", "type": "text/html"}], "timestampUsec": "1515563090982593", "comments": [], "summary": {"content": "<p>This paper demonstrates the development of ontology for satellite databases. \nFirst, I create a computational ontology for the Union of Concerned Scientists \n(UCS) Satellite Database (UCSSD for short), called the UCS Satellite Ontology \n(or UCSSO). Second, in developing UCSSO I show that The Space Situational \nAwareness Ontology (SSAO) (Rovetto and Kelso 2016)--an existing space domain \nreference ontology--and related ontology work by the author (Rovetto 2015, \n2016) can be used either (i) with a database-specific local ontology such as \nUCSSO, or (ii) in its stead. In case (i), local ontologies such as UCSSO can \nreuse SSAO terms, perform term mappings, or extend it. In case (ii), the \nauthor's orbital space ontology work, such as the SSAO, is usable by the UCSSD \nand organizations with other space object catalogs, as a reference ontology \nsuite providing a common semantically-rich domain model. The SSAO, UCSSO, and \nthe broader Orbital Space Environment Domain Ontology project is online at \n<a href=\"http://purl.org/space-ontology\">this http URL</a> and GitHub. This ontology effort aims, in part, \nto provide accurate formal representations of the domain for various \napplications. Ontology engineering has the potential to facilitate the sharing \nand integration of satellite data from federated databases and sensors for \nsafer spaceflight. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b796", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02940"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Lise Verlaet (LERASS), Sidonie Gallot (LERASS)", "title": "Between collective intelligence and semantic web : hypermediating sites. Contribution to technologies of intelligence. (arXiv:1801.03003v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.03003", "type": "text/html"}], "timestampUsec": "1515563090982592", "comments": [], "summary": {"content": "<p>In this paper we present a new form of access to knowledge through what we \ncall \"hypermediator websites\". These hypermediator sites are intermediate \nbetween information devices that just scan the book culture and a \"real\" \nhypertext writing format. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b798", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03003"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Patryk Orzechowski, Moshe Sipper, Xiuzhen Huang, Jason H. Moore", "title": "EBIC: an artificial intelligence-based parallel biclustering algorithm for pattern discovery. (arXiv:1801.03039v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.03039", "type": "text/html"}], "timestampUsec": "1515563090982591", "comments": [], "summary": {"content": "<p>In this paper a novel biclustering algorithm based on artificial intelligence \n(AI) is introduced. The method called EBIC aims to detect biologically \nmeaningful, order-preserving patterns in complex data. The proposed algorithm \nis probably the first one capable of discovering with accuracy exceeding 50\\% \nmultiple complex patterns in real gene expression datasets. It is also one of \nthe very few biclustering methods designed for parallel environments with \nmultiple graphics processing units (GPUs). We demonstrate that EBIC outperforms \nstate-of-the-art biclustering methods, in terms of recovery and relevance, on \nboth synthetic and genetic datasets. EBIC also yields results over 12 times \nfaster than the most accurate reference algorithms. The proposed algorithm is \nanticipated to be added to the repertoire of unsupervised machine learning \nalgorithms for the analysis of datasets, including those from large-scale \ngenomic studies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b79b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03039"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Imon Banerjee, Michael Francis Gensheimer, Douglas J. Wood, Solomon Henry, Daniel Chang, Daniel L. Rubin", "title": "Probabilistic Prognostic Estimates of Survival in Metastatic Cancer Patients (PPES-Met) Utilizing Free-Text Clinical Narratives. (arXiv:1801.03058v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.03058", "type": "text/html"}], "timestampUsec": "1515563090982590", "comments": [], "summary": {"content": "<p>We propose a deep learning model - Probabilistic Prognostic Estimates of \nSurvival in Metastatic Cancer Patients (PPES-Met) for estimating short-term \nlife expectancy (3 months) of the patients by analyzing free-text clinical \nnotes in the electronic medical record, while maintaining the temporal visit \nsequence. In a single framework, we integrated semantic data mapping and neural \nembedding technique to produce a text processing method that extracts relevant \ninformation from heterogeneous types of clinical notes in an unsupervised \nmanner, and we designed a recurrent neural network to model the temporal \ndependency of the patient visits. The model was trained on a large dataset \n(10,293 patients) and validated on a separated dataset (1818 patients). Our \nmethod achieved an area under the ROC curve (AUC) of 0.89. To provide \nexplain-ability, we developed an interactive graphical tool that may improve \nphysician understanding of the basis for the model's predictions. The high \naccuracy and explain-ability of the PPES-Met model may enable our model to be \nused as a decision support tool to personalize metastatic cancer treatment and \nprovide valuable assistance to the physicians. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b79e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03058"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ruth Fong, Andrea Vedaldi", "title": "Interpretable Explanations of Black Boxes by Meaningful Perturbation. (arXiv:1704.03296v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.03296", "type": "text/html"}], "timestampUsec": "1515563090982589", "comments": [], "summary": {"content": "<p>As machine learning algorithms are increasingly applied to high impact yet \nhigh risk tasks, such as medical diagnosis or autonomous driving, it is \ncritical that researchers can explain how such algorithms arrived at their \npredictions. In recent years, a number of image saliency methods have been \ndeveloped to summarize where highly complex neural networks \"look\" in an image \nfor evidence for their predictions. However, these techniques are limited by \ntheir heuristic nature and architectural constraints. In this paper, we make \ntwo main contributions: First, we propose a general framework for learning \ndifferent kinds of explanations for any black box algorithm. Second, we \nspecialise the framework to find the part of an image most responsible for a \nclassifier decision. Unlike previous works, our method is model-agnostic and \ntestable because it is grounded in explicit and interpretable image \nperturbations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b7a7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.03296"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ben Usman, Kate Saenko, Brian Kulis", "title": "Stable Distribution Alignment Using the Dual of the Adversarial Distance. (arXiv:1707.04046v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.04046", "type": "text/html"}], "timestampUsec": "1515563090982588", "comments": [], "summary": {"content": "<p>Methods that align distributions by minimizing an adversarial distance \nbetween them have recently achieved impressive results. However, these \napproaches are difficult to optimize with gradient descent and they often do \nnot converge well without careful hyperparameter tuning and proper \ninitialization. We investigate whether turning the adversarial min-max problem \ninto an optimization problem by replacing the maximization part with its dual \nimproves the quality of the resulting alignment and explore its connections to \nMaximum Mean Discrepancy. Our empirical results suggest that using the dual \nformulation for the restricted family of linear discriminators results in a \nmore stable convergence to a desirable solution when compared with the \nperformance of a primal min-max GAN-like objective and an MMD objective under \nthe same restrictions. We test our hypothesis on the problem of aligning two \nsynthetic point clouds on a plane and on a real-image domain adaptation problem \non digits. In both cases, the dual formulation yields an iterative procedure \nthat gives more stable and monotonic improvement over time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b7b1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.04046"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Wenhan Xiong, Thien Hoang, William Yang Wang", "title": "DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning. (arXiv:1707.06690v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.06690", "type": "text/html"}], "timestampUsec": "1515563090982587", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32940aaa2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32940aaa2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We study the problem of learning to reason in large scale knowledge graphs \n(KGs). More specifically, we describe a novel reinforcement learning framework \nfor learning multi-hop relational paths: we use a policy-based agent with \ncontinuous states based on knowledge graph embeddings, which reasons in a KG \nvector space by sampling the most promising relation to extend its path. In \ncontrast to prior work, our approach includes a reward function that takes the \naccuracy, diversity, and efficiency into consideration. Experimentally, we show \nthat our proposed method outperforms a path-ranking based algorithm and \nknowledge graph embedding methods on Freebase and Never-Ending Language \nLearning datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b7ce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.06690"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Emmanuel de Bezenac, Arthur Pajot, Patrick Gallinari", "title": "Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge. (arXiv:1711.07970v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07970", "type": "text/html"}], "timestampUsec": "1515563090982586", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32945cf39\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32945cf39&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We consider the use of Deep Learning methods for modeling complex phenomena \nlike those occurring in natural physical processes. With the large amount of \ndata gathered on these phenomena the data intensive paradigm could begin to \nchallenge more traditional approaches elaborated over the years in fields like \nmaths or physics. However, despite considerable successes in a variety of \napplication domains, the machine learning field is not yet ready to handle the \nlevel of complexity required by such problems. Using an example application, \nnamely Sea Surface Temperature Prediction, we show how general background \nknowledge gained from physics could be used as a guideline for designing \nefficient Deep Learning models. In order to motivate the approach and to assess \nits generality we demonstrate a formal link between the solution of a class of \ndifferential equations underlying a large family of physical phenomena and the \nproposed model. Experiments and comparison with series of baselines including a \nstate of the art numerical approach is then provided. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b7d6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07970"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pranav Rajpurkar, Jeremy Irvin, Aarti Bagul, Daisy Ding, Tony Duan, Hershel Mehta, Brandon Yang, Kaylie Zhu, Dillon Laird, Robyn L. Ball, Curtis Langlotz, Katie Shpanskaya, Matthew P. Lungren, Andrew Ng", "title": "MURA Dataset: Towards Radiologist-Level Abnormality Detection in Musculoskeletal Radiographs. (arXiv:1712.06957v3 [physics.med-ph] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1712.06957", "type": "text/html"}], "timestampUsec": "1515563090982585", "comments": [], "summary": {"content": "<p>We introduce MURA, a large dataset of musculoskeletal radiographs containing \n40,895 images from 14,982 studies, where each study is manually labeled by \nradiologists as either normal or abnormal. On this dataset, we train a \n169-layer densely connected convolutional network to detect and localize \nabnormalities. To evaluate our model robustly and to get an estimate of \nradiologist performance, we collect additional labels from board-certified \nStanford radiologists on the test set, consisting of 209 musculoskeletal \nstudies. We compared our model and radiologists on the Cohen's kappa statistic, \nwhich expresses the agreement of our model and of each radiologist with the \ngold standard, defined as the majority vote of a disjoint group of \nradiologists. We find that our model achieves performance comparable to that of \nradiologists. Model performance is higher than the best radiologist performance \nin detecting abnormalities on finger studies and equivalent on wrist studies. \nHowever, model performance is lower than best radiologist performance in \ndetecting abnormalities on elbow, forearm, hand, humerus, and shoulder studies, \nindicating that the task is a good challenge for future research. To encourage \nadvances, we have made our dataset freely available at \nhttps://stanfordmlgroup.github.io/projects/mura \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b7db", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06957"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Brenden K. Petersen, Michael B. Mayhew, Kalvin O. E. Ogbuefi, John D. Greene, Vincent X. Liu, Priyadip Ray", "title": "Modeling sepsis progression using hidden Markov models. (arXiv:1801.02736v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.02736", "type": "text/html"}], "timestampUsec": "1515563090982584", "comments": [], "summary": {"content": "<p>Characterizing a patient's progression through stages of sepsis is critical \nfor enabling risk stratification and adaptive, personalized treatment. However, \ncommonly used sepsis diagnostic criteria fail to account for significant \nunderlying heterogeneity, both between patients as well as over time in a \nsingle patient. We introduce a hidden Markov model of sepsis progression that \nexplicitly accounts for patient heterogeneity. Benchmarked against two sepsis \ndiagnostic criteria, the model provides a useful tool to uncover a patient's \nlatent sepsis trajectory and to identify high-risk patients in whom more \naggressive therapy may be indicated. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b7e0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02736"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ian Dewancker, Jakob Bauer, Michael McCourt", "title": "Sequential Preference-Based Optimization. (arXiv:1801.02788v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.02788", "type": "text/html"}], "timestampUsec": "1515563090982583", "comments": [], "summary": {"content": "<p>Many real-world engineering problems rely on human preferences to guide their \ndesign and optimization. We present PrefOpt, an open source package to simplify \nsequential optimization tasks that incorporate human preference feedback. Our \napproach extends an existing latent variable model for binary preferences to \nallow for observations of equivalent preference from users. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b7e4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02788"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Seth Flaxman, Michael Chirico, Pau Pereira, Charles Loeffler", "title": "Scalable high-resolution forecasting of sparse spatiotemporal events with kernel methods: a winning solution to the NIJ \"Real-Time Crime Forecasting Challenge\". (arXiv:1801.02858v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.02858", "type": "text/html"}], "timestampUsec": "1515563090982582", "comments": [], "summary": {"content": "<p>This article describes Team Kernel Glitches' solution to the National \nInstitute of Justice's (NIJ) Real-Time Crime Forecasting Challenge. The goal of \nthe NIJ Real-Time Crime Forecasting Competition was to maximize two different \ncrime hotspot scoring metrics for calls-for-service to the Portland Police \nBureau (PPB) in Portland, Oregon during the period from March 1, 2017 to May \n31, 2017. Our solution to the challenge is a spatiotemporal forecasting model \ncombining scalable randomized Reproducing Kernel Hilbert Space (RKHS) methods \nfor approximating Gaussian processes with autoregressive smoothing kernels in a \nregularized supervised learning framework. Our model can be understood as an \napproximation to the popular log-Gaussian Cox Process model: we discretize the \nspatiotemporal point pattern and learn a log intensity function using the \nPoisson likelihood and highly efficient gradient-based optimization methods. \nModel hyperparameters including quality of RKHS approximation, spatial and \ntemporal kernel lengthscales, number of autoregressive lags, bandwidths for \nsmoothing kernels, as well as cell shape, size, and rotation, were learned \nusing crossvalidation. Resulting predictions exceeded baseline KDE estimates by \n0.157. Performance improvement over baseline predictions were particularly \nlarge for sparse crimes over short forecasting horizons. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b7e9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02858"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Masud Moshtaghi, James C. Bezdek, Sarah M. Erfani, Christopher Leckie, James Bailey", "title": "Online Cluster Validity Indices for Streaming Data. (arXiv:1801.02937v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.02937", "type": "text/html"}], "timestampUsec": "1515563090982581", "comments": [], "summary": {"content": "<p>Cluster analysis is used to explore structure in unlabeled data sets in a \nwide range of applications. An important part of cluster analysis is validating \nthe quality of computationally obtained clusters. A large number of different \ninternal indices have been developed for validation in the offline setting. \nHowever, this concept has not been extended to the online setting. A key \nchallenge is to find an efficient incremental formulation of an index that can \ncapture both cohesion and separation of the clusters over potentially infinite \ndata streams. In this paper, we develop two online versions (with and without \nforgetting factors) of the Xie-Beni and Davies-Bouldin internal validity \nindices, and analyze their characteristics, using two streaming clustering \nalgorithms (sk-means and online ellipsoidal clustering), and illustrate their \nuse in monitoring evolving clusters in streaming data. We also show that \nincremental cluster validity indices are capable of sending a distress signal \nto online monitors when evolving clusters go awry. Our numerical examples \nindicate that the incremental Xie-Beni index with forgetting factor is superior \nto the other three indices tested. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b7f3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02937"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Marton Havasi, Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato, Juan Jos&#xe9; Murillo-Fuentes", "title": "Deep Gaussian Processes with Decoupled Inducing Inputs. (arXiv:1801.02939v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.02939", "type": "text/html"}], "timestampUsec": "1515563090982580", "comments": [], "summary": {"content": "<p>Deep Gaussian Processes (DGP) are hierarchical generalizations of Gaussian \nProcesses (GP) that have proven to work effectively on a multiple supervised \nregression tasks. They combine the well calibrated uncertainty estimates of GPs \nwith the great flexibility of multilayer models. In DGPs, given the inputs, the \noutputs of the layers are Gaussian distributions parameterized by their means \nand covariances. These layers are realized as Sparse GPs where the training \ndata is approximated using a small set of pseudo points. In this work, we show \nthat the computational cost of DGPs can be reduced with no loss in performance \nby using a separate, smaller set of pseudo points when calculating the \nlayerwise variance while using a larger set of pseudo points when calculating \nthe layerwise mean. This enabled us to train larger models that have lower cost \nand better predictive performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b800", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02939"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Marco Cap&#xf3;, Aritz P&#xe9;rez, Jose A. Lozano", "title": "An efficient K -means clustering algorithm for massive data. (arXiv:1801.02949v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.02949", "type": "text/html"}], "timestampUsec": "1515563090982579", "comments": [], "summary": {"content": "<p>The analysis of continously larger datasets is a task of major importance in \na wide variety of scientific fields. In this sense, cluster analysis algorithms \nare a key element of exploratory data analysis, due to their easiness in the \nimplementation and relatively low computational cost. Among these algorithms, \nthe K -means algorithm stands out as the most popular approach, besides its \nhigh dependency on the initial conditions, as well as to the fact that it might \nnot scale well on massive datasets. In this article, we propose a recursive and \nparallel approximation to the K -means algorithm that scales well on both the \nnumber of instances and dimensionality of the problem, without affecting the \nquality of the approximation. In order to achieve this, instead of analyzing \nthe entire dataset, we work on small weighted sets of points that mostly intend \nto extract information from those regions where it is harder to determine the \ncorrect cluster assignment of the original instances. In addition to different \ntheoretical properties, which deduce the reasoning behind the algorithm, \nexperimental results indicate that our method outperforms the state-of-the-art \nin terms of the trade-off between number of distance computations and the \nquality of the solution obtained. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b802", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02949"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alex Huang, Abdullah Al-Dujaili, Erik Hemberg, Una-May O&#x27;Reilly", "title": "Adversarial Deep Learning for Robust Detection of Binary Encoded Malware. (arXiv:1801.02950v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1801.02950", "type": "text/html"}], "timestampUsec": "1515563090982578", "comments": [], "summary": {"content": "<p>Malware is constantly adapting in order to avoid detection. Model based \nmalware detectors, such as SVM and neural networks, are vulnerable to so-called \nadversarial examples which are modest changes to detectable malware that allows \nthe resulting malware to evade detection. Continuous-valued methods that are \nrobust to adversarial examples of images have been developed using saddle-point \noptimization formulations. We are inspired by them to develop similar methods \nfor the discrete, e.g. binary, domain which characterizes the features of \nmalware. A specific extra challenge of malware is that the adversarial examples \nmust be generated in a way that preserves their malicious functionality. We \nintroduce methods capable of generating functionally preserved adversarial \nmalware examples in the binary domain. Using the saddle-point formulation, we \nincorporate the adversarial examples into the training of models that are \nrobust to them. We evaluate the effectiveness of the methods and others in the \nliterature on a set of Portable Execution~(PE) files. Comparison prompts our \nintroduction of an online measure computed during training to assess general \nexpectation of robustness. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b804", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02950"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Milad Zafar Nezhad, Dongxiao Zhu, Najibesadat Sadati, Kai Yang", "title": "A Predictive Approach Using Deep Feature Learning for Electronic Medical Records: A Comparative Study. (arXiv:1801.02961v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.02961", "type": "text/html"}], "timestampUsec": "1515563090982577", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32945d31d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32945d31d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Massive amount of electronic medical records accumulating from patients and \npopulations motivates clinicians and data scientists to collaborate for the \nadvanced analytics to extract knowledge that is essential to address the \nextensive personalized insights needed for patients, clinicians, providers, \nscientists, and health policy makers. In this paper, we propose a new \npredictive approach based on feature representation using deep feature learning \nand word embedding techniques. Our method uses different deep architectures for \nfeature representation in higher-level abstraction to obtain effective and more \nrobust features from EMRs, and then build prediction models on the top of them. \nOur approach is particularly useful when the unlabeled data is abundant whereas \nlabeled one is scarce. We investigate the performance of representation \nlearning through a supervised approach. First, we apply our method on a small \ndataset related to a specific precision medicine problem, which focuses on \nprediction of left ventricular mass indexed to body surface area (LVMI) as an \nindicator of heart damage risk in a vulnerable demographic subgroup \n(African-Americans). Then we use two large datasets from eICU collaborative \nresearch database to predict the length of stay in Cardiac-ICU and Neuro-ICU \nbased on high dimensional features. Finally we provide a comparative study and \nshow that our predictive approach leads to better results in comparison with \nothers. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b807", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02961"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zeyuan Allen-Zhu", "title": "How To Make the Gradients Small Stochastically. (arXiv:1801.02982v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.02982", "type": "text/html"}], "timestampUsec": "1515563090982576", "comments": [], "summary": {"content": "<p>In convex stochastic optimization, convergence rates in terms of minimizing \nthe objective have been well-established. However, in terms of making the \ngradients small, the best known convergence rate was $O(\\varepsilon^{-8/3})$ \nand it was left open how to improve it. \n</p> \n<p>In this paper, we improve this rate to $\\tilde{O}(\\varepsilon^{-2})$, which \nis optimal up to log factors. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b80b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02982"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "V&#xed;ctor Gallego, Pablo Angulo, Pablo Su&#xe1;rez-Garc&#xed;a, David G&#xf3;mez-Ullate", "title": "Sales forecasting and risk management under uncertainty in the media industry. (arXiv:1801.03050v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.03050", "type": "text/html"}], "timestampUsec": "1515563090982575", "comments": [], "summary": {"content": "<p>In this work we propose a data-driven modelization approach for the \nmanagement of advertising investments of a firm. First, we propose an \napplication of dynamic linear models to the prediction of an economic variable, \nsuch as global sales, which can use information from the environment and the \ninvestment levels of the company in different channels. After we build a robust \nand precise model, we propose a metric of risk, which can help the firm to \nmanage their advertisement plans, thus leading to a robust, risk-aware \noptimization of their revenue. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b812", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.03050"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Uri Shaham, Kelly P. Stanton, Jun Zhao, Huamin Li, Khadir Raddassi, Ruth Montgomery, Yuval Kluger", "title": "Removal of Batch Effects using Distribution-Matching Residual Networks. (arXiv:1610.04181v6 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.04181", "type": "text/html"}], "timestampUsec": "1515563090982574", "comments": [], "summary": {"content": "<p>Sources of variability in experimentally derived data include measurement \nerror in addition to the physical phenomena of interest. This measurement error \nis a combination of systematic components, originating from the measuring \ninstrument, and random measurement errors. Several novel biological \ntechnologies, such as mass cytometry and single-cell RNA-seq, are plagued with \nsystematic errors that may severely affect statistical analysis if the data is \nnot properly calibrated. We propose a novel deep learning approach for removing \nsystematic batch effects. Our method is based on a residual network, trained to \nminimize the Maximum Mean Discrepancy (MMD) between the multivariate \ndistributions of two replicates, measured in different batches. We apply our \nmethod to mass cytometry and single-cell RNA-seq datasets, and demonstrate that \nit effectively attenuates batch effects. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b815", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.04181"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sijia Liu, Pin-Yu Chen, Alfred O. Hero", "title": "Accelerated Distributed Dual Averaging over Evolving Networks of Growing Connectivity. (arXiv:1704.05193v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.05193", "type": "text/html"}], "timestampUsec": "1515563090982573", "comments": [], "summary": {"content": "<p>We consider the problem of accelerating distributed optimization in \nmulti-agent networks by sequentially adding edges. Specifically, we extend the \ndistributed dual averaging (DDA) subgradient algorithm to evolving networks of \ngrowing connectivity and analyze the corresponding improvement in convergence \nrate. It is known that the convergence rate of DDA is influenced by the \nalgebraic connectivity of the underlying network, where better connectivity \nleads to faster convergence. However, the impact of network topology design on \nthe convergence rate of DDA has not been fully understood. In this paper, we \nbegin by designing network topologies via edge selection and scheduling. For \nedge selection, we determine the best set of candidate edges that achieves the \noptimal tradeoff between the growth of network connectivity and the usage of \nnetwork resources. The dynamics of network evolution is then incurred by edge \nscheduling. Further, we provide a tractable approach to analyze the improvement \nin the convergence rate of DDA induced by the growth of network connectivity. \nOur analysis reveals the connection between network topology design and the \nconvergence rate of DDA, and provides quantitative evaluation of DDA \nacceleration for distributed optimization that is absent in the existing \nanalysis. Lastly, numerical experiments show that DDA can be significantly \naccelerated using a sequence of well-designed networks, and our theoretical \npredictions are well matched to its empirical convergence behavior. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b822", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.05193"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Hiroaki Sasaki, Takafumi Kanamori, Aapo Hyv&#xe4;rinen, Gang Niu, Masashi Sugiyama", "title": "Mode-Seeking Clustering and Density Ridge Estimation via Direct Estimation of Density-Derivative-Ratios. (arXiv:1707.01711v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.01711", "type": "text/html"}], "timestampUsec": "1515563090982572", "comments": [], "summary": {"content": "<p>Modes and ridges of the probability density function behind observed data are \nuseful geometric features. Mode-seeking clustering assigns cluster labels by \nassociating data samples with the nearest modes, and estimation of density \nridges enables us to find lower-dimensional structures hidden in data. A key \ntechnical challenge both in mode-seeking clustering and density ridge \nestimation is accurate estimation of the ratios of the first- and second-order \ndensity derivatives to the density. A naive approach takes a three-step \napproach of first estimating the data density, then computing its derivatives, \nand finally taking their ratios. However, this three-step approach can be \nunreliable because a good density estimator does not necessarily mean a good \ndensity derivative estimator, and division by the estimated density could \nsignificantly magnify the estimation error. To cope with these problems, we \npropose a novel estimator for the \\emph{density-derivative-ratios}. The \nproposed estimator does not involve density estimation, but rather \n\\emph{directly} approximates the ratios of density derivatives of any order. \nMoreover, we establish a convergence rate of the proposed estimator. Based on \nthe proposed estimator, new methods both for mode-seeking clustering and \ndensity ridge estimation are developed, and corresponding convergence rates to \nthe mode and ridge of the underlying density are also established. Finally, we \nexperimentally demonstrate that the developed methods significantly outperform \nexisting methods, particularly for relatively high-dimensional data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b829", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.01711"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Morgan A. Schmitz, Matthieu Heitz, Nicolas Bonneel, Fred Maurice Ngol&#xe8; Mboula, David Coeurjolly, Marco Cuturi, Gabriel Peyr&#xe9;, Jean-Luc Starck", "title": "Wasserstein Dictionary Learning: Optimal Transport-based unsupervised non-linear dictionary learning. (arXiv:1708.01955v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.01955", "type": "text/html"}], "timestampUsec": "1515563090982571", "comments": [], "summary": {"content": "<p>This article introduces a new non-linear dictionary learning method for \nhistograms in the probability simplex. The method leverages optimal transport \ntheory, in the sense that our aim is to reconstruct histograms using so called \ndisplacement interpolations (a.k.a. Wasserstein barycenters) between dictionary \natoms; such atoms are themselves synthetic histograms in the probability \nsimplex. Our method simultaneously estimates such atoms, and, for each \ndatapoint, the vector of weights that can optimally reconstruct it as an \noptimal transport barycenter of such atoms. Our method is computationally \ntractable thanks to the addition of an entropic regularization to the usual \noptimal transportation problem, leading to an approximation scheme that is \nefficient, parallel and simple to differentiate. Both atoms and weights are \nlearned using a gradient-based descent method. Gradients are obtained by \nautomatic differentiation of the generalized Sinkhorn iterations that yield \nbarycenters with entropic smoothing. Because of its formulation relying on \nWasserstein barycenters instead of the usual matrix product between dictionary \nand codes, our method allows for non-linear relationships between atoms and the \nreconstruction of input data. We illustrate its application in several \ndifferent image processing settings. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b830", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.01955"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Benjamin Baron, Mirco Musolesi", "title": "Interpretable Machine Learning for Privacy-Preserving Pervasive Systems. (arXiv:1710.08464v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.08464", "type": "text/html"}], "timestampUsec": "1515563090982570", "comments": [], "summary": {"content": "<p>The presence of pervasive systems in our everyday lives and the interaction \nof users with connected devices such as smartphones or home appliances generate \nincreasing amounts of traces that reflect users' behavior. A plethora of \nmachine learning techniques enable service providers to process these traces to \nextract latent information about the users. While most of the existing projects \nhave focused on the accuracy of these techniques, little work has been done on \nthe interpretation of the inference and identification algorithms based on \nthem. In this paper, we propose a machine learning interpretability framework \nfor inference algorithms based on data collected through pervasive systems and \nwe outline the open challenges in this research area. Our interpretability \nframework enable users to understand how the traces they generate could expose \ntheir privacy, while allowing for usable and personalized services at the same \ntime. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515563090983", "annotations": [], "published": 1515563091, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035831b834", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.08464"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ting-Hao &#x27;Kenneth&#x27; Huang, Joseph Chee Chang, Jeffrey P. Bigham", "title": "Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time. (arXiv:1801.02668v1 [cs.HC])", "alternate": [{"href": "http://arxiv.org/abs/1801.02668", "type": "text/html"}], "timestampUsec": "1515560458602299", "comments": [], "summary": {"content": "<p>Crowd-powered conversational assistants have been shown to be more robust \nthan automated systems, but do so at the cost of higher response latency and \nmonetary costs. A promising direction is to combine the two approaches for high \nquality, low latency, and low cost solutions. In this paper, we introduce \nEvorus, a crowd-powered conversational assistant built to automate itself over \ntime by (i) allowing new chatbots to be easily integrated to automate more \nscenarios, (ii) reusing prior crowd answers, and (iii) learning to \nautomatically approve response candidates. Our 5-month-long deployment with 80 \nparticipants and 281 conversations shows that Evorus can automate itself \nwithout compromising conversation quality. Crowd-AI architectures have long \nbeen proposed as a way to reduce cost and latency for crowd-powered systems; \nEvorus demonstrates how automation can be introduced successfully in a deployed \nsystem. Its architecture allows future researchers to make further innovation \non the underlying automated components in the context of a deployed open domain \ndialog system. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515560458602", "annotations": [], "published": 1515560459, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003582c6e5a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02668"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ferenc Galk&#xf3;, Carsten Eickhoff", "title": "Biomedical Question Answering via Weighted Neural Network Passage Retrieval. (arXiv:1801.02832v1 [cs.IR])", "alternate": [{"href": "http://arxiv.org/abs/1801.02832", "type": "text/html"}], "timestampUsec": "1515560458602298", "comments": [], "summary": {"content": "<p>The amount of publicly available biomedical literature has been growing \nrapidly in recent years, yet question answering systems still struggle to \nexploit the full potential of this source of data. In a preliminary processing \nstep, many question answering systems rely on retrieval models for identifying \nrelevant documents and passages. This paper proposes a weighted cosine distance \nretrieval scheme based on neural network word embeddings. Our experiments are \nbased on publicly available data and tasks from the BioASQ biomedical question \nanswering challenge and demonstrate significant performance gains over a wide \nrange of state-of-the-art models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515560458602", "annotations": [], "published": 1515560459, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003582c6e60", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02832"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Igor Adamski, Robert Adamski, Tomasz Grel, Adam J&#x119;drych, Kamil Kaczmarek, Henryk Michalewski", "title": "Distributed Deep Reinforcement Learning: Learn how to play Atari games in 21 minutes. (arXiv:1801.02852v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.02852", "type": "text/html"}], "timestampUsec": "1515560458602297", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32945d6ac\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32945d6ac&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present a study in Distributed Deep Reinforcement Learning (DDRL) focused \non scalability of a state-of-the-art Deep Reinforcement Learning algorithm \nknown as Batch Asynchronous Advantage ActorCritic (BA3C). We show that using \nthe Adam optimization algorithm with a batch size of up to 2048 is a viable \nchoice for carrying out large scale machine learning computations. This, \ncombined with careful reexamination of the optimizer's hyperparameters, using \nsynchronous training on the node level (while keeping the local, single node \npart of the algorithm asynchronous) and minimizing the memory footprint of the \nmodel, allowed us to achieve linear scaling for up to 64 CPU nodes. This \ncorresponds to a training time of 21 minutes on 768 CPU cores, as opposed to 10 \nhours when using a single node with 24 cores achieved by a baseline single-node \nimplementation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515560458602", "annotations": [], "published": 1515560459, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003582c6e6a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02852"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Denghui Zhang, Manling Li, Yantao Jia, Yuanzhuo Wang, Xueqi Cheng", "title": "Efficient Parallel Translating Embedding For Knowledge Graphs. (arXiv:1703.10316v4 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.10316", "type": "text/html"}], "timestampUsec": "1515560458602296", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3294c7140\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3294c7140&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Knowledge graph embedding aims to embed entities and relations of knowledge \ngraphs into low-dimensional vector spaces. Translating embedding methods regard \nrelations as the translation from head entities to tail entities, which achieve \nthe state-of-the-art results among knowledge graph embedding methods. However, \na major limitation of these methods is the time consuming training process, \nwhich may take several days or even weeks for large knowledge graphs, and \nresult in great difficulty in practical applications. In this paper, we propose \nan efficient parallel framework for translating embedding methods, called \nParTrans-X, which enables the methods to be paralleled without locks by \nutilizing the distinguished structures of knowledge graphs. Experiments on two \ndatasets with three typical translating embedding methods, i.e., TransE [3], \nTransH [17], and a more efficient variant TransE- AdaGrad [10] validate that \nParTrans-X can speed up the training process by more than an order of \nmagnitude. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515560458602", "annotations": [], "published": 1515560459, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003582c6e6c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.10316"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, Ruslan Salakhutdinov", "title": "Gated-Attention Architectures for Task-Oriented Language Grounding. (arXiv:1706.07230v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.07230", "type": "text/html"}], "timestampUsec": "1515560458602295", "comments": [], "summary": {"content": "<p>To perform tasks specified by natural language instructions, autonomous \nagents need to extract semantically meaningful representations of language and \nmap it to visual elements and actions in the environment. This problem is \ncalled task-oriented language grounding. We propose an end-to-end trainable \nneural architecture for task-oriented language grounding in 3D environments \nwhich assumes no prior linguistic or perceptual knowledge and requires only raw \npixels from the environment and the natural language instruction as input. The \nproposed model combines the image and text representations using a \nGated-Attention mechanism and learns a policy to execute the natural language \ninstruction using standard reinforcement and imitation learning methods. We \nshow the effectiveness of the proposed model on unseen instructions as well as \nunseen maps, both quantitatively and qualitatively. We also introduce a novel \nenvironment based on a 3D game engine to simulate the challenges of \ntask-oriented language grounding over a rich set of instructions and \nenvironment states. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515560458602", "annotations": [], "published": 1515560459, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003582c6e6f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.07230"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1516772882, "author": "Marco Singh, Akshay Pai", "title": "Boundary Optimizing Network (BON). (arXiv:1801.02642v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1801.02642", "type": "text/html"}], "timestampUsec": "1515560458602292", "comments": [], "summary": {"content": "<p>Despite all the success that deep neural networks have seen in classifying \ncertain datasets, the challenge of finding optimal solutions that generalize \nstill remains. In this paper, we propose the Boundary Optimizing Network (BON), \na new approach to generalization for deep neural networks when used for \nsupervised learning. Given a classification network, we propose to use a \ncollaborative generative network that produces new synthetic data points in the \nform of perturbations of original data points. In this way, we create a data \nsupport around each original data point which prevents decision boundaries from \npassing too close to the original data points, i.e. prevents overfitting. We \nshow that BON improves convergence on CIFAR-10 using the state-of-the-art \nDensenet. We do however observe that the generative network suffers from \ncatastrophic forgetting during training, and we therefore propose to use a \nvariation of Memory Aware Synapses to optimize the generative network (called \nBON++). On the Iris dataset, we visualize the effect of BON++ when the \ngenerator does not suffer from catastrophic forgetting and conclude that the \napproach has the potential to create better boundaries in a higher dimensional \nspace. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515560458602", "annotations": [], "published": 1516772881, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003582c6e7e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02642"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yongshuai Liu, Jiyu Chen, Hao Chen", "title": "Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks. (arXiv:1801.02850v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1801.02850", "type": "text/html"}], "timestampUsec": "1515560458602291", "comments": [], "summary": {"content": "<p>Deep neural networks are vulnerable to adversarial examples. Prior defenses \nattempted to make deep networks more robust by either improving the network \narchitecture or adding adversarial examples into the training set, with their \nrespective limitations. We propose a new direction. Motivated by recent \nresearch that shows that outliers in the training set have a high negative \ninfluence on the trained model, our approach makes the model more robust by \ndetecting and removing outliers in the training set without modifying the \nnetwork architecture or requiring adversarial examples. We propose two methods \nfor detecting outliers based on canonical examples and on training errors, \nrespectively. After removing the outliers, we train the classifier with the \nremaining examples to obtain a sanitized model. Our evaluation shows that the \nsanitized model improves classification accuracy and forces the attacks to \ngenerate adversarial examples with higher distortions. Moreover, the \nKullback-Leibler divergence from the output of the original model to that of \nthe sanitized model allows us to distinguish between normal and adversarial \nexamples reliably. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515560458602", "annotations": [], "published": 1515560459, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003582c6e8b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02850"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hiroshi Inoue", "title": "Data Augmentation by Pairing Samples for Images Classification. (arXiv:1801.02929v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.02929", "type": "text/html"}], "timestampUsec": "1515560458602290", "comments": [], "summary": {"content": "<p>Data augmentation is a widely used technique in many machine learning tasks, \nsuch as image classification, to virtually enlarge the training dataset size \nand avoid overfitting. Traditional data augmentation techniques for image \nclassification tasks create new samples from the original training data by, for \nexample, flipping, distorting, adding a small amount of noise to, or cropping a \npatch from an original image. In this paper, we introduce a simple but \nsurprisingly effective data augmentation technique for image classification \ntasks. With our technique, named SamplePairing, we synthesize a new sample from \none image by overlaying another image randomly chosen from the training data \n(i.e., taking an average of two images for each pixel). By using two images \nrandomly selected from the training set, we can generate $N^2$ new samples from \n$N$ training samples. This simple data augmentation technique significantly \nimproved classification accuracy for all the tested datasets; for example, the \ntop-1 error rate was reduced from 33.5% to 29.0% for the ILSVRC 2012 dataset \nwith GoogLeNet and from 8.22% to 6.93% in the CIFAR-10 dataset. We also show \nthat our SamplePairing technique largely improved accuracy when the number of \nsamples in the training set was very small. Therefore, our technique is more \nvaluable for tasks with a limited amount of training data, such as medical \nimaging tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515560458602", "annotations": [], "published": 1515560459, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003582c6e96", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02929"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Patrick T. Komiske, Eric M. Metodiev, Benjamin Nachman, Matthew D. Schwartz", "title": "Pileup Mitigation with Machine Learning (PUMML). (arXiv:1707.08600v3 [hep-ph] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.08600", "type": "text/html"}], "timestampUsec": "1515560458602289", "comments": [], "summary": {"content": "<p>Pileup involves the contamination of the energy distribution arising from the \nprimary collision of interest (leading vertex) by radiation from soft \ncollisions (pileup). We develop a new technique for removing this contamination \nusing machine learning and convolutional neural networks. The network takes as \ninput the energy distribution of charged leading vertex particles, charged \npileup particles, and all neutral particles and outputs the energy distribution \nof particles coming from leading vertex alone. The PUMML algorithm performs \nremarkably well at eliminating pileup distortion on a wide range of simple and \ncomplex jet observables. We test the robustness of the algorithm in a number of \nways and discuss how the network can be trained directly on data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515560458602", "annotations": [], "published": 1515560459, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003582c6ea1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.08600"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Seyyed Hossein Hasanpour, Mohammad Rouhani, Mohsen Fayyaz, Mohammad Sabokrou", "title": "Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures. (arXiv:1608.06037v5 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1608.06037", "type": "text/html"}], "timestampUsec": "1515477220079227", "comments": [], "summary": {"content": "<p>Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, \nResNet, GoogleNet, include tens to hundreds of millions of parameters, which \nimpose considerable computation and memory overhead. This limits their \npractical use for training, optimization and memory efficiency. On the \ncontrary, light-weight architectures, being proposed to address this issue, \nmainly suffer from low accuracy. These inefficiencies mostly stem from \nfollowing an ad hoc procedure. We propose a simple architecture, called \nSimpleNet, based on a set of designing principles and we empirically show that \nSimpleNet provides a good tradeoff between the computation/memory efficiency \nand the accuracy. Our simple 13-layer architecture outperforms most of the \ndeeper and complex architectures to date such as VGGNet, ResNet, and GoogleNet \non several well-known benchmarks while having 2 to 25 times fewer number of \nparameters and operations. This makes it very handy for embedded system or \nsystem with computational and memory limitations. We achieved state-of-the-art \nresult on CIFAR10 outperforming several heavier architectures, near state of \nthe art on MNIST and competitive results on CIFAR100 and SVHN. Models are made \navailable at: https://github.com/Coderx7/SimpleNet \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d5895", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1608.06037"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shuai Tang, Hailin Jin, Chen Fang, Zhaowen Wang, Virginia R. de Sa", "title": "Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning. (arXiv:1710.10380v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.10380", "type": "text/html"}], "timestampUsec": "1515477220079225", "comments": [], "summary": {"content": "<p>Context information plays an important role in human language understanding, \nand it is also useful for machines to learn vector representations of language. \nIn this paper, we explore an asymmetric encoder-decoder structure for \nunsupervised context-based sentence representation learning. As a result, we \nbuild an encoder-decoder architecture with an RNN encoder and a CNN decoder. We \nfurther combine a suite of effective designs to significantly improve model \nefficiency while also achieving better performance. Our model is trained on two \ndifferent large unlabeled corpora, and in both cases transferability is \nevaluated on a set of downstream language understanding tasks. We empirically \nshow that our model is simple and fast while producing rich sentence \nrepresentations that excel in downstream tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d589e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.10380"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nicholas Carlini, David Wagner", "title": "Audio Adversarial Examples: Targeted Attacks on Speech-to-Text. (arXiv:1801.01944v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.01944", "type": "text/html"}], "timestampUsec": "1515477220079222", "comments": [], "summary": {"content": "<p>We construct targeted audio adversarial examples on automatic speech \nrecognition. Given any audio waveform, we can produce another that is over \n99.9% similar, but transcribes as any phrase we choose (at a rate of up to 50 \ncharacters per second). We apply our iterative optimization-based attack to \nMozilla's implementation DeepSpeech end-to-end, and show it has a 100% success \nrate. The feasibility of this attack introduce a new domain to study \nadversarial examples. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d58a6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01944"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Heung-Yeung Shum, Xiaodong He, Di Li", "title": "From Eliza to XiaoIce: Challenges and Opportunities with Social Chatbots. (arXiv:1801.01957v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.01957", "type": "text/html"}], "timestampUsec": "1515477220079221", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3294c7519\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3294c7519&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Conversational systems have come a long way after decades of research and \ndevelopment, from Eliza and Parry in the 60's and 70's, to task-completion \nsystems as in the ATIS project, to intelligent personal assistants such as \nSiri, and to today's social chatbots like XiaoIce. Social chatbots' appeal lies \nin not only their ability to respond to users' diverse requests, but also in \nbeing able to establish an emotional connection with users. The latter is done \nby satisfying the users' essential needs for communication, affection, and \nsocial belonging. The design of social chatbots must focus on user engagement \nand take both intellectual quotient (IQ) and emotional quotient (EQ) into \naccount. Users should want to engage with the social chatbot; as such, we \ndefine the success metric for social chatbots as conversation-turns per session \n(CPS). Using XiaoIce as an illustrative example, we discuss key technologies in \nbuilding social chatbots from core chat to visual sense to skills. We also show \nhow XiaoIce can dynamically recognize emotion and engage the user throughout \nlong conversations with appropriate interpersonal responses. As we become the \nfirst generation of humans ever living with AI, social chatbots that are \nwell-designed to be both useful and empathic will soon be ubiquitous. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d58ad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01957"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Daichi Nishio, Satoshi Yamane", "title": "Faster Deep Q-learning using Neural Episodic Control. (arXiv:1801.01968v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.01968", "type": "text/html"}], "timestampUsec": "1515477220079220", "comments": [], "summary": {"content": "<p>The Research on deep reinforcement learning to estimate Q-value by deep \nlearning has been active in recent years. In deep reinforcement learning, it is \nimportant to efficiently learn the experiences that a agent has collected by \nexploring the environment. In this research, we propose NEC2DQN that improves \nlearning speed of a algorithm with poor sample efficiency by using a algorithm \nwith good one at the beginning of learning, and we demonstrate it in \nexperiments. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d58b3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01968"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zecang Gu, Ling Dong", "title": "Distance formulas capable of unifying Euclidian space and probability space. (arXiv:1801.01972v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.01972", "type": "text/html"}], "timestampUsec": "1515477220079219", "comments": [], "summary": {"content": "<p>For pattern recognition like image recognition, it has become clear that each \nmachine-learning dictionary data actually became data in probability space \nbelonging to Euclidean space. However, the distances in the Euclidean space and \nthe distances in the probability space are separated and ununified when machine \nlearning is introduced in the pattern recognition. There is still a problem \nthat it is impossible to directly calculate an accurate matching relation \nbetween the sampling data of the read image and the learned dictionary data. In \nthis research, we focused on the reason why the distance is changed and the \nextent of change when passing through the probability space from the original \nEuclidean distance among data belonging to multiple probability spaces \ncontaining Euclidean space. By finding the reason of the cause of the distance \nerror and finding the formula expressing the error quantitatively, a possible \ndistance formula to unify Euclidean space and probability space is found. Based \non the results of this research, the relationship between machine-learning \ndictionary data and sampling data was clearly understood for pattern \nrecognition. As a result, the calculation of collation among data and \nmachine-learning to compete mutually between data are cleared, and complicated \ncalculations became unnecessary. Finally, using actual pattern recognition \ndata, experimental demonstration of a possible distance formula to unify \nEuclidean space and probability space discovered by this research was carried \nout, and the effectiveness of the result was confirmed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d58b8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01972"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Justinas Miseikis, Patrick Knobelreiter, Inka Brijacak, Saeed Yahyanejad, Kyrre Glette, Ole Jakob Elle, Jim Torresen", "title": "Robot Localisation and 3D Position Estimation Using a Free-Moving Camera and Cascaded Convolutional Neural Networks. (arXiv:1801.02025v1 [cs.RO])", "alternate": [{"href": "http://arxiv.org/abs/1801.02025", "type": "text/html"}], "timestampUsec": "1515477220079218", "comments": [], "summary": {"content": "<p>Many works in collaborative robotics and human-robot interaction focuses on \nidentifying and predicting human behaviour while considering the information \nabout the robot itself as given. This can be the case when sensors and the \nrobot are calibrated in relation to each other and often the reconfiguration of \nthe system is not possible, or extra manual work is required. We present a deep \nlearning based approach to remove the constraint of having the need for the \nrobot and the vision sensor to be fixed and calibrated in relation to each \nother. The system learns the visual cues of the robot body and is able to \nlocalise it, as well as estimate the position of robot joints in 3D space by \njust using a 2D color image. The method uses a cascaded convolutional neural \nnetwork, and we present the structure of the network, describe our own \ncollected dataset, explain the network training and achieved results. A fully \ntrained system shows promising results in providing an accurate mask of where \nthe robot is located and a good estimate of its joints positions in 3D. The \naccuracy is not good enough for visual servoing applications yet, however, it \ncan be sufficient for general safety and some collaborative tasks not requiring \nvery high precision. The main benefit of our method is the possibility of the \nvision sensor to move freely. This allows it to be mounted on moving objects, \nfor example, a body of the person or a mobile robot working in the same \nenvironment as the robots are operating in. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d58c0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02025"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Benjamin Spector, Serge Belongie", "title": "Sample-Efficient Reinforcement Learning through Transfer and Architectural Priors. (arXiv:1801.02268v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.02268", "type": "text/html"}], "timestampUsec": "1515477220079217", "comments": [], "summary": {"content": "<p>Recent work in deep reinforcement learning has allowed algorithms to learn \ncomplex tasks such as Atari 2600 games just from the reward provided by the \ngame, but these algorithms presently require millions of training steps in \norder to learn, making them approximately five orders of magnitude slower than \nhumans. One reason for this is that humans build robust shared representations \nthat are applicable to collections of problems, making it much easier to \nassimilate new variants. This paper first introduces the idea of \nautomatically-generated game sets to aid in transfer learning research, and \nthen demonstrates the utility of shared representations by showing that models \ncan substantially benefit from the incorporation of relevant architectural \npriors. This technique affords a remarkable 50x positive transfer on a toy \nproblem-set. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d58c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02268"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Vatsal Mahajan", "title": "Winograd Schema - Knowledge Extraction Using Narrative Chains. (arXiv:1801.02281v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.02281", "type": "text/html"}], "timestampUsec": "1515477220079216", "comments": [], "summary": {"content": "<p>The Winograd Schema Challenge (WSC) is a test of machine intelligence, \ndesigned to be an improvement on the Turing test. A Winograd Schema consists of \na sentence and a corresponding question. To successfully answer these \nquestions, one requires the use of commonsense knowledge and reasoning. This \nwork focuses on extracting common sense knowledge which can be used to generate \nanswers for the Winograd schema challenge. Common sense knowledge is extracted \nbased on events (or actions) and their participants; called Event-Based \nConditional Commonsense (ECC). I propose an approach using Narrative Event \nChains [Chambers et al., 2008] to extract ECC knowledge. These are stored in \ntemplates, to be later used for answering the WSC questions. This approach \nworks well with respect to a subset of WSC tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d58cf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02281"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yunlong Mi, Yong Shi, Jinhai Li, Jiabin Liu, biao Li", "title": "A generalized concept-cognitive learning: A machine learning viewpoint. (arXiv:1801.02334v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.02334", "type": "text/html"}], "timestampUsec": "1515477220079215", "comments": [], "summary": {"content": "<p>Concept-cognitive learning (CCL) is a hot topic in recent years, and it has \nattracted much attention from the communities of formal concept analysis, \ngranular computing and cognitive computing. However, the relationship among \ncognitive computing (CC), conceptcognitive computing (CCC), and CCL is not \nclearly described. To this end, we explain the relationship of CC, CCC, and \nCCL. Then, we propose a generalized CCL from the point of view of machine \nlearning. Finally, experiments on seven data sets are conducted to evaluate \nconcept formation and concept-cognitive processes of the proposed generalized \nCCL. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d58d5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02334"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wilson Cai, Anish Doshi, Rafael Valle", "title": "Attacking Speaker Recognition With Deep Generative Models. (arXiv:1801.02384v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1801.02384", "type": "text/html"}], "timestampUsec": "1515477220079214", "comments": [], "summary": {"content": "<p>In this paper we investigate the ability of generative adversarial networks \n(GANs) to synthesize spoofing attacks on modern speaker recognition systems. We \nfirst show that samples generated with SampleRNN and WaveNet are unable to fool \na CNN-based speaker recognition system. We propose a modification of the \nWasserstein GAN objective function to make use of data that is real but not \nfrom the class being learned. Our semi-supervised learning method is able to \nperform both targeted and untargeted attacks, raising questions related to \nsecurity in speaker authentication systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d58e3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02384"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Meysam Golmohammadi, Saeedeh Ziyabari, Vinit Shah, Eva Von Weltin, Christopher Campbell, Iyad Obeid, Joseph Picone", "title": "Gated Recurrent Networks for Seizure Detection. (arXiv:1801.02471v1 [eess.SP])", "alternate": [{"href": "http://arxiv.org/abs/1801.02471", "type": "text/html"}], "timestampUsec": "1515477220079213", "comments": [], "summary": {"content": "<p>Recurrent Neural Networks (RNNs) with sophisticated units that implement a \ngating mechanism have emerged as powerful technique for modeling sequential \nsignals such as speech or electroencephalography (EEG). The latter is the focus \non this paper. A significant big data resource, known as the TUH EEG Corpus \n(TUEEG), has recently become available for EEG research, creating a unique \nopportunity to evaluate these recurrent units on the task of seizure detection. \nIn this study, we compare two types of recurrent units: long short-term memory \nunits (LSTM) and gated recurrent units (GRU). These are evaluated using a state \nof the art hybrid architecture that integrates Convolutional Neural Networks \n(CNNs) with RNNs. We also investigate a variety of initialization methods and \nshow that initialization is crucial since poorly initialized networks cannot be \ntrained. Furthermore, we explore regularization of these convolutional gated \nrecurrent networks to address the problem of overfitting. Our experiments \nrevealed that convolutional LSTM networks can achieve significantly better \nperformance than convolutional GRU networks. The convolutional LSTM \narchitecture with proper initialization and regularization delivers 30% \nsensitivity at 6 false alarms per 24 hours. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d58ec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02471"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yi Tay, Anh Tuan Luu, Siu Cheung Hui", "title": "Latent Relational Metric Learning via Memory-based Attention for Collaborative Ranking. (arXiv:1707.05176v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.05176", "type": "text/html"}], "timestampUsec": "1515477220079212", "comments": [], "summary": {"content": "<p>This paper proposes a new neural architecture for collaborative ranking with \nimplicit feedback. Our model, LRML (\\textit{Latent Relational Metric Learning}) \nis a novel metric learning approach for recommendation. More specifically, \ninstead of simple push-pull mechanisms between user and item pairs, we propose \nto learn latent relations that describe each user item interaction. This helps \nto alleviate the potential geometric inflexibility of existing metric learing \napproaches. This enables not only better performance but also a greater extent \nof modeling capability, allowing our model to scale to a larger number of \ninteractions. In order to do so, we employ a augmented memory module and learn \nto attend over these memory blocks to construct latent relations. The \nmemory-based attention module is controlled by the user-item interaction, \nmaking the learned relation vector specific to each user-item pair. Hence, this \ncan be interpreted as learning an exclusive and optimal relational translation \nfor each user-item interaction. The proposed architecture demonstrates the \nstate-of-the-art performance across multiple recommendation benchmarks. LRML \noutperforms other metric learning models by $6\\%-7.5\\%$ in terms of Hits@10 and \nnDCG@10 on large datasets such as Netflix and MovieLens20M. Moreover, \nqualitative studies also demonstrate evidence that our proposed model is able \nto infer and encode explicit sentiment, temporal and attribute information \ndespite being only trained on implicit feedback. As such, this ascertains the \nability of LRML to uncover hidden relational structure within implicit \ndatasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d58f3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.05176"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lior Deutsch", "title": "Generating Neural Networks with Neural Networks. (arXiv:1801.01952v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.01952", "type": "text/html"}], "timestampUsec": "1515477220079211", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3294c7906\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3294c7906&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Hypernetworks are neural networks that transform a random input vector into \nweights for a specified target neural network. We formulate the hypernetwork \ntraining objective as a compromise between accuracy and diversity, where the \ndiversity takes into account trivial symmetry transformations of the target \nnetwork. We show that this formulation naturally arises as a relaxation of an \noptimistic probability distribution objective for the generated networks, and \nwe explain how it is related to variational inference. We use multi-layered \nperceptrons to form the mapping from the low dimensional input random vector to \nthe high dimensional weight space, and demonstrate how to reduce the number of \nparameters in this mapping by weight sharing. We perform experiments on a four \nlayer convolutional target network which classifies MNIST images, and show that \nthe generated weights are diverse and have interesting distributions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d58fc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01952"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shane Barratt, Rishi Sharma", "title": "A Note on the Inception Score. (arXiv:1801.01973v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.01973", "type": "text/html"}], "timestampUsec": "1515477220079210", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329543997\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329543997&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep generative models are powerful tools that have produced impressive \nresults in recent years. These advances have been for the most part empirically \ndriven, making it essential that we use high quality evaluation metrics. In \nthis paper, we provide new insights into the Inception Score, a recently \nproposed and widely used evaluation metric for generative models, and \ndemonstrate that it fails to provide useful guidance when comparing models. We \ndiscuss both suboptimalities of the metric itself and issues with its \napplication. Finally, we call for researchers to be more systematic and careful \nwhen evaluating and comparing generative models, as the advancement of the \nfield depends upon it. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d5904", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01973"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Amirreza Mahdavi-Shahri, Mahboobeh Houshmand, Mahdi Yaghoobi, Mehrdad Jalali", "title": "Applying an Ensemble Learning Method for Improving Multi-label Classification Performance. (arXiv:1801.02149v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.02149", "type": "text/html"}], "timestampUsec": "1515477220079209", "comments": [], "summary": {"content": "<p>In recent years, multi-label classification problem has become a \ncontroversial issue. In this kind of classification, each sample is associated \nwith a set of class labels. Ensemble approaches are supervised learning \nalgorithms in which an operator takes a number of learning algorithms, namely \nbase-level algorithms and combines their outcomes to make an estimation. The \nsimplest form of ensemble learning is to train the base-level algorithms on \nrandom subsets of data and then let them vote for the most popular \nclassifications or average the predictions of the base-level algorithms. In \nthis study, an ensemble learning method is proposed for improving multi-label \nclassification evaluation criteria. We have compared our method with well-known \nbase-level algorithms on some data sets. Experiment results show the proposed \napproach outperforms the base well-known classifiers for the multi-label \nclassification problem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d590a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02149"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Atsushi Nitanda, Taiji Suzuki", "title": "Gradient Layer: Enhancing the Convergence of Adversarial Training for Generative Models. (arXiv:1801.02227v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.02227", "type": "text/html"}], "timestampUsec": "1515477220079208", "comments": [], "summary": {"content": "<p>We propose a new technique that boosts the convergence of training generative \nadversarial networks. Generally, the rate of training deep models reduces \nseverely after multiple iterations. A key reason for this phenomenon is that a \ndeep network is expressed using a highly non-convex finite-dimensional model, \nand thus the parameter gets stuck in a local optimum. Because of this, methods \noften suffer not only from degeneration of the convergence speed but also from \nlimitations in the representational power of the trained network. To overcome \nthis issue, we propose an additional layer called the gradient layer to seek a \ndescent direction in an infinite-dimensional space. Because the layer is \nconstructed in the infinite-dimensional space, we are not restricted by the \nspecific model structure of finite-dimensional models. As a result, we can get \nout of the local optima in finite-dimensional models and move towards the \nglobal optimal function more directly. In this paper, this phenomenon is \nexplained from the functional gradient method perspective of the gradient \nlayer. Interestingly, the optimization procedure using the gradient layer \nnaturally constructs the deep structure of the network. Moreover, we \ndemonstrate that this procedure can be regarded as a discretization method of \nthe gradient flow that naturally reduces the objective function. Finally, the \nmethod is tested using several numerical experiments, which show its fast \nconvergence. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d590e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02227"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Han Zhu, Pengye Zhang, Guozheng Li, Jie He, Han Li, Kun Gai", "title": "Learning Tree-based Deep Model for Recommender Systems. (arXiv:1801.02294v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.02294", "type": "text/html"}], "timestampUsec": "1515477220079207", "comments": [], "summary": {"content": "<p>We propose a novel recommendation method based on tree. With user behavior \ndata, the tree based model can capture user interests from coarse to fine, by \ntraversing nodes top down and make decisions whether to pick up each node to \nuser. Compared to traditional model-based methods like matrix factorization \n(MF), our tree based model does not have to fetch and estimate each item in the \nentire set. Instead, candidates are drawn from subsets corresponding to user's \nhigh-level interests, which is defined by the tree structure. Meanwhile, \nfinding candidates from the entire corpus brings more novelty than \ncontent-based approaches like item-based collaborative filtering.Moreover, in \nthis paper, we show that the tree structure can also act to refine user \ninterests distribution, to benefit both training and prediction. The \nexperimental results in both open dataset and Taobao display advertising \ndataset indicate that the proposed method outperforms existing methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d5913", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02294"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yu Cheng, Angus Wong, Kevin Hung, Zhizhong Li, Weitong Li", "title": "Deep Nearest Class Mean Model for Incremental Odor Classification. (arXiv:1801.02328v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.02328", "type": "text/html"}], "timestampUsec": "1515477220079206", "comments": [], "summary": {"content": "<p>In recent years, more and more machine learning algorithms have been applied \nto odor recognition. These odor recognition algorithms usually assume that the \ntraining dataset is static. However, for some odor recognition tasks, the odor \ndataset is dynamically growing where not only the training samples but also the \nnumber of classes increase over time. Motivated by this concern, we proposed a \ndeep nearest class mean (DNCM) model which combines the deep learning framework \nand nearest class mean (NCM) method. DNCM not only can leverage deep neural \nnetwork to extract deep features, but also well suited for integrating new \nclasses. Experiments demonstrate that the proposed DNCM model is effective and \nefficient for incremental odor classification, especially for new classes with \nonly a small number of training examples. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d591a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02328"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, Dawn Song", "title": "Generating adversarial examples with adversarial networks. (arXiv:1801.02610v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1801.02610", "type": "text/html"}], "timestampUsec": "1515477220079205", "comments": [], "summary": {"content": "<p>Deep neural networks (DNNs) have been found to be vulnerable to adversarial \nexamples resulting from adding small-magnitude perturbations to inputs. Such \nadversarial examples can mislead DNNs to produce adversary-selected results. \nDifferent attack strategies have been proposed to generate adversarial \nexamples, but how to produce them with high perceptual quality and more \nefficiently requires more research efforts. In this paper, we propose AdvGAN to \ngenerate adversarial examples with generative adversarial networks (GANs), \nwhich can learn and approximate the distribution of original instances. For \nAdvGAN, once the generator is trained, it can generate adversarial \nperturbations efficiently for any instance, so as to potentially accelerate \nadversarial training as defenses. We apply AdvGAN in both semi-whitebox and \nblack-box attack settings. In semi-whitebox attacks, there is no need to access \nthe original target model after the generator is trained, in contrast to \ntraditional white-box attacks. In black-box attacks, we dynamically train a \ndistilled model for the black-box model and optimize the generator accordingly. \nAdversarial examples generated by AdvGAN on different target models have high \nattack success rate under state-of-the-art defenses compared to other attacks. \nOur attack has placed the first with 92.76% accuracy on a public MNIST \nblack-box attack challenge. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515477220079", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576d591c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02610"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Carola Doerr", "title": "Complexity Theory for Discrete Black-Box Optimization Heuristics. (arXiv:1801.02037v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.02037", "type": "text/html"}], "timestampUsec": "1515476480138126", "comments": [], "summary": {"content": "<p>A predominant topic in the theory of evolutionary algorithms and, more \ngenerally, theory of randomized black-box optimization techniques is running \ntime analysis. Running time analysis aims at understanding the performance of a \ngiven heuristic on a given problem by bounding the number of function \nevaluations that are needed by the heuristic to identify a solution of a \ndesired quality. As in general algorithms theory, this running time perspective \nis most useful when it is complemented by a meaningful complexity theory that \nstudies the limits of algorithmic solutions. \n</p> \n<p>In the context of discrete black-box optimization, several black-box \ncomplexity models have been developed to analyze the best possible performance \nthat a black-box optimization algorithm can achieve on a given problem. The \nmodels differ in the classes of algorithms to which these lower bounds apply. \nThis way, black-box complexity contributes to a better understanding of how \ncertain algorithmic choices (such as the amount of memory used by a heuristic, \nits selective pressure, or properties of the strategies that it uses to create \nnew solution candidates) influences performance. \n</p> \n<p>In this chapter we review the different black-box complexity models that have \nbeen proposed in the literature, survey the bounds that have been obtained for \nthese models, and discuss how the interplay of running time analysis and \nblack-box complexity can inspire new algorithmic solutions to well-researched \nproblems in evolutionary computation. We also discuss in this chapter several \ninteresting open questions for future work. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be69f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02037"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1515735585, "author": "Homayoun Hamedmoghadam, Nima Joorabloo, Mahdi Jalili", "title": "Australia's long-term electricity demand forecasting using deep neural networks. (arXiv:1801.02148v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1801.02148", "type": "text/html"}], "timestampUsec": "1515476480138125", "comments": [], "summary": {"content": "<p>Accurate prediction of long-term electricity demand has a significant role in \ndemand side management and electricity network planning and operation. Demand \nover-estimation results in over-investment in network assets, driving up the \nelectricity prices, while demand under-estimation may lead to under-investment \nresulting in unreliable and insecure electricity. In this manuscript, we apply \ndeep neural networks to predict Australia's long-term electricity demand. A \nstacked autoencoder is used in combination with multilayer perceptrons or \ncascade-forward multilayer perceptrons to predict the nation-wide electricity \nconsumption rates for 1-24 months ahead of time. The experimental results show \nthat the deep structures have better performance than classical neural \nnetworks, especially for 12-month to 24-month prediction horizon. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515735585, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6a4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02148"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ahmad B. A. Hassanat, Esra&#x27;a Alkafaween", "title": "On Enhancing Genetic Algorithms Using New Crossovers. (arXiv:1801.02335v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.02335", "type": "text/html"}], "timestampUsec": "1515476480138124", "comments": [], "summary": {"content": "<p>This paper investigates the use of more than one crossover operator to \nenhance the performance of genetic algorithms. Novel crossover operators are \nproposed such as the Collision crossover, which is based on the physical rules \nof elastic collision, in addition to proposing two selection strategies for the \ncrossover operators, one of which is based on selecting the best crossover \noperator and the other randomly selects any operator. Several experiments on \nsome Travelling Salesman Problems (TSP) have been conducted to evaluate the \nproposed methods, which are compared to the well-known Modified crossover \noperator and partially mapped Crossover (PMX) crossover. The results show the \nimportance of some of the proposed methods, such as the collision crossover, in \naddition to the significant enhancement of the genetic algorithms performance, \nparticularly when using more than one crossover operator. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6a6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02335"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ella M. Gale", "title": "Spiking memristor logic gates are a type of time-variant perceptron. (arXiv:1801.02508v1 [cs.ET])", "alternate": [{"href": "http://arxiv.org/abs/1801.02508", "type": "text/html"}], "timestampUsec": "1515476480138123", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329543d43\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329543d43&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Memristors are low-power memory-holding resistors thought to be useful for \nneuromophic computing, which can compute via spike-interactions mediated \nthrough the device's short-term memory. Using interacting spikes, it is \npossible to build an AND gate that computes OR at the same time, similarly a \nfull adder can be built that computes the arithmetical sum of its inputs. Here \nwe show how these gates can be understood by modelling the memristors as a \nnovel type of perceptron: one which is sensitive to input order. The \nmemristor's memory can change the input weights for later inputs, and thus the \nmemristor gates cannot be accurately described by a single perceptron, \nrequiring either a network of time-invarient perceptrons or a complex \ntime-varying self-reprogrammable perceptron. This work demonstrates the high \nfunctionality of memristor logic gates, and also that the addition of \ntheasholding could enable the creation of a standard perceptron in hardware, \nwhich may have use in building neural net chips. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6a9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02508"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Enrique Romero Merino, Ferran Mazzanti Castrillejo, Jordi Delgado Pin, David Buchaca Prats", "title": "Weighted Contrastive Divergence. (arXiv:1801.02567v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.02567", "type": "text/html"}], "timestampUsec": "1515476480138122", "comments": [], "summary": {"content": "<p>Learning algorithms for energy based Boltzmann architectures that rely on \ngradient descent are in general computationally prohibitive, typically due to \nthe exponential number of terms involved in computing the partition function. \nIn this way one has to resort to approximation schemes for the evaluation of \nthe gradient. This is the case of Restricted Boltzmann Machines (RBM) and its \nlearning algorithm Contrastive Divergence (CD). It is well-known that CD has a \nnumber of shortcomings, and its approximation to the gradient has several \ndrawbacks. Overcoming these defects has been the basis of much research and new \nalgorithms have been devised, such as persistent CD. In this manuscript we \npropose a new algorithm that we call Weighted CD (WCD), built from small \nmodifications of the negative phase in standard CD. However small these \nmodifications may be, experimental work reported in this paper suggest that WCD \nprovides a significant improvement over standard CD and persistent CD at a \nsmall additional computational cost. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6b1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02567"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jacob Andreas, Anca Dragan, Dan Klein", "title": "Translating Neuralese. (arXiv:1704.06960v4 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.06960", "type": "text/html"}], "timestampUsec": "1515476480138121", "comments": [], "summary": {"content": "<p>Several approaches have recently been proposed for learning decentralized \ndeep multiagent policies that coordinate via a differentiable communication \nchannel. While these policies are effective for many tasks, interpretation of \ntheir induced communication strategies has remained a challenge. Here we \npropose to interpret agents' messages by translating them. Unlike in typical \nmachine translation problems, we have no parallel data to learn from. Instead \nwe develop a translation model based on the insight that agent messages and \nnatural language strings mean the same thing if they induce the same belief \nabout the world in a listener. We present theoretical guarantees and empirical \nevidence that our approach preserves both the semantics and pragmatics of \nmessages by ensuring that players communicating through a translation layer do \nnot suffer a substantial loss in reward relative to players with a common \nlanguage. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6b6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.06960"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Emmanuel Dauc&#xe9;", "title": "Toward predictive machine learning for active vision. (arXiv:1710.10460v3 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.10460", "type": "text/html"}], "timestampUsec": "1515476480138120", "comments": [], "summary": {"content": "<p>We develop a comprehensive description of the active inference framework, as \nproposed by Friston (2010), under a machine-learning compliant perspective. \nStemming from a biological inspiration and the auto-encoding principles, the \nsketch of a cognitive architecture is proposed that should provide ways to \nimplement estimation-oriented control policies. Computer simulations illustrate \nthe effectiveness of the approach through a foveated inspection of the input \ndata. The pros and cons of the control policy are analyzed in detail, showing \ninteresting promises in terms of processing compression. Though optimizing \nfuture posterior entropy over the actions set is shown enough to attain locally \noptimal action selection, offline calculation using class-specific saliency \nmaps is shown better for it saves processing costs through saccades pathways \npre-processing, with a negligible effect on the recognition/compression rates. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6bc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.10460"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Thiago Serra, Christian Tjandraatmadja, Srikumar Ramalingam", "title": "Bounding and Counting Linear Regions of Deep Neural Networks. (arXiv:1711.02114v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.02114", "type": "text/html"}], "timestampUsec": "1515476480138119", "comments": [], "summary": {"content": "<p>In this paper, we study the representational power of deep neural networks \n(DNN) that belong to the family of piecewise-linear (PWL) functions, based on \nPWL activation units such as rectifier or maxout. We investigate the complexity \nof such networks by studying the number of linear regions of the PWL function. \nTypically, a PWL function from a DNN can be seen as a large family of linear \nfunctions acting on millions of such regions. We directly build upon the work \nof Montufar et al. (2014), Montufar (2017) and Raghu et al. (2017) by refining \nthe upper and lower bounds on the number of linear regions for rectified and \nmaxout networks. In addition to achieving tighter bounds, we also develop a \nnovel method to perform exact enumeration or counting of the number of linear \nregions with a mixed-integer linear formulation that maps the input space to \noutput. We use this new capability to visualize how the number of linear \nregions change while training DNNs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6be", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.02114"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Seyedamin Pouriyeh, Mehdi Allahyari, Krys Kochut, Hamid Reza Arabnia", "title": "A Comprehensive Survey of Ontology Summarization: Measures and Methods. (arXiv:1801.01937v1 [cs.IR])", "alternate": [{"href": "http://arxiv.org/abs/1801.01937", "type": "text/html"}], "timestampUsec": "1515476480138117", "comments": [], "summary": {"content": "<p>The Semantic Web is becoming a large scale framework that enables data to be \npublished, shared, and reused in the form of ontologies. The ontology which is \nconsidered as basic building block of semantic web consists of two layers \nincluding data and schema layer. With the current exponential development of \nontologies in both data size and complexity of schemas, ontology understanding \nwhich is playing an important role in different tasks such as ontology \nengineering, ontology learning, etc., is becoming more difficult. Ontology \nsummarization as a way to distill knowledge from an ontology and generate an \nabridge version to facilitate a better understanding is getting more attention \nrecently. There are various approaches available for ontology summarization \nwhich are focusing on different measures in order to produce a proper summary \nfor a given ontology. In this paper, we mainly focus on the common metrics \nwhich are using for ontology summarization and meet the state-of-the-art in \nontology summarization. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6c2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01937"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "M. Andrecut", "title": "On the inherent competition between valid and spurious inductive inferences in Boolean data. (arXiv:1801.02068v1 [physics.data-an])", "alternate": [{"href": "http://arxiv.org/abs/1801.02068", "type": "text/html"}], "timestampUsec": "1515476480138116", "comments": [], "summary": {"content": "<p>Inductive inference is the process of extracting general rules from specific \nobservations. This problem also arises in the analysis of biological networks, \nsuch as genetic regulatory networks, where the interactions are complex and the \nobservations are incomplete. A typical task in these problems is to extract \ngeneral interaction rules as combinations of Boolean covariates, that explain a \nmeasured response variable. The inductive inference process can be considered \nas an incompletely specified Boolean function synthesis problem. This \nincompleteness of the problem will also generate spurious inferences, which are \na serious threat to valid inductive inference rules. Using random Boolean data \nas a null model, here we attempt to measure the competition between valid and \nspurious inductive inference rules from a given data set. We formulate two \ngreedy search algorithms, which synthesize a given Boolean response variable in \na sparse disjunct normal form, and respectively a sparse generalized algebraic \nnormal form of the variables from the observation data, and we evaluate \nnumerically their performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6c3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02068"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michal &#x160;ustr, Jan Mal&#xfd;, Michal &#x10c;ertick&#xfd;", "title": "Multi-platform Version of StarCraft: Brood War in a Docker Container: Technical Report. (arXiv:1801.02193v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.02193", "type": "text/html"}], "timestampUsec": "1515476480138115", "comments": [], "summary": {"content": "<p>We present a dockerized version of a real-time strategy game StarCraft: Brood \nWar, commonly used as a domain for AI research, with a pre-installed collection \nof AI developement tools supporting all the major types of StarCraft bots. This \nprovides a convenient way to deploy StarCraft AIs on numerous hosts at once and \nacross multiple platforms despite limited OS support of StarCraft. In this \ntechnical report, we describe the design of our Docker images and present a few \nuse cases. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6c7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02193"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yi Wu, Yuxin Wu, Georgia Gkioxari, Yuandong Tian", "title": "Building Generalizable Agents with a Realistic and Rich 3D Environment. (arXiv:1801.02209v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.02209", "type": "text/html"}], "timestampUsec": "1515476480138114", "comments": [], "summary": {"content": "<p>Towards bridging the gap between machine and human intelligence, it is of \nutmost importance to introduce environments that are visually realistic and \nrich in content. In such environments, one can evaluate and improve a crucial \nproperty of practical intelligent systems, namely \\emph{generalization}. In \nthis work, we build \\emph{House3D}, a rich, extensible and efficient \nenvironment that contains 45,622 human-designed 3D scenes of houses, ranging \nfrom single-room studios to multi-storeyed houses, equipped with a diverse set \nof fully labeled 3D objects, textures and scene layouts, based on the SUNCG \ndataset (Song et al., 2017). With an emphasis on semantic-level generalization, \nwe study the task of concept-driven navigation, \\emph{RoomNav}, using a subset \nof houses in House3D. In RoomNav, an agent navigates towards a target specified \nby a semantic concept. To succeed, the agent learns to comprehend the scene it \nlives in by developing perception, understand the concept by mapping it to the \ncorrect semantics, and navigate to the target by obeying the underlying \nphysical rules. We train RL agents with both continuous and discrete action \nspaces and show their ability to generalize in new unseen environments. In \nparticular, we observe that (1) training is substantially harder on large house \nsets but results in better generalization, (2) using semantic signals (e.g., \nsegmentation mask) boosts the generalization performance, and (3) gated \nnetworks on semantic input signal lead to improved training performance and \ngeneralization. We hope House3D, including the analysis of the RoomNav task, \nserves as a building block towards designing practical intelligent systems and \nwe wish it to be broadly adopted by the community. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6c9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02209"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Catherine Xiao, Wanfeng Chen", "title": "Trading the Twitter Sentiment with Reinforcement Learning. (arXiv:1801.02243v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.02243", "type": "text/html"}], "timestampUsec": "1515476480138113", "comments": [], "summary": {"content": "<p>This paper is to explore the possibility to use alternative data and \nartificial intelligence techniques to trade stocks. The efficacy of the daily \nTwitter sentiment on predicting the stock return is examined using machine \nlearning methods. Reinforcement learning(Q-learning) is applied to generate the \noptimal trading policy based on the sentiment signal. The predicting power of \nthe sentiment signal is more significant if the stock price is driven by the \nexpectation of the company growth and when the company has a major event that \ndraws the public attention. The optimal trading strategy based on reinforcement \nlearning outperforms the trading strategy based on the machine learning \nprediction. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6ce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02243"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Bernhard Hengst, Maurice Pagnucco, David Rajaratnam, Claude Sammut, Michael Thielscher", "title": "Perceptual Context in Cognitive Hierarchies. (arXiv:1801.02270v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.02270", "type": "text/html"}], "timestampUsec": "1515476480138112", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329544116\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329544116&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Cognition does not only depend on bottom-up sensor feature abstraction, but \nalso relies on contextual information being passed top-down. Context is higher \nlevel information that helps to predict belief states at lower levels. The main \ncontribution of this paper is to provide a formalisation of perceptual context \nand its integration into a new process model for cognitive hierarchies. Several \nsimple instantiations of a cognitive hierarchy are used to illustrate the role \nof context. Notably, we demonstrate the use context in a novel approach to \nvisually track the pose of rigid objects with just a 2D camera. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6d2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02270"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Shunji Umetani, Masanao Arakawa, Mutsunori Yagiura", "title": "Relaxation heuristics for the set multicover problem with generalized upper bound constraints. (arXiv:1705.04970v2 [cs.DS] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.04970", "type": "text/html"}], "timestampUsec": "1515476480138111", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3295afe14\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3295afe14&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We consider an extension of the set covering problem (SCP) introducing \n(i)~multicover and (ii)~generalized upper bound (GUB)~constraints. For the \nconventional SCP, the pricing method has been introduced to reduce the size of \ninstances, and several efficient heuristic algorithms based on such reduction \ntechniques have been developed to solve large-scale instances. However, GUB \nconstraints often make the pricing method less effective, because they often \nprevent solutions from containing highly evaluated variables together. To \novercome this problem, we develop heuristic algorithms to reduce the size of \ninstances, in which new evaluation schemes of variables are introduced taking \naccount of GUB constraints. We also develop an efficient implementation of a \n2-flip neighborhood local search algorithm that reduces the number of \ncandidates in the neighborhood without sacrificing the solution quality. In \norder to guide the search to visit a wide variety of good solutions, we also \nintroduce a path relinking method that generates new solutions by combining two \nor more solutions obtained so far. According to computational comparison on \nbenchmark instances, the proposed method succeeds in selecting a small number \nof promising variables properly and performs quite effectively even for \nlarge-scale instances having hard GUB constraints. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6d5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.04970"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Srinivas Ravishankar, Chandrahas, Partha Pratim Talukdar", "title": "Revisiting Simple Neural Networks for Learning Representations of Knowledge Graphs. (arXiv:1711.05401v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05401", "type": "text/html"}], "timestampUsec": "1515476480138110", "comments": [], "summary": {"content": "<p>We address the problem of learning vector representations for entities and \nrelations in Knowledge Graphs (KGs) for Knowledge Base Completion (KBC). This \nproblem has received significant attention in the past few years and multiple \nmethods have been proposed. Most of the existing methods in the literature use \na predefined characteristic scoring function for evaluating the correctness of \nKG triples. These scoring functions distinguish correct triples (high score) \nfrom incorrect ones (low score). However, their performance vary across \ndifferent datasets. In this work, we demonstrate that a simple neural network \nbased score function can consistently achieve near start-of-the-art performance \non multiple datasets. We also quantitatively demonstrate biases in standard \nbenchmark datasets, and highlight the need to perform evaluation spanning \nvarious datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6dc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05401"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Hongfu Liu, Jun Li, Yue Wu, Yun Fu", "title": "Clustering with Outlier Removal. (arXiv:1801.01899v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.01899", "type": "text/html"}], "timestampUsec": "1515476480138109", "comments": [], "summary": {"content": "<p>Cluster analysis and outlier detection are strongly coupled tasks in data \nmining area. Cluster structure can be easily destroyed by few outliers; on the \ncontrary, the outliers are defined by the concept of cluster, which are \nrecognized as the points belonging to none of the clusters. However, most \nexisting studies handle them separately. In light of this, we consider the \njoint cluster analysis and outlier detection problem, and propose the \nClustering with Outlier Removal (COR) algorithm. Generally speaking, the \noriginal space is transformed into the binary space via generating basic \npartitions in order to define clusters. Then an objective function based \nHoloentropy is designed to enhance the compactness of each cluster with a few \noutliers removed. With further analyses on the objective function, only partial \nof the problem can be handled by K-means optimization. To provide an integrated \nsolution, an auxiliary binary matrix is nontrivally introduced so that COR \ncompletely and efficiently solves the challenging problem via a unified \nK-means- - with theoretical supports. Extensive experimental results on \nnumerous data sets in various domains demonstrate the effectiveness and \nefficiency of COR significantly over the rivals including K-means- - and other \nstate-of-the-art outlier detection methods in terms of cluster validity and \noutlier detection. Some key factors in COR are further analyzed for practical \nuse. Finally, an application on flight trajectory is provided to demonstrate \nthe effectiveness of COR in the real-world scenario. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01899"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Martin Gubri", "title": "Adversarial Perturbation Intensity Achieving Chosen Intra-Technique Transferability Level for Logistic Regression. (arXiv:1801.01953v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.01953", "type": "text/html"}], "timestampUsec": "1515476480138108", "comments": [], "summary": {"content": "<p>Machine Learning models have been shown to be vulnerable to adversarial \nexamples, ie. the manipulation of data by a attacker to defeat a defender's \nclassifier at test time. We present a novel probabilistic definition of \nadversarial examples in perfect or limited knowledge setting using prior \nprobability distributions on the defender's classifier. Using the asymptotic \nproperties of the logistic regression, we derive a closed-form expression of \nthe intensity of any adversarial perturbation, in order to achieve a given \nexpected misclassification rate. This technique is relevant in a threat model \nof known model specifications and unknown training data. To our knowledge, this \nis the first method that allows an attacker to directly choose the probability \nof attack success. We evaluate our approach on two real-world datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6e6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01953"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Panagiotis Tsilifis, Xun Huan, Cosmin Safta, Khachik Sargsyan, Guilhem Lacaze, Joseph C. Oefelein, Habib N. Najm, Roger G. Ghanem", "title": "Compressive sensing adaptation for polynomial chaos expansions. (arXiv:1801.01961v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.01961", "type": "text/html"}], "timestampUsec": "1515476480138107", "comments": [], "summary": {"content": "<p>Basis adaptation in Homogeneous Chaos spaces rely on a suitable rotation of \nthe underlying Gaussian germ. Several rotations have been proposed in the \nliterature resulting in adaptations with different convergence properties. In \nthis paper we present a new adaptation mechanism that builds on compressive \nsensing algorithms, resulting in a reduced polynomial chaos approximation with \noptimal sparsity. The developed adaptation algorithm consists of a two-step \noptimization procedure that computes the optimal coefficients and the input \nprojection matrix of a low dimensional chaos expansion with respect to an \noptimally rotated basis. We demonstrate the attractive features of our \nalgorithm through several numerical examples including the application on \nLarge-Eddy Simulation (LES) calculations of turbulent combustion in a HIFiRE \nscramjet engine. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6ea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01961"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Joan Bruna, Stephane Mallat", "title": "Multiscale Sparse Microcanonical Models. (arXiv:1801.02013v1 [math-ph])", "alternate": [{"href": "http://arxiv.org/abs/1801.02013", "type": "text/html"}], "timestampUsec": "1515476480138106", "comments": [], "summary": {"content": "<p>We study density estimation of stationary processes defined over an infinite \ngrid from a single, finite realization. Gaussian Processes and Markov Random \nFields avoid the curse of dimensionality by focusing on low-order and localized \npotentials respectively, but its application to complex datasets is limited by \ntheir inability to capture singularities and long-range interactions, and their \nexpensive inference and learning respectively. These are instances of Gibbs \nmodels, defined as maximum entropy distributions under moment constraints \ndetermined by an energy vector. The Boltzmann equivalence principle states that \nunder appropriate ergodicity, such \\emph{macrocanonical} models are \napproximated by their \\emph{microcanonical} counterparts, which replace the \nexpectation by the sample average. Microcanonical models are appealing since \nthey avoid computing expensive Lagrange multipliers to meet the constraints. \nThis paper introduces microcanonical measures whose energy vector is given by a \nwavelet scattering transform, built by cascading wavelet decompositions and \npoint-wise nonlinearities. We study asymptotic properties of generic \nmicrocanonical measures, which reveal the fundamental role of the differential \nstructure of the energy vector in controlling e.g. the entropy rate. Gradient \ninformation is also used to define a microcanonical sampling algorithm, for \nwhich we provide convergence analysis to the microcanonical measure. Whereas \nwavelet transforms capture local regularity at different scales, scattering \ntransforms provide scale interaction information, critical to restore the \ngeometry of many physical phenomena. We demonstrate the efficiency of sparse \nmultiscale microcanonical measures on several processes and real data \nexhibiting long-range interactions, such as Ising, Cox Processes and image and \naudio textures. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6f1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02013"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xingyu Wang, Diego Klabjan", "title": "Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal Demonstrations. (arXiv:1801.02124v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.02124", "type": "text/html"}], "timestampUsec": "1515476480138105", "comments": [], "summary": {"content": "<p>This paper considers the problem of inverse reinforcement learning in \nzero-sum stochastic games when expert demonstrations are known to be not \noptimal. Compared to previous works that decouple agents in the game by \nassuming optimality in expert strategies, we introduce a new objective function \nthat directly pits experts against Nash Equilibrium strategies, and we design \nan algorithm to solve for the reward function in the context of inverse \nreinforcement learning with deep neural networks as model approximations. In \nour setting the model and algorithm do not decouple by agent. In order to find \nNash Equilibrium in large-scale games, we also propose an adversarial training \nalgorithm for zero-sum stochastic games, and show the theoretical appeal of \nnon-existence of local optima in its objective function. In our numerical \nexperiments, we demonstrate that our Nash Equilibrium and inverse reinforcement \nlearning algorithms address games that are not amenable to previous approaches \nusing tabular representations. Moreover, with sub-optimal expert demonstrations \nour algorithms recover both reward functions and strategies with good quality. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6f5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02124"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yuya Onuma, Rachelle Rivero, Tsuyoshi Kato", "title": "Threshold Auto-Tuning Metric Learning. (arXiv:1801.02125v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.02125", "type": "text/html"}], "timestampUsec": "1515476480138104", "comments": [], "summary": {"content": "<p>It has been reported repeatedly that discriminative learning of distance \nmetric boosts the pattern recognition performance. A weak point of ITML-based \nmethods is that the distance threshold for similarity/dissimilarity constraints \nmust be determined manually and it is sensitive to generalization performance, \nalthough the ITML-based methods enjoy an advantage that the Bregman projection \nframework can be applied for optimization of distance metric. In this paper, we \npresent a new formulation of metric learning algorithm in which the distance \nthreshold is optimized together. Since the optimization is still in the Bregman \nprojection framework, the Dykstra algorithm can be applied for optimization. A \nnon-linear equation has to be solved to project the solution onto a half-space \nin each iteration. Na\\\"{i}ve method takes $O(LMn^{3})$ computational time to \nsolve the nonlinear equation. In this study, an efficient technique that can \nsolve the nonlinear equation in $O(Mn^{3})$ has been discovered. We have proved \nthat the root exists and is unique. We empirically show that the accuracy of \npattern recognition for the proposed metric learning algorithm is comparable to \nthe existing metric learning methods, yet the distance threshold is \nautomatically tuned for the proposed metric learning algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be6fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02125"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alexandre Attia, Sharone Dayan", "title": "Detection and segmentation of the Left Ventricle in Cardiac MRI using Deep Learning. (arXiv:1801.02171v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.02171", "type": "text/html"}], "timestampUsec": "1515476480138103", "comments": [], "summary": {"content": "<p>Manual segmentation of the Left Ventricle (LV) is a tedious and meticulous \ntask that can vary depending on the patient, the Magnetic Resonance Images \n(MRI) cuts and the experts. Still today, we consider manual delineation done by \nexperts as being the ground truth for cardiac diagnosticians. Thus, we are \nreviewing the paper - written by Avendi and al. - who presents a combined \napproach with Convolutional Neural Networks, Stacked Auto-Encoders and \nDeformable Models, to try and automate the segmentation while performing more \naccurately. Furthermore, we have implemented parts of the paper (around three \nquarts) and experimented both the original method and slightly modified \nversions when changing the architecture and the parameters. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be705", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02171"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "John Mitro, Derek Bridge, Steven Prestwich", "title": "Denoising Dictionary Learning Against Adversarial Perturbations. (arXiv:1801.02257v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.02257", "type": "text/html"}], "timestampUsec": "1515476480138102", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3295b0228\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3295b0228&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose denoising dictionary learning (DDL), a simple yet effective \ntechnique as a protection measure against adversarial perturbations. We \nexamined denoising dictionary learning on MNIST and CIFAR10 perturbed under two \ndifferent perturbation techniques, fast gradient sign (FGSM) and jacobian \nsaliency maps (JSMA). We evaluated it against five different deep neural \nnetworks (DNN) representing the building blocks of most recent architectures \nindicating a successive progression of model complexity of each other. We show \nthat each model tends to capture different representations based on their \narchitecture. For each model we recorded its accuracy both on the perturbed \ntest data previously misclassified with high confidence and on the denoised one \nafter the reconstruction using dictionary learning. The reconstruction quality \nof each data point is assessed by means of PSNR (Peak Signal to Noise Ratio) \nand Structure Similarity Index (SSI). We show that after applying (DDL) the \nreconstruction of the original data point from a noisy \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be70b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02257"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Avi Ben-Cohen, Eyal Klang, Michal Marianne Amitai, Jacob Goldberger, Hayit Greenspan", "title": "Anatomical Data Augmentation For CNN based Pixel-wise Classification. (arXiv:1801.02261v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.02261", "type": "text/html"}], "timestampUsec": "1515476480138101", "comments": [], "summary": {"content": "<p>In this work we propose a method for anatomical data augmentation that is \nbased on using slices of computed tomography (CT) examinations that are \nadjacent to labeled slices as another resource of labeled data for training the \nnetwork. The extended labeled data is used to train a U-net network for a \npixel-wise classification into different hepatic lesions and normal liver \ntissues. Our dataset contains CT examinations from 140 patients with 333 CT \nimages annotated by an expert radiologist. We tested our approach and compared \nit to the conventional training process. Results indicate superiority of our \nmethod. Using the anatomical data augmentation we achieved an improvement of 3% \nin the success rate, 5% in the classification accuracy, and 4% in Dice. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be710", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02261"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Raaz Dwivedi, Yuansi Chen, Martin J. Wainwright, Bin Yu", "title": "Log-concave sampling: Metropolis-Hastings algorithms are fast!. (arXiv:1801.02309v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.02309", "type": "text/html"}], "timestampUsec": "1515476480138100", "comments": [], "summary": {"content": "<p>We consider the problem of sampling from a strongly log-concave density in \n$\\mathbb{R}^d$, and prove a non-asymptotic upper bound on the mixing time of \nthe Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by \nrunning a Markov chain obtained from the discretization of an appropriate \nLangevin diffusion, combined with an accept-reject step to ensure the correct \nstationary distribution. Relative to known guarantees for the unadjusted \nLangevin algorithm (ULA), our bounds show that the use of an accept-reject step \nin MALA leads to an exponentially improved dependence on the error-tolerance. \nConcretely, in order to obtain samples with TV error at most $\\delta$ for a \ndensity with condition number $\\kappa$, we show that MALA requires $\\mathcal{O} \n\\big(\\kappa d \\log(1/\\delta) \\big)$ steps, as compared to the $\\mathcal{O} \n\\big(\\kappa^2 d/\\delta^2 \\big)$ steps established in past work on ULA. We also \ndemonstrate the gains of MALA over ULA for weakly log-concave densities. \nFurthermore, we derive mixing time bounds for a zeroth-order method \nMetropolized random walk (MRW) and show that it mixes $\\mathcal{O}(\\kappa d)$ \nslower than MALA. We provide numerical examples that support our theoretical \nfindings, and demonstrate the potential gains of Metropolis-Hastings adjustment \nfor Langevin-type algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be715", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02309"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Vinit Shah, Meysam Golmohammadi, Saeedeh Ziyabari, Eva Von Weltin, Iyad Obeid, Joseph Picone", "title": "Optimizing Channel Selection for Seizure Detection. (arXiv:1801.02472v1 [eess.SP])", "alternate": [{"href": "http://arxiv.org/abs/1801.02472", "type": "text/html"}], "timestampUsec": "1515476480138099", "comments": [], "summary": {"content": "<p>Interpretation of electroencephalogram (EEG) signals can be complicated by \nobfuscating artifacts. Artifact detection plays an important role in the \nobservation and analysis of EEG signals. Spatial information contained in the \nplacement of the electrodes can be exploited to accurately detect artifacts. \nHowever, when fewer electrodes are used, less spatial information is available, \nmaking it harder to detect artifacts. In this study, we investigate the \nperformance of a deep learning algorithm, CNN-LSTM, on several channel \nconfigurations. Each configuration was designed to minimize the amount of \nspatial information lost compared to a standard 22-channel EEG. Systems using a \nreduced number of channels ranging from 8 to 20 achieved sensitivities between \n33% and 37% with false alarms in the range of [38, 50] per 24 hours. False \nalarms increased dramatically (e.g., over 300 per 24 hours) when the number of \nchannels was further reduced. Baseline performance of a system that used all 22 \nchannels was 39% sensitivity with 23 false alarms. Since the 22-channel system \nwas the only system that included referential channels, the rapid increase in \nthe false alarm rate as the number of channels was reduced underscores the \nimportance of retaining referential channels for artifact reduction. This \ncautionary result is important because one of the biggest differences between \nvarious types of EEGs administered is the type of referential channel used. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be717", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02472"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Silvia Lopez, Aaron Gross, Scott Yang, Meysam Golmohammadi, Iyad Obeid, Joseph Picone", "title": "An Analysis of Two Common Reference Points for EEGs. (arXiv:1801.02474v1 [eess.SP])", "alternate": [{"href": "http://arxiv.org/abs/1801.02474", "type": "text/html"}], "timestampUsec": "1515476480138098", "comments": [], "summary": {"content": "<p>Clinical electroencephalographic (EEG) data varies significantly depending on \na number of operational conditions (e.g., the type and placement of electrodes, \nthe type of electrical grounding used). This investigation explores the \nstatistical differences present in two different referential montages: Linked \nEar (LE) and Averaged Reference (AR). Each of these accounts for approximately \n45% of the data in the TUH EEG Corpus. In this study, we explore the impact \nthis variability has on machine learning performance. We compare the \nstatistical properties of features generated using these two montages, and \nexplore the impact of performance on our standard Hidden Markov Model (HMM) \nbased classification system. We show that a system trained on LE data \nsignificantly outperforms one trained only on AR data (77.2% vs. 61.4%). We \nalso demonstrate that performance of a system trained on both data sets is \nsomewhat compromised (71.4% vs. 77.2%). A statistical analysis of the data \nsuggests that mean, variance and channel normalization should be considered. \nHowever, cepstral mean subtraction failed to produce an improvement in \nperformance, suggesting that the impact of these statistical differences is \nsubtler. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be71b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02474"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Scott Yang, Silvia Lopez, Meysam Golmohammadi, Iyad Obeid, Joseph Picone", "title": "Semi-automated Annotation of Signal Events in Clinical EEG Data. (arXiv:1801.02476v1 [eess.SP])", "alternate": [{"href": "http://arxiv.org/abs/1801.02476", "type": "text/html"}], "timestampUsec": "1515476480138097", "comments": [], "summary": {"content": "<p>To be effective, state of the art machine learning technology needs large \namounts of annotated data. There are numerous compelling applications in \nhealthcare that can benefit from high performance automated decision support \nsystems provided by deep learning technology, but they lack the comprehensive \ndata resources required to apply sophisticated machine learning models. \nFurther, for economic reasons, it is very difficult to justify the creation of \nlarge annotated corpora for these applications. Hence, automated annotation \ntechniques become increasingly important. In this study, we investigated the \neffectiveness of using an active learning algorithm to automatically annotate a \nlarge EEG corpus. The algorithm is designed to annotate six types of EEG \nevents. Two model training schemes, namely threshold-based and volume-based, \nare evaluated. In the threshold-based scheme the threshold of confidence scores \nis optimized in the initial training iteration, whereas for the volume-based \nscheme only a certain amount of data is preserved after each iteration. \nRecognition performance is improved 2% absolute and the system is capable of \nautomatically annotating previously unlabeled data. Given that the \ninterpretation of clinical EEG data is an exceedingly difficult task, this \nstudy provides some evidence that the proposed method is a viable alternative \nto expensive manual annotation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be71f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02476"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Amir Harati, Meysam Golmohammadi, Silvia Lopez, Iyad Obeid, Joseph Picone", "title": "Improved EEG Event Classification Using Differential Energy. (arXiv:1801.02477v1 [eess.SP])", "alternate": [{"href": "http://arxiv.org/abs/1801.02477", "type": "text/html"}], "timestampUsec": "1515476480138096", "comments": [], "summary": {"content": "<p>Feature extraction for automatic classification of EEG signals typically \nrelies on time frequency representations of the signal. Techniques such as \ncepstral-based filter banks or wavelets are popular analysis techniques in many \nsignal processing applications including EEG classification. In this paper, we \npresent a comparison of a variety of approaches to estimating and \npostprocessing features. To further aid in discrimination of periodic signals \nfrom aperiodic signals, we add a differential energy term. We evaluate our \napproaches on the TUH EEG Corpus, which is the largest publicly available EEG \ncorpus and an exceedingly challenging task due to the clinical nature of the \ndata. We demonstrate that a variant of a standard filter bank-based approach, \ncoupled with first and second derivatives, provides a substantial reduction in \nthe overall error rate. The combination of differential energy and derivatives \nproduces a 24% absolute reduction in the error rate and improves our ability to \ndiscriminate between signal events and background noise. This relatively simple \napproach proves to be comparable to other popular feature extraction approaches \nsuch as wavelets, but is much more computationally efficient. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be725", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02477"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, Dawn Song", "title": "Spatially transformed adversarial examples. (arXiv:1801.02612v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1801.02612", "type": "text/html"}], "timestampUsec": "1515476480138095", "comments": [], "summary": {"content": "<p>Recent studies show that widely used deep neural networks (DNNs) are \nvulnerable to carefully crafted adversarial examples. Many advanced algorithms \nhave been proposed to generate adversarial examples by leveraging the \n$\\mathcal{L}_p$ distance for penalizing perturbations. Researchers have \nexplored different defense methods to defend against such adversarial attacks. \nWhile the effectiveness of $\\mathcal{L}_p$ distance as a metric of perceptual \nquality remains an active research area, in this paper we will instead focus on \na different type of perturbation, namely spatial transformation, as opposed to \nmanipulating the pixel values directly as in prior works. Perturbations \ngenerated through spatial transformation could result in large $\\mathcal{L}_p$ \ndistance measures, but our extensive experiments show that such spatially \ntransformed adversarial examples are perceptually realistic and more difficult \nto defend against with existing defense systems. This potentially provides a \nnew direction in adversarial example generation and the design of corresponding \ndefenses. We visualize the spatial transformation based perturbation for \ndifferent examples and show that our technique can produce realistic \nadversarial examples with smooth image deformation. Finally, we visualize the \nattention of deep networks with different types of adversarial examples to \nbetter understand how these examples are interpreted. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be728", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.02612"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Olivier Delalleau, Aaron Courville, Yoshua Bengio", "title": "Efficient EM Training of Gaussian Mixtures with Missing Data. (arXiv:1209.0521v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1209.0521", "type": "text/html"}], "timestampUsec": "1515476480138094", "comments": [], "summary": {"content": "<p>In data-mining applications, we are frequently faced with a large fraction of \nmissing entries in the data matrix, which is problematic for most discriminant \nmachine learning algorithms. A solution that we explore in this paper is the \nuse of a generative model (a mixture of Gaussians) to compute the conditional \nexpectation of the missing variables given the observed variables. Since \ntraining a Gaussian mixture with many different patterns of missing values can \nbe computationally very expensive, we introduce a spanning-tree based algorithm \nthat significantly speeds up training in these conditions. We also observe that \ngood results can be obtained by using the generative model to fill-in the \nmissing values for a separate discriminant learning algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be730", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1209.0521"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jian Huang, Yuling Jiao, Bangti Jin, Jie Liu, Xiliang Lu, Can Yang", "title": "A Unified Primal Dual Active Set Algorithm for Nonconvex Sparse Recovery. (arXiv:1310.1147v4 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1310.1147", "type": "text/html"}], "timestampUsec": "1515476480138093", "comments": [], "summary": {"content": "<p>In this paper, we consider the problem of recovering a sparse signal based on \npenalized least squares. We develop an algorithm of primal-dual active set type \nfor a class of nonconvex sparsity-promoting penalties, which includes $\\ell^0$, \nbridge, smoothly clipped absolute deviation, capped $\\ell^1$ and minimax \nconcavity penalty. First we establish the existence of a global minimizer for \nthe class of optimization problems. Then we derive a novel necessary optimality \ncondition for the global minimizer using the associated thresholding operator. \nThe solutions to the optimality system are coordinate-wise minimizers, and \nunder minor conditions, they are also local minimizers. Upon introducing the \ndual variable, the active set can be determined from the primal and dual \nvariables. This relation lends itself to an iterative algorithm of active set \ntype which at each step involves updating the primal variable only on the \nactive set and then updating the dual variable explicitly. When combined with a \ncontinuation strategy on the regularization parameter, the primal dual active \nset method converges globally to the underlying regression target under some \nregularity conditions. Extensive numerical experiments demonstrate its superior \nperformance in efficiency and accuracy compared with the existing methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be737", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1310.1147"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michael Hornstein, Roger Fan, Kerby Shedden, Shuheng Zhou", "title": "Joint mean and covariance estimation with unreplicated matrix-variate data. (arXiv:1611.04208v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.04208", "type": "text/html"}], "timestampUsec": "1515476480138091", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3295b0882\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3295b0882&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>It has been proposed that complex populations, such as those that arise in \ngenomics studies, may exhibit dependencies among observations as well as among \nvariables. This gives rise to the challenging problem of analyzing unreplicated \nhigh-dimensional data with unknown mean and dependence structures. \nMatrix-variate approaches that impose various forms of (inverse) covariance \nsparsity allow flexible dependence structures to be estimated, but cannot \ndirectly be applied when the mean and covariance matrices are estimated \njointly. We present a practical method utilizing generalized least squares and \npenalized (inverse) covariance estimation to address this challenge. We \nestablish consistency and obtain rates of convergence for estimating the mean \nparameters and covariance matrices. The advantages of our approaches are: (i) \ndependence graphs and covariance structures can be estimated in the presence of \nunknown mean structure, (ii) the mean structure becomes more efficiently \nestimated when accounting for the dependence structure among observations; and \n(iii) inferences about the mean parameters become correctly calibrated. We use \nsimulation studies and analysis of genomic data from a twin study of ulcerative \ncolitis to illustrate the statistical convergence and the performance of our \nmethods in practical settings. Several lines of evidence show that the test \nstatistics for differential gene expression produced by our methods are \ncorrectly calibrated and improve power over conventional methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be742", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.04208"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Torsten Hothorn, Achim Zeileis", "title": "Transformation Forests. (arXiv:1701.02110v2 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1701.02110", "type": "text/html"}], "timestampUsec": "1515476480138090", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32960eddc\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32960eddc&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Regression models for supervised learning problems with a continuous target \nare commonly understood as models for the conditional mean of the target given \npredictors. This notion is simple and therefore appealing for interpretation \nand visualisation. Information about the whole underlying conditional \ndistribution is, however, not available from these models. A more general \nunderstanding of regression models as models for conditional distributions \nallows much broader inference from such models, for example the computation of \nprediction intervals. Several random forest-type algorithms aim at estimating \nconditional distributions, most prominently quantile regression forests \n(Meinshausen, 2006, JMLR). We propose a novel approach based on a parametric \nfamily of distributions characterised by their transformation function. A \ndedicated novel \"transformation tree\" algorithm able to detect distributional \nchanges is developed. Based on these transformation trees, we introduce \n\"transformation forests\" as an adaptive local likelihood estimator of \nconditional distribution functions. The resulting models are fully parametric \nyet very general and allow broad inference procedures, such as the model-based \nbootstrap, to be applied in a straightforward way. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be750", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1701.02110"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Laurentiu Hinoveanu, Fabrizio Leisen, Cristiano Villa", "title": "Objective Bayesian Analysis for Change Point Problems. (arXiv:1702.05462v2 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1702.05462", "type": "text/html"}], "timestampUsec": "1515476480138089", "comments": [], "summary": {"content": "<p>In this paper we present a loss-based approach to change point analysis. In \nparticular, we look at the problem from two perspectives. The first focuses on \nthe definition of a prior when the number of change points is known a priori. \nThe second contribution aims to estimate the number of change points by using a \nloss-based approach recently introduced in the literature. The latter considers \nchange point estimation as a model selection exercise. We show the performance \nof the proposed approach on simulated data and real data sets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be758", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1702.05462"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zifan Li, Ambuj Tewari", "title": "Beyond the Hazard Rate: More Perturbation Algorithms for Adversarial Multi-armed Bandits. (arXiv:1702.05536v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1702.05536", "type": "text/html"}], "timestampUsec": "1515476480138088", "comments": [], "summary": {"content": "<p>Recent work on follow the perturbed leader (FTPL) algorithms for the \nadversarial multi-armed bandit problem has highlighted the role of the hazard \nrate of the distribution generating the perturbations. Assuming that the hazard \nrate is bounded, it is possible to provide regret analyses for a variety of \nFTPL algorithms for the multi-armed bandit problem. This paper pushes the \ninquiry into regret bounds for FTPL algorithms beyond the bounded hazard rate \ncondition. There are good reasons to do so: natural distributions such as the \nuniform and Gaussian violate the condition. We give regret bounds for both \nbounded support and unbounded support distributions without assuming the hazard \nrate condition. We also disprove a conjecture that the Gaussian distribution \ncannot lead to a low-regret algorithm. In fact, it turns out that it leads to \nnear optimal regret, up to logarithmic factors. A key ingredient in our \napproach is the introduction of a new notion called the generalized hazard \nrate. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be75c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1702.05536"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zi Wang, Chengtao Li, Stefanie Jegelka, Pushmeet Kohli", "title": "Batched High-dimensional Bayesian Optimization via Structural Kernel Learning. (arXiv:1703.01973v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.01973", "type": "text/html"}], "timestampUsec": "1515476480138087", "comments": [], "summary": {"content": "<p>Optimization of high-dimensional black-box functions is an extremely \nchallenging problem. While Bayesian optimization has emerged as a popular \napproach for optimizing black-box functions, its applicability has been limited \nto low-dimensional problems due to its computational and statistical challenges \narising from high-dimensional settings. In this paper, we propose to tackle \nthese challenges by (1) assuming a latent additive structure in the function \nand inferring it properly for more efficient and effective BO, and (2) \nperforming multiple evaluations in parallel to reduce the number of iterations \nrequired by the method. Our novel approach learns the latent structure with \nGibbs sampling and constructs batched queries using determinantal point \nprocesses. Experimental validations on both synthetic and real-world functions \ndemonstrate that the proposed method outperforms the existing state-of-the-art \napproaches. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be762", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.01973"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Duncan Barrack, Simon Preston", "title": "Classification and clustering for samples of event time data using non-homogeneous Poisson process models. (arXiv:1703.02111v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.02111", "type": "text/html"}], "timestampUsec": "1515476480138086", "comments": [], "summary": {"content": "<p>Data of the form of event times arise in various applications. A simple model \nfor such data is a non-homogeneous Poisson process (NHPP) which is specified by \na rate function that depends on time. We consider the problem of having access \nto multiple independent samples of event time data, observed on a common \ninterval, from which we wish to classify or cluster the samples according to \ntheir rate functions. Each rate function is unknown but assumed to belong to a \nfinite number of rate functions each defining a distinct class. We model the \nrate functions using a spline basis expansion, the coefficients of which need \nto be estimated from data. The classification approach consists of using \ntraining data for which the class membership is known, to calculate maximum \nlikelihood estimates of the coefficients for each group, then assigning test \nsamples to a class by a maximum likelihood criterion. For clustering, by \nanalogy to the Gaussian mixture model approach for Euclidean data, we consider \na mixture of NHPP models and use the expectation-maximisation algorithm to \nestimate the coefficients of the rate functions for the component models and \ncluster membership probabilities for each sample. The classification and \nclustering approaches perform well on both synthetic and real-world data sets. \nCode associated with this paper is available at \nhttps://github.com/duncan-barrack/NHPP . \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be767", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.02111"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Bo-Jian Hou, Lijun Zhang, Zhi-Hua Zhou", "title": "Learning with Feature Evolvable Streams. (arXiv:1706.05259v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.05259", "type": "text/html"}], "timestampUsec": "1515476480138084", "comments": [], "summary": {"content": "<p>Learning with streaming data has attracted much attention during the past few \nyears. Though most studies consider data stream with fixed features, in real \npractice the features may be evolvable. For example, features of data gathered \nby limited-lifespan sensors will change when these sensors are substituted by \nnew ones. In this paper, we propose a novel learning paradigm: \\emph{Feature \nEvolvable Streaming Learning} where old features would vanish and new features \nwould occur. Rather than relying on only the current features, we attempt to \nrecover the vanished features and exploit it to improve performance. \nSpecifically, we learn two models from the recovered features and the current \nfeatures, respectively. To benefit from the recovered features, we develop two \nensemble methods. In the first method, we combine the predictions from two \nmodels and theoretically show that with the assistance of old features, the \nperformance on new features can be improved. In the second approach, we \ndynamically select the best single prediction and establish a better \nperformance guarantee when the best model switches. Experiments on both \nsynthetic and real data validate the effectiveness of our proposal. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be76c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.05259"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Faris B. Mismar, Brian L. Evans", "title": "Improving Downlink Coordinated Multipoint Performance in Heterogeneous Networks. (arXiv:1707.03269v2 [cs.NI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.03269", "type": "text/html"}], "timestampUsec": "1515476480138083", "comments": [], "summary": {"content": "<p>We propose a novel method for practical Joint Processing downlink coordinated \nmultipoint (DL CoMP) implementation in LTE/LTE-A systems using supervised \nmachine learning. DL CoMP has not been thoroughly studied in previous work \nalthough cluster formation and interference mitigation have been studied \nextensively. In this paper, we attempt to improve the cell edge data rate \nserved by a heterogeneous network cluster by means of dynamically changing the \nDL SINR threshold at which the DL CoMP feature is triggered. We do so by using \na support vector machine (SVM) classifier. The simulation results show a cell \nedge user throughput improvement of 33.3\\% for pico cells and more than \nfour-fold improvement in user throughput in the cluster. This has resulted from \na reduction in the downlink block error rate (DL BLER) and an improvement in \nthe spectral efficiency due to the informed triggering of the multiple radio \nstreams as part of DL CoMP. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be772", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.03269"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Randal S. Olson, William La Cava, Zairah Mustahsan, Akshay Varik, Jason H. Moore", "title": "Data-driven Advice for Applying Machine Learning to Bioinformatics Problems. (arXiv:1708.05070v2 [q-bio.QM] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.05070", "type": "text/html"}], "timestampUsec": "1515476480138082", "comments": [], "summary": {"content": "<p>As the bioinformatics field grows, it must keep pace not only with new data \nbut with new algorithms. Here we contribute a thorough analysis of 13 \nstate-of-the-art, commonly used machine learning algorithms on a set of 165 \npublicly available classification problems in order to provide data-driven \nalgorithm recommendations to current researchers. We present a number of \nstatistical and visual comparisons of algorithm performance and quantify the \neffect of model selection and algorithm tuning for each algorithm and dataset. \nThe analysis culminates in the recommendation of five algorithms with \nhyperparameters that maximize classifier performance across the tested \nproblems, as well as general guidelines for applying machine learning to \nsupervised classification problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be77a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.05070"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Robert Bamler, Cheng Zhang, Manfred Opper, Stephan Mandt", "title": "Perturbative Black Box Variational Inference. (arXiv:1709.07433v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.07433", "type": "text/html"}], "timestampUsec": "1515476480138081", "comments": [], "summary": {"content": "<p>Black box variational inference (BBVI) with reparameterization gradients \ntriggered the exploration of divergence measures other than the \nKullback-Leibler (KL) divergence, such as alpha divergences. In this paper, we \nview BBVI with generalized divergences as a form of estimating the marginal \nlikelihood via biased importance sampling. The choice of divergence determines \na bias-variance trade-off between the tightness of a bound on the marginal \nlikelihood (low bias) and the variance of its gradient estimators. Drawing on \nvariational perturbation theory of statistical physics, we use these insights \nto construct a family of new variational bounds. Enumerated by an odd integer \norder $K$, this family captures the standard KL bound for $K=1$, and converges \nto the exact marginal likelihood as $K\\to\\infty$. Compared to \nalpha-divergences, our reparameterization gradients have a lower variance. We \nshow in experiments on Gaussian Processes and Variational Autoencoders that the \nnew bounds are more mass covering, and that the resulting posterior covariances \nare closer to the true posterior and lead to higher likelihoods on held-out \ndata. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be784", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.07433"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alessandro Bay, Biswa Sengupta", "title": "GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks. (arXiv:1710.09363v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.09363", "type": "text/html"}], "timestampUsec": "1515476480138080", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32960f056\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32960f056&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The Fisher information metric is an important foundation of information \ngeometry, wherein it allows us to approximate the local geometry of a \nprobability distribution. Recurrent neural networks such as the \nSequence-to-Sequence (Seq2Seq) networks that have lately been used to yield \nstate-of-the-art performance on speech translation or image captioning have so \nfar ignored the geometry of the latent embedding, that they iteratively learn. \nWe propose the information geometric Seq2Seq (GeoSeq2Seq) network which \nabridges the gap between deep recurrent neural networks and information \ngeometry. Specifically, the latent embedding offered by a recurrent network is \nencoded as a Fisher kernel of a parametric Gaussian Mixture Model, a formalism \ncommon in computer vision. We utilise such a network to predict the shortest \nroutes between two nodes of a graph by learning the adjacency matrix using the \nGeoSeq2Seq formalism; our results show that for such a problem the \nprobabilistic representation of the latent embedding supersedes the \nnon-probabilistic embedding by 10-15\\%. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be78c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.09363"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yannic Kilcher, Aurelien Lucchi, Thomas Hofmann", "title": "Flexible Prior Distributions for Deep Generative Models. (arXiv:1710.11383v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.11383", "type": "text/html"}], "timestampUsec": "1515476480138079", "comments": [], "summary": {"content": "<p>We consider the problem of training generative models with deep neural \nnetworks as generators, i.e. to map latent codes to data points. Whereas the \ndominant paradigm combines simple priors over codes with complex deterministic \nmodels, we argue that it might be advantageous to use more flexible code \ndistributions. We demonstrate how these distributions can be induced directly \nfrom the data. The benefits include: more powerful generative models, better \nmodeling of latent structure and explicit control of the degree of \ngeneralization. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be791", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.11383"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, Kevin Murphy", "title": "An Information-Theoretic Analysis of Deep Latent-Variable Models. (arXiv:1711.00464v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.00464", "type": "text/html"}], "timestampUsec": "1515476480138078", "comments": [], "summary": {"content": "<p>We present an information-theoretic framework for understanding trade-offs in \nunsupervised learning of deep latent-variables models using variational \ninference. This framework emphasizes the need to consider latent-variable \nmodels along two dimensions: the ability to reconstruct inputs (distortion) and \nthe communication cost (rate). We derive the optimal frontier of generative \nmodels in the two-dimensional rate-distortion plane, and show how the standard \nevidence lower bound objective is insufficient to select between points along \nthis frontier. However, by performing targeted optimization to learn generative \nmodels with different rates, we are able to learn many models that can achieve \nsimilar generative performance but make vastly different trade-offs in terms of \nthe usage of the latent variable. Through experiments on MNIST and Omniglot \nwith a variety of architectures, we show how our framework sheds light on many \nrecent proposed extensions to the variational autoencoder family. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be795", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.00464"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mufti Mahmud, M. Shamim Kaiser, Amir Hussain, Stefano Vassanelli", "title": "Applications of Deep Learning and Reinforcement Learning to Biological Data. (arXiv:1711.03985v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.03985", "type": "text/html"}], "timestampUsec": "1515476480138077", "comments": [], "summary": {"content": "<p>Rapid advances of hardware-based technologies during the past decades have \nopened up new possibilities for Life scientists to gather multimodal data in \nvarious application domains (e.g., Omics, Bioimaging, Medical Imaging, and \n[Brain/Body]-Machine Interfaces), thus generating novel opportunities for \ndevelopment of dedicated data intensive machine learning techniques. Overall, \nrecent research in Deep learning (DL), Reinforcement learning (RL), and their \ncombination (Deep RL) promise to revolutionize Artificial Intelligence. The \ngrowth in computational power accompanied by faster and increased data storage \nand declining computing costs have already allowed scientists in various fields \nto apply these techniques on datasets that were previously intractable for \ntheir size and complexity. This review article provides a comprehensive survey \non the application of DL, RL, and Deep RL techniques in mining Biological data. \nIn addition, we compare performances of DL techniques when applied to different \ndatasets across various application domains. Finally, we outline open issues in \nthis challenging research area and discuss future development perspectives. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be798", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.03985"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Shubhanshu Shekhar, Tara Javidi", "title": "Gaussian Process bandits with adaptive discretization. (arXiv:1712.01447v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.01447", "type": "text/html"}], "timestampUsec": "1515476480138076", "comments": [], "summary": {"content": "<p>In this paper, the problem of maximizing a black-box function $f:\\mathcal{X} \n\\to \\mathbb{R}$ is studied in the Bayesian framework with a Gaussian Process \n(GP) prior. In particular, a new algorithm for this problem is proposed, and \nhigh probability bounds on its simple and cumulative regret are established. \nThe query point selection rule in most existing methods involves an exhaustive \nsearch over an increasingly fine sequence of uniform discretizations of \n$\\mathcal{X}$. The proposed algorithm, in contrast, adaptively refines \n$\\mathcal{X}$ which leads to a lower computational complexity, particularly \nwhen $\\mathcal{X}$ is a subset of a high dimensional Euclidean space. In \naddition to the computational gains, sufficient conditions are identified under \nwhich the regret bounds of the new algorithm improve upon the known results. \nFinally an extension of the algorithm to the case of contextual bandits is \nproposed, and high probability bounds on the contextual regret are presented. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515476480138", "annotations": [], "published": 1515476480, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003576be79f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01447"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Joan Serr&#xe0;, D&#xed;dac Sur&#xed;s, Marius Miron, Alexandros Karatzoglou", "title": "Overcoming catastrophic forgetting with hard attention to the task. (arXiv:1801.01423v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.01423", "type": "text/html"}], "timestampUsec": "1515390604155517", "comments": [], "summary": {"content": "<p>Catastrophic forgetting occurs when a neural network loses the information \nlearned with the first task, after training on a second task. This problem \nremains a hurdle for general artificial intelligence systems with sequential \nlearning capabilities. In this paper, we propose a task-based hard attention \nmechanism that preserves previous tasks' information without substantially \naffecting the current task's learning. An attention mask is learned \nconcurrently to every task through stochastic gradient descent, and previous \nmasks are exploited to constrain such learning. We show that the proposed \nmechanism is effective for reducing catastrophic forgetting, cutting current \nrates by 33 to 84%. We also show that it is robust to different hyperparameter \nchoices and that it offers a number of monitoring capabilities. The approach \nfeatures the possibility to control both the stability and compactness of the \nlearned knowledge, which we believe makes it also attractive for online \nlearning and network compression applications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b1504a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01423"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Fatema Tuz Zohora, Ngoc Hieu Tran, Xianglilan Zhang, Lei Xin, Baozhen Shan, Ming Li", "title": "DeepIso: A Deep Learning Model for Peptide Feature Detection. (arXiv:1801.01539v1 [q-bio.QM])", "alternate": [{"href": "http://arxiv.org/abs/1801.01539", "type": "text/html"}], "timestampUsec": "1515390604155516", "comments": [], "summary": {"content": "<p>Liquid chromatography with tandem mass spectrometry (LC-MS/MS) based \nproteomics is a well-established research field with major applications such as \nidentification of disease biomarkers, drug discovery, drug design and \ndevelopment. In proteomics, protein identification and quantification is a \nfundamental task, which is done by first enzymatically digesting it into \npeptides, and then analyzing peptides by LC-MS/MS instruments. The peptide \nfeature detection and quantification from an LC-MS map is the first step in \ntypical analysis workflows. In this paper we propose a novel deep learning \nbased model, DeepIso, that uses Convolutional Neural Networks (CNNs) to scan an \nLC-MS map to detect peptide features and estimate their abundance. Existing \ntools are often designed with limited engineered features based on domain \nknowledge, and depend on pretrained parameters which are hardly updated despite \nhuge amount of new coming proteomic data. Our proposed model, on the other \nhand, is capable of learning multiple levels of representation of high \ndimensional data through its many layers of neurons and continuously evolving \nwith newly acquired data. To evaluate our proposed model, we use an antibody \ndataset including a heavy and a light chain, each digested by Asp-N, \nChymotrypsin, Trypsin, thus giving six LC-MS maps for the experiment. Our model \nachieves 93.21% sensitivity with specificity of 99.44% on this dataset. Our \nresults demonstrate that novel deep learning tools are desirable to advance the \nstate-of-the-art in protein identification and quantification. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b15055", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01539"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Filipe Assun&#xe7;&#xe3;o, Nuno Louren&#xe7;o, Penousal Machado, Bernardete Ribeiro", "title": "DENSER: Deep Evolutionary Network Structured Representation. (arXiv:1801.01563v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.01563", "type": "text/html"}], "timestampUsec": "1515390604155515", "comments": [], "summary": {"content": "<p>Deep Evolutionary Network Structured Representation (DENSER) is a novel \napproach to automatically design Artificial Neural Networks (ANNs) using \nEvolutionary Computation (EC). The algorithm not only searches for the best \nnetwork topology (e.g., number of layers, type of layers), but also tunes \nhyper-parameters, such as, learning parameters or data augmentation parameters. \nThe automatic design is achieved using a representation with two distinct \nlevels, where the outer level encodes the general structure of the network, \ni.e., the sequence of layers, and the inner level encodes the parameters \nassociated with each layer. The allowed layers and hyper-parameter value ranges \nare defined by means of a human-readable Context-Free Grammar. DENSER was used \nto evolve ANNs for two widely used image classification benchmarks obtaining an \naverage accuracy result of up to 94.27% on the CIFAR-10 dataset, and of 78.75% \non the CIFAR-100. To the best of our knowledge, our CIFAR-100 results are the \nhighest performing models generated by methods that aim at the automatic design \nof Convolutional Neural Networks (CNNs), and is amongst the best for manually \ndesigned and fine-tuned CNNs . \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b15060", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01563"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Long Jin, Justin Lazarow, Zhuowen Tu", "title": "Introspective Classification with Convolutional Nets. (arXiv:1704.07816v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.07816", "type": "text/html"}], "timestampUsec": "1515390604155514", "comments": [], "summary": {"content": "<p>We propose introspective convolutional networks (ICN) that emphasize the \nimportance of having convolutional neural networks empowered with generative \ncapabilities. We employ a reclassification-by-synthesis algorithm to perform \ntraining using a formulation stemmed from the Bayes theory. Our ICN tries to \niteratively: (1) synthesize pseudo-negative samples; and (2) enhance itself by \nimproving the classification. The single CNN classifier learned is at the same \ntime generative --- being able to directly synthesize new samples within its \nown discriminative model. We conduct experiments on benchmark datasets \nincluding MNIST, CIFAR-10, and SVHN using state-of-the-art CNN architectures, \nand observe improved classification results. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b15067", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.07816"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aditya Grover, Manik Dhar, Stefano Ermon", "title": "Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in Generative Models. (arXiv:1705.08868v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08868", "type": "text/html"}], "timestampUsec": "1515390604155513", "comments": [], "summary": {"content": "<p>Adversarial learning of probabilistic models has recently emerged as a \npromising alternative to maximum likelihood. Implicit models such as generative \nadversarial networks (GAN) often generate better samples compared to explicit \nmodels trained by maximum likelihood. Yet, GANs sidestep the characterization \nof an explicit density which makes quantitative evaluations challenging. To \nbridge this gap, we propose Flow-GANs, a generative adversarial network for \nwhich we can perform exact likelihood evaluation, thus supporting both \nadversarial and maximum likelihood training. When trained adversarially, \nFlow-GANs generate high-quality samples but attain extremely poor \nlog-likelihood scores, inferior even to a mixture model memorizing the training \ndata; the opposite is true when trained by maximum likelihood. Results on MNIST \nand CIFAR-10 demonstrate that hybrid training can attain high held-out \nlikelihoods while retaining visual fidelity in the generated samples. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b1506d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08868"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Miao Zhang, Xiaofei Kang, Yanqing Wang, Lantian Li, Zhiyuan Tang, Haisheng Dai, Dong Wang", "title": "Human and Machine Speaker Recognition Based on Short Trivial Events. (arXiv:1711.05443v2 [cs.SD] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05443", "type": "text/html"}], "timestampUsec": "1515390604155511", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32960f2ae\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32960f2ae&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Trivial events are ubiquitous in human to human conversations, e.g., cough, \nlaugh and sniff. Compared to regular speech, these trivial events are usually \nshort and unclear, thus generally regarded as not speaker discriminative and so \nare largely ignored by present speaker recognition research. However, these \ntrivial events are highly valuable in some particular circumstances such as \nforensic examination, as they are less subjected to intentional change, so can \nbe used to discover the genuine speaker from disguised speech. In this paper, \nwe collect a trivial event speech database that involves 75 speakers and 6 \ntypes of events, and report preliminary speaker recognition results on this \ndatabase, by both human listeners and machines. Particularly, the deep feature \nlearning technique recently proposed by our group is utilized to analyze and \nrecognize the trivial events, which leads to acceptable equal error rates \n(EERs) despite the extremely short durations (0.2-0.5 seconds) of these events. \nComparing different types of events, 'hmm' seems more speaker discriminative. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b15072", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05443"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tanya Khovanova", "title": "Coins and Logic. (arXiv:1801.01143v1 [math.HO])", "alternate": [{"href": "http://arxiv.org/abs/1801.01143", "type": "text/html"}], "timestampUsec": "1515390604155509", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3296637fa\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3296637fa&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We establish fun parallels between coin-weighing puzzles and \nknights-and-knaves puzzles. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b15073", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01143"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aayush Gupta, Daniel Bessonov, Patrick Li", "title": "A Decision-theoretic Approach to Detection-based Target Search with a UAV. (arXiv:1801.01228v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.01228", "type": "text/html"}], "timestampUsec": "1515390604155508", "comments": [], "summary": {"content": "<p>Search and rescue missions and surveillance require finding targets in a \nlarge area. These tasks often use unmanned aerial vehicles (UAVs) with cameras \nto detect and move towards a target. However, common UAV approaches make two \nsimplifying assumptions. First, they assume that observations made from \ndifferent heights are deterministically correct. In practice, observations are \nnoisy, with the noise increasing as the height used for observations increases. \nSecond, they assume that a motion command executes correctly, which may not \nhappen due to wind and other environmental factors. To address these, we \npropose a sequential algorithm that determines actions in real time based on \nobservations, using partially observable Markov decision processes (POMDPs). \nOur formulation handles both observations and motion uncertainty and errors. We \nrun offline simulations and learn a policy. This policy is run on a UAV to find \nthe target efficiently. We employ a novel compact formulation to represent the \ncoordinates of the drone relative to the target coordinates. Our POMDP policy \nfinds the target up to 3.4 times faster when compared to a heuristic policy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b15077", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01228"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yoseob Han, Jingu Kang, Jong Chul Ye", "title": "Deep Learning Reconstruction for 9-View Dual Energy CT Baggage Scanner. (arXiv:1801.01258v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.01258", "type": "text/html"}], "timestampUsec": "1515390604155507", "comments": [], "summary": {"content": "<p>For homeland and transportation security applications, 2D X-ray explosive \ndetection system (EDS) have been widely used, but they have limitations in \nrecognizing 3D shape of the hidden objects. Among various types of 3D computed \ntomography (CT) systems to address this issue, this paper is interested in a \nstationary CT using fixed X-ray sources and detectors. However, due to the \nlimited number of projection views, analytic reconstruction algorithms produce \nsevere streaking artifacts. Inspired by recent success of deep learning \napproach for sparse view CT reconstruction, here we propose a novel image and \nsinogram domain deep learning architecture for 3D reconstruction from very \nsparse view measurement. The algorithm has been tested with the real data from \na prototype 9-view dual energy stationary CT EDS carry-on baggage scanner \ndeveloped by GEMSS Medical Systems, Korea, which confirms the superior \nreconstruction performance over the existing approaches. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b1507b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01258"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, Yuyi Zhong", "title": "VulDeePecker: A Deep Learning-Based System for Vulnerability Detection. (arXiv:1801.01681v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1801.01681", "type": "text/html"}], "timestampUsec": "1515390604155506", "comments": [], "summary": {"content": "<p>The automatic detection of software vulnerabilities is an important research \nproblem. However, existing solutions to this problem rely on human experts to \ndefine features and often miss many vulnerabilities (i.e., incurring high false \nnegative rate). In this paper, we initiate the study of using deep \nlearning-based vulnerability detection to relieve human experts from the \ntedious and subjective task of manually defining features. Since deep learning \nis motivated to deal with problems that are very different from the problem of \nvulnerability detection, we need some guiding principles for applying deep \nlearning to vulnerability detection. In particular, we need to find \nrepresentations of software programs that are suitable for deep learning. For \nthis purpose, we propose using code gadgets to represent programs and then \ntransform them into vectors, where a code gadget is a number of (not \nnecessarily consecutive) lines of code that are semantically related to each \nother. This leads to the design and implementation of a deep learning-based \nvulnerability detection system, called Vulnerability Deep Pecker \n(VulDeePecker). In order to evaluate VulDeePecker, we present the first \nvulnerability dataset for deep learning approaches. Experimental results show \nthat VulDeePecker can achieve much fewer false negatives (with reasonable false \npositives) than other approaches. We further apply VulDeePecker to 3 software \nproducts (namely Xen, Seamonkey, and Libav) and detect 4 vulnerabilities, which \nare not reported in the National Vulnerability Database but were \"silently\" \npatched by the vendors when releasing later versions of these products; in \ncontrast, these vulnerabilities are almost entirely missed by the other \nvulnerability detection systems we experimented with. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b1507f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01681"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Fabricio Olivetti de Franca", "title": "A Greedy Search Tree Heuristic for Symbolic Regression. (arXiv:1801.01807v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.01807", "type": "text/html"}], "timestampUsec": "1515390604155505", "comments": [], "summary": {"content": "<p>Symbolic Regression tries to find a mathematical expression that describes \nthe relationship of a set of explanatory variables to a measured variable. The \nmain objective is to find a model that minimizes the error and, optionally, \nthat also minimizes the expression size. A smaller expression can be seen as an \ninterpretable model considered a reliable decision model. This is often \nperformed with Genetic Programming which represents their solution as \nexpression trees. The shortcoming of this algorithm lies on this representation \nthat defines a rugged search space and contains expressions of any size and \ndifficulty. These pose as a challenge to find the optimal solution under \ncomputational constraints. This paper introduces a new data structure, called \nInteraction-Transformation (IT), that constrains the search space in order to \nexclude a region of larger and more complicated expressions. In order to test \nthis data structure, it was also introduced an heuristic called SymTree. The \nobtained results show evidence that SymTree are capable of obtaining the \noptimal solution whenever the target function is within the search space of the \nIT data structure and competitive results when it is not. Overall, the \nalgorithm found a good compromise between accuracy and simplicity for all the \ngenerated models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b15083", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01807"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gerrit Bagschik, Till Menzel, Markus Maurer", "title": "Ontology based Scene Creation for the Development of Automated Vehicles. (arXiv:1704.01006v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.01006", "type": "text/html"}], "timestampUsec": "1515390604155504", "comments": [], "summary": {"content": "<p>The introduction of automated vehicles without permanent human supervision \ndemands a functional system description, including functional system boundaries \nand a comprehensive safety analysis. These inputs to the technical development \ncan be identified and analyzed by a scenario-based approach. Furthermore, to \nestablish an economical test and release process, a large number of scenarios \nmust be identified to obtain meaningful test results. Experts are doing well to \nidentify scenarios that are difficult to handle or unlikely to happen. However, \nexperts are unlikely to identify all scenarios possible based on the knowledge \nthey have on hand. Expert knowledge modeled for computer aided processing may \nhelp for the purpose of providing a wide range of scenarios. This contribution \nreviews ontologies as knowledge-based systems in the field of automated \nvehicles, and proposes a generation of traffic scenes in natural language as a \nbasis for a scenario creation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b15088", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.01006"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1515995660, "author": "Miko&#x142;aj Bi&#x144;kowski, Dougal J. Sutherland, Michael Arbel, Arthur Gretton", "title": "Demystifying MMD GANs. (arXiv:1801.01401v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1801.01401", "type": "text/html"}], "timestampUsec": "1515390604155502", "comments": [], "summary": {"content": "<p>We investigate the training and performance of generative adversarial \nnetworks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. \nAs our main theoretical contribution, we clarify the situation with bias in GAN \nloss functions raised by recent work: we show that gradient estimators used in \nthe optimization process for both MMD GANs and Wasserstein GANs are unbiased, \nbut learning a discriminator based on samples leads to biased gradients for the \ngenerator parameters. We also discuss the issue of kernel choice for the MMD \ncritic, and characterize the kernel corresponding to the energy distance used \nfor the Cramer GAN critic. Being an integral probability metric, the MMD \nbenefits from training strategies recently developed for Wasserstein GANs. In \nexperiments, the MMD GAN is able to employ a smaller critic network than the \nWasserstein GAN, resulting in a simpler and faster-training algorithm with \nmatching performance. We also propose an improved measure of GAN convergence, \nthe Kernel Inception Distance, and show how to use it to dynamically adapt \nlearning rates during GAN training. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515995660, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b15095", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01401"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hussain Kazmi, Fahad Mehmood, Stefan Lodeweyckx, Johan Driesen", "title": "Deep Reinforcement Learning based Optimal Control of Hot Water Systems. (arXiv:1801.01467v1 [cs.SY])", "alternate": [{"href": "http://arxiv.org/abs/1801.01467", "type": "text/html"}], "timestampUsec": "1515390604155501", "comments": [], "summary": {"content": "<p>Energy consumption for hot water production is a major draw in high \nefficiency buildings. Optimizing this has typically been approached from a \nthermodynamics perspective, decoupled from occupant influence. Furthermore, \noptimization usually presupposes existence of a detailed dynamics model for the \nhot water system. These assumptions lead to suboptimal energy efficiency in the \nreal world. In this paper, we present a novel reinforcement learning based \nmethodology which optimizes hot water production. The proposed methodology is \ncompletely generalizable, and does not require an offline step or human domain \nknowledge to build a model for the hot water vessel or the heating element. \nOccupant preferences too are learnt on the fly. The proposed system is applied \nto a set of 32 houses in the Netherlands where it reduces energy consumption \nfor hot water production by roughly 20% with no loss of occupant comfort. \nExtrapolating, this translates to absolute savings of roughly 200 kWh for a \nsingle household on an annual basis. This performance can be replicated to any \ndomestic hot water system and optimization objective, given that the fairly \nminimal requirements on sensor data are met. With millions of hot water systems \noperational worldwide, the proposed framework has the potential to reduce \nenergy consumption in existing and new systems on a multi Gigawatt-hour scale \nin the years to come. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b1509b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01467"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Uri Shaham, Kelly Stanton, Henry Li, Boaz Nadler, Ronen Basri, Yuval Kluger", "title": "SpectralNet: Spectral Clustering using Deep Neural Networks. (arXiv:1801.01587v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.01587", "type": "text/html"}], "timestampUsec": "1515390604155500", "comments": [], "summary": {"content": "<p>Spectral clustering is a leading and popular technique in unsupervised data \nanalysis. Two of its major limitations are scalability and generalization of \nthe spectral embedding (i.e., out-of-sample-extension). In this paper we \nintroduce a deep learning approach to spectral clustering that overcomes the \nabove shortcomings. Our network, which we call SpectralNet, learns a map that \nembeds input data points into the eigenspace of their associated graph \nLaplacian matrix and subsequently clusters them. We train SpectralNet using a \nprocedure that involves constrained stochastic optimization. Stochastic \noptimization allows it to scale to large datasets, while the constraints, which \nare implemented using a special-purpose output layer, allow us to keep the \nnetwork output orthogonal. Moreover, the map learned by SpectralNet naturally \ngeneralizes the spectral embedding to unseen data points. To further improve \nthe quality of the clustering, we replace the standard pairwise Gaussian \naffinities with affinities leaned from unlabeled data using a Siamese network. \nAdditional improvement can be achieved by applying the network to code \nrepresentations produced, e.g., by standard autoencoders. Our end-to-end \nlearning procedure is fully unsupervised. In addition, we apply VC dimension \ntheory to derive a lower bound on the size of SpectralNet. State-of-the-art \nclustering results are reported on the Reuters dataset. Our implementation is \npublicly available at https://github.com/kstant0725/SpectralNet . \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b150a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01587"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Antonia Creswell, Anil Anthony Bharath", "title": "Denoising Adversarial Autoencoders. (arXiv:1703.01220v4 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.01220", "type": "text/html"}], "timestampUsec": "1515390604155499", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329663e27\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329663e27&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Unsupervised learning is of growing interest because it unlocks the potential \nheld in vast amounts of unlabelled data to learn useful representations for \ninference. Autoencoders, a form of generative model, may be trained by learning \nto reconstruct unlabelled input data from a latent representation space. More \nrobust representations may be produced by an autoencoder if it learns to \nrecover clean input samples from corrupted ones. Representations may be further \nimproved by introducing regularisation during training to shape the \ndistribution of the encoded data in latent space. We suggest denoising \nadversarial autoencoders, which combine denoising and regularisation, shaping \nthe distribution of latent space using adversarial training. We introduce a \nnovel analysis that shows how denoising may be incorporated into the training \nand sampling of adversarial autoencoders. Experiments are performed to assess \nthe contributions that denoising makes to the learning of representations for \nclassification and sample synthesis. Our results suggest that autoencoders \ntrained using a denoising criterion achieve higher classification performance, \nand can synthesise samples that are more consistent with the input data than \nthose trained without a corruption process. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b150a6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.01220"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yunfei Ye", "title": "A Nonlinear Kernel Support Matrix Machine for Matrix Learning. (arXiv:1707.06487v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.06487", "type": "text/html"}], "timestampUsec": "1515390604155498", "comments": [], "summary": {"content": "<p>In many problems of supervised tensor learning (STL), real world data such as \nface images or MRI scans are naturally represented as matrices, which are also \ncalled as second order tensors. Most existing classifiers based on tensor \nrepresentation, such as support tensor machine (STM) need to solve iteratively \nwhich occupy much time and may suffer from local minima. In this paper, we \npresent a kernel support matrix machine (KSMM) to perform supervised learning \nwhen data are represented as matrices. KSMM is a general framework for the \nconstruction of matrix-based hyperplane to exploit structural information. We \nanalyze a unifying optimization problem for which we propose an asymptotically \nconvergent algorithm. Theoretical analysis for the generalization bounds is \nderived based on Rademacher complexity with respect to a probability \ndistribution. We demonstrate the merits of the proposed method by exhaustive \nexperiments on both simulation study and a number of real-word datasets from a \nvariety of application domains. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b150ad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.06487"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiang Zhang, Nishant Vishwamitra, Hongxin Hu, Feng Luo", "title": "CrescendoNet: A Simple Deep Convolutional Neural Network with Ensemble Behavior. (arXiv:1710.11176v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.11176", "type": "text/html"}], "timestampUsec": "1515390604155496", "comments": [], "summary": {"content": "<p>We introduce a new deep convolutional neural network, CrescendoNet, by \nstacking simple building blocks without residual connections. Each Crescendo \nblock contains independent convolution paths with increased depths. The numbers \nof convolution layers and parameters are only increased linearly in Crescendo \nblocks. In experiments, CrescendoNet with only 15 layers outperforms almost all \nnetworks without residual connections on benchmark datasets, CIFAR10, CIFAR100, \nand SVHN. Given sufficient amount of data as in SVHN dataset, CrescendoNet with \n15 layers and 4.1M parameters can match the performance of DenseNet-BC with 250 \nlayers and 15.3M parameters. CrescendoNet provides a new way to construct high \nperformance deep convolutional neural networks without residual connections. \nMoreover, through investigating the behavior and performance of subnetworks in \nCrescendoNet, we note that the high performance of CrescendoNet may come from \nits implicit ensemble behavior, which differs from the FractalNet that is also \na deep convolutional neural network without residual connections. Furthermore, \nthe independence between paths in CrescendoNet allows us to introduce a new \npath-wise training procedure, which can reduce the memory needed for training. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390604156", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b150b2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.11176"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Filipe Rolim Cordeiro, Wellington Pinheiro dos Santos, Abel Guilhermino da Silva Filho", "title": "A semi-supervised fuzzy GrowCut algorithm to segment and classify regions of interest of mammographic images. (arXiv:1801.01443v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.01443", "type": "text/html"}], "timestampUsec": "1515390155460501", "comments": [], "summary": {"content": "<p>According to the World Health Organization, breast cancer is the most common \nform of cancer in women. It is the second leading cause of death among women \nround the world, becoming the most fatal form of cancer. Mammographic image \nsegmentation is a fundamental task to support image analysis and diagnosis, \ntaking into account shape analysis of mammary lesions and their borders. \nHowever, mammogram segmentation is a very hard process, once it is highly \ndependent on the types of mammary tissues. In this work we present a new \nsemi-supervised segmentation algorithm based on the modification of the GrowCut \nalgorithm to perform automatic mammographic image segmentation once a region of \ninterest is selected by a specialist. In our proposal, we used fuzzy Gaussian \nmembership functions to modify the evolution rule of the original GrowCut \nalgorithm, in order to estimate the uncertainty of a pixel being object or \nbackground. The main impact of the proposed method is the significant reduction \nof expert effort in the initialization of seed points of GrowCut to perform \naccurate segmentation, once it removes the need of selection of background \nseeds. We also constructed an automatic point selection process based on the \nsimulated annealing optimization method, avoiding the need of human \nintervention. The proposed approach was qualitatively compared with other \nstate-of-the-art segmentation techniques, considering the shape of segmented \nregions. In order to validate our proposal, we built an image classifier using \na classical multilayer perceptron. We used Zernike moments to extract segmented \nimage features. This analysis employed 685 mammograms from IRMA breast cancer \ndatabase, using fat and fibroid tissues. Results show that the proposed \ntechnique could achieve a classification rate of 91.28\\% for fat tissues, \nevidencing the feasibility of our approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155461", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09104", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01443"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michael Witbrock, Marco Zagha", "title": "An Implementation of Back-Propagation Learning on GF11, a Large SIMD Parallel Computer. (arXiv:1801.01554v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.01554", "type": "text/html"}], "timestampUsec": "1515390155460500", "comments": [], "summary": {"content": "<p>Current connectionist simulations require huge computational resources. We \ndescribe a neural network simulator for the IBM GF11, an experimental SIMD \nmachine with 566 processors and a peak arithmetic performance of 11 Gigaflops. \nWe present our parallel implementation of the backpropagation learning \nalgorithm, techniques for increasing efficiency, performance measurements on \nthe NetTalk text-to-speech benchmark, and a performance model for the \nsimulator. Our simulator currently runs the back-propagation learning algorithm \nat 900 million connections per second, where each \"connection per second\" \nincludes both a forward and backward pass. This figure was obtained on the \nmachine when only 356 processors were working; with all 566 processors \noperational, our simulation will run at over one billion connections per \nsecond. We conclude that the GF11 is well-suited to neural network simulation, \nand we analyze our use of the machine to determine which features are the most \nimportant for high performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155461", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09108", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01554"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Meik D&#xf6;rpinghaus, Izaak Neri, &#xc9;dgar Rold&#xe1;n, Heinrich Meyr, Frank J&#xfc;licher", "title": "Testing Optimality of Sequential Decision-Making. (arXiv:1801.01574v1 [cs.IT])", "alternate": [{"href": "http://arxiv.org/abs/1801.01574", "type": "text/html"}], "timestampUsec": "1515390155460499", "comments": [], "summary": {"content": "<p>This paper provides a statistical method to test whether a system that \nperforms a binary sequential hypothesis test is optimal in the sense of \nminimizing the average decision times while taking decisions with given \nreliabilities. The proposed method requires samples of the decision times, the \ndecision outcomes, and the true hypotheses, but does not require knowledge on \nthe statistics of the observations or the properties of the decision-making \nsystem. The method is based on fluctuation relations for decision time \ndistributions which are proved for sequential probability ratio tests. These \nrelations follow from the martingale property of probability ratios and hold \nunder fairly general conditions. We illustrate these tests with numerical \nexperiments and discuss potential applications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155461", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09115", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01574"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "David Charte, Francisco Charte, Salvador Garc&#xed;a, Mar&#xed;a J. del Jesus, Francisco Herrera", "title": "A practical tutorial on autoencoders for nonlinear feature fusion: Taxonomy, models, software and guidelines. (arXiv:1801.01586v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.01586", "type": "text/html"}], "timestampUsec": "1515390155460498", "comments": [], "summary": {"content": "<p>Many of the existing machine learning algorithms, both supervised and \nunsupervised, depend on the quality of the input characteristics to generate a \ngood model. The amount of these variables is also important, since performance \ntends to decline as the input dimensionality increases, hence the interest in \nusing feature fusion techniques, able to produce feature sets that are more \ncompact and higher level. A plethora of procedures to fuse original variables \nfor producing new ones has been developed in the past decades. The most basic \nones use linear combinations of the original variables, such as PCA (Principal \nComponent Analysis) and LDA (Linear Discriminant Analysis), while others find \nmanifold embeddings of lower dimensionality based on non-linear combinations, \nsuch as Isomap or LLE (Linear Locally Embedding) techniques. \n</p> \n<p>More recently, autoencoders (AEs) have emerged as an alternative to manifold \nlearning for conducting nonlinear feature fusion. Dozens of AE models have been \nproposed lately, each with its own specific traits. Although many of them can \nbe used to generate reduced feature sets through the fusion of the original \nones, there also AEs designed with other applications in mind. \n</p> \n<p>The goal of this paper is to provide the reader with a broad view of what an \nAE is, how they are used for feature fusion, a taxonomy gathering a broad range \nof models, and how they relate to other classical techniques. In addition, a \nset of didactic guidelines on how to choose the proper AE for a given task is \nsupplied, together with a discussion of the software tools available. Finally, \ntwo case studies illustrate the usage of AEs with datasets of handwritten \ndigits and breast cancer. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155461", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b0911f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01586"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Qinxue Meng, Jia Wu, John Ellisy, Paul J. Kennedy", "title": "Dynamic Island Model based on Spectral Clustering in Genetic Algorithm. (arXiv:1801.01620v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.01620", "type": "text/html"}], "timestampUsec": "1515390155460497", "comments": [], "summary": {"content": "<p>How to maintain relative high diversity is important to avoid premature \nconvergence in population-based optimization methods. Island model is widely \nconsidered as a major approach to achieve this because of its flexibility and \nhigh efficiency. The model maintains a group of sub-populations on different \nislands and allows sub-populations to interact with each other via predefined \nmigration policies. However, current island model has some drawbacks. One is \nthat after a certain number of generations, different islands may retain quite \nsimilar, converged sub-populations thereby losing diversity and decreasing \nefficiency. Another drawback is that determining the number of islands to \nmaintain is also very challenging. Meanwhile initializing many sub-populations \nincreases the randomness of island model. To address these issues, we proposed \na dynamic island model~(DIM-SP) which can force each island to maintain \ndifferent sub-populations, control the number of islands dynamically and starts \nwith one sub-population. The proposed island model outperforms the other three \nstate-of-the-art island models in three baseline optimization problems \nincluding job shop scheduler problem, travelling salesmen problem and quadratic \nmultiple knapsack problem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155461", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09127", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01620"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Adriano Barra, Matteo Beccaria, Alberto Fachechi", "title": "A relativistic extension of Hopfield neural networks via the mechanical analogy. (arXiv:1801.01743v1 [cond-mat.dis-nn])", "alternate": [{"href": "http://arxiv.org/abs/1801.01743", "type": "text/html"}], "timestampUsec": "1515390155460496", "comments": [], "summary": {"content": "<p>We propose a modification of the cost function of the Hopfield model whose \nsalient features shine in its Taylor expansion and result in more than pairwise \ninteractions with alternate signs, suggesting a unified framework for handling \nboth with deep learning and network pruning. In our analysis, we heavily rely \non the Hamilton-Jacobi correspondence relating the statistical model with a \nmechanical system. In this picture, our model is nothing but the relativistic \nextension of the original Hopfield model (whose cost function is a quadratic \nform in the Mattis magnetization which mimics the non-relativistic Hamiltonian \nfor a free particle). We focus on the low-storage regime and solve the model \nanalytically by taking advantage of the mechanical analogy, thus obtaining a \ncomplete characterization of the free energy and the associated \nself-consistency equations in the thermodynamic limit. On the numerical side, \nwe test the performances of our proposal with MC simulations, showing that the \nstability of spurious states (limiting the capabilities of the standard Hebbian \nconstruction) is sensibly reduced due to presence of unlearning contributions \nin this extended framework. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155461", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b0912f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01743"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "David Kappel, Robert Legenstein, Stefan Habenschuss, Michael Hsieh, Wolfgang Maass", "title": "A dynamic connectome supports the emergence of stable computational function of neural circuits through reward-based learning. (arXiv:1704.04238v4 [q-bio.NC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.04238", "type": "text/html"}], "timestampUsec": "1515390155460495", "comments": [], "summary": {"content": "<p>Synaptic connections between neurons in the brain are dynamic because of \ncontinuously ongoing spine dynamics, axonal sprouting, and other processes. In \nfact, it was recently shown that the spontaneous synapse-autonomous component \nof spine dynamics is at least as large as the component that depends on the \nhistory of pre- and postsynaptic neural activity. These data are inconsistent \nwith common models for network plasticity, and raise the questions how neural \ncircuits can maintain a stable computational function in spite of these \ncontinuously ongoing processes, and what functional uses these ongoing \nprocesses might have. Here, we present a rigorous theoretical framework for \nthese seemingly stochastic spine dynamics and rewiring processes in the context \nof reward-based learning tasks. We show that spontaneous synapse-autonomous \nprocesses, in combination with reward signals such as dopamine, can explain the \ncapability of networks of neurons in the brain to configure themselves for \nspecific computational tasks, and to compensate automatically for later changes \nin the network or task. Furthermore we show theoretically and through computer \nsimulations that stable computational performance is compatible with \ncontinuously ongoing synapse-autonomous changes. After reaching good \ncomputational performance it causes primarily a slow drift of network \narchitecture and dynamics in task-irrelevant dimensions, as observed for neural \nactivity in motor cortex and other areas. On the more abstract level of \nreinforcement learning the resulting model gives rise to an understanding of \nreward-driven network plasticity as continuous sampling of network \nconfigurations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155461", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09133", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.04238"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Reinhard Heckel, Max Simchowitz, Kannan Ramchandran, Martin J. Wainwright", "title": "Approximate Ranking from Pairwise Comparisons. (arXiv:1801.01253v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.01253", "type": "text/html"}], "timestampUsec": "1515390155460492", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329664219\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329664219&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A common problem in machine learning is to rank a set of n items based on \npairwise comparisons. Here ranking refers to partitioning the items into sets \nof pre-specified sizes according to their scores, which includes identification \nof the top-k items as the most prominent special case. The score of a given \nitem is defined as the probability that it beats a randomly chosen other item. \nFinding an exact ranking typically requires a prohibitively large number of \ncomparisons, but in practice, approximate rankings are often adequate. \nAccordingly, we study the problem of finding approximate rankings from pairwise \ncomparisons. We analyze an active ranking algorithm that counts the number of \ncomparisons won, and decides whether to stop or which pair of items to compare \nnext, based on confidence intervals computed from the data collected in \nprevious steps. We show that this algorithm succeeds in recovering approximate \nrankings using a number of comparisons that is close to optimal up to \nlogarithmic factors. We also present numerical results, showing that in \npractice, approximation can drastically reduce the number of comparisons \nrequired to estimate a ranking. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09149", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01253"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine", "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. (arXiv:1801.01290v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.01290", "type": "text/html"}], "timestampUsec": "1515390155460491", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3296d0503\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3296d0503&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Model-free deep reinforcement learning (RL) algorithms have been demonstrated \non a range of challenging decision making and control tasks. However, these \nmethods typically suffer from two major challenges: very high sample complexity \nand brittle convergence properties, which necessitate meticulous hyperparameter \ntuning. Both of these challenges severely limit the applicability of such \nmethods to complex, real-world domains. In this paper, we propose soft \nactor-critic, an off-policy actor-critic deep RL algorithm based on the maximum \nentropy reinforcement learning framework. In this framework, the actor aims to \nmaximize expected reward while also maximizing entropy - that is, succeed at \nthe task while acting as randomly as possible. Prior deep RL methods based on \nthis framework have been formulated as Q-learning methods. By combining \noff-policy updates with a stable stochastic actor-critic formulation, our \nmethod achieves state-of-the-art performance on a range of continuous control \nbenchmark tasks, outperforming prior on-policy and off-policy methods. \nFurthermore, we demonstrate that, in contrast to other off-policy algorithms, \nour approach is very stable, achieving very similar performance across \ndifferent random seeds. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09154", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01290"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Louise Dennis, Michael Fisher", "title": "Practical Challenges in Explicit Ethical Machine Reasoning. (arXiv:1801.01422v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.01422", "type": "text/html"}], "timestampUsec": "1515390155460490", "comments": [], "summary": {"content": "<p>We examine implemented systems for ethical machine reasoning with a view to \nidentifying the practical challenges (as opposed to philosophical challenges) \nposed by the area. We identify a need for complex ethical machine reasoning not \nonly to be multi-objective, proactive, and scrutable but that it must draw on \nheterogeneous evidential reasoning. We also argue that, in many cases, it needs \nto operate in real time and be verifiable. We propose a general architecture \ninvolving a declarative ethical arbiter which draws upon multiple evidential \nreasoners each responsible for a particular ethical feature of the system's \nenvironment. We claim that this architecture enables some separation of \nconcerns among the practical challenges that ethical machine reasoning poses. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b0915c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01422"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Martin Lackner, Piotr Skowron", "title": "A Quantitative Analysis of Multi-Winner Rules. (arXiv:1801.01527v1 [cs.MA])", "alternate": [{"href": "http://arxiv.org/abs/1801.01527", "type": "text/html"}], "timestampUsec": "1515390155460489", "comments": [], "summary": {"content": "<p>To choose a multi-winner rule, i.e., a voting rule that selects a subset of \n$k$ alternatives based on preferences of a certain population, is a hard and \nambiguous task. Depending on the context, it varies widely what constitutes an \n\"optimal\" committee. In this paper, we offer a new perspective to measure the \nquality of committees and---consequently---multi-winner rules. We provide a \nquantitative analysis using methods from the theory of approximation algorithms \nand estimate how well multi-winner rules approximate two extreme objectives: \ndiversity as captured by the (Approval) Chamberlin--Courant rule (CC) and \nindividual excellence as captured by Approval Voting (AV). With both \ntheoretical and experimental methods we establish a classification of \nmulti-winner rules in terms of their quantitative alignment with these two \nopposing objectives. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b0916b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01527"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jiazhuo Wang, Jason Xu, Xuejun Wang", "title": "Combination of Hyperband and Bayesian Optimization for Hyperparameter Optimization in Deep Learning. (arXiv:1801.01596v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.01596", "type": "text/html"}], "timestampUsec": "1515390155460488", "comments": [], "summary": {"content": "<p>Deep learning has achieved impressive results on many problems. However, it \nrequires high degree of expertise or a lot of experience to tune well the \nhyperparameters, and such manual tuning process is likely to be biased. \nMoreover, it is not practical to try out as many different hyperparameter \nconfigurations in deep learning as in other machine learning scenarios, because \nevaluating each single hyperparameter configuration in deep learning would mean \ntraining a deep neural network, which usually takes quite long time. Hyperband \nalgorithm achieves state-of-the-art performance on various hyperparameter \noptimization problems in the field of deep learning. However, Hyperband \nalgorithm does not utilize history information of previous explored \nhyperparameter configurations, thus the solution found is suboptimal. We \npropose to combine Hyperband algorithm with Bayesian optimization (which does \nnot ignore history when sampling next trial configuration). Experimental \nresults show that our combination approach is superior to other hyperparameter \noptimization approaches including Hyperband algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b0917c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01596"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Han Xiao", "title": "Intelligence Graph. (arXiv:1801.01604v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.01604", "type": "text/html"}], "timestampUsec": "1515390155460487", "comments": [], "summary": {"content": "<p>In fact, there exist three genres of intelligence architectures: logics (e.g. \n\\textit{Random Forest, A$^*$ Searching}), neurons (e.g. \\textit{CNN, LSTM}) and \nprobabilities (e.g. \\textit{Naive Bayes, HMM}), all of which are incompatible \nto each other. However, to construct powerful intelligence systems with various \nmethods, we propose the intelligence graph (short as \\textbf{\\textit{iGraph}}), \nwhich is composed by both of neural and probabilistic graph, under the \nframework of forward-backward propagation. By the paradigm of iGraph, we design \na recommendation model with semantic principle. First, the probabilistic \ndistributions of categories are generated from the embedding representations of \nusers/items, in the manner of neurons. Second, the probabilistic graph infers \nthe distributions of features, in the manner of probabilities. Last, for the \nrecommendation diversity, we perform an expectation computation then conduct a \nlogic judgment, in the manner of logics. Experimentally, we beat the \nstate-of-the-art baselines and verify our conclusions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09183", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01604"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Olivier Cailloux (LAMSADE), S&#xe9;bastien Destercke (Labex MS2T)", "title": "Reasons and Means to Model Preferences as Incomplete. (arXiv:1801.01657v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.01657", "type": "text/html"}], "timestampUsec": "1515390155460486", "comments": [], "summary": {"content": "<p>Literature involving preferences of artificial agents or human beings often \nassume their preferences can be represented using a complete transitive binary \nrelation. Much has been written however on different models of preferences. We \nreview some of the reasons that have been put forward to justify more complex \nmodeling, and review some of the techniques that have been proposed to obtain \nmodels of such preferences. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b0918c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01657"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516122776, "author": "Javier Mata, Ignacio de Miguel, Ram&#xf3; n J. Dur&#xe1; n, Noem&#xed; Merayo, Sandeep Kumar Singh, Admela Jukan, Mohit Chamania", "title": "Artificial Intelligence (AI) Methods in Optical Networks: A Comprehensive Survey. (arXiv:1801.01704v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1801.01704", "type": "text/html"}], "timestampUsec": "1515390155460485", "comments": [], "summary": {"content": "<p>Artificial intelligence (AI) is an extensive scientific discipline which \nenables computer systems to solve problems by emulating complex biological \nprocesses such as learning, reasoning and self-correction. This paper presents \na comprehensive review of the application of AI techniques for improving \nperformance of optical communication systems and networks. The use of AI-based \ntechniques is first studied in applications related to optical transmission, \nranging from the characterization and operation of network components to \nperformance monitoring, mitigation of nonlinearities, and quality of \ntransmission estimation. Then, applications related to optical network control \nand management are also reviewed, including topics like optical network \nplanning and operation in both transport and access networks. Finally, the \npaper also presents a summary of opportunities and challenges in optical \nnetworking where AI is expected to play a key role in the near future. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1516122776, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09192", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01704"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Martijn van Otterlo", "title": "Gatekeeping Algorithms with Human Ethical Bias: The ethics of algorithms in archives, libraries and society. (arXiv:1801.01705v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.01705", "type": "text/html"}], "timestampUsec": "1515390155460484", "comments": [], "summary": {"content": "<p>In the age of algorithms, I focus on the question of how to ensure algorithms \nthat will take over many of our familiar archival and library tasks, will \nbehave according to human ethical norms that have evolved over many years. I \nstart by characterizing physical archives in the context of related \ninstitutions such as libraries and museums. In this setting I analyze how \nethical principles, in particular about access to information, have been \nformalized and communicated in the form of ethical codes, or: codes of \nconducts. After that I describe two main developments: digitalization, in which \nphysical aspects of the world are turned into digital data, and \nalgorithmization, in which intelligent computer programs turn this data into \npredictions and decisions. Both affect interactions that were once physical but \nnow digital. In this new setting I survey and analyze the ethical aspects of \nalgorithms and how they shape a vision on the future of archivists and \nlibrarians, in the form of algorithmic documentalists, or: codementalists. \nFinally I outline a general research strategy, called IntERMEeDIUM, to obtain \nalgorithms that obey are human ethical values encoded in code of ethics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09196", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01705"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Purushottam D. Dixit", "title": "Entropy production rate as a criterion for inconsistency in decision theory. (arXiv:1801.01733v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.01733", "type": "text/html"}], "timestampUsec": "1515390155460483", "comments": [], "summary": {"content": "<p>Evaluating pairwise comparisons breaks down complex decision problems into \ntractable ones. Pairwise comparison matrices (PCMs) are regularly used to solve \nmultiple-criteria decision-making (MCDM) problems using Saaty's analytic \nhierarchy process (AHP) framework. There are two significant drawbacks of using \nPCMs. First, humans evaluate PCM in an inconsistent manner. Second, PCMs of \nlarge problems often have missing entries. We address these two issues by first \nestablishing a novel connection between PCMs and time-irreversible Markov \nprocesses. Specifically, we show that every PCM induces a family of dissipative \nmaximum path entropy random walks (MERW) over the set of alternatives. We show \nthat only `consistent' PCMs correspond to detailed balanced MERWs. We identify \nthe non-equilibrium entropy production in the induced MERWs as a metric of \ninconsistency of the underlying PCMs. Notably, the entropy production satisfies \nall of the recently laid out criteria for reasonable consistency indices. We \nalso propose an approach to use incompletely filled PCMs in AHP. Potential \nfuture avenues are discussed as well. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b091b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01733"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Corina Florescu, Wei Jin", "title": "Learning Feature Representations for Keyphrase Extraction. (arXiv:1801.01768v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1801.01768", "type": "text/html"}], "timestampUsec": "1515390155460482", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3296d06f9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3296d06f9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In supervised approaches for keyphrase extraction, a candidate phrase is \nencoded with a set of hand-crafted features and machine learning algorithms are \ntrained to discriminate keyphrases from non-keyphrases. Although the \nmanually-designed features have shown to work well in practice, feature \nengineering is a difficult process that requires expert knowledge and normally \ndoes not generalize well. In this paper, we present SurfKE, a feature learning \nframework that exploits the text itself to automatically discover patterns that \nkeyphrases exhibit. Our model represents the document as a graph and \nautomatically learns feature representation of phrases. The proposed model \nobtains remarkable improvements in performance over strong baselines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b091c5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01768"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Karl Schlechta", "title": "A Reliability Theory of Truth. (arXiv:1801.01788v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.01788", "type": "text/html"}], "timestampUsec": "1515390155460481", "comments": [], "summary": {"content": "<p>Our approach is basically a coherence approach, but we avoid the well-known \npitfalls of coherence theories of truth. Consistency is replaced by \nreliability, which expresses support and attack, and, in principle, every \ntheory (or agent, message) counts. At the same time, we do not require a \npriviledged access to \"reality\". A centerpiece of our approach is that we \nattribute reliability also to agents, messages, etc., so an unreliable source \nof information will be less important in future. Our ideas can also be extended \nto value systems, and even actions, e.g., of animals. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b091cd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01788"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "N. Bushaw, C. E. Larson, N. Van Cleemput (and Summer 2017 Graph Brain Project Workshop Participants)", "title": "Automated Conjecturing VII: The Graph Brain Project & Big Mathematics. (arXiv:1801.01814v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.01814", "type": "text/html"}], "timestampUsec": "1515390155460480", "comments": [], "summary": {"content": "<p>The Graph Brain Project is an experiment in how the use of automated \nmathematical discovery software, databases, large collaboration, and systematic \ninvestigation provide a model for how mathematical research might proceed in \nthe future. \n</p> \n<p>Our Project began with the development of a program that can be used to \ngenerate invariant-relation and property-relation conjectures in many areas of \nmathematics. This program can produce conjectures which are not implied by \nexisting (published) theorems. Here we propose a new approach to push forward \nexisting mathematical research goals---using automated mathematical discovery \nsoftware. We suggest how to initiate and harness large-scale collaborative \nmathematics. We envision mathematical research labs similar to what exist in \nother sciences, new avenues for funding, new opportunities for training \nstudents, and a more efficient and effective use of published mathematical \nresearch. \n</p> \n<p>And our experiment in graph theory can be imitated in many other areas of \nmathematical research. Big Mathematics is the idea of large, systematic, \ncollaborative research on problems of existing mathematical interest. What is \npossible when we put our skills, tools, and results together systematically? \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b091d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01814"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Haifeng Xu, Milind Tambe, Shaddin Dughmi, Venil Loyd Noronha", "title": "Mitigating the Curse of Correlation in Security Games by Entropy Maximization. (arXiv:1703.03912v2 [cs.GT] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.03912", "type": "text/html"}], "timestampUsec": "1515390155460479", "comments": [], "summary": {"content": "<p>In Stackelberg security games, a defender seeks to randomly allocate limited \nsecurity resources to protect critical targets from an attack. In this paper, \nwe study a fundamental, yet underexplored, phenomenon in security games, which \nwe term the \\emph{Curse of Correlation} (CoC). Specifically, we observe that \nthere are inevitable correlations among the protection status of different \ntargets. Such correlation is a crucial concern, especially in \n\\emph{spatio-temporal} domains like conservation area patrolling, where \nattackers can surveil patrollers at certain areas and then infer their \npatrolling routes using such correlations. To mitigate this issue, we propose \nto design entropy-maximizing defending strategies for spatio-temporal security \ngames, which frequently suffer from CoC. We prove that the problem is \\#P-hard \nin general. However, it admits efficient algorithms in well-motivated special \nsettings. Our experiments show significant advantages of max-entropy algorithms \nover previous algorithms. A scalable implementation of our algorithm is \ncurrently under pre-deployment testing for integration into FAMS software to \nimprove the scheduling of US federal air marshals. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b091de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.03912"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Raghuram Bharadwaj Diddigi, Prabuchandran K.J., Shalabh Bhatnagar", "title": "Novel Sensor Scheduling Scheme for Intruder Tracking in Energy Efficient Sensor Networks. (arXiv:1708.08113v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.08113", "type": "text/html"}], "timestampUsec": "1515390155460478", "comments": [], "summary": {"content": "<p>We consider the problem of tracking an intruder using a network of wireless \nsensors. For tracking the intruder at each instant, the optimal number and the \nright configuration of sensors has to be powered. As powering the sensors \nconsumes energy, there is a trade off between accurately tracking the position \nof the intruder at each instant and the energy consumption of sensors. This \nproblem has been formulated in the framework of Partially Observable Markov \nDecision Process (POMDP). Even for the simplest model considered in [1], the \ncurse of dimensionality renders the problem intractable. We formulate this \nproblem with a suitable state-action space in the framework of POMDP and \ndevelop a reinforcement learning algorithm utilising the Upper Confidence Tree \nSearch (UCT) method to mitigate the state-action space explosion. Through \nsimulations, we illustrate that our algorithm scales well with the increasing \nstate and action space. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b091e8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.08113"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Changshuai Wei, Qing Lu", "title": "Generalized Similarity U: A Non-parametric Test of Association Based on Similarity. (arXiv:1801.01220v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1801.01220", "type": "text/html"}], "timestampUsec": "1515390155460476", "comments": [], "summary": {"content": "<p>Second generation sequencing technologies are being increasingly used for \ngenetic association studies, where the main research interest is to identify \nsets of genetic variants that contribute to various phenotype. The phenotype \ncan be univariate disease status, multivariate responses and even \nhigh-dimensional outcomes. Considering the genotype and phenotype as two \ncomplex objects, this also poses a general statistical problem of testing \nassociation between complex objects. We here proposed a similarity-based test, \ngeneralized similarity U (GSU), that can test the association between complex \nobjects. We first studied the theoretical properties of the test in a general \nsetting and then focused on the application of the test to sequencing \nassociation studies. Based on theoretical analysis, we proposed to use \nLaplacian kernel based similarity for GSU to boost power and enhance \nrobustness. Through simulation, we found that GSU did have advantages over \nexisting methods in terms of power and robustness. We further performed a whole \ngenome sequencing (WGS) scan for Alzherimer Disease Neuroimaging Initiative \n(ADNI) data, identifying three genes, APOE, APOC1 and TOMM40, associated with \nimaging phenotype. We developed a C++ package for analysis of whole genome \nsequencing data using GSU. The source codes can be downloaded at \nhttps://github.com/changshuaiwei/gsu. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b091ec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01220"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Maziar Raissi, Paris Perdikaris, George Em Karniadakis", "title": "Multistep Neural Networks for Data-driven Discovery of Nonlinear Dynamical Systems. (arXiv:1801.01236v1 [math.DS])", "alternate": [{"href": "http://arxiv.org/abs/1801.01236", "type": "text/html"}], "timestampUsec": "1515390155460475", "comments": [], "summary": {"content": "<p>The process of transforming observed data into predictive mathematical models \nof the physical world has always been paramount in science and engineering. \nAlthough data is currently being collected at an ever-increasing pace, devising \nmeaningful models out of such observations in an automated fashion still \nremains an open problem. In this work, we put forth a machine learning approach \nfor identifying nonlinear dynamical systems from data. Specifically, we blend \nclassical tools from numerical analysis, namely the multi-step time-stepping \nschemes, with powerful nonlinear function approximators, namely deep neural \nnetworks, to distill the mechanisms that govern the evolution of a given \ndata-set. We test the effectiveness of our approach for several benchmark \nproblems involving the identification of complex, nonlinear and chaotic \ndynamics, and we demonstrate how this allows us to accurately learn the \ndynamics, forecast future states, and identify basins of attraction. In \nparticular, we study the Lorenz system, the fluid flow behind a cylinder, the \nHopf bifurcation, and the Glycoltic oscillator model as an example of \ncomplicated nonlinear dynamics typical of biological systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b091f8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01236"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sunrita Poddar, Mathews Jacob", "title": "Clustering of Data with Missing Entries. (arXiv:1801.01455v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.01455", "type": "text/html"}], "timestampUsec": "1515390155460474", "comments": [], "summary": {"content": "<p>The analysis of large datasets is often complicated by the presence of \nmissing entries, mainly because most of the current machine learning algorithms \nare designed to work with full data. The main focus of this work is to \nintroduce a clustering algorithm, that will provide good clustering even in the \npresence of missing data. The proposed technique solves an $\\ell_0$ fusion \npenalty based optimization problem to recover the clusters. We theoretically \nanalyze the conditions needed for the successful recovery of the clusters. We \nalso propose an algorithm to solve a relaxation of this problem using \nsaturating non-convex fusion penalties. The method is demonstrated on simulated \nand real datasets, and is observed to perform well in the presence of large \nfractions of missing entries. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b091fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01455"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Florian H&#xe4;se, Lo&#xef;c M. Roch, Christoph Kreisbeck, Al&#xe1;n Aspuru-Guzik", "title": "PHOENICS: A universal deep Bayesian optimizer. (arXiv:1801.01469v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.01469", "type": "text/html"}], "timestampUsec": "1515390155460473", "comments": [], "summary": {"content": "<p>In this work we introduce PHOENICS, a probabilistic global optimization \nalgorithm combining ideas from Bayesian optimization with concepts from \nBayesian kernel density estimation. We propose an inexpensive acquisition \nfunction balancing the explorative and exploitative behavior of the algorithm. \nThis acquisition function enables intuitive sampling strategies for an \nefficient parallel search of global minima. The performance of PHOENICS is \nassessed via an exhaustive benchmark study on a set of 15 discrete, \nquasi-discrete and continuous multidimensional functions. Unlike optimization \nmethods based on Gaussian processes (GP) and random forests (RF), we show that \nPHOENICS is less sensitive to the nature of the co-domain, and outperforms GP \nand RF optimizations. We illustrate the performance of PHOENICS on the \nOregonator, a difficult case-study describing a complex chemical reaction \nnetwork. We demonstrate that only PHOENICS was able to reproduce qualitatively \nand quantitatively the target dynamic behavior of this nonlinear reaction \ndynamics. We recommend PHOENICS for rapid optimization of scalar, possibly \nnon-convex, black-box unknown objective functions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09209", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01469"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sungsoo Ahn, Michael Chertkov, Jinwoo Shin, Adrian Weller", "title": "Gauged Mini-Bucket Elimination for Approximate Inference. (arXiv:1801.01649v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.01649", "type": "text/html"}], "timestampUsec": "1515390155460472", "comments": [], "summary": {"content": "<p>Computing the partition function $Z$ of a discrete graphical model is a \nfundamental inference challenge. Since this is computationally intractable, \nvariational approximations are often used in practice. Recently, so-called \ngauge transformations were used to improve variational lower bounds on $Z$. In \nthis paper, we propose a new gauge-variational approach, termed WMBE-G, which \ncombines gauge transformations with the weighted mini-bucket elimination (WMBE) \nmethod. WMBE-G can provide both upper and lower bounds on $Z$, and is easier to \noptimize than the prior gauge-variational algorithm. We show that WMBE-G \nstrictly improves the earlier WMBE approximation for symmetric models including \nIsing models with no magnetic field. Our experimental results demonstrate the \neffectiveness of WMBE-G even for generic, nonsymmetric models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09211", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01649"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Olivier Gouvert, Thomas Oberlin, C&#xe9;dric F&#xe9;votte", "title": "Negative Binomial Matrix Factorization for Recommender Systems. (arXiv:1801.01708v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.01708", "type": "text/html"}], "timestampUsec": "1515390155460471", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3296d08cb\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3296d08cb&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We introduce negative binomial matrix factorization (NBMF), a matrix \nfactorization technique specially designed for analyzing over-dispersed count \ndata. It can be viewed as an extension of Poisson matrix factorization (PF) \nperturbed by a multiplicative term which models exposure. This term brings a \ndegree of freedom for controlling the dispersion, making NBMF more robust to \noutliers. We show that NBMF allows to skip traditional pre-processing stages, \nsuch as binarization, which lead to loss of information. Two estimation \napproaches are presented: maximum likelihood and variational Bayes inference. \nWe test our model with a recommendation task and show its ability to predict \nuser tastes with better precision than PF. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09218", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01708"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Melody Y. Guan, Heinrich Jiang", "title": "Nonparametric Stochastic Contextual Bandits. (arXiv:1801.01750v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.01750", "type": "text/html"}], "timestampUsec": "1515390155460470", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32972fd72\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32972fd72&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We analyze the $K$-armed bandit problem where the reward for each arm is a \nnoisy realization based on an observed context under mild nonparametric \nassumptions. We attain tight results for top-arm identification and a sublinear \nregret of $\\widetilde{O}\\Big(T^{\\frac{1+D}{2+D}}\\Big)$, where $D$ is the \ncontext dimension, for a modified UCB algorithm that is simple to implement \n($k$NN-UCB). We then give global intrinsic dimension dependent and ambient \ndimension independent regret bounds. We also discuss recovering topological \nstructures within the context space based on expected bandit performance and \nprovide an extension to infinite-armed contextual bandits. Finally, we \nexperimentally show the improvement of our algorithm over existing multi-armed \nbandit approaches for both simulated tasks and MNIST image classification. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b0921f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01750"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Louis Filstroff, Alberto Lumbreras, C&#xe9;dric F&#xe9;votte", "title": "Closed-form marginal likelihood in Gamma-Poisson factorization. (arXiv:1801.01799v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.01799", "type": "text/html"}], "timestampUsec": "1515390155460469", "comments": [], "summary": {"content": "<p>We present novel understandings of the Gamma-Poisson (GaP) model, a \nprobabilistic matrix factorization model for count data. We show that GaP can \nbe rewritten free of the score/activation matrix. This gives us new insights \nabout the estimation of the topic/dictionary matrix by maximum marginal \nlikelihood estimation. In particular, this explains the robustness of this \nestimator to over-specified values of the factorization rank and in particular \nits ability to automatically prune spurious dictionary columns, as empirically \nobserved in previous work. The marginalization of the activation matrix leads \nin turn to a new Monte-Carlo Expectation-Maximization algorithm with favorable \nproperties. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09227", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01799"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alexandre Belloni, Victor Chernozhukov, Ivan Fern&#xe1;ndez-Val, Christian Hansen", "title": "Program Evaluation and Causal Inference with High-Dimensional Data. (arXiv:1311.2645v8 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1311.2645", "type": "text/html"}], "timestampUsec": "1515390155460468", "comments": [], "summary": {"content": "<p>In this paper, we provide efficient estimators and honest confidence bands \nfor a variety of treatment effects including local average (LATE) and local \nquantile treatment effects (LQTE) in data-rich environments. We can handle very \nmany control variables, endogenous receipt of treatment, heterogeneous \ntreatment effects, and function-valued outcomes. Our framework covers the \nspecial case of exogenous receipt of treatment, either conditional on controls \nor unconditionally as in randomized control trials. In the latter case, our \napproach produces efficient estimators and honest bands for (functional) \naverage treatment effects (ATE) and quantile treatment effects (QTE). To make \ninformative inference possible, we assume that key reduced form predictive \nrelationships are approximately sparse. This assumption allows the use of \nregularization and selection methods to estimate those relations, and we \nprovide methods for post-regularization and post-selection inference that are \nuniformly valid (honest) across a wide-range of models. We show that a key \ningredient enabling honest inference is the use of orthogonal or doubly robust \nmoment conditions in estimating certain reduced form functional parameters. We \nillustrate the use of the proposed methods with an application to estimating \nthe effect of 401(k) eligibility and participation on accumulated assets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09231", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1311.2645"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Keisuke Yamazaki", "title": "Bayesian Estimation of Multidimensional Latent Variables and Its Asymptotic Accuracy. (arXiv:1510.01003v6 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1510.01003", "type": "text/html"}], "timestampUsec": "1515390155460467", "comments": [], "summary": {"content": "<p>Hierarchical learning models, such as mixture models and Bayesian networks, \nare widely employed for unsupervised learning tasks, such as clustering \nanalysis. They consist of observable and hidden variables, which represent the \ngiven data and their hidden generation process, respectively. It has been \npointed out that conventional statistical analysis is not applicable to these \nmodels, because redundancy of the latent variable produces singularities in the \nparameter space. In recent years, a method based on algebraic geometry has \nallowed us to analyze the accuracy of predicting observable variables when \nusing Bayesian estimation. However, how to analyze latent variables has not \nbeen sufficiently studied, even though one of the main issues in unsupervised \nlearning is to determine how accurately the latent variable is estimated. A \nprevious study proposed a method that can be used when the range of the latent \nvariable is redundant compared with the model generating data. The present \npaper extends that method to the situation in which the latent variables have \nredundant dimensions. We formulate new error functions and derive their \nasymptotic forms. Calculation of the error functions is demonstrated in \ntwo-layered Bayesian networks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09239", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1510.01003"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Hao Yin, Austin R. Benson, Jure Leskovec", "title": "Higher-order clustering in networks. (arXiv:1704.03913v2 [cs.SI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.03913", "type": "text/html"}], "timestampUsec": "1515390155460466", "comments": [], "summary": {"content": "<p>A fundamental property of complex networks is the tendency for edges to \ncluster. The extent of the clustering is typically quantified by the clustering \ncoefficient, which is the probability that a length-2 path is closed, i.e., \ninduces a triangle in the network. However, higher-order cliques beyond \ntriangles are crucial to understanding complex networks, and the clustering \nbehavior with respect to such higher-order network structures is not well \nunderstood. Here we introduce higher-order clustering coefficients that measure \nthe closure probability of higher-order network cliques and provide a more \ncomprehensive view of how the edges of complex networks cluster. Our \nhigher-order clustering coefficients are a natural generalization of the \ntraditional clustering coefficient. We derive several properties about \nhigher-order clustering coefficients and analyze them under common random graph \nmodels. Finally, we use higher-order clustering coefficients to gain new \ninsights into the structure of real-world networks from several domains. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b0923d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.03913"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jinghui Chen, Lingxiao Wang, Xiao Zhang, Quanquan Gu", "title": "Robust Wirtinger Flow for Phase Retrieval with Arbitrary Corruption. (arXiv:1704.06256v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.06256", "type": "text/html"}], "timestampUsec": "1515390155460465", "comments": [], "summary": {"content": "<p>We consider the robust phase retrieval problem of recovering the unknown \nsignal from the magnitude-only measurements, where the measurements can be \ncontaminated by both sparse arbitrary corruption and bounded random noise. We \npropose a new nonconvex algorithm for robust phase retrieval, namely Robust \nWirtinger Flow to jointly estimate the unknown signal and the sparse \ncorruption. We show that our proposed algorithm is guaranteed to converge \nlinearly to the unknown true signal up to a minimax optimal statistical \nprecision in such a challenging setting. Compared with existing robust phase \nretrieval methods, we achieve an optimal sample complexity of $O(n)$ in both \nnoisy and noise-free settings. Thorough experiments on both synthetic and real \ndatasets corroborate our theory. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09245", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.06256"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Philipp Petersen, Felix Voigtlaender", "title": "Optimal approximation of piecewise smooth functions using deep ReLU neural networks. (arXiv:1709.05289v3 [math.FA] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.05289", "type": "text/html"}], "timestampUsec": "1515390155460464", "comments": [], "summary": {"content": "<p>We study the necessary and sufficient complexity of ReLU neural networks-in \nterms of depth and number of weights-required for approximating classifier \nfunctions in an $L^2$-sense. \n</p> \n<p>As a model, we consider the set $\\mathcal{E}^\\beta (\\mathbb{R}^d)$ of \npossibly discontinuous piecewise $C^\\beta$ functions $f : [-1/2, 1/2]^d \\to \n\\mathbb{R}$, where the different 'smooth regions' of $f$ are separated by \n$C^\\beta$ hypersurfaces. For given dimension $d \\geq 2$, regularity $\\beta &gt; \n0$, and accuracy $\\varepsilon &gt; 0$, we construct ReLU neural networks that \napproximate functions from $\\mathcal{E}^\\beta(\\mathbb{R}^d)$ up to an $L^2$ \nerror of $\\varepsilon$. The constructed networks have a fixed number of layers, \ndepending only on $d$ and $\\beta$ and they have \n$O(\\varepsilon^{-2(d-1)/\\beta})$ many nonzero weights, which we prove to be \noptimal. \n</p> \n<p>In addition to the optimality in terms of the number of weights, we show that \nin order to achieve this optimal approximation rate, one needs ReLU networks of \na certain minimal depth. Precisely, for piecewise $C^\\beta(\\mathbb{R}^d)$ \nfunctions, this minimal depth is given-up to a multiplicative constant-by \n$\\beta/d$. Up to a log factor, our constructed networks match this bound. This \npartly explains the benefits of depth for ReLU networks by showing that deep \nnetworks are necessary to achieve efficient approximation of (piecewise) smooth \nfunctions. \n</p> \n<p>Finally, we analyze approximation in high-dimensional spaces where the \nfunction $f$ to be approximated can be factorized into a smooth dimension \nreducing feature map $\\tau$ and classifier function $g$-defined on a \nlow-dimensional feature space-as $f = g \\circ \\tau$. We show that in this case \nthe approximation rate depends only on the dimension of the feature space and \nnot the input dimension. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b0924b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.05289"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jason Poulos", "title": "Causal Inference for Observational Time-Series with Encoder-Decoder Networks. (arXiv:1712.03553v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.03553", "type": "text/html"}], "timestampUsec": "1515390155460462", "comments": [], "summary": {"content": "<p>This paper proposes a method for estimating the causal effect of a discrete \nintervention in observational time-series using encoder-decoder networks. \nEncoder-decoder networks, which are special class of recurrent neural networks \n(RNNs) suitable for handling variable-length sequential data, are used to \npredict a counterfactual time-series of treated unit outcomes using only the \npre-intervention outcomes of control units as inputs. Unlike the synthetic \ncontrol method, the proposed method does not rely on pretreatment covariates, \nallows for nonconvex combinations of control units, and can handle multiple \ntreated units. Encoder-decoder networks outperform the synthetic control method \nin simulated and empirical data applications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515390155460", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000356b09253", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03553"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Rui Ponte Costa, Yannis M. Assael, Brendan Shillingford, Nando de Freitas, Tim P. Vogels", "title": "Cortical microcircuits as gated-recurrent neural networks. (arXiv:1711.02448v2 [q-bio.NC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.02448", "type": "text/html"}], "timestampUsec": "1515046375503243", "comments": [], "summary": {"content": "<p>Cortical circuits exhibit intricate recurrent architectures that are \nremarkably similar across different brain areas. Such stereotyped structure \nsuggests the existence of common computational principles. However, such \nprinciples have remained largely elusive. Inspired by gated-memory networks, \nnamely long short-term memory networks (LSTMs), we introduce a recurrent neural \nnetwork in which information is gated through inhibitory cells that are \nsubtractive (subLSTM). We propose a natural mapping of subLSTMs onto known \ncanonical excitatory-inhibitory cortical microcircuits. Our empirical \nevaluation across sequential image classification and language modelling tasks \nshows that subLSTM units can achieve similar performance to LSTM units. These \nresults suggest that cortical circuits can be optimised to solve complex \ncontextual problems and proposes a novel view on their computational function. \nOverall our work provides a step towards unifying recurrent networks as used in \nmachine learning with their biological counterparts. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515046375503", "annotations": [], "published": 1515046376, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003542c9df0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.02448"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "T. Nathan Mundhenk, Daniel Ho, Barry Y. Chen", "title": "Improvements to context based self-supervised learning. (arXiv:1711.06379v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.06379", "type": "text/html"}], "timestampUsec": "1515046375503242", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32972ffcf\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32972ffcf&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We develop a set of methods to improve on the results of self-supervised \nlearning using context. We start with a baseline of patch based arrangement \ncontext learning and go from there. Our methods address some overt problems \nsuch as chromatic aberration as well as other potential problems such as \nspatial skew and mid-level feature neglect. We prevent problems with testing \ngeneralization on common self-supervised benchmark tests by using different \ndatasets during our development. The results of our methods combined yield top \nscores on all standard self-supervised benchmarks, including classification and \ndetection on PASCAL VOC 2007, segmentation on PASCAL VOC 2012, and \"linear \ntests\" on the ImageNet and CSAIL Places datasets. We obtain an improvement over \nour baseline method of between 4.0 to 7.1 percentage points on transfer \nlearning classification tests. We also show results on different standard \nnetwork architectures to demonstrate generalization as well as portability. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515046375503", "annotations": [], "published": 1515046376, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003542c9df7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.06379"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Christopher Schulze, Marcus Schulze", "title": "ViZDoom: DRQN with Prioritized Experience Replay, Double-Q Learning, & Snapshot Ensembling. (arXiv:1801.01000v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.01000", "type": "text/html"}], "timestampUsec": "1515046375503241", "comments": [], "summary": {"content": "<p>ViZDoom is a robust, first-person shooter reinforcement learning environment, \ncharacterized by a significant degree of latent state information. In this \npaper, double-Q learning and prioritized experience replay methods are tested \nunder a certain ViZDoom combat scenario using a competitive deep recurrent \nQ-network (DRQN) architecture. In addition, an ensembling technique known as \nsnapshot ensembling is employed using a specific annealed learning rate to \nobserve differences in ensembling efficacy under these two methods. Annealed \nlearning rates are important in general to the training of deep neural network \nmodels, as they shake up the status-quo and counter a model's tending towards \nlocal optima. While both variants show performance exceeding those of built-in \nAI agents of the game, the known stabilizing effects of double-Q learning are \nillustrated, and priority experience replay is again validated in its \nusefulness by showing immediate results early on in agent development, with the \ncaveat that value overestimation is accelerated in this case. In addition, some \nunique behaviors are observed to develop for priority experience replay (PER) \nand double-Q (DDQ) variants, and snapshot ensembling of both PER and DDQ proves \na valuable method for improving performance of the ViZDoom Marine. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515046375503", "annotations": [], "published": 1515046376, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003542c9df9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01000"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Hongyun Cai, Vincent W. Zheng, Kevin Chen-Chuan Chang", "title": "A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications. (arXiv:1709.07604v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.07604", "type": "text/html"}], "timestampUsec": "1515046375503240", "comments": [], "summary": {"content": "<p>Graph is an important data representation which appears in a wide diversity \nof real-world scenarios. Effective graph analytics provides users a deeper \nunderstanding of what is behind the data, and thus can benefit a lot of useful \napplications such as node classification, node recommendation, link prediction, \netc. However, most graph analytics methods suffer the high computation and \nspace cost. Graph embedding is an effective yet efficient way to solve the \ngraph analytics problem. It converts the graph data into a low dimensional \nspace in which the graph structural information and graph properties are \nmaximally preserved. In this survey, we conduct a comprehensive review of the \nliterature in graph embedding. We first introduce the formal definition of \ngraph embedding as well as the related concepts. After that, we propose two \ntaxonomies of graph embedding which correspond to what challenges exist in \ndifferent graph embedding problem settings and how the existing work address \nthese challenges in their solutions. Finally, we summarize the applications \nthat graph embedding enables and suggest four promising future research \ndirections in terms of computation efficiency, problem settings, techniques and \napplication scenarios. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515046375503", "annotations": [], "published": 1515046376, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003542c9dfd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.07604"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alex A. Gorodetsky, John D. Jakeman", "title": "Gradient-based Optimization for Regression in the Functional Tensor-Train Format. (arXiv:1801.00885v1 [stat.CO])", "alternate": [{"href": "http://arxiv.org/abs/1801.00885", "type": "text/html"}], "timestampUsec": "1515046375503238", "comments": [], "summary": {"content": "<p>We consider the task of low-multilinear-rank functional regression, i.e., \nlearning a low-rank parametric representation of functions from scattered \nreal-valued data. Our first contribution is the development and analysis of an \nefficient gradient computation that enables gradient-based optimization \nprocedures, including stochastic gradient descent and quasi-Newton methods, for \nlearning the parameters of a functional tensor-train (FT). The functional \ntensor-train uses the tensor-train (TT) representation of low-rank arrays as an \nansatz for a class of low-multilinear-rank functions. The FT is represented by \na set of matrix-valued functions that contain a set of univariate functions, \nand the regression task is to learn the parameters of these univariate \nfunctions. Our second contribution demonstrates that using nonlinearly \nparameterized univariate functions, e.g., symmetric kernels with moving \ncenters, within each core can outperform the standard approach of using a \nlinear expansion of basis functions. Our final contributions are new rank \nadaptation and group-sparsity regularization procedures to minimize \noverfitting. We use several benchmark problems to demonstrate at least an order \nof magnitude lower accuracy with gradient-based optimization methods than \nstandard alternating least squares procedures in the low-sample number regime. \nWe also demonstrate an order of magnitude reduction in accuracy on a test \nproblem resulting from using nonlinear parameterizations over linear \nparameterizations. Finally we compare regression performance with 22 other \nnonparametric and parametric regression methods on 10 real-world data sets. We \nachieve top-five accuracy for seven of the data sets and best accuracy for two \nof the data sets. These rankings are the best amongst parametric models and \ncompetetive with the best non-parametric methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515046375503", "annotations": [], "published": 1515046376, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003542c9dff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00885"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Christine A. Liang, Lei Chen, Amer Wahed, Andy N.D. Nguyen", "title": "Proteomics Analysis of FLT3-ITD Mutation in Acute Myeloid Leukemia Using Deep Learning Neural Network. (arXiv:1801.01019v1 [q-bio.QM])", "alternate": [{"href": "http://arxiv.org/abs/1801.01019", "type": "text/html"}], "timestampUsec": "1515046375503237", "comments": [], "summary": {"content": "<p>Deep Learning can significantly benefit cancer proteomics and genomics. In \nthis study, we attempt to determine a set of critical proteins that are \nassociated with the FLT3-ITD mutation in newly-diagnosed acute myeloid leukemia \npatients. A Deep Learning network consisting of autoencoders forming a \nhierarchical model from which high-level features are extracted without labeled \ntraining data. Dimensional reduction reduced the number of critical proteins \nfrom 231 to 20. Deep Learning found an excellent correlation between FLT3-ITD \nmutation with the levels of these 20 critical proteins (accuracy 97%, \nsensitivity 90%, specificity 100%). Our Deep Learning network could hone in on \n20 proteins with the strongest association with FLT3-ITD. The results of this \nstudy allow a novel approach to determine critical protein pathways in the \nFLT3-ITD mutation, and provide proof-of-concept for an accurate approach to \nmodel big data in cancer proteomics and genomics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515046375503", "annotations": [], "published": 1515046376, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003542c9e04", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01019"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mu Niu, Pokman Cheung, Lizhen Lin, Zhenwen Dai, Neil Lawrence, David Dunson", "title": "Intrinsic Gaussian processes on complex constrained domains. (arXiv:1801.01061v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.01061", "type": "text/html"}], "timestampUsec": "1515046375503236", "comments": [], "summary": {"content": "<p>We propose a class of intrinsic Gaussian processes (in-GPs) for \ninterpolation, regression and classification on manifolds with a primary focus \non complex constrained domains or irregular shaped spaces arising as subsets or \nsubmanifolds of R, R2, R3 and beyond. For example, in-GPs can accommodate \nspatial domains arising as complex subsets of Euclidean space. in-GPs respect \nthe potentially complex boundary or interior conditions as well as the \nintrinsic geometry of the spaces. The key novelty of the proposed approach is \nto utilise the relationship between heat kernels and the transition density of \nBrownian motion on manifolds for constructing and approximating valid and \ncomputationally feasible covariance kernels. This enables in-GPs to be \npractically applied in great generality, while existing approaches for \nsmoothing on constrained domains are limited to simple special cases. The broad \nutilities of the in-GP approach is illustrated through simulation studies and \ndata examples. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515046375503", "annotations": [], "published": 1515046376, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003542c9e08", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01061"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yangyang Xu", "title": "Hybrid Jacobian and Gauss-Seidel proximal block coordinate update methods for linearly constrained convex programming. (arXiv:1608.03928v2 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1608.03928", "type": "text/html"}], "timestampUsec": "1515046375503235", "comments": [], "summary": {"content": "<p>Recent years have witnessed the rapid development of block coordinate update \n(BCU) methods, which are particularly suitable for problems involving \nlarge-sized data and/or variables. In optimization, BCU first appears as the \ncoordinate descent method that works well for smooth problems or those with \nseparable nonsmooth terms and/or separable constraints. As nonseparable \nconstraints exist, BCU can be applied under primal-dual settings. \n</p> \n<p>In the literature, it has been shown that for weakly convex problems with \nnonseparable linear constraint, BCU with fully Gauss-Seidel updating rule may \nfail to converge and that with fully Jacobian rule can converge sublinearly. \nHowever, empirically the method with Jacobian update is usually slower than \nthat with Gauss-Seidel rule. To maintain their advantages, we propose a hybrid \nJacobian and Gauss-Seidel BCU method for solving linearly constrained \nmulti-block structured convex programming, where the objective may have a \nnonseparable quadratic term and separable nonsmooth terms. At each primal block \nvariable update, the method approximates the augmented Lagrangian function at \nan affine combination of the previous two iterates, and the affinely mixing \nmatrix with desired nice properties can be chosen through solving a \nsemidefinite programming. We show that the hybrid method enjoys the theoretical \nconvergence guarantee as Jacobian BCU. In addition, we numerically demonstrate \nthat the method can perform as well as Gauss-Seidel method and better than a \nrecently proposed randomized primal-dual BCU method. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515046375503", "annotations": [], "published": 1515046376, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003542c9e0f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1608.03928"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Krzysztof Choromanski, Mark Rowland, Adrian Weller", "title": "The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings. (arXiv:1703.00864v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.00864", "type": "text/html"}], "timestampUsec": "1515046375503234", "comments": [], "summary": {"content": "<p>We examine a class of embeddings based on structured random matrices with \northogonal rows which can be applied in many machine learning applications \nincluding dimensionality reduction and kernel approximation. For both the \nJohnson-Lindenstrauss transform and the angular kernel, we show that we can \nselect matrices yielding guaranteed improved performance in accuracy and/or \nspeed compared to earlier methods. We introduce matrices with complex entries \nwhich give significant further accuracy improvement. We provide geometric and \nMarkov chain-based perspectives to help understand the benefits, and empirical \nresults which suggest that the approach is helpful in a wider range of \napplications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515046375503", "annotations": [], "published": 1515046376, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003542c9e14", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.00864"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Monika Grewal, Muktabh Mayank Srivastava, Pulkit Kumar, Srikrishna Varadarajan", "title": "RADNET: Radiologist Level Accuracy using Deep Learning for HEMORRHAGE detection in CT Scans. (arXiv:1710.04934v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.04934", "type": "text/html"}], "timestampUsec": "1515046375503233", "comments": [], "summary": {"content": "<p>We describe a deep learning approach for automated brain hemorrhage detection \nfrom computed tomography (CT) scans. Our model emulates the procedure followed \nby radiologists to analyse a 3D CT scan in real-world. Similar to radiologists, \nthe model sifts through 2D cross-sectional slices while paying close attention \nto potential hemorrhagic regions. Further, the model utilizes 3D context from \nneighboring slices to improve predictions at each slice and subsequently, \naggregates the slice-level predictions to provide diagnosis at CT level. We \nrefer to our proposed approach as Recurrent Attention DenseNet (RADnet) as it \nemploys original DenseNet architecture along with adding the components of \nattention for slice level predictions and recurrent neural network layer for \nincorporating 3D context. The real-world performance of RADnet has been \nbenchmarked against independent analysis performed by three senior radiologists \nfor 77 brain CTs. RADnet demonstrates 81.82% hemorrhage prediction accuracy at \nCT level that is comparable to radiologists. Further, RADnet achieves higher \nrecall than two of the three radiologists, which is remarkable. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515046375503", "annotations": [], "published": 1515046376, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003542c9e16", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.04934"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yannic Kilcher, Aurelien Lucchi, Thomas Hofmann", "title": "Semantic Interpolation in Implicit Models. (arXiv:1710.11381v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.11381", "type": "text/html"}], "timestampUsec": "1515046375503232", "comments": [], "summary": {"content": "<p>In implicit models, one often interpolates between sampled points in latent \nspace. As we show in this paper, care needs to be taken to match-up the \ndistributional assumptions on code vectors with the geometry of the \ninterpolating paths. Otherwise, typical assumptions about the quality and \nsemantics of in-between points may not be justified. Based on our analysis we \npropose to modify the prior code distribution to put significantly more \nprobability mass closer to the origin. As a result, linear interpolation paths \nare not only shortest paths, but they are also guaranteed to pass through \nhigh-density regions, irrespective of the dimensionality of the latent space. \nExperiments on standard benchmark image datasets demonstrate clear visual \nimprovements in the quality of the generated samples and exhibit more \nmeaningful interpolation paths. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515046375503", "annotations": [], "published": 1515046376, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003542c9e18", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.11381"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hojjat Salehinejad, Julianne Baarbe, Sharan Sankar, Joseph Barfett, Errol Colak, Shahrokh Valaee", "title": "Recent Advances in Recurrent Neural Networks. (arXiv:1801.01078v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.01078", "type": "text/html"}], "timestampUsec": "1515043163598954", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3297301e4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3297301e4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Recurrent neural networks (RNNs) are capable of learning features and long \nterm dependencies from sequential and time-series data. The RNNs have a stack \nof non-linear units where at least one connection between units forms a \ndirected cycle. A well-trained RNN can model any dynamical system; however, \ntraining RNNs is mostly plagued by issues in learning long-term dependencies. \nIn this paper, we present a survey on RNNs and several new advances for \nnewcomers and professionals in the field. The fundamentals and recent advances \nare explained and the research challenges are introduced. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515043163599", "annotations": [], "published": 1515043164, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035426ebd6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.01078"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Michael J. Kurtz", "title": "Advice from the Oracle: Really Intelligent Information Retrieval. (arXiv:1801.00815v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.00815", "type": "text/html"}], "timestampUsec": "1515043163598953", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329788c9e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329788c9e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>What is \"intelligent\" information retrieval? Essentially this is asking what \nis intelligence, in this article I will attempt to show some of the aspects of \nhuman intelligence, as related to information retrieval. I will do this by the \ndevice of a semi-imaginary Oracle. Every Observatory has an oracle, someone who \nis a distinguished scientist, has great administrative responsibilities, acts \nas mentor to a number of less senior people, and as trusted advisor to even the \nmost accomplished scientists, and knows essentially everyone in the field. In \nan appendix I will present a brief summary of the Statistical Factor Space \nmethod for text indexing and retrieval, and indicate how it will be used in the \nAstrophysics Data System Abstract Service. 2018 Keywords: Personal Digital \nAssistant; Supervised Topic Models \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515043163599", "annotations": [], "published": 1515043164, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035426ebdb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00815"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1515734115, "author": "Abdelkrime Aries, Djamel Eddine Zegour, Walid Khaled Hidouci", "title": "Sentence Object Notation: Multilingual sentence notation based on Wordnet. (arXiv:1801.00984v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1801.00984", "type": "text/html"}], "timestampUsec": "1515043163598952", "comments": [], "summary": {"content": "<p>The representation of sentences is a very important task. It can be used as a \nway to exchange data inter-applications. One main characteristic, that a \nnotation must have, is a minimal size and a representative form. This can \nreduce the transfer time, and hopefully the processing time as well. \n</p> \n<p>Usually, sentence representation is associated to the processed language. The \ngrammar of this language affects how we represent the sentence. To avoid \nlanguage-dependent notations, we have to come up with a new representation \nwhich don't use words, but their meanings. This can be done using a lexicon \nlike wordnet, instead of words we use their synsets. As for syntactic \nrelations, they have to be universal as much as possible. \n</p> \n<p>Our new notation is called STON \"SenTences Object Notation\", which somehow \nhas similarities to JSON. It is meant to be minimal, representative and \nlanguage-independent syntactic representation. Also, we want it to be readable \nand easy to be created. This simplifies developing simple automatic generators \nand creating test banks manually. Its benefit is to be used as a medium between \ndifferent parts of applications like: text summarization, language translation, \netc. The notation is based on 4 languages: Arabic, English, Franch and \nJapanese; and there are some cases where these languages don't agree on one \nrepresentation. Also, given the diversity of grammatical structure of different \nworld languages, this annotation may fail for some languages which allows more \nfuture improvements. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515043163599", "annotations": [], "published": 1515734114, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035426ebdd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00984"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ganbin Zhou, Ping Luo, Rongyu Cao, Yijun Xiao, Fen Lin, Bo Chen, Qing He", "title": "Tree-Structured Neural Machine for Linguistics-Aware Sentence Generation. (arXiv:1705.00321v4 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.00321", "type": "text/html"}], "timestampUsec": "1515043163598951", "comments": [], "summary": {"content": "<p>Different from other sequential data, sentences in natural language are \nstructured by linguistic grammars. Previous generative conversational models \nwith chain-structured decoder ignore this structure in human language and might \ngenerate plausible responses with less satisfactory relevance and fluency. In \nthis study, we aim to incorporate the results from linguistic analysis into the \nprocess of sentence generation for high-quality conversation generation. \nSpecifically, we use a dependency parser to transform each response sentence \ninto a dependency tree and construct a training corpus of sentence-tree pairs. \nA tree-structured decoder is developed to learn the mapping from a sentence to \nits tree, where different types of hidden states are used to depict the local \ndependencies from an internal tree node to its children. For training \nacceleration, we propose a tree canonicalization method, which transforms trees \ninto equivalent ternary trees. Then, with a proposed tree-structured search \nmethod, the model is able to generate the most probable responses in the form \nof dependency trees, which are finally flattened into sequences as the system \noutput. Experimental results demonstrate that the proposed X2Tree framework \noutperforms baseline methods over 11.15% increase of acceptance ratio. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515043163599", "annotations": [], "published": 1515043164, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035426ebe5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.00321"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Thee Chanyaswad, Alex Dytso, H. Vincent Poor, Prateek Mittal", "title": "MVG Mechanism: Differential Privacy under Matrix-Valued Query. (arXiv:1801.00823v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1801.00823", "type": "text/html"}], "timestampUsec": "1515043163598949", "comments": [], "summary": {"content": "<p>Differential privacy mechanism design has traditionally been tailored for a \nscalar-valued query function. Although many mechanisms such as the Laplace and \nGaussian mechanisms can be extended to a matrix-valued query function by adding \ni.i.d. noise to each element of the matrix, this method is often suboptimal as \nit forfeits an opportunity to exploit the structural characteristics typically \nassociated with matrix analysis. To address this challenge, we propose a novel \ndifferential privacy mechanism called the Matrix-Variate Gaussian (MVG) \nmechanism, which adds a matrix-valued noise drawn from a matrix-variate \nGaussian distribution, and we rigorously prove that the MVG mechanism preserves \n$(\\epsilon,\\delta)$-differential privacy. Furthermore, we introduce the concept \nof directional noise made possible by the design of the MVG mechanism. \nDirectional noise allows the impact of the noise on the utility of the \nmatrix-valued query function to be moderated. Finally, we experimentally \ndemonstrate the performance of our mechanism using three matrix-valued queries \non three privacy-sensitive datasets. We find that the MVG mechanism notably \noutperforms four previous state-of-the-art approaches, and provides comparable \nutility to the non-private baseline. Our work thus presents a promising \nprospect for both future research and implementation of differential privacy \nfor matrix-valued query functions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515043163599", "annotations": [], "published": 1515043164, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035426ebf7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00823"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alireza Karbalayghareh, Xiaoning Qian, Edward R. Dougherty", "title": "Optimal Bayesian Transfer Learning. (arXiv:1801.00857v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.00857", "type": "text/html"}], "timestampUsec": "1515043163598948", "comments": [], "summary": {"content": "<p>Transfer learning has recently attracted significant research attention, as \nit simultaneously learns from different source domains, which have plenty of \nlabeled data, and transfers the relevant knowledge to the target domain with \nlimited labeled data to improve the prediction performance. We propose a \nBayesian transfer learning framework where the source and target domains are \nrelated through the joint prior density of the model parameters. The modeling \nof joint prior densities enables better understanding of the \"transferability\" \nbetween domains. We define a joint Wishart density for the precision matrices \nof the Gaussian feature-label distributions in the source and target domains to \nact like a bridge that transfers the useful information of the source domain to \nhelp classification in the target domain by improving the target posteriors. \nUsing several theorems in multivariate statistics, the posteriors and posterior \npredictive densities are derived in closed forms with hypergeometric functions \nof matrix argument, leading to our novel closed-form and fast Optimal Bayesian \nTransfer Learning (OBTL) classifier. Experimental results on both synthetic and \nreal-world benchmark data confirm the superb performance of the OBTL compared \nto the other state-of-the-art transfer learning and domain adaptation methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515043163599", "annotations": [], "published": 1515043164, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035426ebf9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00857"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mayank Singh, Abhishek Sinha, Balaji Krishnamurthy", "title": "Neural Networks in Adversarial Setting and Ill-Conditioned Weight Space. (arXiv:1801.00905v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.00905", "type": "text/html"}], "timestampUsec": "1515043163598947", "comments": [], "summary": {"content": "<p>Recently, Neural networks have seen a huge surge in its adoption due to their \nability to provide high accuracy on various tasks. On the other hand, the \nexistence of adversarial examples have raised suspicions regarding the \ngeneralization capabilities of neural networks. In this work, we focus on the \nweight matrix learnt by the neural networks and hypothesize that ill \nconditioned weight matrix is one of the contributing factors in neural \nnetwork's susceptibility towards adversarial examples. For ensuring that the \nlearnt weight matrix's condition number remains sufficiently low, we suggest \nusing orthogonal regularizer. We show that this indeed helps in increasing the \nadversarial accuracy on MNIST and F-MNIST datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515043163599", "annotations": [], "published": 1515043164, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035426ebff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00905"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Feng Liu, Guanquan Zhang, Jie Lu", "title": "Heterogeneous Transfer Learning: An Unsupervised Approach. (arXiv:1701.02511v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1701.02511", "type": "text/html"}], "timestampUsec": "1515043163598946", "comments": [], "summary": {"content": "<p>Transfer learning leverages the knowledge in one domain, the source domain, \nto improve learning efficiency in another domain, the target domain. Existing \ntransfer learning research is relatively well-progressed, but only in \nsituations where the feature spaces of the domains are homogeneous and the \ntarget domain contains at least a few labeled instances. However, transfer \nlearning has not been well-studied in heterogeneous settings with an unlabeled \ntarget domain. To contribute to the research in this emerging field, this paper \npresents: (1) an unsupervised knowledge transfer theorem that prevents negative \ntransfer; and (2) a principal angle-based metric to measure the distance \nbetween two pairs of domains. The metric shows the extent to which homogeneous \nrepresentations have preserved the information in original source and target \ndomains. The unsupervised knowledge transfer theorem sets out the transfer \nconditions necessary to prevent negative transfer. Linear monotonic maps meet \nthe transfer conditions of the theorem and, hence, are used to construct \nhomogeneous representations of the heterogeneous domains, which in principle \nprevents negative transfer. The metric and the theorem have been implemented in \nan innovative transfer model, called a Grassmann-LMM-geodesic flow kernel \n(GLG), that is specifically designed for knowledge transfer across \nheterogeneous domains. The GLG model learns homogeneous representations of \nheterogeneous domains by minimizing the proposed metric. Knowledge is \ntransferred through these learned representations via a geodesic flow kernel. \nNotably, the theorem presented in this paper provides the sufficient transfer \nconditions needed to guarantee that knowledge is transferred from a source \ndomain to an unlabeled target domain with correctness. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515043163599", "annotations": [], "published": 1515043164, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035426ec07", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1701.02511"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Haw-Shiuan Chang, Erik Learned-Miller, Andrew McCallum", "title": "Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples. (arXiv:1704.07433v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.07433", "type": "text/html"}], "timestampUsec": "1515043163598945", "comments": [], "summary": {"content": "<p>Self-paced learning and hard example mining re-weight training instances to \nimprove learning accuracy. This paper presents two improved alternatives based \non lightweight estimates of sample uncertainty in stochastic gradient descent \n(SGD): the variance in predicted probability of the correct class across \niterations of mini-batch SGD, and the proximity of the correct class \nprobability to the decision threshold. Extensive experimental results on six \ndatasets show that our methods reliably improve accuracy in various network \narchitectures, including additional gains on top of other popular training \ntechniques, such as residual learning, momentum, ADAM, batch normalization, \ndropout, and distillation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515043163599", "annotations": [], "published": 1515043164, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035426ec0a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.07433"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiaoyu Cao, Neil Zhenqiang Gong", "title": "Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification. (arXiv:1709.05583v2 [cs.CR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.05583", "type": "text/html"}], "timestampUsec": "1515043163598944", "comments": [], "summary": {"content": "<p>Deep neural networks (DNNs) have transformed several artificial intelligence \nresearch areas including computer vision, speech recognition, and natural \nlanguage processing. However, recent studies demonstrated that DNNs are \nvulnerable to adversarial manipulations at testing time. Specifically, suppose \nwe have a testing example, whose label can be correctly predicted by a DNN \nclassifier. An attacker can add a small carefully crafted noise to the testing \nexample such that the DNN classifier predicts an incorrect label, where the \ncrafted testing example is called adversarial example. Such attacks are called \nevasion attacks. Evasion attacks are one of the biggest challenges for \ndeploying DNNs in safety and security critical applications such as \nself-driving cars. In this work, we develop new methods to defend against \nevasion attacks. Our key observation is that adversarial examples are close to \nthe classification boundary. Therefore, we propose region-based classification \nto be robust to adversarial examples. For a benign/adversarial testing example, \nwe ensemble information in a hypercube centered at the example to predict its \nlabel. In contrast, traditional classifiers are point-based classification, \ni.e., given a testing example, the classifier predicts its label based on the \ntesting example alone. Our evaluation results on MNIST and CIFAR-10 datasets \ndemonstrate that our region-based classification can significantly mitigate \nevasion attacks without sacrificing classification accuracy on benign examples. \nSpecifically, our region-based classification achieves the same classification \naccuracy on testing benign examples as point-based classification, but our \nregion-based classification is significantly more robust than point-based \nclassification to various evasion attacks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1515043163599", "annotations": [], "published": 1515043164, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035426ec15", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.05583"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Timothy Rozario, Troy Long, Mingli Chen, Weiguo Lu, Steve Jiang", "title": "Towards automated patient data cleaning using deep learning: A feasibility study on the standardization of organ labeling. (arXiv:1801.00096v1 [physics.med-ph])", "alternate": [{"href": "http://arxiv.org/abs/1801.00096", "type": "text/html"}], "timestampUsec": "1514957675355408", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329788fe7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329788fe7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Data cleaning consumes about 80% of the time spent on data analysis for \nclinical research projects. This is a much bigger problem in the era of big \ndata and machine learning in the field of medicine where large volumes of data \nare being generated. We report an initial effort towards automated patient data \ncleaning using deep learning: the standardization of organ labeling in \nradiation therapy. Organs are often labeled inconsistently at different \ninstitutions (sometimes even within the same institution) and at different time \nperiods, which poses a problem for clinical research, especially for \nmulti-institutional collaborative clinical research where the acquired patient \ndata is not being used effectively. We developed a convolutional neural network \n(CNN) to automatically identify each organ in the CT image and then label it \nwith the standardized nomenclature presented at AAPM Task Group 263. We tested \nthis model on the CT images of 54 patients with prostate and 100 patients with \nhead and neck cancer who previously received radiation therapy. The model \nachieved 100% accuracy in detecting organs and assigning standardized labels \nfor the patients tested. This work shows the feasibility of using deep learning \nin patient data cleaning that enables standardized datasets to be generated for \neffective intra- and interinstitutional collaborative clinical research. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957675, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537482df", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00096"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "W&#x142;odzimierz Funika, Pawe&#x142; Koperek", "title": "Towards co-evolution of fitness predictors and Deep Neural Networks. (arXiv:1801.00119v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.00119", "type": "text/html"}], "timestampUsec": "1514957675355407", "comments": [], "summary": {"content": "<p>Deep neural networks proved to be a very useful and powerful tool with many \npractical applications. They especially excel at learning from large data sets \nwith labeled samples. However, in order to achieve good learning results, the \nnetwork architecture has to be carefully designed. Creating an optimal topology \nrequires a lot of experience and knowledge. Unfortunately there are no \npractically applicable algorithms which could help in this situation. Using an \nevolutionary process to develop new network topologies might solve this \nproblem. The limiting factor in this case is the speed of evaluation of a \nsingle specimen (a single network architecture), which includes learning based \non the whole large dataset. In this paper we propose to overcome this problem \nby using a fitness prediction technique: use subsets of the original training \nset to conduct the training process and use its results as an approximation of \nspecimen's fitness. We discuss the feasibility of this approach in context of \nthe desired fitness predictor features and analyze whether subsets obtained in \nan evolutionary process can be used to estimate the fitness of the network \ntopology. Finally we draw conclusions from our experiments and outline plans \nfor future work. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957675, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537482e3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00119"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Abien Fred Agarap, Francis John Hill Pepito", "title": "Towards Building an Intelligent Anti-Malware System: A Deep Learning Approach using Support Vector Machine (SVM) for Malware Classification. (arXiv:1801.00318v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.00318", "type": "text/html"}], "timestampUsec": "1514957675355406", "comments": [], "summary": {"content": "<p>Effective and efficient mitigation of malware is a long-time endeavor in the \ninformation security community. The development of an anti-malware system that \ncan counteract an unknown malware is a prolific activity that may benefit \nseveral sectors. We envision an intelligent anti-malware system that utilizes \nthe power of deep learning (DL) models. Using such models would enable the \ndetection of newly-released malware through mathematical generalization. That \nis, finding the relationship between a given malware $x$ and its corresponding \nmalware family $y$, $f: x \\mapsto y$. To accomplish this feat, we used the \nMalimg dataset (Nataraj et al., 2011) which consists of malware images that \nwere processed from malware binaries, and then we trained the following DL \nmodels 1 to classify each malware family: CNN-SVM (Tang, 2013), GRU-SVM \n(Agarap, 2017), and MLP-SVM. Empirical evidence has shown that the GRU-SVM \nstands out among the DL models with a predictive accuracy of ~84.92%. This \nstands to reason for the mentioned model had the relatively most sophisticated \narchitecture design among the presented models. The exploration of an even more \noptimal DL-SVM model is the next stage towards the engineering of an \nintelligent anti-malware system. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957675, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537482f1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00318"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ke Li, Renzhi Chen, Dragan Savic, Xin Yao", "title": "Interactive Decomposition Multi-Objective Optimization via Progressively Learned Value Functions. (arXiv:1801.00609v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1801.00609", "type": "text/html"}], "timestampUsec": "1514957675355405", "comments": [], "summary": {"content": "<p>Decomposition has become an increasingly popular technique for evolutionary \nmulti-objective optimization (EMO). A decomposition-based EMO algorithm is \nusually designed to approximate a whole Pareto-optimal front (PF). However, in \npractice, the decision maker (DM) might only be interested in her/his region of \ninterest (ROI), i.e., a part of the PF. Solutions outside that might be useless \nor even noisy to the decision-making procedure. Furthermore, there is no \nguarantee to find the preferred solutions when tackling many-objective \nproblems. This paper develops an interactive framework for the \ndecomposition-based EMO algorithm to lead a DM to the preferred solutions of \nher/his choice. It consists of three modules, i.e., consultation, preference \nelicitation and optimization. Specifically, after every several generations, \nthe DM is asked to score a few candidate solutions in a consultation session. \nThereafter, an approximated value function, which models the DM's preference \ninformation, is progressively learned from the DM's behavior. In the preference \nelicitation session, the preference information learned in the consultation \nmodule is translated into the form that can be used in a decomposition-based \nEMO algorithm, i.e., a set of reference points that are biased toward to the \nROI. The optimization module, which can be any decomposition-based EMO \nalgorithm in principle, utilizes the biased reference points to direct its \nsearch process. Extensive experiments on benchmark problems with three to ten \nobjectives fully demonstrate the effectiveness of our proposed method for \nfinding the DM's preferred solutions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957675, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537482f8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00609"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1515994817, "author": "Yu Ji, YouHui Zhang, WenGuang Chen, Yuan Xie", "title": "Bridging the Gap Between Neural Networks and Neuromorphic Hardware with A Neural Network Compiler. (arXiv:1801.00746v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1801.00746", "type": "text/html"}], "timestampUsec": "1514957675355404", "comments": [], "summary": {"content": "<p>Different from developing neural networks (NNs) for general-purpose \nprocessors, the development for NN chips usually faces with some \nhardware-specific restrictions, such as limited precision of network signals \nand parameters, constrained computation scale, and limited types of non-linear \nfunctions. \n</p> \n<p>This paper proposes a general methodology to address the challenges. We \ndecouple the NN applications from the target hardware by introducing a compiler \nthat can transform an existing trained, unrestricted NN into an equivalent \nnetwork that meets the given hardware's constraints. We propose multiple \ntechniques to make the transformation adaptable to different kinds of NN chips, \nand reliable for restrict hardware constraints. \n</p> \n<p>We have built such a software tool that supports both spiking neural networks \n(SNNs) and traditional artificial neural networks (ANNs). We have demonstrated \nits effectiveness with a fabricated neuromorphic chip and a \nprocessing-in-memory (PIM) design. Tests show that the inference error caused \nby this solution is insignificant and the transformation time is much shorter \nthan the retraining time. Also, we have studied the parameter-sensitivity \nevaluations to explore the tradeoffs between network error and resource \nutilization for different transformation strategies, which could provide \ninsights for co-design optimization of neuromorphic hardware and software. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1515994817, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748301", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00746"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Clemens Rosenbaum, Tim Klinger, Matthew Riemer", "title": "Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning. (arXiv:1711.01239v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.01239", "type": "text/html"}], "timestampUsec": "1514957675355403", "comments": [], "summary": {"content": "<p>Multi-task learning (MTL) with neural networks leverages commonalities in \ntasks to improve performance, but often suffers from task interference which \nreduces the benefits of transfer. To address this issue we introduce the \nrouting network paradigm, a novel neural network and training algorithm. A \nrouting network is a kind of self-organizing neural network consisting of two \ncomponents: a router and a set of one or more function blocks. A function block \nmay be any neural network - for example a fully-connected or a convolutional \nlayer. Given an input the router makes a routing decision, choosing a function \nblock to apply and passing the output back to the router recursively, \nterminating when a fixed recursion depth is reached. In this way the routing \nnetwork dynamically composes different function blocks for each input. We \nemploy a collaborative multi-agent reinforcement learning (MARL) approach to \njointly train the router and function blocks. We evaluate our model against \ncross-stitch networks and shared-layer baselines on multi-task settings of the \nMNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a \nsignificant improvement in accuracy, with sharper convergence. In addition, \nrouting networks have nearly constant per-task training cost while cross-stitch \nnetworks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we \nobtain cross-stitch performance levels with an 85% reduction in training time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957675, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748309", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.01239"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Daniel McNamee", "title": "Characterizing optimal hierarchical policy inference on graphs via non-equilibrium thermodynamics. (arXiv:1801.00048v1 [cs.SY])", "alternate": [{"href": "http://arxiv.org/abs/1801.00048", "type": "text/html"}], "timestampUsec": "1514957675355402", "comments": [], "summary": {"content": "<p>Hierarchies are of fundamental interest in both stochastic optimal control \nand biological control due to their facilitation of a range of desirable \ncomputational traits in a control algorithm and the possibility that they may \nform a core principle of sensorimotor and cognitive control systems. However, a \ntheoretically justified construction of state-space hierarchies over all \nspatial resolutions and their evolution through a policy inference process \nremains elusive. Here, a formalism for deriving such normative representations \nof discrete Markov decision processes is introduced in the context of graphs. \nThe resulting hierarchies correspond to a hierarchical policy inference \nalgorithm approximating a discrete gradient flow between state-space trajectory \ndensities generated by the prior and optimal policies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957675, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748314", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00048"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Boris Belousov, Jan Peters", "title": "f-Divergence constrained policy improvement. (arXiv:1801.00056v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.00056", "type": "text/html"}], "timestampUsec": "1514957675355401", "comments": [], "summary": {"content": "<p>To ensure stability of learning, state-of-the-art generalized policy \niteration algorithms augment the policy improvement step with a trust region \nconstraint bounding the information loss. The size of the trust region is \ncommonly determined by the Kullback-Leibler (KL) divergence, which not only \ncaptures the notion of distance well but also yields closed-form solutions. In \nthis paper, we consider a more general class of f-divergences and derive the \ncorresponding policy update rules. The generic solution is expressed through \nthe derivative of the convex conjugate function to f and includes the KL \nsolution as a special case. Within the class of f-divergences, we further focus \non a one-parameter family of {\\alpha}-divergences to study effects of the \nchoice of divergence on policy improvement. Previously known as well as new \npolicy updates emerge for different values of {\\alpha}. We show that every type \nof policy update comes with a compatible policy evaluation resulting from the \nchosen f-divergence. Interestingly, the mean-squared Bellman error minimization \nis closely related to policy evaluation with the Pearson $\\chi^2$-divergence \npenalty, while the KL divergence results in the soft-max policy update and a \nlog-sum-exp critic. We carry out asymptotic analysis of the solutions for \ndifferent values of {\\alpha} and demonstrate the effects of using different \ndivergence functions on a multi-armed bandit problem and on common standard \nreinforcement learning problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748320", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00056"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ruiyi Zhang, Chunyuan Li, Changyou Chen, Lawrence Carin", "title": "Learning Structural Weight Uncertainty for Sequential Decision-Making. (arXiv:1801.00085v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.00085", "type": "text/html"}], "timestampUsec": "1514957675355400", "comments": [], "summary": {"content": "<p>Learning probability distributions on the weights of neural networks (NNs) \nhas recently proven beneficial in many applications. Bayesian methods, such as \nStein variational gradient descent (SVGD), offer an elegant framework to reason \nabout NN model uncertainty. However, by assuming independent Gaussian priors \nfor the individual NN weights (as often applied), SVGD does not impose prior \nknowledge that there is often structural information (dependence) among \nweights. We propose efficient posterior learning of structural weight \nuncertainty, within an SVGD framework, by employing matrix variate Gaussian \npriors on NN parameters. We further investigate the learned structural \nuncertainty in sequential decision-making problems, including contextual \nbandits and reinforcement learning. Experiments on several synthetic and real \ndatasets indicate the superiority of our model, compared with state-of-the-art \nmethods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035374832b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00085"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yi Tay, Luu Anh Tuan, Siu Cheung Hui", "title": "A Compare-Propagate Architecture with Alignment Factorization for Natural Language Inference. (arXiv:1801.00102v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1801.00102", "type": "text/html"}], "timestampUsec": "1514957675355399", "comments": [], "summary": {"content": "<p>This paper presents a new deep learning architecture for Natural Language \nInference (NLI). Firstly, we introduce a new compare-propagate architecture \nwhere alignments pairs are compared and then propagated to upper layers for \nenhanced representation learning. Secondly, we adopt novel factorization layers \nfor efficient compression of alignment vectors into scalar valued features, \nwhich are then be used to augment the base word representations. The design of \nour approach is aimed to be conceptually simple, compact and yet powerful. We \nconduct experiments on three popular benchmarks, SNLI, MultiNLI and SciTail, \nachieving state-of-the-art performance on all. A lightweight parameterization \nof our model enjoys a $\\approx 300\\%$ reduction in parameter size compared to \nthe ESIM and DIIN, while maintaining competitive performance. Visual analysis \nshows that our propagated features are highly interpretable, opening new \navenues to explainability in neural NLI models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748334", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00102"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Christian Bessiere, Nadjib Lazaar, Yahia Lebbah, Mehdi Maamar", "title": "Users Constraints in Itemset Mining. (arXiv:1801.00345v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.00345", "type": "text/html"}], "timestampUsec": "1514957675355398", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32978934e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32978934e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Discovering significant itemsets is one of the fundamental problems in data \nmining. It has recently been shown that constraint programming is a flexible \nway to tackle data mining tasks. With a constraint programming approach, we can \neasily express and efficiently answer queries with users constraints on items. \nHowever, in many practical cases it is possible that queries also express users \nconstraints on the dataset itself. For instance, asking for a particular \nitemset in a particular part of the dataset. This paper presents a general \nconstraint programming model able to handle any kind of query on the items or \nthe dataset for itemset mining. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035374833a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00345"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Amit Sheth, Utkarshani Jaimini, Hong Yung Yip", "title": "How will the Internet of Things enable Augmented Personalized Health?. (arXiv:1801.00356v1 [cs.CY])", "alternate": [{"href": "http://arxiv.org/abs/1801.00356", "type": "text/html"}], "timestampUsec": "1514957675355397", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3297e28f3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3297e28f3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Internet-of-Things (IoT) is profoundly redefining the way we create, consume, \nand share information. Health aficionados and citizens are increasingly using \nIoT technologies to track their sleep, food intake, activity, vital body \nsignals, and other physiological observations. This is complemented by IoT \nsystems that continuously collect health-related data from the environment and \ninside the living quarters. Together, these have created an opportunity for a \nnew generation of healthcare solutions. However, interpreting data to \nunderstand an individual's health is challenging. It is usually necessary to \nlook at that individual's clinical record and behavioral information, as well \nas social and environmental information affecting that individual. Interpreting \nhow well a patient is doing also requires looking at his adherence to \nrespective health objectives, application of relevant clinical knowledge and \nthe desired outcomes. \n</p> \n<p>We resort to the vision of Augmented Personalized Healthcare (APH) to exploit \nthe extensive variety of relevant data and medical knowledge using Artificial \nIntelligence (AI) techniques to extend and enhance human health to presents \nvarious stages of augmented health management strategies: self-monitoring, \nself-appraisal, self-management, intervention, and disease progress tracking \nand prediction. kHealth technology, a specific incarnation of APH, and its \napplication to Asthma and other diseases are used to provide illustrations and \ndiscuss alternatives for technology-assisted health management. Several \nprominent efforts involving IoT and patient-generated health data (PGHD) with \nrespect converting multimodal data into actionable information (big data to \nsmart data) are also identified. Roles of three components in an evidence-based \nsemantic perception approach- Contextualization, Abstraction, and \nPersonalization are discussed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748340", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00356"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jason Toy", "title": "SenseNet: 3D Objects Database and Tactile Simulator. (arXiv:1801.00361v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.00361", "type": "text/html"}], "timestampUsec": "1514957675355396", "comments": [], "summary": {"content": "<p>The majority of artificial intelligence research, as it relates from which to \nbiological senses has been focused on vision. The recent explosion of machine \nlearning and in particular, dee p learning, can be partially attributed to the \nrelease of high quality data sets for algorithm s from which to model the world \non. Thus, most of these datasets are comprised of images. We believe that \nfocusing on sensorimotor systems and tactile feedback will create algorithms \nthat better mimic human intelligence. Here we present SenseNet: a collection of \ntactile simulators and a large scale dataset of 3D objects for manipulation. \nSenseNet was created for the purpose of researching and training Artificial \nIntelligences (AIs) to interact with the environment via sensorimotor neural \nsystems and tactile feedback. We aim to accelerate that same explosion in image \nprocessing, but for the domain of tactile feedback and sensorimotor research. \nWe hope that SenseNet can offer researchers in both the machine learning and \ncomputational neuroscience communities brand new opportunities and avenues to \nexplore. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748345", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00361"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Akzharkyn Izbassarova, Aidana Irmanova, A. P. James", "title": "Automated rating of recorded classroom presentations using speech analysis in kazakh. (arXiv:1801.00453v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1801.00453", "type": "text/html"}], "timestampUsec": "1514957675355395", "comments": [], "summary": {"content": "<p>Effective presentation skills can help to succeed in business, career and \nacademy. This paper presents the design of speech assessment during the oral \npresentation and the algorithm for speech evaluation based on criteria of \noptimal intonation. As the pace of the speech and its optimal intonation varies \nfrom language to language, developing an automatic identification of language \nduring the presentation is required. Proposed algorithm was tested with \npresentations delivered in Kazakh language. For testing purposes the features \nof Kazakh phonemes were extracted using MFCC and PLP methods and created a \nHidden Markov Model (HMM) [5], [5] of Kazakh phonemes. Kazakh vowel formants \nwere defined and the correlation between the deviation rate in fundamental \nfrequency and the liveliness of the speech to evaluate intonation of the \npresentation was analyzed. It was established that the threshold value between \nmonotone and dynamic speech is 0.16 and the error for intonation evaluation is \n19%. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748347", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00453"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kai Qiao, Chi Zhang, Linyuan Wang, Bin Yan, Jian Chen, Lei Zeng, Li Tong", "title": "Accurate reconstruction of image stimuli from human fMRI based on the decoding model with capsule network architecture. (arXiv:1801.00602v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.00602", "type": "text/html"}], "timestampUsec": "1514957675355394", "comments": [], "summary": {"content": "<p>In neuroscience, all kinds of computation models were designed to answer the \nopen question of how sensory stimuli are encoded by neurons and conversely, how \nsensory stimuli can be decoded from neuronal activities. Especially, functional \nMagnetic Resonance Imaging (fMRI) studies have made many great achievements \nwith the rapid development of the deep network computation. However, comparing \nwith the goal of decoding orientation, position and object category from \nactivities in visual cortex, accurate reconstruction of image stimuli from \nhuman fMRI is a still challenging work. In this paper, the capsule network \n(CapsNet) architecture based visual reconstruction (CNAVR) method is developed \nto reconstruct image stimuli. The capsule means containing a group of neurons \nto perform the better organization of feature structure and representation, \ninspired by the structure of cortical mini column including several hundred \nneurons in primates. The high-level capsule features in the CapsNet includes \ndiverse features of image stimuli such as semantic class, orientation, location \nand so on. We used these features to bridge between human fMRI and image \nstimuli. We firstly employed the CapsNet to train the nonlinear mapping from \nimage stimuli to high-level capsule features, and from high-level capsule \nfeatures to image stimuli again in an end-to-end manner. After estimating the \nserviceability of each voxel by encoding performance to accomplish the \nselecting of voxels, we secondly trained the nonlinear mapping from \ndimension-decreasing fMRI data to high-level capsule features. Finally, we can \npredict the high-level capsule features with fMRI data, and reconstruct image \nstimuli with the CapsNet. We evaluated the proposed CNAVR method on the dataset \nof handwritten digital images, and exceeded about 10% than the accuracy of all \nexisting state-of-the-art methods on the structural similarity index (SSIM). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748349", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00602"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Gary Marcus", "title": "Deep Learning: A Critical Appraisal. (arXiv:1801.00631v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.00631", "type": "text/html"}], "timestampUsec": "1514957675355393", "comments": [], "summary": {"content": "<p>Although deep learning has historical roots going back decades, neither the \nterm \"deep learning\" nor the approach was popular just over five years ago, \nwhen the field was reignited by papers such as Krizhevsky, Sutskever and \nHinton's now classic (2012) deep network model of Imagenet. What has the field \ndiscovered in the five subsequent years? Against a background of considerable \nprogress in areas such as speech recognition, image recognition, and game \nplaying, and considerable enthusiasm in the popular press, I present ten \nconcerns for deep learning, and suggest that deep learning must be supplemented \nby other techniques if we are to reach artificial general intelligence. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035374834b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00631"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Nicola Strisciuglio", "title": "Learning audio and image representations with bio-inspired trainable feature extractors. (arXiv:1801.00688v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1801.00688", "type": "text/html"}], "timestampUsec": "1514957675355392", "comments": [], "summary": {"content": "<p>Recent advancements in pattern recognition and signal processing concern the \nautomatic learning of data representations from labeled training samples. \nTypical approaches are based on deep learning and convolutional neural \nnetworks, which require large amount of labeled training samples. In this work, \nwe propose novel feature extractors that can be used to learn the \nrepresentation of single prototype samples in an automatic configuration \nprocess. We employ the proposed feature extractors in applications of audio and \nimage processing, and show their effectiveness on benchmark data sets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035374834f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00688"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, Martin Riedmiller", "title": "DeepMind Control Suite. (arXiv:1801.00690v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.00690", "type": "text/html"}], "timestampUsec": "1514957675355391", "comments": [], "summary": {"content": "<p>The DeepMind Control Suite is a set of continuous control tasks with a \nstandardised structure and interpretable rewards, intended to serve as \nperformance benchmarks for reinforcement learning agents. The tasks are written \nin Python and powered by the MuJoCo physics engine, making them easy to use and \nmodify. We include benchmarks for several learning algorithms. The Control \nSuite is publicly available at https://www.github.com/deepmind/dm_control. A \nvideo summary of all tasks is available at <a href=\"http://youtu.be/rAai4QzcYbs\">this http URL</a> . \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035374835b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00690"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xinyang Deng, Wen Jiang", "title": "A total uncertainty measure for D numbers based on belief intervals. (arXiv:1801.00702v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.00702", "type": "text/html"}], "timestampUsec": "1514957675355390", "comments": [], "summary": {"content": "<p>As a generalization of Dempster-Shafer theory, the theory of D numbers is a \nnew theoretical framework for uncertainty reasoning. Measuring the uncertainty \nof knowledge or information represented by D numbers is an unsolved issue in \nthat theory. In this paper, inspired by distance based uncertainty measures for \nDempster-Shafer theory, a total uncertainty measure for a D number is proposed \nbased on its belief intervals. The proposed total uncertainty measure can \nsimultaneously capture the discord, and non-specificity, and non-exclusiveness \ninvolved in D numbers. And some basic properties of this total uncertainty \nmeasure, including range, monotonicity, generalized set consistency, are also \npresented. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748360", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00702"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Luis A. A. Meira, Paulo S. Martins, Mauro Menzori, Guilherme A. Zeni", "title": "Multi-Objective Vehicle Routing Problem Applied to Large Scale Post Office Deliveries. (arXiv:1801.00712v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.00712", "type": "text/html"}], "timestampUsec": "1514957675355389", "comments": [], "summary": {"content": "<p>The number of optimization techniques in the combinatorial domain is large \nand diversified. Nevertheless, real-world based benchmarks for testing \nalgorithms are few. This work creates an extensible real-world mail delivery \nbenchmark to the Vehicle Routing Problem (VRP) in a planar graph embedded in \nthe 2D Euclidean space. Such problem is multi-objective on a roadmap with up to \n25 vehicles and 30,000 deliveries per day. Each instance models one generic day \nof mail delivery, allowing both comparison and validation of optimization \nalgorithms for routing problems. The benchmark may be extended to model other \nscenarios. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748363", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00712"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "David Heckerman", "title": "Accounting for hidden common causes when infering cause and effect from observational data. (arXiv:1801.00727v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.00727", "type": "text/html"}], "timestampUsec": "1514957675355388", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3297e2b01\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3297e2b01&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Identifying causal relationships from observation data is difficult, in large \npart, due to the presence of hidden common causes. In some cases, where just \nthe right patterns of conditional independence and dependence lie in the \ndata---for example, Y-structures---it is possible to identify cause and effect. \nIn other cases, the analyst deliberately makes an uncertain assumption that \nhidden common causes are absent, and infers putative causal relationships to be \ntested in a randomized trial. Here, we consider a third approach, where there \nare sufficient clues in the data such that hidden common causes can be \ninferred. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748368", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00727"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Claudio Alexandre, Jo&#xe3;o Balsa", "title": "Um Sistema Multiagente no Combate ao Braqueamento de Capitais. (arXiv:1801.00743v1 [cs.MA])", "alternate": [{"href": "http://arxiv.org/abs/1801.00743", "type": "text/html"}], "timestampUsec": "1514957675355387", "comments": [], "summary": {"content": "<p>Money laundering is a crime that makes it possible to finance other crimes, \nfor this reason, it is important for criminal organizations and their combat is \nprioritized by nations around the world. The anti-money laundering process has \nnot evolved as expected because it has prioritized only the signaling of \nsuspicious transactions. The constant increasing in the volume of transactions \nhas overloaded the indispensable human work of final evaluation of the \nsuspicions. This article presents a multiagent system that aims to go beyond \nthe capture of suspicious transactions, seeking to assist the human expert in \nthe analysis of suspicions. The agents created use data mining techniques to \ncreate transactional behavioral profiles; apply rules generated in learning \nprocess in conjunction with specific rules based on legal aspects and profiles \ncreated to capture suspicious transactions; and analyze these suspicious \ntransactions indicating to the human expert those that require more detailed \nanalysis. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035374836e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00743"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Christoph Dann, Tor Lattimore, Emma Brunskill", "title": "Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning. (arXiv:1703.07710v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.07710", "type": "text/html"}], "timestampUsec": "1514957675355386", "comments": [], "summary": {"content": "<p>Statistical performance bounds for reinforcement learning (RL) algorithms can \nbe critical for high-stakes applications like healthcare. This paper introduces \na new framework for theoretically measuring the performance of such algorithms \ncalled Uniform-PAC, which is a strengthening of the classical Probably \nApproximately Correct (PAC) framework. In contrast to the PAC framework, the \nuniform version may be used to derive high probability regret guarantees and so \nforms a bridge between the two setups that has been missing in the literature. \nWe demonstrate the benefits of the new framework for finite-state episodic MDPs \nwith a new algorithm that is Uniform-PAC and simultaneously achieves optimal \nregret and PAC guarantees except for a factor of the horizon. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748373", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.07710"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Johannes K. Fichte, Markus Hecher, Irina Schindler", "title": "Default Logic and Bounded Treewidth. (arXiv:1706.09393v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.09393", "type": "text/html"}], "timestampUsec": "1514957675355385", "comments": [], "summary": {"content": "<p>In this paper, we study Reiter's propositional default logic when the \ntreewidth of a certain graph representation (semi-primal graph) of the input \ntheory is bounded. We establish a dynamic programming algorithm on tree \ndecompositions that decides whether a theory has a consistent stable extension \n(Ext). Our algorithm can even be used to enumerate all generating defaults \n(ExtEnum) that lead to stable extensions. \n</p> \n<p>We show that our algorithm decides Ext in linear time in the input theory and \ntriple exponential time in the treewidth (so-called fixed-parameter linear \nalgorithm). \n</p> \n<p>Further, our algorithm solves ExtEnum with a pre-computation step that is \nlinear in the input theory and triple exponential in the treewidth followed by \na linear delay to output solutions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748377", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.09393"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Gopal P. Sarma, Nick J. Hay", "title": "Robust Computer Algebra, Theorem Proving, and Oracle AI. (arXiv:1708.02553v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.02553", "type": "text/html"}], "timestampUsec": "1514957675355384", "comments": [], "summary": {"content": "<p>In the context of superintelligent AI systems, the term \"oracle\" has two \nmeanings. One refers to modular systems queried for domain-specific tasks. \nAnother usage, referring to a class of systems which may be useful for \naddressing the value alignment and AI control problems, is a superintelligent \nAI system that only answers questions. The aim of this manuscript is to survey \ncontemporary research problems related to oracles which align with long-term \nresearch goals of AI safety. We examine existing question answering systems and \nargue that their high degree of architectural heterogeneity makes them poor \ncandidates for rigorous analysis as oracles. On the other hand, we identify \ncomputer algebra systems (CASs) as being primitive examples of domain-specific \noracles for mathematics and argue that efforts to integrate computer algebra \nsystems with theorem provers, systems which have largely been developed \nindependent of one another, provide a concrete set of problems related to the \nnotion of provable safety that has emerged in the AI safety community. We \nreview approaches to interfacing CASs with theorem provers, describe \nwell-defined architectural deficiencies that have been identified with CASs, \nand suggest possible lines of research and practical software projects for \nscientists interested in AI safety. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748389", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.02553"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas", "title": "StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks. (arXiv:1710.10916v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.10916", "type": "text/html"}], "timestampUsec": "1514957675355383", "comments": [], "summary": {"content": "<p>Although Generative Adversarial Networks (GANs) have shown remarkable success \nin various tasks, they still face challenges in generating high quality images. \nIn this paper, we propose Stacked Generative Adversarial Networks (StackGAN) \naiming at generating high-resolution photo-realistic images. First, we propose \na two-stage generative adversarial network architecture, StackGAN-v1, for \ntext-to-image synthesis. The Stage-I GAN sketches the primitive shape and \ncolors of the object based on given text description, yielding low-resolution \nimages. The Stage-II GAN takes Stage-I results and text descriptions as inputs, \nand generates high-resolution images with photo-realistic details. Second, an \nadvanced multi-stage generative adversarial network architecture, StackGAN-v2, \nis proposed for both conditional and unconditional generative tasks. Our \nStackGAN-v2 consists of multiple generators and discriminators in a tree-like \nstructure; images at multiple scales corresponding to the same scene are \ngenerated from different branches of the tree. StackGAN-v2 shows more stable \ntraining behavior than StackGAN-v1 by jointly approximating multiple \ndistributions. Extensive experiments demonstrate that the proposed stacked \ngenerative adversarial networks significantly outperform other state-of-the-art \nmethods in generating photo-realistic images. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748393", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.10916"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yen-Chang Hsu, Zhaoyang Lv, Zsolt Kira", "title": "Learning to cluster in order to transfer across domains and tasks. (arXiv:1711.10125v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10125", "type": "text/html"}], "timestampUsec": "1514957675355382", "comments": [], "summary": {"content": "<p>This paper introduces a novel method to perform transfer learning across \ndomains and tasks, formulating it as a problem of learning to cluster. The key \ninsight is that, in addition to features, we can transfer similarity \ninformation and this is sufficient to learn a similarity function and \nclustering network to perform both domain adaptation and cross-task transfer \nlearning. We begin by reducing categorical information to pairwise constraints, \nwhich only considers whether two instances belong to the same class or not. \nThis similarity is category-agnostic and can be learned from data in the source \ndomain using a similarity network. We then present two novel approaches for \nperforming transfer learning using this similarity function. First, for \nunsupervised domain adaptation, we design a new loss function to regularize \nclassification with a constrained clustering loss, hence learning a clustering \nnetwork with the transferred similarity metric generating the training inputs. \nSecond, for cross-task learning (i.e., unsupervised clustering with unseen \ncategories), we propose a framework to reconstruct and estimate the number of \nsemantic clusters, again using the clustering network. Since the similarity \nnetwork is noisy, the key is to use a robust clustering algorithm, and we show \nthat our formulation is more robust than the alternative constrained and \nunconstrained clustering approaches. Using this method, we first show state of \nthe art results for the challenging cross-task problem, applied on Omniglot and \nImageNet. Our results show that we can reconstruct semantic clusters with high \naccuracy. We then evaluate the performance of cross-domain transfer using \nimages from the Office-31 and SVHN-MNIST tasks and present top accuracy on both \ndatasets. Our approach doesn't explicitly deal with domain discrepancy. If we \ncombine with a domain adaptation loss, it shows further improvement. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537483a0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10125"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Dylan J. Foster, Satyen Kale, Mehryar Mohri, Karthik Sridhran", "title": "Parameter-free online learning via model selection. (arXiv:1801.00101v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.00101", "type": "text/html"}], "timestampUsec": "1514957675355379", "comments": [], "summary": {"content": "<p>We introduce an efficient algorithmic framework for model selection in online \nlearning, also known as parameter-free online learning. Departing from previous \nwork, which has focused on highly structured function classes such as nested \nballs in Hilbert space, we propose a generic meta-algorithm framework that \nachieves online model selection oracle inequalities under minimal structural \nassumptions. We give the first computationally efficient parameter-free \nalgorithms that work in arbitrary Banach spaces under mild smoothness \nassumptions; previous results applied only to Hilbert spaces. We further derive \nnew oracle inequalities for matrix classes, non-nested convex sets, and \n$\\mathbb{R}^{d}$ with generic regularizers. Finally, we generalize these \nresults by providing oracle inequalities for arbitrary non-linear classes in \nthe online supervised learning model. These results are all derived through a \nunified meta-algorithm scheme using a novel \"multi-scale\" algorithm for \nprediction with expert advice based on random playout, which may be of \nindependent interest. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537483b8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00101"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pitas Konstantinos, Mike Davies, Pierre Vandergheynst", "title": "PAC-Bayesian Margin Bounds for Convolutional Neural Networks - Technical Report. (arXiv:1801.00171v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.00171", "type": "text/html"}], "timestampUsec": "1514957675355378", "comments": [], "summary": {"content": "<p>Recently the generalisation error of deep neural networks has been analysed \nthrough the PAC-Bayesian framework, for the case of fully connected layers. We \nadapt this approach to the convolutional setting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537483c8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00171"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Piero Mazzarisi, Paolo Barucca, Fabrizio Lillo, Daniele Tantari", "title": "A dynamic network model with persistent links and node-specific latent variables, with an application to the interbank market. (arXiv:1801.00185v1 [cs.SI])", "alternate": [{"href": "http://arxiv.org/abs/1801.00185", "type": "text/html"}], "timestampUsec": "1514957675355377", "comments": [], "summary": {"content": "<p>We propose a dynamic network model where two mechanisms control the \nprobability of a link between two nodes: (i) the existence or absence of this \nlink in the past, and (ii) node-specific latent variables (dynamic fitnesses) \ndescribing the propensity of each node to create links. Assuming a Markov \ndynamics for both mechanisms, we propose an Expectation-Maximization algorithm \nfor model estimation and inference of the latent variables. The estimated \nparameters and fitnesses can be used to forecast the presence of a link in the \nfuture. We apply our methodology to the e-MID interbank network for which the \ntwo linkage mechanisms are associated with two different trading behaviors in \nthe process of network formation, namely preferential trading and trading \ndriven by node-specific characteristics. The empirical results allow to \nrecognise preferential lending in the interbank market and indicate how a \nmethod that does not account for time-varying network topologies tends to \noverestimate preferential linkage. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537483d2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00185"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Wei Chen, Andrew L Ferguson", "title": "Molecular enhanced sampling with autoencoders: On-the-fly collective variable discovery and accelerated free energy landscape exploration. (arXiv:1801.00203v1 [physics.bio-ph])", "alternate": [{"href": "http://arxiv.org/abs/1801.00203", "type": "text/html"}], "timestampUsec": "1514957675355376", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3297e2cd2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3297e2cd2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Macromolecular and biomolecular folding landscapes typically contain high \nfree energy barriers that impede efficient sampling of configurational space by \nstandard molecular dynamics simulation. Biased sampling can artificially drive \nthe simulation along pre-specified collective variables (CVs), but success \ndepends critically on the availability of good CVs associated with the \nimportant collective dynamical motions. Nonlinear machine learning techniques \ncan identify such CVs but typically do not furnish an explicit relationship \nwith the atomic coordinates necessary to perform biased sampling. In this work, \nwe employ auto-associative artificial neural networks (\"autoencoders\") to learn \nnonlinear CVs that are explicit and differentiable functions of the atomic \ncoordinates. Our approach offers substantial speedups in exploration of \nconfigurational space, and is distinguished from exiting approaches by its \ncapacity to simultaneously discover and directly accelerate along data-driven \nCVs. We demonstrate the approach in simulations of alanine dipeptide and \nTrp-cage, and have developed an open-source and freely-available implementation \nwithin OpenMM. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537483ea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00203"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Klaus Broelemann, Thomas Gottron, Gjergji Kasneci", "title": "Restricted Boltzmann Machines for Robust and Fast Latent Truth Discovery. (arXiv:1801.00283v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.00283", "type": "text/html"}], "timestampUsec": "1514957675355375", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329864069\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329864069&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We address the problem of latent truth discovery, LTD for short, where the \ngoal is to discover the underlying true values of entity attributes in the \npresence of noisy, conflicting or incomplete information. Despite a multitude \nof algorithms to address the LTD problem that can be found in literature, only \nlittle is known about their overall performance with respect to effectiveness \n(in terms of truth discovery capabilities), efficiency and robustness. A \npractical LTD approach should satisfy all these characteristics so that it can \nbe applied to heterogeneous datasets of varying quality and degrees of \ncleanliness. \n</p> \n<p>We propose a novel algorithm for LTD that satisfies the above requirements. \nThe proposed model is based on Restricted Boltzmann Machines, thus coined \nLTD-RBM. In extensive experiments on various heterogeneous and publicly \navailable datasets, LTD-RBM is superior to state-of-the-art LTD techniques in \nterms of an overall consideration of effectiveness, efficiency and robustness. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537483f5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00283"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "E.M. Stoudenmire", "title": "Learning Relevant Features of Data with Multi-scale Tensor Networks. (arXiv:1801.00315v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.00315", "type": "text/html"}], "timestampUsec": "1514957675355374", "comments": [], "summary": {"content": "<p>Inspired by coarse-graining approaches used in physics, we show how similar \nalgorithms can be adapted for data. The resulting algorithms are based on \nlayered tree tensor networks and scale linearly with both the dimension of the \ninput and the training set size. Computing most of the layers with an \nunsupervised algorithm, then optimizing just the top layer for supervised \nclassification of the MNIST and fashion-MNIST data sets gives very good \nresults. We also discuss mixing a prior guess for supervised weights together \nwith an unsupervised representation of the data, yielding a smaller number of \nfeatures nevertheless able to give good performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748401", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00315"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yu-Ren Liu, Yi-Qi Hu, Hong Qian, Yang Yu, Chao Qian", "title": "ZOOpt/ZOOjl: Toolbox for Derivative-Free Optimization. (arXiv:1801.00329v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.00329", "type": "text/html"}], "timestampUsec": "1514957675355373", "comments": [], "summary": {"content": "<p>Recent advances of derivative-free optimization allow efficient approximating \nthe global optimal solutions of sophisticated functions, such as functions with \nmany local optima, non-differentiable and non-continuous functions. This \narticle describes the ZOOpt/ZOOjl toolbox that provides efficient \nderivative-free solvers and are designed easy to use. ZOOpt provides a Python \npackage for single-thread optimization, and ZOOjl provides a distributed \nversion with the help of the Julia language for Python described functions. \nZOOpt/ZOOjl toolbox particularly focuses on optimization problems in machine \nlearning, addressing high-dimensional, noisy, and large-scale problems. The \ntoolbox is being maintained toward ready-to-use tools in real-world machine \nlearning tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748411", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00329"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ye Luo, Martin Spindler", "title": "Estimation and Inference of Treatment Effects with $L_2$-Boosting in High-Dimensional Settings. (arXiv:1801.00364v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.00364", "type": "text/html"}], "timestampUsec": "1514957675355372", "comments": [], "summary": {"content": "<p>Boosting algorithms are very popular in Machine Learning and have proven very \nuseful for prediction and variable selection. Nevertheless in many applications \nthe researcher is interested in inference on treatment effects or policy \nvariables in a high-dimensional setting. Empirical researchers are more and \nmore faced with rich datasets containing very many controls or instrumental \nvariables, where variable selection is challenging. In this paper we give \nresults for the valid inference of a treatment effect after selecting from \namong very many control variables and the estimation of instrumental variables \nwith potentially very many instruments when post- or orthogonal $L_2$-Boosting \nis used for the variable selection. This setting allows for valid inference on \nlow-dimensional components in a regression estimated with $L_2$-Boosting. We \ngive simulation results for the proposed methods and an empirical application, \nin which we analyze the effectiveness of a pulmonary artery catheter. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748431", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00364"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Manolis C. Tsakiris, Rene Vidal", "title": "Theoretical Analysis of Sparse Subspace Clustering with Missing Entries. (arXiv:1801.00393v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.00393", "type": "text/html"}], "timestampUsec": "1514957675355371", "comments": [], "summary": {"content": "<p>Sparse Subspace Clustering (SSC) is a popular unsupervised machine learning \nmethod for clustering data lying close to a union of low-dimensional linear \nsubspaces; a problem with numerous applications in pattern recognition and \ncomputer vision. Even though the behavior of SSC for uncorrupted data is by now \nwell-understood, little is known about its theoretical properties when applied \nto data with missing entries. In this paper we give the first interpretable \ntheoretical guarantees for SSC with incomplete data, and analytically establish \nthat projecting the zero-filled data onto the observation patten of the point \nbeing expressed leads to a substantial improvement in performance. Since the \nprojection induces further missing entries, this is a remarkable phenomenon, \nwhose significance potentially extends to the entire class of self-expressive \nmethods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748475", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00393"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jeffrey W. Miller", "title": "An elementary derivation of the Chinese restaurant process from Sethuraman's stick-breaking process. (arXiv:1801.00513v1 [math.ST])", "alternate": [{"href": "http://arxiv.org/abs/1801.00513", "type": "text/html"}], "timestampUsec": "1514957675355370", "comments": [], "summary": {"content": "<p>The Chinese restaurant process and the stick-breaking process are the two \nmost commonly used representations of the Dirichlet process. However, the usual \nproof of the connection between them is indirect, relying on abstract \nproperties of the Dirichlet process that are difficult for nonexperts to \nverify. This short note provides a direct proof that the stick-breaking process \ngives rise to the Chinese restaurant process, without using any measure theory. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035374848d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00513"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mohammad M. Sultan, Hannah K. Wayment-Steele, Vijay S. Pande", "title": "Transferable neural networks for enhanced sampling of protein dynamics. (arXiv:1801.00636v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.00636", "type": "text/html"}], "timestampUsec": "1514957675355369", "comments": [], "summary": {"content": "<p>Variational auto-encoder frameworks have demonstrated success in reducing \ncomplex nonlinear dynamics in molecular simulation to a single non-linear \nembedding. In this work, we illustrate how this non-linear latent embedding can \nbe used as a collective variable for enhanced sampling, and present a simple \nmodification that allows us to rapidly perform sampling in multiple related \nsystems. We first demonstrate our method is able to describe the effects of \nforce field changes in capped alanine dipeptide after learning a model using \nAMBER99. We further provide a simple extension to variational dynamics encoders \nthat allows the model to be trained in a more efficient manner on larger \nsystems by encoding the outputs of a linear transformation using time-structure \nbased independent component analysis (tICA). Using this technique, we show how \nsuch a model trained for one protein, the WW domain, can efficiently be \ntransferred to perform enhanced sampling on a related mutant protein, the GTT \nmutation. This method shows promise for its ability to rapidly sample related \nsystems using a single transferable collective variable and is generally \napplicable to sets of related simulations, enabling us to probe the effects of \nvariation in increasingly large systems of biophysical interest. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537484a7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00636"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jiashu Zhang, Sheng Zhang, Defang Li", "title": "Random Euler Complex-Valued Nonlinear Filters. (arXiv:1801.00668v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.00668", "type": "text/html"}], "timestampUsec": "1514957675355368", "comments": [], "summary": {"content": "<p>Over the last decade, both the neural network and kernel adaptive filter have \nsuccessfully been used for nonlinear signal processing. However, they suffer \nfrom high computational cost caused by their complex/growing network \nstructures. In this paper, we propose two random Euler filters for \ncomplex-valued nonlinear filtering problem, i.e., linear random Euler \ncomplex-valued filter (LRECF) and its widely-linear version (WLRECF), which \npossess a simple and fixed network structure. The transient and steady-state \nperformances are studied in a non-stationary environment. The analytical \nminimum mean square error (MSE) and optimum step-size are derived. Finally, \nnumerical simulations on complex-valued nonlinear system identification and \nnonlinear channel equalization are presented to show the effectiveness of the \nproposed methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537484c3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00668"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Shuheng Wang, Guohao Li, Yifan Bao", "title": "A novel improved fuzzy support vector machine based stock price trend forecast model. (arXiv:1801.00681v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.00681", "type": "text/html"}], "timestampUsec": "1514957675355367", "comments": [], "summary": {"content": "<p>Application of fuzzy support vector machine in stock price forecast. Support \nvector machine is a new type of machine learning method proposed in 1990s. It \ncan deal with classification and regression problems very successfully. Due to \nthe excellent learning performance of support vector machine, the technology \nhas become a hot research topic in the field of machine learning, and it has \nbeen successfully applied in many fields. However, as a new technology, there \nare many limitations to support vector machines. There is a large amount of \nfuzzy information in the objective world. If the training of support vector \nmachine contains noise and fuzzy information, the performance of the support \nvector machine will become very weak and powerless. As the complexity of many \nfactors influence the stock price prediction, the prediction results of \ntraditional support vector machine cannot meet people with precision, this \nstudy improved the traditional support vector machine fuzzy prediction \nalgorithm is proposed to improve the new model precision. NASDAQ Stock Market, \nStandard &amp; Poor's (S&amp;P) Stock market are considered. Novel advanced- fuzzy \nsupport vector machine (NA-FSVM) is the proposed methodology. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537484cc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00681"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Matias Quiroz, Robert Kohn, Mattias Villani, Minh-Ngoc Tran", "title": "Speeding Up MCMC by Efficient Data Subsampling. (arXiv:1404.4178v6 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1404.4178", "type": "text/html"}], "timestampUsec": "1514957675355366", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329864435\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329864435&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose Subsampling MCMC, a Markov Chain Monte Carlo (MCMC) framework \nwhere the likelihood function for $n$ observations is estimated from a random \nsubset of $m$ observations. We introduce a highly efficient unbiased estimator \nof the log-likelihood based on control variates, such that the computing cost \nis much smaller than that of the full log-likelihood in standard MCMC. The \nlikelihood estimate is bias-corrected and used in two dependent pseudo-marginal \nalgorithms to sample from a perturbed posterior, for which we derive the \nasymptotic error with respect to $n$ and $m$, respectively. We propose a \npractical estimator of the error and show that the error is negligible even for \na very small $m$ in our applications. We demonstrate that Subsampling MCMC is \nsubstantially more efficient than standard MCMC in terms of sampling efficiency \nfor a given computational budget, and that it outperforms other subsampling \nmethods for MCMC proposed in the literature. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537484e2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1404.4178"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "&#xc1;lvaro Barbero, Suvrit Sra", "title": "Modular proximal optimization for multidimensional total-variation regularization. (arXiv:1411.0589v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1411.0589", "type": "text/html"}], "timestampUsec": "1514957675355365", "comments": [], "summary": {"content": "<p>We study \\emph{TV regularization}, a widely used technique for eliciting \nstructured sparsity. In particular, we propose efficient algorithms for \ncomputing prox-operators for $\\ell_p$-norm TV. The most important among these \nis $\\ell_1$-norm TV, for whose prox-operator we present a new geometric \nanalysis which unveils a hitherto unknown connection to taut-string methods. \nThis connection turns out to be remarkably useful as it shows how our geometry \nguided implementation results in efficient weighted and unweighted 1D-TV \nsolvers, surpassing state-of-the-art methods. Our 1D-TV solvers provide the \nbackbone for building more complex (two or higher-dimensional) TV solvers \nwithin a modular proximal optimization approach. We review the literature for \nan array of methods exploiting this strategy, and illustrate the benefits of \nour modular design through extensive suite of experiments on (i) image \ndenoising, (ii) image deconvolution, (iii) four variants of fused-lasso, and \n(iv) video denoising. To underscore our claims and permit easy reproducibility, \nwe provide all the reviewed and our new TV solvers in an easy to use \nmulti-threaded C++, Matlab and Python library. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537484f4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1411.0589"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Vijay Kamble, Patrick Loiseau, Jean Walrand", "title": "Repeated Games with Vector Losses: A Set-valued Dynamic Programming Approach. (arXiv:1603.04981v3 [cs.GT] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1603.04981", "type": "text/html"}], "timestampUsec": "1514957675355364", "comments": [], "summary": {"content": "<p>We consider infinitely repeated games with vector losses discounted over \ntime. We characterize the set of minimal upper bounds on expected losses that a \nplayer can simultaneously guarantee across the different dimensions. \nSpecifically, we show that this set is the fixed point of a set-valued dynamic \nprogramming operator. This approach also characterizes the strategies that \nachieve these bounds. These optimal strategies are shown to be independent of \nthe player's own past actions and stationary relative to a compact state space \nobtained by parameterizing the set of the minimal bounds. We also present a \ncomputational procedure to approximate this set and the optimal strategies. \n</p> \n<p>We discuss two applications of our results: 1) characterization of the \noptimal strategy of the uninformed player in zero-sum discounted repeated games \nwith incomplete information on one side; 2) characterization of the minmax \noptimal regret and the regret-optimal strategy in repeated games with \ndiscounted losses. Our approximation procedure can be used to compute \napproximately optimal strategies in both these applications. We illustrate this \nprocedure by computing approximately regret-optimal strategies for the problem \nof prediction using expert advice from two and three experts under \n$\\{0,1\\}-$losses. Our numerical evaluations demonstrate improved performance \nover existing algorithms for this problem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537484fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1603.04981"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Igor Fedorov, Alican Nalci, Ritwik Giri, Bhaskar D. Rao, Truong Q. Nguyen, Harinath Garudadri", "title": "A Unified Framework for Sparse Non-Negative Least Squares using Multiplicative Updates and the Non-Negative Matrix Factorization Problem. (arXiv:1604.02181v6 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1604.02181", "type": "text/html"}], "timestampUsec": "1514957675355363", "comments": [], "summary": {"content": "<p>We study the sparse non-negative least squares (S-NNLS) problem. S-NNLS \noccurs naturally in a wide variety of applications where an unknown, \nnon-negative quantity must be recovered from linear measurements. We present a \nunified framework for S-NNLS based on a rectified power exponential scale \nmixture prior on the sparse codes. We show that the proposed framework \nencompasses a large class of S-NNLS algorithms and provide a computationally \nefficient inference procedure based on multiplicative update rules. Such update \nrules are convenient for solving large sets of S-NNLS problems simultaneously, \nwhich is required in contexts like sparse non-negative matrix factorization \n(S-NMF). We provide theoretical justification for the proposed approach by \nshowing that the local minima of the objective function being optimized are \nsparse and the S-NNLS algorithms presented are guaranteed to converge to a set \nof stationary points of the objective function. We then extend our framework to \nS-NMF, showing that our framework leads to many well known S-NMF algorithms \nunder specific choices of prior and providing a guarantee that a popular \nsubclass of the proposed algorithms converges to a set of stationary points of \nthe objective function. Finally, we study the performance of the proposed \napproaches on synthetic and real-world data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748517", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1604.02181"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Joel A. Tropp, Alp Yurtsever, Madeleine Udell, Volkan Cevher", "title": "Practical sketching algorithms for low-rank matrix approximation. (arXiv:1609.00048v2 [cs.NA] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1609.00048", "type": "text/html"}], "timestampUsec": "1514957675355362", "comments": [], "summary": {"content": "<p>This paper describes a suite of algorithms for constructing low-rank \napproximations of an input matrix from a random linear image of the matrix, \ncalled a sketch. These methods can preserve structural properties of the input \nmatrix, such as positive-semidefiniteness, and they can produce approximations \nwith a user-specified rank. The algorithms are simple, accurate, numerically \nstable, and provably correct. Moreover, each method is accompanied by an \ninformative error bound that allows users to select parameters a priori to \nachieve a given approximation quality. These claims are supported by numerical \nexperiments with real and synthetic data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748527", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1609.00048"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Adel Javanmard, Hamid Nazerzadeh", "title": "Dynamic Pricing in High-dimensions. (arXiv:1609.07574v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1609.07574", "type": "text/html"}], "timestampUsec": "1514957675355361", "comments": [], "summary": {"content": "<p>We study the pricing problem faced by a firm that sells a large number of \nproducts, described via a wide range of features, to customers that arrive over \ntime. Customers independently make purchasing decisions according to a general \nchoice model that includes products features and customers' characteristics, \nencoded as $d$-dimensional numerical vectors, as well as the price offered. The \nparameters of the choice model are a priori unknown to the firm, but can be \nlearned as the (binary-valued) sales data accrues over time. The firm's \nobjective is to minimize the regret, i.e., the expected revenue loss against a \nclairvoyant policy that knows the parameters of the choice model in advance, \nand always offers the revenue-maximizing price. This setting is motivated in \npart by the prevalence of online marketplaces that allow for real-time pricing. \nWe assume a structured choice model, parameters of which depend on $s_0$ out of \nthe $d$ product features. We propose a dynamic policy, called Regularized \nMaximum Likelihood Pricing (RMLP) that leverages the (sparsity) structure of \nthe high-dimensional model and obtains a logarithmic regret in $T$. More \nspecifically, the regret of our algorithm is of $O(s_0 \\log d \\cdot \\log T)$. \nFurthermore, we show that no policy can obtain regret better than $O(s_0 (\\log \nd + \\log T))$. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035374853a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1609.07574"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zi Wang, Stefanie Jegelka", "title": "Max-value Entropy Search for Efficient Bayesian Optimization. (arXiv:1703.01968v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.01968", "type": "text/html"}], "timestampUsec": "1514957675355360", "comments": [], "summary": {"content": "<p>Entropy Search (ES) and Predictive Entropy Search (PES) are popular and \nempirically successful Bayesian Optimization techniques. Both rely on a \ncompelling information-theoretic motivation, and maximize the information \ngained about the $\\arg\\max$ of the unknown function; yet, both are plagued by \nthe expensive computation for estimating entropies. We propose a new criterion, \nMax-value Entropy Search (MES), that instead uses the information about the \nmaximum function value. We show relations of MES to other Bayesian optimization \nmethods, and establish a regret bound. We observe that MES maintains or \nimproves the good empirical performance of ES/PES, while tremendously \nlightening the computational burden. In particular, MES is much more robust to \nthe number of samples used for computing the entropy, and hence more efficient \nfor higher dimensional problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748544", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.01968"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zi Wang, Clement Gehring, Pushmeet Kohli, Stefanie Jegelka", "title": "Batched Large-scale Bayesian Optimization in High-dimensional Spaces. (arXiv:1706.01445v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.01445", "type": "text/html"}], "timestampUsec": "1514957675355359", "comments": [], "summary": {"content": "<p>Bayesian optimization (BO) has become an effective approach for black-box \nfunction optimization problems when function evaluations are expensive and the \noptimum can be achieved within a relatively small number of queries. However, \nmany cases, such as the ones with high-dimensional inputs, may require a much \nlarger number of observations for optimization. Despite an abundance of \nobservations thanks to parallel experiments, current BO techniques have been \nlimited to merely a few thousand observations. In this paper, we propose \nensemble Bayesian optimization (EBO) to address three current challenges in BO \nsimultaneously: (1) large-scale observations; (2) high dimensional input \nspaces; and (3) selections of batch queries that balance quality and diversity. \nThe key idea of EBO is to operate on an ensemble of additive Gaussian process \nmodels, each of which possesses a randomized strategy to divide and conquer. We \nshow unprecedented, previously impossible results of scaling up BO to tens of \nthousands of observations within minutes of computation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748553", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.01445"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516253549, "author": "Xiang Cheng, Niladri S. Chatterji, Peter L. Bartlett, Michael I. Jordan", "title": "Underdamped Langevin MCMC: A non-asymptotic analysis. (arXiv:1707.03663v6 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.03663", "type": "text/html"}], "timestampUsec": "1514957675355358", "comments": [], "summary": {"content": "<p>We study the underdamped Langevin diffusion when the log of the target \ndistribution is smooth and strongly concave. We present a MCMC algorithm based \non its discretization and show that it achieves $\\varepsilon$ error (in \n2-Wasserstein distance) in $\\mathcal{O}(\\sqrt{d}/\\varepsilon)$ steps. This is a \nsignificant improvement over the best known rate for overdamped Langevin MCMC, \nwhich is $\\mathcal{O}(d/\\varepsilon^2)$ steps under the same \nsmoothness/concavity assumptions. \n</p> \n<p>The underdamped Langevin MCMC scheme can be viewed as a version of \nHamiltonian Monte Carlo (HMC) which has been observed to outperform overdamped \nLangevin MCMC methods in a number of application areas. We provide quantitative \nrates that support this empirical wisdom. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035374855a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.03663"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xinkun Nie, Xiaoying Tian, Jonathan Taylor, James Zou", "title": "Why Adaptively Collected Data Have Negative Bias and How to Correct for It. (arXiv:1708.01977v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.01977", "type": "text/html"}], "timestampUsec": "1514957675355357", "comments": [], "summary": {"content": "<p>From scientific experiments to online A/B testing, the previously observed \ndata often affects how future experiments are performed, which in turn affects \nwhich data will be collected. Such adaptivity introduces complex correlations \nbetween the data and the collection procedure. In this paper, we prove that \nwhen the data collection procedure satisfies natural conditions, then sample \nmeans of the data have systematic \\emph{negative} biases. As an example, \nconsider an adaptive clinical trial where additional data points are more \nlikely to be tested for treatments that show initial promise. Our surprising \nresult implies that the average observed treatment effects would underestimate \nthe true effects of each treatment. We quantitatively analyze the magnitude and \nbehavior of this negative bias in a variety of settings. We also propose a \nnovel debiasing algorithm based on selective inference techniques. In \nexperiments, our method can effectively reduce bias and estimation error. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035374855c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.01977"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jian Du, Shanghang Zhang, Guanhang Wu, Jose M. F. Moura, Soummya Kar", "title": "Topology Adaptive Graph Convolutional Networks. (arXiv:1710.10370v4 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.10370", "type": "text/html"}], "timestampUsec": "1514957675355356", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3298647d3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3298647d3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Convolution acts as a local feature extractor in convolutional neural \nnetworks (CNNs). However, the convolution operation is not applicable when the \ninput data is supported on an irregular graph such as with social networks, \ncitation networks, or knowledge graphs. This paper proposes the topology \nadaptive graph convolutional network (TAGCN), a novel graph convolutional \nnetwork that generalizes CNN architectures to graph-structured data and \nprovides a systematic way to design a set of fixed-size learnable filters to \nperform convolutions on graphs. The topologies of these filters are adaptive to \nthe topology of the graph when they scan the graph to perform convolution, \nreplacing the square filter for the grid-structured data in traditional CNNs. \nThe outputs are the weighted sum of these filters' outputs, extraction of both \nvertex features and strength of correlation between vertices. It can be used \nwith both directed and undirected graphs. The proposed TAGCN not only inherits \nthe properties of convolutions in CNN for grid-structured data, but it is also \nconsistent with convolution as defined in graph signal processing. Further, as \nno approximation to the convolution is needed, TAGCN exhibits better \nperformance than existing graph-convolution-approximation methods on a number \nof data sets. As only the polynomials of degree two of the adjacency matrix are \nused, TAGCN is also computationally simpler than other recent methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748561", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.10370"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Andrea Montanari, Ramji Venkataramanan", "title": "Estimation of Low-Rank Matrices via Approximate Message Passing. (arXiv:1711.01682v2 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.01682", "type": "text/html"}], "timestampUsec": "1514957675355355", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3298ba1d5\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3298ba1d5&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Consider the problem of estimating a low-rank symmetric matrix when its \nentries are perturbed by Gaussian noise, a setting that is also known as \n`spiked model' or `deformed Wigner matrix.' If the empirical distribution of \nthe entries of the spikes is known, optimal estimators that exploit this \nknowledge can substantially outperform simple spectral approaches. Recent work \ncharacterizes the asymptotic accuracy of Bayes-optimal estimators in the \nhigh-dimensional limit. In this paper we present a practical algorithm that can \nachieve Bayes-optimal accuracy above the spectral threshold. A bold conjecture \nfrom statistical physics posits that no polynomial-time algorithm achieves \noptimal error below the same threshold (unless the best estimator is trivial). \n</p> \n<p>Our approach uses Approximate Message Passing (AMP) in conjunction with a \nspectral initialization. AMP algorithms have proved successful in a variety of \nstatistical estimation tasks, and are amenable to exact asymptotic analysis via \nstate evolution. Unfortunately, state evolution is uninformative when the \nalgorithm is initialized near an unstable fixed point, as is often happens in \nlow-rank matrix estimation problems. We develop a a new analysis of AMP that \nallows for spectral initializations, and builds on a decoupling between the \noutlier eigenvectors and the bulk in the spiked random matrix model. \n</p> \n<p>Our main theorem is general and applies beyond matrix estimation. However, we \nuse it to derive detailed predictions for the problem of estimating a rank-one \nmatrix in noise. Special cases of these problem are closely related -- via \nuniversality arguments -- to the network community detection problem for two \nasymmetric communities. As a further illustration, we consider the example of a \nblock-constant low-rank matrix with symmetric blocks, which we refer to as \n`Gaussian Block Model'. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353748564", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.01682"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Victor Chernozhukov, Mert Demirer, Esther Duflo, Ivan Fernandez-Val", "title": "Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments. (arXiv:1712.04802v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.04802", "type": "text/html"}], "timestampUsec": "1514957675355354", "comments": [], "summary": {"content": "<p>We propose strategies to estimate and make inference on key features of \nheterogeneous effects in randomized experiments. These key features include \nbest linear predictors of the effects using machine learning proxies, average \neffects sorted by impact groups, and average characteristics of most and least \nimpacted units. The approach is valid in high dimensional settings, where the \neffects are proxied by machine learning methods. We post-process these proxies \ninto the estimates of the key features. Our approach is generic, it can be used \nin conjunction with penalized methods, deep and shallow neural networks, \ncanonical and new random forests, boosted trees, and ensemble methods. Our \napproach is agnostic and does not make unrealistic or hard-to-check \nassumptions; we don't require conditions for consistency of the ML methods. \nEstimation and inference relies on repeated data splitting to avoid overfitting \nand achieve validity. For inference, we take medians of p-values and medians of \nconfidence intervals, resulting from many different data splits, and then \nadjust their nominal level to guarantee uniform validity. This variational \ninference method is shown to be uniformly valid and quantifies the uncertainty \ncoming from both parameter estimation and data splitting. The inference method \ncould be of substantial independent interest in many machine learning \napplications. An empirical application to the impact of micro-credit on \neconomic development illustrates the use of the approach in randomized \nexperiments. An additional application to the impact of the gender \ndiscrimination on wages illustrates the potential use of the approach in \nobservational studies, where machine learning methods can be used to condition \nflexibly on very high-dimensional controls. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957675355", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035374856c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04802"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jo&#xe3;o Sacramento, Rui Ponte Costa, Yoshua Bengio, Walter Senn", "title": "Dendritic error backpropagation in deep cortical microcircuits. (arXiv:1801.00062v1 [q-bio.NC])", "alternate": [{"href": "http://arxiv.org/abs/1801.00062", "type": "text/html"}], "timestampUsec": "1514957559563078", "comments": [], "summary": {"content": "<p>Animal behaviour depends on learning to associate sensory stimuli with the \ndesired motor command. Understanding how the brain orchestrates the necessary \nsynaptic modifications across different brain areas has remained a longstanding \npuzzle. Here, we introduce a multi-area neuronal network model in which \nsynaptic plasticity continuously adapts the network towards a global desired \noutput. In this model synaptic learning is driven by a local dendritic \nprediction error that arises from a failure to predict the top-down input given \nthe bottom-up activities. Such errors occur at apical dendrites of pyramidal \nneurons where both long-range excitatory feedback and local inhibitory \npredictions are integrated. When local inhibition fails to match excitatory \nfeedback an error occurs which triggers plasticity at bottom-up synapses at \nbasal dendrites of the same pyramidal neurons. We demonstrate the learning \ncapabilities of the model in a number of tasks and show that it approximates \nthe classical error backpropagation algorithm. Finally, complementing this \ncortical circuit with a disinhibitory mechanism enables attention-like stimulus \ndenoising and generation. Our framework makes several experimental predictions \non the function of dendritic integration and cortical microcircuits, is \nconsistent with recent observations of cross-area learning, and suggests a \nbiological implementation of deep learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537450cf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00062"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Haik Manukian, Fabio L. Traversa, Massimiliano Di Ventra", "title": "Accelerating Deep Learning with Memcomputing. (arXiv:1801.00512v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.00512", "type": "text/html"}], "timestampUsec": "1514957559563077", "comments": [], "summary": {"content": "<p>Restricted Boltzmann machines (RBMs) and their extensions, often called \n\"deep-belief networks\", are very powerful neural networks that have found \nwidespread applicability in the fields of machine learning and big data. The \nstandard way to training these models resorts to an iterative unsupervised \nprocedure based on Gibbs sampling, called \"contrastive divergence\", and \nadditional supervised tuning via back-propagation. However, this procedure has \nbeen shown not to follow any gradient and can lead to suboptimal solutions. In \nthis paper, we show a very efficient alternative to contrastive divergence by \nmeans of simulations of digital memcomputing machines (DMMs). We test our \napproach on pattern recognition using the standard MNIST data set of \nhand-written numbers. DMMs sample very effectively the vast phase space defined \nby the probability distribution of RBMs over the test sample inputs, and \nprovide a very good approximation close to the optimum. This efficient search \nsignificantly reduces the number of generative pre-training iterations \nnecessary to achieve a given level of accuracy in the MNIST data set, as well \nas a total performance gain over the traditional approaches. In fact, the \nacceleration of the pre-training achieved by simulating DMMs is comparable to, \nin number of iterations, the recently reported hardware application of the \nquantum annealing method on the same network and data set. Notably, however, \nDMMs perform far better than the reported quantum annealing results in terms of \nquality of the training. Our approach is agnostic about the connectivity of the \nnetwork. Therefore, it can be extended to train full Boltzmann machines, and \neven deep networks at once. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537450d5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00512"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Eliya Nachmani, Elad Marciano, Loren Lugosch, Warren J. Gross, David Burshtein, Yair Beery", "title": "Deep Learning Methods for Improved Decoding of Linear Codes. (arXiv:1706.07043v2 [cs.IT] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.07043", "type": "text/html"}], "timestampUsec": "1514957559563076", "comments": [], "summary": {"content": "<p>The problem of low complexity, close to optimal, channel decoding of linear \ncodes with short to moderate block length is considered. It is shown that deep \nlearning methods can be used to improve a standard belief propagation decoder, \ndespite the large example space. Similar improvements are obtained for the \nmin-sum algorithm. It is also shown that tying the parameters of the decoders \nacross iterations, so as to form a recurrent neural network architecture, can \nbe implemented with comparable results. The advantage is that significantly \nless parameters are required. We also introduce a recurrent neural decoder \narchitecture based on the method of successive relaxation. Improvements over \nstandard belief propagation are also observed on sparser Tanner graph \nrepresentations of the codes. Furthermore, we demonstrate that the neural \nbelief propagation decoder can be used to improve the performance, or \nalternatively reduce the computational complexity, of a close to optimal \ndecoder of short BCH codes. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537450d8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.07043"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zichao Long, Yiping Lu, Xianzhong Ma, Bin Dong", "title": "PDE-Net: Learning PDEs from Data. (arXiv:1710.09668v2 [math.NA] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.09668", "type": "text/html"}], "timestampUsec": "1514957559563075", "comments": [], "summary": {"content": "<p>In this paper, we present an initial attempt to learn evolution PDEs from \ndata. Inspired by the latest development of neural network designs in deep \nlearning, we propose a new feed-forward deep network, called PDE-Net, to \nfulfill two objectives at the same time: to accurately predict dynamics of \ncomplex systems and to uncover the underlying hidden PDE models. The basic idea \nof the proposed PDE-Net is to learn differential operators by learning \nconvolution kernels (filters), and apply neural networks or other machine \nlearning methods to approximate the unknown nonlinear responses. Comparing with \nexisting approaches, which either assume the form of the nonlinear response is \nknown or fix certain finite difference approximations of differential \noperators, our approach has the most flexibility by learning both differential \noperators and the nonlinear responses. A special feature of the proposed \nPDE-Net is that all filters are properly constrained, which enables us to \neasily identify the governing PDE models while still maintaining the expressive \nand predictive power of the network. These constrains are carefully designed by \nfully exploiting the relation between the orders of differential operators and \nthe orders of sum rules of filters (an important concept originated from \nwavelet theory). We also discuss relations of the PDE-Net with some existing \nnetworks in computer vision such as Network-In-Network (NIN) and Residual \nNeural Network (ResNet). Numerical experiments show that the PDE-Net has the \npotential to uncover the hidden PDE of the observed dynamics, and predict the \ndynamical behavior for a relatively long time, even in a noisy environment. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537450df", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.09668"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Simon Stiebellehner, Jun Wang, Shuai Yuan", "title": "Learning Continuous User Representations through Hybrid Filtering with doc2vec. (arXiv:1801.00215v1 [cs.IR])", "alternate": [{"href": "http://arxiv.org/abs/1801.00215", "type": "text/html"}], "timestampUsec": "1514957559563074", "comments": [], "summary": {"content": "<p>Players in the online ad ecosystem are struggling to acquire the user data \nrequired for precise targeting. Audience look-alike modeling has the potential \nto alleviate this issue, but models' performance strongly depends on quantity \nand quality of available data. In order to maximize the predictive performance \nof our look-alike modeling algorithms, we propose two novel hybrid filtering \ntechniques that utilize the recent neural probabilistic language model \nalgorithm doc2vec. We apply these methods to data from a large mobile ad \nexchange and additional app metadata acquired from the Apple App store and \nGoogle Play store. First, we model mobile app users through their app usage \nhistories and app descriptions (user2vec). Second, we introduce context \nawareness to that model by incorporating additional user and app-related \nmetadata in model training (context2vec). Our findings are threefold: (1) the \nquality of recommendations provided by user2vec is notably higher than current \nstate-of-the-art techniques. (2) User representations generated through hybrid \nfiltering using doc2vec prove to be highly valuable features in supervised \nmachine learning models for look-alike modeling. This represents the first \napplication of hybrid filtering user models using neural probabilistic language \nmodels, specifically doc2vec, in look-alike modeling. (3) Incorporating context \nmetadata in the doc2vec model training process to introduce context awareness \nhas positive effects on performance and is superior to directly including the \ndata as features in the downstream supervised models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537450ec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00215"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mateusz K. Tarkowski, Tomasz P. Michalak, Talal Rahwan, Michael Wooldridge", "title": "Game-theoretic Network Centrality: A Review. (arXiv:1801.00218v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1801.00218", "type": "text/html"}], "timestampUsec": "1514957559563073", "comments": [], "summary": {"content": "<p>Game-theoretic centrality is a flexible and sophisticated approach to \nidentify the most important nodes in a network. It builds upon the methods from \ncooperative game theory and network theory. The key idea is to treat nodes as \nplayers in a cooperative game, where the value of each coalition is determined \nby certain graph-theoretic properties. Using solution concepts from cooperative \ngame theory, it is then possible to measure how responsible each node is for \nthe worth of the network. \n</p> \n<p>The literature on the topic is already quite large, and is scattered among \ngame-theoretic and computer science venues. We review the main game-theoretic \nnetwork centrality measures from both bodies of literature and organize them \ninto two categories: those that are more focused on the connectivity of nodes, \nand those that are more focused on the synergies achieved by nodes in groups. \nWe present and explain each centrality, with a focus on algorithms and \ncomplexity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537450f4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00218"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Walid Shalaby, Wlodek Zadrozny, Hongxia Jin", "title": "Beyond Word Embeddings: Learning Entity and Concept Representations from Large Scale Knowledge Bases. (arXiv:1801.00388v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1801.00388", "type": "text/html"}], "timestampUsec": "1514957559563072", "comments": [], "summary": {"content": "<p>Text representation using neural word embeddings has proven efficacy in many \nNLP applications. Recently, a lot of research interest goes beyond word \nembeddings by adapting the traditional word embedding models to learn vectors \nof multiword expressions (concepts/entities). However, current methods are \nlimited to textual knowledge bases only (e.g., Wikipedia). In this paper, we \npropose a novel approach for learning concept vectors from two large scale \nknowledge bases (Wikipedia, and Probase). We adapt the skip-gram model to \nseamlessly learn from the knowledge in Wikipedia text and Probase concept \ngraph. We evaluate our concept embedding models intrinsically on two tasks: 1) \nanalogical reasoning where we achieve a state-of-the-art performance of 91% on \nsemantic analogies, 2) concept categorization where we achieve a \nstate-of-the-art performance on two benchmark datasets achieving categorization \naccuracy of 100% on one and 98% on the other. Additionally, we present a case \nstudy to extrinsically evaluate our model on unsupervised argument type \nidentification for neural semantic parsing. We demonstrate the competitive \naccuracy of our unsupervised method and its ability to better generalize to out \nof vocabulary entity mentions compared to the tedious and error prone methods \nwhich depend on gazetteers and regular expressions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537450f8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00388"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gopal P. Sarma, Nick J. Hay", "title": "Mammalian Value Systems. (arXiv:1607.08289v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1607.08289", "type": "text/html"}], "timestampUsec": "1514957559563071", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3298ba3d7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3298ba3d7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Characterizing human values is a topic deeply interwoven with the sciences, \nhumanities, art, and many other human endeavors. In recent years, a number of \nthinkers have argued that accelerating trends in computer science, cognitive \nscience, and related disciplines foreshadow the creation of intelligent \nmachines which meet and ultimately surpass the cognitive abilities of human \nbeings, thereby entangling an understanding of human values with future \ntechnological development. Contemporary research accomplishments suggest \nsophisticated AI systems becoming widespread and responsible for managing many \naspects of the modern world, from preemptively planning users' travel schedules \nand logistics, to fully autonomous vehicles, to domestic robots assisting in \ndaily living. The extrapolation of these trends has been most forcefully \ndescribed in the context of a hypothetical \"intelligence explosion,\" in which \nthe capabilities of an intelligent software agent would rapidly increase due to \nthe presence of feedback loops unavailable to biological organisms. The \npossibility of superintelligent agents, or simply the widespread deployment of \nsophisticated, autonomous AI systems, highlights an important theoretical \nproblem: the need to separate the cognitive and rational capacities of an agent \nfrom the fundamental goal structure, or value system, which constrains and \nguides the agent's actions. The \"value alignment problem\" is to specify a goal \nstructure for autonomous agents compatible with human values. In this brief \narticle, we suggest that recent ideas from affective neuroscience and related \ndisciplines aimed at characterizing neurological and behavioral universals in \nthe mammalian kingdom provide important conceptual foundations relevant to \ndescribing human values. We argue that the notion of \"mammalian value systems\" \npoints to a potential avenue for fundamental research in AI safety and AI \nethics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003537450fd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1607.08289"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christopher Grimm, Dilip Arumugam, Siddharth Karamcheti, David Abel, Lawson L.S. Wong, Michael L. Littman", "title": "Modeling Latent Attention Within Neural Networks. (arXiv:1706.00536v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.00536", "type": "text/html"}], "timestampUsec": "1514957559563070", "comments": [], "summary": {"content": "<p>Deep neural networks are able to solve tasks across a variety of domains and \nmodalities of data. Despite many empirical successes, we lack the ability to \nclearly understand and interpret the learned internal mechanisms that \ncontribute to such effective behaviors or, more critically, failure modes. In \nthis work, we present a general method for visualizing an arbitrary neural \nnetwork's inner mechanisms and their power and limitations. Our dataset-centric \nmethod produces visualizations of how a trained network attends to components \nof its inputs. The computed \"attention masks\" support improved interpretability \nby highlighting which input attributes are critical in determining output. We \ndemonstrate the effectiveness of our framework on a variety of deep neural \nnetwork architectures in domains from computer vision, natural language \nprocessing, and reinforcement learning. The primary contribution of our \napproach is an interpretable visualization of attention that provides unique \ninsights into the network's underlying decision-making process irrespective of \nthe data modality. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353745102", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.00536"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wangyan Feng, Shuning Wu, Xiaodan Li, Kevin Kunkle", "title": "A Deep Belief Network Based Machine Learning System for Risky Host Detection. (arXiv:1801.00025v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1801.00025", "type": "text/html"}], "timestampUsec": "1514957559563068", "comments": [], "summary": {"content": "<p>To assure cyber security of an enterprise, typically SIEM (Security \nInformation and Event Management) system is in place to normalize security \nevent from different preventive technologies and flag alerts. Analysts in the \nsecurity operation center (SOC) investigate the alerts to decide if it is truly \nmalicious or not. However, generally the number of alerts is overwhelming with \nmajority of them being false positive and exceeding the SOC's capacity to \nhandle all alerts. There is a great need to reduce the false positive rate as \nmuch as possible. While most previous research focused on network intrusion \ndetection, we focus on risk detection and propose an intelligent Deep Belief \nNetwork machine learning system. The system leverages alert information, \nvarious security logs and analysts' investigation results in a real enterprise \nenvironment to flag hosts that have high likelihood of being compromised. Text \nmining and graph based method are used to generate targets and create features \nfor machine learning. In the experiment, Deep Belief Network is compared with \nother machine learning algorithms, including multi-layer neural network, random \nforest, support vector machine and logistic regression. Results on real \nenterprise data indicate that the deep belief network machine learning system \nperforms better than other algorithms for our problem and is six times more \neffective than current rule-based system. We also implement the whole system \nfrom data collection, label creation, feature engineering to host score \ngeneration in a real enterprise production environment. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035374510a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00025"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Dawei Yin, Yihong Zhao, Jiliang Tang", "title": "Deep Reinforcement Learning for List-wise Recommendations. (arXiv:1801.00209v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.00209", "type": "text/html"}], "timestampUsec": "1514957559563067", "comments": [], "summary": {"content": "<p>Recommender systems play a crucial role in mitigating the problem of \ninformation overload by suggesting users' personalized items or services. The \nvast majority of traditional recommender systems consider the recommendation \nprocedure as a static process and make recommendations following a fixed \nstrategy. In this paper, we propose a novel recommender system with the \ncapability of continuously improving its strategies during the interactions \nwith users. We model the sequential interactions between users and a \nrecommender system as a Markov Decision Process (MDP) and leverage \nReinforcement Learning (RL) to automatically learn the optimal strategies via \nrecommending trial-and-error items and receiving reinforcements of these items \nfrom users' feedbacks. In particular, we introduce an online user-agent \ninteracting environment simulator, which can pre-train and evaluate model \nparameters offline before applying the model online. Moreover, we validate the \nimportance of list-wise recommendations during the interactions between users \nand agent, and develop a novel approach to incorporate them into the proposed \nframework LIRD for list-wide recommendations. The experimental results based on \na real-world e-commerce dataset demonstrate the effectiveness of the proposed \nframework. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353745110", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00209"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jie Jia, Honggang Zhou, Yunchun Li", "title": "Using Deep Neural Network Approximate Bayesian Network. (arXiv:1801.00282v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.00282", "type": "text/html"}], "timestampUsec": "1514957559563066", "comments": [], "summary": {"content": "<p>We present a new method to approximate posterior probabilities of Bayesian \nNetwork using Deep Neural Network. Experiment results on several public \nBayesian Network datasets shows that Deep Neural Network is capable of learning \njoint probability distri- bution of Bayesian Network by learning from a few \nobservation and posterior probability distribution pairs with high accuracy. \nCompared with traditional approximate method likelihood weighting sampling \nalgorithm, our method is much faster and gains higher accuracy in medium sized \nBayesian Network. Another advantage of our method is that our method can be \nparallelled much easier in GPU without extra effort. We also ex- plored the \nconnection between the accuracy of our model and the number of training \nexamples. The result shows that our model saturate as the number of training \nexamples grow and we don't need many training examples to get reasonably good \nresult. Another contribution of our work is that we have shown discriminative \nmodel like Deep Neural Network can approximate generative model like Bayesian \nNetwork. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353745115", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00282"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexander Zimin, Christoph Lampert", "title": "Towards Practical Conditional Risk Minimization. (arXiv:1801.00507v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.00507", "type": "text/html"}], "timestampUsec": "1514957559563065", "comments": [], "summary": {"content": "<p>We study conditional risk minimization (CRM), i.e. the problem of learning a \nhypothesis of minimal risk for prediction at the next step of a sequentially \narriving dependent data. Despite it being a fundamental problem, successful \nlearning in the CRM sense has so far only been demonstrated using theoretical \nalgorithms that cannot be used for real problems as they would require storing \nall incoming data. In this work, we introduce MACRO, a meta-algorithm for CRM \nthat does not suffer from this shortcoming, as instead of storing all data it \nmaintains and iteratively updates a set of learning subroutines. Using suitable \napproximations, MACRO can be implemented and applied to real data, leading, as \nwe illustrate experimentally, to improved prediction performance compared to \ntraditional non-conditional learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035374511e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00507"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Cedric De Boom, Bart Dhoedt, Thomas Demeester", "title": "Character-level Recurrent Neural Networks in Practice: Comparing Training and Sampling Schemes. (arXiv:1801.00632v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.00632", "type": "text/html"}], "timestampUsec": "1514957559563064", "comments": [], "summary": {"content": "<p>Recurrent neural networks are nowadays successfully used in an abundance of \napplications, going from text, speech and image processing to recommender \nsystems. Backpropagation through time is the algorithm that is commonly used to \ntrain these networks on specific tasks. Many deep learning frameworks have \ntheir own implementation of training and sampling procedures for recurrent \nneural networks, while there are in fact multiple other possibilities to choose \nfrom and other parameters to tune. In existing literature this is very often \noverlooked or ignored. In this paper we therefore give an overview of possible \ntraining and sampling schemes for character-level recurrent neural networks to \nsolve the task of predicting the next token in a given sequence. We test these \ndifferent schemes on a variety of datasets, neural network architectures and \nparameter settings, and formulate a number of take-home recommendations. The \nchoice of training and sampling scheme turns out to be subject to a number of \ntrade-offs, such as training stability, sampling time, model performance and \nimplementation effort, but is largely independent of the data. Perhaps the most \nsurprising result is that transferring hidden states for correctly initializing \nthe model on subsequences often leads to unstable training behavior depending \non the dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353745124", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00632"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shiliang Sun, Rongqing Huang, Ya Gao", "title": "Network-Scale Traffic Modeling and Forecasting with Graphical Lasso and Neural Networks. (arXiv:1801.00711v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.00711", "type": "text/html"}], "timestampUsec": "1514957559563063", "comments": [], "summary": {"content": "<p>Traffic flow forecasting, especially the short-term case, is an important \ntopic in intelligent transportation systems (ITS). This paper does a lot of \nresearch on network-scale modeling and forecasting of short-term traffic flows. \nFirstly, we propose the concepts of single-link and multi-link models of \ntraffic flow forecasting. Secondly, we construct four prediction models by \ncombining the two models with single-task learning and multi-task learning. The \ncombination of the multi-link model and multi-task learning not only improves \nthe experimental efficiency but also the prediction accuracy. Moreover, a new \nmulti-link single-task approach that combines graphical lasso (GL) with neural \nnetwork (NN) is proposed. GL provides a general methodology for solving \nproblems involving lots of variables. Using L1 regularization, GL builds a \nsparse graphical model making use of the sparse inverse covariance matrix. In \naddition, Gaussian process regression (GPR) is a classic regression algorithm \nin Bayesian machine learning. Although there is wide research on GPR, there are \nfew applications of GPR in traffic flow forecasting. In this paper, we apply \nGPR to traffic flow forecasting and show its potential. Through sufficient \nexperiments, we compare all of the proposed approaches and make an overall \nassessment at last. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035374512d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00711"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pegah Karimi, Nicholas Davis, Kazjon Grace, Mary Lou Maher", "title": "Deep Learning for Identifying Potential Conceptual Shifts for Co-creative Drawing. (arXiv:1801.00723v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1801.00723", "type": "text/html"}], "timestampUsec": "1514957559563062", "comments": [], "summary": {"content": "<p>We present a system for identifying conceptual shifts between visual \ncategories, which will form the basis for a co-creative drawing system to help \nusers draw more creative sketches. The system recognizes human sketches and \nmatches them to structurally similar sketches from categories to which they do \nnot belong. This would allow a co-creative drawing system to produce an \nambiguous sketch that blends features from both categories. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353745133", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00723"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Frithjof Gressmann, Franz J. Kir&#xe1;ly, Bilal Mateen, Harald Oberhauser", "title": "Probabilistic supervised learning. (arXiv:1801.00753v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1801.00753", "type": "text/html"}], "timestampUsec": "1514957559563061", "comments": [], "summary": {"content": "<p>Predictive modelling and supervised learning are central to modern data \nscience. With predictions from an ever-expanding number of supervised black-box \nstrategies - e.g., kernel methods, random forests, deep learning aka neural \nnetworks - being employed as a basis for decision making processes, it is \ncrucial to understand the statistical uncertainty associated with these \npredictions. \n</p> \n<p>As a general means to approach the issue, we present an overarching framework \nfor black-box prediction strategies that not only predict the target but also \ntheir own predictions' uncertainty. Moreover, the framework allows for fair \nassessment and comparison of disparate prediction strategies. For this, we \nformally consider strategies capable of predicting full distributions from \nfeature variables, so-called probabilistic supervised learning strategies. \n</p> \n<p>Our work draws from prior work including Bayesian statistics, information \ntheory, and modern supervised machine learning, and in a novel synthesis leads \nto (a) new theoretical insights such as a probabilistic bias-variance \ndecomposition and an entropic formulation of prediction, as well as to (b) new \nalgorithms and meta-algorithms, such as composite prediction strategies, \nprobabilistic boosting and bagging, and a probabilistic predictive independence \ntest. \n</p> \n<p>Our black-box formulation also leads (c) to a new modular interface view on \nprobabilistic supervised learning and a modelling workflow API design, which we \nhave implemented in the newly released skpro machine learning toolbox, \nextending the familiar modelling interface and meta-modelling functionality of \nsklearn. The skpro package provides interfaces for construction, composition, \nand tuning of probabilistic supervised learning strategies, together with \norchestration features for validation and comparison of any such strategy - be \nit frequentist, Bayesian, or other. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353745144", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1801.00753"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pau Perng-Hwa Kung", "title": "Deep Poisson Factorization Machines: factor analysis for mapping behaviors in journalist ecosystem. (arXiv:1512.05840v2 [cs.CY] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1512.05840", "type": "text/html"}], "timestampUsec": "1514957559563060", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3298ba5b4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3298ba5b4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Newsroom in online ecosystem is difficult to untangle. With prevalence of \nsocial media, interactions between journalists and individuals become visible, \nbut lack of understanding to inner processing of information feedback loop in \npublic sphere leave most journalists baffled. Can we provide an organized view \nto characterize journalist behaviors on individual level to know better of the \necosystem? To this end, I propose Poisson Factorization Machine (PFM), a \nBayesian analogue to matrix factorization that assumes Poisson distribution for \ngenerative process. The model generalizes recent studies on Poisson Matrix \nFactorization to account temporal interaction which involves tensor-like \nstructure, and label information. Two inference procedures are designed, one \nbased on batch variational EM and another stochastic variational inference \nscheme that efficiently scales with data size. An important novelty in this \nnote is that I show how to stack layers of PFM to introduce a deep \narchitecture. This work discusses some potential results applying the model and \nexplains how such latent factors may be useful for analyzing latent behaviors \nfor data exploration. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353745152", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1512.05840"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Elad Hoffer, Itay Hubara, Daniel Soudry", "title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks. (arXiv:1705.08741v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08741", "type": "text/html"}], "timestampUsec": "1514957559563059", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329924c9e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329924c9e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Background: Deep learning models are typically trained using stochastic \ngradient descent or one of its variants. These methods update the weights using \ntheir gradient, estimated from a small fraction of the training data. It has \nbeen observed that when using large batch sizes there is a persistent \ndegradation in generalization performance - known as the \"generalization gap\" \nphenomena. Identifying the origin of this gap and closing it had remained an \nopen problem. \n</p> \n<p>Contributions: We examine the initial high learning rate training phase. We \nfind that the weight distance from its initialization grows logarithmically \nwith the number of weight updates. We therefore propose a \"random walk on \nrandom landscape\" statistical model which is known to exhibit similar \n\"ultra-slow\" diffusion behavior. Following this hypothesis we conducted \nexperiments to show empirically that the \"generalization gap\" stems from the \nrelatively small number of updates rather than the batch size, and can be \ncompletely eliminated by adapting the training regime used. We further \ninvestigate different techniques to train models in the large-batch regime and \npresent a novel algorithm named \"Ghost Batch Normalization\" which enables \nsignificant decrease in the generalization gap without increasing the number of \nupdates. To validate our findings we conduct several additional experiments on \nMNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices \nand beliefs concerning training of deep models and suggest they may not be \noptimal to achieve good generalization. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353745159", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08741"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yuhang Song, Main Xu, Songyang Zhang, Liangyu Huo", "title": "Generalization Tower Network: A Novel Deep Neural Network Architecture for Multi-Task Learning. (arXiv:1710.10036v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.10036", "type": "text/html"}], "timestampUsec": "1514957559563058", "comments": [], "summary": {"content": "<p>Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by \nincorporating deep neural networks in learning representations from the input \nto RL. However, the conventional deep neural network architecture is limited in \nlearning representations for multi-task RL (MT-RL), as multiple tasks can refer \nto different kinds of representations. In this paper, we thus propose a novel \ndeep neural network architecture, namely generalization tower network (GTN), \nwhich can achieve MT-RL within a single learned model. Specifically, the \narchitecture of GTN is composed of both horizontal and vertical streams. In our \nGTN architecture, horizontal streams are used to learn representation shared in \nsimilar tasks. In contrast, the vertical streams are introduced to be more \nsuitable for handling diverse tasks, which encodes hierarchical shared \nknowledge of these tasks. The effectiveness of the introduced vertical stream \nis validated by experimental results. Experimental results further verify that \nour GTN architecture is able to advance the state-of-the-art MT-RL, via being \ntested on 51 Atari games. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000353745162", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.10036"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andrew K. Lampinen, James L. McClelland", "title": "One-shot and few-shot learning of word embeddings. (arXiv:1710.10280v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.10280", "type": "text/html"}], "timestampUsec": "1514957559563057", "comments": [], "summary": {"content": "<p>Standard deep learning systems require thousands or millions of examples to \nlearn a concept, and cannot integrate new concepts easily. By contrast, humans \nhave an incredible ability to do one-shot or few-shot learning. For instance, \nfrom just hearing a word used in a sentence, humans can infer a great deal \nabout it, by leveraging what the syntax and semantics of the surrounding words \ntells us. Here, we draw inspiration from this to highlight a simple technique \nby which deep recurrent networks can similarly exploit their prior knowledge to \nlearn a useful representation for a new word from little data. This could make \nnatural language processing systems much more flexible, by allowing them to \nlearn continually from the new words they encounter. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514957559563", "annotations": [], "published": 1514957560, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035374516a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.10280"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Marco Martinolli, Wulfram Gerstner, Aditya Gilra", "title": "Multi-timescale memory dynamics in a reinforcement learning network with attention-gated memory. (arXiv:1712.10062v1 [q-bio.NC])", "alternate": [{"href": "http://arxiv.org/abs/1712.10062", "type": "text/html"}], "timestampUsec": "1514784910796488", "comments": [], "summary": {"content": "<p>Learning and memory are intertwined in our brain and their relationship is at \nthe core of several recent neural network models. In particular, the \nAttention-Gated MEmory Tagging model (AuGMEnT) is a reinforcement learning \nnetwork with an emphasis on biological plausibility of memory dynamics and \nlearning. We find that the AuGMEnT network does not solve some hierarchical \ntasks, where higher-level stimuli have to be maintained over a long time, while \nlower-level stimuli need to be remembered and forgotten over a shorter \ntimescale. To overcome this limitation, we introduce hybrid AuGMEnT, with leaky \nor short-timescale and non-leaky or long-timescale units in memory, that allow \nto exchange lower-level information while maintaining higher-level one, thus \nsolving both hierarchical and distractor tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003524584f5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10062"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li", "title": "TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning. (arXiv:1705.07878v6 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.07878", "type": "text/html"}], "timestampUsec": "1514784910796487", "comments": [], "summary": {"content": "<p>High network communication cost for synchronizing gradients and parameters is \nthe well-known bottleneck of distributed training. In this work, we propose \nTernGrad that uses ternary gradients to accelerate distributed deep learning in \ndata parallelism. Our approach requires only three numerical levels {-1,0,1}, \nwhich can aggressively reduce the communication time. We mathematically prove \nthe convergence of TernGrad under the assumption of a bound on gradients. \nGuided by the bound, we propose layer-wise ternarizing and gradient clipping to \nimprove its convergence. Our experiments show that applying TernGrad on AlexNet \ndoes not incur any accuracy loss and can even improve accuracy. The accuracy \nloss of GoogLeNet induced by TernGrad is less than 2% on average. Finally, a \nperformance model is proposed to study the scalability of TernGrad. Experiments \nshow significant speed gains for various deep neural networks. Our source code \nis available. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003524584fb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.07878"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kian Ahrabian, Bagher Babaali", "title": "On Usage of Autoencoders and Siamese Networks for Online Handwritten Signature Verification. (arXiv:1712.02781v2 [cs.NE] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02781", "type": "text/html"}], "timestampUsec": "1514784910796486", "comments": [], "summary": {"content": "<p>In this paper, we propose a novel writer-independent global feature \nextraction framework for the task of automatic signature verification which \naims to make robust systems for automatically distinguishing negative and \npositive samples. Our method consists of an autoencoder for modeling the sample \nspace into a fixed length latent space and a Siamese Network for classifying \nthe fixed-length samples obtained from the autoencoder based on the reference \nsamples of a subject as being \"Genuine\" or \"Forged.\" During our experiments, \nusage of Attention Mechanism and applying Downsampling significantly improved \nthe accuracy of the proposed framework. We evaluated our proposed framework \nusing SigWiComp2013 Japanese and GPDSsyntheticOnLineOffLineSignature datasets. \nOn the SigWiComp2013 Japanese dataset, we achieved 8.65% EER that means 1.2% \nrelative improvement compared to the best-reported result. Furthermore, on the \nGPDSsyntheticOnLineOffLineSignature dataset, we achieved average EERs of 0.13%, \n0.12%, 0.21% and 0.25% respectively for 150, 300, 1000 and 2000 test subjects \nwhich indicates improvement of relative EER on the best-reported result by \n95.67%, 95.26%, 92.9% and 91.52% respectively. Apart from the accuracy gain, \nbecause of the nature of our proposed framework which is based on neural \nnetworks and consequently is as simple as some consecutive matrix \nmultiplications, it has less computational cost than conventional methods such \nas DTW and could be used concurrently on devices such as GPU, TPU, etc. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352458505", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02781"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Anqi Liu, Brian D. Ziebart", "title": "Robust Covariate Shift Prediction with General Losses and Feature Views. (arXiv:1712.10043v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.10043", "type": "text/html"}], "timestampUsec": "1514784910796485", "comments": [], "summary": {"content": "<p>Covariate shift relaxes the widely-employed independent and identically \ndistributed (IID) assumption by allowing different training and testing input \ndistributions. Unfortunately, common methods for addressing covariate shift by \ntrying to remove the bias between training and testing distributions using \nimportance weighting often provide poor performance guarantees in theory and \nunreliable predictions with high variance in practice. Recently developed \nmethods that construct a predictor that is inherently robust to the \ndifficulties of learning under covariate shift are restricted to minimizing \nlogloss and can be too conservative when faced with high-dimensional learning \ntasks. We address these limitations in two ways: by robustly minimizing various \nloss functions, including non-convex ones, under the testing distribution; and \nby separately shaping the influence of covariate shift according to different \nfeature-based views of the relationship between input variables and example \nlabels. These generalizations make robust covariate shift prediction applicable \nto more task scenarios. We demonstrate the benefits on classification under \ncovariate shift tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035245850a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10043"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Edgar Altszyler, Mariano Sigman, Diego Fernandez Slezak", "title": "Corpus specificity in LSA and Word2vec: the role of out-of-domain documents. (arXiv:1712.10054v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.10054", "type": "text/html"}], "timestampUsec": "1514784910796484", "comments": [], "summary": {"content": "<p>Latent Semantic Analysis (LSA) and Word2vec are some of the most widely used \nword embeddings. Despite the popularity of these techniques, the precise \nmechanisms by which they acquire new semantic relations between words remain \nunclear. In the present article we investigate whether LSA and Word2vec \ncapacity to identify relevant semantic dimensions increases with size of \ncorpus. One intuitive hypothesis is that the capacity to identify relevant \ndimensions should increase as the amount of data increases. However, if corpus \nsize grow in topics which are not specific to the domain of interest, signal to \nnoise ratio may weaken. Here we set to examine and distinguish these \nalternative hypothesis. To investigate the effect of corpus specificity and \nsize in word-embeddings we study two ways for progressive elimination of \ndocuments: the elimination of random documents vs. the elimination of documents \nunrelated to a specific task. We show that Word2vec can take advantage of all \nthe documents, obtaining its best performance when it is trained with the whole \ncorpus. On the contrary, the specialization (removal of out-of-domain \ndocuments) of the training corpus, accompanied by a decrease of dimensionality, \ncan increase LSA word-representation quality while speeding up the processing \ntime. Furthermore, we show that the specialization without the decrease in LSA \ndimensionality can produce a strong performance reduction in specific tasks. \nFrom a cognitive-modeling point of view, we point out that LSA's word-knowledge \nacquisitions may not be efficiently exploiting higher-order co-occurrences and \nglobal relations, whereas Word2vec does. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035245850c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10054"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "James M. Foster, Matt Jones", "title": "Reinforcement Learning with Analogical Similarity to Guide Schema Induction and Attention. (arXiv:1712.10070v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.10070", "type": "text/html"}], "timestampUsec": "1514784910796483", "comments": [], "summary": {"content": "<p>Research in analogical reasoning suggests that higher-order cognitive \nfunctions such as abstract reasoning, far transfer, and creativity are founded \non recognizing structural similarities among relational systems. Here we \nintegrate theories of analogy with the computational framework of reinforcement \nlearning (RL). We propose a psychology theory that is a computational synergy \nbetween analogy and RL, in which analogical comparison provides the RL learning \nalgorithm with a measure of relational similarity, and RL provides feedback \nsignals that can drive analogical learning. Simulation results support the \npower of this approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352458513", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10070"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yoseob Han, Jawook Gu, Jong Chul Ye", "title": "Deep Learning Interior Tomography for Region-of-Interest Reconstruction. (arXiv:1712.10248v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.10248", "type": "text/html"}], "timestampUsec": "1514784910796482", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329925012\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329925012&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Interior tomography for the region-of-interest (ROI) imaging has advantages \nof using a small detector and reducing X-ray radiation dose. However, standard \nanalytic reconstruction suffers from severe cupping artifacts due to existence \nof null space in the truncated Radon transform. Existing penalized \nreconstruction methods may address this problem but they require extensive \ncomputations due to the iterative reconstruction. Inspired by the recent deep \nlearning approaches to low-dose and sparse view CT, here we propose a deep \nlearning architecture that removes null space signals from the FBP \nreconstruction. Experimental results have shown that the proposed method \nprovides near-perfect reconstruction with about 7-10 dB improvement in PSNR \nover existing methods in spite of significantly reduced run-time complexity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352458516", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10248"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Hongbo Jia", "title": "First Draft on the xInf Model for Universal Physical Computation and Reverse Engineering of Natural Intelligence. (arXiv:1712.10280v1 [q-bio.NC])", "alternate": [{"href": "http://arxiv.org/abs/1712.10280", "type": "text/html"}], "timestampUsec": "1514784910796481", "comments": [], "summary": {"content": "<p>Turing Machines are universal computing machines in theory. It has been a \nlong debate whether Turing Machines can simulate the consciousness mind \nbehaviors in the materialistic universe. Three different hypotheses come out of \nsuch debate, in short:(A) Can; (B) Cannot; (C) Super-Turing machines can. \nBecause Turing Machines or other kinds of theoretical computing models are \nabstract objects while behaviors are real observables, this debate involves at \nleast three distinct fields of science and technology: physics, computer \nengineering, and experimental neuroscience. However, the languages used in \nthese different fields are highly heterogeneous and not easily interpretable \nfor each other, making it very difficult to reach partial agreements regarding \nthis debate, Therefore, the main goal of this manuscript is to establish a \nproper language that can translate among those different fields. First, I \npropose a theoretical model for analyzing how theoretical computing machines \nwould physically run in physical time. This model, termed as the xInf, is at \nfirst place Turing-complete in theory, and depending on the properties of \nphysical time, it can be either Turing-equivalent or Super-Turing in the \nphysical universe. The xInf Model is demonstrated to be a suitable universal \nlanguage to translate among physics, computer engineering, and neuroscience. \nFinally, I propose a conjecture that there exists a Minimal Complete Set of \nrules in the xInf Model that enables the construction of a physical machine \nusing inorganic materials that can pass the Turing Test in physical time. I \ncannot demonstrate whether such a conjecture to be testified or falsified on \npaper using finite-order logic, my only solution is physical time itself, i.e. \nan evolutionary competition will eventually tell the conclusion. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035245851d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10280"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Chao Ning, Fengqi You", "title": "Data-Driven Stochastic Robust Optimization: A General Computational Framework and Algorithm for Optimization under Uncertainty in the Big Data Era. (arXiv:1707.09198v4 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.09198", "type": "text/html"}], "timestampUsec": "1514784910796480", "comments": [], "summary": {"content": "<p>A novel data-driven stochastic robust optimization (DDSRO) framework is \nproposed for optimization under uncertainty leveraging labeled multi-class \nuncertainty data. Uncertainty data in large datasets are often collected from \nvarious conditions, which are encoded by class labels. Machine learning methods \nincluding Dirichlet process mixture model and maximum likelihood estimation are \nemployed for uncertainty modeling. A DDSRO framework is further proposed based \non the data-driven uncertainty model through a bi-level optimization structure. \nThe outer optimization problem follows a two-stage stochastic programming \napproach to optimize the expected objective across different data classes; \nadaptive robust optimization is nested as the inner problem to ensure the \nrobustness of the solution while maintaining computational tractability. A \ndecomposition-based algorithm is further developed to solve the resulting \nmulti-level optimization problem efficiently. Case studies on process network \ndesign and planning are presented to demonstrate the applicability of the \nproposed framework and algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352458525", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.09198"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Wengong Jin, Connor W. Coley, Regina Barzilay, Tommi Jaakkola", "title": "Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network. (arXiv:1709.04555v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04555", "type": "text/html"}], "timestampUsec": "1514784910796479", "comments": [], "summary": {"content": "<p>The prediction of organic reaction outcomes is a fundamental problem in \ncomputational chemistry. Since a reaction may involve hundreds of atoms, fully \nexploring the space of possible transformations is intractable. The current \nsolution utilizes reaction templates to limit the space, but it suffers from \ncoverage and efficiency issues. In this paper, we propose a template-free \napproach to efficiently explore the space of product molecules by first \npinpointing the reaction center -- the set of nodes and edges where graph edits \noccur. Since only a small number of atoms contribute to reaction center, we can \ndirectly enumerate candidate products. The generated candidates are scored by a \nWeisfeiler-Lehman Difference Network that models high-order interactions \nbetween changes occurring at nodes across the molecule. Our framework \noutperforms the top-performing template-based approach with a 10\\% margin, \nwhile running orders of magnitude faster. Finally, we demonstrate that the \nmodel accuracy rivals the performance of domain experts. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035245852b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04555"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ron Amit, Ron Meir", "title": "Lifelong Learning by Adjusting Priors. (arXiv:1711.01244v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.01244", "type": "text/html"}], "timestampUsec": "1514784910796478", "comments": [], "summary": {"content": "<p>In representational lifelong learning an agent aims to learn to solve novel \ntasks while updating its representation in light of previous tasks. Under the \nassumption that future tasks are `related' to previous tasks, representations \nshould be learned in such a way that they capture the common structure across \nlearned tasks, while allowing the learner sufficient flexibility to adapt to \nnovel aspects of a new task. We develop a framework for lifelong learning in \ndeep neural networks that is based on generalization bounds, developed within \nthe PAC-Bayes framework. Learning takes place through the construction of a \ndistribution over networks based on the tasks seen so far, and its utilization \nfor learning a new task. Thus, prior knowledge is incorporated through setting \na history-dependent prior for novel tasks. We develop a gradient-based \nalgorithm implementing these ideas, based on minimizing an objective function \nmotivated by generalization bounds, and demonstrate its effectiveness through \nnumerical examples. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352458531", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.01244"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang, William Yang Wang", "title": "Video Captioning via Hierarchical Reinforcement Learning. (arXiv:1711.11135v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.11135", "type": "text/html"}], "timestampUsec": "1514784910796477", "comments": [], "summary": {"content": "<p>Video captioning is the task of automatically generating a textual \ndescription of the actions in a video. Although previous work (e.g. \nsequence-to-sequence model) has shown promising results in abstracting a coarse \ndescription of a short video, it is still very challenging to caption a video \ncontaining multiple fine-grained actions with a detailed description. This \npaper aims to address the challenge by proposing a novel hierarchical \nreinforcement learning framework for video captioning, where a high-level \nManager module learns to design sub-goals and a low-level Worker module \nrecognizes the primitive actions to fulfill the sub-goal. With this \ncompositional framework to reinforce video captioning at different levels, our \napproach significantly outperforms all the baseline methods on a newly \nintroduced large-scale dataset for fine-grained video captioning. Furthermore, \nour non-ensemble model has already achieved the state-of-the-art results on the \nwidely-used MSR-VTT dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352458537", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11135"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jiren Jin, Hideki Nakayama", "title": "Annotation Order Matters: Recurrent Image Annotator for Arbitrary Length Image Tagging. (arXiv:1604.05225v3 [cs.CV] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1604.05225", "type": "text/html"}], "timestampUsec": "1514784910796475", "comments": [], "summary": {"content": "<p>Automatic image annotation has been an important research topic in \nfacilitating large scale image management and retrieval. Existing methods focus \non learning image-tag correlation or correlation between tags to improve \nannotation accuracy. However, most of these methods evaluate their performance \nusing top-k retrieval performance, where k is fixed. Although such setting \ngives convenience for comparing different methods, it is not the natural way \nthat humans annotate images. The number of annotated tags should depend on \nimage contents. Inspired by the recent progress in machine translation and \nimage captioning, we propose a novel Recurrent Image Annotator (RIA) model that \nforms image annotation task as a sequence generation problem so that RIA can \nnatively predict the proper length of tags according to image contents. We \nevaluate the proposed model on various image annotation datasets. In addition \nto comparing our model with existing methods using the conventional top-k \nevaluation measures, we also provide our model as a high quality baseline for \nthe arbitrary length image tagging task. Moreover, the results of our \nexperiments show that the order of tags in training phase has a great impact on \nthe final annotation performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035245853f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1604.05225"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Vira Semenova", "title": "Machine Learning for Partial Identification: Example of Bracketed Data. (arXiv:1712.10024v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.10024", "type": "text/html"}], "timestampUsec": "1514784910796474", "comments": [], "summary": {"content": "<p>Partially identified models occur commonly in economic applications. A common \nproblem in this literature is a regression problem with bracketed \n(interval-censored) outcome variable Y, which creates a set-identified \nparameter of interest. The recent studies have only considered \nfinite-dimensional linear regression in such context. To incorporate more \ncomplex controls into the problem, we consider a partially linear projection of \nY on the set functions that are linear in treatment/policy variables and \nnonlinear in the controls. We characterize the identified set for the linear \ncomponent of this projection and propose an estimator of its support function. \nOur estimator converges at parametric rate and has asymptotic normality \nproperties. It may be useful for labor economics applications that involve \nbracketed salaries and rich, high-dimensional demographic data about the \nsubjects of the study. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352458546", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10024"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "W. D. Brinda, Jason M. Klusowski", "title": "Finite-sample risk bounds for maximum likelihood estimation with arbitrary penalties. (arXiv:1712.10087v1 [math.ST])", "alternate": [{"href": "http://arxiv.org/abs/1712.10087", "type": "text/html"}], "timestampUsec": "1514784910796473", "comments": [], "summary": {"content": "<p>The MDL two-part coding $ \\textit{index of resolvability} $ provides a \nfinite-sample upper bound on the statistical risk of penalized likelihood \nestimators over countable models. However, the bound does not apply to \nunpenalized maximum likelihood estimation or procedures with exceedingly small \npenalties. In this paper, we point out a more general inequality that holds for \narbitrary penalties. In addition, this approach makes it possible to derive \nexact risk bounds of order $1/n$ for iid parametric models, which improves on \nthe order $(\\log n)/n$ resolvability bounds. We conclude by discussing \nimplications for adaptive estimation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035245854a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10087"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Su Yan, Wei Lin, Tianshu Wu, Daorui Xiao, Kaipeng Liu, Bo Wu", "title": "Beyond Keywords and Relevance: A Personalized Ad Retrieval Framework in E-Commerce Sponsored Search. (arXiv:1712.10110v1 [cs.IR])", "alternate": [{"href": "http://arxiv.org/abs/1712.10110", "type": "text/html"}], "timestampUsec": "1514784910796472", "comments": [], "summary": {"content": "<p>On most sponsored search platforms, advertisers bid on some keywords for \ntheir advertisements (ads). Given a search request, ad retrieval module \nrewrites the query into bidding keywords, and uses these keywords as keys to \nselect Top N ads through inverted indexes. In this way, an ad will not be \nretrieved even if queries are related when the advertiser does not bid on \ncorresponding keywords. Moreover, most ad retrieval approaches regard rewriting \nand ad-selecting as two separated tasks, and focus on boosting relevance \nbetween search queries and ads. Recently, in e-commerce sponsored search more \nand more personalized information has been introduced, such as user profiles, \nlong-time and real-time clicks. Personalized information makes ad retrieval \nable to employ more elements (e.g. real-time clicks) as search signals and \nretrieval keys, however it makes ad retrieval more difficult to measure ads \nretrieved through different signals. To address these problems, we propose a \nnovel ad retrieval framework beyond keywords and relevance in e-commerce \nsponsored search. Firstly, we employ historical ad click data to initialize a \nhierarchical network representing signals, keys and ads, in which personalized \ninformation is introduced. Then we train a model on top of the hierarchical \nnetwork by learning the weights of edges. Finally we select the best edges \naccording to the model, boosting RPM/CTR. Experimental results on our \ne-commerce platform demonstrate that our ad retrieval framework achieves good \nperformance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035245854f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10110"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Frank E. Curtis, Katya Scheinberg, Rui Shi", "title": "A Stochastic Trust Region Algorithm. (arXiv:1712.10277v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1712.10277", "type": "text/html"}], "timestampUsec": "1514784910796471", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32992530d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32992530d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>An algorithm is proposed for solving stochastic and finite sum minimization \nproblems. Based on a trust region methodology, the algorithm employs normalized \nsteps, at least as long as the norms of the stochastic gradient estimates are \nwithin a user-defined interval. The complete algorithm---which dynamically \nchooses whether or not to employ normalized steps---is proved to have \nconvergence guarantees that are similar to those possessed by a traditional \nstochastic gradient approach under various sets of conditions related to the \naccuracy of the stochastic gradient estimates and choice of stepsize sequence. \nThe results of numerical experiments are presented when the method is employed \nto minimize convex and nonconvex machine learning test problems, illustrating \nthat the method can outperform a traditional stochastic gradient approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352458552", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10277"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Cencheng Shen, Li Chen, Yuexiao Dong, Carey E. Priebe", "title": "Sparse Representation Classification Beyond L1 Minimization and the Subspace Assumption. (arXiv:1502.01368v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1502.01368", "type": "text/html"}], "timestampUsec": "1514784910796470", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32997918c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32997918c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The sparse representation classifier (SRC) has been utilized in various \nclassification problems, which makes use of L1 minimization and is shown to \nwork well for image recognition problems that satisfy a subspace assumption. In \nthis paper we propose a new implementation of SRC via screening, establish its \nequivalence to the original SRC under regularity conditions, and prove its \nclassification consistency under a latent subspace model. The results are \ndemonstrated via simulations and real data experiments, where the new algorithm \nachieves comparable numerical performance but significantly faster. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352458556", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1502.01368"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jian Du, Shaodan Ma, Yik-Chung Wu, Soummya Kar, Jos&#xe9; M. F. Moura", "title": "Convergence Analysis of Distributed Inference with Vector-Valued Gaussian Belief Propagation. (arXiv:1611.02010v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.02010", "type": "text/html"}], "timestampUsec": "1514784910796469", "comments": [], "summary": {"content": "<p>This paper considers inference over distributed linear Gaussian models using \nfactor graphs and Gaussian belief propagation (BP). The distributed inference \nalgorithm involves only local computation of the information matrix and of the \nmean vector, and message passing between neighbors. Under broad conditions, it \nis shown that the message information matrix converges to a unique positive \ndefinite limit matrix for arbitrary positive semidefinite initialization, and \nit approaches an arbitrarily small neighborhood of this limit matrix at a \ndoubly exponential rate. A necessary and sufficient convergence condition for \nthe belief mean vector to converge to the optimal centralized estimator is \nprovided under the assumption that the message information matrix is \ninitialized as a positive semidefinite matrix. Further, it is shown that \nGaussian BP always converges when the underlying factor graph is given by the \nunion of a forest and a single loop. The proposed convergence condition in the \nsetup of distributed linear Gaussian models is shown to be strictly weaker than \nother existing convergence conditions and requirements, including the Gaussian \nMarkov random field based walk-summability condition, and applicable to a large \nclass of scenarios. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035245855b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.02010"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ishan Jindal, Matthew Nokleby", "title": "Classification and Representation via Separable Subspaces: Performance Limits and Algorithms. (arXiv:1705.02556v2 [cs.IT] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.02556", "type": "text/html"}], "timestampUsec": "1514784910796468", "comments": [], "summary": {"content": "<p>We study the classification performance of Kronecker-structured models in two \nasymptotic regimes and developed an algorithm for separable, fast and compact \nK-S dictionary learning for better classification and representation of \nmultidimensional signals by exploiting the structure in the signal. First, we \nstudy the classification performance in terms of diversity order and pairwise \ngeometry of the subspaces. We derive an exact expression for the diversity \norder as a function of the signal and subspace dimensions of a K-S model. Next, \nwe study the classification capacity, the maximum rate at which the number of \nclasses can grow as the signal dimension goes to infinity. Then we describe a \nfast algorithm for Kronecker-Structured Learning of Discriminative Dictionaries \n(K-SLD2). Finally, we evaluate the empirical classification performance of K-S \nmodels for the synthetic data, showing that they agree with the diversity order \nanalysis. We also evaluate the performance of K-SLD2 on synthetic and \nreal-world datasets showing that the K-SLD2 balances compact signal \nrepresentation and good classification performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514784910796", "annotations": [], "published": 1514784911, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352458563", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.02556"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aditya Gilra, Wulfram Gerstner", "title": "Non-linear motor control by local learning in spiking neural networks. (arXiv:1712.10158v1 [q-bio.NC])", "alternate": [{"href": "http://arxiv.org/abs/1712.10158", "type": "text/html"}], "timestampUsec": "1514782946958137", "comments": [], "summary": {"content": "<p>Learning weights in a spiking neural network with hidden neurons, using \nlocal, stable and online rules, to control non-linear body dynamics is an open \nproblem. Here, we employ a supervised scheme, Feedback-based Online Local \nLearning Of Weights (FOLLOW), to train a network of heterogeneous spiking \nneurons with hidden layers, to control a two-link arm so as to reproduce a \ndesired state trajectory. The network first learns an inverse model of the \nnon-linear dynamics, i.e. from state trajectory as input to the network, it \nlearns to infer the continuous-time command that produced the trajectory. \nConnection weights are adjusted via a local plasticity rule that involves \npre-synaptic firing and post-synaptic feedback of the error in the inferred \ncommand. We choose a network architecture, termed differential feedforward, \nthat gives the lowest test error from different feedforward and recurrent \narchitectures. The learned inverse model is then used to generate a \ncontinuous-time motor command to control the arm, given a desired trajectory. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514782946958", "annotations": [], "published": 1514782947, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352430d0f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10158"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anqi Liu, Rizal Fathony, Brian D. Ziebart", "title": "Kernel Robust Bias-Aware Prediction under Covariate Shift. (arXiv:1712.10050v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.10050", "type": "text/html"}], "timestampUsec": "1514782946958136", "comments": [], "summary": {"content": "<p>Under covariate shift, training (source) data and testing (target) data \ndiffer in input space distribution, but share the same conditional label \ndistribution. This poses a challenging machine learning task. Robust Bias-Aware \n(RBA) prediction provides the conditional label distribution that is robust to \nthe worstcase logarithmic loss for the target distribution while matching \nfeature expectation constraints from the source distribution. However, \nemploying RBA with insufficient feature constraints may result in high \ncertainty predictions for much of the source data, while leaving too much \nuncertainty for target data predictions. To overcome this issue, we extend the \nrepresenter theorem to the RBA setting, enabling minimization of regularized \nexpected target risk by a reweighted kernel expectation under the source \ndistribution. By applying kernel methods, we establish consistency guarantees \nand demonstrate better performance of the RBA classifier than competing methods \non synthetically biased UCI datasets as well as datasets that have natural \ncovariate shift. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514782946958", "annotations": [], "published": 1514782947, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352430d12", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10050"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Juan J. Merelo-Guerv&#xf3;s, Antonio Fern&#xe1;ndez-Ares, Antonio &#xc1;lvarez Caballero, Pablo Garc&#xed;a-S&#xe1;nchez, Victor Rivas", "title": "RedDwarfData: a simplified dataset of StarCraft matches. (arXiv:1712.10179v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.10179", "type": "text/html"}], "timestampUsec": "1514782946958135", "comments": [], "summary": {"content": "<p>The game Starcraft is one of the most interesting arenas to test new machine \nlearning and computational intelligence techniques; however, StarCraft matches \ntake a long time and creating a good dataset for training can be hard. Besides, \nanalyzing match logs to extract the main characteristics can also be done in \nmany different ways to the point that extracting and processing data itself can \ntake an inordinate amount of time and of course, depending on what you choose, \ncan bias learning algorithms. In this paper we present a simplified dataset \nextracted from the set of matches published by Robinson and Watson, which we \nhave called RedDwarfData, containing several thousand matches processed to \nframes, so that temporal studies can also be undertaken. This dataset is \navailable from GitHub under a free license. An initial analysis and appraisal \nof these matches is also made. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514782946958", "annotations": [], "published": 1514782947, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352430d15", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10179"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jiren Jin, Richard G. Calland, Takeru Miyato, Brian K. Vogel, Hideki Nakayama", "title": "Parameter Reference Loss for Unsupervised Domain Adaptation. (arXiv:1711.07170v2 [cs.CV] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07170", "type": "text/html"}], "timestampUsec": "1514782946958134", "comments": [], "summary": {"content": "<p>The success of deep learning in computer vision is mainly attributed to an \nabundance of data. However, collecting large-scale data is not always possible, \nespecially for the supervised labels. Unsupervised domain adaptation (UDA) aims \nto utilize labeled data from a source domain to learn a model that generalizes \nto a target domain of unlabeled data. A large amount of existing work uses \nSiamese network-based models, where two streams of neural networks process the \nsource and the target domain data respectively. Nevertheless, most of these \napproaches focus on minimizing the domain discrepancy, overlooking the \nimportance of preserving the discriminative ability for target domain features. \nAnother important problem in UDA research is how to evaluate the methods \nproperly. Common evaluation procedures require target domain labels for \nhyper-parameter tuning and model selection, contradicting the definition of the \nUDA task. Hence we propose a more reasonable evaluation principle that avoids \nthis contradiction by simply adopting the latest snapshot of a model for \nevaluation. This adds an extra requirement for UDA methods besides the main \nperformance criteria: the stability during training. We design a novel method \nthat connects the target domain stream to the source domain stream with a \nParameter Reference Loss (PRL) to solve these problems simultaneously. \nExperiments on various datasets show that the proposed PRL not only improves \nthe performance on the target domain, but also stabilizes the training \nprocedure. As a result, PRL based models do not need the contradictory model \nselection, and thus are more suitable for practical applications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514782946958", "annotations": [], "published": 1514782947, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352430d1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07170"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yao Zhang, Woong-Je Sung, Dimitri Mavris", "title": "Application of Convolutional Neural Network to Predict Airfoil Lift Coefficient. (arXiv:1712.10082v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.10082", "type": "text/html"}], "timestampUsec": "1514782946958133", "comments": [], "summary": {"content": "<p>The adaptability of the convolutional neural network (CNN) technique for \naerodynamic meta-modeling tasks is probed in this work. The primary objective \nis to develop suitable CNN architecture for variable flow conditions and object \ngeometry, in addition to identifying a sufficient data preparation process. \nMultiple CNN structures were trained to learn the lift coefficients of the \nairfoils with a variety of shapes in multiple flow Mach numbers, Reynolds \nnumbers, and diverse angles of attack. This is conducted to illustrate the \nconcept of the technique. A multi-layered perceptron (MLP) is also used for the \ntraining sets. The MLP results are compared with that of the CNN results. The \nnewly proposed meta-modeling concept has been found to be comparable with the \nMLP in learning capability; and more importantly, our CNN model exhibits a \ncompetitive prediction accuracy with minimal constraints in a geometric \nrepresentation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514782946958", "annotations": [], "published": 1514782947, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352430d20", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10082"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Saeedeh Ziyabari, Vinit Shah, Meysam Golmohammadi, Iyad Obeid, Joseph Picone", "title": "Objective evaluation metrics for automatic classification of EEG events. (arXiv:1712.10107v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.10107", "type": "text/html"}], "timestampUsec": "1514782946958132", "comments": [], "summary": {"content": "<p>The evaluation of machine learning algorithms in biomedical fields for \napplications involving sequential data lacks standardization. Common \nquantitative scalar evaluation metrics such as sensitivity and specificity can \noften be misleading depending on the requirements of the application. \nEvaluation metrics must ultimately reflect the needs of users yet be \nsufficiently sensitive to guide algorithm development. Feedback from critical \ncare clinicians who use automated event detection software in clinical \napplications has been overwhelmingly emphatic that a low false alarm rate, \ntypically measured in units of the number of errors per 24 hours, is the single \nmost important criterion for user acceptance. Though using a single metric is \nnot often as insightful as examining performance over a range of operating \nconditions, there is a need for a single scalar figure of merit. In this paper, \nwe discuss the deficiencies of existing metrics for a seizure detection task \nand propose several new metrics that offer a more balanced view of performance. \nWe demonstrate these metrics on a seizure detection task based on the TUH EEG \nCorpus. We show that two promising metrics are a measure based on a concept \nborrowed from the spoken term detection literature, Actual Term-Weighted Value, \nand a new metric, Time-Aligned Event Scoring (TAES), that accounts for the \ntemporal alignment of the hypothesis to the reference annotation. We also \ndemonstrate that state of the art technology based on deep learning, though \nimpressive in its performance, still needs significant improvement before it \nwill meet very strict user acceptance guidelines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514782946958", "annotations": [], "published": 1514782947, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352430d24", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10107"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Michela Paganini, Luke de Oliveira, Benjamin Nachman", "title": "CaloGAN: Simulating 3D High Energy Particle Showers in Multi-Layer Electromagnetic Calorimeters with Generative Adversarial Networks. (arXiv:1712.10321v1 [hep-ex])", "alternate": [{"href": "http://arxiv.org/abs/1712.10321", "type": "text/html"}], "timestampUsec": "1514782946958131", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3299794f6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3299794f6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The precise modeling of subatomic particle interactions and propagation \nthrough matter is paramount for the advancement of nuclear and particle physics \nsearches and precision measurements. The most computationally expensive step in \nthe simulation pipeline of a typical experiment at the Large Hadron Collider \n(LHC) is the detailed modeling of the full complexity of physics processes that \ngovern the motion and evolution of particle showers inside calorimeters. We \nintroduce \\textsc{CaloGAN}, a new fast simulation technique based on generative \nadversarial networks (GANs). We apply these neural networks to the modeling of \nelectromagnetic showers in a longitudinally segmented calorimeter, and achieve \nspeedup factors comparable to or better than existing full simulation \ntechniques on CPU ($100\\times$-$1000\\times$) and even faster on GPU (up to \n$\\sim10^5\\times$). There are still challenges for achieving precision across \nthe entire phase space, but our solution can reproduce a variety of geometric \nshower shape properties of photons, positrons and charged pions. This \nrepresents a significant stepping stone toward a full neural network-based \ndetector simulation that could save significant computing time and enable many \nanalyses now and in the future. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514782946958", "annotations": [], "published": 1514782947, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352430d2c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10321"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra", "title": "Matching Networks for One Shot Learning. (arXiv:1606.04080v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1606.04080", "type": "text/html"}], "timestampUsec": "1514782946958130", "comments": [], "summary": {"content": "<p>Learning from a few examples remains a key challenge in machine learning. \nDespite recent advances in important domains such as vision and language, the \nstandard supervised deep learning paradigm does not offer a satisfactory \nsolution for learning new concepts rapidly from little data. In this work, we \nemploy ideas from metric learning based on deep neural features and from recent \nadvances that augment neural networks with external memories. Our framework \nlearns a network that maps a small labelled support set and an unlabelled \nexample to its label, obviating the need for fine-tuning to adapt to new class \ntypes. We then define one-shot learning problems on vision (using Omniglot, \nImageNet) and language tasks. Our algorithm improves one-shot accuracy on \nImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to \ncompeting approaches. We also demonstrate the usefulness of the same model on \nlanguage modeling by introducing a one-shot task on the Penn Treebank. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514782946958", "annotations": [], "published": 1514782947, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000352430d2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1606.04080"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Subhash Kak", "title": "Learning Based on CC1 and CC4 Neural Networks. (arXiv:1712.09331v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.09331", "type": "text/html"}], "timestampUsec": "1514527610846356", "comments": [], "summary": {"content": "<p>We propose that a general learning system should have three kinds of agents \ncorresponding to sensory, short-term, and long-term memory that implicitly will \nfacilitate context-free and context-sensitive aspects of learning. These three \nagents perform mututally complementary functions that capture aspects of the \nhuman cognition system. We investigate the use of CC1 and CC4 networks for use \nas models of short-term and sensory memory. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbafe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09331"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1515523577, "author": "Qiangeng Xu, John Kender", "title": "Report: Dynamic Eye Movement Matching and Visualization Tool in Neuro Gesture. (arXiv:1712.09709v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.09709", "type": "text/html"}], "timestampUsec": "1514527610846355", "comments": [], "summary": {"content": "<p>In the research of the impact of gestures using by a lecturer, one \nchallenging task is to infer the attention of a group of audiences. Two \nimportant measurements that can help infer the level of attention are eye \nmovement data and Electroencephalography (EEG) data. Under the fundamental \nassumption that a group of people would look at the same place if they all pay \nattention at the same time, we apply a method, \"Time Warp Edit Distance\", to \ncalculate the similarity of their eye movement trajectories. Moreover, we also \ncluster eye movement pattern of audiences based on these pair-wised similarity \nmetrics. Besides, since we don't have a direct metric for the \"attention\" \nground truth, a visual assessment would be beneficial to evaluate the \ngesture-attention relationship. Thus we also implement a visualization tool. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1515523577, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbb06", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09709"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Anton V. Eremeev", "title": "Evolutionary algorithms. (arXiv:1511.06987v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1511.06987", "type": "text/html"}], "timestampUsec": "1514527610846354", "comments": [], "summary": {"content": "<p>This manuscript contains an outline of lectures course \"Evolutionary \nAlgorithms\" read by the author in Omsk State University n.a. F.M.Dostoevsky. \nThe course covers Canonic Genetic Algorithm and various other genetic \nalgorithms as well as evolutionary strategies, genetic programming, tabu search \nand the class of evolutionary algorithms in general. Some facts, such as the \nRotation Property of crossover, the Schemata Theorem, GA performance as a local \nsearch and \"almost surely\" convergence of evolutionary algorithms are given \nwith complete proofs. The text is in Russian. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbb0b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1511.06987"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Vahid Behzadan, Arslan Munir", "title": "Whatever Does Not Kill Deep Reinforcement Learning, Makes It Stronger. (arXiv:1712.09344v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.09344", "type": "text/html"}], "timestampUsec": "1514527610846353", "comments": [], "summary": {"content": "<p>Recent developments have established the vulnerability of deep Reinforcement \nLearning (RL) to policy manipulation attacks via adversarial perturbations. In \nthis paper, we investigate the robustness and resilience of deep RL to \ntraining-time and test-time attacks. Through experimental results, we \ndemonstrate that under noncontiguous training-time attacks, Deep Q-Network \n(DQN) agents can recover and adapt to the adversarial conditions by reactively \nadjusting the policy. Our results also show that policies learned under \nadversarial perturbations are more robust to test-time attacks. Furthermore, we \ncompare the performance of $\\epsilon$-greedy and parameter-space noise \nexploration methods in terms of robustness and resilience against adversarial \nperturbations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbb19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09344"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ming Zhu, Xiao-Yang Liu, Xiaodong Wang", "title": "An Online Ride-Sharing Path Planning Strategy for Public Vehicle Systems. (arXiv:1712.09356v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.09356", "type": "text/html"}], "timestampUsec": "1514527610846352", "comments": [], "summary": {"content": "<p>As efficient traffic-management platforms, public vehicle (PV) systems are \nenvisioned to be a promising approach to solving traffic congestions and \npollutions for future smart cities. PV systems provide online/dynamic \npeer-to-peer ride-sharing services with the goal of serving sufficient number \nof customers with minimum number of vehicles and lowest possible cost. A key \ncomponent of the PV system is the online ride-sharing scheduling strategy. In \nthis paper, we propose an efficient path planning strategy that focuses on a \nlimited potential search area for each vehicle by filtering out the requests \nthat violate passenger service quality level, so that the global search is \nreduced to local search. We analyze the performance of the proposed solution \nsuch as reduction ratio of computational complexity. Simulations based on the \nManhattan taxi data set show that, the computing time is reduced by 22% \ncompared with the exhaustive search method under the same service quality \nperformance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbb24", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09356"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Hang Zhao, Zhicheng Yan, Heng Wang, Lorenzo Torresani, Antonio Torralba", "title": "SLAC: A Sparsely Labeled Dataset for Action Classification and Localization. (arXiv:1712.09374v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.09374", "type": "text/html"}], "timestampUsec": "1514527610846351", "comments": [], "summary": {"content": "<p>This paper describes a procedure for the creation of large-scale video \ndatasets for action classification and localization from unconstrained, \nrealistic web data. The scalability of the proposed procedure is demonstrated \nby building a novel video benchmark, named SLAC (Sparsely Labeled ACtions), \nconsisting of over 520K untrimmed videos and 1.75M clip annotations spanning \n200 action categories. Using our proposed framework, annotating a clip takes \nmerely 8.8 seconds on average. This represents a saving in labeling time of \nover 95% compared to the traditional procedure of manual trimming and \nlocalization of actions. Our approach dramatically reduces the amount of human \nlabeling by automatically identifying hard clips, i.e., clips that contain \ncoherent actions but lead to prediction disagreement between action \nclassifiers. A human annotator can disambiguate whether such a clip truly \ncontains the hypothesized action in a handful of seconds, thus generating \nlabels for highly informative samples at little cost. We show that our \nlarge-scale dataset can be used to effectively pre-train action recognition \nmodels, significantly improving final metrics on smaller-scale benchmarks after \nfine-tuning. On Kinetics, UCF-101 and HMDB-51, models pre-trained on SLAC \noutperform baselines trained from scratch, by 2.0%, 20.1% and 35.4% in top-1 \naccuracy, respectively when RGB input is used. Furthermore, we introduce a \nsimple procedure that leverages the sparse labels in SLAC to pre-train action \nlocalization models. On THUMOS14 and ActivityNet-v1.3, our localization model \nimproves the mAP of baseline model by 8.6% and 2.5%, respectively. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbb2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09374"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1515649358, "author": "Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Joseph Gonzalez, Ken Goldberg, Ion Stoica", "title": "Ray RLlib: A Composable and Scalable Reinforcement Learning Library. (arXiv:1712.09381v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.09381", "type": "text/html"}], "timestampUsec": "1514527610846350", "comments": [], "summary": {"content": "<p>Reinforcement learning (RL) algorithms involve the deep nesting of distinct \ncomponents, where each component typically exhibits opportunities for \ndistributed computation. Current RL libraries offer parallelism at the level of \nthe entire program, coupling all the components together and making existing \nimplementations difficult to extend, combine, and reuse. We argue for building \ncomposable RL components by encapsulating parallelism and resource requirements \nwithin individual components, which can be achieved by building on top of a \nflexible task-based programming model. We demonstrate this principle by \nbuilding Ray RLlib on top of Ray and show that we can implement a wide range of \nstate-of-the-art algorithms by composing and reusing a handful of standard \ncomponents. This composability does not come at the cost of performance --- in \nour experiments, RLlib matches or exceeds the performance of highly optimized \nreference implementations. Ray RLlib is available as part of Ray at \nhttps://github.com/ray-project/ray/. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbb37", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09381"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "D J Strouse, David J Schwab", "title": "The information bottleneck and geometric clustering. (arXiv:1712.09657v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.09657", "type": "text/html"}], "timestampUsec": "1514527610846349", "comments": [], "summary": {"content": "<p>The information bottleneck (IB) approach to clustering takes a joint \ndistribution $P\\!\\left(X,Y\\right)$ and maps the data $X$ to cluster labels $T$ \nwhich retain maximal information about $Y$ (Tishby et al., 1999). This \nobjective results in an algorithm that clusters data points based upon the \nsimilarity of their conditional distributions $P\\!\\left(Y\\mid X\\right)$. This \nis in contrast to classic \"geometric clustering\" algorithms such as $k$-means \nand gaussian mixture models (GMMs) which take a set of observed data points \n$\\left\\{ \\mathbf{x}_{i}\\right\\}_{i=1:N}$ and cluster them based upon their \ngeometric (typically Euclidean) distance from one another. Here, we show how to \nuse the deterministic information bottleneck (DIB) (Strouse and Schwab, 2017), \na variant of IB, to perform geometric clustering, by choosing cluster labels \nthat preserve information about data point location on a smoothed dataset. We \nalso introduce a novel intuitive method to choose the number of clusters, via \nkinks in the information curve. We apply this approach to a variety of simple \nclustering problems, showing that DIB with our model selection procedure \nrecovers the generative cluster labels. We also show that, for one simple case, \nDIB interpolates between the cluster boundaries of GMMs and $k$-means in the \nlarge data limit. Thus, our IB approach to clustering also provides an \ninformation-theoretic perspective on these classic algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbb3c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09657"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pooyan Ehsani, Jia Yuan Yu", "title": "The Merits of Sharing a Ride. (arXiv:1712.10011v1 [cs.SI])", "alternate": [{"href": "http://arxiv.org/abs/1712.10011", "type": "text/html"}], "timestampUsec": "1514527610846348", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329979815\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329979815&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The culture of sharing instead of ownership is sharply increasing in \nindividuals behaviors. Particularly in transportation, concepts of sharing a \nride in either carpooling or ridesharing have been recently adopted. An \nefficient optimization approach to match passengers in real-time is the core of \nany ridesharing system. In this paper, we model ridesharing as an online \nmatching problem on general graphs such that passengers do not drive private \ncars and use shared taxis. We propose an optimization algorithm to solve it. \nThe outlined algorithm calculates the optimal waiting time when a passenger \narrives. This leads to a matching with minimal overall overheads while \nmaximizing the number of partnerships. To evaluate the behavior of our \nalgorithm, we used NYC taxi real-life data set. Results represent a substantial \nreduction in overall overheads. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbb40", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.10011"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Robert J. Rovetto", "title": "An Ontological Architecture for Orbital Debris Data. (arXiv:1704.01014v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.01014", "type": "text/html"}], "timestampUsec": "1514527610846347", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3299c6a8d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3299c6a8d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The orbital debris problem presents an opportunity for inter-agency and \ninternational cooperation toward the mutually beneficial goals of debris \nprevention, mitigation, remediation, and improved space situational awareness \n(SSA). Achieving these goals requires sharing orbital debris and other SSA \ndata. Toward this, I present an ontological architecture for the orbital debris \ndomain, taking steps in the creation of an orbital debris ontology (ODO). The \npurpose of this ontological system is to (I) represent general orbital debris \nand SSA domain knowledge, (II) structure, and standardize where needed, orbital \ndata and terminology, and (III) foster semantic interoperability and \ndata-sharing. In doing so I hope to (IV) contribute to solving the orbital \ndebris problem, improving peaceful global SSA, and ensuring safe space travel \nfor future generations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbb47", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.01014"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Roberto Rossi, &#xd6;zg&#xfc;r Akg&#xfc;n, Steven Prestwich, S. Armagan Tarim", "title": "Declarative Statistics. (arXiv:1708.01829v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.01829", "type": "text/html"}], "timestampUsec": "1514527610846346", "comments": [], "summary": {"content": "<p>In this work we introduce declarative statistics, a suite of declarative \nmodelling tools for statistical analysis. Statistical constraints represent the \nkey building block of declarative statistics. First, we introduce a range of \nrelevant counting and matrix constraints and associated decompositions, some of \nwhich novel, that are instrumental in the design of statistical constraints. \nSecond, we introduce a selection of novel statistical constraints and \nassociated decompositions, which constitute a self-contained toolbox that can \nbe used to tackle a wide range of problems typically encountered by \nstatisticians. Finally, we deploy these statistical constraints to a wide range \nof application areas drawn from classical statistics and we contrast our \nframework against established practices. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbb5f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.01829"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Arbaaz Khan, Clark Zhang, Nikolay Atanasov, Konstantinos Karydis, Vijay Kumar, Daniel D. Lee", "title": "Memory Augmented Control Networks. (arXiv:1709.05706v4 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.05706", "type": "text/html"}], "timestampUsec": "1514527610846345", "comments": [], "summary": {"content": "<p>Planning problems in partially observable environments cannot be solved \ndirectly with convolutional networks and require some form of memory. But, even \nmemory networks with sophisticated addressing schemes are unable to learn \nintelligent reasoning satisfactorily due to the complexity of simultaneously \nlearning to access memory and plan. To mitigate these challenges we introduce \nthe Memory Augmented Control Network (MACN). The proposed network architecture \nconsists of three main parts. The first part uses convolutions to extract \nfeatures and the second part uses a neural network-based planning module to \npre-plan in the environment. The third part uses a network controller that \nlearns to store those specific instances of past information that are necessary \nfor planning. The performance of the network is evaluated in discrete grid \nworld environments for path planning in the presence of simple and complex \nobstacles. We show that our network learns to plan and can generalize to new \nenvironments. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbbbc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.05706"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zachary Sunberg, Mykel Kochenderfer", "title": "POMCPOW: An online algorithm for POMDPs with continuous state, action, and observation spaces. (arXiv:1709.06196v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.06196", "type": "text/html"}], "timestampUsec": "1514527610846344", "comments": [], "summary": {"content": "<p>Online solvers for partially observable Markov decision processes have been \napplied to problems with large discrete state spaces, but continuous state, \naction, and observation spaces remain a challenge. This paper begins by \ninvestigating double progressive widening (DPW) as a solution to this \nchallenge. However, we prove that this modification alone is not sufficient \nbecause the belief representations in the search tree collapse to a single \nparticle causing the algorithm to converge to a policy that is suboptimal \nregardless of the computation time. The main contribution of the paper is to \npropose a new algorithm, POMCPOW, that incorporates DPW and weighted particle \nfiltering to overcome this deficiency and attack continuous problems. \nSimulation results show that these modifications allow the algorithm to be \nsuccessful where previous approaches fail. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbbe5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.06196"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Nawal Benabbou, Mithun Chakraborty, Vinh Ho Xuan, Jakub Sliwinski, Yair Zick", "title": "Diversity Constraints in Public Housing Allocation. (arXiv:1711.10241v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10241", "type": "text/html"}], "timestampUsec": "1514527610846343", "comments": [], "summary": {"content": "<p>The state of Singapore operates a national public housing program, accounting \nfor over 80% of its residential real estate. Singapore uses its housing \nallocation program to ensure ethnic diversity in its neighborhoods; it does so \nby imposing ethnic quotas: every ethnic group must not own more than a certain \npercentage in a housing project, thus ensuring that every neighborhood contains \nmembers from each ethnic group. However, imposing diversity constraints \nnaturally results in some welfare loss. Our work studies the tradeoff between \ndiversity and social welfare from the perspective of computational economics. \nWe model the problem as an extension of the classic assignment problem, with \nadditional diversity constraints. While the classic assignment program is \npoly-time computable, we show that adding diversity constraints makes the \nproblem computationally intractable; however, we identify a \n$\\tfrac{1}{2}$-approximation algorithm, as well as reasonable agent utility \nmodels which admit poly-time algorithms. In addition, we study the price of \ndiversity: this is the loss in welfare incurred by imposing diversity \nconstraints; we provide upper bounds on the price of diversity as a function of \nnatural problem parameters; next, we analyze public data from Singapore's \nHousing and Development Board, and create a simulated framework testing the \nwelfare loss due to diversity constraints in realistic large-scale scenarios. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbc04", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10241"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Gintare Karolina Dziugaite, Daniel M. Roy", "title": "Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy. (arXiv:1712.09376v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.09376", "type": "text/html"}], "timestampUsec": "1514527610846342", "comments": [], "summary": {"content": "<p>We show that Entropy-SGD (Chaudhari et al., 2016), when viewed as a learning \nalgorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) \nclassifier, i.e., a randomized classifier obtained by a risk-sensitive \nperturbation of the weights of a learned classifier. Entropy-SGD works by \noptimizing the bound's prior, violating the hypothesis of the PAC-Bayes theorem \nthat the prior is chosen independently of the data. Indeed, available \nimplementations of Entropy-SGD rapidly obtain zero training error on random \nlabels and the same holds of the Gibbs posterior. In order to obtain a valid \ngeneralization bound, we show that an $\\epsilon$-differentially private prior \nyields a valid PAC-Bayes bound, a straightforward consequence of results \nconnecting generalization with differential privacy. Using stochastic gradient \nLangevin dynamics (SGLD) to approximate the well-known exponential release \nmechanism, we observe that generalization error on MNIST (measured on held out \ndata) falls within the (empirically nonvacuous) bounds computed under the \nassumption that SGLD produces perfect samples. In particular, Entropy-SGLD can \nbe configured to yield relatively tight generalization bounds and still fit \nreal labels, although these same settings do not obtain state-of-the-art \nperformance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbc11", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09376"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Rajiv Khanna, Anastasios Kyrillidis", "title": "IHT dies hard: Provable accelerated Iterative Hard Thresholding. (arXiv:1712.09379v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1712.09379", "type": "text/html"}], "timestampUsec": "1514527610846341", "comments": [], "summary": {"content": "<p>We study --both in theory and practice-- the use of momentum motions in \nclassic iterative hard thresholding (IHT) methods. By simply modifying plain \nIHT, we investigate its convergence behavior on convex optimization criteria \nwith non-convex constraints, under standard assumptions. In diverse scenaria, \nwe observe that acceleration in IHT leads to significant improvements, compared \nto state of the art projected gradient descent and Frank-Wolfe variants. As a \nbyproduct of our inspection, we study the impact of selecting the momentum \nparameter: similar to convex settings, two modes of behavior are observed \n--\"rippling\" and linear-- depending on the level of momentum. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbc1f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09379"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Huaian Diao, Zhao Song, Wen Sun, David P. Woodruff", "title": "Sketching for Kronecker Product Regression and P-splines. (arXiv:1712.09473v1 [cs.DS])", "alternate": [{"href": "http://arxiv.org/abs/1712.09473", "type": "text/html"}], "timestampUsec": "1514527610846340", "comments": [], "summary": {"content": "<p>TensorSketch is an oblivious linear sketch introduced in Pagh'13 and later \nused in Pham, Pagh'13 in the context of SVMs for polynomial kernels. It was \nshown in Avron, Nguyen, Woodruff'14 that TensorSketch provides a subspace \nembedding, and therefore can be used for canonical correlation analysis, low \nrank approximation, and principal component regression for the polynomial \nkernel. We take TensorSketch outside of the context of polynomials kernels, and \nshow its utility in applications in which the underlying design matrix is a \nKronecker product of smaller matrices. This allows us to solve Kronecker \nproduct regression and non-negative Kronecker product regression, as well as \nregularized spline regression. Our main technical result is then in extending \nTensorSketch to other norms. That is, TensorSketch only provides input sparsity \ntime for Kronecker product regression with respect to the $2$-norm. We show how \nto solve Kronecker product regression with respect to the $1$-norm in time \nsublinear in the time required for computing the Kronecker product, as well as \nfor more general $p$-norms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbc2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09473"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xingwei Cao, Guillaume Rabusseau, Joelle Pineau", "title": "Tensor Regression Networks with various Low-Rank Tensor Approximations. (arXiv:1712.09520v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.09520", "type": "text/html"}], "timestampUsec": "1514527610846339", "comments": [], "summary": {"content": "<p>Tensor regression networks achieve high rate of compression of model \nparameters in multilayer perceptrons (MLP) while having slight impact on \nperformances. Tensor regression layer imposes low-rank constraints on the \ntensor regression layer which replaces the flattening operation of traditional \nMLP. We investigate tensor regression networks using various low-rank tensor \napproximations, aiming to leverage the multi-modal structure of high \ndimensional data by enforcing efficient low-rank constraints. We provide a \ntheoretical analysis giving insights on the choice of the rank parameters. We \nevaluated performance of proposed model with state-of-the-art deep \nconvolutional models. For CIFAR-10 dataset, we achieved the compression rate of \n0.018 with the sacrifice of accuracy less than 1%. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbc45", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09520"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "O.B. Sezer, M. Ozbayoglu, E. Dogdu", "title": "An Artificial Neural Network-based Stock Trading System Using Technical Analysis and Big Data Framework. (arXiv:1712.09592v1 [cs.CE])", "alternate": [{"href": "http://arxiv.org/abs/1712.09592", "type": "text/html"}], "timestampUsec": "1514527610846338", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3299c6da5\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3299c6da5&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, a neural network-based stock price prediction and trading \nsystem using technical analysis indicators is presented. The model developed \nfirst converts the financial time series data into a series of buy-sell-hold \ntrigger signals using the most commonly preferred technical analysis \nindicators. Then, a Multilayer Perceptron (MLP) artificial neural network (ANN) \nmodel is trained in the learning stage on the daily stock prices between 1997 \nand 2007 for all of the Dow30 stocks. Apache Spark big data framework is used \nin the training stage. The trained model is then tested with data from 2007 to \n2017. The results indicate that by choosing the most appropriate technical \nindicators, the neural network model can achieve comparable results against the \nBuy and Hold strategy in most of the cases. Furthermore, fine tuning the \ntechnical indicators and/or optimization strategy can enhance the overall \ntrading performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbc5f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09592"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kostas Hatalis, Shalinee Kishore", "title": "A Composite Quantile Fourier Neural Network for Multi-Horizon Probabilistic Forecasting. (arXiv:1712.09641v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.09641", "type": "text/html"}], "timestampUsec": "1514527610846337", "comments": [], "summary": {"content": "<p>A novel quantile Fourier neural network is presented for nonparametric \nprobabilistic forecasting. Prediction are provided in the form of composite \nquantiles using time as the only input to the model. This effectively is a form \nof extrapolation based quantile regression applied for forecasting. Empirical \nresults showcase that for time series data that have clear seasonality and \ntrend, the model provides high quality probabilistic predictions. This work \nintroduces a new class of forecasting of using only time as the input versus \nusing past data such as an autoregressive model. Extrapolation based regression \nhas not been studied before for probabilistic forecasting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbc71", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09641"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Nicolas Loizou, Peter Richt&#xe1;rik", "title": "Momentum and Stochastic Momentum for Stochastic Gradient, Newton, Proximal Point and Subspace Descent Methods. (arXiv:1712.09677v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1712.09677", "type": "text/html"}], "timestampUsec": "1514527610846336", "comments": [], "summary": {"content": "<p>In this paper we study several classes of stochastic optimization algorithms \nenriched with heavy ball momentum. Among the methods studied are: stochastic \ngradient descent, stochastic Newton, stochastic proximal point and stochastic \ndual subspace ascent. This is the first time momentum variants of several of \nthese methods are studied. We choose to perform our analysis in a setting in \nwhich all of the above methods are equivalent. We prove global nonassymptotic \nlinear convergence rates for all methods and various measures of success, \nincluding primal function values, primal iterates (in L2 sense), and dual \nfunction values. We also show that the primal iterates converge at an \naccelerated linear rate in the L1 sense. This is the first time a linear rate \nis shown for the stochastic heavy ball method (i.e., stochastic gradient \ndescent method with momentum). Under somewhat weaker conditions, we establish a \nsublinear convergence rate for Cesaro averages of primal iterates. Moreover, we \npropose a novel concept, which we call stochastic momentum, aimed at decreasing \nthe cost of performing the momentum step. We prove linear convergence of \nseveral stochastic methods with stochastic momentum, and show that in some \nsparse data regimes and for sufficiently small momentum parameters, these \nmethods enjoy better overall complexity than methods with deterministic \nmomentum. Finally, we perform extensive numerical testing on artificial and \nreal datasets, including data coming from average consensus problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbc87", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09677"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Haolei Weng, Yang Feng", "title": "A note on estimation in a simple probit model under dependency. (arXiv:1712.09694v1 [math.ST])", "alternate": [{"href": "http://arxiv.org/abs/1712.09694", "type": "text/html"}], "timestampUsec": "1514527610846335", "comments": [], "summary": {"content": "<p>We consider a probit model without covariates, but the latent Gaussian \nvariables having compound symmetry covariance structure with a single parameter \ncharacterizing the common correlation. We study the parameter estimation \nproblem under such one-parameter probit models. As a surprise, we demonstrate \nthat the likelihood function does not yield consistent estimates for the \ncorrelation. We then formally prove the parameter's nonestimability by deriving \na non-vanishing minimax lower bound. This counter-intuitive phenomenon provides \nan interesting insight that one bit information of the latent Gaussian \nvariables is not sufficient to consistently recover their correlation. On the \nother hand, we further show that trinary data generated from the Gaussian \nvariables can consistently estimate the correlation with parametric convergence \nrate. Hence we reveal a phase transition phenomenon regarding the \ndiscretization of latent Gaussian variables while preserving the estimability \nof the correlation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbc9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09694"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Bethany Lusch, J. Nathan Kutz, Steven L. Brunton", "title": "Deep learning for universal linear embeddings of nonlinear dynamics. (arXiv:1712.09707v1 [math.DS])", "alternate": [{"href": "http://arxiv.org/abs/1712.09707", "type": "text/html"}], "timestampUsec": "1514527610846334", "comments": [], "summary": {"content": "<p>Identifying coordinate transformations that make strongly nonlinear dynamics \napproximately linear is a central challenge in modern dynamical systems. These \ntransformations have the potential to enable prediction, estimation, and \ncontrol of nonlinear systems using standard linear theory. The Koopman operator \nhas emerged as a leading data-driven embedding, as eigenfunctions of this \noperator provide intrinsic coordinates that globally linearize the dynamics. \nHowever, identifying and representing these eigenfunctions has proven to be \nmathematically and computationally challenging. This work leverages the power \nof deep learning to discover representations of Koopman eigenfunctions from \ntrajectory data of dynamical systems. Our network is parsimonious and \ninterpretable by construction, embedding the dynamics on a low-dimensional \nmanifold that is of the intrinsic rank of the dynamics and parameterized by the \nKoopman eigenfunctions. In particular, we identify nonlinear coordinates on \nwhich the dynamics are globally linear using a modified auto-encoder. We also \ngeneralize Koopman representations to include a ubiquitous class of systems \nthat exhibit continuous spectra, ranging from the simple pendulum to nonlinear \noptics and broadband turbulence. Our framework parametrizes the continuous \nfrequency using an auxiliary network, enabling a compact and efficient \nembedding at the intrinsic rank, while connecting our models to half a century \nof asymptotics. In this way, we benefit from the power and generality of deep \nlearning, while retaining the physical interpretability of Koopman embeddings. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbcc0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09707"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Charles Zheng, Rakesh Achanta, Yuval Benjamini", "title": "Extrapolating Expected Accuracies for Large Multi-Class Problems. (arXiv:1712.09713v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.09713", "type": "text/html"}], "timestampUsec": "1514527610846333", "comments": [], "summary": {"content": "<p>The difficulty of multi-class classification generally increases with the \nnumber of classes. Using data from a subset of the classes, can we predict how \nwell a classifier will scale with an increased number of classes? Under the \nassumptions that the classes are sampled identically and independently from a \npopulation, and that the classifier is based on independently learned scoring \nfunctions, we show that the expected accuracy when the classifier is trained on \nk classes is the (k-1)st moment of a certain distribution that can be estimated \nfrom data. We present an unbiased estimation method based on the theory, and \ndemonstrate its application on a facial recognition example. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbcd8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09713"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Meysam Golmohammadi, Amir Hossein Harati Nejad Torbati, Silvia Lopez de Diego, Iyad Obeid, Joseph Picone", "title": "Automatic Analysis of EEGs Using Big Data and Hybrid Deep Learning Architectures. (arXiv:1712.09771v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.09771", "type": "text/html"}], "timestampUsec": "1514527610846332", "comments": [], "summary": {"content": "<p>Objective: A clinical decision support tool that automatically interprets \nEEGs can reduce time to diagnosis and enhance real-time applications such as \nICU monitoring. Clinicians have indicated that a sensitivity of 95% with a \nspecificity below 5% was the minimum requirement for clinical acceptance. We \npropose a highperformance classification system based on principles of big data \nand machine learning. Methods: A hybrid machine learning system that uses \nhidden Markov models (HMM) for sequential decoding and deep learning networks \nfor postprocessing is proposed. These algorithms were trained and evaluated \nusing the TUH EEG Corpus, which is the world's largest publicly available \ndatabase of clinical EEG data. Results: Our approach delivers a sensitivity \nabove 90% while maintaining a specificity below 5%. This system detects three \nevents of clinical interest: (1) spike and/or sharp waves, (2) periodic \nlateralized epileptiform discharges, (3) generalized periodic epileptiform \ndischarges. It also detects three events used to model background noise: (1) \nartifacts, (2) eye movement (3) background. Conclusions: A hybrid HMM/deep \nlearning system can deliver a low false alarm rate on EEG event detection, \nmaking automated analysis a viable option for clinicians. Significance: The TUH \nEEG Corpus enables application of highly data consumptive machine learning \nalgorithms to EEG analysis. Performance is approaching clinical acceptance for \nreal-time applications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbce9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09771"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Meysam Golmohammadi, Saeedeh Ziyabari, Vinit Shah, Silvia Lopez de Diego, Iyad Obeid, Joseph Picone", "title": "Deep Architectures for Automated Seizure Detection in Scalp EEGs. (arXiv:1712.09776v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.09776", "type": "text/html"}], "timestampUsec": "1514527610846331", "comments": [], "summary": {"content": "<p>Automated seizure detection using clinical electroencephalograms is a \nchallenging machine learning problem because the multichannel signal often has \nan extremely low signal to noise ratio. Events of interest such as seizures are \neasily confused with signal artifacts (e.g, eye movements) or benign variants \n(e.g., slowing). Commercially available systems suffer from unacceptably high \nfalse alarm rates. Deep learning algorithms that employ high dimensional models \nhave not previously been effective due to the lack of big data resources. In \nthis paper, we use the TUH EEG Seizure Corpus to evaluate a variety of hybrid \ndeep structures including Convolutional Neural Networks and Long Short-Term \nMemory Networks. We introduce a novel recurrent convolutional architecture that \ndelivers 30% sensitivity at 7 false alarms per 24 hours. We have also evaluated \nour system on a held-out evaluation set based on the Duke University Seizure \nCorpus and demonstrate that performance trends are similar to the TUH EEG \nSeizure Corpus. This is a significant finding because the Duke corpus was \ncollected with different instrumentation and at different hospitals. Our work \nshows that deep learning architectures that integrate spatial and temporal \ncontexts are critical to achieving state of the art performance and will enable \na new generation of clinically-acceptable technology. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbcfc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09776"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Hao Li, Zheng Xu, Gavin Taylor, Tom Goldstein", "title": "Visualizing the Loss Landscape of Neural Nets. (arXiv:1712.09913v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.09913", "type": "text/html"}], "timestampUsec": "1514527610846330", "comments": [], "summary": {"content": "<p>Neural network training relies on our ability to find \"good\" minimizers of \nhighly non-convex loss functions. It is well known that certain network \narchitecture designs (e.g., skip connections) produce loss functions that train \neasier, and well-chosen training parameters (batch size, learning rate, \noptimizer) produce minimizers that generalize better. However, the reasons for \nthese differences, and their effect on the underlying loss landscape, is not \nwell understood. In this paper, we explore the structure of neural loss \nfunctions, and the effect of loss landscapes on generalization, using a range \nof visualization methods. First, we introduce a simple \"filter normalization\" \nmethod that helps us visualize loss function curvature, and make meaningful \nside-by-side comparisons between loss functions. Then, using a variety of \nvisualizations, we explore how network architecture affects the loss landscape, \nand how training parameters affect the shape of minimizers. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbd17", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09913"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516253549, "author": "Yanning Shen, Tianyi Chen, Georgios B. Giannakis", "title": "Random Feature-based Online Multi-kernel Learning in Environments with Unknown Dynamics. (arXiv:1712.09983v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.09983", "type": "text/html"}], "timestampUsec": "1514527610846329", "comments": [], "summary": {"content": "<p>Kernel-based methods exhibit well-documented performance in various nonlinear \nlearning tasks. Most of them rely on a preselected kernel, whose prudent choice \npresumes task-specific prior information. Especially when the latter is not \navailable, multi-kernel learning has gained popularity thanks to its \nflexibility in choosing kernels from a prescribed kernel dictionary. Leveraging \nthe random feature approximation and its recent orthogonality-promoting \nvariant, the present contribution develops a scalable multi-kernel learning \nscheme (termed Raker) to obtain the sought nonlinear learning function `on the \nfly,' first for static environments. To further boost performance in dynamic \nenvironments, an adaptive multi-kernel learning scheme (termed AdaRaker) is \ndeveloped using weighted combinations of advices from hierarchical ensembles of \nexperts. The weights account not only for each kernel's contribution to the \nlearning, but also for the unknown dynamics. Performance is analyzed in terms \nof both static and dynamic regrets. AdaRaker is uniquely capable of tracking \nnonlinear learning functions in environments with unknown dynamics, with \nanalytic performance guarantees. Tests with synthetic and real datasets are \ncarried out to showcase the effectiveness of the novel algorithms, and their \nperformance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbd2d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09983"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1515649358, "author": "Victor Chernozhukov, Matt Goldman, Vira Semenova, Matt Taddy", "title": "Orthogonal Machine Learning for Demand Estimation: High Dimensional Causal Inference in Dynamic Panels. (arXiv:1712.09988v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.09988", "type": "text/html"}], "timestampUsec": "1514527610846328", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b3299c706f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b3299c706f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>There has been growing interest in how economists can import machine learning \ntools designed for prediction to accelerate and automate the model selection \nprocess, while still retaining desirable inference properties for causal \nparameters. Focusing on partially linear models, we extend the Double ML \nframework to allow for (1) a number of treatments that may grow with the sample \nsize and (2) the analysis of panel data under sequentially exogenous errors. \nOur low-dimensional treatment (LD) regime directly extends the work in \n[Chernozhukov et al., 2016], by showing that the coefficients from a second \nstage, ordinary least squares estimator attain root-n convergence and desired \ncoverage even if the dimensionality of treatment is allowed to grow. In a \nhigh-dimensional sparse (HDS) regime, we show that second stage LASSO and \ndebiased LASSO have asymptotic properties equivalent to oracle estimators with \nno upstream error. We argue that these advances make Double ML methods a \ndesirable alternative for practitioners estimating short-term demand \nelasticities in non-contractual settings. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbd3c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09988"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Raymond K. W. Wong, Thomas C. M. Lee", "title": "Matrix Completion with Noisy Entries and Outliers. (arXiv:1503.00214v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1503.00214", "type": "text/html"}], "timestampUsec": "1514527610846327", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329a37d2c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329a37d2c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper considers the problem of matrix completion when the observed \nentries are noisy and contain outliers. It begins with introducing a new \noptimization criterion for which the recovered matrix is defined as its \nsolution. This criterion uses the celebrated Huber function from the robust \nstatistics literature to downweigh the effects of outliers. A practical \nalgorithm is developed to solve the optimization involved. This algorithm is \nfast, straightforward to implement, and monotonic convergent. Furthermore, the \nproposed methodology is theoretically shown to be stable in a well defined \nsense. Its promising empirical performance is demonstrated via a sequence of \nsimulation experiments, including image inpainting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbd4e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1503.00214"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Giri Gopalan, Saeqa Dil Vrtilek, Luke Bornn", "title": "Classifying X-ray Binaries: A Probabilistic Approach. (arXiv:1507.03538v2 [astro-ph.HE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1507.03538", "type": "text/html"}], "timestampUsec": "1514527610846326", "comments": [], "summary": {"content": "<p>In X-ray binary star systems consisting of a compact object that accretes \nmaterial from an orbiting secondary star, there is no straightforward means to \ndecide if the compact object is a black hole or a neutron star. To assist this \nclassification, we develop a Bayesian statistical model that makes use of the \nfact that X-ray binary systems appear to cluster based on their compact object \ntype when viewed from a 3-dimensional coordinate system derived from X-ray \nspectral data. The first coordinate of this data is the ratio of counts in mid \nto low energy band (color 1), the second coordinate is the ratio of counts in \nhigh to low energy band (color 2), and the third coordinate is the sum of \ncounts in all three bands. We use this model to estimate the probabilities that \nan X-ray binary system contains a black hole, non-pulsing neutron star, or \npulsing neutron star. In particular, we utilize a latent variable model in \nwhich the latent variables follow a Gaussian process prior distribution, and \nhence we are able to induce the spatial correlation we believe exists between \nsystems of the same type. The utility of this approach is evidenced by the \naccurate prediction of system types using Rossi X-ray Timing Explorer All Sky \nMonitor data, but it is not flawless. In particular, non-pulsing neutron \nsystems containing \"bursters\" that are close to the boundary demarcating \nsystems containing black holes tend to be classified as black hole systems. As \na byproduct of our analyses, we provide the astronomer with public R code that \ncan be used to predict the compact object type of X-ray binaries given training \ndata. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbd69", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1507.03538"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Fatemeh Sheikholeslami, Dimitris Berberidis, Georgios B.Giannakis", "title": "Large-scale Kernel-based Feature Extraction via Budgeted Nonlinear Subspace Tracking. (arXiv:1601.07947v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1601.07947", "type": "text/html"}], "timestampUsec": "1514527610846325", "comments": [], "summary": {"content": "<p>Kernel-based methods enjoy powerful generalization capabilities in handling a \nvariety of learning tasks. When such methods are provided with sufficient \ntraining data, broadly-applicable classes of nonlinear functions can be \napproximated with desired accuracy. Nevertheless, inherent to the nonparametric \nnature of kernel-based estimators are computational and memory requirements \nthat become prohibitive with large-scale datasets. In response to this \nformidable challenge, the present work puts forward a low-rank, kernel-based, \nfeature extraction approach that is particularly tailored for online operation, \nwhere data streams need not be stored in memory. A novel generative model is \nintroduced to approximate high-dimensional (possibly infinite) features via a \nlow-rank nonlinear subspace, the learning of which leads to a direct kernel \nfunction approximation. Offline and online solvers are developed for the \nsubspace learning task, along with affordable versions, in which the number of \nstored data vectors is confined to a predefined budget. Analytical results \nprovide performance bounds on how well the kernel matrix as well as \nkernel-based classification and regression tasks can be approximated by \nleveraging budgeted online subspace learning and feature extraction schemes. \nTests on synthetic and real datasets demonstrate and benchmark the efficiency \nof the proposed method when linear classification and regression is applied to \nthe extracted features. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbd81", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1601.07947"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mark Harmon, Patrick Lucey, Diego Klabjan", "title": "Predicting Shot Making in Basketball Learnt from Adversarial Multiagent Trajectories. (arXiv:1609.04849v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1609.04849", "type": "text/html"}], "timestampUsec": "1514527610846324", "comments": [], "summary": {"content": "<p>In this paper, we predict the likelihood of a player making a shot in \nbasketball from multiagent trajectories. Previous approaches to similar \nproblems center on hand-crafting features to capture domain specific knowledge. \nAlthough intuitive, recent work in deep learning has shown this approach is \nprone to missing important predictive features. To circumvent this issue, we \npresent a convolutional neural network (CNN) approach where we initially \nrepresent the multiagent behavior as an image. To encode the adversarial nature \nof basketball, we use a multi-channel image which we then feed into a CNN. \nAdditionally, to capture the temporal aspect of the trajectories we \"fade\" the \nplayer trajectories. We find that this approach is superior to a traditional \nFFN model. By using gradient ascent to create images using an already trained \nCNN, we discover what features the CNN filters learn. Last, we find that a \ncombined CNN+FFN is the best performing network with an error rate of 39%. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbd90", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1609.04849"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, Joseph Salmon", "title": "Gap Safe screening rules for sparsity enforcing penalties. (arXiv:1611.05780v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.05780", "type": "text/html"}], "timestampUsec": "1514527610846323", "comments": [], "summary": {"content": "<p>In high dimensional regression settings, sparsity enforcing penalties have \nproved useful to regularize the data-fitting term. A recently introduced \ntechnique called screening rules propose to ignore some variables in the \noptimization leveraging the expected sparsity of the solutions and consequently \nleading to faster solvers. When the procedure is guaranteed not to discard \nvariables wrongly the rules are said to be safe. In this work, we propose a \nunifying framework for generalized linear models regularized with standard \nsparsity enforcing penalties such as $\\ell_1$ or $\\ell_1/\\ell_2$ norms. Our \ntechnique allows to discard safely more variables than previously considered \nsafe rules, particularly for low regularization parameters. Our proposed Gap \nSafe rules (so called because they rely on duality gap computation) can cope \nwith any iterative solver but are particularly well suited to (block) \ncoordinate descent methods. Applied to many standard learning tasks, Lasso, \nSparse-Group Lasso, multi-task Lasso, binary and multinomial logistic \nregression, etc., we report significant speed-ups compared to previously \nproposed safe rules on all tested data sets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbd94", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.05780"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jonathan Mei, Jose&#x27; M.F. Moura", "title": "SILVar: Single Index Latent Variable Models. (arXiv:1705.03536v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.03536", "type": "text/html"}], "timestampUsec": "1514527610846322", "comments": [], "summary": {"content": "<p>A semi-parametric, non-linear regression model in the presence of latent \nvariables is introduced. These latent variables can correspond to unmodeled \nphenomena or unmeasured agents in a complex networked system. This new \nformulation allows joint estimation of certain non-linearities in the system, \nthe direct interactions between measured variables, and the effects of \nunmodeled elements on the observed system. The particular form of the model \nadopted is justified, and learning is posed as a regularized maximum likelihood \nestimation. This leads to classes of structured convex optimization problems \nwith a \"sparse plus low-rank\" flavor. Relations between the proposed model and \nseveral common model paradigms, such as those of Robust Principal Component \nAnalysis (PCA) and Vector Autoregression (VAR), are established. Particularly \nin the VAR setting, the low-rank contributions can come from broad trends \nexhibited in the time series. Details of the algorithm for learning the model \nare presented. Experiments demonstrate the performance of the model and the \nestimation algorithm on simulated and real data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbda3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.03536"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Huahui Liu, Mingrui Zhu, Xiaonan Meng, Yi Hu, Hao Wang", "title": "Dual Based DSP Bidding Strategy and its Application. (arXiv:1705.09416v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.09416", "type": "text/html"}], "timestampUsec": "1514527610846321", "comments": [], "summary": {"content": "<p>In recent years, RTB(Real Time Bidding) becomes a popular online \nadvertisement trading method. During the auction, each DSP(Demand Side \nPlatform) is supposed to evaluate current opportunity and respond with an ad \nand corresponding bid price. It's essential for DSP to find an optimal ad \nselection and bid price determination strategy which maximizes revenue or \nperformance under budget and ROI(Return On Investment) constraints in P4P(Pay \nFor Performance) or P4U(Pay For Usage) mode. We solve this problem by 1) \nformalizing the DSP problem as a constrained optimization problem, 2) proposing \nthe augmented MMKP(Multi-choice Multi-dimensional Knapsack Problem) with \ngeneral solution, 3) and demonstrating the DSP problem is a special case of the \naugmented MMKP and deriving specialized strategy. Our strategy is verified \nthrough simulation and outperforms state-of-the-art strategies in real \napplication. To the best of our knowledge, our solution is the first dual based \nDSP bidding framework that is derived from strict second price auction \nassumption and generally applicable to the multiple ads scenario with various \nobjectives and constraints. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbdb5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.09416"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Katarzyna Bozek, Laetitia Hebert, Alexander S Mikheyev, Greg J Stephens", "title": "Towards dense object tracking in a 2D honeybee hive. (arXiv:1712.08324v1 [cs.CV] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1712.08324", "type": "text/html"}], "timestampUsec": "1514527610846320", "comments": [], "summary": {"content": "<p>From human crowds to cells in tissue, the detection and efficient tracking of \nmultiple objects in dense configurations is an important and unsolved problem. \nIn the past, limitations of image analysis have restricted studies of dense \ngroups to tracking a single or subset of marked individuals, or to \ncoarse-grained group-level dynamics, all of which yield incomplete information. \nHere, we combine convolutional neural networks (CNNs) with the model \nenvironment of a honeybee hive to automatically recognize all individuals in a \ndense group from raw image data. We create new, adapted individual labeling and \nuse the segmentation architecture U-Net with a loss function dependent on both \nobject identity and orientation. We additionally exploit temporal regularities \nof the video recording in a recurrent manner and achieve near human-level \nperformance while reducing the network size by 94% compared to the original \nU-Net architecture. Given our novel application of CNNs, we generate extensive \nproblem-specific image data in which labeled examples are produced through a \ncustom interface with Amazon Mechanical Turk. This dataset contains over \n375,000 labeled bee instances across 720 video frames at 2 FPS, representing an \nextensive resource for the development and testing of tracking methods. We \ncorrectly detect 96% of individuals with a location error of ~7% of a typical \nbody dimension, and orientation error of 12 degrees, approximating the \nvariability of human raters. Our results provide an important step towards \nefficient image-based dense object tracking by allowing for the accurate \ndetermination of object location and orientation across time-series image data \nefficiently within one network architecture. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514527610846", "annotations": [], "published": 1514527611, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003509cbdc5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08324"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Qiming Chen, Ren Wu", "title": "CNN Is All You Need. (arXiv:1712.09662v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.09662", "type": "text/html"}], "timestampUsec": "1514524596323164", "comments": [], "summary": {"content": "<p>The Convolution Neural Network (CNN) has demonstrated the unique advantage in \naudio, image and text learning; recently it has also challenged Recurrent \nNeural Networks (RNNs) with long short-term memory cells (LSTM) in \nsequence-to-sequence learning, since the computations involved in CNN are \neasily parallelizable whereas those involved in RNN are mostly sequential, \nleading to a performance bottleneck. However, unlike RNN, the native CNN lacks \nthe history sensitivity required for sequence transformation; therefore \nenhancing the sequential order awareness, or position-sensitivity, becomes the \nkey to make CNN the general deep learning model. In this work we introduce an \nextended CNN model with strengthen position-sensitivity, called PoseNet. A \nnotable feature of PoseNet is the asymmetric treatment of position information \nin the encoder and the decoder. Experiments shows that PoseNet allows us to \nimprove the accuracy of CNN based sequence-to-sequence learning significantly, \nachieving around 33-36 BLEU scores on the WMT 2014 English-to-German \ntranslation task, and around 44-46 BLEU scores on the English-to-French \ntranslation task. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514524596323", "annotations": [], "published": 1514524597, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035097cb29", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09662"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tim Rockt&#xe4;schel", "title": "Combining Representation Learning with Logic for Language Processing. (arXiv:1712.09687v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.09687", "type": "text/html"}], "timestampUsec": "1514524596323163", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329a381aa\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329a381aa&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The current state-of-the-art in many natural language processing and \nautomated knowledge base completion tasks is held by representation learning \nmethods which learn distributed vector representations of symbols via \ngradient-based optimization. They require little or no hand-crafted features, \nthus avoiding the need for most preprocessing steps and task-specific \nassumptions. However, in many cases representation learning requires a large \namount of annotated training data to generalize well to unseen data. Such \nlabeled training data is provided by human annotators who often use formal \nlogic as the language for specifying annotations. This thesis investigates \ndifferent combinations of representation learning methods with logic for \nreducing the need for annotated training data, and for improving \ngeneralization. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514524596323", "annotations": [], "published": 1514524597, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035097cb31", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09687"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, Tong Wang, Adam Trischler", "title": "Learning Rapid-Temporal Adaptations. (arXiv:1712.09926v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.09926", "type": "text/html"}], "timestampUsec": "1514524596323162", "comments": [], "summary": {"content": "<p>A hallmark of human intelligence and cognition is its flexibility. One of the \nlong-standing goals in AI research is to replicate this flexibility in a \nlearning machine. In this work we describe a mechanism by which artificial \nneural networks can learn rapid-temporal adaptation - the ability to adapt \nquickly to new environments or tasks - that we call adaptive neurons. Adaptive \nneurons modify their activations with task-specific values retrieved from a \nworking memory. On standard metalearning and few-shot learning benchmarks in \nboth vision and language domains, models augmented with adaptive neurons \nachieve state-of-the-art results. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514524596323", "annotations": [], "published": 1514524597, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035097cb36", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09926"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Abien Fred Agarap", "title": "A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data. (arXiv:1709.03082v6 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.03082", "type": "text/html"}], "timestampUsec": "1514524596323161", "comments": [], "summary": {"content": "<p>Gated Recurrent Unit (GRU) is a recently-developed variation of the long \nshort-term memory (LSTM) unit, both of which are types of recurrent neural \nnetwork (RNN). Through empirical evidence, both models have been proven to be \neffective in a wide variety of machine learning tasks such as natural language \nprocessing (Wen et al., 2015), speech recognition (Chorowski et al., 2015), and \ntext classification (Yang et al., 2016). Conventionally, like most neural \nnetworks, both of the aforementioned RNN variants employ the Softmax function \nas its final output layer for its prediction, and the cross-entropy function \nfor computing its loss. In this paper, we present an amendment to this norm by \nintroducing linear support vector machine (SVM) as the replacement for Softmax \nin the final output layer of a GRU model. Furthermore, the cross-entropy \nfunction shall be replaced with a margin-based function. While there have been \nsimilar studies (Alalshekmubarak &amp; Smith, 2013; Tang, 2013), this proposal is \nprimarily intended for binary classification on intrusion detection using the \n2013 network traffic data from the honeypot systems of Kyoto University. \nResults show that the GRU-SVM model performs relatively higher than the \nconventional GRU-Softmax model. The proposed model reached a training accuracy \nof ~81.54% and a testing accuracy of ~84.15%, while the latter was able to \nreach a training accuracy of ~63.07% and a testing accuracy of ~70.75%. In \naddition, the juxtaposition of these two final output layers indicate that the \nSVM would outperform Softmax in prediction time - a theoretical implication \nwhich was supported by the actual training and testing time in the study. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514524596323", "annotations": [], "published": 1514524597, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035097cb3e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.03082"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Vitaliy Liptchinsky, Gabriel Synnaeve, Ronan Collobert", "title": "Letter-Based Speech Recognition with Gated ConvNets. (arXiv:1712.09444v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.09444", "type": "text/html"}], "timestampUsec": "1514524596323160", "comments": [], "summary": {"content": "<p>In this paper we introduce a new speech recognition system, leveraging a \nsimple letter-based ConvNet acoustic model. The acoustic model requires -- only \naudio transcription for training -- no alignment annotations, nor any forced \nalignment step is needed. At inference, our decoder takes only a word list and \na language model, and is fed with letter scores from the -- acoustic model -- \nno phonetic word lexicon is needed. Key ingredients for the acoustic model are \nGated Linear Units and high dropout. We show near state-of-the-art results in \nword error rate on the LibriSpeech corpus using log-mel filterbanks, both on \nthe \"clean\" and \"other\" configurations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514524596323", "annotations": [], "published": 1514524597, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035097cb44", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09444"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andreas Holzinger, Chris Biemann, Constantinos S. Pattichis, Douglas B. Kell", "title": "What do we need to build explainable AI systems for the medical domain?. (arXiv:1712.09923v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.09923", "type": "text/html"}], "timestampUsec": "1514524596323159", "comments": [], "summary": {"content": "<p>Artificial intelligence (AI) generally and machine learning (ML) specifically \ndemonstrate impressive practical success in many different application domains, \ne.g. in autonomous driving, speech recognition, or recommender systems. Deep \nlearning approaches, trained on extremely large data sets or using \nreinforcement learning methods have even exceeded human performance in visual \ntasks, particularly on playing games such as Atari, or mastering the game of \nGo. Even in the medical domain there are remarkable results. The central \nproblem of such models is that they are regarded as black-box models and even \nif we understand the underlying mathematical principles, they lack an explicit \ndeclarative knowledge representation, hence have difficulty in generating the \nunderlying explanatory structures. This calls for systems enabling to make \ndecisions transparent, understandable and explainable. A huge motivation for \nour approach are rising legal and privacy aspects. The new European General \nData Protection Regulation entering into force on May 25th 2018, will make \nblack-box approaches difficult to use in business. This does not imply a ban on \nautomatic learning approaches or an obligation to explain everything all the \ntime, however, there must be a possibility to make the results re-traceable on \ndemand. In this paper we outline some of our research topics in the context of \nthe relatively new area of explainable-AI with a focus on the application in \nmedicine, which is a very special domain. This is due to the fact that medical \nprofessionals are working mostly with distributed heterogeneous and complex \nsources of data. In this paper we concentrate on three sources: images, *omics \ndata and text. We argue that research in explainable-AI would generally help to \nfacilitate the implementation of AI/ML in the medical domain, and specifically \nhelp to facilitate transparency and trust. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514524596323", "annotations": [], "published": 1514524597, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035097cb4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09923"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1515560459, "author": "Sungjin Lee", "title": "Toward Continual Learning for Conversational Agents. (arXiv:1712.09943v3 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.09943", "type": "text/html"}], "timestampUsec": "1514524596323158", "comments": [], "summary": {"content": "<p>While end-to-end neural conversation models have led to promising advances in \nreducing hand-crafted features and errors induced by the traditional complex \nsystem architecture, they typically require an enormous amount of data due to \nthe lack of modularity. Previous studies adopted a hybrid approach with \nknowledge-based components either to abstract out domain-specific information \nor to augment data to cover more diverse patterns. On the contrary, we propose \nto directly address the problem using recent developments in the space of \ncontinual learning for neural models. Specifically, we adopt a \ndomain-independent neural conversational model and introduce a novel neural \ncontinual learning algorithm that allows a conversational agent to accumulate \nskills across different tasks in a data-efficient way. To the best of our \nknowledge, this is the first work that applies continual learning to \nconversation systems. We verified the efficacy of our method through a \nconversational skill transfer from either synthetic dialogs or human-human \ndialogs to human-computer conversations in a customer support domain. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514524596323", "annotations": [], "published": 1515560459, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035097cb54", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09943"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aritra Ghosh, Himanshu Kumar, P.S. Sastry", "title": "Robust Loss Functions under Label Noise for Deep Neural Networks. (arXiv:1712.09482v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.09482", "type": "text/html"}], "timestampUsec": "1514524596323157", "comments": [], "summary": {"content": "<p>In many applications of classifier learning, training data suffers from label \nnoise. Deep networks are learned using huge training data where the problem of \nnoisy labels is particularly relevant. The current techniques proposed for \nlearning deep networks under label noise focus on modifying the network \narchitecture and on algorithms for estimating true labels from noisy labels. An \nalternate approach would be to look for loss functions that are inherently \nnoise-tolerant. For binary classification there exist theoretical results on \nloss functions that are robust to label noise. In this paper, we provide some \nsufficient conditions on a loss function so that risk minimization under that \nloss function would be inherently tolerant to label noise for multiclass \nclassification problems. These results generalize the existing results on \nnoise-tolerant loss functions for binary classification. We study some of the \nwidely used loss functions in deep networks and show that the loss function \nbased on mean absolute value of error is inherently robust to label noise. Thus \nstandard back propagation is enough to learn the true classifier even under \nlabel noise. Through experiments, we illustrate the robustness of risk \nminimization with such loss functions for learning neural networks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514524596323", "annotations": [], "published": 1514524597, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035097cb5b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09482"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jens Berg, Kaj Nystr&#xf6;m", "title": "Neural network augmented inverse problems for PDEs. (arXiv:1712.09685v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.09685", "type": "text/html"}], "timestampUsec": "1514524596323156", "comments": [], "summary": {"content": "<p>In this paper we show how to augment classical methods for inverse problems \nwith artificial neural networks. The neural network acts as a parametric \ncontainer for the coefficient to be estimated from noisy data. Neural networks \nare global, smooth function approximators and as such they do not require \nregularization of the error functional to recover smooth solutions and \ncoefficients. We give detailed examples using the Poisson equation in 1, 2, and \n3 space dimensions and show that the neural network augmentation is robust with \nrespect to noisy data, mesh, and geometry. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514524596323", "annotations": [], "published": 1514524597, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035097cb61", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09685"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1514568400, "author": "Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, Pieter Abbeel", "title": "PixelSNAIL: An Improved Autoregressive Generative Model. (arXiv:1712.09763v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.09763", "type": "text/html"}], "timestampUsec": "1514524596323155", "comments": [], "summary": {"content": "<p>Autoregressive generative models consistently achieve the best results in \ndensity estimation tasks involving high dimensional data, such as images or \naudio. They pose density estimation as a sequence modeling task, where a \nrecurrent neural network (RNN) models the conditional distribution over the \nnext element conditioned on all previous elements. In this paradigm, the \nbottleneck is the extent to which the RNN can model long-range dependencies, \nand the most successful approaches rely on causal convolutions, which offer \nbetter access to earlier parts of the sequence than conventional RNNs. Taking \ninspiration from recent work in meta reinforcement learning, where dealing with \nlong-range dependencies is also essential, we introduce a new generative model \narchitecture that combines causal convolutions with self attention. In this \nnote, we describe the resulting model and present state-of-the-art \nlog-likelihood results on CIFAR-10 (2.85 bits per dim) and $32 \\times 32$ \nImageNet (3.80 bits per dim). Our implementation is available at \nhttps://github.com/neocxi/pixelsnail-public \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514524596323", "annotations": [], "published": 1514568400, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035097cb67", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09763"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Dimitris Berberidis, Georgios B. Giannakis", "title": "Data-adaptive Active Sampling for Efficient Graph-Cognizant Classification. (arXiv:1705.07220v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.07220", "type": "text/html"}], "timestampUsec": "1514524596323154", "comments": [], "summary": {"content": "<p>The present work deals with active sampling of graph nodes representing \ntraining data for binary classification. The graph may be given or constructed \nusing similarity measures among nodal features. Leveraging the graph for \nclassification builds on the premise that labels across neighboring nodes are \ncorrelated according to a categorical Markov random field (MRF). This model is \nfurther relaxed to a Gaussian (G)MRF with labels taking continuous values - an \napproximation that not only mitigates the combinatorial complexity of the \ncategorical model, but also offers optimal unbiased soft predictors of the \nunlabeled nodes. The proposed sampling strategy is based on querying the node \nwhose label disclosure is expected to inflict the largest change on the GMRF, \nand in this sense it is the most informative on average. Such a strategy \nsubsumes several measures of expected model change, including uncertainty \nsampling, variance minimization, and sampling based on the $\\Sigma-$optimality \ncriterion. A simple yet effective heuristic is also introduced for increasing \nthe exploration capabilities of the sampler, and reducing bias of the resultant \nclassifier, by taking into account the confidence on the model label \npredictions. The novel sampling strategies are based on quantities that are \nreadily available without the need for model retraining, rendering them \ncomputationally efficient and scalable to large graphs. Numerical tests using \nsynthetic and real data demonstrate that the proposed methods achieve accuracy \nthat is comparable or superior to the state-of-the-art even at reduced runtime. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514524596323", "annotations": [], "published": 1514524597, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000035097cb6f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.07220"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yan Ru Pei, Fabio L. Traversa, Massimiliano Di Ventra", "title": "On the Universality of Memcomputing Machines. (arXiv:1712.08702v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.08702", "type": "text/html"}], "timestampUsec": "1514352521979608", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329a38576\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329a38576&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Universal memcomputing machines (UMMs) [IEEE Trans. Neural Netw. Learn. Syst. \n26, 2702 (2015)] represent a novel computational model in which memory (time \nnon-locality) accomplishes both tasks of storing and processing of information. \nUMMs have been shown to be Turing-complete, namely they can simulate any Turing \nmachine. In this paper, using set theory and cardinality arguments, we compare \nthem with liquid-state machines (or \"reservoir computing\") and quantum machines \n(\"quantum computing\"). We show that UMMs can simulate both types of machines, \nhence they are both \"liquid-\" or \"reservoir-complete\" and \"quantum-complete\". \nOf course, these statements pertain only to the type of problems these machines \ncan solve, and not to the amount of resources required for such simulations. \nNonetheless, the method presented here provides a general framework in which to \ndescribe the relation between UMMs and any other type of computational model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a02b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08702"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tao Lei, Yu Zhang, Yoav Artzi", "title": "Training RNNs as Fast as CNNs. (arXiv:1709.02755v4 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.02755", "type": "text/html"}], "timestampUsec": "1514352521979607", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329a88238\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329a88238&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Common recurrent neural network architectures scale poorly due to the \nintrinsic difficulty in parallelizing their state computations. In this work, \nwe propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that \nsimplifies the computation and exposes more parallelism. In SRU, the majority \nof computation for each step is independent of the recurrence and can be easily \nparallelized. SRU is as fast as a convolutional layer and 5-10x faster than an \noptimized LSTM implementation. We study SRUs on a wide range of applications, \nincluding classification, question answering, language modeling, translation \nand speech recognition. Our experiments demonstrate the effectiveness of SRU \nand the trade-off it enables between speed and performance. We open source our \nimplementation in PyTorch and CNTK. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a02f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.02755"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wei Wen, Yuxiong He, Samyam Rajbhandari, Wenhan Wang, Fang Liu, Bin Hu, Yiran Chen, Hai Li", "title": "Learning Intrinsic Sparse Structures within Long Short-Term Memory. (arXiv:1709.05027v4 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.05027", "type": "text/html"}], "timestampUsec": "1514352521979606", "comments": [], "summary": {"content": "<p>Model compression is significant for the wide adoption of Recurrent Neural \nNetworks (RNNs) in both user devices possessing limited resources and business \nclusters requiring quick responses to large-scale service requests. This work \naims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the \nsizes of basic structures within LSTM units, including input updates, gates, \nhidden states, cell states and outputs. Independently reducing the sizes of \nbasic structures can result in inconsistent dimensions among them, and \nconsequently, end up with invalid LSTM units. To overcome the problem, we \npropose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS \nwill simultaneously decrease the sizes of all basic structures by one and \nthereby always maintain the dimension consistency. By learning ISS within LSTM \nunits, the obtained LSTMs remain regular while having much smaller basic \nstructures. Based on group Lasso regularization, our method achieves 10.59x \nspeedup without losing any perplexity of a language modeling of Penn TreeBank \ndataset. It is also successfully evaluated through a compact model with only \n2.69M weights for machine Question Answering of SQuAD dataset. Our approach is \nsuccessfully extended to non- LSTM RNNs, like Recurrent Highway Networks \n(RHNs). Our source code is publicly available at \nhttps://github.com/wenwei202/iss-rnns \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a032", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.05027"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "A. Murat Ozbayoglu, Gokhan Kucukayan, Erdogan Dogdu", "title": "A Real-Time Autonomous Highway Accident Detection Model Based on Big Data Processing and Computational Intelligence. (arXiv:1712.09227v1 [cs.CE])", "alternate": [{"href": "http://arxiv.org/abs/1712.09227", "type": "text/html"}], "timestampUsec": "1514352521979605", "comments": [], "summary": {"content": "<p>Due to increasing urban population and growing number of motor vehicles, \ntraffic congestion is becoming a major problem of the 21st century. One of the \nmain reasons behind traffic congestion is accidents which can not only result \nin casualties and losses for the participants, but also in wasted and lost time \nfor the others that are stuck behind the wheels. Early detection of an accident \ncan save lives, provides quicker road openings, hence decreases wasted time and \nresources, and increases efficiency. In this study, we propose a preliminary \nreal-time autonomous accident-detection system based on computational \nintelligence techniques. Istanbul City traffic-flow data for the year 2015 from \nvarious sensor locations are populated using big data processing methodologies. \nThe extracted features are then fed into a nearest neighbor model, a regression \ntree, and a feed-forward neural network model. For the output, the possibility \nof an occurrence of an accident is predicted. The results indicate that even \nthough the number of false alarms dominates the real accident cases, the system \ncan still provide useful information that can be used for status verification \nand early reaction to possible accidents. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a037", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09227"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Arkar Min Aung, Yousef Fadila, Radian Gondokaryono, Luis Gonzalez", "title": "Building Robust Deep Neural Networks for Road Sign Detection. (arXiv:1712.09327v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.09327", "type": "text/html"}], "timestampUsec": "1514352521979604", "comments": [], "summary": {"content": "<p>Deep Neural Networks are built to generalize outside of training set in mind \nby using techniques such as regularization, early stopping and dropout. But \nconsiderations to make them more resilient to adversarial examples are rarely \ntaken. As deep neural networks become more prevalent in mission-critical and \nreal-time systems, miscreants start to attack them by intentionally making deep \nneural networks to misclassify an object of one type to be seen as another \ntype. This can be catastrophic in some scenarios where the classification of a \ndeep neural network can lead to a fatal decision by a machine. In this work, we \nused GTSRB dataset to craft adversarial samples by Fast Gradient Sign Method \nand Jacobian Saliency Method, used those crafted adversarial samples to attack \nanother Deep Convolutional Neural Network and built the attacked network to be \nmore resilient against adversarial attacks by making it more robust by \nDefensive Distillation and Adversarial Training \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a03b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09327"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Amit Dhurandhar, Vijay Iyengar, Ronny Luss, Karthikeyan Shanmugam", "title": "TIP: Typifying the Interpretability of Procedures. (arXiv:1706.02952v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.02952", "type": "text/html"}], "timestampUsec": "1514352521979603", "comments": [], "summary": {"content": "<p>We provide a novel notion of what it means to be interpretable, looking past \nthe usual association with human understanding. Our key insight is that \ninterpretability is not an absolute concept and so we define it relative to a \ntarget model, which may or may not be a human. We define a framework that \nallows for comparing interpretable procedures by linking it to important \npractical aspects such as accuracy and robustness. We characterize many of the \ncurrent state-of-the-art interpretable methods in our framework portraying its \ngeneral applicability. Finally, principled interpretable strategies are \nproposed and empirically evaluated on synthetic data, as well as on the largest \npublic olfaction dataset that was made recently available \\cite{olfs}. We also \nexperiment on MNIST with a simple target model and different oracle models of \nvarying complexity. This leads to the insight that the improvement in the \ntarget model is not only a function of the oracle models performance, but also \nits relative complexity with respect to the target model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a03e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.02952"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chun-Hao Chang, Ladislav Rampasek, Anna Goldenberg", "title": "Dropout Feature Ranking for Deep Learning Models. (arXiv:1712.08645v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08645", "type": "text/html"}], "timestampUsec": "1514352521979602", "comments": [], "summary": {"content": "<p>Deep neural networks are a promising technology achieving state-of-the-art \nresults in biological and healthcare domains. Unfortunately, DNNs are notorious \nfor their non-interpretability. Clinicians are averse to black boxes and thus \ninterpretability is paramount to broadly adopting this technology. We aim to \nclose this gap by proposing a new general feature ranking method for deep \nlearning. We show that our method outperforms LASSO, Elastic Net, Deep Feature \nSelection and various heuristics on a simulated dataset. We also compare our \nmethod in a multivariate clinical time-series dataset and demonstrate our \nranking rivals or outperforms other methods in Recurrent Neural Network \nsetting. Finally, we apply our feature ranking to the Variational Autoencoder \nrecently proposed to predict drug response in cell lines and show that it \nidentifies meaningful genes corresponding to the drug response. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a042", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08645"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Fnu Suya, Yuan Tian, David Evans, Paolo Papotti", "title": "Query-limited Black-box Attacks to Classifiers. (arXiv:1712.08713v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1712.08713", "type": "text/html"}], "timestampUsec": "1514352521979601", "comments": [], "summary": {"content": "<p>We study black-box attacks on machine learning classifiers where each query \nto the model incurs some cost or risk of detection to the adversary. We focus \nexplicitly on minimizing the number of queries as a major objective. \nSpecifically, we consider the problem of attacking machine learning classifiers \nsubject to a budget of feature modification cost while minimizing the number of \nqueries, where each query returns only a class and confidence score. We \ndescribe an approach that uses Bayesian optimization to minimize the number of \nqueries, and find that the number of queries can be reduced to approximately \none tenth of the number needed through a random strategy for scenarios where \nthe feature modification cost budget is low. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a049", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08713"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Qingjiu Zhang, Shiliang Sun", "title": "Weighted Data Normalization Based on Eigenvalues for Artificial Neural Network Classification. (arXiv:1712.08885v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08885", "type": "text/html"}], "timestampUsec": "1514352521979600", "comments": [], "summary": {"content": "<p>Artificial neural network (ANN) is a very useful tool in solving learning \nproblems. Boosting the performances of ANN can be mainly concluded from two \naspects: optimizing the architecture of ANN and normalizing the raw data for \nANN. In this paper, a novel method which improves the effects of ANN by \npreprocessing the raw data is proposed. It totally leverages the fact that \ndifferent features should play different roles. The raw data set is firstly \npreprocessed by principle component analysis (PCA), and then its principle \ncomponents are weighted by their corresponding eigenvalues. Several aspects of \nanalysis are carried out to analyze its theory and the applicable occasions. \nThree classification problems are launched by an active learning algorithm to \nverify the proposed method. From the empirical results, conclusion comes to the \nfact that the proposed method can significantly improve the performance of ANN. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a059", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08885"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1514524597, "author": "Itay Safran, Ohad Shamir", "title": "Spurious Local Minima are Common in Two-Layer ReLU Neural Networks. (arXiv:1712.08968v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.08968", "type": "text/html"}], "timestampUsec": "1514352521979599", "comments": [], "summary": {"content": "<p>We consider the optimization problem associated with training simple ReLU \nneural networks of the form $\\mathbf{x}\\mapsto \n\\sum_{i=1}^{k}\\max\\{0,\\mathbf{w}_i^\\top \\mathbf{x}\\}$ with respect to the \nsquared loss. We provide a computer-assisted proof that even if the input \ndistribution is standard Gaussian, even if the dimension is unrestricted, and \neven if the target values are generated by such a network, with orthonormal \nparameter vectors, the problem can still have spurious local minima once $k\\geq \n6$. By a continuity argument, this implies that in high dimensions, \n\\emph{nearly all} target networks of the relevant sizes lead to spurious local \nminima. Moreover, we conduct experiments which show that the probability of \nhitting such local minima is quite high, and increasing with the network size. \nOn the positive side, mild over-parameterization appears to drastically reduce \nsuch local minima, indicating that an over-parameterization assumption is \nnecessary to get a positive result in this setting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514524597, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a06a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08968"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Debdeep Pati, Anirban Bhattacharya, Yun Yang", "title": "On Statistical Optimality of Variational Bayes. (arXiv:1712.08983v1 [math.ST])", "alternate": [{"href": "http://arxiv.org/abs/1712.08983", "type": "text/html"}], "timestampUsec": "1514352521979598", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329a8858d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329a8858d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The article addresses a long-standing open problem on the justification of \nusing variational Bayes methods for parameter estimation. We provide general \nconditions for obtaining optimal risk bounds for point estimates acquired from \nmean-field variational Bayesian inference. The conditions pertain to the \nexistence of certain test functions for the distance metric on the parameter \nspace and minimal assumptions on the prior. A general recipe for verification \nof the conditions is outlined which is broadly applicable to existing Bayesian \nmodels with or without latent variables. As illustrations, specific \napplications to Latent Dirichlet Allocation and Gaussian mixture models are \ndiscussed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a075", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08983"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rongqing Huang, Shiliang Sun", "title": "Kernel Regression with Sparse Metric Learning. (arXiv:1712.09001v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.09001", "type": "text/html"}], "timestampUsec": "1514352521979597", "comments": [], "summary": {"content": "<p>Kernel regression is a popular non-parametric fitting technique. It aims at \nlearning a function which estimates the targets for test inputs as precise as \npossible. Generally, the function value for a test input is estimated by a \nweighted average of the surrounding training examples. The weights are \ntypically computed by a distance-based kernel function and they strongly depend \non the distances between examples. In this paper, we first review the latest \ndevelopments of sparse metric learning and kernel regression. Then a novel \nkernel regression method involving sparse metric learning, which is called \nkernel regression with sparse metric learning (KR$\\_$SML), is proposed. The \nsparse kernel regression model is established by enforcing a mixed $(2,1)$-norm \nregularization over the metric matrix. It learns a Mahalanobis distance metric \nby a gradient descent procedure, which can simultaneously conduct \ndimensionality reduction and lead to good prediction results. Our work is the \nfirst to combine kernel regression with sparse metric learning. To verify the \neffectiveness of the proposed method, it is evaluated on 19 data sets for \nregression. Furthermore, the new method is also applied to solving practical \nproblems of forecasting short-term traffic flows. In the end, we compare the \nproposed method with other three related kernel regression methods on all test \ndata sets under two criterions. Experimental results show that the proposed \nmethod is much more competitive. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a07e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09001"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Qibing Li, Xiaolin Zheng", "title": "Deep Collaborative Autoencoder for Recommender Systems: A Unified Framework for Explicit and Implicit Feedback. (arXiv:1712.09043v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.09043", "type": "text/html"}], "timestampUsec": "1514352521979596", "comments": [], "summary": {"content": "<p>In recent years, deep neural networks have yielded state-of-the-art \nperformance on several tasks. Although some recent works have focused on \ncombining deep learning with recommendation, we highlight three issues of \nexisting works. First, most works perform deep content feature learning and \nresort to matrix factorization, which cannot effectively model the highly \ncomplex user-item interaction function. Second, due to the difficulty on \ntraining deep neural networks, existing models utilize a shallow architecture, \nand thus limit the expressiveness potential of deep learning. Third, neural \nnetwork models are easy to overfit on the implicit setting, because negative \ninteractions are not taken into account. To tackle these issues, we present a \nnovel recommender framework called Deep Collaborative Autoencoder (DCAE) for \nboth explicit feedback and implicit feedback, which can effectively capture the \nrelationship between interactions via its non-linear expressiveness. To \noptimize the deep architecture of DCAE, we develop a three-stage pre-training \nmechanism that combines supervised and unsupervised feature learning. Moreover, \nwe propose a popularity-based error reweighting module and a sparsity-aware \ndata-augmentation strategy for DCAE to prevent overfitting on the implicit \nsetting. Extensive experiments on three real-world datasets demonstrate that \nDCAE can significantly advance the state-of-the-art. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a07f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09043"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Bai Li, Changyou Chen, Hao Liu, Lawrence Carin", "title": "On Connecting Stochastic Gradient MCMC and Differential Privacy. (arXiv:1712.09097v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.09097", "type": "text/html"}], "timestampUsec": "1514352521979595", "comments": [], "summary": {"content": "<p>Significant success has been realized recently on applying machine learning \nto real-world applications. There have also been corresponding concerns on the \nprivacy of training data, which relates to data security and confidentiality \nissues. Differential privacy provides a principled and rigorous privacy \nguarantee on machine learning models. While it is common to design a model \nsatisfying a required differential-privacy property by injecting noise, it is \ngenerally hard to balance the trade-off between privacy and utility. We show \nthat stochastic gradient Markov chain Monte Carlo (SG-MCMC) -- a class of \nscalable Bayesian posterior sampling algorithms proposed recently -- satisfies \nstrong differential privacy with carefully chosen step sizes. We develop theory \non the performance of the proposed differentially-private SG-MCMC method. We \nconduct experiments to support our analysis and show that a standard SG-MCMC \nsampler without any modification (under a default setting) can reach \nstate-of-the-art performance in terms of both privacy and utility on Bayesian \nlearning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a081", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09097"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Romain Cosentino, Randall Balestriero, Richard Baraniuk, Ankit Patel", "title": "Overcomplete Frame Thresholding for Acoustic Scene Analysis. (arXiv:1712.09117v1 [eess.AS])", "alternate": [{"href": "http://arxiv.org/abs/1712.09117", "type": "text/html"}], "timestampUsec": "1514352521979594", "comments": [], "summary": {"content": "<p>In this work, we derive a generic overcomplete frame thresholding scheme \nbased on risk minimization. Overcomplete frames being favored for analysis \ntasks such as classification, regression or anomaly detection, we provide a way \nto leverage those optimal representations in real-world applications through \nthe use of thresholding. We validate the method on a large scale bird activity \ndetection task via the scattering network architecture performed by means of \ncontinuous wavelets, known for being an adequate dictionary in audio \nenvironments. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a086", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09117"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shameem A Puthiya Parambath, Nishant Vijayakumar, Sanjay Chawla", "title": "SAGA: A Submodular Greedy Algorithm For Group Recommendation. (arXiv:1712.09123v1 [cs.IR])", "alternate": [{"href": "http://arxiv.org/abs/1712.09123", "type": "text/html"}], "timestampUsec": "1514352521979593", "comments": [], "summary": {"content": "<p>In this paper, we propose a unified framework and an algorithm for the \nproblem of group recommendation where a fixed number of items or alternatives \ncan be recommended to a group of users. The problem of group recommendation \narises naturally in many real world contexts, and is closely related to the \nbudgeted social choice problem studied in economics. We frame the group \nrecommendation problem as choosing a subgraph with the largest group consensus \nscore in a completely connected graph defined over the item affinity matrix. We \npropose a fast greedy algorithm with strong theoretical guarantees, and show \nthat the proposed algorithm compares favorably to the state-of-the-art group \nrecommendation algorithms according to commonly used relevance and coverage \nperformance measures on benchmark dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a08c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09123"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville", "title": "Improved Training of Wasserstein GANs. (arXiv:1704.00028v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.00028", "type": "text/html"}], "timestampUsec": "1514352521979592", "comments": [], "summary": {"content": "<p>Generative Adversarial Networks (GANs) are powerful generative models, but \nsuffer from training instability. The recently proposed Wasserstein GAN (WGAN) \nmakes progress toward stable training of GANs, but sometimes can still generate \nonly low-quality samples or fail to converge. We find that these problems are \noften due to the use of weight clipping in WGAN to enforce a Lipschitz \nconstraint on the critic, which can lead to undesired behavior. We propose an \nalternative to clipping weights: penalize the norm of gradient of the critic \nwith respect to its input. Our proposed method performs better than standard \nWGAN and enables stable training of a wide variety of GAN architectures with \nalmost no hyperparameter tuning, including 101-layer ResNets and language \nmodels over discrete data. We also achieve high quality generations on CIFAR-10 \nand LSUN bedrooms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a08f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.00028"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Filipe Rodrigues, Francisco Pereira", "title": "Deep learning from crowds. (arXiv:1709.01779v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.01779", "type": "text/html"}], "timestampUsec": "1514352521979591", "comments": [], "summary": {"content": "<p>Over the last few years, deep learning has revolutionized the field of \nmachine learning by dramatically improving the state-of-the-art in various \ndomains. However, as the size of supervised artificial neural networks grows, \ntypically so does the need for larger labeled datasets. Recently, crowdsourcing \nhas established itself as an efficient and cost-effective solution for labeling \nlarge sets of data in a scalable manner, but it often requires aggregating \nlabels from multiple noisy contributors with different levels of expertise. In \nthis paper, we address the problem of learning deep neural networks from \ncrowds. We begin by describing an EM algorithm for jointly learning the \nparameters of the network and the reliabilities of the annotators. Then, a \nnovel general-purpose crowd layer is proposed, which allows us to train deep \nneural networks end-to-end, directly from the noisy labels of multiple \nannotators, using only backpropagation. We empirically show that the proposed \napproach is able to internally capture the reliability and biases of different \nannotators and achieve new state-of-the-art results for various crowdsourced \ndatasets across different settings, namely classification, regression and \nsequence labeling. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a091", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.01779"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ji Chen, Xiaodong Li", "title": "Memory-efficient Kernel PCA via Partial Matrix Sampling and Nonconvex Optimization: a Model-free Analysis of Local Minima. (arXiv:1711.01742v2 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.01742", "type": "text/html"}], "timestampUsec": "1514352521979590", "comments": [], "summary": {"content": "<p>Kernel PCA is a widely used nonlinear dimension reduction technique in \nmachine learning, but storing the kernel matrix is notoriously challenging when \nthe sample size is large. Inspired by Yi et al. [2016], where the idea of \npartial matrix sampling followed by nonconvex optimization is proposed for \nmatrix completion and robust PCA, we apply a similar approach to \nmemory-efficient Kernel PCA. In theory, with no assumptions on the kernel \nmatrix in terms of eigenvalues or eigenvectors, we established a model-free \ntheory for the low-rank approximation based on any local minimum of the \nproposed objective function. As interesting byproducts, when the underlying \npositive semidefinite matrix is assumed to be low-rank and highly structured, \ncorollaries of our main theorem improve the state-of-the-art results of Ge et \nal. [2016, 2017] for nonconvex matrix completion with no spurious local minima. \nNumerical experiments also show that our approach is competitive in terms of \napproximation accuracy compared to the well-known Nystr\\\"{o}m algorithm for \nKernel PCA. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a092", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.01742"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Katya Gonina, Navdeep Jaitly, Bo Li, Jan Chorowski, Michiel Bacchiani", "title": "State-of-the-art Speech Recognition With Sequence-to-Sequence Models. (arXiv:1712.01769v4 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.01769", "type": "text/html"}], "timestampUsec": "1514352521979589", "comments": [], "summary": {"content": "<p>Attention-based encoder-decoder architectures such as Listen, Attend, and \nSpell (LAS), subsume the acoustic, pronunciation and language model components \nof a traditional automatic speech recognition (ASR) system into a single neural \nnetwork. In our previous work, we have shown that such architectures are \ncomparable to state-of-the-art ASR systems on dictation tasks, but it was not \nclear if such architectures would be practical for more challenging tasks such \nas voice search. In this work, we explore a variety of structural and \noptimization improvements to our LAS model which significantly improve \nperformance. On the structural side, we show that word piece models can be used \ninstead of graphemes. We introduce a multi-head attention architecture, which \noffers improvements over the commonly-used single-head attention. On the \noptimization side, we explore techniques such as synchronous training, \nscheduled sampling, label smoothing, and minimum word error rate optimization, \nwhich are all shown to improve accuracy. We present results with a \nunidirectional LSTM encoder for streaming recognition. On a 12,500 hour voice \nsearch task, we find that the proposed changes improve the WER of the LAS \nsystem from 9.2% to 5.6%, while the best conventional system achieve 6.7% WER. \nWe also test both models on a dictation dataset, and our model provide 4.1% WER \nwhile the conventional system provides 5% WER. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a096", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01769"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Greg Yang, Samuel S. Schoenholz", "title": "Mean Field Residual Networks: On the Edge of Chaos. (arXiv:1712.08969v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.08969", "type": "text/html"}], "timestampUsec": "1514351435275181", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329a88895\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329a88895&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We study randomly initialized residual networks using mean field theory and \nthe theory of difference equations. Classical feedforward neural networks, such \nas those with tanh activations, exhibit exponential behavior on the average \nwhen propagating inputs forward or gradients backward. The exponential forward \ndynamics causes rapid collapsing of the input space geometry, while the \nexponential backward dynamics causes drastic vanishing or exploding gradients. \nWe show, in contrast, that by adding skip connections, the network will, \ndepending on the nonlinearity, adopt subexponential forward and backward \ndynamics, and in many cases in fact polynomial. The exponents of these \npolynomials are obtained through analytic methods and proved and verified \nempirically to be correct. In terms of the \"edge of chaos\" hypothesis, these \nsubexponential and polynomial laws allow residual networks to \"hover over the \nboundary between stability and chaos,\" thus preserving the geometry of the \ninput space and the gradient information flow. In our experiments, for each \nactivation function we study here, we initialize residual networks with \ndifferent hyperparameters and train them on MNIST. Remarkably, our \ninitialization time theory can accurately predict test time performance of \nthese networks, by tracking either the expected amount of gradient explosion or \nthe expected squared distance between the images of two input vectors. \nImportantly, we show, theoretically as well as empirically, that common \ninitializations such as the Xavier or the He schemes are not optimal for \nresidual networks, because the optimal initialization variances depend on the \ndepth. Finally, we have made mathematical contributions by deriving several new \nidentities for the kernels of powers of ReLU functions by relating them to the \nzeroth Bessel function of the second kind. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda10", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08969"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "M. J. Gagen", "title": "Null Dynamical State Models of Human Cognitive Dysfunction. (arXiv:1712.09014v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.09014", "type": "text/html"}], "timestampUsec": "1514351435275180", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329acfb88\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329acfb88&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The hard problem in artificial intelligence asks how the shuffling of \nsyntactical symbols in a program can lead to systems which experience semantics \nand qualia. We address this question in three stages. First, we introduce a new \nclass of human semantic symbols which appears when unexpected and drastic \nenvironmental change causes humans to become surprised, confused, uncertain, \nand in extreme cases, unresponsive, passive and dysfunctional. For this class \nof symbols, pre-learned programs become inoperative so these syntactical \nprograms cannot be the source of experienced qualia. Second, we model the \ndysfunctional human response to a radically changed environment as being the \nnatural response of any learning machine facing novel inputs from well outside \nits previous training set. In this situation, learning machines are unable to \nextract information from their input and will typically enter a dynamical state \ncharacterized by null outputs and a lack of response. This state immediately \npredicts and explains the characteristics of the semantic experiences of humans \nin similar circumstances. In the third stage, we consider learning machines \ntrained to implement multiple functions in simple sequential programs using \nenvironmental data to specify subroutine names, control flow instructions, \nmemory calls, and so on. Drastic change in any of these environmental inputs \ncan again lead to inoperative programs. By examining changes specific to people \nor locations we can model human cognitive symbols featuring these dependencies, \nsuch as attachment and grief. Our approach links known dynamical machines \nstates with human qualia and thus offers new insight into the hard problem of \nartificial intelligence. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda13", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09014"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516253548, "author": "Priyadarshini Panda, Kaushik Roy", "title": "Chaos-guided Input Structuring for Improved Learning in Recurrent Neural Networks. (arXiv:1712.09206v2 [q-bio.NC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.09206", "type": "text/html"}], "timestampUsec": "1514351435275179", "comments": [], "summary": {"content": "<p>Anatomical studies demonstrate that brain reformats input information to \ngenerate reliable responses for performing computations. However, it remains \nunclear how neural circuits encode complex spatio-temporal patterns. We show \nthat neural dynamics are strongly influenced by the phase alignment between the \ninput and the spontaneous chaotic activity. Input structuring along the \ndominant chaotic projections causes the chaotic trajectories to become stable \nchannels (or attractors), hence, improving the computational capability of a \nrecurrent network. Using mean field analysis, we derive the impact of input \nstructuring on the overall stability of attractors formed. Our results indicate \nthat input alignment determines the extent of intrinsic noise suppression and \nhence, alters the attractor state stability, thereby controlling the network's \ninference ability. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda17", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09206"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kenji Kawaguchi, Leslie Pack Kaelbling, Yoshua Bengio", "title": "Generalization in Deep Learning. (arXiv:1710.05468v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.05468", "type": "text/html"}], "timestampUsec": "1514351435275178", "comments": [], "summary": {"content": "<p>This paper explains why deep learning can generalize well, despite large \ncapacity and possible algorithmic instability, nonrobustness, and sharp minima, \neffectively addressing an open problem in the literature. Based on our \ntheoretical insight, this paper also proposes a family of new regularization \nmethods. Its simplest member was empirically shown to improve base models and \nachieve competitive performance on MNIST and CIFAR-10 benchmarks. Moreover, \nthis paper presents both data-dependent and data-independent generalization \nguarantees with improved convergence rates. Our results suggest several new \nopen areas of research. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.05468"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Wentao Zhu, Xiang Xiang, Trac D. Tran, Gregory D. Hager, Xiaohui Xie", "title": "Adversarial Deep Structured Nets for Mass Segmentation from Mammograms. (arXiv:1710.09288v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.09288", "type": "text/html"}], "timestampUsec": "1514351435275177", "comments": [], "summary": {"content": "<p>Mass segmentation provides effective morphological features which are \nimportant for mass diagnosis. In this work, we propose a novel end-to-end \nnetwork for mammographic mass segmentation which employs a fully convolutional \nnetwork (FCN) to model a potential function, followed by a CRF to perform \nstructured learning. Because the mass distribution varies greatly with pixel \nposition, the FCN is combined with a position priori. Further, we employ \nadversarial training to eliminate over-fitting due to the small sizes of \nmammogram datasets. Multi-scale FCN is employed to improve the segmentation \nperformance. Experimental results on two public datasets, INbreast and \nDDSM-BCRP, demonstrate that our end-to-end network achieves better performance \nthan state-of-the-art approaches. \n\\footnote{https://github.com/wentaozhu/adversarial-deep-structural-networks.git} \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda1f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.09288"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Fattaneh Jabbari, Mahdi Pakdaman Naeini, Gregory F. Cooper", "title": "Obtaining Accurate Probabilistic Causal Inference by Post-Processing Calibration. (arXiv:1712.08626v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08626", "type": "text/html"}], "timestampUsec": "1514351435275176", "comments": [], "summary": {"content": "<p>Discovery of an accurate causal Bayesian network structure from observational \ndata can be useful in many areas of science. Often the discoveries are made \nunder uncertainty, which can be expressed as probabilities. To guide the use of \nsuch discoveries, including directing further investigation, it is important \nthat those probabilities be well-calibrated. In this paper, we introduce a \nnovel framework to derive calibrated probabilities of causal relationships from \nobservational data. The framework consists of three components: (1) an \napproximate method for generating initial probability estimates of the edge \ntypes for each pair of variables, (2) the availability of a relatively small \nnumber of the causal relationships in the network for which the truth status is \nknown, which we call a calibration training set, and (3) a calibration method \nfor using the approximate probability estimates and the calibration training \nset to generate calibrated probabilities for the many remaining pairs of \nvariables. We also introduce a new calibration method based on a shallow neural \nnetwork. Our experiments on simulated data support that the proposed approach \nimproves the calibration of causal edge predictions. The results also support \nthat the approach often improves the precision and recall of predictions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda21", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08626"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alexander Trott, Caiming Xiong, Richard Socher", "title": "Interpretable Counting for Visual Question Answering. (arXiv:1712.08697v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08697", "type": "text/html"}], "timestampUsec": "1514351435275175", "comments": [], "summary": {"content": "<p>Questions that require counting a variety of objects in images remain a major \nchallenge in visual question answering (VQA). The most common approaches to VQA \ninvolve either classifying answers based on fixed length representations of \nboth the image and question or summing fractional counts estimated from each \nsection of the image. In contrast, we treat counting as a sequential decision \nprocess and force our model to make discrete choices of what to count. \nSpecifically, the model sequentially selects from detected objects and learns \ninteractions between objects that influence subsequent selections. A \ndistinction of our approach is its intuitive and interpretable output, as \ndiscrete counts are automatically grounded in the image. Furthermore, our \nmethod outperforms the state of the art architecture for VQA on multiple \nmetrics that evaluate counting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda27", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08697"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tom Hanika, Jens Zumbr&#xe4;gel", "title": "Towards Collaborative Conceptual Exploration. (arXiv:1712.08858v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08858", "type": "text/html"}], "timestampUsec": "1514351435275174", "comments": [], "summary": {"content": "<p>In domains with high knowledge distribution a natural objective is to create \nprinciple foundations for collaborative interactive learning environments. We \npresent a first mathematical characterization of a collaborative learning \ngroup, a consortium, based on closure systems of attribute sets and the \nwell-known attribute exploration algorithm from formal concept analysis. To \nthis end, we introduce (weak) local experts for subdomains of a given knowledge \ndomain. These entities are able to refute and potentially accept a given \n(implicational) query for some closure system that is a restriction of the \nwhole domain. On this we build up a consortial expert and show first insights \nabout the ability of such an expert to answer queries. Furthermore, we depict \ntechniques on how to cope with falsely accepted implications and on combining \ncounterexamples. Using notions from combinatorial design theory we further \nexpand those insights as far as providing first results on the decidability \nproblem if a given consortium is able to explore some target domain. \nApplications in conceptual knowledge acquisition as well as in collaborative \ninteractive ontology learning are at hand. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda29", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08858"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Meng Wang, Yihe Chen, Buyue Qian, Jun Liu, Sen Wang, Guodong Long, Fei Wang", "title": "Predicting Rich Drug-Drug Interactions via Biomedical Knowledge Graphs and Text Jointly Embedding. (arXiv:1712.08875v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08875", "type": "text/html"}], "timestampUsec": "1514351435275173", "comments": [], "summary": {"content": "<p>Minimizing adverse reactions caused by drug-drug interactions has always been \na momentous research topic in clinical pharmacology. Detecting all possible \ninteractions through clinical studies before a drug is released to the market \nis a demanding task. The power of big data is opening up new approaches to \ndiscover various drug-drug interactions. However, these discoveries contain a \nhuge amount of noise and provide knowledge bases far from complete and \ntrustworthy ones to be utilized. Most existing studies focus on predicting \nbinary drug-drug interactions between drug pairs but ignore other interactions. \nIn this paper, we propose a novel framework, called PRD, to predict drug-drug \ninteractions. The framework uses the graph embedding that can overcome data \nincompleteness and sparsity issues to achieve multiple DDI label prediction. \nFirst, a large-scale drug knowledge graph is generated from different sources. \nThen, the knowledge graph is embedded with comprehensive biomedical text into a \ncommon low dimensional space. Finally, the learned embeddings are used to \nefficiently compute rich DDI information through a link prediction process. To \nvalidate the effectiveness of the proposed framework, extensive experiments \nwere conducted on real-world datasets. The results demonstrate that our model \noutperforms several state-of-the-art baseline methods in terms of capability \nand accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda2d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08875"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alan F. T. Winfield", "title": "How Intelligent is your Intelligent Robot?. (arXiv:1712.08878v1 [cs.RO])", "alternate": [{"href": "http://arxiv.org/abs/1712.08878", "type": "text/html"}], "timestampUsec": "1514351435275172", "comments": [], "summary": {"content": "<p>How intelligent is robot A compared with robot B? And how intelligent are \nrobots A and B compared with animals (or plants) X and Y? These are both \ninteresting and deeply challenging questions. In this paper we address the \nquestion \"how intelligent is your intelligent robot?\" by proposing that \nembodied intelligence emerges from the interaction and integration of four \ndifferent and distinct kinds of intelligence. We then suggest a simple \ndiagrammatic representation on which these kinds of intelligence are shown as \nfour axes in a star diagram. A crude qualitative comparison of the intelligence \ngraphs of animals and robots both exposes and helps to explain the chronic \nintelligence deficit of intelligent robots. Finally we examine the options for \ndetermining numerical values for the four kinds of intelligence in an effort to \nmove toward a quantifiable intelligence vector. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda30", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08878"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Shiliang Sun, Changshui Zhang, Yi Zhang", "title": "Traffic Flow Forecasting Using a Spatio-Temporal Bayesian Network Predictor. (arXiv:1712.08883v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08883", "type": "text/html"}], "timestampUsec": "1514351435275171", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329ad00da\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329ad00da&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A novel predictor for traffic flow forecasting, namely spatio-temporal \nBayesian network predictor, is proposed. Unlike existing methods, our approach \nincorporates all the spatial and temporal information available in a \ntransportation network to carry our traffic flow forecasting of the current \nsite. The Pearson correlation coefficient is adopted to rank the input \nvariables (traffic flows) for prediction, and the best-first strategy is \nemployed to select a subset as the cause nodes of a Bayesian network. Given the \nderived cause nodes and the corresponding effect node in the spatio-temporal \nBayesian network, a Gaussian Mixture Model is applied to describe the \nstatistical relationship between the input and output. Finally, traffic flow \nforecasting is performed under the criterion of Minimum Mean Square Error \n(M.M.S.E.). Experimental results with the urban vehicular flow data of Beijing \ndemonstrate the effectiveness of our presented spatio-temporal Bayesian network \npredictor. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda33", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08883"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "ElMouatez Billah Karbab, Mourad Debbabi, Abdelouahid Derhab, Djedjiga Mouheb", "title": "Android Malware Detection using Deep Learning on API Method Sequences. (arXiv:1712.08996v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1712.08996", "type": "text/html"}], "timestampUsec": "1514351435275170", "comments": [], "summary": {"content": "<p>Android OS experiences a blazing popularity since the last few years. This \npredominant platform has established itself not only in the mobile world but \nalso in the Internet of Things (IoT) devices. This popularity, however, comes \nat the expense of security, as it has become a tempting target of malicious \napps. Hence, there is an increasing need for sophisticated, automatic, and \nportable malware detection solutions. In this paper, we propose MalDozer, an \nautomatic Android malware detection and family attribution framework that \nrelies on sequences classification using deep learning techniques. Starting \nfrom the raw sequence of the app's API method calls, MalDozer automatically \nextracts and learns the malicious and the benign patterns from the actual \nsamples to detect Android malware. MalDozer can serve as a ubiquitous malware \ndetection system that is not only deployed on servers, but also on mobile and \neven IoT devices. We evaluate MalDozer on multiple Android malware datasets \nranging from 1K to 33K malware apps, and 38K benign apps. The results show that \nMalDozer can correctly detect malware and attribute them to their actual \nfamilies with an F1-Score of 96%-99% and a false positive rate of 0.06%-2%, \nunder all tested datasets and settings. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08996"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Luis M. Briceno-Arias, Giovanni Chierchia, Emilie Chouzenoux, Jean-Christophe Pesquet", "title": "A Random Block-Coordinate Douglas-Rachford Splitting Method with Low Computational Complexity for Binary Logistic Regression. (arXiv:1712.09131v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1712.09131", "type": "text/html"}], "timestampUsec": "1514351435275169", "comments": [], "summary": {"content": "<p>In this paper, we propose a new optimization algorithm for sparse logistic \nregression based on a stochastic version of the Douglas-Rachford splitting \nmethod. Our algorithm sweeps the training set by randomly selecting a \nmini-batch of data at each iteration, and it allows us to update the variables \nin a block coordinate manner. Our approach leverages the proximity operator of \nthe logistic loss, which is expressed with the generalized Lambert W function. \nExperiments carried out on standard datasets demonstrate the efficiency of our \napproach w.r.t. stochastic gradient-like methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda38", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09131"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Liat Cohen, Solomon Eyal Shimony, Gera Weiss", "title": "Estimating the Probability of Meeting a Deadline in Hierarchical Plans. (arXiv:1503.01327v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1503.01327", "type": "text/html"}], "timestampUsec": "1514351435275168", "comments": [], "summary": {"content": "<p>Given a hierarchical plan (or schedule) with uncertain task times, we propose \na deterministic polynomial (time and memory) algorithm for estimating the \nprobability that its meets a deadline, or, alternately, that its {\\em makespan} \nis less than a given duration. Approximation is needed as it is known that this \nproblem is NP-hard even for sequential plans (just, a sum of random variables). \nIn addition, we show two new complexity results: (1) Counting the number of \nevents that do not cross deadline is \\#P-hard; (2)~Computing the expected \nmakespan of a hierarchical plan is NP-hard. For the proposed approximation \nalgorithm, we establish formal approximation bounds and show that the time and \nmemory complexities grow polynomially with the required accuracy, the number of \nnodes in the plan, and with the size of the support of the random variables \nthat represent the durations of the primitive tasks. We examine these \napproximation bounds empirically and demonstrate, using task networks taken \nfrom the literature, how our scheme outperforms sampling techniques and exact \ncomputation in terms of accuracy and run-time. As the empirical data shows much \nbetter error bounds than guaranteed, we also suggest a method for tightening \nthe bounds in some cases. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda39", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1503.01327"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Martin C. Cooper, Stanislav &#x17d;ivn&#xfd;", "title": "The Power of Arc Consistency for CSPs Defined by Partially-Ordered Forbidden Patterns. (arXiv:1604.07981v4 [cs.CC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1604.07981", "type": "text/html"}], "timestampUsec": "1514351435275167", "comments": [], "summary": {"content": "<p>Characterising tractable fragments of the constraint satisfaction problem \n(CSP) is an important challenge in theoretical computer science and artificial \nintelligence. Forbidding patterns (generic sub-instances) provides a means of \ndefining CSP fragments which are neither exclusively language-based nor \nexclusively structure-based. It is known that the class of binary CSP instances \nin which the broken-triangle pattern (BTP) does not occur, a class which \nincludes all tree-structured instances, are decided by arc consistency (AC), a \nubiquitous reduction operation in constraint solvers. We provide a \ncharacterisation of simple partially-ordered forbidden patterns which have this \nAC-solvability property. It turns out that BTP is just one of five such \nAC-solvable patterns. The four other patterns allow us to exhibit new tractable \nclasses. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda3f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1604.07981"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ali Jazayeri, Hiroki Sayama", "title": "A Polynomial-Time Deterministic Approach to the Traveling Salesperson Problem. (arXiv:1608.01716v3 [cs.DS] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1608.01716", "type": "text/html"}], "timestampUsec": "1514351435275166", "comments": [], "summary": {"content": "<p>We propose a new polynomial-time deterministic algorithm that produces an \napproximated solution for the traveling salesperson problem. The proposed \nalgorithm ranks cities based on their priorities calculated using a power \nfunction of means and standard deviations of their distances from other cities \nand then connects the cities to their neighbors in the order of their \npriorities. When connecting a city, a neighbor is selected based on their \nneighbors' priorities calculated as another power function that additionally \nincludes their distance from the focal city to be connected. This repeats until \nall the cities are connected into a single loop. The time complexity of the \nproposed algorithm is $O(n^2)$, where $n$ is the number of cities. Numerical \nevaluation shows that, despite its simplicity, the proposed algorithm produces \nshorter tours with less time complexity than other conventional tour \nconstruction heuristics. The proposed algorithm can be used by itself or as an \ninitial tour generator for other more complex heuristic optimization \nalgorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda44", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1608.01716"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Raef Bassily, Shay Moran, Ido Nachum, Jonathan Shafer, Amir Yehudayoff", "title": "Learners that Leak Little Information. (arXiv:1710.05233v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.05233", "type": "text/html"}], "timestampUsec": "1514351435275165", "comments": [], "summary": {"content": "<p>We study learning algorithms that are restricted to using a small amount of \ninformation from their input sample. We introduce a category of learning \nalgorithms we term d-bit information learners, which are algorithms whose \noutput conveys at most d bits of information on their input. A central theme in \nthis work is that such algorithms generalize. \n</p> \n<p>We focus on the learning capacity of these algorithms, and prove sample \ncomplexity bounds with tight dependencies on the confidence and error \nparameters. We also observe connections with well studied notions such as \nsample compression schemes, Occam's razor, PAC-Bayes and differential privacy. \n</p> \n<p>We discuss an approach that allows us to prove upper bounds on the amount of \ninformation that algorithms reveal about their inputs, and also provide a lower \nbound by showing a simple concept class for which every (possibly randomized) \nempirical risk minimizer must reveal a lot of information. On the other hand, \nwe show that in the distribution-dependent setting every VC class has empirical \nrisk minimizers that do not reveal a lot of information. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda45", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.05233"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sandra Wachter, Brent Mittelstadt, Chris Russell", "title": "Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR. (arXiv:1711.00399v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.00399", "type": "text/html"}], "timestampUsec": "1514351435275164", "comments": [], "summary": {"content": "<p>There has been much discussion of the right to explanation in the EU General \nData Protection Regulation, and its existence, merits, and disadvantages. \nImplementing a right to explanation that opens the black box of algorithmic \ndecision-making faces major legal and technical barriers. Explaining the \nfunctionality of complex algorithmic decision-making systems and their \nrationale in specific cases is a technically challenging problem. Some \nexplanations may offer little meaningful information to data subjects, raising \nquestions around their value. Explanations of automated decisions need not \nhinge on the general public understanding how algorithmic systems function. \nEven though such interpretability is of great importance and should be pursued, \nexplanations can, in principle, be offered without opening the black box. \nLooking at explanations as a means to help a data subject act rather than \nmerely understand, one could gauge the scope and content of explanations \naccording to the specific goal or action they are intended to support. From the \nperspective of individuals affected by automated decision-making, we propose \nthree aims for explanations: (1) to inform and help the individual understand \nwhy a particular decision was reached, (2) to provide grounds to contest the \ndecision if the outcome is undesired, and (3) to understand what would need to \nchange in order to receive a desired result in the future, based on the current \ndecision-making model. We assess how each of these goals finds support in the \nGDPR. We suggest data controllers should offer a particular type of \nexplanation, unconditional counterfactual explanations, to support these three \naims. These counterfactual explanations describe the smallest change to the \nworld that can be made to obtain a desirable outcome, or to arrive at the \nclosest possible world, without needing to explain the internal logic of the \nsystem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda48", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.00399"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Stephen Tu, Benjamin Recht", "title": "Least-Squares Temporal Difference Learning for the Linear Quadratic Regulator. (arXiv:1712.08642v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08642", "type": "text/html"}], "timestampUsec": "1514351435275163", "comments": [], "summary": {"content": "<p>Reinforcement learning (RL) has been successfully used to solve many \ncontinuous control tasks. Despite its impressive results however, fundamental \nquestions regarding the sample complexity of RL on continuous problems remain \nopen. We study the performance of RL in this setting by considering the \nbehavior of the Least-Squares Temporal Difference (LSTD) estimator on the \nclassic Linear Quadratic Regulator (LQR) problem from optimal control. We give \nthe first finite-time analysis of the number of samples needed to estimate the \nvalue function for a fixed static state-feedback policy to within \n$\\varepsilon$-relative error. In the process of deriving our result, we give a \ngeneral characterization for when the minimum eigenvalue of the empirical \ncovariance matrix formed along the sample path of a fast-mixing stochastic \nprocess concentrates above zero, extending a result by Koltchinskii and \nMendelson in the independent covariates setting. Finally, we provide \nexperimental evidence indicating that our analysis correctly captures the \nqualitative behavior of LSTD on several LQR instances. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda4f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08642"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516080170, "author": "Michael Bianco, Peter Gerstoft", "title": "Sparse travel time tomography with adaptive dictionaries. (arXiv:1712.08655v2 [physics.geo-ph] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.08655", "type": "text/html"}], "timestampUsec": "1514351435275162", "comments": [], "summary": {"content": "<p>We develop a 2D travel time tomography method which regularizes the inversion \nby modeling groups of slowness pixels from discrete slowness maps, called \npatches, as sparse linear combinations of atoms from a dictionary. We further \npropose to learn optimal slowness dictionaries using dictionary learning, in \nparallel with the inversion. This patch regularization, which we call the local \nmodel, is integrated into the overall slowness map, called the global model. \nWhere the local model considers small-scale variations using a sparsity \nconstraint, the global model considers larger-scale features which are \nconstrained using $\\ell_2$-norm regularization. This local-global modeling \nstrategy with dictionary learning has been successful for image restoration \ntasks such as denoising and inpainting, where diverse image content is \nrecovered from noisy or incomplete measurements. We use this strategy in our \nlocally-sparse travel time tomography (LST) approach to model simultaneously \nsmooth and discontinuous slowness features. This is in contrast to conventional \ntomography methods, which constrain models to be exclusively smooth or \ndiscontinuous. We develop a $\\textit{maximum a posteriori}$ formulation for LST \nand exploit the sparsity of slowness patches using dictionary learning. We \ndemonstrate the LST approach on densely, but irregularly sampled synthetic \nslowness maps. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda53", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08655"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michael P.B. Gallaugher, Paul D. McNicholas", "title": "Mixtures of Matrix Variate Bilinear Factor Analyzers. (arXiv:1712.08664v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.08664", "type": "text/html"}], "timestampUsec": "1514351435275161", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329ad0452\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329ad0452&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Over the years data is becoming increasingly higher dimensional, which has \nprompted an increased need for dimension reduction techniques, in particular \nfor clustering and classification. Although dimension reduction in the area of \nclustering for multivariate data has been thoroughly discussed in the \nliterature there is relatively little work in the area of three way (matrix \nvariate) data. Herein, we develop a mixture of matrix variate bilinear factor \nanalyzers (MMVBFA) model for use in clustering high dimensional matrix variate \ndata. Parameter estimation is discussed, and the MMVBFA model is illustrated \nusing simulated data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda58", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08664"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Siddique Latif, Rajib Rana, Junaid Qadir, Julien Epps", "title": "Variational Autoencoders for Learning Latent Representations of Speech Emotion. (arXiv:1712.08708v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1712.08708", "type": "text/html"}], "timestampUsec": "1514351435275160", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329b34a8f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329b34a8f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Latent representation of data in unsupervised fashion is a very interesting \nprocess. It provides more relevant features that can enhance the performance of \na classifier. For speech emotion recognition tasks generating effective \nfeatures is very crucial. Recently, deep generative models such as Variational \nAutoencoders (VAEs) have gained enormous success to model natural images. Being \ninspired by that in this paper, we use VAE for the modeling of emotions in \nhuman speech. We derive the latent representation of speech signal and use this \nfor classification of emotions. We demonstrate that features learned by VAEs \ncan achieve state-of-the-art emotion recognition results. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda59", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08708"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Hirofumi Ohta, Satoshi Hara", "title": "On Estimation of Conditional Modes Using Multiple Quantile Regressions. (arXiv:1712.08754v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08754", "type": "text/html"}], "timestampUsec": "1514351435275159", "comments": [], "summary": {"content": "<p>We propose an estimation method for the conditional mode when the \nconditioning variable is high-dimensional. In the proposed method, we first \nestimate the conditional density by solving quantile regressions multiple \ntimes. We then estimate the conditional mode by finding the maximum of the \nestimated conditional density. The proposed method has two advantages in that \nit is computationally stable because it has no initial parameter dependencies, \nand it is statistically efficient with a fast convergence rate. Synthetic and \nreal-world data experiments demonstrate the better performance of the proposed \nmethod compared to other existing ones. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda5c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08754"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Chao Chen, Xiao Lin, Gabriel Terejanu", "title": "An Approximate Bayesian Long Short-Term Memory Algorithm for Outlier Detection. (arXiv:1712.08773v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08773", "type": "text/html"}], "timestampUsec": "1514351435275158", "comments": [], "summary": {"content": "<p>Long Short-Term Memory networks trained with gradient descent and \nback-propagation have received great success in various applications. However, \npoint estimation of the weights of the networks is prone to over-fitting \nproblems and lacks important uncertainty information associated with the \nestimation. However, exact Bayesian neural network methods are intractable and \nnon-applicable for real-world applications. In this study, we propose an \napproximate estimation of the weights uncertainty using Ensemble Kalman Filter, \nwhich is easily scalable to a large number of weights. Furthermore, we optimize \nthe covariance of the noise distribution in the ensemble update step using \nmaximum likelihood estimation. To assess the proposed algorithm, we apply it to \noutlier detection in five real-world events retrieved from the Twitter \nplatform. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda62", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08773"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Anna D. Peterson, Arka P. Ghosh, Ranjan Maitra", "title": "Merging $K$-means with hierarchical clustering for identifying general-shaped groups. (arXiv:1712.08786v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08786", "type": "text/html"}], "timestampUsec": "1514351435275157", "comments": [], "summary": {"content": "<p>Clustering partitions a dataset such that observations placed together in a \ngroup are similar but different from those in other groups. Hierarchical and \n$K$-means clustering are two approaches but have different strengths and \nweaknesses. For instance, hierarchical clustering identifies groups in a \ntree-like structure but suffers from computational complexity in large datasets \nwhile $K$-means clustering is efficient but designed to identify homogeneous \nspherically-shaped clusters. We present a hybrid non-parametric clustering \napproach that amalgamates the two methods to identify general-shaped clusters \nand that can be applied to larger datasets. Specifically, we first partition \nthe dataset into spherical groups using $K$-means. We next merge these groups \nusing hierarchical methods with a data-driven distance measure as a stopping \ncriterion. Our proposal has the potential to reveal groups with general shapes \nand structure in a dataset. We demonstrate good performance on several \nsimulated and real datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda63", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08786"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Petros Drineas, Michael W. Mahoney", "title": "Lectures on Randomized Numerical Linear Algebra. (arXiv:1712.08880v1 [cs.DS])", "alternate": [{"href": "http://arxiv.org/abs/1712.08880", "type": "text/html"}], "timestampUsec": "1514351435275156", "comments": [], "summary": {"content": "<p>This chapter is based on lectures on Randomized Numerical Linear Algebra from \nthe 2016 Park City Mathematics Institute summer school on The Mathematics of \nData. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda67", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08880"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516686212, "author": "Ahmed M. Alaa, Mihaela van der Schaar", "title": "Bayesian Nonparametric Causal Inference: Information Rates and Learning Algorithms. (arXiv:1712.08914v2 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.08914", "type": "text/html"}], "timestampUsec": "1514351435275155", "comments": [], "summary": {"content": "<p>We investigate the problem of estimating the causal effect of a treatment on \nindividual subjects from observational data, this is a central problem in \nvarious application domains, including healthcare, social sciences, and online \nadvertising. Within the Neyman Rubin potential outcomes model, we use the \nKullback Leibler (KL) divergence between the estimated and true distributions \nas a measure of accuracy of the estimate, and we define the information rate of \nthe Bayesian causal inference procedure as the (asymptotic equivalence class of \nthe) expected value of the KL divergence between the estimated and true \ndistributions as a function of the number of samples. Using Fano method, we \nestablish a fundamental limit on the information rate that can be achieved by \nany Bayesian estimator, and show that this fundamental limit is independent of \nthe selection bias in the observational data. We characterize the Bayesian \npriors on the potential (factual and counterfactual) outcomes that achieve the \noptimal information rate. As a consequence, we show that a particular class of \npriors that have been widely used in the causal inference literature cannot \nachieve the optimal information rate. On the other hand, a broader class of \npriors can achieve the optimal information rate. We go on to propose a prior \nadaptation procedure (which we call the information based empirical Bayes \nprocedure) that optimizes the Bayesian prior by maximizing an information \ntheoretic criterion on the recovered causal effects rather than maximizing the \nmarginal likelihood of the observed (factual) data. Building on our analysis, \nwe construct an information optimal Bayesian causal inference algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1516686210, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda6d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08914"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "George C. Linderman, Manas Rachh, Jeremy G. Hoskins, Stefan Steinerberger, Yuval Kluger", "title": "Efficient Algorithms for t-distributed Stochastic Neighborhood Embedding. (arXiv:1712.09005v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.09005", "type": "text/html"}], "timestampUsec": "1514351435275154", "comments": [], "summary": {"content": "<p>t-distributed Stochastic Neighborhood Embedding (t-SNE) is a method for \ndimensionality reduction and visualization that has become widely popular in \nrecent years. Efficient implementations of t-SNE are available, but they scale \npoorly to datasets with hundreds of thousands to millions of high dimensional \ndata-points. We present Fast Fourier Transform-accelerated Interpolation-based \nt-SNE (FIt-SNE), which dramatically accelerates the computation of t-SNE. The \nmost time-consuming step of t-SNE is a convolution that we accelerate by \ninterpolating onto an equispaced grid and subsequently using the fast Fourier \ntransform to perform the convolution. We also optimize the computation of input \nsimilarities in high dimensions using multi-threaded approximate nearest \nneighbors. We further present a modification to t-SNE called \"late \nexaggeration,\" which allows for easier identification of clusters in t-SNE \nembeddings. Finally, for datasets that cannot be loaded into the memory, we \npresent out-of-core randomized principal component analysis (oocPCA), so that \nthe top principal components of a dataset can be computed without ever fully \nloading the matrix, hence allowing for t-SNE of large datasets to be computed \non resource-limited machines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda74", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09005"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "David Liau, Eric Price, Zhao Song, Ger Yang", "title": "Stochastic Multi-armed Bandits in Constant Space. (arXiv:1712.09007v1 [cs.DS])", "alternate": [{"href": "http://arxiv.org/abs/1712.09007", "type": "text/html"}], "timestampUsec": "1514351435275153", "comments": [], "summary": {"content": "<p>We consider the stochastic bandit problem in the sublinear space setting, \nwhere one cannot record the win-loss record for all $K$ arms. We give an \nalgorithm using $O(1)$ words of space with regret \\[ \n</p> \n<p>\\sum_{i=1}^{K}\\frac{1}{\\Delta_i}\\log \\frac{\\Delta_i}{\\Delta}\\log T \\] where \n$\\Delta_i$ is the gap between the best arm and arm $i$ and $\\Delta$ is the gap \nbetween the best and the second-best arms. If the rewards are bounded away from \n$0$ and $1$, this is within an $O(\\log 1/\\Delta)$ factor of the optimum regret \npossible without space constraints. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda7a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09007"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ruben Loaiza-Maya, Michael Stanley Smith", "title": "Variational Bayes Estimation of Time Series Copulas for Multivariate Ordinal and Mixed Data. (arXiv:1712.09150v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.09150", "type": "text/html"}], "timestampUsec": "1514351435275152", "comments": [], "summary": {"content": "<p>We propose a new variational Bayes method for estimating high-dimensional \ncopulas with discrete, or discrete and continuous, margins. The method is based \non a variational approximation to a tractable augmented posterior, and is \nsubstantially faster than previous likelihood-based approaches. We use it to \nestimate drawable vine copulas for univariate and multivariate Markov ordinal \nand mixed time series. These have dimension $rT$, where $T$ is the number of \nobservations and $r$ is the number of series, and are difficult to estimate \nusing previous methods. The vine pair-copulas are carefully selected to allow \nfor heteroskedasticity, which is a common feature of ordinal time series data. \nWhen combined with flexible margins, the resulting time series models also \nallow for other common features of ordinal data, such as zero inflation, \nmultiple modes and under- or over-dispersion. Using data on homicides in New \nSouth Wales, and also U.S bankruptcies, we illustrate both the flexibility of \nthe time series copula models, and the efficacy of the variational Bayes \nestimator for copulas of up to 792 dimensions and 60 parameters. This far \nexceeds the size and complexity of copula models for discrete data that can be \nestimated using previous methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda7d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09150"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Andrew Ilyas, Ajil Jalal, Eirini Asteri, Constantinos Daskalakis, Alexandros G. Dimakis", "title": "The Robust Manifold Defense: Adversarial Training using Generative Models. (arXiv:1712.09196v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.09196", "type": "text/html"}], "timestampUsec": "1514351435275151", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329b34d10\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329b34d10&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep neural networks are demonstrating excellent performance on several \nclassical vision problems. However, these networks are vulnerable to \nadversarial examples, minutely modified images that induce arbitrary \nattacker-chosen output from the network. We propose a mechanism to protect \nagainst these adversarial inputs based on a generative model of the data. We \nintroduce a pre-processing step that projects on the range of a generative \nmodel using gradient descent before feeding an input into a classifier. We show \nthat this step provides the classifier with robustness against first-order, \nsubstitute model, and combined adversarial attacks. Using a min-max \nformulation, we show that there may exist adversarial examples even in the \nrange of the generator, natural-looking images extremely close to the decision \nboundary for which the classifier has unjustifiedly high confidence. We show \nthat adversarial training on the generative manifold can be used to make a \nclassifier that is robust to these attacks. \n</p> \n<p>Finally, we show how our method can be applied even without a pre-trained \ngenerative model using a recent method called the deep image prior. We evaluate \nour method on MNIST, CelebA and Imagenet and show robustness against the \ncurrent state of the art attacks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda82", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09196"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yenisel Plasencia-Cala&#xf1;a, Mauricio Orozco-Alzate, Heydi M&#xe9;ndez-V&#xe1;zquez, Edel Garc&#xed;a-Reyes, Robert P.W. Duin", "title": "Scalable Prototype Selection by Genetic Algorithms and Hashing. (arXiv:1712.09277v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.09277", "type": "text/html"}], "timestampUsec": "1514351435275150", "comments": [], "summary": {"content": "<p>Classification in the dissimilarity space has become a very active research \narea since it provides a possibility to learn from data given in the form of \npairwise non-metric dissimilarities, which otherwise would be difficult to cope \nwith. The selection of prototypes is a key step for the further creation of the \nspace. However, despite previous efforts to find good prototypes, how to select \nthe best representation set remains an open issue. In this paper we proposed \nscalable methods to select the set of prototypes out of very large datasets. \nThe methods are based on genetic algorithms, dissimilarity-based hashing, and \ntwo different unsupervised and supervised scalable criteria. The unsupervised \ncriterion is based on the Minimum Spanning Tree of the graph created by the \nprototypes as nodes and the dissimilarities as edges. The supervised criterion \nis based on counting matching labels of objects and their closest prototypes. \nThe suitability of these type of algorithms is analyzed for the specific case \nof dissimilarity representations. The experimental results showed that the \nmethods select good prototypes taking advantage of the large datasets, and they \ndo so at low runtimes. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda85", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09277"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Subhadeep Mukhopadhyay, Emanuel Parzen", "title": "Nonlinear Time Series Modeling: A Unified Perspective, Algorithm, and Application. (arXiv:1308.0642v4 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1308.0642", "type": "text/html"}], "timestampUsec": "1514351435275149", "comments": [], "summary": {"content": "<p>A new comprehensive approach to nonlinear time series analysis and modeling \nis developed in the present paper. We introduce novel data-specific \nmid-distribution based Legendre Polynomial (LP) like nonlinear transformations \nof the original time series Y(t) that enables us to adapt all the existing \nstationary linear Gaussian time series modeling strategy and made it applicable \nfor non-Gaussian and nonlinear processes in a robust fashion. The emphasis of \nthe present paper is on empirical time series modeling via the algorithm \nLPTime. We demonstrate the effectiveness of our theoretical framework using \ndaily S&amp;P 500 return data between Jan/2/1963 - Dec/31/2009. Our proposed LPTime \nalgorithm systematically discovers all the `stylized facts' of the financial \ntime series automatically all at once, which were previously noted by many \nresearchers one at a time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda8a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1308.0642"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xingguo Li, Raman Arora, Han Liu, Jarvis Haupt, Tuo Zhao", "title": "Nonconvex Sparse Learning via Stochastic Optimization with Progressive Variance Reduction. (arXiv:1605.02711v5 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1605.02711", "type": "text/html"}], "timestampUsec": "1514351435275148", "comments": [], "summary": {"content": "<p>We propose a stochastic variance reduced optimization algorithm for solving \nsparse learning problems with cardinality constraints. Sufficient conditions \nare provided, under which the proposed algorithm enjoys strong linear \nconvergence guarantees and optimal estimation accuracy in high dimensions. We \nfurther extend the proposed algorithm to an asynchronous parallel variant with \na near linear speedup. Numerical experiments demonstrate the efficiency of our \nalgorithm in terms of both parameter estimation and computational performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda8d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1605.02711"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "J&#xf6;rg L&#xfc;cke", "title": "Truncated Variational Expectation Maximization. (arXiv:1610.03113v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.03113", "type": "text/html"}], "timestampUsec": "1514351435275147", "comments": [], "summary": {"content": "<p>We derive a novel variational expectation maximization approach based on \ntruncated variational distributions. Truncated distributions are proportional \nto exact posteriors within a subset of a discrete state space and equal zero \notherwise. The novel variational approach is realized by first generalizing the \nstandard variational EM framework to include variational distributions with \nexact (`hard') zeros. A fully variational treatment of truncated distributions \nthen allows for deriving novel and mathematically grounded results, which in \nturn can be used to formulate novel efficient algorithms to optimize the \nparameters of probabilistic generative models. We find the free energies which \ncorrespond to truncated distributions to be given by concise and efficiently \ncomputable expressions, while update equations for model parameters (M-steps) \nremain in their standard form. Furthermore, we obtain generic expressions for \nexpectation values w.r.t. truncated distributions. Based on these observations, \nwe show how efficient and easily applicable meta-algorithms can be formulated \nthat guarantee a monotonic increase of the free energy. Example applications of \nthe here derived framework provide novel theoretical results and learning \nprocedures for latent variable models as well as mixture models including \nprocedures to tightly couple sampling and variational optimization approaches. \nFurthermore, by considering a special case of truncated variational \ndistributions, we can cleanly and fully embed the well-known `hard EM' \napproaches into the variational EM framework, and we show that `hard EM' (for \nmodels with discrete latents) provably optimizes a lower free energy bound of \nthe data log-likelihood. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda99", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.03113"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Andrey Y. Lokhov, Marc Vuffray, Sidhant Misra, Michael Chertkov", "title": "Optimal structure and parameter learning of Ising models. (arXiv:1612.05024v2 [cond-mat.stat-mech] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1612.05024", "type": "text/html"}], "timestampUsec": "1514351435275146", "comments": [], "summary": {"content": "<p>Reconstruction of structure and parameters of an Ising model from binary \nsamples is a problem of practical importance in a variety of disciplines, \nranging from statistical physics and computational biology to image processing \nand machine learning. The focus of the research community shifted towards \ndeveloping universal reconstruction algorithms which are both computationally \nefficient and require the minimal amount of expensive data. We introduce a new \nmethod, Interaction Screening, which accurately estimates the model parameters \nusing local optimization problems. The algorithm provably achieves perfect \ngraph structure recovery with an information-theoretically optimal number of \nsamples, notably in the low-temperature regime which is known to be the hardest \nfor learning. The efficacy of Interaction Screening is assessed through \nextensive numerical tests on synthetic Ising models of various topologies with \ndifferent types of interactions, as well as on a real data produced by a D-Wave \nquantum computer. This study shows that the Interaction Screening method is an \nexact, tractable and optimal technique universally solving the inverse Ising \nproblem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdaa4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.05024"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pratik Jawanpuria, Bamdev Mishra", "title": "A unified framework for structured low-rank matrix learning. (arXiv:1704.07352v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.07352", "type": "text/html"}], "timestampUsec": "1514351435275145", "comments": [], "summary": {"content": "<p>We propose a novel optimization framework for learning a low-rank matrix \nwhich is also constrained to lie in a linear subspace. Exploiting the duality \ntheory, we present a factorization that decouples the low-rank and structural \nconstraints onto separate factors. The optimization problem is formulated on \nthe Riemannian spectrahedron manifold, where the Riemannian framework allows to \ndevelop computationally efficient conjugate gradient and trust-region \nalgorithms. Our approach easily accommodates popular non-smooth loss functions, \ne.g., L1-loss, and our algorithms are scalable to large-scale problem \ninstances. The numerical comparisons show that our algorithms outperform \nstate-of-the-art in standard, robust, and non-negative matrix completion, \nHankel matrix learning, and multi-task feature learning problems on various \nbenchmarks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdaaf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.07352"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michela Paganini, Luke de Oliveira, Benjamin Nachman", "title": "Accelerating Science with Generative Adversarial Networks: An Application to 3D Particle Showers in Multi-Layer Calorimeters. (arXiv:1705.02355v2 [hep-ex] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.02355", "type": "text/html"}], "timestampUsec": "1514351435275144", "comments": [], "summary": {"content": "<p>Physicists at the Large Hadron Collider (LHC) rely on detailed simulations of \nparticle collisions to build expectations of what experimental data may look \nlike under different theory modeling assumptions. Petabytes of simulated data \nare needed to develop analysis techniques, though they are expensive to \ngenerate using existing algorithms and computing resources. The modeling of \ndetectors and the precise description of particle cascades as they interact \nwith the material in the calorimeter are the most computationally demanding \nsteps in the simulation pipeline. We therefore introduce a deep neural \nnetwork-based generative model to enable high-fidelity, fast, electromagnetic \ncalorimeter simulation. There are still challenges for achieving precision \nacross the entire phase space, but our current solution can reproduce a variety \nof particle shower properties while achieving speed-up factors of up to \n100,000$\\times$. This opens the door to a new era of fast simulation that could \nsave significant computing time and disk space, while extending the reach of \nphysics searches and precision measurements at the LHC and beyond. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdab6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.02355"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Eduardo Pavez, Hilmi E. Egilmez, Antonio Ortega", "title": "Learning Graphs with Monotone Topology Properties and Multiple Connected Components. (arXiv:1705.10934v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.10934", "type": "text/html"}], "timestampUsec": "1514351435275143", "comments": [], "summary": {"content": "<p>Recent papers have formulated the problem of learning graphs from data as an \ninverse covariance estimation with graph Laplacian constraints. While such \nproblems are convex, existing methods cannot guarantee that solutions will have \nspecific graph topology properties (e.g., being a tree or k-partite), which are \ndesirable for some applications. In fact, the problem of learning a graph with \ngiven topology properties, e.g., finding the k-partite graph that best matches \nthe data, is in general non-convex. In this paper, we develop novel results \nthat provide theoretical guarantees for an approach to solve these problems by \ndecomposing them into two sub-problems, for which efficient solutions are \nknown. Specifically, a graph topology inference (GTI) step is employed to \nselect a feasible graph topology, i.e., one having the desired topology \nproperty. Then, a graph weight estimation (GWE) step is performed by solving a \ngeneralized graph Laplacian estimation problem, where edges are constrained by \nthe topology found in the GTI step. Our main result is a bound on the error of \nthe GWE step as a function of the error in the GTI step. This error bound \nindicates that the GTI step should be solved using an algorithm that \napproximates the similarity matrix (which in general corresponds to a complete \nweighted graph) by another matrix whose entries have been thresholded to zero \nto have the desired type of graph topology. The GTI stage can leverage existing \nmethods (e.g., state of the art approaches for graph coloring) which are \ntypically based on minimizing the total weight of removed edges. Since the GWE \nstage is formulated as an inverse covariance estimation problem with linear \nconstraints, it can be solved using existing convex optimization methods. We \ndemonstrate that our two step approach can achieve good results for both \nsynthetic and texture image data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdaba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.10934"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sumanta Basu, Karl Kumbier, James B. Brown, Bin Yu", "title": "Iterative Random Forests to detect predictive and stable high-order interactions. (arXiv:1706.08457v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.08457", "type": "text/html"}], "timestampUsec": "1514351435275142", "comments": [], "summary": {"content": "<p>Genomics has revolutionized biology, enabling the interrogation of whole \ntranscriptomes, genome-wide binding sites for proteins, and many other \nmolecular processes. However, individual genomic assays measure elements that \ninteract in vivo as components of larger molecular machines. Understanding how \nthese high-order interactions drive gene expression presents a substantial \nstatistical challenge. Building on Random Forests (RF), Random Intersection \nTrees (RITs), and through extensive, biologically inspired simulations, we \ndeveloped the iterative Random Forest algorithm (iRF). iRF trains a \nfeature-weighted ensemble of decision trees to detect stable, high-order \ninteractions with same order of computational cost as RF. We demonstrate the \nutility of iRF for high-order interaction discovery in two prediction problems: \nenhancer activity in the early Drosophila embryo and alternative splicing of \nprimary transcripts in human derived cell lines. In Drosophila, among the 20 \npairwise transcription factor interactions iRF identifies as stable (returned \nin more than half of bootstrap replicates), 80% have been previously reported \nas physical interactions. Moreover, novel third-order interactions, e.g. \nbetween Zelda (Zld), Giant (Gt), and Twist (Twi), suggest high-order \nrelationships that are candidates for follow-up experiments. In human-derived \ncells, iRF re-discovered a central role of H3K36me3 in chromatin-mediated \nsplicing regulation, and identified novel 5th and 6th order interactions, \nindicative of multi-valent nucleosomes with specific roles in splicing \nregulation. By decoupling the order of interactions from the computational cost \nof identification, iRF opens new avenues of inquiry into the molecular \nmechanisms underlying genome biology. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdabe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.08457"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kun Yuan, Bicheng Ying, Jiageng Liu, Ali H. Sayed", "title": "Variance-Reduced Stochastic Learning by Networked Agents under Random Reshuffling. (arXiv:1708.01384v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.01384", "type": "text/html"}], "timestampUsec": "1514351435275141", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329b34f2c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329b34f2c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A new amortized variance-reduced gradient (AVRG) algorithm was developed in \n[1], which has constant storage requirement in comparison to SAGA and balanced \ngradient computations in comparison to SVRG. One key advantage of the AVRG \nstrategy is its amenability to decentralized implementations. In this work, we \nshow how AVRG can be extended to the network case where multiple learning \nagents are assumed to be connected by a graph topology. In this scenario, each \nagent observes data that is spatially distributed and all agents are only \nallowed to communicate with direct neighbors. Moreover, the amount of data \nobserved by the individual agents may differ drastically. For such situations, \nthe balanced gradient computation property of AVRG becomes a real advantage in \nreducing idle time caused by unbalanced local data storage requirements, which \nis characteristic of other reduced-variance gradient algorithms. The resulting \ndiffusion-AVRG algorithm is shown to have linear convergence to the exact \nsolution, and is much more memory efficient than other alternative algorithms. \nIn addition, by using a mini-batch strategy, it is shown that diffusion-AVRG is \nmore computationally efficient than exact diffusion or EXTRA while maintaining \nalmost the same amount of communications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdac2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.01384"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Heinrich Jiang", "title": "On the Consistency of Quick Shift. (arXiv:1710.10646v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.10646", "type": "text/html"}], "timestampUsec": "1514351435275140", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329ba4eec\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329ba4eec&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Quick Shift is a popular mode-seeking and clustering algorithm. We present \nfinite sample statistical consistency guarantees for Quick Shift on mode and \ncluster recovery under mild distributional assumptions. We then apply our \nresults to construct a consistent modal regression algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdac7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.10646"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Nicolas Loizou, Peter Richt&#xe1;rik", "title": "Linearly convergent stochastic heavy ball method for minimizing generalization error. (arXiv:1710.10737v2 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.10737", "type": "text/html"}], "timestampUsec": "1514351435275139", "comments": [], "summary": {"content": "<p>In this work we establish the first linear convergence result for the \nstochastic heavy ball method. The method performs SGD steps with a fixed \nstepsize, amended by a heavy ball momentum term. In the analysis, we focus on \nminimizing the expected loss and not on finite-sum minimization, which is \ntypically a much harder problem. While in the analysis we constrain ourselves \nto quadratic loss, the overall objective is not necessarily strongly convex. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdacb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.10737"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, Matthew P. Lungren, Andrew Y. Ng", "title": "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning. (arXiv:1711.05225v3 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05225", "type": "text/html"}], "timestampUsec": "1514351435275138", "comments": [], "summary": {"content": "<p>We develop an algorithm that can detect pneumonia from chest X-rays at a \nlevel exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer \nconvolutional neural network trained on ChestX-ray14, currently the largest \npublicly available chest X-ray dataset, containing over 100,000 frontal-view \nX-ray images with 14 diseases. Four practicing academic radiologists annotate a \ntest set, on which we compare the performance of CheXNet to that of \nradiologists. We find that CheXNet exceeds average radiologist performance on \nthe F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and \nachieve state of the art results on all 14 diseases. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdad1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05225"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pravesh K. Kothari, David Steurer", "title": "Outlier-robust moment-estimation via sum-of-squares. (arXiv:1711.11581v2 [cs.DS] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.11581", "type": "text/html"}], "timestampUsec": "1514351435275137", "comments": [], "summary": {"content": "<p>We develop efficient algorithms for estimating low-degree moments of unknown \ndistributions in the presence of adversarial outliers. The guarantees of our \nalgorithms improve in many cases significantly over the best previous ones, \nobtained in recent works of Diakonikolas et al, Lai et al, and Charikar et al. \nWe also show that the guarantees of our algorithms match information-theoretic \nlower-bounds for the class of distributions we consider. These improved \nguarantees allow us to give improved algorithms for independent component \nanalysis and learning mixtures of Gaussians in the presence of outliers. \n</p> \n<p>Our algorithms are based on a standard sum-of-squares relaxation of the \nfollowing conceptually-simple optimization problem: Among all distributions \nwhose moments are bounded in the same way as for the unknown distribution, find \nthe one that is closest in statistical distance to the empirical distribution \nof the adversarially-corrupted sample. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdadc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11581"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Rui Gao, Xi Chen, Anton J. Kleywegt", "title": "Wasserstein Distributional Robustness and Regularization in Statistical Learning. (arXiv:1712.06050v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.06050", "type": "text/html"}], "timestampUsec": "1514351435275136", "comments": [], "summary": {"content": "<p>A central question in statistical learning is to design algorithms that not \nonly perform well on training data, but also generalize to new and unseen data. \nIn this paper, we tackle this question by formulating a distributionally robust \nstochastic optimization (DRSO) problem, which seeks a solution that minimizes \nthe worst-case expected loss over a family of distributions that are close to \nthe empirical distribution in Wasserstein distances. We establish a connection \nbetween such Wasserstein DRSO and regularization. More precisely, we identify a \nbroad class of loss functions, for which the Wasserstein DRSO is asymptotically \nequivalent to a regularization problem with a gradient-norm penalty. Such \nrelation provides new interpretations for problems involving regularization, \nincluding a great number of statistical learning problems and discrete choice \nmodels (e.g. multinomial logit). The connection suggests a principled way to \nregularize high-dimensional, non-convex problems. This is demonstrated through \nthe training of Wasserstein generative adversarial networks in deep learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdae5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06050"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kushagra Rastogi, Navreet Saini", "title": "Virtual Sensor Modelling using Neural Networks with Coefficient-based Adaptive Weights and Biases Search Algorithm for Diesel Engines. (arXiv:1712.08319v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.08319", "type": "text/html"}], "timestampUsec": "1514182860509778", "comments": [], "summary": {"content": "<p>With the explosion in the field of Big Data and introduction of more \nstringent emission norms every three to five years, automotive companies must \nnot only continue to enhance the fuel economy ratings of their products, but \nalso provide valued services to their customers such as delivering engine \nperformance and health reports at regular intervals. A reasonable solution to \nboth issues is installing a variety of sensors on the engine. Sensor data can \nbe used to develop fuel economy features and will directly indicate engine \nperformance. However, mounting a plethora of sensors is impractical in a very \ncost-sensitive industry. Thus, virtual sensors can replace physical sensors by \nreducing cost while capturing essential engine data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ecdc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08319"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Luiza Mici, German I. Parisi, Stefan Wermter", "title": "An Incremental Self-Organizing Architecture for Sensorimotor Learning and Prediction. (arXiv:1712.08521v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.08521", "type": "text/html"}], "timestampUsec": "1514182860509777", "comments": [], "summary": {"content": "<p>During visuomotor tasks, robots have to compensate for the temporal delays \ninherent in their sensorimotor processing systems. This capability becomes \ncrucial in a dynamic environment where the visual input is constantly changing, \ne.g. when interacting with humans. For this purpose, the robot should be \nequipped with a prediction mechanism able to use the acquired perceptual \nexperience in order to estimate possible future motor commands. In this paper, \nwe present a novel neural network architecture that learns prototypical \nvisuomotor representations and provides reliable predictions to compensate for \nthe delayed robot behavior in an online manner. We investigate the performance \nof our method in the context of a synchronization task, where a humanoid robot \nhas to generate visually perceived arm motion trajectories in synchrony with a \nhuman demonstrator. We evaluate the prediction accuracy in terms of mean \nprediction error and analyze the response of the network to novel movement \ndemonstrations. Additionally, we provide experiments with the system receiving \nincomplete data sequences, showing the robustness of the proposed architecture \nin the case of a noisy and faulty visual sensor. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ece5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08521"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jesse Engel, Matthew Hoffman, Adam Roberts", "title": "Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models. (arXiv:1711.05772v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05772", "type": "text/html"}], "timestampUsec": "1514182860509776", "comments": [], "summary": {"content": "<p>Deep generative neural networks have proven effective at both conditional and \nunconditional modeling of complex data distributions. Conditional generation \nenables interactive control, but creating new controls often requires expensive \nretraining. In this paper, we develop a method to condition generation without \nretraining the model. By post-hoc learning latent constraints, value functions \nthat identify regions in latent space that generate outputs with desired \nattributes, we can conditionally sample from these regions with gradient-based \noptimization or amortized actor functions. Combining attribute constraints with \na universal \"realism\" constraint, which enforces similarity to the data \ndistribution, we generate realistic conditional images from an unconditional \nvariational autoencoder. Further, using gradient-based optimization, we \ndemonstrate identity-preserving transformations that make the minimal \nadjustment in latent space to modify the attributes of an image. Finally, with \ndiscrete sequences of musical notes, we demonstrate zero-shot conditional \ngeneration, learning latent constraints in the absence of labeled data or a \ndifferentiable reward function. Code with dedicated cloud instance has been \nmade publicly available (https://goo.gl/STGMGx). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ecee", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05772"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Boris Chidlovskii", "title": "Multi-task learning of time series and its application to the travel demand. (arXiv:1712.08164v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08164", "type": "text/html"}], "timestampUsec": "1514182860509775", "comments": [], "summary": {"content": "<p>We address the problem of modeling and prediction of a set of temporal events \nin the context of intelligent transportation systems. To leverage the \ninformation shared by different events, we propose a multi-task learning \nframework. We develop a support vector regression model for joint learning of \nmutually dependent time series. It is the regularization-based multi-task \nlearning previously developed for the classification case and extended to time \nseries. We discuss the relatedness of observed time series and first deploy the \ndynamic time warping distance measure to identify groups of similar series. \nThen we take into account both time and scale warping and propose to align \nmultiple time series by inferring their common latent representation. We test \nthe proposed models on the problem of travel demand prediction in Nancy \n(France) public transport system and analyze the benefits of multi-task \nlearning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ecf6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08164"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Saurabh Kumar, Pararth Shah, Dilek Hakkani-Tur, Larry Heck", "title": "Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning. (arXiv:1712.08266v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08266", "type": "text/html"}], "timestampUsec": "1514182860509774", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329ba5113\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329ba5113&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present a framework combining hierarchical and multi-agent deep \nreinforcement learning approaches to solve coordination problems among a \nmultitude of agents using a semi-decentralized model. The framework extends the \nmulti-agent learning setup by introducing a meta-controller that guides the \ncommunication between agent pairs, enabling agents to focus on communicating \nwith only one other agent at any step. This hierarchical decomposition of the \ntask allows for efficient exploration to learn policies that identify globally \noptimal solutions even as the number of collaborating agents increases. We show \npromising initial experimental results on a simulated distributed scheduling \nproblem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ecfd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08266"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, Marcin Detyniecki", "title": "Inverse Classification for Comparison-based Interpretability in Machine Learning. (arXiv:1712.08443v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08443", "type": "text/html"}], "timestampUsec": "1514182860509773", "comments": [], "summary": {"content": "<p>In the context of post-hoc interpretability, this paper addresses the task of \nexplaining the prediction of a classifier, considering the case where no \ninformation is available, neither on the classifier itself, nor on the \nprocessed data (neither the training nor the test data). It proposes an \ninstance-based approach whose principle consists in determining the minimal \nchanges needed to alter a prediction: given a data point whose classification \nmust be explained, the proposed method consists in identifying a close \nneighbour classified differently, where the closeness definition integrates a \nsparsity constraint. This principle is implemented using observation generation \nin the Growing Spheres algorithm. Experimental results on two datasets \nillustrate the relevance of the proposed approach that can be used to gain \nknowledge about the classifier. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed03", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08443"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kathryn Laing, Peter Adam Thwaites, John Paul Gosling", "title": "Rank Pruning for Dominance Queries in CP-Nets. (arXiv:1712.08588v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08588", "type": "text/html"}], "timestampUsec": "1514182860509772", "comments": [], "summary": {"content": "<p>Conditional preference networks (CP-nets) are a graphical representation of a \nperson's (conditional) preferences over a set of discrete variables. In this \npaper, we introduce a novel method of quantifying preference for any given \noutcome based on a CP-net representation of a user's preferences. We \ndemonstrate that these values are useful for reasoning about user preferences. \nIn particular, they allow us to order (any subset of) the possible outcomes in \naccordance with the user's preferences. Further, these values can be used to \nimprove the efficiency of outcome dominance testing. That is, given a pair of \noutcomes, we can determine which the user prefers more efficiently. We show \nthat these results also hold for CP-nets that express indifference between \nvariable values. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed13", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08588"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Clement Carbonnel, David A. Cohen, Martin C. Cooper, Stanislav Zivny", "title": "On Singleton Arc Consistency for CSPs Defined by Monotone Patterns. (arXiv:1704.06215v3 [cs.CC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.06215", "type": "text/html"}], "timestampUsec": "1514182860509771", "comments": [], "summary": {"content": "<p>Singleton arc consistency is an important type of local consistency which has \nbeen recently shown to solve all constraint satisfaction problems (CSPs) over \nconstraint languages of bounded width. We aim to characterise all classes of \nCSPs defined by a forbidden pattern that are solved by singleton arc \nconsistency and closed under removing constraints. We identify five new \npatterns whose absence ensures solvability by singleton arc consistency, four \nof which are provably maximal and three of which generalise 2-SAT. Combined \nwith simple counter-examples for other patterns, we make significant progress \ntowards a complete classification. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed27", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.06215"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Liron Cohen, Tansel Uras, Shiva Jahangiri, Aliyah Arunasalam, Sven Koenig, T.K. Satish Kumar", "title": "The FastMap Algorithm for Shortest Path Computations. (arXiv:1706.02792v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.02792", "type": "text/html"}], "timestampUsec": "1514182860509770", "comments": [], "summary": {"content": "<p>We present a new preprocessing algorithm for embedding the nodes of a given \nedge-weighted undirected graph into a Euclidean space. The Euclidean distance \nbetween any two nodes in this space approximates the length of the shortest \npath between them in the given graph. Later, at runtime, a shortest path \nbetween any two nodes can be computed with A* search using the Euclidean \ndistances as heuristic. Our preprocessing algorithm, called FastMap, is \ninspired by the data mining algorithm of the same name and runs in near-linear \ntime. Hence, FastMap is orders of magnitude faster than competing approaches \nthat produce a Euclidean embedding using Semidefinite Programming. FastMap also \nproduces admissible and consistent heuristics and therefore guarantees the \ngeneration of shortest paths. Moreover, FastMap applies to general undirected \ngraphs for which many traditional heuristics, such as the Manhattan Distance \nheuristic, are not well defined. Empirically, we demonstrate that A* search \nusing the FastMap heuristic is competitive with A* search using other \nstate-of-the-art heuristics, such as the Differential heuristic. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed34", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.02792"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sam Toyer, Felipe Trevizan, Sylvie Thi&#xe9;baux, Lexing Xie", "title": "Action Schema Networks: Generalised Policies with Deep Learning. (arXiv:1709.04271v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04271", "type": "text/html"}], "timestampUsec": "1514182860509769", "comments": [], "summary": {"content": "<p>In this paper, we introduce the Action Schema Network (ASNet): a neural \nnetwork architecture for learning generalised policies for probabilistic \nplanning problems. By mimicking the relational structure of planning problems, \nASNets are able to adopt a weight-sharing scheme which allows the network to be \napplied to any problem from a given planning domain. This allows the cost of \ntraining the network to be amortised over all problems in that domain. Further, \nwe propose a training method which balances exploration and supervised training \non small problems to produce a policy which remains robust when evaluated on \nlarger problems. In experiments, we show that ASNet's learning capability \nallows it to significantly outperform traditional non-learning planners in \nseveral challenging domains. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed47", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04271"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sahisnu Mazumder, Bing Liu", "title": "Context-aware Path Ranking for Knowledge Base Completion. (arXiv:1712.07745v1 [cs.CL] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1712.07745", "type": "text/html"}], "timestampUsec": "1514182860509768", "comments": [], "summary": {"content": "<p>Knowledge base (KB) completion aims to infer missing facts from existing ones \nin a KB. Among various approaches, path ranking (PR) algorithms have received \nincreasing attention in recent years. PR algorithms enumerate paths between \nentity pairs in a KB and use those paths as features to train a model for \nmissing fact prediction. Due to their good performances and high model \ninterpretability, several methods have been proposed. However, most existing \nmethods suffer from scalability (high RAM consumption) and feature explosion \n(trains on an exponentially large number of features) problems. This paper \nproposes a Context-aware Path Ranking (C-PR) algorithm to solve these problems \nby introducing a selective path exploration strategy. C-PR learns global \nsemantics of entities in the KB using word embedding and leverages the \nknowledge of entity semantics to enumerate contextually relevant paths using \nbidirectional random walk. Experimental results on three large KBs show that \nthe path features (fewer in number) discovered by C-PR not only improve \npredictive performance but also are more interpretable than existing baselines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed53", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07745"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Baptiste Goujaud, Eric W. Tramel, Pierre Courtiol, Mikhail Zaslavskiy, Gilles Wainrib", "title": "Robust Detection of Covariate-Treatment Interactions in Clinical Trials. (arXiv:1712.08211v1 [stat.AP])", "alternate": [{"href": "http://arxiv.org/abs/1712.08211", "type": "text/html"}], "timestampUsec": "1514182860509767", "comments": [], "summary": {"content": "<p>Detection of interactions between treatment effects and patient descriptors \nin clinical trials is critical for optimizing the drug development process. The \nincreasing volume of data accumulated in clinical trials provides a unique \nopportunity to discover new biomarkers and further the goal of personalized \nmedicine, but it also requires innovative robust biomarker detection methods \ncapable of detecting non-linear, and sometimes weak, signals. We propose a set \nof novel univariate statistical tests, based on the theory of random walks, \nwhich are able to capture non-linear and non-monotonic covariate-treatment \ninteractions. We also propose a novel combined test, which leverages the power \nof all of our proposed univariate tests into a single general-case tool. We \npresent results for both synthetic trials as well as real-world clinical \ntrials, where we compare our method with state-of-the-art techniques and \ndemonstrate the utility and robustness of our approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed5e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08211"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "N. Olspert, J. Pelt, M. J. K&#xe4;pyl&#xe4;, J. J. Lehtinen", "title": "Estimating activity cycles with probabilistic methods I. Bayesian Generalised Lomb-Scargle Periodogram with Trend. (arXiv:1712.08235v1 [astro-ph.SR])", "alternate": [{"href": "http://arxiv.org/abs/1712.08235", "type": "text/html"}], "timestampUsec": "1514182860509766", "comments": [], "summary": {"content": "<p>Period estimation is one of the central topics in astronomical time series \nanalysis, where data is often unevenly sampled. Especially challenging are \nstudies of stellar magnetic cycles, as there the periods looked for are of the \norder of the same length than the datasets themselves. The datasets often \ncontain trends, the origin of which is either a real long-term cycle or an \ninstrumental effect, but these effects cannot be reliably separated, while they \ncan lead to erroneous period determinations if not properly handled. In this \nstudy we aim at developing a method that can handle the trends properly, and by \nperforming extensive set of testing, we show that this is the optimal procedure \nwhen contrasted with methods that do not include the trend directly to the \nmodel. The effect of the noise model on the results is also investigated. We \nintroduce a Bayesian Generalised Lomb-Scargle Periodogram with Trend (BGLST), \nwhich is a probabilistic linear regression model using Gaussian priors for the \ncoefficients and uniform prior for the frequency parameter. We show, using \nsynthetic data, that when there is no prior information on whether and to what \nextent the true model of the data contains a linear trend, the introduced BGLST \nmethod is preferable to the methods which either detrend the data or leave the \ndata untrended before fitting the periodic model. Whether to use different from \nconstant noise model depends on the density of the data sampling as well as on \nthe true noise model of the process. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed69", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08235"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "N. Olspert, J. J. Lehtinen, M. J. K&#xe4;pyl&#xe4;, J. Pelt, A. Grigorievskiy", "title": "Estimating activity cycles with probabilistic methods II. The Mount Wilson Ca H&K data. (arXiv:1712.08240v1 [astro-ph.SR])", "alternate": [{"href": "http://arxiv.org/abs/1712.08240", "type": "text/html"}], "timestampUsec": "1514182860509765", "comments": [], "summary": {"content": "<p>Debate over the existence versus nonexistence of trends in the stellar \nactivity-rotation diagrams continues. Application of modern time series \nanalysis tools to study the mean cycle periods in chromospheric activity index \nis lacking. We develop such models, based on Gaussian processes, for \none-dimensional time series and apply it to the extended Mount Wilson Ca H&amp;K \nsample. Our main aim is to study how the previously commonly used assumption of \nstrict harmonicity of the stellar cycles affects the results. We introduce \nthree methods of different complexity, starting with the simple harmonic model \nand followed by Gaussian Process models with periodic and quasi-periodic \ncovariance functions. We confirm the existence of two populations in the \nactivity-period diagram. We find only one significant trend in the inactive \npopulation, namely that the cycle periods get shorter with increasing rotation. \nThis is in contrast with earlier studies, that postulate the existence of \ntrends in both of the populations. In terms of rotation to cycle period ratio, \nour data is consistent with only two activity branches such that the active \nbranch merges together with the transitional one. The retrieved stellar cycles \nare uniformly distributed over the R'HK activity index, indicating that the \noperation of stellar large-scale dynamos carries smoothly over the \nVaughan-Preston gap. At around the solar activity index, however, indications \nof a disruption in the cyclic dynamo action are seen. Our study shows that \nstellar cycle estimates depend significantly on the model applied. Such \nmodel-dependent aspects include the improper treatment of linear trends and too \nsimple assumptions of the noise variance model. Assumption of strict \nharmonicity can result in the appearance of double cyclicities that seem more \nlikely to be explained by the quasi-periodicity of the cycles. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed79", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08240"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Maryan Morel, Emmanuel Bacry, St&#xe9;phane Ga&#xef;ffas, Agathe Guilloux, Fanny Leroy", "title": "ConvSCCS: convolutional self-controlled case series model for lagged adverse event detection. (arXiv:1712.08243v1 [stat.AP])", "alternate": [{"href": "http://arxiv.org/abs/1712.08243", "type": "text/html"}], "timestampUsec": "1514182860509764", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329ba52eb\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329ba52eb&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>With the increased availability of large databases of electronic health \nrecords (EHRs) comes the chance of enhancing health risks screening. Most \npost-marketing detections of adverse drug reaction (ADR) rely on physicians' \nspontaneous reports, leading to under reporting. To take up this challenge, we \ndevelop a scalable model to estimate the effect of multiple longitudinal \nfeatures (drug exposures) on a rare longitudinal outcome. Our procedure is \nbased on a conditional Poisson model also known as self-controlled case series \n(SCCS). We model the intensity of outcomes using a convolution between \nexposures and step functions, that are penalized using a combination of \ngroup-Lasso and total-variation. This approach does not require the \nspecification of precise risk periods, and allows to study in the same model \nseveral exposures at the same time. We illustrate the fact that this approach \nimproves the state-of-the-art for the estimation of the relative risks both on \nsimulations and on a cohort of diabetic patients, extracted from the large \nFrench national health insurance database (SNIIRAM). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed8c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08243"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mohammad Reza Bonyadi, Viktor Vegh, David C. Reutens", "title": "Linear centralization classifier. (arXiv:1712.08259v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08259", "type": "text/html"}], "timestampUsec": "1514182860509763", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329c1a2b5\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329c1a2b5&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A classification algorithm, called the Linear Centralization Classifier \n(LCC), is introduced. The algorithm seeks to find a transformation that best \nmaps instances from the feature space to a space where they concentrate towards \nthe center of their own classes, while maximimizing the distance between class \ncenters. We formulate the classifier as a quadratic program with quadratic \nconstraints. We then simplify this formulation to a linear program that can be \nsolved effectively using a linear programming solver (e.g., simplex-dual). We \nextend the formulation for LCC to enable the use of kernel functions for \nnon-linear classification applications. We compare our method with two standard \nclassification methods (support vector machine and linear discriminant \nanalysis) and four state-of-the-art classification methods when they are \napplied to eight standard classification datasets. Our experimental results \nshow that LCC is able to classify instances more accurately (based on the area \nunder the receiver operating characteristic) in comparison to other tested \nmethods on the chosen datasets. We also report the results for LCC with a \nparticular kernel to solve for synthetic non-linear classification problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20edab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08259"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kui Zhao, Yuechuan Li, Zhaoqian Shuai, Cheng Yang", "title": "Joint IDs Embedding and its Applications in E-commerce. (arXiv:1712.08289v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08289", "type": "text/html"}], "timestampUsec": "1514182860509762", "comments": [], "summary": {"content": "<p>E-commerce has become an important part of our daily lives and there are \ngreat challenges due to its dynamic and complex business environment. Many \nmachine intelligence techniques are developed to overcome these challenges. One \nof the essential elements in those techniques is the representation of data, \nespecially for ID-type data, e.g. item ID, product ID, store ID, brand ID, \ncategory ID etc. The classical one-hot encoding suffers sparsity problems due \nto its high dimension. Moreover, it cannot reflect the relationships among IDs, \neither homogeneous or heterogeneous ones. In this paper, we propose a novel \nhierarchical embedding model to jointly learn low-dimensional representations \nfor different types of IDs from the implicit feedback of users. Our approach \nincorporates the structural information among IDs and embeds all types of IDs \ninto a semantic space. The low-dimensional representations can be effectively \nextended to many applications including recommendation and forecast etc. We \nevaluate our approach in several scenarios of \"Hema App\" and the experimental \nresults validate the effectiveness of our approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20edbe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08289"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jan Chorowski, Ron J. Weiss, Rif A. Saurous, Samy Bengio", "title": "On Using Backpropagation for Speech Texture Generation and Voice Conversion. (arXiv:1712.08363v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1712.08363", "type": "text/html"}], "timestampUsec": "1514182860509761", "comments": [], "summary": {"content": "<p>Inspired by recent work on neural network image generation which rely on \nbackpropagation towards the network inputs, we present a proof-of-concept \nsystem for speech texture synthesis and voice conversion based on two \nmechanisms: approximate inversion of the representation learned by a speech \nrecognition neural network, and on matching statistics of neuron activations \nbetween different source and target utterances. Similar to image texture \nsynthesis and neural style transfer, the system works by optimizing a cost \nfunction with respect to the input waveform samples. To this end we use a \ndifferentiable mel-filterbank feature extraction pipeline and train a \nconvolutional CTC speech recognition network. Our system is able to extract \nspeaker characteristics from very limited amounts of target speaker data, as \nlittle as a few seconds, and can be used to generate realistic speech babble or \nreconstruct an utterance in a different voice. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20edcc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08363"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yann Ollivier", "title": "True Asymptotic Natural Gradient Optimization. (arXiv:1712.08449v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08449", "type": "text/html"}], "timestampUsec": "1514182860509760", "comments": [], "summary": {"content": "<p>We introduce a simple algorithm, True Asymptotic Natural Gradient \nOptimization (TANGO), that converges to a true natural gradient descent in the \nlimit of small learning rates, without explicit Fisher matrix estimation. \n</p> \n<p>For quadratic models the algorithm is also an instance of averaged stochastic \ngradient, where the parameter is a moving average of a \"fast\", constant-rate \ngradient descent. TANGO appears as a particular de-linearization of averaged \nSGD, and is sometimes quite different on non-quadratic models. This further \nconnects averaged SGD and natural gradient, both of which are arguably optimal \nasymptotically. \n</p> \n<p>In large dimension, small learning rates will be required to approximate the \nnatural gradient well. Still, this shows it is possible to get arbitrarily \nclose to exact natural gradient descent with a lightweight algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20edde", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08449"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Shounak Datta, Sayak Nag, Sankha Subhra Mullick, Swagatam Das", "title": "Diversifying Support Vector Machines for Boosting using Kernel Perturbation: Applications to Class Imbalance and Small Disjuncts. (arXiv:1712.08493v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08493", "type": "text/html"}], "timestampUsec": "1514182860509759", "comments": [], "summary": {"content": "<p>The diversification (generating slightly varying separating discriminators) \nof Support Vector Machines (SVMs) for boosting has proven to be a challenge due \nto the strong learning nature of SVMs. Based on the insight that perturbing the \nSVM kernel may help in diversifying SVMs, we propose two kernel perturbation \nbased boosting schemes where the kernel is modified in each round so as to \nincrease the resolution of the kernel-induced Reimannian metric in the vicinity \nof the datapoints misclassified in the previous round. We propose a method for \nidentifying the disjuncts in a dataset, dispelling the dependence on rule-based \nlearning methods for identifying the disjuncts. We also present a new \nperformance measure called Geometric Small Disjunct Index (GSDI) to quantify \nthe performance on small disjuncts for balanced as well as class imbalanced \ndatasets. Experimental comparison with a variety of state-of-the-art algorithms \nis carried out using the best classifiers of each type selected by a new \napproach inspired by multi-criteria decision making. The proposed method is \nfound to outperform the contending state-of-the-art methods on different \ndatasets (ranging from mildly imbalanced to highly imbalanced and characterized \nby varying number of disjuncts) in terms of three different performance indices \n(including the proposed GSDI). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20edee", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08493"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "R&#xe9;mi Le Priol, Ahmed Touati, Simon Lacoste-Julien", "title": "Adaptive Stochastic Dual Coordinate Ascent for Conditional Random Fields. (arXiv:1712.08577v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08577", "type": "text/html"}], "timestampUsec": "1514182860509758", "comments": [], "summary": {"content": "<p>This work investigates training Conditional Random Fields (CRF) by Stochastic \nDual Coordinate Ascent (SDCA). SDCA enjoys a linear convergence rate and a \nstrong empirical performance for independent classification problems. However, \nit has never been used to train CRF. Yet it benefits from an exact line search \nwith a single marginalization oracle call, unlike previous approaches. In this \npaper, we adapt SDCA to train CRF and we enhance it with an adaptive \nnon-uniform sampling strategy. Our preliminary experiments suggest that this \nmethod matches state-of-the-art CRF optimization techniques. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ee00", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08577"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jing Lei", "title": "Cross-Validation with Confidence. (arXiv:1703.07904v2 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.07904", "type": "text/html"}], "timestampUsec": "1514182860509757", "comments": [], "summary": {"content": "<p>Cross-validation is one of the most popular model selection methods in \nstatistics and machine learning. Despite its wide applicability, traditional \ncross validation methods tend to select overfitting models, due to the \nignorance of the uncertainty in the testing sample. We develop a new, \nstatistically principled inference tool based on cross-validation that takes \ninto account the uncertainty in the testing sample. This new method outputs a \nset of highly competitive candidate models containing the best one with \nguaranteed probability. As a consequence, our method can achieve consistent \nvariable selection in a classical linear regression setting, for which existing \ncross-validation methods require unconventional split ratios. When used for \nregularizing tuning parameter selection, the method can provide a further \ntrade-off between prediction accuracy and model interpretability. We \ndemonstrate the performance of the proposed method in several simulated and \nreal data examples. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ee0d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.07904"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Danielle Ensign, Sorelle A. Friedler, Scott Neville, Carlos Scheidegger, Suresh Venkatasubramanian", "title": "Runaway Feedback Loops in Predictive Policing. (arXiv:1706.09847v3 [cs.CY] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.09847", "type": "text/html"}], "timestampUsec": "1514182860509756", "comments": [], "summary": {"content": "<p>Predictive policing systems are increasingly used to determine how to \nallocate police across a city in order to best prevent crime. Discovered crime \ndata (e.g., arrest counts) are used to help update the model, and the process \nis repeated. Such systems have been empirically shown to be susceptible to \nrunaway feedback loops, where police are repeatedly sent back to the same \nneighborhoods regardless of the true crime rate. \n</p> \n<p>In response, we develop a mathematical model of predictive policing that \nproves why this feedback loop occurs, show empirically that this model exhibits \nsuch problems, and demonstrate how to change the inputs to a predictive \npolicing system (in a black-box manner) so the runaway feedback loop does not \noccur, allowing the true crime rate to be learned. Our results are \nquantitative: we can establish a link (in our model) between the degree to \nwhich runaway feedback causes problems and the disparity in crime rates between \nareas. Moreover, we can also demonstrate the way in which \\emph{reported} \nincidents of crime (those reported by residents) and \\emph{discovered} \nincidents of crime (i.e. those directly observed by police officers dispatched \nas a result of the predictive policing algorithm) interact: in brief, while \nreported incidents can attenuate the degree of runaway feedback, they cannot \nentirely remove it without the interventions we suggest. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ee18", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.09847"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Bita Darvish Rouhani, Mohammad Samragh, Tara Javidi, Farinaz Koushanfar", "title": "CuRTAIL: ChaRacterizing and Thwarting AdversarIal deep Learning. (arXiv:1709.02538v2 [cs.CR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.02538", "type": "text/html"}], "timestampUsec": "1514182860509755", "comments": [], "summary": {"content": "<p>This paper proposes CuRTAIL, an end-to-end computing framework for \ncharacterizing and thwarting adversarial space in the context of Deep Learning \n(DL). The framework protects deep neural networks against adversarial samples, \nwhich are perturbed inputs carefully crafted by malicious entities to mislead \nthe underlying DL model. The precursor for the proposed methodology is a set of \nnew quantitative metrics to assess the vulnerability of various deep learning \narchitectures to adversarial samples. CuRTAIL formalizes the goal of preventing \nadversarial samples as a minimization of the space unexplored by the pertinent \nDL model that is characterized in CuRTAIL vulnerability analysis step. To \nthwart the adversarial machine learning attack, CuRTAIL introduces the concept \nof Modular Robust Redundancy (MRR) as a viable solution to achieve the \nformalized minimization objective. The MRR methodology explicitly characterizes \nthe geometry of the input data and the DL model parameters. It then learns a \nset of complementary but disjoint models which maximally cover the unexplored \nsubspaces of the target DL model, thus reducing the risk of integrity attacks. \nWe extensively evaluate CuRTAIL performance against the state-of-the-art attack \nmodels including fast-sign-gradient, Jacobian Saliency Map Attack, Deepfool, \nand Carlini&amp;WagnerL2. Proof-of-concept implementations for analyzing various \ndata collections including MNIST, CIFAR10, and ImageNet corroborate CuRTAIL \neffectiveness to detect adversarial samples in different settings. The \ncomputations in each MRR module can be performed independently. As such, \nCuRTAIL detection algorithm can be completely parallelized among multiple \nhardware settings to achieve maximum throughput. We further provide an \naccompanying API to facilitate the adoption of the proposed framework for \nvarious applications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ee1c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.02538"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1515433204, "author": "Xiaoyu Liu, Diyu Yang, Aly El Gamal", "title": "Deep Neural Network Architectures for Modulation Classification. (arXiv:1712.00443v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00443", "type": "text/html"}], "timestampUsec": "1514182860509754", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329c1a660\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329c1a660&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this work, we investigate the value of employing deep learning for the \ntask of wireless signal modulation recognition. Recently in [1], a framework \nhas been introduced by generating a dataset using GNU radio that mimics the \nimperfections in a real wireless channel, and uses 10 different modulation \ntypes. Further, a convolutional neural network (CNN) architecture was developed \nand shown to deliver performance that exceeds that of expert-based approaches. \nHere, we follow the framework of [1] and find deep neural network architectures \nthat deliver higher accuracy than the state of the art. We tested the \narchitecture of [1] and found it to achieve an accuracy of approximately 75% of \ncorrectly recognizing the modulation type. We first tune the CNN architecture \nof [1] and find a design with four convolutional layers and two dense layers \nthat gives an accuracy of approximately 83.8% at high SNR. We then develop \narchitectures based on the recently introduced ideas of Residual Networks \n(ResNet [2]) and Densely Connected Networks (DenseNet [3]) to achieve high SNR \naccuracies of approximately 83.5% and 86.6%, respectively. Finally, we \nintroduce a Convolutional Long Short-term Deep Neural Network (CLDNN [4]) to \nachieve an accuracy of approximately 88.5% at high SNR. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1515433204, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ee21", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00443"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1515477220, "author": "Ekaba Bisong", "title": "Benchmarking Decoupled Neural Interfaces with Synthetic Gradients. (arXiv:1712.08314v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.08314", "type": "text/html"}], "timestampUsec": "1514178133841825", "comments": [], "summary": {"content": "<p>Artifical Neural Networks are a particular class of learning systems modeled \nafter biological neural functions with an interesting penchant for Hebbian \nlearning, that is \"neurons that wire together, fire together\". However, unlike \ntheir natural counterparts, artificial neural networks have a close and \nstringent coupling between the modules of neurons in the network. This coupling \nor locking imposes upon the network a strict and inflexible structure that \nprevent layers in the network from updating their weights until a full \nfeed-forward and backward pass has occurred. Such a constraint though may have \nsufficed for a while, is now no longer feasible in the era of very-large-scale \nmachine learning, coupled with the increased desire for parallelization of the \nlearning process across multiple computing infrastructures. To solve this \nproblem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are \nintroduced as a viable alternative to the backpropagation algorithm. This paper \nperforms a speed benchmark to compare the speed and accuracy capabilities of \nSG-DNI as opposed to a standard neural interface using multilayer perceptron \nMLP. SG-DNI shows good promise, in that it not only captures the learning \nproblem, it is also over 3-fold faster due to it asynchronous learning \ncapabilities. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1515477220, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a298f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08314"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pierre Baldi, Peter Sadowski, Zhiqin Lu", "title": "Learning in the Machine: the Symmetries of the Deep Learning Channel. (arXiv:1712.08608v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.08608", "type": "text/html"}], "timestampUsec": "1514178133841824", "comments": [], "summary": {"content": "<p>In a physical neural system, learning rules must be local both in space and \ntime. In order for learning to occur, non-local information must be \ncommunicated to the deep synapses through a communication channel, the deep \nlearning channel. We identify several possible architectures for this learning \nchannel (Bidirectional, Conjoined, Twin, Distinct) and six symmetry challenges: \n1) symmetry of architectures; 2) symmetry of weights; 3) symmetry of neurons; \n4) symmetry of derivatives; 5) symmetry of processing; and 6) symmetry of \nlearning rules. Random backpropagation (RBP) addresses the second and third \nsymmetry, and some of its variations, such as skipped RBP (SRBP) address the \nfirst and the fourth symmetry. Here we address the last two desirable \nsymmetries showing through simulations that they can be achieved and that the \nlearning channel is particularly robust to symmetry variations. Specifically, \nrandom backpropagation and its variations can be performed with the same \nnon-linear neurons used in the main input-output forward channel, and the \nconnections in the learning channel can be adapted using the same algorithm \nused in the forward channel, removing the need for any specialized hardware in \nthe learning channel. Finally, we provide mathematical results in simple cases \nshowing that the learning equations in the forward and backward channels \nconverge to fixed points, for almost any initial conditions. In symmetric \narchitectures, if the weights in both channels are small at initialization, \nadaptation in both channels leads to weights that are essentially symmetric \nduring and after learning. Biological connections are discussed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a2992", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08608"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pierre Baldi, Peter Sadowski, Zhiqin Lu", "title": "Learning in the Machine: Random Backpropagation and the Deep Learning Channel. (arXiv:1612.02734v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1612.02734", "type": "text/html"}], "timestampUsec": "1514178133841823", "comments": [], "summary": {"content": "<p>Random backpropagation (RBP) is a variant of the backpropagation algorithm \nfor training neural networks, where the transpose of the forward matrices are \nreplaced by fixed random matrices in the calculation of the weight updates. It \nis remarkable both because of its effectiveness, in spite of using random \nmatrices to communicate error information, and because it completely removes \nthe taxing requirement of maintaining symmetric weights in a physical neural \nsystem. To better understand random backpropagation, we first connect it to the \nnotions of local learning and learning channels. Through this connection, we \nderive several alternatives to RBP, including skipped RBP (SRPB), adaptive RBP \n(ARBP), sparse RBP, and their combinations (e.g. ASRBP) and analyze their \ncomputational complexity. We then study their behavior through simulations \nusing the MNIST and CIFAR-10 bechnmark datasets. These simulations show that \nmost of these variants work robustly, almost as well as backpropagation, and \nthat multiplication by the derivatives of the activation functions is \nimportant. As a follow-up, we study also the low-end of the number of bits \nrequired to communicate error information over the learning channel. We then \nprovide partial intuitive explanations for some of the remarkable properties of \nRBP and its variations. Finally, we prove several mathematical results, \nincluding the convergence to fixed points of linear chains of arbitrary length, \nthe convergence to fixed points of linear autoencoders with decorrelated data, \nthe long-term existence of solutions for linear systems with a single hidden \nlayer and convergence in special cases, and the convergence to fixed points of \nnon-linear chains, when the derivative of the activation functions is included. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a2997", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.02734"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Weiming Xiang, Hoang-Dung Tran, Taylor T. Johnson", "title": "Reachable Set Computation and Safety Verification for Neural Networks with ReLU Activations. (arXiv:1712.08163v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08163", "type": "text/html"}], "timestampUsec": "1514178133841822", "comments": [], "summary": {"content": "<p>Neural networks have been widely used to solve complex real-world problems. \nDue to the complicate, nonlinear, non-convex nature of neural networks, formal \nsafety guarantees for the output behaviors of neural networks will be crucial \nfor their applications in safety-critical systems.In this paper, the output \nreachable set computation and safety verification problems for a class of \nneural networks consisting of Rectified Linear Unit (ReLU) activation functions \nare addressed. A layer-by-layer approach is developed to compute output \nreachable set. The computation is formulated in the form of a set of \nmanipulations for a union of polyhedra, which can be efficiently applied with \nthe aid of polyhedron computation tools. Based on the output reachable set \ncomputation results, the safety verification for a ReLU neural network can be \nperformed by checking the intersections of unsafe regions and output reachable \nset described by a union of polyhedra. A numerical example of a randomly \ngenerated ReLU neural network is provided to show the effectiveness of the \napproach developed in this paper. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a299b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08163"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos Kalogerakis, Subhransu Maji", "title": "CSGNet: Neural Shape Parser for Constructive Solid Geometry. (arXiv:1712.08290v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.08290", "type": "text/html"}], "timestampUsec": "1514178133841821", "comments": [], "summary": {"content": "<p>We present a neural architecture that takes as input a 2D or 3D shape and \ninduces a program to generate it. The in- structions in our program are based \non constructive solid geometry principles, i.e., a set of boolean operations on \nshape primitives defined recursively. Bottom-up techniques for this task that \nrely on primitive detection are inherently slow since the search space over \npossible primitive combi- nations is large. In contrast, our model uses a \nrecurrent neural network conditioned on the input shape to produce a sequence \nof instructions in a top-down manner and is sig- nificantly faster. It is also \nmore effective as a shape detec- tor than existing state-of-the-art detection \ntechniques. We also demonstrate that our network can be trained on novel \ndatasets without ground-truth program annotations through policy gradient \ntechniques. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a29a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08290"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "James Sunthonlap, Phuoc Nguyen, Zilong Ye", "title": "Intelligent Device Discovery in the Internet of Things - Enabling the Robot Society. (arXiv:1712.08296v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08296", "type": "text/html"}], "timestampUsec": "1514178133841820", "comments": [], "summary": {"content": "<p>The Internet of Things (IoT) is continuously growing to connect billions of \nsmart devices anywhere and anytime in an Internet-like structure, which enables \na variety of applications, services and interactions between human and objects. \nIn the future, the smart devices are supposed to be able to autonomously \ndiscover a target device with desired features and generate a set of entirely \nnew services and applications that are not supervised or even imagined by human \nbeings. The pervasiveness of smart devices, as well as the heterogeneity of \ntheir design and functionalities, raise a major concern: How can a smart device \nefficiently discover a desired target device? In this paper, we propose a \nSocial-Aware and Distributed (SAND) scheme that achieves a fast, scalable and \nefficient device discovery in the IoT. The proposed SAND scheme adopts a novel \ndevice ranking criteria that measures the device's degree, social relationship \ndiversity, clustering coefficient and betweenness. Based on the device ranking \ncriteria, the discovery request can be guided to travel through critical \ndevices that stand at the major intersections of the network, and thus quickly \nreach the desired target device by contacting only a limited number of \nintermediate devices. With the help of such an intelligent device discovery as \nSAND, the IoT devices, as well as other computing facilities, software and data \non the Internet, can autonomously establish new social connections with each \nother as human being do. They can formulate self-organized computing groups to \nperform required computing tasks, facilitate a fusion of a variety of computing \nservice, network service and data to generate novel applications and services, \nevolve from the individual aritificial intelligence to the collaborative \nintelligence, and eventually enable the birth of a robot society. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a29a5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08296"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lajanugen Logeswaran, Honglak Lee, Dragomir Radev", "title": "Sentence Ordering and Coherence Modeling using Recurrent Neural Networks. (arXiv:1611.02654v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.02654", "type": "text/html"}], "timestampUsec": "1514178133841819", "comments": [], "summary": {"content": "<p>Modeling the structure of coherent texts is a key NLP problem. The task of \ncoherently organizing a given set of sentences has been commonly used to build \nand evaluate models that understand such structure. We propose an end-to-end \nunsupervised deep learning approach based on the set-to-sequence framework to \naddress this problem. Our model strongly outperforms prior methods in the order \ndiscrimination task and a novel task of ordering abstracts from scientific \narticles. Furthermore, our work shows that useful text representations can be \nobtained by learning to order sentences. Visualizing the learned sentence \nrepresentations shows that the model captures high-level logical structure in \nparagraphs. Our representations perform comparably to state-of-the-art \npre-training methods on sentence similarity and paraphrase detection tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a29a9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.02654"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aditya Grover, Stefano Ermon", "title": "Boosted Generative Models. (arXiv:1702.08484v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1702.08484", "type": "text/html"}], "timestampUsec": "1514178133841818", "comments": [], "summary": {"content": "<p>We propose a novel approach for using unsupervised boosting to create an \nensemble of generative models, where models are trained in sequence to correct \nearlier mistakes. Our meta-algorithmic framework can leverage any existing base \nlearner that permits likelihood evaluation, including recent deep expressive \nmodels. Further, our approach allows the ensemble to include discriminative \nmodels trained to distinguish real data from model-generated data. We show \ntheoretical conditions under which incorporating a new model in the ensemble \nwill improve the fit and empirically demonstrate the effectiveness of our \nblack-box boosting algorithms on density estimation, classification, and sample \ngeneration on benchmark datasets for a wide range of generative models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a29ad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1702.08484"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anna Leontjeva, Ilya Kuzovkin", "title": "Combining Static and Dynamic Features for Multivariate Sequence Classification. (arXiv:1712.08160v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08160", "type": "text/html"}], "timestampUsec": "1514178133841817", "comments": [], "summary": {"content": "<p>Model precision in a classification task is highly dependent on the feature \nspace that is used to train the model. Moreover, whether the features are \nsequential or static will dictate which classification method can be applied as \nmost of the machine learning algorithms are designed to deal with either one or \nanother type of data. In real-life scenarios, however, it is often the case \nthat both static and dynamic features are present, or can be extracted from the \ndata. In this work, we demonstrate how generative models such as Hidden Markov \nModels (HMM) and Long Short-Term Memory (LSTM) artificial neural networks can \nbe used to extract temporal information from the dynamic data. We explore how \nthe extracted information can be combined with the static features in order to \nimprove the classification performance. We evaluate the existing techniques and \nsuggest a hybrid approach, which outperforms other methods on several public \ndatasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a29b3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08160"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Edward Raff, Jared Sylvester, Steven Mills", "title": "Fair Forests: Regularized Tree Induction to Minimize Model Bias. (arXiv:1712.08197v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08197", "type": "text/html"}], "timestampUsec": "1514178133841816", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329c1a8b4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329c1a8b4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The potential lack of fairness in the outputs of machine learning algorithms \nhas recently gained attention both within the research community as well as in \nsociety more broadly. Surprisingly, there is no prior work developing \ntree-induction algorithms for building fair decision trees or fair random \nforests. These methods have widespread popularity as they are one of the few to \nbe simultaneously interpretable, non-linear, and easy-to-use. In this paper we \ndevelop, to our knowledge, the first technique for the induction of fair \ndecision trees. We show that our \"Fair Forest\" retains the benefits of the \ntree-based approach, while providing both greater accuracy and fairness than \nother alternatives, for both \"group fairness\" and \"individual fairness.'\" We \nalso introduce new measures for fairness which are able to handle multinomial \nand continues attributes as well as regression problems, as opposed to binary \nattributes and labels only. Finally, we demonstrate a new, more robust \nevaluation procedure for algorithms that considers the dataset in its entirety \nrather than only a specific protected attribute. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a29b6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08197"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tengyuan Liang", "title": "How Well Can Generative Adversarial Networks (GAN) Learn Densities: A Nonparametric View. (arXiv:1712.08244v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08244", "type": "text/html"}], "timestampUsec": "1514178133841815", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329c84f80\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329c84f80&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We study in this paper the rate of convergence for learning densities under \nthe Generative Adversarial Networks (GANs) framework, borrowing insights from \nnonparametric statistics. We introduce an improved GAN estimator that achieves \na faster rate, through leveraging the level of smoothness in the target density \nand the evaluation metric, which in theory remedies the mode collapse problem \nreported in the literature. A minimax lower bound is constructed to show that \nwhen the dimension is large, the exponent in the rate for the new GAN estimator \nis near optimal. One can view our results as answering in a quantitative way \nhow well GAN learns a wide range of densities with different smoothness \nproperties, under a hierarchy of evaluation metrics. As a byproduct, we also \nobtain improved bounds for GAN with deeper ReLU discriminator network. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a29b8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08244"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Soufiane Belharbi, Cl&#xe9;ment Chatelain, Romain H&#xe9;rault, S&#xe9;bastien Adam", "title": "Neural Networks Regularization Through Class-wise Invariant Representation Learning. (arXiv:1709.01867v4 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.01867", "type": "text/html"}], "timestampUsec": "1514178133841814", "comments": [], "summary": {"content": "<p>Training deep neural networks is known to require a large number of training \nsamples. However, in many applications only few training samples are available. \nIn this work, we tackle the issue of training neural networks for \nclassification task when few training samples are available. We attempt to \nsolve this issue by proposing a new regularization term that constrains the \nhidden layers of a network to learn class-wise invariant representations. In \nour regularization framework, learning invariant representations is generalized \nto the class membership where samples with the same class should have the same \nrepresentation. Numerical experiments over MNIST and its variants showed that \nour proposal helps improving the generalization of neural network particularly \nwhen trained with few samples. We provide the source code of our framework \nhttps://github.com/sbelharbi/learning-class-invariant-features . \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a29bc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.01867"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1515390156, "author": "Benjamin Doerr", "title": "An Elementary Analysis of the Probability That a Binomial Random Variable Exceeds Its Expectation. (arXiv:1712.00519v4 [math.PR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00519", "type": "text/html"}], "timestampUsec": "1513924821253989", "comments": [], "summary": {"content": "<p>We give an elementary proof of the fact that a binomial random variable $X$ \nwith parameters $n$ and $0.29/n \\le p &lt; 1$ with probability at least $1/4$ \nstrictly exceeds its expectation. We also show that for $1/n \\le p &lt; 1 - 1/n$, \n$X$ exceeds its expectation by more than one with probability at least \n$0.0370$. Both probabilities approach $1/2$ when $np$ and $n(1-p)$ tend to \ninfinity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b2c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00519"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Vladimir Marochko, Leonard Johard, Manuel Mazzara, Luca Longo", "title": "Pseudorehearsal in actor-critic agents with neural network function approximation. (arXiv:1712.07686v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07686", "type": "text/html"}], "timestampUsec": "1513924821253988", "comments": [], "summary": {"content": "<p>Catastrophic forgetting has a significant negative impact in reinforcement \nlearning. The purpose of this study is to investigate how pseudorehearsal can \nchange performance of an actor-critic agent with neural-network function \napproximation. We tested agent in a pole balancing task and compared different \npseudorehearsal approaches. We have found that pseudorehearsal can assist \nlearning and decrease forgetting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b30", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07686"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Rajesh Chidambaram", "title": "Towards an unanimous international regulatory body for responsible use of Artificial Intelligence [UIRB-AI]. (arXiv:1712.07752v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07752", "type": "text/html"}], "timestampUsec": "1513924821253987", "comments": [], "summary": {"content": "<p>Artificial Intelligence (AI), is once again in the phase of drastic \nadvancements. Unarguably, the technology itself can revolutionize the way we \nlive our everyday life. But the exponential growth of technology poses a \ndaunting task for policy researchers and law makers in making amendments to the \nexisting norms. In addition, not everyone in the society is studying the \npotential socio-economic intricacies and cultural drifts that AI can bring \nabout. It is prudence to reflect from our historical past to propel the \ndevelopment of technology in the right direction. To benefit the society of the \npresent and future, I scientifically explore the societal impact of AI. While \nthere are many public and private partnerships working on similar aspects, here \nI describe the necessity for an Unanimous International Regulatory Body for all \napplications of AI (UIRB-AI). I also discuss the benefits and drawbacks of such \nan organization. To combat any drawbacks in the formation of an UIRB-AI, both \nidealistic and pragmatic perspectives are discussed alternatively. The paper \nfurther advances the discussion by proposing novel policies on how such \norganization should be structured and how it can bring about a win-win \nsituation for everyone in the society. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b38", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07752"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Seonmo Kim, Stephen McCamant", "title": "Bit-Vector Model Counting using Statistical Estimation. (arXiv:1712.07770v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1712.07770", "type": "text/html"}], "timestampUsec": "1513924821253986", "comments": [], "summary": {"content": "<p>Approximate model counting for bit-vector SMT formulas (generalizing \\#SAT) \nhas many applications such as probabilistic inference and quantitative \ninformation-flow security, but it is computationally difficult. Adding random \nparity constraints (XOR streamlining) and then checking satisfiability is an \neffective approximation technique, but it requires a prior hypothesis about the \nmodel count to produce useful results. We propose an approach inspired by \nstatistical estimation to continually refine a probabilistic estimate of the \nmodel count for a formula, so that each XOR-streamlined query yields as much \ninformation as possible. We implement this approach, with an approximate \nprobability model, as a wrapper around an off-the-shelf SMT solver or SAT \nsolver. Experimental results show that the implementation is faster than the \nmost similar previous approaches which used simpler refinement strategies. The \ntechnique also lets us model count formulas over floating-point constraints, \nwhich we demonstrate with an application to a vulnerability in differential \nprivacy mechanisms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b3e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07770"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Leon Bottou, Martin Arjovsky, David Lopez-Paz, Maxime Oquab", "title": "Geometrical Insights for Implicit Generative Modeling. (arXiv:1712.07822v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07822", "type": "text/html"}], "timestampUsec": "1513924821253985", "comments": [], "summary": {"content": "<p>Learning algorithms for implicit generative models can optimize a variety of \ncriteria that measure how the data distribution differs from the implicit model \ndistribution, including the Wasserstein distance, the Energy distance, and the \nMaximum Mean Discrepancy criterion. A careful look at the geometries induced by \nthese distances on the space of probability measures reveals interesting \ndifferences. In particular, we can establish surprising approximate global \nconvergence guarantees for the $1$-Wasserstein distance,even when the \nparametric generator has a nonconvex parametrization. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b45", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07822"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Soma Suzuki", "title": "Multiagent-based Participatory Urban Simulation through Inverse Reinforcement Learning. (arXiv:1712.07887v1 [cs.MA])", "alternate": [{"href": "http://arxiv.org/abs/1712.07887", "type": "text/html"}], "timestampUsec": "1513924821253984", "comments": [], "summary": {"content": "<p>The multiagent-based participatory simulation features prominently in urban \nplanning as the acquired model is considered as the hybrid system of the domain \nand the local knowledge. However, the key problem of generating realistic \nagents for particular social phenomena invariably remains. The existing models \nhave attempted to dictate the factors involving human behavior, which appeared \nto be intractable. In this paper, Inverse Reinforcement Learning (IRL) is \nintroduced to address this problem. IRL is developed for computational modeling \nof human behavior and has achieved great successes in robotics, psychology and \nmachine learning. The possibilities presented by this new style of modeling are \ndrawn out as conclusions, and the relative challenges with this modeling are \nhighlighted. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07887"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zhang-Wei Hong, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang Chang, Chun-Yi Lee", "title": "A Deep Policy Inference Q-Network for Multi-Agent Systems. (arXiv:1712.07893v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07893", "type": "text/html"}], "timestampUsec": "1513924821253983", "comments": [], "summary": {"content": "<p>We present DPIQN, a deep policy inference Q-network that targets multi-agent \nsystems composed of controllable agents, collaborators, and opponents that \ninteract with each other. We focus on one challenging issue in such \nsystems---modeling agents with varying strategies---and propose to employ \n\"policy features\" learned from raw observations (e.g., raw images) of \ncollaborators and opponents by inferring their policies. DPIQN incorporates the \nlearned policy features as a hidden vector into its own deep Q-network (DQN), \nsuch that it is able to predict better Q values for the controllable agents \nthan the state-of-the-art deep reinforcement learning models. We further \npropose an enhanced version of DPIQN, called deep recurrent policy inference \nQ-network (DRPIQN), for handling partial observability. Both DPIQN and DRPIQN \nare trained by an adaptive training procedure, which adjusts the network's \nattention to learn the policy features and its own Q-values at different phases \nof the training process. We present a comprehensive analysis of DPIQN and \nDRPIQN, and highlight their effectiveness and generalizability in various \nmulti-agent settings. Our models are evaluated in a classic soccer game \ninvolving both competitive and collaborative scenarios. Experimental results \nperformed on 1 vs. 1 and 2 vs. 2 games show that DPIQN and DRPIQN demonstrate \nsuperior performance to the baseline DQN and deep recurrent Q-network (DRQN) \nmodels. We also explore scenarios in which collaborators or opponents \ndynamically change their policies, and show that DPIQN and DRPIQN do lead to \nbetter overall performance in terms of stability and mean scores. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b52", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07893"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mario Lezcano Casado, Atilim Gunes Baydin, David Martinez Rubio, Tuan Anh Le, Frank Wood, Lukas Heinrich, Gilles Louppe, Kyle Cranmer, Karen Ng, Wahid Bhimji, Prabhat", "title": "Improvements to Inference Compilation for Probabilistic Programming in Large-Scale Scientific Simulators. (arXiv:1712.07901v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07901", "type": "text/html"}], "timestampUsec": "1513924821253982", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329c85165\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329c85165&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We consider the problem of Bayesian inference in the family of probabilistic \nmodels implicitly defined by stochastic generative models of data. In \nscientific fields ranging from population biology to cosmology, low-level \nmechanistic components are composed to create complex generative models. These \nmodels lead to intractable likelihoods and are typically non-differentiable, \nwhich poses challenges for traditional approaches to inference. We extend \nprevious work in \"inference compilation\", which combines universal \nprobabilistic programming and deep learning methods, to large-scale scientific \nsimulators, and introduce a C++ based probabilistic programming library called \nCPProb. We successfully use CPProb to interface with SHERPA, a large code-base \nused in particle physics. Here we describe the technical innovations realized \nand planned for this library. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b56", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07901"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jordi de la Torre, Aida Valls, Domenec Puig", "title": "A Deep Learning Interpretable Classifier for Diabetic Retinopathy Disease Grading. (arXiv:1712.08107v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08107", "type": "text/html"}], "timestampUsec": "1513924821253981", "comments": [], "summary": {"content": "<p>Deep neural network models have been proven to be very successful in image \nclassification tasks, also for medical diagnosis, but their main concern is its \nlack of interpretability. They use to work as intuition machines with high \nstatistical confidence but unable to give interpretable explanations about the \nreported results. The vast amount of parameters of these models make difficult \nto infer a rationale interpretation from them. In this paper we present a \ndiabetic retinopathy interpretable classifier able to classify retine images \ninto the different levels of disease severity and of explaining its results by \nassigning a score for every point in the hidden and input space, evaluating its \ncontribution to the final classification in a linear way. The generated visual \nmaps can be interpreted by an expert in order to compare its own knowledge with \nthe interpretation given by the model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b5d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08107"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tianyu Li, Guillaume Rabusseau, Doina Precup", "title": "Neural Network Based Nonlinear Weighted Finite Automata. (arXiv:1709.04380v2 [cs.FL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04380", "type": "text/html"}], "timestampUsec": "1513924821253980", "comments": [], "summary": {"content": "<p>Weighted finite automata (WFA) can expressively model functions defined over \nstrings but are inherently linear models. Given the recent successes of \nnonlinear models in machine learning, it is natural to wonder whether \nex-tending WFA to the nonlinear setting would be beneficial. In this paper, we \npropose a novel model of neural network based nonlinearWFA model (NL-WFA) along \nwith a learning algorithm. Our learning algorithm is inspired by the spectral \nlearning algorithm for WFAand relies on a nonlinear decomposition of the \nso-called Hankel matrix, by means of an auto-encoder network. The expressive \npower of NL-WFA and the proposed learning algorithm are assessed on both \nsynthetic and real-world data, showing that NL-WFA can lead to smaller model \nsizes and infer complex grammatical structures from data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b67", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04380"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mauro Annarumma, Giovanni Montana", "title": "Deep metric learning for multi-labelled radiographs. (arXiv:1712.07682v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07682", "type": "text/html"}], "timestampUsec": "1513924821253978", "comments": [], "summary": {"content": "<p>Many radiological studies can reveal the presence of several co-existing \nabnormalities, each one represented by a distinct visual pattern. In this \narticle we address the problem of learning a distance metric for plain \nradiographs that captures a notion of \"radiological similarity\": two chest \nradiographs are considered to be similar if they share similar abnormalities. \nDeep convolutional neural networks (DCNs) are used to learn a low-dimensional \nembedding for the radiographs that is equipped with the desired metric. Two \nloss functions are proposed to deal with multi-labelled images and potentially \nnoisy labels. We report on a large-scale study involving over 745,000 chest \nradiographs whose labels were automatically extracted from free-text \nradiological reports through a natural language processing system. Using 4,500 \nvalidated exams, we demonstrate that the methodology performs satisfactorily on \nclustering and image retrieval tasks. Remarkably, the learned metric separates \nnormal exams from those having radiological abnormalities. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b70", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07682"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Brooke E. Husic, Vijay S. Pande", "title": "Unsupervised learning of dynamical and molecular similarity using variance minimization. (arXiv:1712.07704v1 [physics.bio-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.07704", "type": "text/html"}], "timestampUsec": "1513924821253977", "comments": [], "summary": {"content": "<p>In this report, we present an unsupervised machine learning method for \ndetermining groups of molecular systems according to similarity in their \ndynamics or structures using Ward's minimum variance objective function. We \nfirst apply the minimum variance clustering to a set of simulated tripeptides \nusing the information theoretic Jensen-Shannon divergence between Markovian \ntransition matrices in order to gain insight into how point mutations affect \nprotein dynamics. Then, we extend the method to partition two chemoinformatic \ndatasets according to structural similarity to motivate a train/validation/test \nsplit for supervised learning that avoids overfitting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b79", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07704"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Dejiao Zhang, Yifan Sun, Brian Eriksson, Laura Balzano", "title": "Deep Unsupervised Clustering Using Mixture of Autoencoders. (arXiv:1712.07788v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.07788", "type": "text/html"}], "timestampUsec": "1513924821253976", "comments": [], "summary": {"content": "<p>Unsupervised clustering is one of the most fundamental challenges in machine \nlearning. A popular hypothesis is that data are generated from a union of \nlow-dimensional nonlinear manifolds; thus an approach to clustering is \nidentifying and separating these manifolds. In this paper, we present a novel \napproach to solve this problem by using a mixture of autoencoders. Our model \nconsists of two parts: 1) a collection of autoencoders where each autoencoder \nlearns the underlying manifold of a group of similar objects, and 2) a mixture \nassignment neural network, which takes the concatenated latent vectors from the \nautoencoders as input and infers the distribution over clusters. By jointly \noptimizing the two parts, we simultaneously assign data to clusters and learn \nthe underlying manifolds of each cluster. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b89", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07788"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Amal Agarwal, Lingzhou Xue", "title": "Model-Based Clustering of Nonparametric Weighted Networks. (arXiv:1712.07800v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.07800", "type": "text/html"}], "timestampUsec": "1513924821253975", "comments": [], "summary": {"content": "<p>Water pollution is a major global environmental problem, and it poses a great \nenvironmental risk to public health and biological diversity. This work is \nmotivated by assessing the potential environmental threat of coal mining \nthrough increased sulfate concentrations in river networks, which do not belong \nto any simple parametric distribution. However, existing network models mainly \nfocus on binary or discrete networks and weighted networks with known \nparametric weight distributions. We propose a principled nonparametric weighted \nnetwork model based on exponential-family random graph models and local \nlikelihood estimation and study its model-based clustering with application to \nlarge-scale water pollution network analysis. We do not require any parametric \ndistribution assumption on network weights. The proposed method greatly extends \nthe methodology and applicability of statistical network models. Furthermore, \nit is scalable to large and complex networks in large-scale environmental \nstudies and geoscientific research. The power of our proposed methods is \ndemonstrated in simulation studies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b93", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07800"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Takashi Kurokawa, Taihei Oki, Hiromichi Nagao", "title": "Multi-dimensional Graph Fourier Transform. (arXiv:1712.07811v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.07811", "type": "text/html"}], "timestampUsec": "1513924821253974", "comments": [], "summary": {"content": "<p>Many signals on Cartesian product graphs appear in the real world, such as \ndigital images, sensor observation time series, and movie ratings on Netflix. \nThese signals are \"multi-dimensional\" and have directional characteristics \nalong each factor graph. However, the existing graph Fourier transform does not \ndistinguish these directions, and assigns 1-D spectra to signals on product \ngraphs. Further, these spectra are often multi-valued at some frequencies. Our \nmain result is a multi-dimensional graph Fourier transform that solves such \nproblems associated with the conventional GFT. Using algebraic properties of \nCartesian products, the proposed transform rearranges 1-D spectra obtained by \nthe conventional GFT into the multi-dimensional frequency domain, of which each \ndimension represents a directional frequency along each factor graph. Thus, the \nmulti-dimensional graph Fourier transform enables directional frequency \nanalysis, in addition to frequency analysis with the conventional GFT. \nMoreover, this rearrangement resolves the multi-valuedness of spectra in some \ncases. The multi-dimensional graph Fourier transform is a foundation of novel \nfilterings and stationarities that utilize dimensional information of graph \nsignals, which are also discussed in this study. The proposed methods are \napplicable to a wide variety of data that can be regarded as signals on \nCartesian product graphs. This study also notes that multivariate graph signals \ncan be regarded as 2-D univariate graph signals. This correspondence provides \nnatural definitions of the multivariate graph Fourier transform and the \nmultivariate stationarity based on their 2-D univariate versions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b98", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07811"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Prateek Jain, Purushottam Kar", "title": "Non-convex Optimization for Machine Learning. (arXiv:1712.07897v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07897", "type": "text/html"}], "timestampUsec": "1513924821253973", "comments": [], "summary": {"content": "<p>A vast majority of machine learning algorithms train their models and perform \ninference by solving optimization problems. In order to capture the learning \nand prediction problems accurately, structural constraints such as sparsity or \nlow rank are frequently imposed or else the objective itself is designed to be \na non-convex function. This is especially true of algorithms that operate in \nhigh-dimensional spaces or that train non-linear models such as tensor models \nand deep networks. \n</p> \n<p>The freedom to express the learning problem as a non-convex optimization \nproblem gives immense modeling power to the algorithm designer, but often such \nproblems are NP-hard to solve. A popular workaround to this has been to relax \nnon-convex problems to convex ones and use traditional methods to solve the \n(convex) relaxed optimization problems. However this approach may be lossy and \nnevertheless presents significant challenges for large scale optimization. \n</p> \n<p>On the other hand, direct approaches to non-convex optimization have met with \nresounding success in several domains and remain the methods of choice for the \npractitioner, as they frequently outperform relaxation-based techniques - \npopular heuristics include projected gradient descent and alternating \nminimization. However, these are often poorly understood in terms of their \nconvergence and other properties. \n</p> \n<p>This monograph presents a selection of recent advances that bridge a \nlong-standing gap in our understanding of these heuristics. The monograph will \nlead the reader through several widely used non-convex optimization techniques, \nas well as applications thereof. The goal of this monograph is to both, \nintroduce the rich literature in this area, as well as equip the reader with \nthe tools and techniques needed to analyze these simple procedures for \nnon-convex problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b9f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07897"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Philipp Hacker, Emil Wiedemann", "title": "A continuous framework for fairness. (arXiv:1712.07924v1 [cs.CY])", "alternate": [{"href": "http://arxiv.org/abs/1712.07924", "type": "text/html"}], "timestampUsec": "1513924821253972", "comments": [], "summary": {"content": "<p>Increasingly, discrimination by algorithms is perceived as a societal and \nlegal problem. As a response, a number of criteria for implementing algorithmic \nfairness in machine learning have been developed in the literature. This paper \nproposes the Continuous Fairness Algorithm (CFA$\\theta$) which enables a \ncontinuous interpolation between different fairness definitions. More \nspecifically, we make three main contributions to the existing literature. \nFirst, our approach allows the decision maker to continuously vary between \nconcepts of individual and group fairness. As a consequence, the algorithm \nenables the decision maker to adopt intermediate \"worldviews\" on the degree of \ndiscrimination encoded in algorithmic processes, adding nuance to the extreme \ncases of \"we're all equal\" (WAE) and \"what you see is what you get\" (WYSIWYG) \nproposed so far in the literature. Second, we use optimal transport theory, and \nspecifically the concept of the barycenter, to maximize decision maker utility \nunder the chosen fairness constraints. Third, the algorithm is able to handle \ncases of intersectionality, i.e., of multi-dimensional discrimination of \ncertain groups on grounds of several criteria. We discuss three main examples \n(college admissions; credit application; insurance contracts) and map out the \npolicy implications of our approach. The explicit formalization of the \ntrade-off between individual and group fairness allows this post-processing \napproach to be tailored to different situational contexts in which one or the \nother fairness criterion may take precedence. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704ba4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07924"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ravi Tejwani, Adam Liska, Hongyuan You, Jenna Reinen, Payel Das", "title": "Autism Classification Using Brain Functional Connectivity Dynamics and Machine Learning. (arXiv:1712.08041v1 [q-bio.NC])", "alternate": [{"href": "http://arxiv.org/abs/1712.08041", "type": "text/html"}], "timestampUsec": "1513924821253971", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329c8532c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329c8532c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The goal of the present study is to identify autism using machine learning \ntechniques and resting-state brain imaging data, leveraging the temporal \nvariability of the functional connections (FC) as the only information. We \nestimated and compared the FC variability across brain regions between typical, \nhealthy subjects and autistic population by analyzing brain imaging data from a \nworld-wide multi-site database known as ABIDE (Autism Brain Imaging Data \nExchange). Our analysis revealed that patients diagnosed with autism spectrum \ndisorder (ASD) show increased FC variability in several brain regions that are \nassociated with low FC variability in the typical brain. We then used the \nenhanced FC variability of brain regions as features for training machine \nlearning models for ASD classification and achieved 65% accuracy in \nidentification of ASD versus control subjects within the dataset. We also used \nnode strength estimated from number of functional connections per node averaged \nover the whole scan as features for ASD classification.The results reveal that \nthe dynamic FC measures outperform or are comparable with the static FC \nmeasures in predicting ASD. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704ba9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08041"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Topi Paananen, Juho Piironen, Michael Riis Andersen, Aki Vehtari", "title": "Model selection for Gaussian processes utilizing sensitivity of posterior predictive distribution. (arXiv:1712.08048v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.08048", "type": "text/html"}], "timestampUsec": "1513924821253970", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329cf0a5f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329cf0a5f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose two novel methods for simplifying Gaussian process (GP) models by \nexamining the predictions of a full model in the vicinity of the training \npoints and thereby ordering the covariates based on their predictive relevance. \nOur results on synthetic and real world data sets demonstrate improved variable \nselection compared to automatic relevance determination (ARD) in terms of \nconsistency and predictive performance. We expect our proposed methods to be \nuseful in interpreting and understanding complex Gaussian process models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704bb2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08048"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Christian Dansereau, Angela Tam, AmanPreet Badhwar, Sebastian Urchs, Pierre Orban, Pedro Rosa-Neto, Pierre Bellec", "title": "A brain signature highly predictive of future progression to Alzheimer's dementia. (arXiv:1712.08058v1 [q-bio.QM])", "alternate": [{"href": "http://arxiv.org/abs/1712.08058", "type": "text/html"}], "timestampUsec": "1513924821253969", "comments": [], "summary": {"content": "<p>Early prognosis of Alzheimer's dementia is hard. Mild cognitive impairment \n(MCI) typically precedes Alzheimer's dementia, yet only a fraction (30%-50%) of \nMCI individuals will progress to dementia. Even when a prognosis of dementia is \nestablished using machine learning models and biomarkers, the fraction of MCI \nprogressors remain limited (50%-75%). Instead of aiming at precise diagnosis in \nlarge clinical cohorts known for their heterogeneity, we propose here to \nidentify only a subset of individuals who share a common brain signature highly \npredictive of oncoming dementia. This signature was discovered using a machine \nlearning model in a reference public sample (ADNI), where the model was trained \nto identify patterns of brain atrophy and functional dysconnectivity commonly \nseen in patients suffering from dementia (N = 24), and not seen in cognitively \nnormal individuals (N = 49). The model then recognized the same brain signature \nin 10 MCI individuals, out of N = 56, 90% of which progressed to dementia \nwithin three years. This result is a marked improvement on the state-of-the-art \nin prognostic precision, while the brain signature still identified 47% of all \nMCI progressors (N = 19). We thus discovered a sizable MCI subpopulation with \nhomogeneous brain abnormalities and highly predictable clinical trajectories, \nwhich may represent an excellent recruitment target for clinical trials at the \nprodromal stage of Alzheimer's disease. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704bbd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08058"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sebastiaan H&#xf6;ppner, Eugen Stripling, Bart Baesens, Seppe vanden Broucke, Tim Verdonck", "title": "Profit Driven Decision Trees for Churn Prediction. (arXiv:1712.08101v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08101", "type": "text/html"}], "timestampUsec": "1513924821253968", "comments": [], "summary": {"content": "<p>Customer retention campaigns increasingly rely on predictive models to detect \npotential churners in a vast customer base. From the perspective of machine \nlearning, the task of predicting customer churn can be presented as a binary \nclassification problem. Using data on historic behavior, classification \nalgorithms are built with the purpose of accurately predicting the probability \nof a customer defecting. The predictive churn models are then commonly selected \nbased on accuracy related performance measures such as the area under the ROC \ncurve (AUC). However, these models are often not well aligned with the core \nbusiness requirement of profit maximization, in the sense that, the models fail \nto take into account not only misclassification costs, but also the benefits \noriginating from a correct classification. Therefore, the aim is to construct \nchurn prediction models that are profitable and preferably interpretable too. \nThe recently developed expected maximum profit measure for customer churn \n(EMPC) has been proposed in order to select the most profitable churn model. We \npresent a new classifier that integrates the EMPC metric directly into the \nmodel construction. Our technique, called ProfTree, uses an evolutionary \nalgorithm for learning profit driven decision trees. In a benchmark study with \nreal-life data sets from various telecommunication service providers, we show \nthat ProfTree achieves significant profit improvements compared to classic \naccuracy driven tree-based methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704bc4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08101"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Paul Hand, Babhru Joshi", "title": "A Convex Program for Mixed Linear Regression with a Recovery Guarantee for Well-Separated Data. (arXiv:1612.06067v2 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1612.06067", "type": "text/html"}], "timestampUsec": "1513924821253967", "comments": [], "summary": {"content": "<p>We introduce a convex approach for mixed linear regression over $d$ features. \nThis approach is a second-order cone program, based on L1 minimization, which \nassigns an estimate regression coefficient in $\\mathbb{R}^{d}$ for each data \npoint. These estimates can then be clustered using, for example, $k$-means. For \nproblems with two or more mixture classes, we prove that the convex program \nexactly recovers all of the mixture components in the noiseless setting under \ntechnical conditions that include a well-separation assumption on the data. \nUnder these assumptions, recovery is possible if each class has at least $d$ \nindependent measurements. We also explore an iteratively reweighted least \nsquares implementation of this method on real and synthetic data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704bca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.06067"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Johanna &#xc4;rje, Ville Tirronen, Salme K&#xe4;rkk&#xe4;inen, Kristian Meissner, Jenni Raitoharju, Alexandros Iosifidis, Moncef Gabbouj, Serkan Kiranyaz", "title": "Human experts vs. machines in taxa recognition. (arXiv:1708.06899v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.06899", "type": "text/html"}], "timestampUsec": "1513924821253966", "comments": [], "summary": {"content": "<p>Biomonitoring of waterbodies is vital as the number of anthropogenic \nstressors on aquatic ecosystems keeps growing. However, the continuous decrease \nin funding makes it impossible to meet monitoring goals or sustain traditional \nmanual sample processing. In this paper, we review what kind of statistical \ntools can be used to enhance the cost efficiency of biomonitoring: We explore \nautomated identification of freshwater macroinvertebrates which are used as one \nindicator group in biomonitoring of aquatic ecosystems. We present the first \nclassification results of a new imaging system producing multiple images per \nspecimen. Moreover, these results are compared with the results of human \nexperts. On a data set of 29 taxonomical groups, automated classification \nproduces a higher average accuracy than human experts. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704bdc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.06899"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Christian Donner, Manfred Opper", "title": "Inverse Ising problem in continuous time: A latent variable approach. (arXiv:1709.04495v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04495", "type": "text/html"}], "timestampUsec": "1513924821253965", "comments": [], "summary": {"content": "<p>We consider the inverse Ising problem, i.e. the inference of network \ncouplings from observed spin trajectories for a model with continuous time \nGlauber dynamics. By introducing two sets of auxiliary latent random variables \nwe render the likelihood into a form, which allows for simple iterative \ninference algorithms with analytical updates. The variables are: (1) Poisson \nvariables to linearise an exponential term which is typical for point process \nlikelihoods and (2) P\\'olya-Gamma variables, which make the likelihood \nquadratic in the coupling parameters. Using the augmented likelihood, we derive \nan expectation-maximization (EM) algorithm to obtain the maximum likelihood \nestimate of network parameters. Using a third set of latent variables we extend \nthe EM algorithm to sparse couplings via L1 regularization. Finally, we develop \nan efficient approximate Bayesian inference algorithm using a variational \napproach. We demonstrate the performance of our algorithms on data simulated \nfrom an Ising model. For data which are simulated from a more biologically \nplausible network with spiking neurons, we show that the Ising model captures \nwell the low order statistics of the data and how the Ising couplings are \nrelated to the underlying synaptic structure of the simulated network. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704be2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04495"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Lester Mackey, Vasilis Syrgkanis, Ilias Zadik", "title": "Orthogonal Machine Learning: Power and Limitations. (arXiv:1711.00342v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.00342", "type": "text/html"}], "timestampUsec": "1513924821253964", "comments": [], "summary": {"content": "<p>Double machine learning provides $\\sqrt{n}$-consistent estimates of \nparameters of interest even when high-dimensional or nonparametric nuisance \nparameters are estimated at an $n^{-1/4}$ rate. The key is to employ \nNeyman-orthogonal moment equations which are first-order insensitive to \nperturbations in the nuisance parameters. We show that the $n^{-1/4}$ \nrequirement can be improved to $n^{-1/(2k+2)}$ by employing a $k$-th order \nnotion of orthogonality that grants robustness to more complex or \nhigher-dimensional nuisance parameters. In the partially linear regression \nsetting popular in causal inference, we show that we can construct second-order \northogonal moments if and only if the treatment residual is not normally \ndistributed. Our proof relies on Stein's lemma and may be of independent \ninterest. We conclude by demonstrating the robustness benefits of an explicit \ndoubly-orthogonal estimation procedure for treatment effect. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704bea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.00342"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Davide Venturelli, Minh Do, Eleanor Rieffel, Jeremy Frank", "title": "Compiling quantum circuits to realistic hardware architectures using temporal planners. (arXiv:1705.08927v2 [quant-ph] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08927", "type": "text/html"}], "timestampUsec": "1513920223111160", "comments": [], "summary": {"content": "<p>To run quantum algorithms on emerging gate-model quantum hardware, quantum \ncircuits must be compiled to take into account constraints on the hardware. For \nnear-term hardware, with only limited means to mitigate decoherence, it is \ncritical to minimize the duration of the circuit. We investigate the \napplication of temporal planners to the problem of compiling quantum circuits \nto newly emerging quantum hardware. While our approach is general, we focus on \ncompiling to superconducting hardware architectures with nearest neighbor \nconstraints. Our initial experiments focus on compiling Quantum Alternating \nOperator Ansatz (QAOA) circuits whose high number of commuting gates allow \ngreat flexibility in the order in which the gates can be applied. That freedom \nmakes it more challenging to find optimal compilations but also means there is \na greater potential win from more optimized compilation than for less flexible \ncircuits. We map this quantum circuit compilation problem to a temporal \nplanning problem, and generated a test suite of compilation problems for QAOA \ncircuits of various sizes to a realistic hardware architecture. We report \ncompilation results from several state-of-the-art temporal planners on this \ntest set. This early empirical evaluation demonstrates that temporal planning \nis a viable approach to quantum circuit compilation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513920223111", "annotations": [], "published": 1513920224, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c67bd79", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08927"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tien Huu Do, Duc Minh Nguyen, Evaggelia Tsiligianni, Bruno Cornelis, Nikos Deligiannis", "title": "Multiview Deep Learning for Predicting Twitter Users' Location. (arXiv:1712.08091v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08091", "type": "text/html"}], "timestampUsec": "1513920223111158", "comments": [], "summary": {"content": "<p>The problem of predicting the location of users on large social networks like \nTwitter has emerged from real-life applications such as social unrest detection \nand online marketing. Twitter user geolocation is a difficult and active \nresearch topic with a vast literature. Most of the proposed methods follow \neither a content-based or a network-based approach. The former exploits \nuser-generated content while the latter utilizes the connection or interaction \nbetween Twitter users. In this paper, we introduce a novel method combining the \nstrength of both approaches. Concretely, we propose a multi-entry neural \nnetwork architecture named MENET leveraging the advances in deep learning and \nmultiview learning. The generalizability of MENET enables the integration of \nmultiple data representations. In the context of Twitter user geolocation, we \nrealize MENET with textual, network, and metadata features. Considering the \nnatural distribution of Twitter users across the concerned geographical area, \nwe subdivide the surface of the earth into multi-scale cells and train MENET \nwith the labels of the cells. We show that our method outperforms the state of \nthe art by a large margin on three benchmark datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513920223111", "annotations": [], "published": 1513920224, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c67bd9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08091"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "J&#xf6;rg L&#xfc;cke, Zhenwen Dai, Georgios Exarchakis", "title": "Truncated Variational Sampling for \"Black Box\" Optimization of Generative Models. (arXiv:1712.08104v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08104", "type": "text/html"}], "timestampUsec": "1513920223111157", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329cf0c92\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329cf0c92&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We investigate the optimization of two generative models with binary hidden \nvariables using a novel variational EM approach. The novel approach \ndistinguishes itself from previous variational approaches by using hidden \nstates as variational parameters. Here we use efficient and general purpose \nsampling procedures to vary the hidden states, and investigate the \"black box\" \napplicability of the resulting optimization procedure. For general purpose \napplicability, samples are drawn from approximate marginal distributions of the \nconsidered generative model and from the prior distribution of a given \ngenerative model. As such, sampling is defined in a generic form with no \nadditional derivations required. As a proof of concept, we then apply the novel \nprocedure (A) to Binary Sparse Coding (a model with continuous observables), \nand (B) to basic Sigmoid Belief Networks (which are models with binary \nobservables). The approach is applicable without any further analytical steps \nand efficiently as well as effectively increases the variational free-energy \nobjective. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513920223111", "annotations": [], "published": 1513920224, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c67bda8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08104"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David Venuto, Toby Dylan Hocking, Lakjaree Sphanurattana, Masashi Sugiyama", "title": "Support vector comparison machines. (arXiv:1401.8008v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1401.8008", "type": "text/html"}], "timestampUsec": "1513920223111156", "comments": [], "summary": {"content": "<p>In ranking problems, the goal is to learn a ranking function from labeled \npairs of input points. In this paper, we consider the related comparison \nproblem, where the label indicates which element of the pair is better, or if \nthere is no significant difference. We cast the learning problem as a margin \nmaximization, and show that it can be solved by converting it to a standard \nSVM. We use simulated nonlinear patterns, a real learning to rank sushi data \nset, and a chess data set to show that our proposed SVMcompare algorithm \noutperforms SVMrank when there are equality pairs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513920223111", "annotations": [], "published": 1513920224, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c67bdaf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1401.8008"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jamie Hayes, George Danezis", "title": "Learning Universal Adversarial Perturbations with Generative Models. (arXiv:1708.05207v2 [cs.CR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.05207", "type": "text/html"}], "timestampUsec": "1513920223111155", "comments": [], "summary": {"content": "<p>Neural networks are known to be vulnerable to adversarial examples, inputs \nthat have been intentionally perturbed to remain visually similar to the source \ninput, but cause a misclassification. It was recently shown that given a \ndataset and classifier, there exists so called universal adversarial \nperturbations, a single perturbation that causes a misclassification when \napplied to any input. In this work, we introduce universal adversarial \nnetworks, a generative network that is capable of fooling a target classifier \nwhen it's generated output is added to a clean sample from a dataset. We show \nthat this technique improves on known universal adversarial attacks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513920223111", "annotations": [], "published": 1513920224, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c67bdb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.05207"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chelsea Finn, Sergey Levine", "title": "Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm. (arXiv:1710.11622v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.11622", "type": "text/html"}], "timestampUsec": "1513833379178756", "comments": [], "summary": {"content": "<p>Learning to learn is a powerful paradigm for enabling models to learn from \ndata more effectively and efficiently. A popular approach to meta-learning is \nto train a recurrent model to read in a training dataset as input and output \nthe parameters of a learned model, or output predictions for new test inputs. \nAlternatively, a more recent approach to meta-learning aims to acquire deep \nrepresentations that can be effectively fine-tuned, via standard gradient \ndescent, to new tasks. In this paper, we consider the meta-learning problem \nfrom the perspective of universality, formalizing the notion of learning \nalgorithm approximation and comparing the expressive power of the \naforementioned recurrent models to the more recent approaches that embed \ngradient descent into the meta-learner. In particular, we seek to answer the \nfollowing question: does deep representation combined with standard gradient \ndescent have sufficient capacity to approximate any learning algorithm? We find \nthat this is indeed true, and further find, in our experiments, that \ngradient-based meta-learning consistently leads to learning strategies that \ngeneralize more widely compared to those represented by recurrent models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833379, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa2815", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.11622"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Huishuai Zhang, Caiming Xiong, James Bradbury, Richard Socher", "title": "Block-diagonal Hessian-free Optimization for Training Neural Networks. (arXiv:1712.07296v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.07296", "type": "text/html"}], "timestampUsec": "1513833379178755", "comments": [], "summary": {"content": "<p>Second-order methods for neural network optimization have several advantages \nover methods based on first-order gradient descent, including better scaling to \nlarge mini-batch sizes and fewer updates needed for convergence. But they are \nrarely applied to deep learning in practice because of high computational cost \nand the need for model-dependent algorithmic variations. We introduce a variant \nof the Hessian-free method that leverages a block-diagonal approximation of the \ngeneralized Gauss-Newton matrix. Our method computes the curvature \napproximation matrix only for pairs of parameters from the same layer or block \nof the neural network and performs conjugate gradient updates independently for \neach block. Experiments on deep autoencoders, deep convolutional networks, and \nmultilayer LSTMs demonstrate better convergence and generalization compared to \nthe original Hessian-free approach and the Adam method. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833379, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa282e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07296"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiangyu Kong, Bo Xin, Fangchen Liu, Yizhou Wang", "title": "Revisiting the Master-Slave Architecture in Multi-Agent Deep Reinforcement Learning. (arXiv:1712.07305v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07305", "type": "text/html"}], "timestampUsec": "1513833379178754", "comments": [], "summary": {"content": "<p>Many tasks in artificial intelligence require the collaboration of multiple \nagents. We exam deep reinforcement learning for multi-agent domains. Recent \nresearch efforts often take the form of two seemingly conflicting perspectives, \nthe decentralized perspective, where each agent is supposed to have its own \ncontroller; and the centralized perspective, where one assumes there is a \nlarger model controlling all agents. In this regard, we revisit the idea of the \nmaster-slave architecture by incorporating both perspectives within one \nframework. Such a hierarchical structure naturally leverages advantages from \none another. The idea of combining both perspectives is intuitive and can be \nwell motivated from many real world systems, however, out of a variety of \npossible realizations, we highlights three key ingredients, i.e. composed \naction representation, learnable communication and independent reasoning. With \nnetwork designs to facilitate these explicitly, our proposal consistently \noutperforms latest competing methods both in synthetic experiments and when \napplied to challenging StarCraft micromanagement tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833379, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa2844", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07305"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Martin Schrimpf, Stephen Merity, James Bradbury, Richard Socher", "title": "A Flexible Approach to Automated RNN Architecture Generation. (arXiv:1712.07316v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.07316", "type": "text/html"}], "timestampUsec": "1513833379178753", "comments": [], "summary": {"content": "<p>The process of designing neural architectures requires expert knowledge and \nextensive trial and error. While automated architecture search may simplify \nthese requirements, the recurrent neural network (RNN) architectures generated \nby existing methods are limited in both flexibility and components. We propose \na domain-specific language (DSL) for use in automated architecture search which \ncan produce novel RNNs of arbitrary depth and width. The DSL is flexible enough \nto define standard architectures such as the Gated Recurrent Unit and Long \nShort Term Memory and allows the introduction of non-standard RNN components \nsuch as trigonometric curves and layer normalization. Using two different \ncandidate generation techniques, random search with a ranking function and \nreinforcement learning, we explore the novel architectures produced by the RNN \nDSL for language modeling and machine translation domains. The resulting \narchitectures do not follow human intuition yet perform well on their targeted \ntasks, suggesting the space of usable RNN architectures is far larger than \npreviously assumed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833379, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa284e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07316"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiaowei Jia, Ankush Khandelwal, Anuj Karpatne, Vipin Kumar", "title": "Discovery of Shifting Patterns in Sequence Classification. (arXiv:1712.07203v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.07203", "type": "text/html"}], "timestampUsec": "1513833379178752", "comments": [], "summary": {"content": "<p>In this paper, we investigate the multi-variate sequence classification \nproblem from a multi-instance learning perspective. Real-world sequential data \ncommonly show discriminative patterns only at specific time periods. For \ninstance, we can identify a cropland during its growing season, but it looks \nsimilar to a barren land after harvest or before planting. Besides, even within \nthe same class, the discriminative patterns can appear in different periods of \nsequential data. Due to such property, these discriminative patterns are also \nreferred to as shifting patterns. The shifting patterns in sequential data \nseverely degrade the performance of traditional classification methods without \nsufficient training data. \n</p> \n<p>We propose a novel sequence classification method by automatically mining \nshifting patterns from multi-variate sequence. The method employs a \nmulti-instance learning approach to detect shifting patterns while also \nmodeling temporal relationships within each multi-instance bag by an LSTM model \nto further improve the classification performance. We extensively evaluate our \nmethod on two real-world applications - cropland mapping and affective state \nrecognition. The experiments demonstrate the superiority of our proposed method \nin sequence classification performance and in detecting discriminative shifting \npatterns. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833379, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa2866", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07203"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "John E. Herr, Kun Yao, Ryker McIntyre, David Toth, John Parkhill", "title": "Metadynamics for Training Neural Network Model Chemistries: a Competitive Assessment. (arXiv:1712.07240v1 [physics.chem-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.07240", "type": "text/html"}], "timestampUsec": "1513833379178751", "comments": [], "summary": {"content": "<p>Neural network (NN) model chemistries (MCs) promise to facilitate the \naccurate exploration of chemical space and simulation of large reactive \nsystems. One important path to improving these models is to add layers of \nphysical detail, especially long-range forces. At short range, however, these \nmodels are data driven and data limited. Little is systematically known about \nhow data should be sampled, and `test data' chosen randomly from some sampling \ntechniques can provide poor information about generality. If the sampling \nmethod is narrow `test error' can appear encouragingly tiny while the model \nfails catastrophically elsewhere. In this manuscript we competitively evaluate \ntwo common sampling methods: molecular dynamics (MD), normal-mode sampling \n(NMS) and one uncommon alternative, Metadynamics (MetaMD), for preparing \ntraining geometries. We show that MD is an inefficient sampling method in the \nsense that additional samples do not improve generality. We also show MetaMD is \neasily implemented in any NNMC software package with cost that scales linearly \nwith the number of atoms in a sample molecule. MetaMD is a black-box way to \nensure samples always reach out to new regions of chemical space, while \nremaining relevant to chemistry near $k_bT$. It is one cheap tool to address \nthe issue of generalization. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833379, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa2879", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07240"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Martin Wistuba", "title": "Finding Competitive Network Architectures Within a Day Using UCT. (arXiv:1712.07420v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.07420", "type": "text/html"}], "timestampUsec": "1513833379178750", "comments": [], "summary": {"content": "<p>The design of neural network architectures for a new data set is a laborious \ntask which requires human deep learning expertise. In order to make deep \nlearning available for a broader audience, automated methods for finding a \nneural network architecture are vital. Recently proposed methods can already \nachieve human expert level performances. However, these methods have run times \nof months or even years of GPU computing time, ignoring hardware constraints as \nfaced by many researchers and companies. We propose the use of Monte Carlo \nplanning in combination with two different UCT (upper confidence bound applied \nto trees) derivations to search for network architectures. We adapt the UCT \nalgorithm to the needs of network architecture search by proposing two ways of \nsharing information between different branches of the search tree. In an \nempirical study we are able to demonstrate that this method is able to find \ncompetitive networks for MNIST, SVHN and CIFAR-10 in just a single GPU day. \nExtending the search time to five GPU days, we are able to outperform human \narchitectures and our competitors which consider the same types of layers. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833379, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa288b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07420"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Vishwak Srinivasan, Adepu Ravi Sankar, Vineeth N Balasubramanian", "title": "ADINE: An Adaptive Momentum Method for Stochastic Gradient Descent. (arXiv:1712.07424v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07424", "type": "text/html"}], "timestampUsec": "1513833379178749", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329cf0e8f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329cf0e8f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Two major momentum-based techniques that have achieved tremendous success in \noptimization are Polyak's heavy ball method and Nesterov's accelerated \ngradient. A crucial step in all momentum-based methods is the choice of the \nmomentum parameter $m$ which is always suggested to be set to less than $1$. \nAlthough the choice of $m &lt; 1$ is justified only under very strong theoretical \nassumptions, it works well in practice even when the assumptions do not \nnecessarily hold. In this paper, we propose a new momentum based method \n$\\textit{ADINE}$, which relaxes the constraint of $m &lt; 1$ and allows the \nlearning algorithm to use adaptive higher momentum. We motivate our hypothesis \non $m$ by experimentally verifying that a higher momentum ($\\ge 1$) can help \nescape saddles much faster. Using this motivation, we propose our method \n$\\textit{ADINE}$ that helps weigh the previous updates more (by setting the \nmomentum parameter $&gt; 1$), evaluate our proposed algorithm on deep neural \nnetworks and show that $\\textit{ADINE}$ helps the learning algorithm to \nconverge much faster without compromising on the generalization error. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833379, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa28ba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07424"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Markus Wulfmeier, Alex Bewley, Ingmar Posner", "title": "Incremental Adversarial Domain Adaptation. (arXiv:1712.07436v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07436", "type": "text/html"}], "timestampUsec": "1513833379178748", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329d7e045\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329d7e045&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Continuous appearance shifts such as changes in weather and lighting \nconditions can impact the performance of deployed machine learning models. \nUnsupervised domain adaptation aims to address this challenge, though current \napproaches do not utilise the continuity of the occurring shifts. Many robotic \napplications exhibit these conditions and thus facilitate the potential to \nincrementally adapt a learnt model over minor shifts which integrate to massive \ndifferences over time. Our work presents an adversarial approach for lifelong, \nincremental domain adaptation which benefits from unsupervised alignment to a \nseries of sub-domains which successively diverge from the labelled source \ndomain. We demonstrate on a drivable-path segmentation task that our \nincremental approach can better handle large appearance changes, e.g. day to \nnight, compared with a prior single alignment step approach. Furthermore, by \napproximating the marginal feature distribution for the source domain with a \ngenerative adversarial network, the deployment module can be rendered fully \nindependent of retaining potentially large amounts of the related source \ntraining data for only a minor reduction in performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833380, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa28d9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07436"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Robin C. Geyer, Tassilo Klein, Moin Nabi", "title": "Differentially Private Federated Learning: A Client Level Perspective. (arXiv:1712.07557v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1712.07557", "type": "text/html"}], "timestampUsec": "1513833379178747", "comments": [], "summary": {"content": "<p>Federated learning is a recent advance in privacy protection. In this \ncontext, a trusted curator aggregates parameters optimized in decentralized \nfashion by multiple clients. The resulting model is then distributed back to \nall clients, ultimately converging to a joint representative model without \nexplicitly having to share the data. However, the protocol is vulnerable to \ndifferential attacks, which could originate from any party contributing during \nfederated optimization. In such an attack, a client's contribution during \ntraining and information about their data set is revealed through analyzing the \ndistributed model. We tackle this problem and propose an algorithm for client \nsided differential privacy preserving federated optimization. The aim is to \nhide clients' contributions during training, balancing the trade-off between \nprivacy loss and model performance. Empirical studies suggest that given a \nsufficiently large number of participating clients, our proposed procedure can \nmaintain client-level differential privacy at only a minor cost in model \nperformance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833380, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa28eb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07557"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kristof T. Sch&#xfc;tt, Pieter-Jan Kindermans, Huziel E. Sauceda, Stefan Chmiela, Alexandre Tkatchenko, Klaus-Robert M&#xfc;ller", "title": "SchNet: A continuous-filter convolutional neural network for modeling quantum interactions. (arXiv:1706.08566v5 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.08566", "type": "text/html"}], "timestampUsec": "1513833379178746", "comments": [], "summary": {"content": "<p>Deep learning has the potential to revolutionize quantum chemistry as it is \nideally suited to learn representations for structured data and speed up the \nexploration of chemical space. While convolutional neural networks have proven \nto be the first choice for images, audio and video data, the atoms in molecules \nare not restricted to a grid. Instead, their precise locations contain \nessential physical information, that would get lost if discretized. Thus, we \npropose to use continuous-filter convolutional layers to be able to model local \ncorrelations without requiring the data to lie on a grid. We apply those layers \nin SchNet: a novel deep learning architecture modeling quantum interactions in \nmolecules. We obtain a joint model for the total energy and interatomic forces \nthat follows fundamental quantum-chemical principles. This includes \nrotationally invariant energy predictions and a smooth, differentiable \npotential energy surface. Our architecture achieves state-of-the-art \nperformance for benchmarks of equilibrium molecules and molecular dynamics \ntrajectories. Finally, we introduce a more challenging benchmark with chemical \nand structural variations that suggests the path for further work. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833380, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa28f8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.08566"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Joeseph P. Smith, Andrew D. Gronewold", "title": "Development and analysis of a Bayesian water balance model for large lake systems. (arXiv:1710.10161v2 [stat.AP] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.10161", "type": "text/html"}], "timestampUsec": "1513833379178745", "comments": [], "summary": {"content": "<p>Water balance models are often employed to improve understanding of drivers \nof change in regional hydrologic cycles. Most of these models, however, are \nphysically-based, and few employ state-of-the-art statistical methods to \nreconcile measurement uncertainty and bias. Here, we introduce a framework for \ndeveloping, analyzing, and selecting among alternative formulations of a \nstatistical water balance model for large lake systems that addresses this \nresearch gap. We demonstrate our new analytical framework using a model \ncustomized for Lakes Superior and Michigan-Huron, the two largest lakes on \nEarth by surface area. The selected model (from among 26 alternatives) closed \nthe water balance across both lakes with an order of magnitude less computation \ntime than prototype versions of the same model. We expect our new framework \nwill be used to improve computational efficiency and skill of water balance \nmodels for other lakes around the world. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833380, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa290d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.10161"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sidney Marlon Lopes de Lima, Abel Guilhermino da Silva Filho, Wellington Pinheiro dos Santos", "title": "Detection and classification of masses in mammographic images in a multi-kernel approach. (arXiv:1712.07116v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.07116", "type": "text/html"}], "timestampUsec": "1513832809776845", "comments": [], "summary": {"content": "<p>According to the World Health Organization, breast cancer is the main cause \nof cancer death among adult women in the world. Although breast cancer occurs \nindiscriminately in countries with several degrees of social and economic \ndevelopment, among developing and underdevelopment countries mortality rates \nare still high, due to low availability of early detection technologies. From \nthe clinical point of view, mammography is still the most effective diagnostic \ntechnology, given the wide diffusion of the use and interpretation of these \nimages. Herein this work we propose a method to detect and classify \nmammographic lesions using the regions of interest of images. Our proposal \nconsists in decomposing each image using multi-resolution wavelets. Zernike \nmoments are extracted from each wavelet component. Using this approach we can \ncombine both texture and shape features, which can be applied both to the \ndetection and classification of mammary lesions. We used 355 images of fatty \nbreast tissue of IRMA database, with 233 normal instances (no lesion), 72 \nbenign, and 83 malignant cases. Classification was performed by using SVM and \nELM networks with modified kernels, in order to optimize accuracy rates, \nreaching 94.11%. Considering both accuracy rates and training times, we defined \nthe ration between average percentage accuracy and average training time in a \nreverse order. Our proposal was 50 times higher than the ratio obtained using \nthe best method of the state-of-the-art. As our proposed model can combine high \naccuracy rate with low learning time, whenever a new data is received, our work \nwill be able to save a lot of time, hours, in learning process in relation to \nthe best method of the state-of-the-art. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91ee7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07116"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tyler A. Spears, Brandon G. Jacques, Marc W. Howard, Per B. Sederberg", "title": "Scale-invariant temporal history (SITH): optimal slicing of the past in an uncertain world. (arXiv:1712.07165v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07165", "type": "text/html"}], "timestampUsec": "1513832809776844", "comments": [], "summary": {"content": "<p>In both the human brain and any general artificial intelligence (AI), a \nrepresentation of the past is necessary to predict the future. However, perfect \nstorage of all experiences is not possible. One possibility, utilized in many \napplications, is to retain information about the past in a buffer. A limitation \nof this approach is that although events in the buffer are represented with \nperfect accuracy, the resources necessary to represent information at a \nparticular time scale go up rapidly. Here we present a neurally-plausible, \ncompressed, scale-free memory representation we call Scale-Invariant Temporal \nHistory (SITH). This representation covers an exponentially large period of \ntime in the past at the cost of sacrificing temporal accuracy for events \nfurther in the past. The form of this decay is scale-invariant and can be shown \nto be optimal in that it is able to respond to worlds with a wide range of time \nscales. We demonstrate the utility of this representation in learning to play a \nsimple video game. In this environment, SITH exhibits better learning \nperformance than a fixed-size buffer history representation. Whereas the buffer \nperforms well as long as the temporal dependencies can be represented within \nthe buffer, SITH performs well over a much larger range of time scales for the \nsame amount of resources. Finally, we discuss how the application of SITH, \nalong with other human-inspired models of cognition, could improve \nreinforcement and machine learning algorithms in general. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91ee8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07165"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Rajesh Bordawekar, Bortik Bandyopadhyay, Oded Shmueli", "title": "Cognitive Database: A Step towards Endowing Relational Databases with Artificial Intelligence Capabilities. (arXiv:1712.07199v1 [cs.DB])", "alternate": [{"href": "http://arxiv.org/abs/1712.07199", "type": "text/html"}], "timestampUsec": "1513832809776843", "comments": [], "summary": {"content": "<p>We propose Cognitive Databases, an approach for transparently enabling \nArtificial Intelligence (AI) capabilities in relational databases. A novel \naspect of our design is to first view the structured data source as meaningful \nunstructured text, and then use the text to build an unsupervised neural \nnetwork model using a Natural Language Processing (NLP) technique called word \nembedding. This model captures the hidden inter-/intra-column relationships \nbetween database tokens of different types. For each database token, the model \nincludes a vector that encodes contextual semantic relationships. We seamlessly \nintegrate the word embedding model into existing SQL query infrastructure and \nuse it to enable a new class of SQL-based analytics queries called cognitive \nintelligence (CI) queries. CI queries use the model vectors to enable complex \nqueries such as semantic matching, inductive reasoning queries such as \nanalogies, predictive queries using entities not present in a database, and, \nmore generally, using knowledge from external sources. We demonstrate unique \ncapabilities of Cognitive Databases using an Apache Spark based prototype to \nexecute inductive reasoning CI queries over a multi-modal database containing \ntext and images. We believe our first-of-a-kind system exemplifies using AI \nfunctionality to endow relational databases with capabilities that were \npreviously very hard to realize in practice. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91eef", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07199"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jianfu Zhang, Naiyan Wang, Liqing Zhang", "title": "Multi-shot Pedestrian Re-identification via Sequential Decision Making. (arXiv:1712.07257v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.07257", "type": "text/html"}], "timestampUsec": "1513832809776842", "comments": [], "summary": {"content": "<p>Multi-shot pedestrian re-identification problem is at the core of \nsurveillance video analysis. It matches two tracks of pedestrians from \ndifferent cameras. In contrary to existing works that aggregate single frames \nfeatures by time series model such as recurrent neural network, in this paper, \nwe propose an interpretable reinforcement learning based approach to this \nproblem. Particularly, we train an agent to verify a pair of images at each \ntime. The agent could choose to output the result (same or different) or \nrequest another pair of images to see (unsure). By this way, our model \nimplicitly learns the difficulty of image pairs, and postpone the decision when \nthe model does not accumulate enough evidence. Moreover, by adjusting the \nreward for unsure action, we can easily trade off between speed and accuracy. \nIn three open benchmarks, our method are competitive with the state-of-the-art \nmethods while only using 3% to 6% images. These promising results demonstrate \nthat our method is favorable in both efficiency and performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91ef4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07257"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Filipe Rolim Cordeiro, Wellington Pinheiro dos Santos, Abel Guilhermino da Silva Filho", "title": "Analysis of supervised and semi-supervised GrowCut applied to segmentation of masses in mammography images. (arXiv:1712.07312v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.07312", "type": "text/html"}], "timestampUsec": "1513832809776841", "comments": [], "summary": {"content": "<p>Breast cancer is already one of the most common form of cancer worldwide. \nMammography image analysis is still the most effective diagnostic method to \npromote the early detection of breast cancer. Accurately segmenting tumors in \ndigital mammography images is important to improve diagnosis capabilities of \nhealth specialists and avoid misdiagnosis. In this work, we evaluate the \nfeasibility of applying GrowCut to segment regions of tumor and we propose two \nGrowCut semi-supervised versions. All the analysis was performed by evaluating \nthe application of segmentation techniques to a set of images obtained from the \nMini-MIAS mammography image database. GrowCut segmentation was compared to \nRegion Growing, Active Contours, Random Walks and Graph Cut techniques. \nExperiments showed that GrowCut, when compared to the other techniques, was \nable to acquire better results for the metrics analyzed. Moreover, the proposed \nsemi-supervised versions of GrowCut was proved to have a clinically \nsatisfactory quality of segmentation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07312"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michael Bukatin, Jon Anthony", "title": "Dataflow Matrix Machines and V-values: a Bridge between Programs and Neural Nets. (arXiv:1712.07447v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.07447", "type": "text/html"}], "timestampUsec": "1513832809776840", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329d7e253\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329d7e253&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Dataflow matrix machines generalize neural nets by replacing streams of \nnumbers with streams of vectors (or other kinds of linear streams admitting a \nnotion of linear combination of several streams) and adding a few more changes \non top of that, namely arbitrary input and output arities for activation \nfunctions, countable-sized networks with finite dynamically changeable active \npart capable of unbounded growth, and a very expressive self-referential \nmechanism. \n</p> \n<p>While recurrent neural networks are Turing-complete, they form an esoteric \nprogramming platform, not conductive for practical general-purpose programming. \nDataflow matrix machines are more suitable as a general-purpose programming \nplatform, although it remains to be seen whether this platform can be made \nfully competitive with more traditional programming platforms currently in use. \nAt the same time, dataflow matrix machines retain the key property of recurrent \nneural networks: programs are expressed via matrices of real numbers, and \ncontinuous changes to those matrices produce arbitrarily small variations in \nthe programs associated with those matrices. \n</p> \n<p>Spaces of vector-like elements are of particular importance in this context. \nIn particular, we focus on the vector space $V$ of finite linear combinations \nof strings, which can be also understood as the vector space of finite prefix \ntrees with numerical leaves, the vector space of \"mixed rank tensors\", or the \nvector space of recurrent maps. \n</p> \n<p>This space, and a family of spaces of vector-like elements derived from it, \nare sufficiently expressive to cover all cases of interest we are currently \naware of, and allow a compact and streamlined version of dataflow matrix \nmachines based on a single space of vector-like elements and variadic neurons. \nWe call elements of these spaces V-values. Their role in our context is \nsomewhat similar to the role of S-expressions in Lisp. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f06", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07447"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Albert Gatt, Emiel Krahmer", "title": "Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation. (arXiv:1703.09902v3 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.09902", "type": "text/html"}], "timestampUsec": "1513832809776839", "comments": [], "summary": {"content": "<p>This paper surveys the current state of the art in Natural Language \nGeneration (NLG), defined as the task of generating text or speech from \nnon-linguistic input. A survey of NLG is timely in view of the changes that the \nfield has undergone over the past decade or so, especially in relation to new \n(usually data-driven) methods, as well as new applications of NLG technology. \nThis survey therefore aims to (a) give an up-to-date synthesis of research on \nthe core tasks in NLG and the architectures adopted in which such tasks are \norganised; (b) highlight a number of relatively recent research topics that \nhave arisen partly as a result of growing synergies between NLG and other areas \nof artificial intelligence; (c) draw attention to the challenges in NLG \nevaluation, relating them to similar challenges faced in other areas of Natural \nLanguage Processing, with an emphasis on different evaluation methods and the \nrelationships between them. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f12", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.09902"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tianmin Shu, Caiming Xiong, Richard Socher", "title": "Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning. (arXiv:1712.07294v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07294", "type": "text/html"}], "timestampUsec": "1513832809776838", "comments": [], "summary": {"content": "<p>Learning policies for complex tasks that require multiple different skills is \na major challenge in reinforcement learning (RL). It is also a requirement for \nits deployment in real-world scenarios. This paper proposes a novel framework \nfor efficient multi-task reinforcement learning. Our framework trains agents to \nemploy hierarchical policies that decide when to use a previously learned \npolicy and when to learn a new skill. This enables agents to continually \nacquire new skills during different stages of training. Each learned task \ncorresponds to a human language description. Because agents can only access \npreviously learned skills through these descriptions, the agent can always \nprovide a human-interpretable description of its choices. In order to help the \nagent learn the complex temporal dependencies necessary for the hierarchical \npolicy, we provide it with a stochastic temporal grammar that modulates when to \nrely on previously learned skills and when to execute new skills. We validate \nour approach on Minecraft games designed to explicitly test the ability to \nreuse previously learned skills while simultaneously learning new skills. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07294"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tobias Fromm", "title": "Self-Supervised Damage-Avoiding Manipulation Strategy Optimization via Mental Simulation. (arXiv:1712.07452v1 [cs.RO])", "alternate": [{"href": "http://arxiv.org/abs/1712.07452", "type": "text/html"}], "timestampUsec": "1513832809776837", "comments": [], "summary": {"content": "<p>Everyday robotics are challenged to deal with autonomous product handling in \napplications like logistics or retail, possibly causing damage on the items \nduring manipulation. Traditionally, most approaches try to minimize physical \ninteraction with goods. However, we propose to take into account any unintended \nmotion of objects in the scene and to learn manipulation strategies in a \nself-supervised way which minimize the potential damage. The presented approach \nconsists of a planning method that determines the optimal sequence to \nmanipulate a number of objects in a scene with respect to possible damage by \nsimulating interaction and hence anticipating scene dynamics. The planned \nmanipulation sequences are taken as input to a machine learning process which \ngeneralizes to new, unseen scenes in the same application scenario. This \nlearned manipulation strategy is continuously refined in a self-supervised \noptimization cycle dur- ing load-free times of the system. Such a \nsimulation-in-the-loop setup is commonly known as mental simulation and allows \nfor efficient, fully automatic generation of training data as opposed to \nclassical supervised learning approaches. In parallel, the generated \nmanipulation strategies can be deployed in near-real time in an anytime \nfashion. We evaluate our approach on one industrial scenario (autonomous \ncontainer unloading) and one retail scenario (autonomous shelf replenishment). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f1f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07452"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ayush Singhal, Pradeep Sinha, Rakesh Pant", "title": "Use of Deep Learning in Modern Recommendation System: A Summary of Recent Works. (arXiv:1712.07525v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07525", "type": "text/html"}], "timestampUsec": "1513832809776836", "comments": [], "summary": {"content": "<p>With the exponential increase in the amount of digital information over the \ninternet, online shops, online music, video and image libraries, search engines \nand recommendation system have become the most convenient ways to find relevant \ninformation within a short time. In the recent times, deep learning's advances \nhave gained significant attention in the field of speech recognition, image \nprocessing and natural language processing. Meanwhile, several recent studies \nhave shown the utility of deep learning in the area of recommendation systems \nand information retrieval as well. In this short review, we cover the recent \nadvances made in the field of recommendation using various variants of deep \nlearning technology. We organize the review in three parts: Collaborative \nsystem, Content based system and Hybrid system. The review also discusses the \ncontribution of deep learning integrated recommendation systems into several \napplication domains. The review concludes by discussion of the impact of deep \nlearning in recommendation system in various domain and whether deep learning \nhas shown any significant improvement over the conventional systems for \nrecommendation. Finally, we also provide future directions of research which \nare possible based on the current state of use of deep learning in \nrecommendation systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f22", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07525"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tom&#xe1;s Teijeiro, Paulo F&#xe9;lix", "title": "On the adoption of abductive reasoning for time series interpretation. (arXiv:1609.05632v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1609.05632", "type": "text/html"}], "timestampUsec": "1513832809776835", "comments": [], "summary": {"content": "<p>Time series interpretation aims to provide an explanation of what is observed \nin terms of its underlying processes. The present work is based on the \nassumption that common classification-based approaches to time series \ninterpretation suffer from a set of inherent weaknesses whose ultimate cause \nlies in the monotonic nature of the deductive reasoning paradigm. In this \ndocument we propose a new approach to this problem based on the initial \nhypothesis that abductive reasoning properly accounts for the human ability to \nidentify and characterize patterns appearing in a time series. The result of \nthe interpretation is a set of conjectures in the form of observations, \norganized into an abstraction hierarchy, and explaining what has been observed. \nA knowledge-based framework and a set of algorithms for the interpretation task \nare provided, implementing a hypothesize-and-test cycle guided by an \nattentional mechanism. As a representative application domain, the \ninterpretation of the electrocardiogram allows us to highlight the strengths of \nthe proposed approach in comparison with traditional classification-based \napproaches. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f27", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1609.05632"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Petar Veli&#x10d;kovi&#x107;, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li&#xf2;, Yoshua Bengio", "title": "Graph Attention Networks. (arXiv:1710.10903v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.10903", "type": "text/html"}], "timestampUsec": "1513832809776834", "comments": [], "summary": {"content": "<p>We present graph attention networks (GATs), novel neural network \narchitectures that operate on graph-structured data, leveraging masked \nself-attentional layers to address the shortcomings of prior methods based on \ngraph convolutions or their approximations. By stacking layers in which nodes \nare able to attend over their neighborhoods' features, we enable (implicitly) \nspecifying different weights to different nodes in a neighborhood, without \nrequiring any kind of costly matrix operation (such as inversion) or depending \non knowing the graph structure upfront. In this way, we address several key \nchallenges of spectral-based graph neural networks simultaneously, and make our \nmodel readily applicable to inductive as well as transductive problems. Our GAT \nmodels have achieved or matched state-of-the-art results across four \nestablished transductive and inductive graph benchmarks: the Cora, Citeseer and \nPubmed citation network datasets, as well as a protein-protein interaction \ndataset (wherein test graphs remain unseen during training). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f2b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.10903"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Dmitri S. Pavlichin, Jiantao Jiao, Tsachy Weissman", "title": "Approximate Profile Maximum Likelihood. (arXiv:1712.07177v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.07177", "type": "text/html"}], "timestampUsec": "1513832809776832", "comments": [], "summary": {"content": "<p>We propose an efficient algorithm for approximate computation of the profile \nmaximum likelihood (PML), a variant of maximum likelihood maximizing the \nprobability of observing a sufficient statistic rather than the empirical \nsample. The PML has appealing theoretical properties, but is difficult to \ncompute exactly. Inspired by observations gleaned from exactly solvable cases, \nwe look for an approximate PML solution, which, intuitively, clumps comparably \nfrequent symbols into one symbol. This amounts to lower-bounding a certain \nmatrix permanent by summing over a subgroup of the symmetric group rather than \nthe whole group during the computation. We extensively experiment with the \napproximate solution, and find the empirical performance of our approach is \ncompetitive and sometimes significantly better than state-of-the-art \nperformance for various estimation problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f2e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07177"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yehezkel S. Resheff, Moni Shahar", "title": "Fusing Multifaceted Transaction Data for User Modeling and Demographic Prediction. (arXiv:1712.07230v1 [cs.SI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07230", "type": "text/html"}], "timestampUsec": "1513832809776831", "comments": [], "summary": {"content": "<p>Inferring user characteristics such as demographic attributes is of the \nutmost importance in many user-centric applications. Demographic data is an \nenabler of personalization, identity security, and other applications. Despite \nthat, this data is sensitive and often hard to obtain. Previous work has shown \nthat purchase history can be used for multi-task prediction of many demographic \nfields such as gender and marital status. Here we present an embedding based \nmethod to integrate multifaceted sequences of transaction data, together with \nauxiliary relational tables, for better user modeling and demographic \nprediction. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f33", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07230"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pushparaja Murugan", "title": "Hyperparameters Optimization in Deep Convolutional Neural Network / Bayesian Approach with Gaussian Process Prior. (arXiv:1712.07233v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.07233", "type": "text/html"}], "timestampUsec": "1513832809776830", "comments": [], "summary": {"content": "<p>Convolutional Neural Network is known as ConvNet have been extensively used \nin many complex machine learning tasks. However, hyperparameters optimization \nis one of a crucial step in developing ConvNet architectures, since the \naccuracy and performance are reliant on the hyperparameters. This multilayered \narchitecture parameterized by a set of hyperparameters such as the number of \nconvolutional layers, number of fully connected dense layers &amp; neurons, the \nprobability of dropout implementation, learning rate. Hence the searching the \nhyperparameter over the hyperparameter space are highly difficult to build such \ncomplex hierarchical architecture. Many methods have been proposed over the \ndecade to explore the hyperparameter space and find the optimum set of \nhyperparameter values. Reportedly, Gird search and Random search are said to be \ninefficient and extremely expensive, due to a large number of hyperparameters \nof the architecture. Hence, Sequential model-based Bayesian Optimization is a \npromising alternative technique to address the extreme of the unknown cost \nfunction. The recent study on Bayesian Optimization by Snoek in nine \nconvolutional network parameters is achieved the lowerest error report in the \nCIFAR-10 benchmark. This article is intended to provide the overview of the \nmathematical concept behind the Bayesian Optimization over a Gaussian prior. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07233"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kevin H. Lee, Lingzhou Xue, David R. Hunter", "title": "Model-Based Clustering of Time-Evolving Networks through Temporal Exponential-Family Random Graph Models. (arXiv:1712.07325v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.07325", "type": "text/html"}], "timestampUsec": "1513832809776829", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329d7e41d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329d7e41d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Dynamic networks are a general language for describing time-evolving complex \nsystems, and discrete time network models provide an emerging statistical \ntechnique for various applications. It is a fundamental research question to \ndetect the community structure in time-evolving networks. However, due to \nsignificant computational challenges and difficulties in modeling communities \nof time-evolving networks, there is little progress in the current literature \nto effectively find communities in time-evolving networks. In this work, we \npropose a novel model-based clustering framework for time-evolving networks \nbased on discrete time exponential-family random graph models. To choose the \nnumber of communities, we use conditional likelihood to construct an effective \nmodel selection criterion. Furthermore, we propose an efficient variational \nexpectation-maximization (EM) algorithm to find approximate maximum likelihood \nestimates of network parameters and mixing proportions. By using variational \nmethods and minorization-maximization (MM) techniques, our method has appealing \nscalability for large-scale time-evolving networks. The power of our method is \ndemonstrated in simulation studies and empirical applications to international \ntrade networks and the collaboration networks of a large American research \nuniversity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f3c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07325"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sven Klaassen, Jannis Kueck, Martin Spindler", "title": "Transformation Models in High-Dimensions. (arXiv:1712.07364v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.07364", "type": "text/html"}], "timestampUsec": "1513832809776828", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329ddec68\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329ddec68&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Transformation models are a very important tool for applied statisticians and \neconometricians. In many applications, the dependent variable is transformed so \nthat homogeneity or normal distribution of the error holds. In this paper, we \nanalyze transformation models in a high-dimensional setting, where the set of \npotential covariates is large. We propose an estimator for the transformation \nparameter and we show that it is asymptotically normally distributed using an \northogonalized moment condition where the nuisance functions depend on the \ntarget parameter. In a simulation study, we show that the proposed estimator \nworks well in small samples. A common practice in labor economics is to \ntransform wage with the log-function. In this study, we test if this \ntransformation holds in CPS data from the United States. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f41", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07364"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1513924823, "author": "Hong Wang, Ashkan Rezaei, Brian D. Ziebart", "title": "Adversarial Structured Prediction for Multivariate Measures. (arXiv:1712.07374v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.07374", "type": "text/html"}], "timestampUsec": "1513832809776827", "comments": [], "summary": {"content": "<p>Many predicted structured objects (e.g., sequences, matchings, trees) are \nevaluated using the F-score, alignment error rate (AER), or other multivariate \nperformance measures. Since inductively optimizing these measures using \ntraining data is typically computationally difficult, empirical risk \nminimization of surrogate losses is employed, using, e.g., the hinge loss for \n(structured) support vector machines. These approximations often introduce a \nmismatch between the learner's objective and the desired application \nperformance, leading to inconsistency. We take a different approach: \nadversarially approximate training data while optimizing the exact F-score or \nAER. Structured predictions under this formulation result from solving zero-sum \ngames between a predictor seeking the best performance and an adversary seeking \nthe worst while required to (approximately) match certain structured properties \nof the training data. We explore this approach for word alignment (AER \nevaluation) and named entity recognition (F-score evaluation) with linear-chain \nconstraints. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f44", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07374"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Robert P.W. Duin, Sergey Verzakov", "title": "Fast kNN mode seeking clustering applied to active learning. (arXiv:1712.07454v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07454", "type": "text/html"}], "timestampUsec": "1513832809776826", "comments": [], "summary": {"content": "<p>A significantly faster algorithm is presented for the original kNN mode \nseeking procedure. It has the advantages over the well-known mean shift \nalgorithm that it is feasible in high-dimensional vector spaces and results in \nuniquely, well defined modes. Moreover, without any additional computational \neffort it may yield a multi-scale hierarchy of clusterings. The time complexity \nis just O(n^1.5). resulting computing times range from seconds for 10^4 objects \nto minutes for 10^5 objects and to less than an hour for 10^6 objects. The \nspace complexity is just O(n). The procedure is well suited for finding large \nsets of small clusters and is thereby a candidate to analyze thousands of \nclusters in millions of objects. \n</p> \n<p>The kNN mode seeking procedure can be used for active learning by assigning \nthe clusters to the class of the modal objects of the clusters. Its feasibility \nis shown by some examples with up to 1.5 million handwritten digits. The \nobtained classification results based on the clusterings are compared with \nthose obtained by the nearest neighbor rule and the support vector classifier \nbased on the same labeled objects for training. It can be concluded that using \nthe clustering structure for classification can be significantly better than \nusing the trained classifiers. A drawback of using the clustering for \nclassification, however, is that no classifier is obtained that may be used for \nout-of-sample objects. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07454"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Wenjie Zheng, Aur&#xe9;lien Bellet, Patrick Gallinari", "title": "A Distributed Frank-Wolfe Framework for Learning Low-Rank Matrices with the Trace Norm. (arXiv:1712.07495v1 [cs.DC])", "alternate": [{"href": "http://arxiv.org/abs/1712.07495", "type": "text/html"}], "timestampUsec": "1513832809776825", "comments": [], "summary": {"content": "<p>We consider the problem of learning a high-dimensional but low-rank matrix \nfrom a large-scale dataset distributed over several machines, where \nlow-rankness is enforced by a convex trace norm constraint. We propose \nDFW-Trace, a distributed Frank-Wolfe algorithm which leverages the low-rank \nstructure of its updates to achieve efficiency in time, memory and \ncommunication usage. The step at the heart of DFW-Trace is solved approximately \nusing a distributed version of the power method. We provide a theoretical \nanalysis of the convergence of DFW-Trace, showing that we can ensure sublinear \nconvergence in expectation to an optimal solution with few power iterations per \nepoch. We implement DFW-Trace in the Apache Spark distributed programming \nframework and validate the usefulness of our approach on synthetic and real \ndata, including the ImageNet dataset with high-dimensional features extracted \nfrom a deep neural network. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f53", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07495"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tengyuan Liang, Weijie Su", "title": "Statistical Inference for the Population Landscape via Moment Adjusted Stochastic Gradients. (arXiv:1712.07519v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07519", "type": "text/html"}], "timestampUsec": "1513832809776824", "comments": [], "summary": {"content": "<p>Modern statistical inference tasks often require iterative optimization \nmethods to approximate the solution. Convergence analysis from optimization \nonly tells us how well we are approximating the solution deterministically, but \noverlooks the sampling nature of the data. However, due to the randomness in \nthe data, statisticians are keen to provide uncertainty quantification, or \nconfidence, for the answer obtained after certain steps of optimization. \nTherefore, it is important yet challenging to understand the sampling \ndistribution of the iterative optimization methods. \n</p> \n<p>This paper makes some progress along this direction by introducing a new \nstochastic optimization method for statistical inference, the moment adjusted \nstochastic gradient descent. We establish non-asymptotic theory that \ncharacterizes the statistical distribution of the iterative methods, with good \noptimization guarantee. On the statistical front, the theory allows for model \nmisspecification, with very mild conditions on the data. For optimization, the \ntheory is flexible for both the convex and non-convex cases. Remarkably, the \nmoment adjusting idea motivated from \"error standardization\" in statistics \nachieves similar effect as Nesterov's acceleration in optimization, for certain \nconvex problems as in fitting generalized linear models. We also demonstrate \nthis acceleration effect in the non-convex setting through experiments. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f59", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07519"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Daniel Krefl, Stefano Carrazza, Babak Haghighat, Jens Kahlen", "title": "Riemann-Theta Boltzmann Machine. (arXiv:1712.07581v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07581", "type": "text/html"}], "timestampUsec": "1513832809776823", "comments": [], "summary": {"content": "<p>A general Boltzmann machine with continuous visible and discrete integer \nvalued hidden states is introduced. Under mild assumptions about the connection \nmatrices, the probability density function of the visible units can be solved \nfor analytically, yielding a novel parametric density function involving a \nratio of Riemann-Theta functions. The conditional expectation of a hidden state \nfor given visible states can also be calculated analytically, yielding a \nderivative of the logarithmic Riemann-Theta function. The conditional \nexpectation can be used as activation function in a feedforward neural network, \nthereby increasing the modelling capacity of the network. Both the Boltzmann \nmachine and the derived feedforward neural network can be successfully trained \nvia standard gradient- and non-gradient-based optimization techniques. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f5c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07581"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "R. Lily Hu, Jeremy Karnowski, Ross Fadely, Jean-Patrick Pommier", "title": "Image Segmentation to Distinguish Between Overlapping Human Chromosomes. (arXiv:1712.07639v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.07639", "type": "text/html"}], "timestampUsec": "1513832809776822", "comments": [], "summary": {"content": "<p>In medicine, visualizing chromosomes is important for medical diagnostics, \ndrug development, and biomedical research. Unfortunately, chromosomes often \noverlap and it is necessary to identify and distinguish between the overlapping \nchromosomes. A segmentation solution that is fast and automated will enable \nscaling of cost effective medicine and biomedical research. We apply neural \nnetwork-based image segmentation to the problem of distinguishing between \npartially overlapping DNA chromosomes. A convolutional neural network is \ncustomized for this problem. The results achieved intersection over union (IOU) \nscores of 94.7% for the overlapping region and 88-94% on the non-overlapping \nchromosome regions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f63", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07639"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yining Wang, Adams Wei Yu, Aarti Singh", "title": "On Computationally Tractable Selection of Experiments in Measurement-Constrained Regression Models. (arXiv:1601.02068v6 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1601.02068", "type": "text/html"}], "timestampUsec": "1513832809776821", "comments": [], "summary": {"content": "<p>We derive computationally tractable methods to select a small subset of \nexperiment settings from a large pool of given design points. The primary focus \nis on linear regression models, while the technique extends to generalized \nlinear models and Delta's method (estimating functions of linear regression \nmodels) as well. The algorithms are based on a continuous relaxation of an \notherwise intractable combinatorial optimization problem, with sampling or \ngreedy procedures as post-processing steps. Formal approximation guarantees are \nestablished for both algorithms, and numerical results on both synthetic and \nreal-world data confirm the effectiveness of the proposed methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f6a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1601.02068"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "William B Levy, Toby Berger, Mustafa Sungkar", "title": "Neural computation from first principles: Using the maximum entropy method to obtain an optimal bits-per-joule neuron. (arXiv:1606.03063v2 [q-bio.NC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1606.03063", "type": "text/html"}], "timestampUsec": "1513832809776820", "comments": [], "summary": {"content": "<p>Optimization results are one method for understanding neural computation from \nNature's perspective and for defining the physical limits on neuron-like \nengineering. Earlier work looks at individual properties or performance \ncriteria and occasionally a combination of two, such as energy and information. \nHere we make use of Jaynes' maximum entropy method and combine a larger set of \nconstraints, possibly dimensionally distinct, each expressible as an \nexpectation. The method identifies a likelihood-function and a sufficient \nstatistic arising from each such optimization. This likelihood is a \nfirst-hitting time distribution in the exponential class. Particular constraint \nsets are identified that, from an optimal inference perspective, justify \nearlier neurocomputational models. Interactions between constraints, mediated \nthrough the inferred likelihood, restrict constraint-set parameterizations, \ne.g., the energy-budget limits estimation performance which, in turn, matches \nan axonal communication constraint. Such linkages are, for biologists, \nexperimental predictions of the method. In addition to the related likelihood, \nat least one type of constraint set implies marginal distributions, and in this \ncase, a Shannon bits/joule statement arises. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f73", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1606.03063"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xiansheng Guo, Nirwan Ansari", "title": "Localization by Fusing a Group of Fingerprints via Multiple Antennas in Indoor Environment. (arXiv:1609.00661v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1609.00661", "type": "text/html"}], "timestampUsec": "1513832809776819", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329ddee74\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329ddee74&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Most existing fingerprints-based indoor localization approaches are based on \nsome single fingerprints, such as received signal strength (RSS), channel \nimpulse response (CIR), and signal subspace. However, the localization accuracy \nobtained by the single fingerprint approach is rather susceptible to the \nchanging environment, multi-path, and non-line-of-sight (NLOS) propagation. \nFurthermore, building the fingerprints is a very time consuming process. In \nthis paper, we propose a novel localization framework by Fusing A Group Of \nfingerprinTs (FAGOT) via multiple antennas for the indoor environment. We first \nbuild a GrOup Of Fingerprints (GOOF), which includes five different \nfingerprints, namely, RSS, covariance matrix, signal subspace, fractional low \norder moment, and fourth-order cumulant, which are obtained by different \ntransformations of the received signals from multiple antennas in the offline \nstage. Then, we design a parallel GOOF multiple classifiers based on AdaBoost \n(GOOF-AdaBoost) to train each of these fingerprints in parallel as five strong \nmultiple classifiers. In the online stage, we input the corresponding \ntransformations of the real measurements into these strong classifiers to \nobtain independent decisions. Finally, we propose an efficient combination \nfusion algorithm, namely, MUltiple Classifiers mUltiple Samples (MUCUS) fusion \nalgorithm to improve the accuracy of localization by combining the predictions \nof multiple classifiers with different samples. As compared with the single \nfingerprint approaches, the prediction probability of our proposed approach is \nimproved significantly. The process for building fingerprints can also be \nreduced drastically. We demonstrate the feasibility and performance of the \nproposed algorithm through extensive simulations as well as via real \nexperimental data using a Universal Software Radio Peripheral (USRP) platform \nwith four antennas. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f78", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1609.00661"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Luca Martino, Victor Elvira, Gustau Camps-Valls", "title": "The Recycling Gibbs Sampler for Efficient Learning. (arXiv:1611.07056v2 [stat.CO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.07056", "type": "text/html"}], "timestampUsec": "1513832809776818", "comments": [], "summary": {"content": "<p>Monte Carlo methods are essential tools for Bayesian inference. Gibbs \nsampling is a well-known Markov chain Monte Carlo (MCMC) algorithm, extensively \nused in signal processing, machine learning, and statistics, employed to draw \nsamples from complicated high-dimensional posterior distributions. The key \npoint for the successful application of the Gibbs sampler is the ability to \ndraw efficiently samples from the full-conditional probability density \nfunctions. Since in the general case this is not possible, in order to speed up \nthe convergence of the chain, it is required to generate auxiliary samples \nwhose information is eventually disregarded. In this work, we show that these \nauxiliary samples can be recycled within the Gibbs estimators, improving their \nefficiency with no extra cost. This novel scheme arises naturally after \npointing out the relationship between the standard Gibbs sampler and the chain \nrule used for sampling purposes. Numerical simulations involving simple and \nreal inference problems confirm the excellent performance of the proposed \nscheme in terms of accuracy and computational efficiency. In particular we give \nempirical evidence of performance in a toy example, inference of Gaussian \nprocesses hyperparameters, and learning dependence graphs through regression. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f7f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.07056"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xiansheng Guo, Sihua Shao, Nirwan Ansari, Abdallah Khreishah", "title": "Indoor Localization Using Visible Light Via Fusion Of Multiple Classifiers. (arXiv:1703.02184v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.02184", "type": "text/html"}], "timestampUsec": "1513832809776817", "comments": [], "summary": {"content": "<p>A multiple classifiers fusion localization technique using received signal \nstrengths (RSSs) of visible light is proposed, in which the proposed system \ntransmits different intensity modulated sinusoidal signals by LEDs and the \nsignals received by a Photo Diode (PD) placed at various grid points. First, we \nobtain some {\\emph{approximate}} received signal strengths (RSSs) fingerprints \nby capturing the peaks of power spectral density (PSD) of the received signals \nat each given grid point. Unlike the existing RSSs based algorithms, several \nrepresentative machine learning approaches are adopted to train multiple \nclassifiers based on these RSSs fingerprints. The multiple classifiers \nlocalization estimators outperform the classical RSS-based LED localization \napproaches in accuracy and robustness. To further improve the localization \nperformance, two robust fusion localization algorithms, namely, grid \nindependent least square (GI-LS) and grid dependent least square (GD-LS), are \nproposed to combine the outputs of these classifiers. We also use a singular \nvalue decomposition (SVD) based LS (LS-SVD) method to mitigate the numerical \nstability problem when the prediction matrix is singular. Experiments conducted \non intensity modulated direct detection (IM/DD) systems have demonstrated the \neffectiveness of the proposed algorithms. The experimental results show that \nthe probability of having mean square positioning error (MSPE) of less than 5cm \nachieved by GD-LS is improved by 93.03\\% and 93.15\\%, respectively, as compared \nto those by the RSS ratio (RSSR) and RSS matching methods with the FFT length \nof 2000. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f84", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.02184"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Dave Zachariah, Petre Stoica", "title": "Model-Robust Counterfactual Prediction Method. (arXiv:1705.07019v3 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.07019", "type": "text/html"}], "timestampUsec": "1513832809776816", "comments": [], "summary": {"content": "<p>We develop a method for assessing counterfactual predictions with multiple \ngroups. It is tuning-free and operational in high-dimensional covariate \nscenarios, with a runtime that scales linearly in the number of datapoints. The \ncomputational efficiency is leveraged to produce valid confidence intervals \nusing the conformal prediction approach. The method is model-robust in that it \nenables inferences from observational data even when the data model is \nmisspecified. The approach is illustrated using both real and synthetic \ndatasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f8c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.07019"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yanbo Fan, Siwei Lyu, Yiming Ying, Bao-Gang Hu", "title": "Learning with Average Top-k Loss. (arXiv:1705.08826v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08826", "type": "text/html"}], "timestampUsec": "1513832809776815", "comments": [], "summary": {"content": "<p>In this work, we introduce the {\\em average top-$k$} (\\atk) loss as a new \naggregate loss for supervised learning, which is the average over the $k$ \nlargest individual losses over a training dataset. We show that the \\atk loss \nis a natural generalization of the two widely used aggregate losses, namely the \naverage loss and the maximum loss, but can combine their advantages and \nmitigate their drawbacks to better adapt to different data distributions. \nFurthermore, it remains a convex function over all individual losses, which can \nlead to convex optimization problems that can be solved effectively with \nconventional gradient-based methods. We provide an intuitive interpretation of \nthe \\atk loss based on its equivalent effect on the continuous individual loss \nfunctions, suggesting that it can reduce the penalty on correctly classified \ndata. We further give a learning theory analysis of \\matk learning on the \nclassification calibration of the \\atk loss and the error bounds of \\atk-SVM. \nWe demonstrate the applicability of minimum average top-$k$ learning for binary \nclassification and regression using synthetic and real datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f91", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08826"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Boris Hanin", "title": "Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations. (arXiv:1708.02691v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.02691", "type": "text/html"}], "timestampUsec": "1513832809776814", "comments": [], "summary": {"content": "<p>This article concerns the expressive power of depth in neural nets with ReLU \nactivations and bounded width. We are particularly interested in the following \nquestions: what is the minimal width $w_{\\text{min}}(d)$ so that ReLU nets of \nwidth $w_{\\text{min}}(d)$ (and arbitrary depth) can approximate any continuous \nfunction on the unit cube $[0,1]^d$ aribitrarily well? For ReLU nets near this \nminimal width, what can one say about the depth necessary to approximate a \ngiven function? Our approach to this paper is based on the observation that, \ndue to the convexity of the ReLU activation, ReLU nets are particularly \nwell-suited for representing convex functions. In particular, we prove that \nReLU nets with width $d+1$ can approximate any continuous convex function of \n$d$ variables arbitrarily well. These results then give quantitative depth \nestimates for the rate of approximation of any continuous scalar function on \nthe $d$-dimensional cube $[0,1]^d$ by ReLU nets with width $d+3.$ \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f95", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.02691"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Francois Petitjean, Wray Buntine, Geoffrey I. Webb, Nayyar Zaidi", "title": "Accurate parameter estimation for Bayesian Network Classifiers using Hierarchical Dirichlet Processes. (arXiv:1708.07581v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.07581", "type": "text/html"}], "timestampUsec": "1513832809776813", "comments": [], "summary": {"content": "<p>This paper introduces a novel parameter estimation method for the probability \ntables of Bayesian network classifiers (BNCs), using hierarchical Dirichlet \nprocesses (HDPs). The main result of this paper is to show that improved \nparameter estimation allows BNCs to outperform leading learning methods such as \nRandom Forest for both 0-1 loss and RMSE, albeit just on categorical datasets. \n</p> \n<p>As data assets become larger, entering the hyped world of \"big\", efficient \naccurate classification requires three main elements: (1) classifiers with \nlow-bias that can capture the fine-detail of large datasets (2) out-of-core \nlearners that can learn from data without having to hold it all in main memory \nand (3) models that can classify new data very efficiently. \n</p> \n<p>The latest Bayesian network classifiers (BNCs) satisfy these requirements. \nTheir bias can be controlled easily by increasing the number of parents of the \nnodes in the graph. Their structure can be learned out of core with a limited \nnumber of passes over the data. However, as the bias is made lower to \naccurately model classification tasks, so is the accuracy of their parameters' \nestimates, as each parameter is estimated from ever decreasing quantities of \ndata. In this paper, we introduce the use of Hierarchical Dirichlet Processes \nfor accurate BNC parameter estimation. \n</p> \n<p>We conduct an extensive set of experiments on 68 standard datasets and \ndemonstrate that our resulting classifiers perform very competitively with \nRandom Forest in terms of prediction, while keeping the out-of-core capability \nand superior classification time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f9c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.07581"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Andreas Mardt, Luca Pasquali, Hao Wu, Frank No&#xe9;", "title": "VAMPnets: Deep learning of molecular kinetics. (arXiv:1710.06012v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.06012", "type": "text/html"}], "timestampUsec": "1513832809776812", "comments": [], "summary": {"content": "<p>There is an increasing demand for computing the relevant structures, \nequilibria and long-timescale kinetics of biomolecular processes, such as \nprotein-drug binding, from high-throughput molecular dynamics simulations. \nCurrent methods employ transformation of simulated coordinates into structural \nfeatures, dimension reduction, clustering the dimension-reduced data, and \nestimation of a Markov state model or related model of the interconversion \nrates between molecular structures. This handcrafted approach demands a \nsubstantial amount of modeling expertise, as poor decisions at any step will \nlead to large modeling errors. Here we employ the variational approach for \nMarkov processes (VAMP) to develop a deep learning framework for molecular \nkinetics using neural networks, dubbed VAMPnets. A VAMPnet encodes the entire \nmapping from molecular coordinates to Markov states, thus combining the whole \ndata processing pipeline in a single end-to-end framework. Our method performs \nequally or better than state-of-the art Markov modeling methods and provides \neasily interpretable few-state kinetic models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91fa2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.06012"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Erin Craig, Carlos Arias, David Gillman", "title": "Predicting readmission risk from doctors' notes. (arXiv:1711.10663v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10663", "type": "text/html"}], "timestampUsec": "1513832809776811", "comments": [], "summary": {"content": "<p>We develop a model using deep learning techniques and natural language \nprocessing on unstructured text from medical records to predict hospital-wide \n$30$-day unplanned readmission, with c-statistic $.70$. Our model is \nconstructed to allow physicians to interpret the significant features for \nprediction. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91faa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10663"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ruimin Sun, Xiaoyong Yuan, Pan He, Qile Zhu, Aokun Chen, Andre Gregio, Daniela Oliveira, Xiaolin Li", "title": "Learning Fast and Slow: PROPEDEUTICA for Real-time Malware Detection. (arXiv:1712.01145v1 [cs.CR] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1712.01145", "type": "text/html"}], "timestampUsec": "1513832809776808", "comments": [], "summary": {"content": "<p>In this paper, we introduce and evaluate PROPEDEUTICA, a novel methodology \nand framework for efficient and effective real-time malware detection, \nleveraging the best of conventional machine learning (ML) and deep learning \n(DL) algorithms. In PROPEDEUTICA, all software processes in the system start \nexecution subjected to a conventional ML detector for fast classification. If a \npiece of software receives a borderline classification, it is subjected to \nfurther analysis via more performance expensive and more accurate DL methods, \nvia our newly proposed DL algorithm DEEPMALWARE. Further, we introduce delays \nto the execution of software subjected to deep learning analysis as a way to \n\"buy time\" for DL analysis and to rate-limit the impact of possible malware in \nthe system. We evaluated PROPEDEUTICA with a set of 9,115 malware samples and \n877 commonly used benign software samples from various categories for the \nWindows OS. Our results show that the false positive rate for conventional ML \nmethods can reach 20%, and for modern DL methods it is usually below 6%. \nHowever, the classification time for DL can be 100X longer than conventional ML \nmethods. PROPEDEUTICA improved the detection F1-score from 77.54% (conventional \nML method) to 90.25%, and reduced the detection time by 54.86%. Further, the \npercentage of software subjected to DL analysis was approximately 40% on \naverage. Further, the application of delays in software subjected to ML reduced \nthe detection time by approximately 10%. Finally, we found and discussed a \ndiscrepancy between the detection accuracy offline (analysis after all traces \nare collected) and on-the-fly (analysis in tandem with trace collection). Our \ninsights show that conventional ML and modern DL-based malware detectors in \nisolation cannot meet the needs of efficient and effective malware detection: \nhigh accuracy, low false positive rate, and short classification time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91fb4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01145"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mansour Sheikhan, Ehsan Hemmati", "title": "PSO-Optimized Hopfield Neural Network-Based Multipath Routing for Mobile Ad-hoc Networks. (arXiv:1712.07019v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.07019", "type": "text/html"}], "timestampUsec": "1513747858834900", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329ddf044\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329ddf044&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Mobile ad-hoc network (MANET) is a dynamic collection of mobile computers \nwithout the need for any existing infrastructure. Nodes in a MANET act as hosts \nand routers. Designing of robust routing algorithms for MANETs is a challenging \ntask. Disjoint multipath routing protocols address this problem and increase \nthe reliability, security and lifetime of network. However, selecting an \noptimal multipath is an NP-complete problem. In this paper, Hopfield neural \nnetwork (HNN) which its parameters are optimized by particle swarm optimization \n(PSO) algorithm is proposed as multipath routing algorithm. Link expiration \ntime (LET) between each two nodes is used as the link reliability estimation \nmetric. This approach can find either node-disjoint or link-disjoint paths in \nsingle phase route discovery. Simulation results confirm that PSO-HNN routing \nalgorithm has better performance as compared to backup path set selection \nalgorithm (BPSA) in terms of the path set reliability and number of paths in \nthe set. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed904c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07019"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "A.V. Eremeev, Yu.V. Kovalenko", "title": "Genetic Algorithm with Optimal Recombination for the Asymmetric Travelling Salesman Problem. (arXiv:1706.06920v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.06920", "type": "text/html"}], "timestampUsec": "1513747858834899", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329e408c2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329e408c2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a new genetic algorithm with optimal recombination for the \nasymmetric instances of travelling salesman problem. The algorithm incorporates \nseveral new features that contribute to its effectiveness: (i) Optimal \nrecombination problem is solved within crossover operator. (ii) A new mutation \noperator performs a random jump within 3-opt or 4-opt neighborhood. (iii) \nGreedy constructive heuristic of W.Zhang and 3-opt local search heuristic are \nused to generate the initial population. A computational experiment on TSPLIB \ninstances shows that the proposed algorithm yields competitive results to other \nwell-known memetic algorithms for asymmetric travelling salesman problem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9050", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.06920"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zehao Huang, Naiyan Wang", "title": "Data-Driven Sparse Structure Selection for Deep Neural Networks. (arXiv:1707.01213v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.01213", "type": "text/html"}], "timestampUsec": "1513747858834898", "comments": [], "summary": {"content": "<p>Deep convolutional neural networks have liberated its extraordinary power on \nvarious tasks. However, it is still very challenging to deploy state-of-the-art \nmodels into real-world applications due to their high computational complexity. \nHow can we design a compact and effective network without massive experiments \nand expert knowledge? In this paper, we propose a simple and effective \nframework to learn and prune deep models in an end-to-end manner. In our \nframework, a new type of parameter -- scaling factor is first introduced to \nscale the outputs of specific structures, such as neurons, groups or residual \nblocks. Then we add sparsity regularizations on these factors, and solve this \noptimization problem by a modified stochastic Accelerated Proximal Gradient \n(APG) method. By forcing some of the factors to zero, we can safely remove the \ncorresponding structures, thus prune the unimportant parts of a CNN. Comparing \nwith other structure selection methods that may need thousands of trials or \niterative fine-tuning, our method is trained fully end-to-end in one training \npass without bells and whistles. We evaluate our method, Sparse Structure \nSelection with several state-of-the-art CNNs, and demonstrate very promising \nresults with adaptive depth and width selection. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9056", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.01213"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zehao Huang, Naiyan Wang", "title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer. (arXiv:1707.01219v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.01219", "type": "text/html"}], "timestampUsec": "1513747858834897", "comments": [], "summary": {"content": "<p>Despite deep neural networks have demonstrated extraordinary power in various \napplications, their superior performances are at expense of high storage and \ncomputational costs. Consequently, the acceleration and compression of neural \nnetworks have attracted much attention recently. Knowledge Transfer (KT), which \naims at training a smaller student network by transferring knowledge from a \nlarger teacher model, is one of the popular solutions. In this paper, we \npropose a novel knowledge transfer method by treating it as a distribution \nmatching problem. Particularly, we match the distributions of neuron \nselectivity patterns between teacher and student networks. To achieve this \ngoal, we devise a new KT loss function by minimizing the Maximum Mean \nDiscrepancy (MMD) metric between these distributions. Combined with the \noriginal loss function, our method can significantly improve the performance of \nstudent networks. We validate the effectiveness of our method across several \ndatasets, and further combine it with other KT methods to explore the best \npossible results. Last but not least, we fine-tune the model to other tasks \nsuch as object detection. The results are also encouraging, which confirm the \ntransferability of the learned features. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9058", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.01219"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang", "title": "DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer. (arXiv:1707.01220v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.01220", "type": "text/html"}], "timestampUsec": "1513747858834896", "comments": [], "summary": {"content": "<p>We have witnessed rapid evolution of deep neural network architecture design \nin the past years. These latest progresses greatly facilitate the developments \nin various areas such as computer vision and natural language processing. \nHowever, along with the extraordinary performance, these state-of-the-art \nmodels also bring in expensive computational cost. Directly deploying these \nmodels into applications with real-time requirement is still infeasible. \nRecently, Hinton etal. have shown that the dark knowledge within a powerful \nteacher model can significantly help the training of a smaller and faster \nstudent network. These knowledge are vastly beneficial to improve the \ngeneralization ability of the student model. Inspired by their work, we \nintroduce a new type of knowledge -- cross sample similarities for model \ncompression and acceleration. This knowledge can be naturally derived from deep \nmetric learning model. To transfer them, we bring the \"learning to rank\" \ntechnique into deep metric learning formulation. We test our proposed DarkRank \nmethod on various metric learning tasks including pedestrian re-identification, \nimage retrieval and image clustering. The results are quite encouraging. Our \nmethod can improve over the baseline method by a large margin. Moreover, it is \nfully compatible with other existing methods. When combined, the performance \ncan be further boosted. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed905f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.01220"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Andreas Holzinger, Bernd Malle, Peter Kieseberg, Peter M. Roth, Heimo M&#xfc;ller, Robert Reihs, Kurt Zatloukal", "title": "Towards the Augmented Pathologist: Challenges of Explainable-AI in Digital Pathology. (arXiv:1712.06657v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.06657", "type": "text/html"}], "timestampUsec": "1513747858834894", "comments": [], "summary": {"content": "<p>Digital pathology is not only one of the most promising fields of diagnostic \nmedicine, but at the same time a hot topic for fundamental research. Digital \npathology is not just the transfer of histopathological slides into digital \nrepresentations. The combination of different data sources (images, patient \nrecords, and *omics data) together with current advances in artificial \nintelligence/machine learning enable to make novel information accessible and \nquantifiable to a human expert, which is not yet available and not exploited in \ncurrent medical settings. The grand goal is to reach a level of usable \nintelligence to understand the data in the context of an application task, \nthereby making machine decisions transparent, interpretable and explainable. \nThe foundation of such an \"augmented pathologist\" needs an integrated approach: \nWhile machine learning algorithms require many thousands of training examples, \na human expert is often confronted with only a few data points. Interestingly, \nhumans can learn from such few examples and are able to instantly interpret \ncomplex patterns. Consequently, the grand goal is to combine the possibilities \nof artificial intelligence with human intelligence and to find a well-suited \nbalance between them to enable what neither of them could do on their own. This \ncan raise the quality of education, diagnosis, prognosis and prediction of \ncancer and other diseases. In this paper we describe some (incomplete) research \nissues which we believe should be addressed in an integrated and concerted \neffort for paving the way towards the augmented pathologist. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9063", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06657"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Saptarshi Pal, Soumya K Ghosh", "title": "Learning Representations from Road Network for End-to-End Urban Growth Simulation. (arXiv:1712.06778v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.06778", "type": "text/html"}], "timestampUsec": "1513747858834893", "comments": [], "summary": {"content": "<p>From our experiences in the past, we have seen that the growth of cities is \nvery much dependent on the transportation networks. In mega cities, \ntransportation networks determine to a significant extent as to where the \npeople will move and houses will be built. Hence, transportation network data \nis crucial to an urban growth prediction system. Existing works have used \nmanually derived distance based features based on the road networks to build \nmodels on urban growth. But due to the non-generic and laborious nature of the \nmanual feature engineering process, we can shift to End-to-End systems which do \nnot rely on manual feature engineering. In this paper, we propose a method to \nintegrate road network data to an existing Rule based End-to-End framework \nwithout manual feature engineering. Our method employs recurrent neural \nnetworks to represent road networks in a structured way such that it can be \nplugged into the previously proposed End-to-End framework. The proposed \napproach enhances the performance in terms of Figure of Merit, Producer's \naccuracy, User's accuracy and Overall accuracy of the existing Rule based \nEnd-to-End framework. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9066", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06778"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Christoph Wernhard", "title": "Heinrich Behmann's Contributions to Second-Order Quantifier Elimination from the View of Computational Logic. (arXiv:1712.06868v1 [cs.LO])", "alternate": [{"href": "http://arxiv.org/abs/1712.06868", "type": "text/html"}], "timestampUsec": "1513747858834892", "comments": [], "summary": {"content": "<p>For relational monadic formulas (the L\\\"owenheim class) second-order \nquantifier elimination, which is closely related to computation of uniform \ninterpolants, projection and forgetting - operations that currently receive \nmuch attention in knowledge processing - always succeeds. The decidability \nproof for this class by Heinrich Behmann from 1922 explicitly proceeds by \nelimination with equivalence preserving formula rewriting. Here we reconstruct \nthe results from Behmann's publication in detail and discuss related issues \nthat are relevant in the context of modern approaches to second-order \nquantifier elimination in computational logic. In addition, an extensive \ndocumentation of the letters and manuscripts in Behmann's bequest that concern \nsecond-order quantifier elimination is given, including a commented register \nand English abstracts of the German sources with focus on technical material. \nIn the late 1920s Behmann attempted to develop an elimination-based decision \nmethod for formulas with predicates whose arity is larger than one. His \nmanuscripts and the correspondence with Wilhelm Ackermann show technical \naspects that are still of interest today and give insight into the genesis of \nAckermann's landmark paper \"Untersuchungen \\\"uber das Eliminationsproblem der \nmathematischen Logik\" from 1935, which laid the foundation of the two \nprevailing modern approaches to second-order quantifier elimination. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed906a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06868"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516598533, "author": "Romain Laroche, Paul Trichelair", "title": "Safe Policy Improvement with Baseline Bootstrapping. (arXiv:1712.06924v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.06924", "type": "text/html"}], "timestampUsec": "1513747858834891", "comments": [], "summary": {"content": "<p>A common goal in Reinforcement Learning is to derive a good strategy given a \nlimited batch of data. In this paper, we adopt the safe policy improvement \n(SPI) approach: we compute a target policy guaranteed to perform at least as \nwell as a given baseline policy. Our SPI strategy, inspired by the \nknows-what-it-knows paradigms, consists in bootstrapping the target policy with \nthe baseline policy when it does not know. We develop two computationally \nefficient bootstrapping algorithms, a value-based and a policy-based, both \naccompanied with theoretical SPI bounds. Three algorithm variants are proposed. \nWe empirically show the literature algorithms limits on a small stochastic \ngridworld problem, and then demonstrate that our five algorithms not only \nimprove the worst case scenarios, but also the mean performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1516598532, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed906f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06924"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Serdar Kadioglu", "title": "Column Generation for Interaction Coverage in Combinatorial Software Testing. (arXiv:1712.07081v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07081", "type": "text/html"}], "timestampUsec": "1513747858834890", "comments": [], "summary": {"content": "<p>This paper proposes a novel column generation framework for combinatorial \nsoftware testing. In particular, it combines Mathematical Programming and \nConstraint Programming in a hybrid decomposition to generate covering arrays. \nThe approach allows generating parameterized test cases with coverage \nguarantees between parameter interactions of a given application. Compared to \nexhaustive testing, combinatorial test case generation reduces the number of \ntests to run significantly. Our column generation algorithm is generic and can \naccommodate mixed coverage arrays over heterogeneous alphabets. The algorithm \nis realized in practice as a cloud service and recognized as one of the five \nwinners of the company-wide cloud application challenge at Oracle. The service \nis currently helping software developers from a range of different product \nteams in their testing efforts while exposing declarative constraint models and \nhybrid optimization techniques to a broader audience. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9072", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07081"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Fangyi Zhang, J&#xfc;rgen Leitner, Michael Milford, Peter Corke", "title": "Modular Deep Q Networks for Sim-to-real Transfer of Visuo-motor Policies. (arXiv:1610.06781v4 [cs.RO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.06781", "type": "text/html"}], "timestampUsec": "1513747858834889", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329e40c38\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329e40c38&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>While deep learning has had significant successes in computer vision thanks \nto the abundance of visual data, collecting sufficiently large real-world \ndatasets for robot learning can be costly. To increase the practicality of \nthese techniques on real robots, we propose a modular deep reinforcement \nlearning method capable of transferring models trained in simulation to a \nreal-world robotic task. We introduce a bottleneck between perception and \ncontrol, enabling the networks to be trained independently, but then merged and \nfine-tuned in an end-to-end manner to further improve hand-eye coordination. On \na canonical, planar visually-guided robot reaching task a fine-tuned accuracy \nof 1.6 pixels is achieved, a significant improvement over naive transfer (17.5 \npixels), showing the potential for more complicated and broader applications. \nOur method provides a technique for more efficient learning and transfer of \nvisuo-motor policies for real robotic systems without relying entirely on large \nreal-world robot datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9075", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.06781"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Christoph Wernhard", "title": "The Boolean Solution Problem from the Perspective of Predicate Logic - Extended Version. (arXiv:1706.08329v3 [cs.LO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.08329", "type": "text/html"}], "timestampUsec": "1513747858834888", "comments": [], "summary": {"content": "<p>Finding solution values for unknowns in Boolean equations was a principal \nreasoning mode in the Algebra of Logic of the 19th century. Schr\\\"oder \ninvestigated it as \"Aufl\\\"osungsproblem\" (\"solution problem\"). It is closely \nrelated to the modern notion of Boolean unification. Today it is commonly \npresented in an algebraic setting, but seems potentially useful also in \nknowledge representation based on predicate logic. We show that it can be \nmodeled on the basis of first-order logic extended by second-order \nquantification. A wealth of classical results transfers, foundations for \nalgorithms unfold, and connections with second-order quantifier elimination and \nCraig interpolation show up. Although for first-order inputs the set of \nsolutions is recursively enumerable, the development of constructive methods \nremains a challenge. We identify some cases that allow constructions, most of \nthem based on Craig interpolation, and show a method to take vocabulary \nrestrictions on solution components into account. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed907a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.08329"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ethan Perez, Harm de Vries, Florian Strub, Vincent Dumoulin, Aaron Courville", "title": "Learning Visual Reasoning Without Strong Priors. (arXiv:1707.03017v5 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.03017", "type": "text/html"}], "timestampUsec": "1513747858834887", "comments": [], "summary": {"content": "<p>Achieving artificial visual reasoning - the ability to answer image-related \nquestions which require a multi-step, high-level process - is an important step \ntowards artificial general intelligence. This multi-modal task requires \nlearning a question-dependent, structured reasoning process over images from \nlanguage. Standard deep learning approaches tend to exploit biases in the data \nrather than learn this underlying structure, while leading methods learn to \nvisually reason successfully but are hand-crafted for reasoning. We show that a \ngeneral-purpose, Conditional Batch Normalization approach achieves \nstate-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4% \nerror rate. We outperform the next best end-to-end method (4.5%) and even \nmethods that use extra supervision (3.1%). We probe our model to shed light on \nhow it reasons, showing it has learned a question-dependent, multi-step \nprocess. Previous work has operated under the assumption that visual reasoning \ncalls for a specialized architecture, but we show that a general architecture \nwith proper conditioning can learn to visually reason effectively. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9080", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.03017"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, Aaron Courville", "title": "FiLM: Visual Reasoning with a General Conditioning Layer. (arXiv:1709.07871v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.07871", "type": "text/html"}], "timestampUsec": "1513747858834886", "comments": [], "summary": {"content": "<p>We introduce a general-purpose conditioning method for neural networks called \nFiLM: Feature-wise Linear Modulation. FiLM layers influence neural network \ncomputation via a simple, feature-wise affine transformation based on \nconditioning information. We show that FiLM layers are highly effective for \nvisual reasoning - answering image-related questions which require a \nmulti-step, high-level process - a task which has proven difficult for standard \ndeep learning methods that do not explicitly model reasoning. Specifically, we \nshow on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error \nfor the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are \nrobust to ablations and architectural modifications, and 4) generalize well to \nchallenging, new data from few examples or even zero-shot. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9087", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.07871"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516080170, "author": "Farshid Rayhan, Sajid Ahmed, Asif Mahbub, Md. Rafsan Jani, Swakkhar Shatabda, Dewan Md. Farid, Chowdhury Mofizur Rahman", "title": "MEBoost: Mixing Estimators with Boosting for Imbalanced Data Classification. (arXiv:1712.06658v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.06658", "type": "text/html"}], "timestampUsec": "1513747858834885", "comments": [], "summary": {"content": "<p>Class imbalance problem has been a challenging research problem in the fields \nof machine learning and data mining as most real life datasets are imbalanced. \nSeveral existing machine learning algorithms try to maximize the accuracy \nclassification by correctly identifying majority class samples while ignoring \nthe minority class. However, the concept of the minority class instances \nusually represents a higher interest than the majority class. Recently, several \ncost sensitive methods, ensemble models and sampling techniques have been used \nin literature in order to classify imbalance datasets. In this paper, we \npropose MEBoost, a new boosting algorithm for imbalanced datasets. MEBoost \nmixes two different weak learners with boosting to improve the performance on \nimbalanced datasets. MEBoost is an alternative to the existing techniques such \nas SMOTEBoost, RUSBoost, Adaboost, etc. The performance of MEBoost has been \nevaluated on 12 benchmark imbalanced datasets with state of the art ensemble \nmethods like SMOTEBoost, RUSBoost, Easy Ensemble, EUSBoost, DataBoost. \nExperimental results show significant improvement over the other methods and it \ncan be concluded that MEBoost is an effective and promising algorithm to deal \nwith imbalance datasets. The python version of the code is available here: \nhttps://github.com/farshidrayhanuiu/ \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1516080168, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed908c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06658"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yash Deshpande, Lester Mackey, Vasilis Syrgkanis, Matt Taddy", "title": "Accurate Inference for Adaptive Linear Models. (arXiv:1712.06695v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06695", "type": "text/html"}], "timestampUsec": "1513747858834884", "comments": [], "summary": {"content": "<p>Estimators computed from adaptively collected data do not behave like their \nnon-adaptive brethren. Rather, the sequential dependence of the collection \npolicy can lead to severe distributional biases that persist even in the \ninfinite data limit. We develop a general method decorrelation procedure -- \nW-decorrelation -- for transforming the bias of adaptive linear regression \nestimators into variance. The method uses only coarse-grained information about \nthe data collection policy and does not need access to propensity scores or \nexact knowledge of the policy. We bound the finite-sample bias and variance of \nthe W-estimator and develop asymptotically correct confidence intervals based \non a novel martingale central limit theorem. We then demonstrate the empirical \nbenefits of the generic W-decorrelation procedure in two different adaptive \ndata settings: the multi-armed bandits and autoregressive time series models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9091", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06695"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kriste Krstovski, Michael J. Kurtz, David A. Smith, Alberto Accomazzi", "title": "Multilingual Topic Models. (arXiv:1712.06704v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06704", "type": "text/html"}], "timestampUsec": "1513747858834883", "comments": [], "summary": {"content": "<p>Scientific publications have evolved several features for mitigating \nvocabulary mismatch when indexing, retrieving, and computing similarity between \narticles. These mitigation strategies range from simply focusing on high-value \narticle sections, such as titles and abstracts, to assigning keywords, often \nfrom controlled vocabularies, either manually or through automatic annotation. \nVarious document representation schemes possess different cost-benefit \ntradeoffs. In this paper, we propose to model different representations of the \nsame article as translations of each other, all generated from a common latent \nrepresentation in a multilingual topic model. We start with a methodological \noverview on latent variable models for parallel document representations that \ncould be used across many information science tasks. We then show how solving \nthe inference problem of mapping diverse representations into a shared topic \nspace allows us to evaluate representations based on how topically similar they \nare to the original article. In addition, our proposed approach provides means \nto discover where different concept vocabularies require improvement. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9094", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06704"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jiajun Shen, Yali Amit", "title": "Deformable Classifiers. (arXiv:1712.06715v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06715", "type": "text/html"}], "timestampUsec": "1513747858834882", "comments": [], "summary": {"content": "<p>Geometric variations of objects, which do not modify the object class, pose a \nmajor challenge for object recognition. These variations could be rigid as well \nas non-rigid transformations. In this paper, we design a framework for training \ndeformable classifiers, where latent transformation variables are introduced, \nand a transformation of the object image to a reference instantiation is \ncomputed in terms of the classifier output, separately for each class. The \nclassifier outputs for each class, after transformation, are compared to yield \nthe final decision. As a by-product of the classification this yields a \ntransformation of the input object to a reference pose, which can be used for \ndownstream tasks such as the computation of object support. We apply a two-step \ntraining mechanism for our framework, which alternates between optimizing over \nthe latent transformation variables and the classifier parameters to minimize \nthe loss function. We show that multilayer perceptrons, also known as deep \nnetworks, are well suited for this approach and achieve state of the art \nresults on the rotated MNIST and the Google Earth dataset, and produce \ncompetitive results on MNIST and CIFAR-10 when training on smaller subsets of \ntraining data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9097", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06715"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jun Kitazono, Ryota Kanai, Masafumi Oizumi", "title": "Efficient Algorithms for Searching the Minimum Information Partition in Integrated Information Theory. (arXiv:1712.06745v1 [q-bio.NC])", "alternate": [{"href": "http://arxiv.org/abs/1712.06745", "type": "text/html"}], "timestampUsec": "1513747858834881", "comments": [], "summary": {"content": "<p>The ability to integrate information in the brain is considered to be an \nessential property for cognition and consciousness. Integrated Information \nTheory (IIT) hypothesizes that the amount of integrated information ($\\Phi$) in \nthe brain is related to the level of consciousness. IIT proposes that to \nquantify information integration in a system as a whole, integrated information \nshould be measured across the partition of the system at which information loss \ncaused by partitioning is minimized, called the Minimum Information Partition \n(MIP). The computational cost for exhaustively searching for the MIP grows \nexponentially with system size, making it difficult to apply IIT to real neural \ndata. It has been previously shown that if a measure of $\\Phi$ satisfies a \nmathematical property, submodularity, the MIP can be found in a polynomial \norder by an optimization algorithm. However, although the first version of \n$\\Phi$ is submodular, the later versions are not. In this study, we empirically \nexplore to what extent the algorithm can be applied to the non-submodular \nmeasures of $\\Phi$ by evaluating the accuracy of the algorithm in simulated \ndata and real neural data. We find that the algorithm identifies the MIP in a \nnearly perfect manner even for the non-submodular measures. Our results show \nthat the algorithm allows us to measure $\\Phi$ in large systems within a \npractical amount of time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed909d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06745"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Marzieh Haghighi, Simon K. Warfield, Sila Kurugol", "title": "Automatic Renal Segmentation in DCE-MRI using Convolutional Neural Networks. (arXiv:1712.07022v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.07022", "type": "text/html"}], "timestampUsec": "1513747858834880", "comments": [], "summary": {"content": "<p>Kidney function evaluation using dynamic contrast-enhanced MRI (DCE-MRI) \nimages could help in diagnosis and treatment of kidney diseases of children. \nAutomatic segmentation of renal parenchyma is an important step in this \nprocess. In this paper, we propose a time and memory efficient fully automated \nsegmentation method which achieves high segmentation accuracy with running time \nin the order of seconds in both normal kidneys and kidneys with hydronephrosis. \nThe proposed method is based on a cascaded application of two 3D convolutional \nneural networks that employs spatial and temporal information at the same time \nin order to learn the tasks of localization and segmentation of kidneys, \nrespectively. Segmentation performance is evaluated on both normal and abnormal \nkidneys with varying levels of hydronephrosis. We achieved a mean dice \ncoefficient of 91.4 and 83.6 for normal and abnormal kidneys of pediatric \npatients, respectively. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed909f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07022"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Adil Salim, Pascal Bianchi, Walid Hachem", "title": "Snake: a Stochastic Proximal Gradient Algorithm for Regularized Problems over Large Graphs. (arXiv:1712.07027v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1712.07027", "type": "text/html"}], "timestampUsec": "1513747858834879", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329e40f73\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329e40f73&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A regularized optimization problem over a large unstructured graph is \nstudied, where the regularization term is tied to the graph geometry. Typical \nregularization examples include the total variation and the Laplacian \nregularizations over the graph. When applying the proximal gradient algorithm \nto solve this problem, there exist quite affordable methods to implement the \nproximity operator (backward step) in the special case where the graph is a \nsimple path without loops. In this paper, an algorithm, referred to as \"Snake\", \nis proposed to solve such regularized problems over general graphs, by taking \nbenefit of these fast methods. The algorithm consists in properly selecting \nrandom simple paths in the graph and performing the proximal gradient algorithm \nover these simple paths. This algorithm is an instance of a new general \nstochastic proximal gradient algorithm, whose convergence is proven. \nApplications to trend filtering and graph inpainting are provided among others. \nNumerical experiments are conducted over large graphs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07027"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1515046376, "author": "Marta M. Stepniewska-Dziubinska, Piotr Zielenkiewicz, Pawel Siedlecki", "title": "Development and evaluation of a deep learning model for protein-ligand binding affinity prediction. (arXiv:1712.07042v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.07042", "type": "text/html"}], "timestampUsec": "1513747858834878", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329e8d558\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329e8d558&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Structure based ligand discovery is one of the most successful approaches for \naugmenting the drug discovery process. Currently, there is a notable shift \ntowards machine learning (ML) methodologies to aid such procedures. Deep \nlearning has recently gained considerable attention as it allows the model to \n\"learn\" to extract features that are relevant for the task at hand. We have \ndeveloped a novel deep neural network estimating the binding affinity of \nligand-receptor complexes. The complex is represented with a 3D grid, and the \nmodel utilizes a 3D convolution to produce a feature map of this \nrepresentation, treating the atoms of both proteins and ligands in the same \nmanner. Our network was tested on the CASF \"scoring power\" benchmark and Astex \nDiverse Set and outperformed classical scoring functions. The model, together \nwith usage instructions and examples, is available as a git repository at \n<a href=\"http://gitlab.com/cheminfIBB/pafnucy\">this http URL</a> \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1515046376, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90a8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07042"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yingbo Zhou, Caiming Xiong, Richard Socher", "title": "Improving End-to-End Speech Recognition with Policy Learning. (arXiv:1712.07101v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.07101", "type": "text/html"}], "timestampUsec": "1513747858834877", "comments": [], "summary": {"content": "<p>Connectionist temporal classification (CTC) is widely used for maximum \nlikelihood learning in end-to-end speech recognition models. However, there is \nusually a disparity between the negative maximum likelihood and the performance \nmetric used in speech recognition, e.g., word error rate (WER). This results in \na mismatch between the objective function and metric during training. We show \nthat the above problem can be mitigated by jointly training with maximum \nlikelihood and policy gradient. In particular, with policy learning we are able \nto directly optimize on the (otherwise non-differentiable) performance metric. \nWe show that joint training improves relative performance by 4% to 13% for our \nend-to-end model as compared to the same model learned through maximum \nlikelihood. The model achieves 5.53% WER on Wall Street Journal dataset, and \n5.42% and 14.70% on Librispeech test-clean and test-other set, respectively. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90ad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07101"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Shahin Shahrampour, Ahmad Beirami, Vahid Tarokh", "title": "On Data-Dependent Random Features for Improved Generalization in Supervised Learning. (arXiv:1712.07102v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07102", "type": "text/html"}], "timestampUsec": "1513747858834876", "comments": [], "summary": {"content": "<p>The randomized-feature approach has been successfully employed in large-scale \nkernel approximation and supervised learning. The distribution from which the \nrandom features are drawn impacts the number of features required to \nefficiently perform a learning task. Recently, it has been shown that employing \ndata-dependent randomization improves the performance in terms of the required \nnumber of random features. In this paper, we are concerned with the \nrandomized-feature approach in supervised learning for good generalizability. \nWe propose the Energy-based Exploration of Random Features (EERF) algorithm \nbased on a data-dependent score function that explores the set of possible \nfeatures and exploits the promising regions. We prove that the proposed score \nfunction with high probability recovers the spectrum of the best fit within the \nmodel class. Our empirical results on several benchmark datasets further verify \nthat our method requires smaller number of random features to achieve a certain \ngeneralization error compared to the state-of-the-art while introducing \nnegligible pre-processing overhead. EERF can be implemented in a few lines of \ncode and requires no additional tuning parameters. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90b1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07102"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jayaraman J. Thiagarajan, Shusen Liu, Karthikeyan Natesan Ramamurthy, Peer-Timo Bremer", "title": "Exploring High-Dimensional Structure via Axis-Aligned Decomposition of Linear Projections. (arXiv:1712.07106v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07106", "type": "text/html"}], "timestampUsec": "1513747858834875", "comments": [], "summary": {"content": "<p>Two-dimensional embeddings remain the dominant approach to visualize high \ndimensional data. The choice of embeddings ranges from highly non-linear ones, \nwhich can capture complex relationships but are difficult to interpret \nquantitatively, to axis-aligned projections, which are easy to interpret but \nare limited to bivariate relationships. Linear project can be considered as a \ncompromise between complexity and interpretability, as they allow explicit axes \nlabels, yet provide significantly more degrees of freedom compared to \naxis-aligned projections. Nevertheless, interpreting the axes directions, which \nare linear combinations often with many non-trivial components, remains \ndifficult. To address this problem we introduce a structure aware decomposition \nof (multiple) linear projections into sparse sets of axis aligned projections, \nwhich jointly capture all information of the original linear ones. In \nparticular, we use tools from Dempster-Shafer theory to formally define how \nrelevant a given axis aligned project is to explain the neighborhood relations \ndisplayed in some linear projection. Furthermore, we introduce a new approach \nto discover a diverse set of high quality linear projections and show that in \npractice the information of $k$ linear projections is often jointly encoded in \n$\\sim k$ axis aligned plots. We have integrated these ideas into an interactive \nvisualization system that allows users to jointly browse both linear \nprojections and their axis aligned representatives. Using a number of case \nstudies we show how the resulting plots lead to more intuitive visualizations \nand new insight. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90b4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07106"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yingbo Zhou, Caiming Xiong, Richard Socher", "title": "Improved Regularization Techniques for End-to-End Speech Recognition. (arXiv:1712.07108v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.07108", "type": "text/html"}], "timestampUsec": "1513747858834874", "comments": [], "summary": {"content": "<p>Regularization is important for end-to-end speech models, since the models \nare highly flexible and easy to overfit. Data augmentation and dropout has been \nimportant for improving end-to-end models in other domains. However, they are \nrelatively under explored for end-to-end speech models. Therefore, we \ninvestigate the effectiveness of both methods for end-to-end trainable, deep \nspeech recognition models. We augment audio data through random perturbations \nof tempo, pitch, volume, temporal alignment, and adding random noise.We further \ninvestigate the effect of dropout when applied to the inputs of all layers of \nthe network. We show that the combination of data augmentation and dropout give \na relative performance improvement on both Wall Street Journal (WSJ) and \nLibriSpeech dataset of over 20%. Our model performance is also competitive with \nother end-to-end speech models on both datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90b8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07108"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Per Mattsson, Dave Zachariah, Petre Stoica", "title": "Recursive nonlinear-system identification using latent variables. (arXiv:1606.04366v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1606.04366", "type": "text/html"}], "timestampUsec": "1513747858834873", "comments": [], "summary": {"content": "<p>In this paper we develop a method for learning nonlinear systems with \nmultiple outputs and inputs. We begin by modelling the errors of a nominal \npredictor of the system using a latent variable framework. Then using the \nmaximum likelihood principle we derive a criterion for learning the model. The \nresulting optimization problem is tackled using a majorization-minimization \napproach. Finally, we develop a convex majorization technique and show that it \nenables a recursive identification method. The method learns parsimonious \npredictive models and is tested on both synthetic and real nonlinear systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90bb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1606.04366"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jen-Cheng Hou, Syu-Siang Wang, Ying-Hui Lai, Yu Tsao, Hsiu-Wen Chang, Hsin-Min Wang", "title": "Audio-Visual Speech Enhancement based on Multimodal Deep Convolutional Neural Networks. (arXiv:1703.10893v5 [cs.SD] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.10893", "type": "text/html"}], "timestampUsec": "1513747858834872", "comments": [], "summary": {"content": "<p>Speech enhancement (SE) aims to reduce noise in speech signals. Most SE \ntechniques focus only on addressing audio information. In this work, inspired \nby multimodal learning, which utilizes data from different modalities, and the \nrecent success of convolutional neural networks (CNNs) in SE, we propose an \naudio-visual deep CNNs (AVDCNN) SE model, which incorporates audio and visual \nstreams into a unified network model. We also propose a multi-task learning \nframework for reconstructing audio and visual signals at the output layer. \nPrecisely speaking, the proposed AVDCNN model is structured as an audio-visual \nencoder-decoder network, in which audio and visual data are first processed \nusing individual CNNs, and then fused into a joint network to generate enhanced \nspeech (the primary task) and reconstructed images (the secondary task) at the \noutput layer. The model is trained in an end-to-end manner, and parameters are \njointly learned through back-propagation. We evaluate enhanced speech using \nfive instrumental criteria. Results show that the AVDCNN model yields a notably \nsuperior performance compared with an audio-only CNN-based SE model and two \nconventional SE approaches, confirming the effectiveness of integrating visual \ninformation into the SE process. In addition, the AVDCNN model also outperforms \nan existing audio-visual SE model, confirming its capability of effectively \ncombining audio and visual information in SE. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90c0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.10893"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Stephan Cl&#xe9;men&#xe7;on, Anna Korba, Eric Sibony", "title": "Ranking Median Regression: Learning to Order through Local Consensus. (arXiv:1711.00070v2 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.00070", "type": "text/html"}], "timestampUsec": "1513747858834871", "comments": [], "summary": {"content": "<p>This article is devoted to the problem of predicting the value taken by a \nrandom permutation $\\Sigma$, describing the preferences of an individual over a \nset of numbered items $\\{1,\\; \\ldots,\\; n\\}$ say, based on the observation of \nan input/explanatory r.v. $X$ e.g. characteristics of the individual), when \nerror is measured by the Kendall $\\tau$ distance. In the probabilistic \nformulation of the 'Learning to Order' problem we propose, which extends the \nframework for statistical Kemeny ranking aggregation developped in \n\\citet{CKS17}, this boils down to recovering conditional Kemeny medians of \n$\\Sigma$ given $X$ from i.i.d. training examples $(X_1, \\Sigma_1),\\; \\ldots,\\; \n(X_N, \\Sigma_N)$. For this reason, this statistical learning problem is \nreferred to as \\textit{ranking median regression} here. Our contribution is \ntwofold. We first propose a probabilistic theory of ranking median regression: \nthe set of optimal elements is characterized, the performance of empirical risk \nminimizers is investigated in this context and situations where fast learning \nrates can be achieved are also exhibited. Next we introduce the concept of \nlocal consensus/median, in order to derive efficient methods for ranking median \nregression. The major advantage of this local learning approach lies in its \nclose connection with the widely studied Kemeny aggregation problem. From an \nalgorithmic perspective, this permits to build predictive rules for ranking \nmedian regression by implementing efficient techniques for (approximate) Kemeny \nmedian computations at a local level in a tractable manner. In particular, \nversions of $k$-nearest neighbor and tree-based methods, tailored to ranking \nmedian regression, are investigated. Accuracy of piecewise constant ranking \nmedian regression rules is studied under a specific smoothness assumption for \n$\\Sigma$'s conditional distribution given $X$. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90c2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.00070"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Andreas Svensson, Dave Zachariah, Thomas B. Sch&#xf6;n", "title": "How consistent is my model with the data? Information-Theoretic Model Check. (arXiv:1712.02675v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02675", "type": "text/html"}], "timestampUsec": "1513747858834870", "comments": [], "summary": {"content": "<p>The choice of model class is fundamental in statistical learning and system \nidentification, no matter whether the class is derived from physical principles \nor is a generic black-box. We develop a method to evaluate the specified model \nclass by assessing its capability of reproducing data that is similar to the \nobserved data record. This model check is based on the information-theoretic \nproperties of models viewed as data generators and is applicable to e.g. \nsequential data and nonlinear dynamical models. The method can be understood as \na specific two-sided posterior predictive test. We apply the \ninformation-theoretic model check to both synthetic and real data and compare \nit with a classical whiteness test. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02675"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Danyang Sun, Tongzheng Ren, Chongxun Li, Jun Zhu, Hang Su", "title": "Learning to Write Stylized Chinese Characters by Reading a Handful of Examples. (arXiv:1712.06424v1 [cs.CV] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1712.06424", "type": "text/html"}], "timestampUsec": "1513747858834869", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329e8d8bb\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329e8d8bb&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Automatically writing stylized Chinese characters is an attractive yet \nchallenging task due to its wide applicabilities. In this paper, we propose a \nnovel framework named Style-Aware Variational Auto-Encoder (SA-VAE) to flexibly \ngenerate Chinese characters. Specifically, we propose to capture the different \ncharacteristics of a Chinese character by disentangling the latent features \ninto content-related and style-related components. Considering of the complex \nshapes and structures, we incorporate the structure information as prior \nknowledge into our framework to guide the generation. Our framework shows a \npowerful one-shot/low-shot generalization ability by inferring the style \ncomponent given a character with unseen style. To the best of our knowledge, \nthis is the first attempt to learn to write new-style Chinese characters by \nobserving only one or a few examples. Extensive experiments demonstrate its \neffectiveness in generating different stylized Chinese characters by fusing the \nfeature vectors corresponding to different contents and styles, which is of \nsignificant importance in real-world applications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90ca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06424"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tom&#xe1;&#x161; Ko&#x10d;isk&#xfd;, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G&#xe1;bor Melis, Edward Grefenstette", "title": "The NarrativeQA Reading Comprehension Challenge. (arXiv:1712.07040v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.07040", "type": "text/html"}], "timestampUsec": "1513746930967327", "comments": [], "summary": {"content": "<p>Reading comprehension (RC)---in contrast to information retrieval---requires \nintegrating information and reasoning about events, entities, and their \nrelations across a full document. Question answering is conventionally used to \nassess RC ability, in both artificial agents and children learning to read. \nHowever, existing RC datasets and tasks are dominated by questions that can be \nsolved by selecting answers using superficial information (e.g., local context \nsimilarity or global term frequency); they thus fail to test for the essential \nintegrative aspect of RC. To encourage progress on deeper comprehension of \nlanguage, we present a new dataset and set of tasks in which the reader must \nanswer questions about stories by reading entire books or movie scripts. These \ntasks are designed so that successfully answering their questions requires \nunderstanding the underlying narrative rather than relying on shallow pattern \nmatching or salience. We show that although humans solve the tasks easily, \nstandard RC models struggle on the tasks presented here. We provide an analysis \nof the dataset and the challenges it presents. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda18", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07040"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lars Eidnes, Arild N&#xf8;kland", "title": "Shifting Mean Activation Towards Zero with Bipolar Activation Functions. (arXiv:1709.04054v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04054", "type": "text/html"}], "timestampUsec": "1513746930967326", "comments": [], "summary": {"content": "<p>We propose a simple extension to the ReLU-family of activation functions that \nallows them to shift the mean activation across a layer towards zero. Combined \nwith proper weight initialization, this alleviates the need for normalization \nlayers. We explore the training of deep vanilla recurrent neural networks \n(RNNs) with up to 144 layers, and show that bipolar activation functions help \nlearning in this setting. On the Penn Treebank and Text8 language modeling \ntasks we obtain competitive results, improving on the best reported results for \nnon-gated networks. In experiments with convolutional neural networks without \nbatch normalization, we find that bipolar activations produce a faster drop in \ntraining error, and results in a lower test error on the CIFAR-10 \nclassification task. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda26", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04054"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Boris Chidlovskii", "title": "Mining Smart Card Data for Travelers' Mini Activities. (arXiv:1712.06935v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.06935", "type": "text/html"}], "timestampUsec": "1513746930967325", "comments": [], "summary": {"content": "<p>In the context of public transport modeling and simulation, we address the \nproblem of mismatch between simulated transit trips and observed ones. We point \nto the weakness of the current travel demand modeling process; the trips it \ngenerates are over-optimistic and do not reflect the real passenger choices. We \nintroduce the notion of mini activities the travelers do during the trips; they \ncan explain the deviation of simulated trips from the observed trips. We \npropose to mine the smart card data to extract the mini activities. We develop \na technique to integrate them in the generated trips and learn such an \nintegration from two available sources, the trip history and trip planner \nrecommendations. For an input travel demand, we build a Markov chain over the \ntrip collection and apply the Monte Carlo Markov Chain algorithm to integrate \nmini activities in such a way that the selected characteristics converge to the \ndesired distributions. We test our method in different settings on the \npassenger trip collection of Nancy, France. We report experimental results \ndemonstrating a very important mismatch reduction. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda2e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06935"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rasoul Kaljahi, Jennifer Foster", "title": "Any-gram Kernels for Sentence Classification: A Sentiment Analysis Case Study. (arXiv:1712.07004v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.07004", "type": "text/html"}], "timestampUsec": "1513746930967324", "comments": [], "summary": {"content": "<p>Any-gram kernels are a flexible and efficient way to employ bag-of-n-gram \nfeatures when learning from textual data. They are also compatible with the use \nof word embeddings so that word similarities can be accounted for. While the \noriginal any-gram kernels are implemented on top of tree kernels, we propose a \nnew approach which is independent of tree kernels and is more efficient. We \nalso propose a more effective way to make use of word embeddings than the \noriginal any-gram formulation. When applied to the task of sentiment \nclassification, our new formulation achieves significantly better performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda34", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07004"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Upol Ehsan, Brent Harrison, Larry Chan, Mark O. Riedl", "title": "Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations. (arXiv:1702.07826v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1702.07826", "type": "text/html"}], "timestampUsec": "1513746930967323", "comments": [], "summary": {"content": "<p>We introduce AI rationalization, an approach for generating explanations of \nautonomous system behavior as if a human had performed the behavior. We \ndescribe a rationalization technique that uses neural machine translation to \ntranslate internal state-action representations of an autonomous agent into \nnatural language. We evaluate our technique in the Frogger game environment, \ntraining an autonomous game playing agent to rationalize its action choices \nusing natural language. A natural language training corpus is collected from \nhuman players thinking out loud as they play the game. We motivate the use of \nrationalization as an approach to explanation generation and show the results \nof two experiments evaluating the effectiveness of rationalization. Results of \nthese evaluations show that neural machine translation is able to accurately \ngenerate rationalizations that describe agent behavior, and that \nrationalizations are more satisfying to humans than other alternative methods \nof explanation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda39", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1702.07826"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ardhendu Tripathy, Ye Wang, Prakash Ishwar", "title": "Privacy-Preserving Adversarial Networks. (arXiv:1712.07008v1 [cs.IT])", "alternate": [{"href": "http://arxiv.org/abs/1712.07008", "type": "text/html"}], "timestampUsec": "1513746930967322", "comments": [], "summary": {"content": "<p>We propose a data-driven framework for optimizing privacy-preserving data \nrelease mechanisms toward the information-theoretically optimal tradeoff \nbetween minimizing distortion of useful data and concealing sensitive \ninformation. Our approach employs adversarially-trained neural networks to \nimplement randomized mechanisms and to perform a variational approximation of \nmutual information privacy. We empirically validate our Privacy-Preserving \nAdversarial Networks (PPAN) framework with experiments conducted on discrete \nand continuous synthetic data, as well as the MNIST handwritten digits dataset. \nWith the synthetic data, we find that our model-agnostic PPAN approach achieves \ntradeoff points very close to the optimal tradeoffs that are \nanalytically-derived from model knowledge. In experiments with the MNIST data, \nwe visually demonstrate a learned tradeoff between minimizing the pixel-level \ndistortion versus concealing the written digit. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda43", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07008"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiaoyong Yuan, Pan He, Qile Zhu, Rajendra Rana Bhat, Xiaolin Li", "title": "Adversarial Examples: Attacks and Defenses for Deep Learning. (arXiv:1712.07107v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.07107", "type": "text/html"}], "timestampUsec": "1513746930967321", "comments": [], "summary": {"content": "<p>With rapid progress and great successes in a wide spectrum of applications, \ndeep learning is being applied in many safety-critical environments. However, \ndeep neural networks have been recently found vulnerable to well-designed input \nsamples, called \\textit{adversarial examples}. Adversarial examples are \nimperceptible to human but can easily fool deep neural networks in the \ntesting/deploying stage. The vulnerability to adversarial examples becomes one \nof the major risks for applying deep neural networks in safety-critical \nscenarios. Therefore, the attacks and defenses on adversarial examples draw \ngreat attention. \n</p> \n<p>In this paper, we review recent findings on adversarial examples against deep \nneural networks, summarize the methods for generating adversarial examples, and \npropose a taxonomy of these methods. Under the taxonomy, applications and \ncountermeasures for adversarial examples are investigated. We further elaborate \non adversarial examples and explore the challenges and the potential solutions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07107"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin", "title": "Query-Efficient Black-box Adversarial Examples. (arXiv:1712.07113v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.07113", "type": "text/html"}], "timestampUsec": "1513746930967320", "comments": [], "summary": {"content": "<p>Current neural network-based image classifiers are susceptible to adversarial \nexamples, even in the black-box setting, where the attacker is limited to query \naccess without access to gradients. Previous methods --- substitute networks \nand coordinate-based finite-difference methods --- are either unreliable or \nquery-inefficient, making these methods impractical for certain problems. \n</p> \n<p>We introduce a new method for reliably generating adversarial examples under \nmore restricted, practical black-box threat models. First, we apply natural \nevolution strategies to perform black-box attacks using two to three orders of \nmagnitude fewer queries than previous methods. Second, we introduce a new \nalgorithm to perform targeted adversarial attacks in the partial-information \nsetting, where the attacker only has access to a limited number of target \nclasses. Using these techniques, we successfully perform the first targeted \nadversarial attack against a commercially deployed machine learning system, the \nGoogle Cloud Vision API, in the partial information setting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda53", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07113"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Na Lei, Kehua Su, Li Cui, Shing-Tung Yau, David Xianfeng Gu", "title": "A Geometric View of Optimal Transportation and Generative Model. (arXiv:1710.05488v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.05488", "type": "text/html"}], "timestampUsec": "1513746930967319", "comments": [], "summary": {"content": "<p>In this work, we show the intrinsic relations between optimal transportation \nand convex geometry, especially the variational approach to solve Alexandrov \nproblem: constructing a convex polytope with prescribed face normals and \nvolumes. This leads to a geometric interpretation to generative models, and \nleads to a novel framework for generative models. By using the optimal \ntransportation view of GAN model, we show that the discriminator computes the \nKantorovich potential, the generator calculates the transportation map. For a \nlarge class of transportation costs, the Kantorovich potential can give the \noptimal transportation map by a close-form formula. Therefore, it is sufficient \nto solely optimize the discriminator. This shows the adversarial competition \ncan be avoided, and the computational architecture can be simplified. \nPreliminary experimental results show the geometric method outperforms WGAN for \napproximating probability measures with multiple clusters in low dimensional \nspace. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda60", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.05488"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, Bernhard Schoelkopf", "title": "Wasserstein Auto-Encoders. (arXiv:1711.01558v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.01558", "type": "text/html"}], "timestampUsec": "1513746930967318", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329e8dc4a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329e8dc4a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building \na generative model of the data distribution. WAE minimizes a penalized form of \nthe Wasserstein distance between the model distribution and the target \ndistribution, which leads to a different regularizer than the one used by the \nVariational Auto-Encoder (VAE). This regularizer encourages the encoded \ntraining distribution to match the prior. We compare our algorithm with several \nother techniques and show that it is a generalization of adversarial \nauto-encoders (AAE). Our experiments show that WAE shares many of the \nproperties of VAEs (stable training, encoder-decoder architecture, nice latent \nmanifold structure) while generating samples of better quality, as measured by \nthe FID score. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda67", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.01558"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Chih-Cheng Chang, Pin-Chun Chen, Teyuh Chou, I-Ting Wang, Boris Hudec, Che-Chia Chang, Chia-Ming Tsai, Tian-Sheuan Chang, Tuo-Hung Hou", "title": "Mitigating Asymmetric Nonlinear Weight Update Effects in Hardware Neural Network based on Analog Resistive Synapse. (arXiv:1712.05895v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.05895", "type": "text/html"}], "timestampUsec": "1513676967346947", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329ee5377\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329ee5377&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Asymmetric nonlinear weight update is considered as one of the major \nobstacles for realizing hardware neural networks based on analog resistive \nsynapses because it significantly compromises the online training capability. \nThis paper provides new solutions to this critical issue through \nco-optimization with the hardware-applicable deep-learning algorithms. New \ninsights on engineering activation functions and a threshold weight update \nscheme effectively suppress the undesirable training noise induced by \ninaccurate weight update. We successfully trained a two-layer perceptron \nnetwork online and improved the classification accuracy of MNIST handwritten \ndigit dataset to 87.8/94.8% by using 6-bit/8-bit analog synapses, respectively, \nwith extremely high asymmetric nonlinearity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e648", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05895"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Han Xiao", "title": "NDT: Neual Decision Tree Towards Fully Functioned Neural Graph. (arXiv:1712.05934v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.05934", "type": "text/html"}], "timestampUsec": "1513676967346946", "comments": [], "summary": {"content": "<p>Though traditional algorithms could be embedded into neural architectures \nwith the proposed principle of \\cite{xiao2017hungarian}, the variables that \nonly occur in the condition of branch could not be updated as a special case. \nTo tackle this issue, we multiply the conditioned branches with Dirac symbol \n(i.e. $\\mathbf{1}_{x&gt;0}$), then approximate Dirac symbol with the continuous \nfunctions (e.g. $1 - e^{-\\alpha|x|}$). In this way, the gradients of \ncondition-specific variables could be worked out in the back-propagation \nprocess, approximately, making a fully functioned neural graph. Within our \nnovel principle, we propose the neural decision tree \\textbf{(NDT)}, which \ntakes simplified neural networks as decision function in each branch and \nemploys complex neural networks to generate the output in each leaf. Extensive \nexperiments verify our theoretical analysis and demonstrate the effectiveness \nof our model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e659", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05934"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Assaf Shocher, Nadav Cohen, Michal Irani", "title": "\"Zero-Shot\" Super-Resolution using Deep Internal Learning. (arXiv:1712.06087v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.06087", "type": "text/html"}], "timestampUsec": "1513676967346945", "comments": [], "summary": {"content": "<p>Deep Learning has led to a dramatic leap in Super-Resolution (SR) performance \nin the past few years. However, being supervised, these SR methods are \nrestricted to specific training data, where the acquisition of the \nlow-resolution (LR) images from their high-resolution (HR) counterparts is \npredetermined (e.g., bicubic downscaling), without any distracting artifacts \n(e.g., sensor noise, image compression, non-ideal PSF, etc). Real LR images, \nhowever, rarely obey these restrictions, resulting in poor SR results by SotA \n(State of the Art) methods. In this paper we introduce \"Zero-Shot\" SR, which \nexploits the power of Deep Learning, but does not rely on prior training. We \nexploit the internal recurrence of information inside a single image, and train \na small image-specific CNN at test time, on examples extracted solely from the \ninput image itself. As such, it can adapt itself to different settings per \nimage. This allows to perform SR of real old photos, noisy images, biological \ndata, and other images where the acquisition process is unknown or non-ideal. \nOn such images, our method outperforms SotA CNN-based SR methods, as well as \nprevious unsupervised SR methods. To the best of our knowledge, this is the \nfirst unsupervised CNN-based SR method. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e66b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06087"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Rudy Raymond, Takayuki Osogami, Sakyasingha Dasgupta", "title": "Dynamic Boltzmann Machines for Second Order Moments and Generalized Gaussian Distributions. (arXiv:1712.06132v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06132", "type": "text/html"}], "timestampUsec": "1513676967346944", "comments": [], "summary": {"content": "<p>Dynamic Boltzmann Machine (DyBM) has been shown highly efficient to predict \ntime-series data. Gaussian DyBM is a DyBM that assumes the predicted data is \ngenerated by a Gaussian distribution whose first-order moment (mean) \ndynamically changes over time but its second-order moment (variance) is fixed. \nHowever, in many financial applications, the assumption is quite limiting in \ntwo aspects. First, even when the data follows a Gaussian distribution, its \nvariance may change over time. Such variance is also related to important \ntemporal economic indicators such as the market volatility. Second, financial \ntime-series data often requires learning datasets generated by the generalized \nGaussian distribution with an additional shape parameter that is important to \napproximate heavy-tailed distributions. Addressing those aspects, we show how \nto extend DyBM that results in significant performance improvement in \npredicting financial time-series data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e679", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06132"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1515390156, "author": "Sheng Xin Zhang, Wing Shing Chan, Zi Kang Peng, Shao Yong Zheng, Kit Sang Tang", "title": "Selective-Candidate Framework with Similarity Selection Rule for Evolutionary Optimization. (arXiv:1712.06338v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.06338", "type": "text/html"}], "timestampUsec": "1513676967346943", "comments": [], "summary": {"content": "<p>This paper proposes to resolve limitations of the traditional \none-reproduction (OR) framework which produces only one candidate in a single \nreproduction procedure. A selective-candidate framework with similarity \nselection rule (SCSS) is suggested to make possible, a selective direction of \nsearch. In the SCSS framework, M (M &gt; 1) candidates are generated from each \ncurrent solution by independently conducting the reproduction procedure M \ntimes. The winner is then determined by employing a similarity selection rule. \nTo maintain balanced exploitation and exploration capabilities, an efficient \nsimilarity selection rule based on the Euclidian distances between each of the \nM candidates and the corresponding current solution is proposed. The SCSS \nframework can be easily applied to any evolutionary algorithms or swarm \nintelligences. Experiments conducted with 60 benchmark functions show the \nsuperiority of SCSS over OR in three classic, four state-of-the-art and four \nup-to-date algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1515390156, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e68a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06338"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516954929, "author": "Brian Kenji Iwana, Seiichi Uchida", "title": "Dynamic Weight Alignment for Convolutional Neural Networks. (arXiv:1712.06530v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.06530", "type": "text/html"}], "timestampUsec": "1513676967346942", "comments": [], "summary": {"content": "<p>In this paper, we propose a method of improving Convolutional Neural Networks \n(CNN) by determining the optimal alignment of weights and inputs using dynamic \nprogramming. Conventional CNNs convolve learnable shared weights, or filters, \nacross the input data. The filters use a linear matching of weights to inputs \nusing an inner product between the filter and a window of the input. However, \nit is possible that there exists a more optimal alignment of weights. Thus, we \npropose the use of Dynamic Time Warping (DTW) to dynamically align the weights \nto optimized input elements. This dynamic alignment is useful for time series \nrecognition due to the complexities of temporal relations and temporal \ndistortions. We demonstrate the effectiveness of the proposed architecture on \nthe Unipen online handwritten digit and character datasets, the UCI Spoken \nArabic Digit dataset, and the UCI Activities of Daily Life dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1516954929, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e695", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06530"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1515649357, "author": "Noah Golowich, Alexander Rakhlin, Ohad Shamir", "title": "Size-Independent Sample Complexity of Neural Networks. (arXiv:1712.06541v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.06541", "type": "text/html"}], "timestampUsec": "1513676967346941", "comments": [], "summary": {"content": "<p>We study the sample complexity of learning neural networks, by providing new \nbounds on their Rademacher complexity assuming norm constraints on the \nparameter matrix of each layer. Compared to previous work, these complexity \nbounds have improved dependence on the network depth, and under some additional \nassumptions, are fully independent of the network size (both depth and width). \nThese results are derived using some novel techniques, which may be of \nindependent interest. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1515649357, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e6a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06541"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xingwen Zhang, Jeff Clune, Kenneth O. Stanley", "title": "On the Relationship Between the OpenAI Evolution Strategy and Stochastic Gradient Descent. (arXiv:1712.06564v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.06564", "type": "text/html"}], "timestampUsec": "1513676967346940", "comments": [], "summary": {"content": "<p>Because stochastic gradient descent (SGD) has shown promise optimizing neural \nnetworks with millions of parameters and few if any alternatives are known to \nexist, it has moved to the heart of leading approaches to reinforcement \nlearning (RL). For that reason, the recent result from OpenAI showing that a \nparticular kind of evolution strategy (ES) can rival the performance of \nSGD-based deep RL methods with large neural networks provoked surprise. This \nresult is difficult to interpret in part because of the lingering ambiguity on \nhow ES actually relates to SGD. The aim of this paper is to significantly \nreduce this ambiguity through a series of MNIST-based experiments designed to \nuncover their relationship. As a simple supervised problem without domain noise \n(unlike in most RL), MNIST makes it possible (1) to measure the correlation \nbetween gradients computed by ES and SGD and (2) then to develop an SGD-based \nproxy that accurately predicts the performance of different ES population \nsizes. These innovations give a new level of insight into the real capabilities \nof ES, and lead also to some unconventional means for applying ES to supervised \nproblems that shed further light on its differences from SGD. Incorporating \nthese lessons, the paper concludes by demonstrating that ES can achieve 99% \naccuracy on MNIST, a number higher than any previously published result for any \nevolutionary method. While not by any means suggesting that ES should \nsubstitute for SGD in supervised learning, the suite of experiments herein \nenables more informed decisions on the application of ES within RL and other \nparadigms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e6b3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06564"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1516253548, "author": "Joel Lehman, Jay Chen, Jeff Clune, Kenneth O. Stanley", "title": "ES Is More Than Just a Traditional Finite-Difference Approximator. (arXiv:1712.06568v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.06568", "type": "text/html"}], "timestampUsec": "1513676967346939", "comments": [], "summary": {"content": "<p>An evolution strategy (ES) variant recently attracted significant attention \ndue to its surprisingly good performance at optimizing neural networks in \nchallenging deep reinforcement learning domains. It searches directly in the \nparameter space of neural networks by generating perturbations to the current \nset of parameters, checking their performance, and moving in the direction of \nhigher reward. The resemblance of this algorithm to a traditional \nfinite-difference approximation of the reward gradient in parameter space \nnaturally leads to the assumption that it is just that. However, this \nassumption is incorrect. The aim of this paper is to definitively demonstrate \nthis point empirically. ES is a gradient approximator, but optimizes for a \ndifferent gradient than just reward (especially when the magnitude of candidate \nperturbations is high). Instead, it optimizes for the average reward of the \nentire population, often also promoting parameters that are robust to \nperturbation. This difference can channel ES into significantly different areas \nof the search space than gradient descent in parameter space, and also \nconsequently to networks with significantly different properties. This unique \nrobustness-seeking property, and its consequences for optimization, are \ndemonstrated in several domains. They include humanoid locomotion, where \nnetworks from policy gradient-based reinforcement learning are far less robust \nto parameter perturbation than ES-based policies that solve the same task. \nWhile the implications of such robustness and robustness-seeking remain open to \nfurther study, the main contribution of this work is to highlight that such \ndifferences indeed exist and deserve attention. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1516253548, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e6bf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06568"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Aaron Tuor, Samuel Kaplan, Brian Hutchinson, Nicole Nichols, Sean Robinson", "title": "Deep Learning for Unsupervised Insider Threat Detection in Structured Cybersecurity Data Streams. (arXiv:1710.00811v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.00811", "type": "text/html"}], "timestampUsec": "1513676967346938", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329ee5641\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329ee5641&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Analysis of an organization's computer network activity is a key component of \nearly detection and mitigation of insider threat, a growing concern for many \norganizations. Raw system logs are a prototypical example of streaming data \nthat can quickly scale beyond the cognitive power of a human analyst. As a \nprospective filter for the human analyst, we present an online unsupervised \ndeep learning approach to detect anomalous network activity from system logs in \nreal time. Our models decompose anomaly scores into the contributions of \nindividual user behavior features for increased interpretability to aid \nanalysts reviewing potential cases of insider threat. Using the CERT Insider \nThreat Dataset v6.2 and threat detection recall as our performance metric, our \nnovel deep and recurrent neural network models outperform Principal Component \nAnalysis, Support Vector Machine and Isolation Forest based anomaly detection \nbaselines. For our best model, the events labeled as insider threat activity in \nour dataset had an average anomaly score in the 95.53 percentile, demonstrating \nour approach's potential to greatly reduce analyst workloads. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e6d0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.00811"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Maithra Raghu, Alex Irpan, Jacob Andreas, Robert Kleinberg, Quoc V. Le, Jon Kleinberg", "title": "Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?. (arXiv:1711.02301v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.02301", "type": "text/html"}], "timestampUsec": "1513676967346937", "comments": [], "summary": {"content": "<p>Deep reinforcement learning has achieved many recent successes, but our \nunderstanding of its strengths and limitations is hampered by the lack of rich \nenvironments in which we can fully characterize optimal behavior, and \ncorrespondingly diagnose individual actions against such a characterization. \nHere we consider a family of combinatorial games, arising from work of Erdos, \nSelfridge, and Spencer, and we propose their use as environments for evaluating \nand comparing different approaches to reinforcement learning. These games have \na number of appealing features: they are challenging for current learning \napproaches, but they form (i) a low-dimensional, simply parametrized \nenvironment where (ii) there is a linear closed form solution for optimal \nbehavior from any state, and (iii) the difficulty of the game can be tuned by \nchanging environment parameters in an interpretable way. We use these \nErdos-Selfridge-Spencer games not only to compare different algorithms, but \nalso to compare approaches based on supervised and reinforcement learning, to \nanalyze the power of multi-agent approaches in improving performance, and to \nevaluate generalization to environments outside the training set. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e6dc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.02301"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1514957677, "author": "Stuart Armstrong, S&#xf6;ren Mindermann", "title": "Impossibility of deducing preferences and rationality from human policy. (arXiv:1712.05812v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.05812", "type": "text/html"}], "timestampUsec": "1513676967346936", "comments": [], "summary": {"content": "<p>Inverse reinforcement learning (IRL) attempts to infer human rewards or \npreferences from observed behavior. However, human planning systematically \ndeviates from rationality. Though there has been some IRL work which assumes \nhumans are noisily rational, there has been little analysis of the general \nproblem of inferring the reward of a human of unknown rationality. The observed \nbehavior can, in principle, be decomposed into two components: a reward \nfunction and a planning algorithm that maps reward function to policy. Both of \nthese variables have to be inferred from behaviour. This paper presents a \"No \nFree Lunch\" theorem in this area, showing that, without making `normative' \nassumptions beyond the data, nothing about the human reward function can be \ndeduced from human behaviour. Unlike most No Free Lunch theorems, this cannot \nbe alleviated by regularising with simplicity assumptions. The simplest \nhypotheses are generally degenerate. The paper will then sketch how one might \nbegin to use normative assumptions to get around the problem, without which \nsolving the general IRL problem is impossible. The reward function-planning \nalgorithm formalism can also be used to encode what it means for an agent to \nmanipulate or override human preferences. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1514957676, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e6e3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05812"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ion Stoica, Dawn Song, Raluca Ada Popa, David Patterson, Michael W. Mahoney, Randy Katz, Anthony D. Joseph, Michael Jordan, Joseph M. Hellerstein, Joseph E. Gonzalez, Ken Goldberg, Ali Ghodsi, David Culler, Pieter Abbeel", "title": "A Berkeley View of Systems Challenges for AI. (arXiv:1712.05855v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.05855", "type": "text/html"}], "timestampUsec": "1513676967346935", "comments": [], "summary": {"content": "<p>With the increasing commoditization of computer vision, speech recognition \nand machine translation systems and the widespread deployment of learning-based \nback-end technologies such as digital advertising and intelligent \ninfrastructures, AI (Artificial Intelligence) has moved from research labs to \nproduction. These changes have been made possible by unprecedented levels of \ndata and computation, by methodological advances in machine learning, by \ninnovations in systems software and architectures, and by the broad \naccessibility of these technologies. \n</p> \n<p>The next generation of AI systems promises to accelerate these developments \nand increasingly impact our lives via frequent interactions and making (often \nmission-critical) decisions on our behalf, often in highly personalized \ncontexts. Realizing this promise, however, raises daunting challenges. In \nparticular, we need AI systems that make timely and safe decisions in \nunpredictable environments, that are robust against sophisticated adversaries, \nand that can process ever increasing amounts of data across organizations and \nindividuals without compromising confidentiality. These challenges will be \nexacerbated by the end of the Moore's Law, which will constrain the amount of \ndata these technologies can store and process. In this paper, we propose \nseveral open research directions in systems, architectures, and security that \ncan address these challenges and help unlock AI's potential to improve lives \nand society. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e6f1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05855"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, William Paul, Michael I. Jordan, Ion Stoica", "title": "Ray: A Distributed Framework for Emerging AI Applications. (arXiv:1712.05889v1 [cs.DC])", "alternate": [{"href": "http://arxiv.org/abs/1712.05889", "type": "text/html"}], "timestampUsec": "1513676967346934", "comments": [], "summary": {"content": "<p>The next generation of AI applications will continuously interact with the \nenvironment and learn from these interactions. These applications impose new \nand demanding systems requirements, both in terms of performance and \nflexibility. In this paper, we consider these requirements and present Ray---a \ndistributed system to address them. Ray implements a dynamic task graph \ncomputation model that supports both the task-parallel and the actor \nprogramming models. To meet the performance requirements of AI applications, we \npropose an architecture that logically centralizes the system's control state \nusing a sharded storage system and a novel bottom-up distributed scheduler. In \nour experiments, we demonstrate sub-millisecond remote task latencies and \nlinear throughput scaling beyond 1.8 million tasks per second. We empirically \nvalidate that Ray speeds up challenging benchmarks and serves as both a natural \nand performant fit for an emerging class of reinforcement learning applications \nand algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e6f5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05889"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Bhavya Kailkhura, Jayaraman J. Thiagarajan, Charvi Rastogi, Pramod K. Varshney, Peer-Timo Bremer", "title": "A Spectral Approach for the Design of Experiments: Design, Analysis and Algorithms. (arXiv:1712.06028v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06028", "type": "text/html"}], "timestampUsec": "1513676967346933", "comments": [], "summary": {"content": "<p>This paper proposes a new approach to construct high quality space-filling \nsample designs. First, we propose a novel technique to quantify the \nspace-filling property and optimally trade-off uniformity and randomness in \nsample designs in arbitrary dimensions. Second, we connect the proposed metric \n(defined in the spatial domain) to the objective measure of the design \nperformance (defined in the spectral domain). This connection serves as an \nanalytic framework for evaluating the qualitative properties of space-filling \ndesigns in general. Using the theoretical insights provided by this \nspatial-spectral analysis, we derive the notion of optimal space-filling \ndesigns, which we refer to as space-filling spectral designs. Third, we propose \nan efficient estimator to evaluate the space-filling properties of sample \ndesigns in arbitrary dimensions and use it to develop an optimization framework \nto generate high quality space-filling designs. Finally, we carry out a \ndetailed performance comparison on two different applications in 2 to 6 \ndimensions: a) image reconstruction and b) surrogate modeling on several \nbenchmark optimization functions and an inertial confinement fusion (ICF) \nsimulation code. We demonstrate that the propose spectral designs significantly \noutperform existing approaches especially in high dimensions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e700", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06028"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yeo Hun Yoon, Shujaat Khan, Jaeyoung Huh, Jong Chul Ye", "title": "Deep Learning in RF Sub-sampled B-mode Ultrasound Imaging. (arXiv:1712.06096v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.06096", "type": "text/html"}], "timestampUsec": "1513676967346932", "comments": [], "summary": {"content": "<p>In portable, three dimensional, and ultra-fast ultrasound (US) imaging \nsystems, there is an increasing need to reconstruct high quality images from a \nlimited number of RF data from receiver (Rx) or scan-line (SC) sub-sampling. \nHowever, due to the severe side lobe artifacts from RF sub-sampling, the \nstandard beam-former often produces blurry images with less contrast that are \nnot suitable for diagnostic purpose. To address this problem, some researchers \nhave studied compressed sensing (CS) to exploit the sparsity of the image or RF \ndata in some domains. However, the existing CS approaches require either \nhardware changes or computationally expensive algorithms. To overcome these \nlimitations, here we propose a novel deep learning approach that directly \ninterpolates the missing RF data by utilizing redundancy in the Rx-SC plane. In \nparticular, the network design principle derives from a novel interpretation of \nthe deep neural network as a cascaded convolution framelets that learns the \ndata-driven bases for Hankel matrix decomposition. Our extensive experimental \nresults from sub-sampled RF data from a real US system confirmed that the \nproposed method can effectively reduce the data rate without sacrificing the \nimage quality. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e715", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06096"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jin-Hwa Kim, Byoung-Tak Zhang", "title": "Visual Explanations from Hadamard Product in Multimodal Deep Networks. (arXiv:1712.06228v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.06228", "type": "text/html"}], "timestampUsec": "1513676967346931", "comments": [], "summary": {"content": "<p>The visual explanation of learned representation of models helps to \nunderstand the fundamentals of learning. The attentional models of previous \nworks used to visualize the attended regions over an image or text using their \nlearned weights to confirm their intended mechanism. Kim et al. (2016) show \nthat the Hadamard product in multimodal deep networks, which is well-known for \nthe joint function of visual question answering tasks, implicitly performs an \nattentional mechanism for visual inputs. In this work, we extend their work to \nshow that the Hadamard product in multimodal deep networks performs not only \nfor visual inputs but also for textual inputs simultaneously using the proposed \ngradient-based visualization technique. The attentional effect of Hadamard \nproduct is visualized for both visual and textual inputs by analyzing the two \ninputs and an output of the Hadamard product with the proposed method and \ncompared with learned attentional weights of a visual question answering model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e726", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06228"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jose Oramas, Kaili Wang, Tinne Tuytelaars", "title": "Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks. (arXiv:1712.06302v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.06302", "type": "text/html"}], "timestampUsec": "1513676967346930", "comments": [], "summary": {"content": "<p>Learning-based representations have become the defacto means to address \ncomputer vision tasks. Despite their massive adoption, the amount of work \naiming at understanding the internal representations learned by these models is \nrather limited. Existing methods aimed at model interpretation either require \nexhaustive manual inspection of visualizations, or link internal network \nactivations with external \"possibly useful\" annotated concepts. We propose an \nintermediate scheme in which, given a pretrained model, we automatically \nidentify internal features relevant for the set of classes considered by the \nmodel, without requiring additional annotations. We interpret the model through \naverage visualizations of these features. Then, at test time, we explain the \nnetwork prediction by accompanying the predicted class label with supporting \nheatmap visualizations derived from the identified relevant features. In \naddition, we propose a method to address the artifacts introduced by strided \noperations in deconvnet-based visualizations. Our evaluation on the MNIST, \nILSVRC 12 and Fashion 144k datasets quantitatively shows that the proposed \nmethod is able to identify relevant internal features for the classes of \ninterest while improving the quality of the produced visualizations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e736", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06302"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Stuart Armstrong", "title": "`Indifference' methods for managing agent rewards. (arXiv:1712.06365v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.06365", "type": "text/html"}], "timestampUsec": "1513676967346929", "comments": [], "summary": {"content": "<p>Indifference is a class of methods that are used to control a reward based \nagent, by, for example, safely changing their reward or policy, or making the \nagent behave as if a certain outcome could never happen. These methods of \ncontrol work even if the implications of the agent's reward are otherwise not \nfully understood. Though they all come out of similar ideas, indifference \ntechniques can be classified as way of achieving one or more of three distinct \ngoals: rewards dependent on certain events (with no motivation for the agent to \nmanipulate the probability of those events), effective disbelief that an event \nwill ever occur, and seamless transition from one behaviour to another. There \nare five basic methods to achieve these three goals. This paper classifies and \nanalyses these methods on POMDPs (though the methods are highly portable to \nother agent designs), and establishes their uses, strengths, and limitations. \nIt aims to make the tools of indifference generally accessible and usable to \nagent designers. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e73e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06365"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Feng Liu, Yong Shi, Ying Liu", "title": "Three IQs of AI Systems and their Testing Methods. (arXiv:1712.06440v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.06440", "type": "text/html"}], "timestampUsec": "1513676967346928", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329ee5982\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329ee5982&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The rapid development of artificial intelligence has brought the artificial \nintelligence threat theory as well as the problem about how to evaluate the \nintelligence level of intelligent products. Both need to find a quantitative \nmethod to evaluate the intelligence level of intelligence systems, including \nhuman intelligence. Based on the standard intelligence system and the extended \nVon Neumann architecture, this paper proposes General IQ, Service IQ and Value \nIQ evaluation methods for intelligence systems, depending on different \nevaluation purposes. Among them, the General IQ of intelligence systems is to \nanswer the question of whether the artificial intelligence can surpass the \nhuman intelligence, which is reflected in putting the intelligence systems on \nan equal status and conducting the unified evaluation. The Service IQ and Value \nIQ of intelligence systems are used to answer the question of how the \nintelligent products can better serve the human, reflecting the intelligence \nand required cost of each intelligence system as a product in the process of \nserving human. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e74a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06440"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O. Stanley, Jeff Clune", "title": "Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents. (arXiv:1712.06560v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.06560", "type": "text/html"}], "timestampUsec": "1513676967346927", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329f4dbbf\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329f4dbbf&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Evolution strategies (ES) are a family of black-box optimization algorithms \nable to train deep neural networks roughly as well as Q-learning and policy \ngradient methods on challenging deep reinforcement learning (RL) problems, but \nare much faster (e.g. hours vs. days) because they parallelize better. However, \nmany RL problems require directed exploration because they have reward \nfunctions that are sparse or deceptive (i.e. contain local optima), and it is \nnot known how to encourage such exploration with ES. Here we show that \nalgorithms that have been invented to promote directed exploration in \nsmall-scale evolved neural networks via populations of exploring agents, \nspecifically novelty search (NS) and quality diversity (QD) algorithms, can be \nhybridized with ES to improve its performance on sparse or deceptive deep RL \ntasks, while retaining scalability. Our experiments confirm that the resultant \nnew algorithms, NS-ES and a version of QD we call NSR-ES, avoid local optima \nencountered by ES to achieve higher performance on tasks ranging from playing \nAtari to simulated robots learning to walk around a deceptive trap. This paper \nthus introduces a family of fast, scalable algorithms for reinforcement \nlearning that are capable of directed exploration. It also adds this new family \nof exploration algorithms to the RL toolbox and raises the interesting \npossibility that analogous algorithms with multiple simultaneous paths of \nexploration might also combine well with existing RL algorithms outside ES. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e754", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06560"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Maxim Naumov", "title": "Parallel Complexity of Forward and Backward Propagation. (arXiv:1712.06577v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.06577", "type": "text/html"}], "timestampUsec": "1513676967346926", "comments": [], "summary": {"content": "<p>We show that the forward and backward propagation can be formulated as a \nsolution of lower and upper triangular systems of equations. For standard \nfeedforward (FNNs) and recurrent neural networks (RNNs) the triangular systems \nare always block bi-diagonal, while for a general computation graph (directed \nacyclic graph) they can have a more complex triangular sparsity pattern. We \ndiscuss direct and iterative parallel algorithms that can be used for their \nsolution and interpreted as different ways of performing model parallelism. \nAlso, we show that for FNNs and RNNs with $k$ layers and $\\tau$ time steps the \nbackward propagation can be performed in parallel in O($\\log k$) and O($\\log k \n\\log \\tau$) steps, respectively. Finally, we outline the generalization of this \ntechnique using Jacobians that potentially allows us to handle arbitrary \nlayers. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e761", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06577"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Supratik Paul, Konstantinos Chatzilygeroudis, Kamil Ciosek, Jean-Baptiste Mouret, Michael A. Osborne, Shimon Whiteson", "title": "Alternating Optimisation and Quadrature for Robust Control. (arXiv:1605.07496v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1605.07496", "type": "text/html"}], "timestampUsec": "1513676967346925", "comments": [], "summary": {"content": "<p>Bayesian optimisation has been successfully applied to a variety of \nreinforcement learning problems. However, the traditional approach for learning \noptimal policies in simulators does not utilise the opportunity to improve \nlearning by adjusting certain environment variables: state features that are \nunobservable and randomly determined by the environment in a physical setting \nbut are controllable in a simulator. This paper considers the problem of \nfinding a robust policy while taking into account the impact of environment \nvariables. We present Alternating Optimisation and Quadrature (ALOQ), which \nuses Bayesian optimisation and Bayesian quadrature to address such settings. \nALOQ is robust to the presence of significant rare events, which may not be \nobservable under random sampling, but play a substantial role in determining \nthe optimal policy. Experimental results across different domains show that \nALOQ can learn more efficiently and robustly than existing methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e775", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1605.07496"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Han Xiao", "title": "KSR: A Semantic Representation of Knowledge Graph within a Novel Unsupervised Paradigm. (arXiv:1608.07685v6 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1608.07685", "type": "text/html"}], "timestampUsec": "1513676967346924", "comments": [], "summary": {"content": "<p>Knowledge representation is a long-history topic in AI, which is very \nimportant. A variety of models have been proposed for knowledge graph \nembedding, which projects symbolic entities and relations into continuous \nvector space. However, most related methods merely focus on the data-fitting of \nknowledge graph, and ignore the interpretable semantic expression. Thus, \ntraditional embedding methods are not friendly for applications that require \nsemantic analysis, such as question answering and entity retrieval. To this \nend, this paper proposes a semantic representation method for knowledge graph \n\\textbf{(KSR)}, which imposes a two-level hierarchical generative process that \nglobally extracts many aspects and then locally assigns a specific category in \neach aspect for every triple. Since both aspects and categories are \nsemantics-relevant, the collection of categories in each aspect is treated as \nthe semantic representation of this triple. Extensive experiments show that our \nmodel outperforms other state-of-the-art baselines substantially. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e787", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1608.07685"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Shai Shalev-Shwartz, Shaked Shammah, Amnon Shashua", "title": "On a Formal Model of Safe and Scalable Self-driving Cars. (arXiv:1708.06374v4 [cs.RO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.06374", "type": "text/html"}], "timestampUsec": "1513676967346923", "comments": [], "summary": {"content": "<p>In recent years, car makers and tech companies have been racing towards self \ndriving cars. It seems that the main parameter in this race is who will have \nthe first car on the road. The goal of this paper is to add to the equation two \nadditional crucial parameters. The first is standardization of safety assurance \n--- what are the minimal requirements that every self-driving car must satisfy, \nand how can we verify these requirements. The second parameter is scalability \n--- engineering solutions that lead to unleashed costs will not scale to \nmillions of cars, which will push interest in this field into a niche academic \ncorner, and drive the entire field into a \"winter of autonomous driving\". In \nthe first part of the paper we propose a white-box, interpretable, mathematical \nmodel for safety assurance, which we call Responsibility-Sensitive Safety \n(RSS). In the second part we describe a design of a system that adheres to our \nsafety assurance requirements and is scalable to millions of cars. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e79e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.06374"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yujian Li", "title": "Can Machines Think in Radio Language?. (arXiv:1710.02648v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.02648", "type": "text/html"}], "timestampUsec": "1513676967346922", "comments": [], "summary": {"content": "<p>People can think in auditory, visual and tactile forms of language, so can \nmachines principally. But is it possible for them to think in radio language? \nAccording to a first principle presented for general intelligence, i.e. the \nprinciple of language's relativity, the answer may give an exceptional solution \nfor robot astronauts to talk with each other in space exploration. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e7ac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.02648"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "An Bian, Kfir Y. Levy, Andreas Krause, Joachim M. Buhmann", "title": "Continuous DR-submodular Maximization: Structure and Algorithms. (arXiv:1711.02515v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.02515", "type": "text/html"}], "timestampUsec": "1513676967346921", "comments": [], "summary": {"content": "<p>DR-submodular continuous functions are important objectives with wide \nreal-world applications spanning MAP inference in determinantal point processes \n(DPPs), and mean-field inference for probabilistic submodular models, amongst \nothers. DR-submodularity captures a subclass of non-convex functions that \nenables both exact minimization and approximate maximization in polynomial \ntime. \n</p> \n<p>In this work we study the problem of maximizing non-monotone DR-submodular \ncontinuous functions under general down-closed convex constraints. We start by \ninvestigating geometric properties that underlie such objectives, e.g., a \nstrong relation between (approximately) stationary points and global optimum is \nproved. These properties are then used to devise two optimization algorithms \nwith provable guarantees. Concretely, we first devise a \"two-phase\" algorithm \nwith $1/4$ approximation guarantee. This algorithm allows the use of existing \nmethods for finding (approximately) stationary points as a subroutine, thus, \nharnessing recent progress in non-convex optimization. Then we present a \nnon-monotone Frank-Wolfe variant with $1/e$ approximation guarantee and \nsublinear convergence rate. Finally, we extend our approach to a broader class \nof generalized DR-submodular continuous functions, which captures a wider \nspectrum of applications. Our theoretical findings are validated on synthetic \nand real-world problem instances. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e7c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.02515"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Avgoustinos Vouros, Tiago V. Gehring, Kinga Szydlowska, Artur Janusz, Mike Croucher, Katarzyna Lukasiuk, Witold Konopka, Carmen Sandi, Zehai Tu, Eleni Vasilaki", "title": "A generalised framework for detailed classification of swimming paths inside the Morris Water Maze. (arXiv:1711.07446v2 [q-bio.QM] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07446", "type": "text/html"}], "timestampUsec": "1513676967346920", "comments": [], "summary": {"content": "<p>The Morris Water Maze is commonly used in behavioural neuroscience for the \nstudy of spatial learning with rodents. Over the years, various methods of \nanalysing rodent data collected in this task have been proposed. These methods \nspan from classical performance measurements (e.g. escape latency, rodent \nspeed, quadrant preference) to more sophisticated methods of categorisation \nwhich classify the animal swimming path into behavioural classes known as \nstrategies. Classification techniques provide additional insight in relation to \nthe actual animal behaviours but still only a limited amount of studies utilise \nthem mainly because they highly depend on machine learning knowledge. We have \npreviously demonstrated that the animals implement various strategies and by \nclassifying whole trajectories can lead to the loss of important information. \nIn this work, we developed a generalised and robust classification methodology \nwhich implements majority voting to boost the classification performance and \nsuccessfully nullify the need of manual tuning. Based on this framework, we \nbuilt a complete software, capable of performing the full analysis described in \nthis paper. The software provides an easy to use graphical user interface (GUI) \nthrough which users can enter their trajectory data, segment and label them and \nfinally generate reports and figures of the results. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e7d0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07446"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michael Gastegger, Ludwig Schwiedrzik, Marius Bittermann, Florian Berzsenyi, Philipp Marquetand", "title": "WACSF - Weighted Atom-Centered Symmetry Functions as Descriptors in Machine Learning Potentials. (arXiv:1712.05861v1 [physics.chem-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.05861", "type": "text/html"}], "timestampUsec": "1513676967346919", "comments": [], "summary": {"content": "<p>We introduce weighted atom-centered symmetry functions (wACSFs) as \ndescriptors of a chemical system's geometry for use in the prediction of \nchemical properties such as enthalpies or potential energies via machine \nlearning. The wACSFs are based on conventional atom-centered symmetry functions \n(ACSFs) but overcome the undesirable scaling of the latter with increasing \nnumber of different elements in a chemical system. The performance of these two \ndescriptors is compared using them as inputs in high-dimensional neural network \npotentials (HDNNPs), employing the molecular structures and associated \nenthalpies of the 133855 molecules containing up to five different elements \nreported in the QM9 database as reference data. A substantially smaller number \nof wACSFs than ACSFs is needed to obtain a comparable spatial resolution of the \nmolecular structures. At the same time, this smaller set of wACSFs leads to \nsignificantly better generalization performance in the machine learning \npotential than the large set of conventional ACSFs. Furthermore, we show that \nthe intrinsic parameters of the descriptors can in principle be optimized with \na genetic algorithm in a highly automated manner. For the wACSFs employed here, \nwe find however that using a simple empirical parametrization scheme is \nsufficient in order to obtain HDNNPs with high accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e7e1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05861"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Amir Karami", "title": "Taming Wild High Dimensional Text Data with a Fuzzy Lash. (arXiv:1712.05997v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.05997", "type": "text/html"}], "timestampUsec": "1513676967346918", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329f4ddd9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329f4ddd9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The bag of words (BOW) represents a corpus in a matrix whose elements are the \nfrequency of words. However, each row in the matrix is a very high-dimensional \nsparse vector. Dimension reduction (DR) is a popular method to address sparsity \nand high-dimensionality issues. Among different strategies to develop DR \nmethod, Unsupervised Feature Transformation (UFT) is a popular strategy to map \nall words on a new basis to represent BOW. The recent increase of text data and \nits challenges imply that DR area still needs new perspectives. Although a wide \nrange of methods based on the UFT strategy has been developed, the fuzzy \napproach has not been considered for DR based on this strategy. This research \ninvestigates the application of fuzzy clustering as a DR method based on the \nUFT strategy to collapse BOW matrix to provide a lower-dimensional \nrepresentation of documents instead of the words in a corpus. The quantitative \nevaluation shows that fuzzy clustering produces superior performance and \nfeatures to Principal Components Analysis (PCA) and Singular Value \nDecomposition (SVD), two popular DR methods based on the UFT strategy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e7ee", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05997"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Julio Amador, Axel Oehmichen, Miguel Molina-Solana", "title": "Characterizing Political Fake News in Twitter by its Meta-Data. (arXiv:1712.05999v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.05999", "type": "text/html"}], "timestampUsec": "1513676967346917", "comments": [], "summary": {"content": "<p>This article presents a preliminary approach towards characterizing political \nfake news on Twitter through the analysis of their meta-data. In particular, we \nfocus on more than 1.5M tweets collected on the day of the election of Donald \nTrump as 45th president of the United States of America. We use the meta-data \nembedded within those tweets in order to look for differences between tweets \ncontaining fake news and tweets not containing them. Specifically, we perform \nour analysis only on tweets that went viral, by studying proxies for users' \nexposure to the tweets, by characterizing accounts spreading fake news, and by \nlooking at their polarization. We found significant differences on the \ndistribution of followers, the number of URLs on tweets, and the verification \nof the users. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e7f9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05999"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Aditya Devarakonda, Kimon Fountoulakis, James Demmel, Michael W. Mahoney", "title": "Avoiding Synchronization in First-Order Methods for Sparse Convex Optimization. (arXiv:1712.06047v1 [cs.DC])", "alternate": [{"href": "http://arxiv.org/abs/1712.06047", "type": "text/html"}], "timestampUsec": "1513676967346916", "comments": [], "summary": {"content": "<p>Parallel computing has played an important role in speeding up convex \noptimization methods for big data analytics and large-scale machine learning \n(ML). However, the scalability of these optimization methods is inhibited by \nthe cost of communicating and synchronizing processors in a parallel setting. \nIterative ML methods are particularly sensitive to communication cost since \nthey often require communication every iteration. In this work, we extend \nwell-known techniques from Communication-Avoiding Krylov subspace methods to \nfirst-order, block coordinate descent methods for Support Vector Machines and \nProximal Least-Squares problems. Our Synchronization-Avoiding (SA) variants \nreduce the latency cost by a tunable factor of $s$ at the expense of a factor \nof $s$ increase in flops and bandwidth costs. We show that the SA-variants are \nnumerically stable and can attain large speedups of up to $5.1\\times$ on a Cray \nXC30 supercomputer. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e805", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06047"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Praneeth Narayanamurthy, Namrata Vaswani", "title": "MEDRoP: Memory-Efficient Dynamic Robust PCA. (arXiv:1712.06061v1 [cs.IT])", "alternate": [{"href": "http://arxiv.org/abs/1712.06061", "type": "text/html"}], "timestampUsec": "1513676967346915", "comments": [], "summary": {"content": "<p>Robust PCA (RPCA) is the problem of separating a given data matrix into the \nsum of a sparse matrix and a low-rank matrix. The column span of the low-rank \nmatrix gives the PCA solution. Dynamic RPCA is the time-varying extension of \nRPCA. It assumes that the true data vectors lie in a low-dimensional subspace \nthat can change with time, albeit slowly. The goal is to track this changing \nsubspace over time in the presence of sparse outliers. We propose an algorithm \nthat we call Memory-Efficient Dynamic Robust PCA (MEDRoP). This relies on the \nrecently studied recursive projected compressive sensing (ReProCS) framework \nfor solving dynamic RPCA problems, however, the actual algorithm is \nsignificantly different from, and simpler than, previous ReProCS-based methods. \nThe main contribution of this work is a theoretical guarantee that MEDRoP \nprovably solves dynamic RPCA under weakened versions of standard RPCA \nassumptions, a mild assumption on slow subspace change, and two simple \nassumptions (a lower bound on most outlier magnitudes and mutual independence \nof the true data vectors). Our result is important because (i) it removes the \nstrong assumptions needed by the three previous complete guarantees for \nReProCS-based algorithms; (ii) it shows that, it is possible to achieve \nsignificantly improved outlier tolerance compared to static RPCA solutions by \nexploiting slow subspace change and a lower bound on most outlier magnitudes; \n(iii) it is able to track a changed subspace within a delay that is more than \nthe subspace dimension by only logarithmic factors and thus is near-optimal; \nand (iv) it studies an algorithm that is online (after initialization), fast, \nand, memory-efficient (its memory complexity is within logarithmic factors of \nthe optimal). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e80b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06061"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sivaraman Balakrishnan, Larry Wasserman", "title": "Hypothesis Testing for High-Dimensional Multinomials: A Selective Review. (arXiv:1712.06120v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06120", "type": "text/html"}], "timestampUsec": "1513676967346914", "comments": [], "summary": {"content": "<p>The statistical analysis of discrete data has been the subject of extensive \nstatistical research dating back to the work of Pearson. In this survey we \nreview some recently developed methods for testing hypotheses about \nhigh-dimensional multinomials. Traditional tests like the $\\chi^2$ test and the \nlikelihood ratio test can have poor power in the high-dimensional setting. Much \nof the research in this area has focused on finding tests with asymptotically \nNormal limits and developing (stringent) conditions under which tests have \nNormal limits. We argue that this perspective suffers from a significant \ndeficiency: it can exclude many high-dimensional cases when - despite having \nnon Normal null distributions - carefully designed tests can have high power. \nFinally, we illustrate that taking a minimax perspective and considering \nrefinements of this perspective can lead naturally to powerful and practical \ntests. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e816", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06120"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Nathan Killoran, Leo J. Lee, Andrew Delong, David Duvenaud, Brendan J. Frey", "title": "Generating and designing DNA with deep generative models. (arXiv:1712.06148v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.06148", "type": "text/html"}], "timestampUsec": "1513676967346913", "comments": [], "summary": {"content": "<p>We propose generative neural network methods to generate DNA sequences and \ntune them to have desired properties. We present three approaches: creating \nsynthetic DNA sequences using a generative adversarial network; a DNA-based \nvariant of the activation maximization (\"deep dream\") design method; and a \njoint procedure which combines these two approaches together. We show that \nthese tools capture important structures of the data and, when applied to \ndesigning probes for protein binding microarrays, allow us to generate new \nsequences whose properties are estimated to be superior to those found in the \ntraining data. We believe that these results open the door for applying deep \ngenerative models to advance genomics research. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e81d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06148"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kuang Gong, Jaewon Yang, Kyungsang Kim, Georges El Fakhri, Youngho Seo, Quanzheng Li", "title": "Attenuation Correction for Brain PET imaging using Deep Neural Network based on Dixon and ZTE MR images. (arXiv:1712.06203v1 [physics.med-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.06203", "type": "text/html"}], "timestampUsec": "1513676967346912", "comments": [], "summary": {"content": "<p>Positron Emission Tomography (PET) is a functional imaging modality widely \nused in neuroscience studies. To obtain meaningful quantitative results from \nPET images, attenuation correction is necessary during image reconstruction. \nFor PET/MR hybrid systems, PET attenuation is challenging as Magnetic Resonance \n(MR) images do not reflect attenuation coefficients directly. To address this \nissue, we present deep neural network methods to derive the continuous \nattenuation coefficients for brain PET imaging from MR images. With only Dixon \nMR images as the network input, the existing U-net structure was adopted and \nanalysis using forty patient data sets shows it is superior than other Dixon \nbased methods. When both Dixon and zero echo time (ZTE) images are available, \napart from stacking multiple MR images along the U-net input channels, we have \nproposed a new network structure to extract the features from Dixon and ZTE \nimages independently at early layers and combine them together at later layers. \nQuantitative analysis based on fourteen real patient data sets demonstrates \nthat both network approaches can perform better than the standard methods, and \nthe proposed network structure can further reduce the PET quantification error \ncompared to the U-net structure with multiple inputs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e82b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06203"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Anna Little, Mauro Maggioni, James M. Murphy", "title": "Path-Based Spectral Clustering: Guarantees, Robustness to Outliers, and Fast Algorithms. (arXiv:1712.06206v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06206", "type": "text/html"}], "timestampUsec": "1513676967346911", "comments": [], "summary": {"content": "<p>We consider the problem of clustering with the longest leg path distance \n(LLPD) metric, which is informative for elongated and irregularly shaped \nclusters. We prove finite-sample guarantees on the performance of clustering \nwith respect to this metric when random samples are drawn from multiple \nintrinsically low-dimensional clusters in high-dimensional space, in the \npresence of a large number of high-dimensional outliers. By combining these \nresults with spectral clustering with respect to LLPD, we provide conditions \nunder which the eigengap statistic correctly determines the number of clusters \nfor a large class of data sets, and prove guarantees on the number of points \nmislabeled by the proposed algorithm. Our methods are quite general and provide \nperformance guarantees for spectral clustering with any ultrametric. We also \nintroduce an efficient approximation algorithm, easy to implement, for the \nLLPD, based on a multiscale analysis of adjacency graphs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e835", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06206"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Cameron Carlin, Long Van Ho, David Ledbetter, Melissa Aczon, Randall Wetzel", "title": "Predicting Individual Physiologically Acceptable States for Discharge from a Pediatric Intensive Care Unit. (arXiv:1712.06214v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06214", "type": "text/html"}], "timestampUsec": "1513676967346910", "comments": [], "summary": {"content": "<p>Objective: Predict patient-specific vitals deemed medically acceptable for \ndischarge from a pediatric intensive care unit (ICU). Design: The means of each \npatient's hr, sbp and dbp measurements between their medical and physical \ndischarge from the ICU were computed as a proxy for their physiologically \nacceptable state space (PASS) for successful ICU discharge. These individual \nPASS values were compared via root mean squared error (rMSE) to population \nage-normal vitals, a polynomial regression through the PASS values of a \nPediatric ICU (PICU) population and predictions from two recurrent neural \nnetwork models designed to predict personalized PASS within the first twelve \nhours following ICU admission. Setting: PICU at Children's Hospital Los Angeles \n(CHLA). Patients: 6,899 PICU episodes (5,464 patients) collected between 2009 \nand 2016. Interventions: None. Measurements: Each episode data contained 375 \nvariables representing vitals, labs, interventions, and drugs. They also \nincluded a time indicator for PICU medical discharge and physical discharge. \nMain Results: The rMSEs between individual PASS values and population \nage-normals (hr: 25.9 bpm, sbp: 13.4 mmHg, dbp: 13.0 mmHg) were larger than the \nrMSEs corresponding to the polynomial regression (hr: 19.1 bpm, sbp: 12.3 mmHg, \ndbp: 10.8 mmHg). The rMSEs from the best performing RNN model were the lowest \n(hr: 16.4 bpm; sbp: 9.9 mmHg, dbp: 9.0 mmHg). Conclusion: PICU patients are a \nunique subset of the general population, and general age-normal vitals may not \nbe suitable as target values indicating physiologic stability at discharge. \nAge-normal vitals that were specifically derived from the medical-to-physical \ndischarge window of ICU patients may be more appropriate targets for \n'acceptable' physiologic state for critical care patients. Going beyond simple \nage bins, an RNN model can provide more personalized target values. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e840", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06214"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Brian E. Moore, Chen Gao, Raj Rao Nadakuditi", "title": "Panoramic Robust PCA for Foreground-Background Separation on Noisy, Free-Motion Camera Video. (arXiv:1712.06229v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06229", "type": "text/html"}], "timestampUsec": "1513676967346909", "comments": [], "summary": {"content": "<p>This work presents a novel approach for robust PCA with total variation \nregularization for foreground-background separation and denoising on noisy, \nmoving camera video. Our proposed algorithm registers the raw (possibly \ncorrupted) frames of a video and then jointly processes the registered frames \nto produce a decomposition of the scene into a low-rank background component \nthat captures the static components of the scene, a smooth foreground component \nthat captures the dynamic components of the scene, and a sparse component that \nisolates corruptions. Unlike existing methods, our proposed algorithm produces \na panoramic low-rank component that spans the entire field of view, \nautomatically stitching together corrupted data from partially overlapping \nscenes. The low-rank portion of our robust PCA model is based on a recently \ndiscovered optimal low-rank matrix estimator (OptShrink) that requires no \nparameter tuning. We demonstrate the performance of our algorithm on both \nstatic and moving camera videos corrupted by noise and outliers. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e846", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06229"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zhuoran Yang, Lin F. Yang, Ethan X. Fang, Tuo Zhao, Zhaoran Wang, Matey Neykov", "title": "Misspecified Nonconvex Statistical Optimization for Phase Retrieval. (arXiv:1712.06245v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06245", "type": "text/html"}], "timestampUsec": "1513676967346908", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329f4dfa9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329f4dfa9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Existing nonconvex statistical optimization theory and methods crucially rely \non the correct specification of the underlying \"true\" statistical models. To \naddress this issue, we take a first step towards taming model misspecification \nby studying the high-dimensional sparse phase retrieval problem with \nmisspecified link functions. In particular, we propose a simple variant of the \nthresholded Wirtinger flow algorithm that, given a proper initialization, \nlinearly converges to an estimator with optimal statistical accuracy for a \nbroad family of unknown link functions. We further provide extensive numerical \nexperiments to support our theoretical findings. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e853", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06245"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Takashi Matsubara, Tetsuo Tashiro, Kuniaki Uehara", "title": "Deep Neural Generative Model of Functional MRI Images for Psychiatric Disorder Diagnosis. (arXiv:1712.06260v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06260", "type": "text/html"}], "timestampUsec": "1513676967346907", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329fbc549\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329fbc549&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Accurate diagnosis of psychiatric disorders plays a critical role in \nimproving quality of life for patients and potentially supports the development \nof new treatments. Many studies have been conducted on machine learning \ntechniques that seek brain imaging data for specific biomarkers of disorders. \nThese studies have encountered the following dilemma: An end-to-end \nclassification overfits to a small number of high-dimensional samples but \nunsupervised feature-extraction has the risk of extracting a signal of no \ninterest. In addition, such studies often provided only diagnoses for patients \nwithout presenting the reasons for these diagnoses. This study proposed a deep \nneural generative model of resting-state functional magnetic resonance imaging \n(fMRI) data. The proposed model is conditioned by the assumption of the \nsubject's state and estimates the posterior probability of the subject's state \ngiven the imaging data, using Bayes' rule. This study applied the proposed \nmodel to diagnose schizophrenia and bipolar disorders. Diagnosis accuracy was \nimproved by a large margin over competitive approaches, namely a support vector \nmachine, logistic regression, and multilayer perceptron with or without \nunsupervised feature-extractors in addition to a Gaussian mixture model. The \nproposed model visualizes brain regions largely related to the disorders, thus \nmotivating further biological investigation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e868", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06260"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Adam J. Riesselman, John B. Ingraham, Debora S. Marks", "title": "Deep generative models of genetic variation capture mutation effects. (arXiv:1712.06527v1 [q-bio.QM])", "alternate": [{"href": "http://arxiv.org/abs/1712.06527", "type": "text/html"}], "timestampUsec": "1513676967346906", "comments": [], "summary": {"content": "<p>The functions of proteins and RNAs are determined by a myriad of interactions \nbetween their constituent residues, but most quantitative models of how \nmolecular phenotype depends on genotype must approximate this by simple \nadditive effects. While recent models have relaxed this constraint to also \naccount for pairwise interactions, these approaches do not provide a tractable \npath towards modeling higher-order dependencies. Here, we show how latent \nvariable models with nonlinear dependencies can be applied to capture \nbeyond-pairwise constraints in biomolecules. We present a new probabilistic \nmodel for sequence families, DeepSequence, that can predict the effects of \nmutations across a variety of deep mutational scanning experiments \nsignificantly better than site independent or pairwise models that are based on \nthe same evolutionary data. The model, learned in an unsupervised manner solely \nfrom sequence information, is grounded with biologically motivated priors, \nreveals latent organization of sequence families, and can be used to \nextrapolate to new parts of sequence space \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e871", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06527"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Gian-Andrea Thanei, Nicolai Meinshausen, Rajen D. Shah", "title": "The xyz algorithm for fast interaction search in high-dimensional data. (arXiv:1610.05108v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.05108", "type": "text/html"}], "timestampUsec": "1513676967346905", "comments": [], "summary": {"content": "<p>When performing regression on a dataset with $p$ variables, it is often of \ninterest to go beyond using main linear effects and include interactions as \nproducts between individual variables. For small-scale problems, these \ninteractions can be computed explicitly but this leads to a computational \ncomplexity of at least $\\mathcal{O}(p^2)$ if done naively. This cost can be \nprohibitive if $p$ is very large. We introduce a new randomised algorithm that \nis able to discover interactions with high probability and under mild \nconditions has a runtime that is subquadratic in $p$. We show that strong \ninteractions can be discovered in almost linear time, whilst finding weaker \ninteractions requires $\\mathcal{O}(p^\\alpha)$ operations for $1 &lt; \\alpha &lt; 2$ \ndepending on their strength. The underlying idea is to transform interaction \nsearch into a closestpair problem which can be solved efficiently in \nsubquadratic time. The algorithm is called $\\mathit{xyz}$ and is implemented in \nthe language R. We demonstrate its efficiency for application to genome-wide \nassociation studies, where more than $10^{11}$ interactions can be screened in \nunder $280$ seconds with a single-core $1.2$ GHz CPU. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e878", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.05108"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alexey Zaytsev, Evgeny Burnaev", "title": "Minimax Error of Interpolation and Optimal Design of Experiments for Variable Fidelity Data. (arXiv:1610.06731v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.06731", "type": "text/html"}], "timestampUsec": "1513676967346904", "comments": [], "summary": {"content": "<p>Engineering problems often involve data sources of variable fidelity with \ndifferent costs of obtaining an observation. In particular, one can use both a \ncheap low fidelity function (e.g. a computational experiment with a CFD code) \nand an expensive high fidelity function (e.g. a wind tunnel experiment) to \ngenerate a data sample in order to construct a regression model of a high \nfidelity function. The key question in this setting is how the sizes of the \nhigh and low fidelity data samples should be selected in order to stay within a \ngiven computational budget and maximize accuracy of the regression model prior \nto committing resources on data acquisition. \n</p> \n<p>In this paper we obtain minimax interpolation errors for single and variable \nfidelity scenarios for a multivariate Gaussian process regression. Evaluation \nof the minimax errors allows us to identify cases when the variable fidelity \ndata provides better interpolation accuracy than the exclusively high fidelity \ndata for the same computational budget. \n</p> \n<p>These results allow us to calculate the optimal shares of variable fidelity \ndata samples under the given computational budget constraint. Real and \nsynthetic data experiments suggest that using the obtained optimal shares often \noutperforms natural heuristics in terms of the regression accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e880", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.06731"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alexander Jung, Nguyen Tran Quang, Alexandru Mara", "title": "When is Network Lasso Accurate?. (arXiv:1704.02107v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.02107", "type": "text/html"}], "timestampUsec": "1513676967346903", "comments": [], "summary": {"content": "<p>The \"least absolute shrinkage and selection operator\" (Lasso) method has been \nadapted recently for networkstructured datasets. In particular, this network \nLasso method allows to learn graph signals from a small number of noisy signal \nsamples by using the total variation of a graph signal for regularization. \nWhile efficient and scalable implementations of the network Lasso are \navailable, only little is known about the conditions on the underlying network \nstructure which ensure network Lasso to be accurate. By leveraging concepts of \ncompressed sensing, we address this gap and derive precise conditions on the \nunderlying network topology and sampling set which guarantee the network Lasso \nfor a particular loss function to deliver an accurate estimate of the entire \nunderlying graph signal. We also quantify the error incurred by network Lasso \nin terms of two constants which reflect the connectivity of the sampled nodes. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e88a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.02107"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Anubhav Ashok, Nicholas Rhinehart, Fares Beainy, Kris M. Kitani", "title": "N2N Learning: Network to Network Compression via Policy Gradient Reinforcement Learning. (arXiv:1709.06030v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.06030", "type": "text/html"}], "timestampUsec": "1513676967346902", "comments": [], "summary": {"content": "<p>While bigger and deeper neural network architectures continue to advance the \nstate-of-the-art for many computer vision tasks, real-world adoption of these \nnetworks is impeded by hardware and speed constraints. Conventional model \ncompression methods attempt to address this problem by modifying the \narchitecture manually or using pre-defined heuristics. Since the space of all \nreduced architectures is very large, modifying the architecture of a deep \nneural network in this way is a difficult task. In this paper, we tackle this \nissue by introducing a principled method for learning reduced network \narchitectures in a data-driven way using reinforcement learning. Our approach \ntakes a larger `teacher' network as input and outputs a compressed `student' \nnetwork derived from the `teacher' network. In the first stage of our method, a \nrecurrent policy network aggressively removes layers from the large `teacher' \nmodel. In the second stage, another recurrent policy network carefully reduces \nthe size of each remaining layer. The resulting network is then evaluated to \nobtain a reward -- a score based on the accuracy and compression of the \nnetwork. Our approach uses this reward signal with policy gradients to train \nthe policies to find a locally optimal student network. Our experiments show \nthat we can achieve compression rates of more than 10x for models such as \nResNet-34 while maintaining similar performance to the input `teacher' network. \nWe also present a valuable transfer learning result which shows that policies \nwhich are pre-trained on smaller `teacher' networks can be used to rapidly \nspeed up training on larger `teacher' networks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e892", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.06030"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Been Kim, Justin Gilmer, Fernanda Viegas, Ulfar Erlingsson, Martin Wattenberg", "title": "TCAV: Relative concept importance testing with Linear Concept Activation Vectors. (arXiv:1711.11279v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.11279", "type": "text/html"}], "timestampUsec": "1513676967346901", "comments": [], "summary": {"content": "<p>Neural networks commonly offer high utility but remain difficult to \ninterpret. Developing methods to explain their decisions is challenging due to \ntheir large size, complex structure, and inscrutable internal representations. \nThis work argues that the language of explanations should be expanded from that \nof input features (e.g., assigning importance weightings to pixels) to include \nthat of higher-level, human-friendly concepts. For example, an understandable \nexplanation of why an image classifier outputs the label \"zebra\" would ideally \nrelate to concepts such as \"stripes\" rather than a set of particular pixel \nvalues. This paper introduces the \"concept activation vector\" (CAV) which \nallows quantitative analysis of a concept's relative importance to \nclassification, with a user-provided set of input data examples defining the \nconcept. CAVs may be easily used by non-experts, who need only provide \nexamples, and with CAVs the high-dimensional structure of neural networks turns \ninto an aid to interpretation, rather than an obstacle. Using the domain of \nimage classification as a testing ground, we describe how CAVs may be used to \ntest hypotheses about classifiers and also generate insights into the \ndeficiencies and correlations in training data. CAVs also provide us a directed \napproach to choose the combinations of neurons to visualize with the DeepDream \ntechnique, which traditionally has chosen neurons or linear combinations of \nneurons at random to visualize. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e89e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11279"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Casey Chu, Andrey Zhmoginov, Mark Sandler", "title": "CycleGAN, a Master of Steganography. (arXiv:1712.02950v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02950", "type": "text/html"}], "timestampUsec": "1513676967346900", "comments": [], "summary": {"content": "<p>CycleGAN (Zhu et al. 2017) is one recent successful approach to learn a \ntransformation between two image distributions. In a series of experiments, we \ndemonstrate an intriguing property of the model: CycleGAN learns to \"hide\" \ninformation about a source image into the images it generates in a nearly \nimperceptible, high-frequency signal. This trick ensures that the generator can \nrecover the original sample and thus satisfy the cyclic consistency \nrequirement, while the generated image remains realistic. We connect this \nphenomenon with adversarial attacks by viewing CycleGAN's training procedure as \ntraining a generator of adversarial examples and demonstrate that the cyclic \nconsistency loss causes CycleGAN to be especially vulnerable to adversarial \nattacks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e8a7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02950"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michael Gastegger, J&#xf6;rg Behler, Philipp Marquetand", "title": "Machine Learning Molecular Dynamics for the Simulation of Infrared Spectra. (arXiv:1705.05907v1 [physics.chem-ph] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1705.05907", "type": "text/html"}], "timestampUsec": "1513676967346899", "comments": [], "summary": {"content": "<p>Machine learning has emerged as an invaluable tool in many research areas. In \nthe present work, we harness this power to predict highly accurate molecular \ninfrared spectra with unprecedented computational efficiency. To account for \nvibrational anharmonic and dynamical effects -- typically neglected by \nconventional quantum chemistry approaches -- we base our machine learning \nstrategy on ab initio molecular dynamics simulations. While these simulations \nare usually extremely time consuming even for small molecules, we overcome \nthese limitations by leveraging the power of a variety of machine learning \ntechniques, not only accelerating simulations by several orders of magnitude, \nbut also greatly extending the size of systems that can be treated. To this \nend, we develop a molecular dipole moment model based on environment dependent \nneural network charges and combine it with the neural network potentials of \nBehler and Parrinello. Contrary to the prevalent big data philosophy, we are \nable to obtain very accurate machine learning models for the prediction of \ninfrared spectra based on only a few hundreds of electronic structure reference \npoints. This is made possible through the introduction of a fully automated \nsampling scheme and the use of molecular forces during neural network potential \ntraining. We demonstrate the power of our machine learning approach by applying \nit to model the infrared spectra of a methanol molecule, n-alkanes containing \nup to 200 atoms and the protonated alanine tripeptide, which at the same time \nrepresents the first application of machine learning techniques to simulate the \ndynamics of a peptide. In all these case studies we find excellent agreement \nbetween the infrared spectra predicted via machine learning models and the \nrespective theoretical and experimental spectra. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e8b1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.05907"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Vasily Morzhakov, Alexey Redozubov", "title": "An Artificial Neural Network Architecture Based on Context Transformations in Cortical Minicolumns. (arXiv:1712.05954v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.05954", "type": "text/html"}], "timestampUsec": "1513669229487545", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329fbc927\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329fbc927&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Cortical minicolumns are considered a model of cortical organization. Their \nfunction is still a source of research and not reflected properly in modern \narchitecture of nets in algorithms of Artificial Intelligence. We assume its \nfunction and describe it in this article. Furthermore, we show how this \nproposal allows to construct a new architecture, that is not based on \nconvolutional neural networks, test it on MNIST data and receive close to \nConvolutional Neural Network accuracy. We also show that the proposed \narchitecture possesses an ability to train on a small quantity of samples. To \nachieve these results, we enable the minicolumns to remember context \ntransformations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac3a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05954"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andres Felipe Cruz Salinas, Jonatan Gomez Perdomo", "title": "Self-adaptation of Genetic Operators Through Genetic Programming Techniques. (arXiv:1712.06070v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.06070", "type": "text/html"}], "timestampUsec": "1513669229487544", "comments": [], "summary": {"content": "<p>Here we propose an evolutionary algorithm that self modifies its operators at \nthe same time that candidate solutions are evolved. This tackles convergence \nand lack of diversity issues, leading to better solutions. Operators are \nrepresented as trees and are evolved using genetic programming (GP) techniques. \nThe proposed approach is tested with real benchmark functions and an analysis \nof operator evolution is provided. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac44", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06070"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Farhan Shafiq, Takato Yamada, Antonio T. Vilchez, Sakyasingha Dasgupta", "title": "Automated flow for compressing convolution neural networks for efficient edge-computation with FPGA. (arXiv:1712.06272v1 [cs.AR])", "alternate": [{"href": "http://arxiv.org/abs/1712.06272", "type": "text/html"}], "timestampUsec": "1513669229487543", "comments": [], "summary": {"content": "<p>Deep convolutional neural networks (CNN) based solutions are the current \nstate- of-the-art for computer vision tasks. Due to the large size of these \nmodels, they are typically run on clusters of CPUs or GPUs. However, power \nrequirements and cost budgets can be a major hindrance in adoption of CNN for \nIoT applications. Recent research highlights that CNN contain significant \nredundancy in their structure and can be quantized to lower bit-width \nparameters and activations, while maintaining acceptable accuracy. Low \nbit-width and especially single bit-width (binary) CNN are particularly \nsuitable for mobile applications based on FPGA implementation, due to the \nbitwise logic operations involved in binarized CNN. Moreover, the transition to \nlower bit-widths opens new avenues for performance optimizations and model \nimprovement. In this paper, we present an automatic flow from trained \nTensorFlow models to FPGA system on chip implementation of binarized CNN. This \nflow involves quantization of model parameters and activations, generation of \nnetwork and model in embedded-C, followed by automatic generation of the FPGA \naccelerator for binary convolutions. The automated flow is demonstrated through \nimplementation of binarized \"YOLOV2\" on the low cost, low power Cyclone- V FPGA \ndevice. Experiments on object detection using binarized YOLOV2 demonstrate \nsignificant performance benefit in terms of model size and inference speed on \nFPGA as compared to CPU and mobile CPU platforms. Furthermore, the entire \nautomated flow from trained models to FPGA synthesis can be completed within \none hour. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac48", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06272"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1516256208, "author": "Joel Lehman, Jay Chen, Jeff Clune, Kenneth O. Stanley", "title": "Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients. (arXiv:1712.06563v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.06563", "type": "text/html"}], "timestampUsec": "1513669229487542", "comments": [], "summary": {"content": "<p>While neuroevolution (evolving neural networks) has a successful track record \nacross a variety of domains from reinforcement learning to artificial life, it \nis rarely applied to large, deep neural networks. A central reason is that \nwhile random mutation generally works in low dimensions, a random perturbation \nof thousands or millions of weights is likely to break existing functionality, \nproviding no learning signal even if some individual weight changes were \nbeneficial. This paper proposes a solution by introducing a family of safe \nmutation (SM) operators that aim within the mutation operator itself to find a \ndegree of change that does not alter network behavior too much, but still \nfacilitates exploration. Importantly, these SM operators do not require any \nadditional interactions with the environment. The most effective SM variant \ncapitalizes on the intriguing opportunity to scale the degree of mutation of \neach individual weight according to the sensitivity of the network's outputs to \nthat weight, which requires computing the gradient of outputs with respect to \nthe weights (instead of the gradient of error, as in conventional deep \nlearning). This safe mutation through gradients (SM-G) operator dramatically \nincreases the ability of a simple genetic algorithm-based neuroevolution method \nto find solutions in high-dimensional domains that require deep and/or \nrecurrent neural networks (which tend to be particularly brittle to mutation), \nincluding domains that require processing raw pixels. By improving our ability \nto evolve deep neural networks, this new safer approach to mutation expands the \nscope of domains amenable to neuroevolution. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1516256208, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac4f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06563"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O. Stanley, Jeff Clune", "title": "Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning. (arXiv:1712.06567v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.06567", "type": "text/html"}], "timestampUsec": "1513669229487541", "comments": [], "summary": {"content": "<p>Deep artificial neural networks (DNNs) are typically trained via \ngradient-based learning algorithms, namely backpropagation. Evolution \nstrategies (ES) can rival backprop-based algorithms such as Q-learning and \npolicy gradients on challenging deep reinforcement learning (RL) problems. \nHowever, ES can be considered a gradient-based algorithm because it performs \nstochastic gradient descent via an operation similar to a finite-difference \napproximation of the gradient. That raises the question of whether \nnon-gradient-based evolutionary algorithms can work at DNN scales. Here we \ndemonstrate they can: we evolve the weights of a DNN with a simple, \ngradient-free, population-based genetic algorithm (GA) and it performs well on \nhard deep RL problems, including Atari and humanoid locomotion. The Deep GA \nsuccessfully evolves networks with over four million free parameters, the \nlargest neural networks ever evolved with a traditional evolutionary algorithm. \nThese results (1) expand our sense of the scale at which GAs can operate, (2) \nsuggest intriguingly that in some cases following the gradient is not the best \nchoice for optimizing performance, and (3) make immediately available the \nmultitude of techniques that have been developed in the neuroevolution \ncommunity to improve performance on RL problems. To demonstrate the latter, we \nshow that combining DNNs with novelty search, which was designed to encourage \nexploration on tasks with deceptive or sparse reward functions, can solve a \nhigh-dimensional problem on which reward-maximizing algorithms (e.g. DQN, A3C, \nES, and the GA) fail. Additionally, the Deep GA parallelizes better than ES, \nA3C, and DQN, and enables a state-of-the-art compact encoding technique that \ncan represent million-parameter DNNs in thousands of bytes. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac58", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06567"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Isabeau Pr&#xe9;mont-Schwarz, Alexander Ilin, Tele Hotloo Hao, Antti Rasmus, Rinu Boney, Harri Valpola", "title": "Recurrent Ladder Networks. (arXiv:1707.09219v4 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.09219", "type": "text/html"}], "timestampUsec": "1513669229487539", "comments": [], "summary": {"content": "<p>We propose a recurrent extension of the Ladder networks whose structure is \nmotivated by the inference required in hierarchical latent variable models. We \ndemonstrate that the recurrent Ladder is able to handle a wide variety of \ncomplex learning tasks that benefit from iterative inference and temporal \nmodeling. The architecture shows close-to-optimal results on temporal modeling \nof video data, competitive results on music modeling, and improved perceptual \ngrouping based on higher order abstractions, such as stochastic textures and \nmotion cues. We present results for fully supervised, semi-supervised, and \nunsupervised tasks. The results suggest that the proposed architecture and \nprinciples are powerful tools for learning a hierarchy of abstractions, \nlearning iterative inference and handling temporal information. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac6c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.09219"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513920224, "author": "Zahra Mahoor, Jack Felag, Josh Bongard", "title": "Morphology dictates a robot's ability to ground crowd-proposed language. (arXiv:1712.05881v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.05881", "type": "text/html"}], "timestampUsec": "1513669229487538", "comments": [], "summary": {"content": "<p>As more robots act in physical proximity to people, it is essential to ensure \nthey make decisions and execute actions that align with human values. To do so, \nrobots need to understand the true intentions behind human-issued commands. In \nthis paper, we define a safe robot as one that receives a natural-language \ncommand from humans, considers an action in response to that command, and \naccurately predicts how humans will judge that action if is executed in \nreality. Our contribution is two-fold: First, we introduce a web platform for \nhuman users to propose commands to simulated robots. The robots receive \ncommands and act based on those proposed commands, and then the users provide \npositive and/or negative reinforcement. Next, we train a critic for each robot \nto predict the crowd's responses to one of the crowd-proposed commands. Second, \nwe show that the morphology of a robot plays a role in the way it grounds \nlanguage: The critics show that two of the robots used in the experiment \nachieve a lower prediction error than the others. Thus, those two robots are \nsafer, according to our definition, since they ground the proposed command more \naccurately. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513920224, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac76", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05881"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mu Qiao, Luis Bathen, Simon-Pierre G&#xe9;not, Sunhwan Lee, Ramani Routray", "title": "StackInsights: Cognitive Learning for Hybrid Cloud Readiness. (arXiv:1712.06015v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.06015", "type": "text/html"}], "timestampUsec": "1513669229487537", "comments": [], "summary": {"content": "<p>Hybrid cloud is an integrated cloud computing environment utilizing a mix of \npublic cloud, private cloud, and on-premise traditional IT infrastructures. \nWorkload awareness, defined as a detailed full range understanding of each \nindividual workload, is essential in implementing the hybrid cloud. While it is \ncritical to perform an accurate analysis to determine which workloads are \nappropriate for on-premise deployment versus which workloads can be migrated to \na cloud off-premise, the assessment is mainly performed by rule or policy based \napproaches. In this paper, we introduce StackInsights, a novel cognitive system \nto automatically analyze and predict the cloud readiness of workloads for an \nenterprise. Our system harnesses the critical metrics across the entire stack: \n1) infrastructure metrics, 2) data relevance metrics, and 3) application \ntaxonomy, to identify workloads that have characteristics of a) low sensitivity \nwith respect to business security, criticality and compliance, and b) low \nresponse time requirements and access patterns. Since the capture of the data \nrelevance metrics involves an intrusive and in-depth scanning of the content of \nstorage objects, a machine learning model is applied to perform the business \nrelevance classification by learning from the meta level metrics harnessed \nacross stack. In contrast to traditional methods, StackInsights significantly \nreduces the total time for hybrid cloud readiness assessment by orders of \nmagnitude. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac84", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06015"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Per-Arne Andersen, Morten Goodwin, Ole-Christoffer Granmo", "title": "Towards a Deep Reinforcement Learning Approach for Tower Line Wars. (arXiv:1712.06180v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.06180", "type": "text/html"}], "timestampUsec": "1513669229487536", "comments": [], "summary": {"content": "<p>There have been numerous breakthroughs with reinforcement learning in the \nrecent years, perhaps most notably on Deep Reinforcement Learning successfully \nplaying and winning relatively advanced computer games. There is undoubtedly an \nanticipation that Deep Reinforcement Learning will play a major role when the \nfirst AI masters the complicated game plays needed to beat a professional \nReal-Time Strategy game player. For this to be possible, there needs to be a \ngame environment that targets and fosters AI research, and specifically Deep \nReinforcement Learning. Some game environments already exist, however, these \nare either overly simplistic such as Atari 2600 or complex such as Starcraft II \nfrom Blizzard Entertainment. We propose a game environment in between Atari \n2600 and Starcraft II, particularly targeting Deep Reinforcement Learning \nalgorithm research. The environment is a variant of Tower Line Wars from \nWarcraft III, Blizzard Entertainment. Further, as a proof of concept that the \nenvironment can harbor Deep Reinforcement algorithms, we propose and apply a \nDeep Q-Reinforcement architecture. The architecture simplifies the state space \nso that it is applicable to Q-learning, and in turn improves performance \ncompared to current state-of-the-art methods. Our experiments show that the \nproposed architecture can learn to play the environment well, and score 33% \nbetter than standard Deep Q-learning which in turn proves the usefulness of the \ngame environment. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac97", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06180"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Erik Bodin, Iman Malik, Carl Henrik Ek, Neill D. F. Campbell", "title": "Nonparametric Inference for Auto-Encoding Variational Bayes. (arXiv:1712.06536v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06536", "type": "text/html"}], "timestampUsec": "1513669229487535", "comments": [], "summary": {"content": "<p>We would like to learn latent representations that are low-dimensional and \nhighly interpretable. A model that has these characteristics is the Gaussian \nProcess Latent Variable Model. The benefits and negative of the GP-LVM are \ncomplementary to the Variational Autoencoder, the former provides interpretable \nlow-dimensional latent representations while the latter is able to handle large \namounts of data and can use non-Gaussian likelihoods. Our inspiration for this \npaper is to marry these two approaches and reap the benefits of both. In order \nto do so we will introduce a novel approximate inference scheme inspired by the \nGP-LVM and the VAE. We show experimentally that the approximation allows the \ncapacity of the generative bottle-neck (Z) of the VAE to be arbitrarily large \nwithout losing a highly interpretable representation, allowing reconstruction \nquality to be unlimited by Z at the same time as a low-dimensional space can be \nused to perform ancestral sampling from as well as a means to reason about the \nembedded data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eacb0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06536"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kyle Brown, Derek Doran", "title": "Realistic Traffic Generation for Web Robots. (arXiv:1712.05813v1 [cs.NI])", "alternate": [{"href": "http://arxiv.org/abs/1712.05813", "type": "text/html"}], "timestampUsec": "1513669229487534", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b329fbcca0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b329fbcca0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Critical to evaluating the capacity, scalability, and availability of web \nsystems are realistic web traffic generators. Web traffic generation is a \nclassic research problem, no generator accounts for the characteristics of web \nrobots or crawlers that are now the dominant source of traffic to a web server. \nAdministrators are thus unable to test, stress, and evaluate how their systems \nperform in the face of ever increasing levels of web robot traffic. To resolve \nthis problem, this paper introduces a novel approach to generate synthetic web \nrobot traffic with high fidelity. It generates traffic that accounts for both \nthe temporal and behavioral qualities of robot traffic by statistical and \nBayesian models that are fitted to the properties of robot traffic seen in web \nlogs from North America and Europe. We evaluate our traffic generator by \ncomparing the characteristics of generated traffic to those of the original \ndata. We look at session arrival rates, inter-arrival times and session \nlengths, comparing and contrasting them between generated and real traffic. \nFinally, we show that our generated traffic affects cache performance similarly \nto actual traffic, using the common LRU and LFU eviction policies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eacbc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05813"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Dustin Anderson, Jean-Roch Vlimant, Maria Spiropulu", "title": "An MPI-Based Python Framework for Distributed Training with Keras. (arXiv:1712.05878v1 [cs.DC])", "alternate": [{"href": "http://arxiv.org/abs/1712.05878", "type": "text/html"}], "timestampUsec": "1513669229487533", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a07c2f1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a07c2f1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present a lightweight Python framework for distributed training of neural \nnetworks on multiple GPUs or CPUs. The framework is built on the popular Keras \nmachine learning library. The Message Passing Interface (MPI) protocol is used \nto coordinate the training process, and the system is well suited for job \nsubmission at supercomputing sites. We detail the software's features, describe \nits use, and demonstrate its performance on systems of varying sizes on a \nbenchmark problem drawn from high-energy physics research. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eaccc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05878"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Junghoon Seo, Taegyun Jeon", "title": "On reproduction of On the regularization of Wasserstein GANs. (arXiv:1712.05882v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.05882", "type": "text/html"}], "timestampUsec": "1513669229487532", "comments": [], "summary": {"content": "<p>This report has several purposes. First, our report is written to investigate \nthe reproducibility of the submitted paper On the regularization of Wasserstein \nGANs (2018). Second, among the experiments performed in the submitted paper, \nfive aspects were emphasized and reproduced: learning speed, stability, \nrobustness against hyperparameter, estimating the Wasserstein distance, and \nvarious sampling method. Finally, we identify which parts of the contribution \ncan be reproduced, and at what cost in terms of resources. All source code for \nreproduction is open to the public. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eacda", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05882"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ryan Turner, Brady Neal", "title": "How well does your sampler really work?. (arXiv:1712.06006v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06006", "type": "text/html"}], "timestampUsec": "1513669229487531", "comments": [], "summary": {"content": "<p>We present a new data-driven benchmark system to evaluate the performance of \nnew MCMC samplers. Taking inspiration from the COCO benchmark in optimization, \nwe view this task as having critical importance to machine learning and \nstatistics given the rate at which new samplers are proposed. The common \nhand-crafted examples to test new samplers are unsatisfactory; we take a \nmeta-learning-like approach to generate benchmark examples from a large corpus \nof data sets and models. Surrogates of posteriors found in real problems are \ncreated using highly flexible density models including modern neural network \nbased approaches. We provide new insights into the real effective sample size \nof various samplers per unit time and the estimation efficiency of the samplers \nper sample. Additionally, we provide a meta-analysis to assess the predictive \nutility of various MCMC diagnostics and perform a nonparametric regression to \ncombine them. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eacde", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06006"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David Alvarez-Melis, Tommi S. Jaakkola, Stefanie Jegelka", "title": "Structured Optimal Transport. (arXiv:1712.06199v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06199", "type": "text/html"}], "timestampUsec": "1513669229487530", "comments": [], "summary": {"content": "<p>Optimal Transport has recently gained interest in machine learning for \napplications ranging from domain adaptation, sentence similarities to deep \nlearning. Yet, its ability to capture frequently occurring structure beyond the \n\"ground metric\" is limited. In this work, we develop a nonlinear generalization \nof (discrete) optimal transport that is able to reflect much additional \nstructure. We demonstrate how to leverage the geometry of this new model for \nfast algorithms, and explore connections and properties. Illustrative \nexperiments highlight the benefit of the induced structured couplings for tasks \nin domain adaptation and natural language processing. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eace6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06199"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Guoqing Chao, Shiliang Sun, Jinbo Bi", "title": "A Survey on Multi-View Clustering. (arXiv:1712.06246v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.06246", "type": "text/html"}], "timestampUsec": "1513669229487529", "comments": [], "summary": {"content": "<p>With the fast development of information technology, especially the \npopularization of internet, multi-view learning becomes more and more popular \nin machine learning and data mining fields. As we all know that, multi-view \nsemi-supervised learning, such as co-training, co-regularization has gained \nconsiderable attentions. Although recently, multi-view clustering (MVC) has \ndeveloped rapidly, there are not a survey or review to summarize and analyze \nthe current progress. Therefore, this paper sums up the common strategies of \ncombining multiple views and based on that we proposed a novel taxonomy of the \nMVC approaches. We also discussed the relationships between MVC and multi-view \nrepresentation, ensemble clustering, multi-task clustering, multi-view \nsupervised and multi-view semi-supervised learning. Several representative \nreal-world applications are elaborated. To promote the further development of \nMVC, we pointed out several open problems that are worth exploring in the \nfuture. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eaceb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06246"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Luca Franceschi (1 and 2), Michele Donini (2), Paolo Frasconi (3), Massimiliano Pontil (1 and 2) ((1) University College London, (2) Istituto Italiano di Teconologia, (3) Universita&#x27; degli Studi di Firenze)", "title": "A Bridge Between Hyperparameter Optimization and Larning-to-learn. (arXiv:1712.06283v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06283", "type": "text/html"}], "timestampUsec": "1513669229487528", "comments": [], "summary": {"content": "<p>We consider a class of a nested optimization problems involving inner and \nouter objectives. We observe that by taking into explicit account the \noptimization dynamics for the inner objective it is possible to derive a \ngeneral framework that unifies gradient-based hyperparameter optimization and \nmeta-learning (or learning-to-learn). Depending on the specific setting, the \nvariables of the outer objective take either the meaning of hyperparameters in \na supervised learning problem or parameters of a meta-learner. We show that \nsome recently proposed methods in the latter setting can be instantiated in our \nframework and tackled with the same gradient-based algorithms. Finally, we \ndiscuss possible design patterns for learning-to-learn and present encouraging \npreliminary experiments for few-shot learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eacef", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06283"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Siyuan Ma, Raef Bassily, Mikhail Belkin", "title": "The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning. (arXiv:1712.06559v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.06559", "type": "text/html"}], "timestampUsec": "1513669229487527", "comments": [], "summary": {"content": "<p>Stochastic Gradient Descent (SGD) with small mini-batch is a key component in \nmodern large-scale machine learning. However, its efficiency has not been easy \nto analyze as most theoretical results require adaptive rates and show \nconvergence rates far slower than that for gradient descent, making \ncomputational comparisons difficult. \n</p> \n<p>In this paper we aim to clarify the issue of fast SGD convergence. The key \nobservation is that most modern architectures are over-parametrized and are \ntrained to interpolate the data by driving the empirical loss (classification \nand regression) close to zero. While it is still unclear why these interpolated \nsolutions perform well on test data, these regimes allow for very fast \nconvergence of SGD, comparable in the number of iterations to gradient descent. \n</p> \n<p>Specifically, consider the setting with quadratic objective function, or near \na minimum, where the quadratic term is dominant. We show that: (1) Mini-batch \nsize $1$ with constant step size is optimal in terms of computations to achieve \na given error. (2) There is a critical mini-batch size such that: (a. linear \nscaling) SGD iteration with mini-batch size $m$ smaller than the critical size \nis nearly equivalent to $m$ iterations of mini-batch size $1$. (b. saturation) \nSGD iteration with mini-batch larger than the critical size is nearly \nequivalent to a gradient descent step. \n</p> \n<p>The critical mini-batch size can be viewed as the limit for effective \nmini-batch parallelization. It is also nearly independent of the data size, \nimplying $O(n)$ acceleration over GD per unit of computation. \n</p> \n<p>We give experimental evidence on real data, with the results closely \nfollowing our theoretical analyses. \n</p> \n<p>Finally, we show how the interpolation perspective and our results fit with \nrecent developments in training deep neural networks and discuss connections to \nadaptive rates for SGD and variance reduction. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eacf4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06559"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Saptarshi Sengupta, Sanchita Basak, Richard Alan Peters II", "title": "Data Clustering using a Hybrid of Fuzzy C-Means and Quantum-behaved Particle Swarm Optimization. (arXiv:1712.05512v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.05512", "type": "text/html"}], "timestampUsec": "1513575516304230", "comments": [], "summary": {"content": "<p>Fuzzy clustering has become a widely used data mining technique and plays an \nimportant role in grouping, traversing and selectively using data for user \nspecified applications. The deterministic Fuzzy C-Means (FCM) algorithm may \nresult in suboptimal solutions when applied to multidimensional data in \nreal-world, time-constrained problems. In this paper the Quantum-behaved \nParticle Swarm Optimization (QPSO) with a fully connected topology is coupled \nwith the Fuzzy C-Means Clustering algorithm and is tested on a suite of \ndatasets from the UCI Machine Learning Repository. The global search ability of \nthe QPSO algorithm helps in avoiding stagnation in local optima while the soft \nclustering approach of FCM helps to partition data based on membership \nprobabilities. Clustering performance indices such as F-Measure, Accuracy, \nQuantization Error, Intercluster and Intracluster distances are reported for \ncompetitive techniques such as PSO K-Means, QPSO K-Means and QPSO FCM over all \ndatasets considered. Experimental results indicate that QPSO FCM provides \ncomparable and in most cases superior results when compared to the others. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d367", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05512"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Altaf H. Khan", "title": "Lightweight Neural Networks. (arXiv:1712.05695v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.05695", "type": "text/html"}], "timestampUsec": "1513575516304229", "comments": [], "summary": {"content": "<p>Most of the weights in a Lightweight Neural Network have a value of zero, \nwhile the remaining ones are either +1 or -1. These universal approximators \nrequire approximately 1.1 bits/weight of storage, posses a quick forward pass \nand achieve classification accuracies similar to conventional continuous-weight \nnetworks. Their training regimen focuses on error reduction initially, but \nlater emphasizes discretization of weights. They ignore insignificant inputs, \nremove unnecessary weights, and drop unneeded hidden neurons. We have \nsuccessfully tested them on the MNIST, credit card fraud, and credit card \ndefaults data sets using networks having 2 to 16 hidden layers and up to 4.4 \nmillion weights. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d387", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05695"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Huda Hakami, Danushka Bollegala, Hayashi Kohei", "title": "Why PairDiff works? -- A Mathematical Analysis of Bilinear Relational Compositional Operators for Analogy Detection. (arXiv:1709.06673v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.06673", "type": "text/html"}], "timestampUsec": "1513575516304228", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a07c6e5\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a07c6e5&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Representing the semantic relations that exist between two given words (or \nentities) is an important first step in a wide-range of NLP applications such \nas analogical reasoning, knowledge base completion and relational information \nretrieval. A simple, yet surprisingly accurate method for representing a \nrelation between two words is to compute the vector offset (\\PairDiff) between \ntheir corresponding word embeddings. Despite the empirical success, it remains \nunclear as to whether \\PairDiff is the best operator for obtaining a relational \nrepresentation from word embeddings. We conduct a theoretical analysis of \ngeneralised bilinear operators that can be used to measure the $\\ell_{2}$ \nrelational distance between two word-pairs. We show that, if the word \nembeddings are standardised and uncorrelated, such an operator will be \nindependent of bilinear terms, and can be simplified to a linear form, where \n\\PairDiff is a special case. For numerous word embedding types, we empirically \nverify the uncorrelation assumption, demonstrating the general applicability of \nour theoretical result. Moreover, we experimentally discover \\PairDiff from the \nbilinear relation composition operator on several benchmark analogy datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d399", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.06673"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Guillaume Bellec, David Kappel, Wolfgang Maass, Robert Legenstein", "title": "Deep Rewiring: Training very sparse deep networks. (arXiv:1711.05136v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05136", "type": "text/html"}], "timestampUsec": "1513575516304227", "comments": [], "summary": {"content": "<p>Neuromorphic hardware tends to pose limits on the connectivity of deep \nnetworks that one can run on them. But also generic hardware and software \nimplementations of deep learning run more efficiently on sparse networks. \nSeveral methods exist for pruning connections of a neural network after it was \ntrained without connectivity constraints. We present an algorithm, DEEP R, that \nenables us to train directly a sparsely connected neural network. DEEP R \nautomatically rewires the network during supervised training so that \nconnections are there where they are most needed for the task, while its total \nnumber is all the time strictly bounded. We demonstrate that DEEP R can be used \nto train very sparse feedforward and recurrent neural networks on standard \nbenchmark tasks with just a minor loss in performance. DEEP R is based on a \nrigorous theoretical foundation that views rewiring as stochastic sampling of \nnetwork configurations from a posterior. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d3ae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05136"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yi Tay, Anh Tuan Luu, Siu Cheung Hui", "title": "Learning to Attend via Word-Aspect Associative Fusion for Aspect-based Sentiment Analysis. (arXiv:1712.05403v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.05403", "type": "text/html"}], "timestampUsec": "1513575516304226", "comments": [], "summary": {"content": "<p>Aspect-based sentiment analysis (ABSA) tries to predict the polarity of a \ngiven document with respect to a given aspect entity. While neural network \narchitectures have been successful in predicting the overall polarity of \nsentences, aspect-specific sentiment analysis still remains as an open problem. \nIn this paper, we propose a novel method for integrating aspect information \ninto the neural model. More specifically, we incorporate aspect information \ninto the neural model by modeling word-aspect relationships. Our novel model, \n\\textit{Aspect Fusion LSTM} (AF-LSTM) learns to attend based on associative \nrelationships between sentence words and aspect which allows our model to \nadaptively focus on the correct words given an aspect term. This ameliorates \nthe flaws of other state-of-the-art models that utilize naive concatenations to \nmodel word-aspect similarity. Instead, our model adopts circular convolution \nand circular correlation to model the similarity between aspect and words and \nelegantly incorporates this within a differentiable neural attention framework. \nFinally, our model is end-to-end differentiable and highly related to \nconvolution-correlation (holographic like) memories. Our proposed neural model \nachieves state-of-the-art performance on benchmark datasets, outperforming \nATAE-LSTM by $4\\%-5\\%$ on average across multiple datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d3c9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05403"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi", "title": "AI2-THOR: An Interactive 3D Environment for Visual AI. (arXiv:1712.05474v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.05474", "type": "text/html"}], "timestampUsec": "1513575516304225", "comments": [], "summary": {"content": "<p>We introduce The House Of inteRactions (THOR), a framework for visual AI \nresearch, available at <a href=\"http://ai2thor.allenai.org.\">this http URL</a> AI2-THOR consists of near \nphoto-realistic 3D indoor scenes, where AI agents can navigate in the scenes \nand interact with objects to perform tasks. AI2-THOR enables research in many \ndifferent domains including but not limited to deep reinforcement learning, \nimitation learning, learning by interaction, planning, visual question \nanswering, unsupervised representation learning, object detection and \nsegmentation, and learning models of cognition. The goal of AI2-THOR is to \nfacilitate building visually intelligent models and push the research forward \nin this domain. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d3da", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05474"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ashwin Khadke, Manuela Veloso", "title": "What Can This Robot Do? Learning from Appearance and Experiments. (arXiv:1712.05497v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.05497", "type": "text/html"}], "timestampUsec": "1513575516304224", "comments": [], "summary": {"content": "<p>When presented with an unknown robot (subject) how can an autonomous agent \n(learner) figure out what this new robot can do? The subject's appearance can \nprovide cues to its physical as well as cognitive capabilities. Seeing a \nhumanoid can make one wonder if it can kick balls, climb stairs or recognize \nfaces. What if the learner can request the subject to perform these tasks? We \npresent an approach to make the learner build a model of the subject at a task \nbased on the latter's appearance and refine it by experimentation. Apart from \nthe subject's inherent capabilities, certain extrinsic factors may affect its \nperformance at a task. Based on the subject's appearance and prior knowledge \nabout the task a learner can identify a set of potential factors, a subset of \nwhich we assume are controllable. Our approach picks values of controllable \nfactors to generate the most informative experiments to test the subject at. \nAdditionally, we present a metric to determine if a factor should be \nincorporated in the model. We present results of our approach on modeling a \nhumanoid robot at the task of kicking a ball. Firstly, we show that actively \npicking values for controllable factors, even in noisy experiments, leads to \nfaster learning of the subject's model for the task. Secondly, starting from a \nminimal set of factors our metric identifies the set of relevant factors to \nincorporate in the model. Lastly, we show that the refined model better \nrepresents the subject's performance at the task. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d3e2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05497"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Siddharthan Rajasekaran, Jinwei Zhang, Jie Fu", "title": "Inverse Reinforce Learning with Nonparametric Behavior Clustering. (arXiv:1712.05514v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.05514", "type": "text/html"}], "timestampUsec": "1513575516304223", "comments": [], "summary": {"content": "<p>Inverse Reinforcement Learning (IRL) is the task of learning a single reward \nfunction given a Markov Decision Process (MDP) without defining the reward \nfunction, and a set of demonstrations generated by humans/experts. However, in \npractice, it may be unreasonable to assume that human behaviors can be \nexplained by one reward function since they may be inherently inconsistent. \nAlso, demonstrations may be collected from various users and aggregated to \ninfer and predict user's behaviors. In this paper, we introduce the \nNon-parametric Behavior Clustering IRL algorithm to simultaneously cluster \ndemonstrations and learn multiple reward functions from demonstrations that may \nbe generated from more than one behaviors. Our method is iterative: It \nalternates between clustering demonstrations into different behavior clusters \nand inverse learning the reward functions until convergence. It is built upon \nthe Expectation-Maximization formulation and non-parametric clustering in the \nIRL setting. Further, to improve the computation efficiency, we remove the need \nof completely solving multiple IRL problems for multiple clusters during the \niteration steps and introduce a resampling technique to avoid generating too \nmany unlikely clusters. We demonstrate the convergence and efficiency of the \nproposed method through learning multiple driver behaviors from demonstrations \ngenerated from a grid-world environment and continuous trajectories collected \nfrom autonomous robot cars using the Gazebo robot simulator. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d3e9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05514"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jin-Hwa Kim, Devi Parikh, Dhruv Batra, Byoung-Tak Zhang, Yuandong Tian", "title": "CoDraw: Visual Dialog for Collaborative Drawing. (arXiv:1712.05558v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.05558", "type": "text/html"}], "timestampUsec": "1513575516304222", "comments": [], "summary": {"content": "<p>In this work, we propose a goal-driven collaborative task that contains \nvision, language, and action in a virtual environment as its core components. \nSpecifically, we develop a collaborative `Image Drawing' game between two \nagents, called CoDraw. Our game is grounded in a virtual world that contains \nmovable clip art objects. Two players, Teller and Drawer, are involved. The \nTeller sees an abstract scene containing multiple clip arts in a semantically \nmeaningful configuration, while the Drawer tries to reconstruct the scene on an \nempty canvas using available clip arts. The two players communicate via two-way \ncommunication using natural language. We collect the CoDraw dataset of ~10K \ndialogs consisting of 138K messages exchanged between a Teller and a Drawer \nfrom Amazon Mechanical Turk (AMT). We analyze our dataset and present three \nmodels to model the players' behaviors, including an attention model to \ndescribe and draw multiple clip arts at each round. The attention models are \nquantitatively compared to the other models to show how the conventional \napproaches work for this new task. We also present qualitative visualizations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d3fb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05558"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "V.I. Yukalov, E.P. Yukalova, D. Sornette", "title": "Information Processing by Networks of Quantum Decision Makers. (arXiv:1712.05734v1 [physics.soc-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.05734", "type": "text/html"}], "timestampUsec": "1513575516304221", "comments": [], "summary": {"content": "<p>We suggest a model of a multi-agent society of decision makers taking \ndecisions being based on two criteria, one is the utility of the prospects and \nthe other is the attractiveness of the considered prospects. The model is the \ngeneralization of quantum decision theory, developed earlier for single \ndecision makers realizing one-step decisions, in two principal aspects. First, \nseveral decision makers are considered simultaneously, who interact with each \nother through information exchange. Second, a multistep procedure is treated, \nwhen the agents exchange information many times. Several decision makers \nexchanging information and forming their judgement, using quantum rules, form a \nkind of a quantum information network, where collective decisions develop in \ntime as a result of information exchange. In addition to characterizing \ncollective decisions that arise in human societies, such networks can describe \ndynamical processes occurring in artificial quantum intelligence composed of \nseveral parts or in a cluster of quantum computers. The practical usage of the \ntheory is illustrated on the dynamic disjunction effect for which three \nquantitative predictions are made: (i) the probabilistic behavior of decision \nmakers at the initial stage of the process is described; (ii) the decrease of \nthe difference between the initial prospect probabilities and the related \nutility factors is proved; (iii) the existence of a common consensus after \nmultiple exchange of information is predicted. The predicted numerical values \nare in very good agreement with empirical data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d410", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05734"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1516599275, "author": "Jordan Prosky, Xingyou Song, Andrew Tan, Michael Zhao", "title": "Sentiment Predictability for Stocks. (arXiv:1712.05785v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.05785", "type": "text/html"}], "timestampUsec": "1513575516304220", "comments": [], "summary": {"content": "<p>In this work, we present our findings and experiments for stock-market \nprediction using various textual sentiment analysis tools, such as mood \nanalysis and event extraction, as well as prediction models, such as LSTMs and \nspecific convolutional architectures. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1516599275, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d41c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05785"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Martin C. Cooper, Stanislav &#x17d;ivn&#xfd;", "title": "The Power of Arc Consistency for CSPs Defined by Partially-Ordered Forbidden Patterns. (arXiv:1604.07981v3 [cs.CC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1604.07981", "type": "text/html"}], "timestampUsec": "1513575516304219", "comments": [], "summary": {"content": "<p>Characterising tractable fragments of the constraint satisfaction problem \n(CSP) is an important challenge in theoretical computer science and artificial \nintelligence. Forbidding patterns (generic sub-instances) provides a means of \ndefining CSP fragments which are neither exclusively language-based nor \nexclusively structure-based. It is known that the class of binary CSP instances \nin which the broken-triangle pattern (BTP) does not occur, a class which \nincludes all tree-structured instances, are decided by arc consistency (AC), a \nubiquitous reduction operation in constraint solvers. We provide a \ncharacterisation of simple partially-ordered forbidden patterns which have this \nAC-solvability property. It turns out that BTP is just one of five such \nAC-solvable patterns. The four other patterns allow us to exhibit new tractable \nclasses. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d430", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1604.07981"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sam Kriegman, Nick Cheney, Josh Bongard", "title": "How morphological development can guide evolution. (arXiv:1711.07387v2 [q-bio.PE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07387", "type": "text/html"}], "timestampUsec": "1513575516304218", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a07ca8d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a07ca8d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Organisms result from multiple adaptive processes occurring and interacting \nat different time scales. One such interaction is that between development and \nevolution. In modeling studies, it has been shown that development sweeps over \na series of traits in a single agent, and sometimes exposes promising static \ntraits. Subsequent evolution can then canalize these rare traits. Thus, \ndevelopment can, under the right conditions, increase evolvability. Here, we \nreport on a previously unknown phenomenon when embodied agents are allowed to \ndevelop and evolve: Evolution discovers body plans which are robust to control \nchanges, these body plans become genetically assimilated, yet controllers for \nthese agents are not assimilated. This allows evolution to continue climbing \nfitness gradients by tinkering with the developmental programs for controllers \nwithin these permissive body plans. This exposes a previously unknown detail \nabout the Baldwin effect: instead of all useful traits becoming genetically \nassimilated, only phenotypic traits that render the agent robust to changes in \nother traits become assimilated. We refer to this phenomenon as differential \ncanalization. This finding also has important implications for the evolutionary \ndesign of artificial and embodied agents such as robots: robots that are robust \nto internal changes in their controllers may also be robust to external changes \nin their environment, such as transferal from simulation to reality, or \ndeployment in novel environments. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d43c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07387"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Atsushi Nitanda, Taiji Suzuki", "title": "Stochastic Particle Gradient Descent for Infinite Ensembles. (arXiv:1712.05438v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.05438", "type": "text/html"}], "timestampUsec": "1513575516304216", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a0e0ebb\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a0e0ebb&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The superior performance of ensemble methods with infinite models are well \nknown. Most of these methods are based on optimization problems in \ninfinite-dimensional spaces with some regularization, for instance, boosting \nmethods and convex neural networks use $L^1$-regularization with the \nnon-negative constraint. However, due to the difficulty of handling \n$L^1$-regularization, these problems require early stopping or a rough \napproximation to solve it inexactly. In this paper, we propose a new ensemble \nlearning method that performs in a space of probability measures, that is, our \nmethod can handle the $L^1$-constraint and the non-negative constraint in a \nrigorous way. Such an optimization is realized by proposing a general purpose \nstochastic optimization method for learning probability measures via \nparameterization using transport maps on base models. As a result of running \nthe method, a transport map to output an infinite ensemble is obtained, which \nforms a residual-type network. From the perspective of functional gradient \nmethods, we give a convergence rate as fast as that of a stochastic \noptimization method for finite dimensional nonconvex problems. Moreover, we \nshow an interior optimality property of a local optimality condition used in \nour analysis. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d449", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05438"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexander LeNail, Ludwig Schmidt, Johnathan Li, Tobias Ehrenberger, Karen Sachs, Stefanie Jegelka, Ernest Fraenkel", "title": "Graph-Sparse Logistic Regression. (arXiv:1712.05510v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.05510", "type": "text/html"}], "timestampUsec": "1513575516304215", "comments": [], "summary": {"content": "<p>We introduce Graph-Sparse Logistic Regression, a new algorithm for \nclassification for the case in which the support should be sparse but connected \non a graph. We val- idate this algorithm against synthetic data and benchmark \nit against L1-regularized Logistic Regression. We then explore our technique in \nthe bioinformatics context of proteomics data on the interactome graph. We make \nall our experimental code public and provide GSLR as an open source package. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d45a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05510"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kyriakos Polymenakos, Alessandro Abate, Stephen Roberts", "title": "Safe Policy Search with Gaussian Process Models. (arXiv:1712.05556v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.05556", "type": "text/html"}], "timestampUsec": "1513575516304214", "comments": [], "summary": {"content": "<p>We propose a method to optimise the parameters of a policy which will be used \nto safely perform a given task in a data-efficient manner. We train a Gaussian \nprocess model to capture the system dynamics, based on the PILCO framework. Our \nmodel has useful analytic properties, which allow closed form computation of \nerror gradients and estimating the probability of violating given state space \nconstraints. During training, as well as operation, only policies that are \ndeemed safe are implemented on the real system, minimising the risk of failure. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d467", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05556"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1516082465, "author": "Milana Gataric, Tengyao Wang, Richard J. Samworth", "title": "Sparse principal component analysis via random projections. (arXiv:1712.05630v2 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.05630", "type": "text/html"}], "timestampUsec": "1513575516304213", "comments": [], "summary": {"content": "<p>We introduce a new method for sparse principal component analysis, based on \nthe aggregation of eigenvector information from carefully-selected random \nprojections of the sample covariance matrix. Unlike most alternative \napproaches, our algorithm is non-iterative, so is not vulnerable to a bad \nchoice of initialisation. Our theory provides great detail on the statistical \nand computational trade-off in our procedure, revealing a subtle interplay \nbetween the effective sample size and the number of random projections that are \nrequired to achieve the minimax optimal rate. Numerical studies provide further \ninsight into the procedure and confirm its highly competitive finite-sample \nperformance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1516082464, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d472", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05630"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hongzhou Lin, Julien Mairal, Zaid Harchaoui", "title": "Catalyst Acceleration for First-order Convex Optimization: from Theory to Practice. (arXiv:1712.05654v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.05654", "type": "text/html"}], "timestampUsec": "1513575516304212", "comments": [], "summary": {"content": "<p>We introduce a generic scheme for accelerating gradient-based optimization \nmethods in the sense of Nesterov. The approach, called Catalyst, builds upon \nthe inexact acceler- ated proximal point algorithm for minimizing a convex \nobjective function, and consists of approximately solving a sequence of \nwell-chosen auxiliary problems, leading to faster convergence. One of the key \nto achieve acceleration in theory and in practice is to solve these \nsub-problems with appropriate accuracy by using the right stopping criterion \nand the right warm-start strategy. In this paper, we give practical guidelines \nto use Catalyst and present a comprehensive theoretical analysis of its global \ncomplexity. We show that Catalyst applies to a large class of algorithms, \nincluding gradient descent, block coordinate descent, incremental algorithms \nsuch as SAG, SAGA, SDCA, SVRG, Finito/MISO, and their proximal variants. For \nall of these methods, we provide acceleration and explicit sup- port for \nnon-strongly convex objectives. We conclude with extensive experiments showing \nthat acceleration is useful in practice, especially for ill-conditioned \nproblems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d47a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05654"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Guangxi Li, Jinmian Ye, Haiqin Yang, Di Chen, Shuicheng Yan, Zenglin Xu", "title": "BT-Nets: Simplifying Deep Neural Networks via Block Term Decomposition. (arXiv:1712.05689v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.05689", "type": "text/html"}], "timestampUsec": "1513575516304211", "comments": [], "summary": {"content": "<p>Recently, deep neural networks (DNNs) have been regarded as the \nstate-of-the-art classification methods in a wide range of applications, \nespecially in image classification. Despite the success, the huge number of \nparameters blocks its deployment to situations with light computing resources. \nResearchers resort to the redundancy in the weights of DNNs and attempt to find \nhow fewer parameters can be chosen while preserving the accuracy at the same \ntime. Although several promising results have been shown along this research \nline, most existing methods either fail to significantly compress a \nwell-trained deep network or require a heavy fine-tuning process for the \ncompressed network to regain the original performance. In this paper, we \npropose the \\textit{Block Term} networks (BT-nets) in which the commonly used \nfully-connected layers (FC-layers) are replaced with block term layers \n(BT-layers). In BT-layers, the inputs and the outputs are reshaped into two \nlow-dimensional high-order tensors, then block-term decomposition is applied as \ntensor operators to connect them. We conduct extensive experiments on benchmark \ndatasets to demonstrate that BT-layers can achieve a very large compression \nratio on the number of parameters while preserving the representation power of \nthe original FC-layers as much as possible. Specifically, we can get a higher \nperformance while requiring fewer parameters compared with the tensor train \nmethod. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d482", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05689"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Felix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem Sokolov, Ann Clifton, Matt Post", "title": "Sockeye: A Toolkit for Neural Machine Translation. (arXiv:1712.05690v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.05690", "type": "text/html"}], "timestampUsec": "1513575516304210", "comments": [], "summary": {"content": "<p>We describe Sockeye (version 1.12), an open-source sequence-to-sequence \ntoolkit for Neural Machine Translation (NMT). Sockeye is a production-ready \nframework for training and applying models as well as an experimental platform \nfor researchers. Written in Python and built on MXNet, the toolkit offers \nscalable training and inference for the three most prominent encoder-decoder \narchitectures: attentional recurrent neural networks, self-attentional \ntransformers, and fully convolutional networks. Sockeye also supports a wide \nrange of optimizers, normalization and regularization techniques, and inference \nimprovements from current NMT literature. Users can easily run standard \ntraining recipes, explore different model settings, and incorporate new ideas. \nIn this paper, we highlight Sockeye's features and benchmark it against other \nNMT toolkits on two language arcs from the 2017 Conference on Machine \nTranslation (WMT): English-German and Latvian-English. We report competitive \nBLEU scores across all three architectures, including an overall best score for \nSockeye's transformer implementation. To facilitate further comparison, we \nrelease all system outputs and training scripts used in our experiments. The \nSockeye toolkit is free software released under the Apache 2.0 license. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d48e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05690"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Brian Bierig, Jonathan Hollenbeck, Alexander Stroud", "title": "Understanding Career Progression in Baseball Through Machine Learning. (arXiv:1712.05754v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.05754", "type": "text/html"}], "timestampUsec": "1513575516304209", "comments": [], "summary": {"content": "<p>Professional baseball players are increasingly guaranteed expensive long-term \ncontracts, with over 70 deals signed in excess of \\$90 million, mostly in the \nlast decade. These are substantial sums compared to a typical franchise \nvaluation of \\$1-2 billion. Hence, the players to whom a team chooses to give \nsuch a contract can have an enormous impact on both competitiveness and profit. \nDespite this, most published approaches examining career progression in \nbaseball are fairly simplistic. We applied four machine learning algorithms to \nthe problem and soundly improved upon existing approaches, particularly for \nbatting data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d49b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05754"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Cl&#xe9;ment Godard, Kevin Matzen, Matt Uyttendaele", "title": "Deep Burst Denoising. (arXiv:1712.05790v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.05790", "type": "text/html"}], "timestampUsec": "1513575516304208", "comments": [], "summary": {"content": "<p>Noise is an inherent issue of low-light image capture, one which is \nexacerbated on mobile devices due to their narrow apertures and small sensors. \nOne strategy for mitigating noise in a low-light situation is to increase the \nshutter time of the camera, thus allowing each photosite to integrate more \nlight and decrease noise variance. However, there are two downsides of long \nexposures: (a) bright regions can exceed the sensor range, and (b) camera and \nscene motion will result in blurred images. Another way of gathering more light \nis to capture multiple short (thus noisy) frames in a \"burst\" and intelligently \nintegrate the content, thus avoiding the above downsides. In this paper, we use \nthe burst-capture strategy and implement the intelligent integration via a \nrecurrent fully convolutional deep neural net (CNN). We build our novel, \nmultiframe architecture to be a simple addition to any single frame denoising \nmodel, and design to handle an arbitrary number of noisy input frames. We show \nthat it achieves state of the art denoising results on our burst dataset, \nimproving on the best published multi-frame techniques, such as VBM4D and \nFlexISP. Finally, we explore other applications of image enhancement by \nintegrating content from multiple frames and demonstrate that our DNN \narchitecture generalizes well to image super-resolution. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d4ae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05790"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Simon Lee Dettmer, Johannes Berg", "title": "Inferring the parameters of a Markov process from snapshots of the steady state. (arXiv:1707.04114v3 [cond-mat.stat-mech] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.04114", "type": "text/html"}], "timestampUsec": "1513575516304207", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a0e118b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a0e118b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We seek to infer the parameters of an ergodic Markov process from samples \ntaken independently from the steady state. Our focus is on non-equilibrium \nprocesses, where the steady state is not described by the Boltzmann measure, \nbut is generally unknown and hard to compute, which prevents the application of \nestablished equilibrium inference methods. We propose a quantity we call \npropagator likelihood, which takes on the role of the likelihood in equilibrium \nprocesses. This propagator likelihood is based on fictitious transitions \nbetween those configurations of the system which occur in the samples. The \npropagator likelihood can be derived by minimising the relative entropy between \nthe empirical distribution and a distribution generated by propagating the \nempirical distribution forward in time. Maximising the propagator likelihood \nleads to an efficient reconstruction of the parameters of the underlying model \nin different systems, both with discrete configurations and with continuous \nconfigurations. We apply the method to non-equilibrium models from statistical \nphysics and theoretical biology, including the asymmetric simple exclusion \nprocess (ASEP), the kinetic Ising model, and replicator dynamics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d4b5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.04114"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alejandro Cholaquidis, Ricardo Fraiman, Mariela Sued", "title": "Semi-supervised learning. (arXiv:1709.05673v2 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.05673", "type": "text/html"}], "timestampUsec": "1513575516304206", "comments": [], "summary": {"content": "<p>Semi-supervised learning deals with the problem of how, if possible, to take \nadvantage of a huge amount of not classified data, to perform classification, \nin situations when, typically, the labelled data are few. Even though this is \nnot always possible (it depends on how useful is to know the distribution of \nthe unlabelled data in the inference of the labels), several algorithm have \nbeen proposed recently. A new algorithm is proposed, that under almost \nneccesary conditions, attains asymptotically the performance of the best \ntheoretical rule, when the size of unlabeled data tends to infinity. The set of \nnecessary assumptions, although reasonables, show that semi-parametric \nclassification only works for very well conditioned problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d4bd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.05673"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Freweyni K. Teklehaymanot, Michael Muma, Abdelhak M. Zoubir", "title": "A Novel Bayesian Cluster Enumeration Criterion for Unsupervised Learning. (arXiv:1710.07954v2 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.07954", "type": "text/html"}], "timestampUsec": "1513575516304205", "comments": [], "summary": {"content": "<p>We derive a new Bayesian Information Criterion (BIC) from first principles by \nformulating the problem of estimating the number of clusters in an observed \ndata set as maximization of the posterior probability of the candidate models. \nGiven that some mild assumptions are satisfied, we provide a general BIC \nexpression for a broad class of data distributions. This serves as an important \nmilestone when deriving the BIC for specific data distributions. Along this \nline, we provide a closed-form BIC expression for multivariate Gaussian \ndistributed observations. We show that incorporating data structure of the \nclustering problem into the derivation of the BIC results in an expression \nwhose penalty term is different from that of the original BIC. We propose a \ntwo-step cluster enumeration algorithm. First, a model-based unsupervised \nlearning algorithm partitions the data according to a given set of candidate \nmodels. Subsequently, the optimal cluster number is determined as the one \nassociated to the model for which the proposed BIC is maximal. The performance \nof the proposed criterion is tested using synthetic and real data sets. Despite \nthe fact that the original BIC is a generic criterion which does not include \ninformation about the specific model selection problem at hand, it has been \nwidely used in the literature to estimate the number of clusters in an observed \ndata set. We, therefore, consider it as a benchmark comparison. Simulation \nresults show that our proposed criterion outperforms the existing cluster \nenumeration methods that are based on the original BIC. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d4d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.07954"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kay Gregor Hartmann, Robin Tibor Schirrmeister, Tonio Ball", "title": "Hierarchical internal representation of spectral features in deep convolutional networks trained for EEG decoding. (arXiv:1711.07792v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07792", "type": "text/html"}], "timestampUsec": "1513575516304204", "comments": [], "summary": {"content": "<p>Recently, there is increasing interest and research on the interpretability \nof machine learning models, for example how they transform and internally \nrepresent EEG signals in Brain-Computer Interface (BCI) applications. This can \nhelp to understand the limits of the model and how it may be improved, in \naddition to possibly provide insight about the data itself. Schirrmeister et \nal. (2017) have recently reported promising results for EEG decoding with deep \nconvolutional neural networks (ConvNets) trained in an end-to-end manner and, \nwith a causal visualization approach, showed that they learn to use spectral \namplitude changes in the input. In this study, we investigate how ConvNets \nrepresent spectral features through the sequence of intermediate stages of the \nnetwork. We show higher sensitivity to EEG phase features at earlier stages and \nhigher sensitivity to EEG amplitude features at later stages. Intriguingly, we \nobserved a specialization of individual stages of the network to the classical \nEEG frequency bands alpha, beta, and high gamma. Furthermore, we find first \nevidence that particularly in the last convolutional layer, the network learns \nto detect more complex oscillatory patterns beyond spectral phase and \namplitude, reminiscent of the representation of complex visual features in \nlater layers of ConvNets in computer vision tasks. Our findings thus provide \ninsights into how ConvNets hierarchically represent spectral EEG features in \ntheir intermediate layers and suggest that ConvNets can exploit and might help \nto better understand the compositional structure of EEG time series. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d4ed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07792"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yafeng Liu, Shimin Feng, Zhikai Zhao, Enjie Ding", "title": "Highly Efficient Human Action Recognition with Quantum Genetic Algorithm Optimized Support Vector Machine. (arXiv:1711.09511v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09511", "type": "text/html"}], "timestampUsec": "1513575516304203", "comments": [], "summary": {"content": "<p>In this paper we propose the use of quantum genetic algorithm to optimize the \nsupport vector machine (SVM) for human action recognition. The Microsoft Kinect \nsensor can be used for skeleton tracking, which provides the joints' position \ndata. However, how to extract the motion features for representing the dynamics \nof a human skeleton is still a challenge due to the complexity of human motion. \nWe present a highly efficient features extraction method for action \nclassification, that is, using the joint angles to represent a human skeleton \nand calculating the variance of each angle during an action time window. Using \nthe proposed representation, we compared the human action classification \naccuracy of two approaches, including the optimized SVM based on quantum \ngenetic algorithm and the conventional SVM with grid search. Experimental \nresults on the MSR-12 dataset show that the conventional SVM achieved an \naccuracy of $ 93.85\\% $. The proposed approach outperforms the conventional \nmethod with an accuracy of $ 96.15\\% $. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d4fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09511"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yanan Sun, Bing Xue, Mengjie Zhang", "title": "A Particle Swarm Optimization-based Flexible Convolutional Auto-Encoder for Image Classification. (arXiv:1712.05042v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.05042", "type": "text/html"}], "timestampUsec": "1513315287371381", "comments": [], "summary": {"content": "<p>Convolutional auto-encoders have shown their remarkable performance in \nstacking to deep convolutional neural networks for classifying image data \nduring past several years. However, they are unable to construct the \nstate-of-the-art convolutional neural networks due to their intrinsic \narchitectures. In this regard, we propose a flexible convolutional auto-encoder \nby eliminating the constraints on the numbers of convolutional layers and \npooling layers from the traditional convolutional auto-encoder. We also design \nan architecture discovery method by using particle swarm optimization, which is \ncapable of automatically searching for the optimal architectures of the \nproposed flexible convolutional auto-encoder with much less computational \nresource and without any manual intervention. We use the designed architecture \noptimization algorithm to test the proposed flexible convolutional auto-encoder \nthrough utilizing one graphic processing unit card on four extensively used \nimage classification datasets. Experimental results show that our work in this \npaper significantly outperform the peer competitors including the \nstate-of-the-art algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfe38", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05042"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yanan Sun, Gary G. Yen, Zhang Yi", "title": "Evolving Unsupervised Deep Neural Networks for Learning Meaningful Representations. (arXiv:1712.05043v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.05043", "type": "text/html"}], "timestampUsec": "1513315287371380", "comments": [], "summary": {"content": "<p>Deep Learning (DL) aims at learning the \\emph{meaningful representations}. A \nmeaningful representation refers to the one that gives rise to significant \nperformance improvement of associated Machine Learning (ML) tasks by replacing \nthe raw data as the input. However, optimal architecture design and model \nparameter estimation in DL algorithms are widely considered to be intractable. \nEvolutionary algorithms are much preferable for complex and non-convex problems \ndue to its inherent characteristics of gradient-free and insensitivity to local \noptimum. In this paper, we propose a computationally economical algorithm for \nevolving \\emph{unsupervised deep neural networks} to efficiently learn \n\\emph{meaningful representations}, which is very suitable in the current Big \nData era where sufficient labeled data for training is often expensive to \nacquire. In the proposed algorithm, finding an appropriate architecture and the \ninitialized parameter values for a ML task at hand is modeled by one \ncomputational efficient gene encoding approach, which is employed to \neffectively model the task with a large number of parameters. In addition, a \nlocal search strategy is incorporated to facilitate the exploitation search for \nfurther improving the performance. Furthermore, a small proportion labeled data \nis utilized during evolution search to guarantee the learnt representations to \nbe meaningful. The performance of the proposed algorithm has been thoroughly \ninvestigated over classification tasks. Specifically, error classification rate \non MNIST with $1.15\\%$ is reached by the proposed algorithm consistently, which \nis a very promising result against state-of-the-art unsupervised DL algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfe4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05043"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "V.I. Avrutskiy", "title": "Neural networks catching up with finite differences in solving partial differential equations in higher dimensions. (arXiv:1712.05067v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.05067", "type": "text/html"}], "timestampUsec": "1513315287371379", "comments": [], "summary": {"content": "<p>Fully connected multilayer perceptrons are used for obtaining numerical \nsolutions of partial differential equations in various dimensions. Independent \nvariables are fed into the input layer, and the output is considered as \nsolution's value. To train such a network one can use square of equation's \nresidual as a cost function and minimize it with respect to weights by gradient \ndescent. Following previously developed method, derivatives of the equation's \nresidual along random directions in space of independent variables are also \nadded to cost function. Similar procedure is known to produce nearly machine \nprecision results using less than 8 grid points per dimension for 2D case. The \nsame effect is observed here for higher dimensions: solutions are obtained on \nlow density grids, but maintain their precision in the entire region. Boundary \nvalue problems for linear and nonlinear Poisson equations are solved inside 2, \n3, 4, and 5 dimensional balls. Grids for linear cases have 40, 159, 512 and \n1536 points and for nonlinear 64, 350, 1536 and 6528 points respectively. In \nall cases maximum error is less than $8.8\\cdot10^{-6}$, and median error is \nless than $2.4\\cdot10^{-6}$. Very weak grid requirements enable neural networks \nto obtain solution of 5D linear problem within 22 minutes, whereas projected \nsolving time for finite differences on the same hardware is 50 minutes. Method \nis applied to second order equation, but requires little to none modifications \nto solve systems or higher order PDEs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfe5a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05067"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Freek Stulp, Pierre-Yves Oudeyer", "title": "Proximodistal Exploration in Motor Learning as an Emergent Property of Optimization. (arXiv:1712.05249v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.05249", "type": "text/html"}], "timestampUsec": "1513315287371378", "comments": [], "summary": {"content": "<p>To harness the complexity of their high-dimensional bodies during \nsensorimotor development, infants are guided by patterns of freezing and \nfreeing of degrees of freedom. For instance, when learning to reach, infants \nfree the degrees of freedom in their arm proximodistally, i.e. from joints that \nare closer to the body to those that are more distant. Here, we formulate and \nstudy computationally the hypothesis that such patterns can emerge \nspontaneously as the result of a family of stochastic optimization processes \n(evolution strategies with covariance-matrix adaptation), without an innate \nencoding of a maturational schedule. In particular, we present simulated \nexperiments with an arm where a computational learner progressively acquires \nreaching skills through adaptive exploration, and we show that a proximodistal \norganization appears spontaneously, which we denote PDFF (ProximoDistal \nFreezing and Freeing of degrees of freedom). We also compare this emergent \norganization between different arm morphologies -- from human-like to quite \nunnatural ones -- to study the effect of different kinematic structures on the \nemergence of PDFF. Keywords: human motor learning; proximo-distal exploration; \nstochastic optimization; modelling; evolution strategies; cross-entropy \nmethods; policy search; morphology.} \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfe69", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05249"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Miguel Aguilera, Manuel G. Bedia", "title": "Adaptation to criticality through organizational invariance in embodied agents. (arXiv:1712.05284v1 [nlin.AO])", "alternate": [{"href": "http://arxiv.org/abs/1712.05284", "type": "text/html"}], "timestampUsec": "1513315287371377", "comments": [], "summary": {"content": "<p>Many biological and cognitive systems do not operate deep within one or other \nregime of activity. Instead, they are poised at critical points located at \ntransitions of their parameter space. The pervasiveness of criticality suggests \nthat there may be general principles inducing this behaviour, yet there is no \nwell-founded theory for understanding how criticality is found at a wide range \nof levels and contexts. In this paper we present a general adaptive mechanism \nthat maintains an internal organizational structure in order to drive a system \ntowards critical points while it interacts with different environments. We \nimplement the mechanism in artificial embodied agents controlled by a neural \nnetwork maintaining a correlation structure randomly sampled from an Ising \nmodel at critical temperature. Agents are evaluated in two classical \nreinforcement learning scenarios: the Mountain Car and the Acrobot double \npendulum. In both cases the neural controller reaches a point of criticality, \nwhich coincides with a transition point between two regimes of the agent's \nbehaviour. These results suggest that adaptation to criticality could be used \nas a general adaptive mechanism in some circumstances, providing an alternative \nexplanation for the pervasive presence of criticality in biological and \ncognitive systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfe76", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05284"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sam Kriegman, Marcin Szubert, Josh C. Bongard, Christian Skalka", "title": "Evolving Spatially Aggregated Features from Satellite Imagery for Regional Modeling. (arXiv:1706.07888v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.07888", "type": "text/html"}], "timestampUsec": "1513315287371376", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a0e13db\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a0e13db&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Satellite imagery and remote sensing provide explanatory variables at \nrelatively high resolutions for modeling geospatial phenomena, yet regional \nsummaries are often desirable for analysis and actionable insight. In this \npaper, we propose a novel method of inducing spatial aggregations as a \ncomponent of the machine learning process, yielding regional model features \nwhose construction is driven by model prediction performance rather than prior \nassumptions. Our results demonstrate that Genetic Programming is particularly \nwell suited to this type of feature construction because it can automatically \nsynthesize appropriate aggregations, as well as better incorporate them into \npredictive models compared to other regression methods we tested. In our \nexperiments we consider a specific problem instance and real-world dataset \nrelevant to predicting snow properties in high-mountain Asia. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfe8a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.07888"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jindong Wang, Yiqiang Chen, Shuji Hao, Xiaohui Peng, Lisha Hu", "title": "Deep Learning for Sensor-based Activity Recognition: A Survey. (arXiv:1707.03502v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.03502", "type": "text/html"}], "timestampUsec": "1513315287371375", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a133502\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a133502&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Sensor-based activity recognition seeks the profound high-level knowledge \nabout human activities from multitudes of low-level sensor readings. \nConventional pattern recognition approaches have made tremendous progress in \nthe past years. However, those methods often heavily rely on heuristic \nhand-crafted feature extraction, which could hinder their generalization \nperformance. Additionally, existing methods are undermined for unsupervised and \nincremental learning tasks. Recently, the recent advancement of deep learning \nmakes it possible to perform automatic high-level feature extraction thus \nachieves promising performance in many areas. Since then, deep learning based \nmethods have been widely adopted for the sensor-based activity recognition \ntasks. This paper surveys the recent advance of deep learning based \nsensor-based activity recognition. We summarize existing literature from three \naspects: sensor modality, deep model, and application. We also present detailed \ninsights on existing work and propose grand challenges for future research. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfe8e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.03502"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ishai Rosenberg, Asaf Shabtai, Lior Rokach, Yuval Elovici", "title": "Generic Black-Box End-to-End Attack Against State of the Art API Call Based Malware Classifiers. (arXiv:1707.05970v3 [cs.CR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.05970", "type": "text/html"}], "timestampUsec": "1513315287371374", "comments": [], "summary": {"content": "<p>In this paper, we present a black-box attack against API call based machine \nlearning malware classifiers, focusing on generating adversarial API call \nsequences that would be misclassified by the classifier without affecting the \nmalware functionality. We show that this attack is effective against many \nclassifiers due to the transferability principle between RNN variants, feed \nforward DNNs, and traditional machine learning classifiers such as SVM. We \nfurther extend our attack against hybrid classifiers based on a combination of \nstatic and dynamic features, focusing on printable strings and API calls. \nFinally, we implement GADGET, a software framework to convert any malware \nbinary to a binary undetected by malware classifiers, using the proposed \nattack, without access to the malware source code. We conclude by discussing \npossible defense mechanisms against the attack. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfe9b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.05970"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yundong Zhang, Naveen Suda, Liangzhen Lai, Vikas Chandra", "title": "Hello Edge: Keyword Spotting on Microcontrollers. (arXiv:1711.07128v2 [cs.SD] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07128", "type": "text/html"}], "timestampUsec": "1513315287371373", "comments": [], "summary": {"content": "<p>Keyword spotting (KWS) is a critical component for enabling speech based user \ninteractions on smart devices. It requires real-time response and high accuracy \nfor good user experience. Recently, neural networks have become an attractive \nchoice for KWS architecture because of their superior accuracy compared to \ntraditional speech processing algorithms. Due to its always-on nature, KWS \napplication has highly constrained power budget and typically runs on tiny \nmicrocontrollers with limited memory and compute capability. The design of \nneural network architecture for KWS must consider these constraints. In this \nwork, we perform neural network architecture evaluation and exploration for \nrunning KWS on resource-constrained microcontrollers. We train various neural \nnetwork architectures for keyword spotting published in literature to compare \ntheir accuracy and memory/compute requirements. We show that it is possible to \noptimize these neural network architectures to fit within the memory and \ncompute constraints of microcontrollers without sacrificing accuracy. We \nfurther explore the depthwise separable convolutional neural network (DS-CNN) \nand compare it against other neural network architectures. DS-CNN achieves an \naccuracy of 95.4%, which is ~10% higher than the DNN model with similar number \nof parameters. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfeac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07128"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yi-Hsuan Tsai, Ming-Yu Liu, Deqing Sun, Ming-Hsuan Yang, Jan Kautz", "title": "Learning Binary Residual Representations for Domain-specific Video Streaming. (arXiv:1712.05087v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.05087", "type": "text/html"}], "timestampUsec": "1513315287371371", "comments": [], "summary": {"content": "<p>We study domain-specific video streaming. Specifically, we target a streaming \nsetting where the videos to be streamed from a server to a client are all in \nthe same domain and they have to be compressed to a small size for low-latency \ntransmission. Several popular video streaming services, such as the video game \nstreaming services of GeForce Now and Twitch, fall in this category. While \nconventional video compression standards such as H.264 are commonly used for \nthis task, we hypothesize that one can leverage the property that the videos \nare all in the same domain to achieve better video quality. Based on this \nhypothesis, we propose a novel video compression pipeline. Specifically, we \nfirst apply H.264 to compress domain-specific videos. We then train a novel \nbinary autoencoder to encode the leftover domain-specific residual information \nframe-by-frame into binary representations. These binary representations are \nthen compressed and sent to the client together with the H.264 stream. In our \nexperiments, we show that our pipeline yields consistent gains over standard \nH.264 compression across several benchmark datasets while using the same \nchannel bandwidth. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfeca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05087"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tom Bocklisch, Joey Faulker, Nick Pawlowski, Alan Nichol", "title": "Rasa: Open Source Language Understanding and Dialogue Management. (arXiv:1712.05181v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.05181", "type": "text/html"}], "timestampUsec": "1513315287371370", "comments": [], "summary": {"content": "<p>We introduce a pair of tools, Rasa NLU and Rasa Core, which are open source \npython libraries for building conversational software. Their purpose is to make \nmachine-learning based dialogue management and language understanding \naccessible to non-specialist software developers. In terms of design \nphilosophy, we aim for ease of use, and bootstrapping from minimal (or no) \ninitial training data. Both packages are extensively documented and ship with a \ncomprehensive suite of tests. The code is available at \nhttps://github.com/RasaHQ/ \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfedb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05181"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sachin Pawar, Girish K. Palshikar, Pushpak Bhattacharyya", "title": "Relation Extraction : A Survey. (arXiv:1712.05191v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.05191", "type": "text/html"}], "timestampUsec": "1513315287371369", "comments": [], "summary": {"content": "<p>With the advent of the Internet, large amount of digital text is generated \neveryday in the form of news articles, research publications, blogs, question \nanswering forums and social media. It is important to develop techniques for \nextracting information automatically from these documents, as lot of important \ninformation is hidden within them. This extracted information can be used to \nimprove access and management of knowledge hidden in large text corpora. \nSeveral applications such as Question Answering, Information Retrieval would \nbenefit from this information. Entities like persons and organizations, form \nthe most basic unit of the information. Occurrences of entities in a sentence \nare often linked through well-defined relations; e.g., occurrences of person \nand organization in a sentence may be linked through relations such as employed \nat. The task of Relation Extraction (RE) is to identify such relations \nautomatically. In this paper, we survey several important supervised, \nsemi-supervised and unsupervised RE techniques. We also cover the paradigms of \nOpen Information Extraction (OIE) and Distant Supervision. Finally, we describe \nsome of the recent trends in the RE techniques and possible future research \ndirections. This survey would be useful for three kinds of readers - i) \nNewcomers in the field who want to quickly learn about RE; ii) Researchers who \nwant to know how the various RE techniques evolved over time and what are \npossible future research directions and iii) Practitioners who just need to \nknow which RE technique works best in various settings. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfee3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05191"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Matthew Piekenbrock, Derek Doran", "title": "Intrinsic Point of Interest Discovery from Trajectory Data. (arXiv:1712.05247v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.05247", "type": "text/html"}], "timestampUsec": "1513315287371368", "comments": [], "summary": {"content": "<p>This paper presents a framework for intrinsic point of interest discovery \nfrom trajectory databases. Intrinsic points of interest are regions of a \ngeospatial area innately defined by the spatial and temporal aspects of \ntrajectory data, and can be of varying size, shape, and resolution. Any \ntrajectory database exhibits such points of interest, and hence are intrinsic, \nas compared to most other point of interest definitions which are said to be \nextrinsic, as they require trajectory metadata, external knowledge about the \nregion the trajectories are observed, or other application-specific \ninformation. Spatial and temporal aspects are qualities of any trajectory \ndatabase, making the framework applicable to data from any domain and of any \nresolution. The framework is developed under recent developments on the \nconsistency of nonparametric hierarchical density estimators and enables the \npossibility of formal statistical inference and evaluation over such intrinsic \npoints of interest. Comparisons of the POIs uncovered by the framework in \nsynthetic truth data to thousands of parameter settings for common POI \ndiscovery methods show a marked improvement in fidelity without the need to \ntune any parameters by hand. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfeed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05247"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gianluca Brero, S&#xe9;bastien Lahaie", "title": "A Bayesian Clearing Mechanism for Combinatorial Auctions. (arXiv:1712.05291v1 [cs.GT])", "alternate": [{"href": "http://arxiv.org/abs/1712.05291", "type": "text/html"}], "timestampUsec": "1513315287371367", "comments": [], "summary": {"content": "<p>We cast the problem of combinatorial auction design in a Bayesian framework \nin order to incorporate prior information into the auction process and minimize \nthe number of rounds to convergence. We first develop a generative model of \nagent valuations and market prices such that clearing prices become maximum a \nposteriori estimates given observed agent valuations. This generative model \nthen forms the basis of an auction process which alternates between refining \nestimates of agent valuations and computing candidate clearing prices. We \nprovide an implementation of the auction using assumed density filtering to \nestimate valuations and expectation maximization to compute prices. An \nempirical evaluation over a range of valuation domains demonstrates that our \nBayesian auction mechanism is highly competitive against the combinatorial \nclock auction in terms of rounds to convergence, even under the most favorable \nchoices of price increment for this baseline. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfef8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05291"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Damla Kizilay, Deniz T. Eliiyi, Pascal Van Hentenryck", "title": "Constraint and Mathematical Programming Models for Integrated Port Container Terminal Operations. (arXiv:1712.05302v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.05302", "type": "text/html"}], "timestampUsec": "1513315287371366", "comments": [], "summary": {"content": "<p>This paper considers the integrated problem of quay crane assignment, quay \ncrane scheduling, yard location assignment, and vehicle dispatching operations \nat a container terminal. The main objective is to minimize vessel turnover \ntimes and maximize the terminal throughput, which are key economic drivers in \nterminal operations. Due to their computational complexities, these problems \nare not optimized jointly in existing work. This paper revisits this limitation \nand proposes Mixed Integer Programming (MIP) and Constraint Programming (CP) \nmodels for the integrated problem, under some realistic assumptions. \nExperimental results show that the MIP formulation can only solve small \ninstances, while the CP model finds optimal solutions in reasonable times for \nrealistic instances derived from actual container terminal operations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfeff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05302"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Irene C&#xf3;rdoba S&#xe1;nchez, Concha Bielza, Pedro Larra&#xf1;aga", "title": "On Gaussian Markov models for conditional independence. (arXiv:1606.07282v3 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1606.07282", "type": "text/html"}], "timestampUsec": "1513315287371365", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a133d02\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a133d02&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Markov models lie at the interface between statistical independence in a \nprobability distribution and graph separation properties. We review model \nselection and estimation in directed and undirected Markov models with Gaussian \nparametrization, emphasizing the main similarities and differences. These two \nmodels are similar but not equivalent, although they share a common \nintersection. We present the existing results from a historical perspective, \ntaking into account the amount of literature existing from both the artificial \nintelligence and statistics research communities, where these models were \noriginated. We also discuss how the Gaussian assumption can be relaxed. We \nfinally point out the main areas of application where these Markov models are \nnowadays used. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff07", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1606.07282"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, Shimon Whiteson", "title": "Counterfactual Multi-Agent Policy Gradients. (arXiv:1705.08926v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08926", "type": "text/html"}], "timestampUsec": "1513315287371364", "comments": [], "summary": {"content": "<p>Cooperative multi-agent systems can be naturally used to model many real \nworld problems, such as network packet routing and the coordination of \nautonomous vehicles. There is a great need for new reinforcement learning \nmethods that can efficiently learn decentralised policies for such systems. To \nthis end, we propose a new multi-agent actor-critic method called \ncounterfactual multi-agent (COMA) policy gradients. COMA uses a centralised \ncritic to estimate the Q-function and decentralised actors to optimise the \nagents' policies. In addition, to address the challenges of multi-agent credit \nassignment, it uses a counterfactual baseline that marginalises out a single \nagent's action, while keeping the other agents' actions fixed. COMA also uses a \ncritic representation that allows the counterfactual baseline to be computed \nefficiently in a single forward pass. We evaluate COMA in the testbed of \nStarCraft unit micromanagement, using a decentralised variant with significant \npartial observability. COMA significantly improves average performance over \nother multi-agent actor-critic methods in this setting, and the best performing \nagents are competitive with state-of-the-art centralised controllers that get \naccess to the full state. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff11", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08926"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sirui Yao, Bert Huang", "title": "New Fairness Metrics for Recommendation that Embrace Differences. (arXiv:1706.09838v2 [cs.CY] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.09838", "type": "text/html"}], "timestampUsec": "1513315287371363", "comments": [], "summary": {"content": "<p>We study fairness in collaborative-filtering recommender systems, which are \nsensitive to discrimination that exists in historical data. Biased data can \nlead collaborative filtering methods to make unfair predictions against \nminority groups of users. We identify the insufficiency of existing fairness \nmetrics and propose four new metrics that address different forms of \nunfairness. These fairness metrics can be optimized by adding fairness terms to \nthe learning objective. Experiments on synthetic and real data show that our \nnew metrics can better measure fairness than the baseline, and that the \nfairness objectives effectively help reduce unfairness. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff16", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.09838"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jose M. Pe&#xf1;a", "title": "Unifying DAGs and UGs. (arXiv:1708.08722v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.08722", "type": "text/html"}], "timestampUsec": "1513315287371361", "comments": [], "summary": {"content": "<p>We introduce a new class of graphical models that generalizes \nLauritzen-Wermuth-Frydenberg chain graphs by relaxing the semi-directed \nacyclity constraint so that only directed cycles are forbidden. Moreover, up to \ntwo edges are allowed between any pair of nodes. Specifically, we present \nlocal, pairwise and global Markov properties for the new graphical models and \nprove their equivalence. We also present an equivalent factorization property. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff23", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.08722"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lei Lin, Zhengbing He, Srinivas Peeta, Xuejin Wen", "title": "Predicting Station-level Hourly Demands in a Large-scale Bike-sharing Network: A Graph Convolutional Neural Network Approach. (arXiv:1712.04997v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04997", "type": "text/html"}], "timestampUsec": "1513315287371360", "comments": [], "summary": {"content": "<p>Bike sharing is a vital piece in a modern multi-modal transportation system. \nHowever, it suffers from the bike unbalancing problem due to fluctuating \nspatial and temporal demands. Accurate bike sharing demand predictions can help \noperators to make optimal routes and schedules for bike redistributions, and \ntherefore enhance the system efficiency. In this study, we propose a novel \nGraph Convolutional Neural Network with Data-driven Graph Filter (GCNN-DDGF) \nmodel to predict station-level hourly demands in a large-scale bike-sharing \nnetwork. With each station as a vertex in the network, the new proposed \nGCNN-DDGF model is able to automatically learn the hidden correlations between \nstations, and thus overcomes a common issue reported in the previous studies, \ni.e., the quality and performance of GCNN models rely on the predefinition of \nthe adjacency matrix. To show the performance of the proposed model, this study \ncompares the GCNN-DDGF model with four GCNNs models, whose adjacency matrices \nare from different bike sharing system matrices including the Spatial Distance \nmatrix (SD), the Demand matrix (DE), the Average Trip Duration matrix (ATD) and \nthe Demand Correlation matrix (DC), respectively. The five types of GCNN models \nand the classic Support Vector Regression model are built on a Citi Bike \ndataset from New York City which includes 272 stations and over 28 million \ntransactions from 2013 to 2016. Results show that the GCNN-DDGF model has the \nlowest Root Mean Square Error, followed by the GCNN-DC model, and the GCNN-ATD \nmodel has the worst performance. Through a further examination, we find the \nlearned DDGF captures some similar information embedded in the SD, DE and DC \nmatrices, and it also uncovers more hidden heterogeneous pairwise correlations \nbetween stations that are not revealed by any of those matrices. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff28", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04997"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexandre Lacoste, Thomas Boquet, Negar Rostamzadeh, Boris Oreshki, Wonchang Chung, David Krueger", "title": "Deep Prior. (arXiv:1712.05016v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.05016", "type": "text/html"}], "timestampUsec": "1513315287371359", "comments": [], "summary": {"content": "<p>The recent literature on deep learning offers new tools to learn a rich \nprobability distribution over high dimensional data such as images or sounds. \nIn this work we investigate the possibility of learning the prior distribution \nover neural network parameters using such tools. Our resulting variational \nBayes algorithm generalizes well to new tasks, even when very few training \nexamples are provided. Furthermore, this learned prior allows the model to \nextrapolate correctly far from a given task's training data on a meta-dataset \nof periodic signals. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff2b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05016"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jinmian Ye, Linnan Wang, Guangxi Li, Di Chen, Shandian Zhe, Xinqi Chu, Zenglin Xu", "title": "Learning Compact Recurrent Neural Networks with Block-Term Tensor Decomposition. (arXiv:1712.05134v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.05134", "type": "text/html"}], "timestampUsec": "1513315287371358", "comments": [], "summary": {"content": "<p>Recurrent Neural Networks (RNNs) are powerful sequence modeling tools. \nHowever, when dealing with high dimensional inputs, the training of RNNs \nbecomes computational expensive due to the large number of model parameters. \nThis hinders RNNs from solving many important computer vision tasks, such as \nAction Recognition in Videos and Image Captioning. To overcome this problem, we \npropose a compact and flexible structure, namely Block-Term tensor \ndecomposition, which greatly reduces the parameters of RNNs and improves their \ntraining efficiency. Compared with alternative low-rank approximations, such as \ntensor-train RNN (TT-RNN), our method, Block-Term RNN (BT-RNN), is not only \nmore concise (when using the same rank), but also able to attain a better \napproximation to the original RNNs with much fewer parameters. On three \nchallenging tasks, including Action Recognition in Videos, Image Captioning and \nImage Generation, BT-RNN outperforms TT-RNN and the standard RNN in terms of \nboth prediction accuracy and convergence rate. Specifically, BT-LSTM utilizes \n17,388 times fewer parameters than the standard LSTM to achieve an accuracy \nimprovement over 15.6\\% in the Action Recognition task on the UCF11 dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05134"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ingo Steinwart, Johanna F. Ziegel", "title": "Strictly proper kernel scores and characteristic kernels on compact spaces. (arXiv:1712.05279v1 [math.FA])", "alternate": [{"href": "http://arxiv.org/abs/1712.05279", "type": "text/html"}], "timestampUsec": "1513315287371357", "comments": [], "summary": {"content": "<p>Strictly proper kernel scores are well-known tool in probabilistic \nforecasting, while characteristic kernels have been extensively investigated in \nthe machine learning literature. We first show that both notions coincide, so \nthat insights from one part of the literature can be used in the other. We then \nshow that the metric induced by a characteristic kernel cannot reliably \ndistinguish between distributions that are far apart in the total variation \nnorm as soon as the underlying space of measures is infinite dimensional. In \naddition, we provide a characterization of characteristic kernels in terms of \neigenvalues and -functions and apply this characterization to the case of \ncontinuous kernels on (locally) compact spaces. In the compact case we further \nshow that characteristic kernels exist if and only if the space is metrizable. \nAs special cases of our general theory we investigate translation-invariant \nkernels on compact Abelian groups and isotropic kernels on spheres. The latter \nare of particular interest for forecast evaluation of probabilistic predictions \non spherical domains as frequently encountered in meteorology and climatology. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff37", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05279"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chung-Cheng Chiu, Colin Raffel", "title": "Monotonic Chunkwise Attention. (arXiv:1712.05382v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.05382", "type": "text/html"}], "timestampUsec": "1513315287371356", "comments": [], "summary": {"content": "<p>Sequence-to-sequence models with soft attention have been successfully \napplied to a wide variety of problems, but their decoding process incurs a \nquadratic time and space cost and is inapplicable to real-time sequence \ntransduction. To address these issues, we propose Monotonic Chunkwise Attention \n(MoChA), which adaptively splits the input sequence into small chunks over \nwhich soft attention is computed. We show that models utilizing MoChA can be \ntrained efficiently with standard backpropagation while allowing online and \nlinear-time decoding at test time. When applied to online speech recognition, \nwe obtain state-of-the-art results and match the performance of a model using \nan offline soft attention mechanism. In document summarization experiments \nwhere we do not expect monotonic alignments, we show significantly improved \nperformance compared to a baseline monotonic attention-based model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff3c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05382"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "John Duchi, Hongseok Namkoong", "title": "Variance-based regularization with convex objectives. (arXiv:1610.02581v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.02581", "type": "text/html"}], "timestampUsec": "1513315287371355", "comments": [], "summary": {"content": "<p>We develop an approach to risk minimization and stochastic optimization that \nprovides a convex surrogate for variance, allowing near-optimal and \ncomputationally efficient trading between approximation and estimation error. \nOur approach builds off of techniques for distributionally robust optimization \nand Owen's empirical likelihood, and we provide a number of finite-sample and \nasymptotic results characterizing the theoretical performance of the estimator. \nIn particular, we show that our procedure comes with certificates of \noptimality, achieving (in some scenarios) faster rates of convergence than \nempirical risk minimization by virtue of automatically balancing bias and \nvariance. We give corroborating empirical evidence showing that in practice, \nthe estimator indeed trades between variance and absolute performance on a \ntraining sample, improving out-of-sample (test) performance over standard \nempirical risk minimization for a number of classification problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff56", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.02581"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ricardo Pio Monti, Christoforos Anagnostopoulos, Giovanni Montana", "title": "Adaptive regularization for Lasso models in the context of non-stationary data streams. (arXiv:1610.09127v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.09127", "type": "text/html"}], "timestampUsec": "1513315287371354", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a133f7e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a133f7e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Large scale, streaming datasets are ubiquitous in modern machine learning. \nStreaming algorithms must be scalable, amenable to incremental training and \nrobust to the presence of non-stationarity. In this work consider the problem \nof learning $\\ell_1$ regularized linear models in the context of streaming \ndata. In particular, the focus of this work revolves around how to select the \nregularization parameter when data arrives sequentially and the underlying \ndistribution is non-stationary (implying the choice of optimal regularization \nparameter is itself time-varying). We propose a framework through which to \ninfer an adaptive regularization parameter. Our approach employs an $\\ell_1$ \npenalty constraint where the corresponding sparsity parameter is iteratively \nupdated via stochastic gradient descent. This serves to reformulate the choice \nof regularization parameter in a principled framework for online learning. The \nproposed method is derived for linear regression and subsequently extended to \ngeneralized linear models. We validate our approach using simulated and real \ndatasets and present an application to a neuroimaging dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff66", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.09127"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Elizabeth Hou, Kumar Sricharan, Alfred O. Hero", "title": "Latent Laplacian Maximum Entropy Discrimination for Detection of High-Utility Anomalies. (arXiv:1702.05148v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1702.05148", "type": "text/html"}], "timestampUsec": "1513315287371353", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a180b86\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a180b86&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Data-driven anomaly detection methods suffer from the drawback of detecting \nall instances that are statistically rare, irrespective of whether the detected \ninstances have real-world significance or not. In this paper, we are interested \nin the problem of specifically detecting anomalous instances that are known to \nhave high real-world utility, while ignoring the low-utility statistically \nanomalous instances. To this end, we propose a novel method called Latent \nLaplacian Maximum Entropy Discrimination (LatLapMED) as a potential solution. \nThis method uses the EM algorithm to simultaneously incorporate the Geometric \nEntropy Minimization principle for identifying statistical anomalies, and the \nMaximum Entropy Discrimination principle to incorporate utility labels, in \norder to detect high-utility anomalies. We apply our method in both simulated \nand real datasets to demonstrate that it has superior performance over existing \nalternatives that independently pre-process with unsupervised anomaly detection \nalgorithms before classifying. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff70", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1702.05148"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, Alistair Stewart", "title": "Being Robust (in High Dimensions) Can Be Practical. (arXiv:1703.00893v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.00893", "type": "text/html"}], "timestampUsec": "1513315287371352", "comments": [], "summary": {"content": "<p>Robust estimation is much more challenging in high dimensions than it is in \none dimension: Most techniques either lead to intractable optimization problems \nor estimators that can tolerate only a tiny fraction of errors. Recent work in \ntheoretical computer science has shown that, in appropriate distributional \nmodels, it is possible to robustly estimate the mean and covariance with \npolynomial time algorithms that can tolerate a constant fraction of \ncorruptions, independent of the dimension. However, the sample and time \ncomplexity of these algorithms is prohibitively large for high-dimensional \napplications. In this work, we address both of these issues by establishing \nsample complexity bounds that are optimal, up to logarithmic factors, as well \nas giving various refinements that allow the algorithms to tolerate a much \nlarger fraction of corruptions. Finally, we show on both synthetic and real \ndata that our algorithms have state-of-the-art performance and suddenly make \nhigh-dimensional robust estimation a realistic possibility. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff79", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.00893"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jack Baker, Paul Fearnhead, Emily B. Fox, Christopher Nemeth", "title": "Control Variates for Stochastic Gradient MCMC. (arXiv:1706.05439v2 [stat.CO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.05439", "type": "text/html"}], "timestampUsec": "1513315287371351", "comments": [], "summary": {"content": "<p>It is well known that Markov chain Monte Carlo (MCMC) methods scale poorly \nwith dataset size. A popular class of methods for solving this issue is \nstochastic gradient MCMC. These methods use a noisy estimate of the gradient of \nthe log posterior, which reduces the per iteration computational cost of the \nalgorithm. Despite this, there are a number of results suggesting that \nstochastic gradient Langevin dynamics (SGLD), probably the most popular of \nthese methods, still has computational cost proportional to the dataset size. \nWe suggest an alternative log posterior gradient estimate for stochastic \ngradient MCMC, which uses control variates to reduce the variance. We analyse \nSGLD using this gradient estimate, and show that, under log-concavity \nassumptions on the target distribution, the computational cost required for a \ngiven level of accuracy is independent of the dataset size. Next we show that a \ndifferent control variate technique, known as zero variance control variates \ncan be applied to SGMCMC algorithms for free. This post-processing step \nimproves the inference of the algorithm by reducing the variance of the MCMC \noutput. Zero variance control variates rely on the gradient of the log \nposterior; we explore how the variance reduction is affected by replacing this \nwith the noisy gradient estimate calculated by SGMCMC. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff88", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.05439"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Torsten Hothorn", "title": "Top-down Transformation Choice. (arXiv:1706.08269v2 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.08269", "type": "text/html"}], "timestampUsec": "1513315287371350", "comments": [], "summary": {"content": "<p>Simple models are preferred over complex models, but over-simplistic models \ncould lead to erroneous interpretations. The classical approach is to start \nwith a simple model, whose shortcomings are assessed in residual-based model \ndiagnostics. Eventually, one increases the complexity of this initial overly \nsimple model and obtains a better-fitting model. I illustrate how \ntransformation analysis can be used as an alternative approach to model choice. \nInstead of adding complexity to simple models, step-wise complexity reduction \nis used to help identify simpler and better-interpretable models. As an \nexample, body mass index distributions in Switzerland are modelled by means of \ntransformation models to understand the impact of sex, age, smoking and other \nlifestyle factors on a person's body mass index. In this process, I searched \nfor a compromise between model fit and model interpretability. Special emphasis \nis given to the understanding of the connections between transformation models \nof increasing complexity. The models used in this analysis ranged from \nevergreens, such as the normal linear regression model with constant variance, \nto novel models with extremely flexible conditional distribution functions, \nsuch as transformation trees and transformation forests. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff8c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.08269"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Carolin Lawrence, Artem Sokolov, Stefan Riezler", "title": "Counterfactual Learning from Bandit Feedback under Deterministic Logging: A Case Study in Statistical Machine Translation. (arXiv:1707.09118v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.09118", "type": "text/html"}], "timestampUsec": "1513315287371349", "comments": [], "summary": {"content": "<p>The goal of counterfactual learning for statistical machine translation (SMT) \nis to optimize a target SMT system from logged data that consist of user \nfeedback to translations that were predicted by another, historic SMT system. A \nchallenge arises by the fact that risk-averse commercial SMT systems \ndeterministically log the most probable translation. The lack of sufficient \nexploration of the SMT output space seemingly contradicts the theoretical \nrequirements for counterfactual learning. We show that counterfactual learning \nfrom deterministic bandit logs is possible nevertheless by smoothing out \ndeterministic components in learning. This can be achieved by additive and \nmultiplicative control variates that avoid degenerate behavior in empirical \nrisk minimization. Our simulation experiments show improvements of up to 2 BLEU \npoints by counterfactual learning from deterministic bandit feedback. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff90", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.09118"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jack Baker, Paul Fearnhead, Emily B. Fox, Christopher Nemeth", "title": "sgmcmc: An R Package for Stochastic Gradient Markov Chain Monte Carlo. (arXiv:1710.00578v2 [stat.CO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.00578", "type": "text/html"}], "timestampUsec": "1513315287371348", "comments": [], "summary": {"content": "<p>This paper introduces the R package sgmcmc; which can be used for Bayesian \ninference on problems with large datasets using stochastic gradient Markov \nchain Monte Carlo (SGMCMC). Traditional Markov chain Monte Carlo (MCMC) \nmethods, such as Metropolis-Hastings, are known to run prohibitively slowly as \nthe dataset size increases. SGMCMC solves this issue by only using a subset of \ndata at each iteration. SGMCMC requires calculating gradients of the log \nlikelihood and log priors, which can be time consuming and error prone to \nperform by hand. The sgmcmc package calculates these gradients itself using \nautomatic differentiation, making the implementation of these methods much \neasier. To do this, the package uses the software library TensorFlow, which has \na variety of statistical distributions and mathematical operations as standard, \nmeaning a wide class of models can be built using this framework. SGMCMC has \nbecome widely adopted in the machine learning literature, but less so in the \nstatistics community. We believe this may be partly due to lack of software; \nthis package aims to bridge this gap. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff95", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.00578"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Adam N. Elmachtoub, Paul Grigas", "title": "Smart \"Predict, then Optimize\". (arXiv:1710.08005v2 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.08005", "type": "text/html"}], "timestampUsec": "1513315287371347", "comments": [], "summary": {"content": "<p>Many real-world analytics problems involve two significant challenges: \nprediction and optimization. Due to the typically complex nature of each \nchallenge, the standard paradigm is to predict, then optimize. By and large, \nmachine learning tools are intended to minimize prediction error and do not \naccount for how the predictions will be used in a downstream optimization \nproblem. In contrast, we propose a new and very general framework, called Smart \n\"Predict, then Optimize\" (SPO), which directly leverages the optimization \nproblem structure, i.e., its objective and constraints, for designing \nsuccessful analytics tools. A key component of our framework is the SPO loss \nfunction, which measures the quality of a prediction by comparing the objective \nvalues of the solutions generated using the predicted and observed parameters, \nrespectively. Training a model with respect to the SPO loss is computationally \nchallenging, and therefore we also develop a surrogate loss function, called \nthe SPO+ loss, which upper bounds the SPO loss, has desirable convexity \nproperties, and is statistically consistent under mild conditions. We also \npropose a stochastic gradient descent algorithm which allows for situations in \nwhich the number of training samples is large, model regularization is desired, \nand/or the optimization problem of interest is nonlinear or integer. Finally, \nwe perform computational experiments to empirically verify the success of our \nSPO framework in comparison to the standard predict-then-optimize approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff9d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.08005"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Carolin Lawrence, Pratik Gajane, Stefan Riezler", "title": "Counterfactual Learning for Machine Translation: Degeneracies and Solutions. (arXiv:1711.08621v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08621", "type": "text/html"}], "timestampUsec": "1513315287371346", "comments": [], "summary": {"content": "<p>Counterfactual learning is a natural scenario to improve web-based machine \ntranslation services by offline learning from feedback logged during user \ninteractions. In order to avoid the risk of showing inferior translations to \nusers, in such scenarios mostly exploration-free deterministic logging policies \nare in place. We analyze possible degeneracies of inverse and reweighted \npropensity scoring estimators, in stochastic and deterministic settings, and \nrelate them to recently proposed techniques for counterfactual learning under \ndeterministic logging. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bffa5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08621"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Cong Ma, Kaizheng Wang, Yuejie Chi, Yuxin Chen", "title": "Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval, Matrix Completion and Blind Deconvolution. (arXiv:1711.10467v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10467", "type": "text/html"}], "timestampUsec": "1513315287371345", "comments": [], "summary": {"content": "<p>Recent years have seen a flurry of activities in designing provably efficient \nnonconvex procedures for solving statistical estimation problems. Due to the \nhighly nonconvex nature of the empirical loss, state-of-the-art procedures \noften require proper regularization (e.g. trimming, regularized cost, \nprojection) in order to guarantee fast convergence. For vanilla procedures such \nas gradient descent, however, prior theory either recommends highly \nconservative learning rates to avoid overshooting, or completely lacks \nperformance guarantees. \n</p> \n<p>This paper uncovers a striking phenomenon in nonconvex optimization: even in \nthe absence of explicit regularization, gradient descent enforces proper \nregularization implicitly under various statistical models. In fact, gradient \ndescent follows a trajectory staying within a basin that enjoys nice geometry, \nconsisting of points incoherent with the sampling mechanism. This \"implicit \nregularization\" feature allows gradient descent to proceed in a far more \naggressive fashion without overshooting, which in turn results in substantial \ncomputational savings. Focusing on three fundamental statistical estimation \nproblems, i.e. phase retrieval, low-rank matrix completion, and blind \ndeconvolution, we establish that gradient descent achieves near-optimal \nstatistical and computational guarantees without explicit regularization. In \nparticular, by marrying statistical modeling with generic optimization theory, \nwe develop a general recipe for analyzing the trajectories of iterative \nalgorithms via a leave-one-out perturbation argument. As a byproduct, for noisy \nmatrix completion, we demonstrate that gradient descent achieves near-optimal \nerror control --- measured entrywise and by the spectral norm --- which might \nbe of independent interest. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bffb7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10467"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Khondkar Islam, Pouyan Ahmadi, Salman Yousaf", "title": "Assessment Formats and Student Learning Performance: What is the Relation?. (arXiv:1711.10396v1 [physics.ed-ph] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10396", "type": "text/html"}], "timestampUsec": "1513315287371342", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a180f18\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a180f18&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Although compelling assessments have been examined in recent years, more \nstudies are required to yield a better understanding of the several methods \nwhere assessment techniques significantly affect student learning process. Most \nof the educational research in this area does not consider demographics data, \ndiffering methodologies, and notable sample size. To address these drawbacks, \nthe objective of our study is to analyse student learning outcomes of multiple \nassessment formats for a web-facilitated in-class section with an asynchronous \nonline class of a core data communications course in the Undergraduate IT \nprogram of the Information Sciences and Technology (IST) Department at George \nMason University (GMU). In this study, students were evaluated based on course \nassessments such as home and lab assignments, skill-based assessments, and \ntraditional midterm and final exams across all four sections of the course. All \nsections have equivalent content, assessments, and teaching methodologies. \nStudent demographics such as exam type and location preferences are considered \nin our study to determine whether they have any impact on their learning \napproach. Large amount of data from the learning management system (LMS), \nBlackboard (BB) Learn, had to be examined to compare the results of several \nassessment outcomes for all students within their respective section and \namongst students of other sections. To investigate the effect of dissimilar \nassessment formats on student performance, we had to correlate individual \nquestion formats with the overall course grade. The results show that \ncollective assessment formats allow students to be effective in demonstrating \ntheir knowledge. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bffdc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10396"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "V.I. Avrutskiy", "title": "Enhancing approximation abilities of neural networks by training derivatives. (arXiv:1712.04473v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.04473", "type": "text/html"}], "timestampUsec": "1513228697215439", "comments": [], "summary": {"content": "<p>Method for increasing precision of feedforward networks is presented. With \nthe aid of it they can serve as a better tool for describing smooth functions. \nNamely, it is shown that when training uses derivatives of target function up \nto the fourth order, approximation can be nearly machine precise. It is \ndemonstrated in a number of cases: 2D function approximation, training \nautoencoder to compress 3D spiral into 1D, and solving 2D boundary value \nproblem for Poisson equation with nonlinear source. In the first case cost \nfunction in addition to squared difference between output and target contains \nsquared differences between their derivatives with respect to input variables. \nTraining autoencoder is similar, but differentiation is done with respect to \nparameter that generates the spiral. Supplied with derivatives up to the fourth \nthe method is found to be 30-200 times more accurate than regular training \nprovided networks are of sufficient size and depth. Solving PDE is more \npractical since higher derivatives are not calculated beforehand, but \ninformation about them is extracted from the equation itself. Classical \napproach is to put perceptron in place of unknown function, choose the cost as \nsquared residual and to minimize it with respect to weights. This would ensure \nthat equation holds within some margin of error. Additional terms used in cost \nfunction are squared derivatives of the residual with respect to independent \nvariables. Supplied with terms up to the second order the method is found to be \n5 times more accurate. Efficient GPU version of algorithm is proposed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96aa0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04473"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David M. Schwartz, O. Ozan Koyluoglu", "title": "On the organization of grid and place cells: Neural de-noising via subspace learning. (arXiv:1712.04602v1 [q-bio.NC])", "alternate": [{"href": "http://arxiv.org/abs/1712.04602", "type": "text/html"}], "timestampUsec": "1513228697215438", "comments": [], "summary": {"content": "<p>Place cells in the hippocampus are active when an animal visits a certain \nlocations (referred to as place fields) within an environment and remain silent \notherwise. Grid cells in the medial entorhinal cortex (MEC) respond at multiple \nlocations, with firing fields that exhibit a hexagonally symmetric periodic \npattern. The joint activity of grid and place cell populations, as a function \nof location, forms a neural code for space. An ensemble of codes, for a given \nset of parameters, is generated by selecting grid and place cell population and \ntuning curve parameters. For each ensemble, codewords are generated by \nstimulating a network with a discrete set of locations. In this manuscript, we \ndevelop an understanding of the relationships between coding theoretic \nproperties of these combined populations and code construction parameters. \nThese observations are revisited by measuring the performances of biologically \nrealizable algorithms (e.g. neural bit-flipping) implemented by a network of \nplace and grid cell populations, as well as interneurons, which perform \nde-noising operations. Simulations demonstrate that de-noising mechanisms \nanalyzed here can significantly improve fidelity of this neural representation \nof space. Further, patterns observed in connectivity of each population of \nsimulated cells suggest the existence of heretofore unobserved neurobiological \nphenomena. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96aba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04602"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chase Gaudet, Anthony Maida", "title": "Deep Quaternion Networks. (arXiv:1712.04604v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.04604", "type": "text/html"}], "timestampUsec": "1513228697215437", "comments": [], "summary": {"content": "<p>The field of deep learning has seen significant advancement in recent years. \nHowever, much of the existing work has been focused on real-valued numbers. \nRecent work has shown that a deep learning system using the complex numbers can \nbe deeper for a set parameter budget compared to its real-valued counterpart. \nIn this work, we explore the benefits of generalizing one step further into the \nhyper-complex numbers, quaternions specifically, and provide the architecture \ncomponents needed to build deep quaternion networks. We go over quaternion \nconvolutions, present a quaternion weight initialization scheme, and present \nalgorithms for quaternion batch-normalization. These pieces are tested by \nend-to-end training on the CIFAR-10 and CIFAR-100 data sets to show the \nimproved convergence to a real-valued network. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96ad7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04604"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gabriele Scheler", "title": "Logarithmic distributions prove that intrinsic learning is Hebbian. (arXiv:1410.5610v3 [q-bio.NC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1410.5610", "type": "text/html"}], "timestampUsec": "1513228697215436", "comments": [], "summary": {"content": "<p>In this paper, we present data for the lognormal distributions of spike \nrates, synaptic weights and intrinsic excitability (gain) for neurons in \nvarious brain areas, such as auditory or visual cortex, hippocampus, \ncerebellum, striatum, midbrain nuclei. We find a remarkable consistency of \nheavy-tailed, specifically lognormal, distributions for rates, weights and \ngains in all brain areas examined. The difference between strongly recurrent \nand feed-forward connectivity (cortex vs. striatum and cerebellum), \nneurotransmitter (GABA (striatum) or glutamate (cortex)) or the level of \nactivation (low in cortex, high in Purkinje cells and midbrain nuclei) turns \nout to be irrelevant for this feature. Logarithmic scale distribution of \nweights and gains appears to be a general, functional property in all cases \nanalyzed. We then created a generic neural model to investigate adaptive \nlearning rules that create and maintain lognormal distributions. We \nconclusively demonstrate that not only weights, but also intrinsic gains, need \nto have strong Hebbian learning in order to produce and maintain the \nexperimentally attested distributions. This provides a solution to the \nlong-standing question about the type of plasticity exhibited by intrinsic \nexcitability. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96ae1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1410.5610"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Leslie N. Smith, Nicholay Topin", "title": "Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates. (arXiv:1708.07120v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.07120", "type": "text/html"}], "timestampUsec": "1513228697215435", "comments": [], "summary": {"content": "<p>In this paper, we show a phenomenon, which we named \"super-convergence\", \nwhere residual networks can be trained using an order of magnitude fewer \niterations than is used with standard training methods. The existence of \nsuper-convergence is relevant to understanding why deep networks generalize \nwell. One of the key elements of super-convergence is training with cyclical \nlearning rates and a large maximum learning rate. Furthermore, we present \nevidence that training with large learning rates improves performance by \nregularizing the network. In addition, we show that super-convergence provides \na greater boost in performance relative to standard training when the amount of \nlabeled training data is limited. We also derive a simplification of the \nHessian Free optimization method to compute an estimate of the optimal learning \nrate. The architectures and code to replicate the figures in this paper are \navailable at github.com/lnsmith54/super-convergence. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96aea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.07120"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Milad Makkie, Heng Huang, Yu Zhao, Athanasios V. Vasilakos, Tianming Liu", "title": "Fast and Scalable Distributed Deep Convolutional Autoencoder for fMRI Big Data Analytics. (arXiv:1710.08961v2 [cs.DC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.08961", "type": "text/html"}], "timestampUsec": "1513228697215434", "comments": [], "summary": {"content": "<p>In recent years, analyzing task-based fMRI (tfMRI) data has become an \nessential tool for understanding brain function and networks. However, due to \nthe sheer size of tfMRI data, its intrinsic complex structure, and lack of \nground truth of underlying neural activities, modeling tfMRI data is hard and \nchallenging. Previously proposed data-modeling methods including Independent \nComponent Analysis (ICA) and Sparse Dictionary Learning only provided a weakly \nestablished model based on blind source separation under the strong assumption \nthat original fMRI signals could be linearly decomposed into time series \ncomponents with corresponding spatial maps. Meanwhile, analyzing and learning a \nlarge amount of tfMRI data from a variety of subjects has been shown to be very \ndemanding but yet challenging even with technological advances in computational \nhardware. Given the Convolutional Neural Network (CNN), a robust method for \nlearning high-level abstractions from low-level data such as tfMRI time series, \nin this work we propose a fast and scalable novel framework for distributed \ndeep Convolutional Autoencoder model. This model aims to both learn the complex \nhierarchical structure of the tfMRI data and to leverage the processing power \nof multiple GPUs in a distributed fashion. To implement such a model, we have \ncreated an enhanced processing pipeline on the top of Apache Spark and \nTensorflow library, leveraging from a very large cluster of GPU machines. \nExperimental data from applying the model on the Human Connectome Project (HCP) \nshow that the proposed model is efficient and scalable toward tfMRI big data \nanalytics, thus enabling data-driven extraction of hierarchical neuroscientific \ninformation from massive fMRI big data in the future. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96aee", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.08961"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, David Barber", "title": "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning. (arXiv:1711.01577v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.01577", "type": "text/html"}], "timestampUsec": "1513228697215433", "comments": [], "summary": {"content": "<p>Long Short-Term Memory (LSTM) is a popular approach to boosting the ability \nof Recurrent Neural Networks to store longer term temporal information. The \ncapacity of an LSTM network can be increased by widening and adding layers. \nHowever, usually the former introduces additional parameters, while the latter \nincreases the runtime. As an alternative we propose the Tensorized LSTM in \nwhich the hidden states are represented by tensors and updated via a \ncross-layer convolution. By increasing the tensor size, the network can be \nwidened efficiently without additional parameters since the parameters are \nshared across different locations in the tensor; by delaying the output, the \nnetwork can be deepened implicitly with little additional runtime since deep \ncomputations for each timestep are merged into temporal computations of the \nsequence. Experiments conducted on five challenging sequence learning tasks \nshow the potential of the proposed model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96afc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.01577"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Bo Wu, Wen-Huang Cheng, Yongdong Zhang, Qiushi Huang, Jintao Li, Tao Mei", "title": "Sequential Prediction of Social Media Popularity with Deep Temporal Context Networks. (arXiv:1712.04443v1 [cs.SI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04443", "type": "text/html"}], "timestampUsec": "1513228697215432", "comments": [], "summary": {"content": "<p>Prediction of popularity has profound impact for social media, since it \noffers opportunities to reveal individual preference and public attention from \nevolutionary social systems. Previous research, although achieves promising \nresults, neglects one distinctive characteristic of social data, i.e., \nsequentiality. For example, the popularity of online content is generated over \ntime with sequential post streams of social media. To investigate the \nsequential prediction of popularity, we propose a novel prediction framework \ncalled Deep Temporal Context Networks (DTCN) by incorporating both temporal \ncontext and temporal attention into account. Our DTCN contains three main \ncomponents, from embedding, learning to predicting. With a joint embedding \nnetwork, we obtain a unified deep representation of multi-modal user-post data \nin a common embedding space. Then, based on the embedded data sequence over \ntime, temporal context learning attempts to recurrently learn two adaptive \ntemporal contexts for sequential popularity. Finally, a novel temporal \nattention is designed to predict new popularity (the popularity of a new \nuser-post pair) with temporal coherence across multiple time-scales. \nExperiments on our released image dataset with about 600K Flickr photos \ndemonstrate that DTCN outperforms state-of-the-art deep prediction algorithms, \nwith an average of 21.51% relative performance improvement in the popularity \nprediction (Spearman Ranking Correlation). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b03", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04443"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Son-Il Kwak, Oh-Chol Gwon, Chung-Jin Kwak", "title": "Consideration on Example 2 of \"An Algorithm of General Fuzzy InferenceWith The Reductive Property\". (arXiv:1712.04596v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04596", "type": "text/html"}], "timestampUsec": "1513228697215431", "comments": [], "summary": {"content": "<p>In this paper, we will show that (1) the results about the fuzzy reasoning \nalgoritm obtained in the paper \"Computer Sciences Vol. 34, No.4, pp.145-148, \n2007\" according to the paper \"IEEE Transactions On systems, Man and \ncybernetics, 18, pp.1049-1056, 1988\" are correct; (2) example 2 in the paper \n\"An Algorithm of General Fuzzy Inference With The Reductive Property\" presented \nby He Ying-Si, Quan Hai-Jin and Deng Hui-Wen according to the paper \"An \napproximate analogical reasoning approach based on similarity measures\" \npresented by Tursken I.B. and Zhong zhao is incorrect; (3) the mistakes in \ntheir paper are modified and then a calculation example of FMT is supplemented. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b0d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04596"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jinyoung Choi, Beom-Jin Lee, Byoung-Tak Zhang", "title": "Multi-focus Attention Network for Efficient Deep Reinforcement Learning. (arXiv:1712.04603v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04603", "type": "text/html"}], "timestampUsec": "1513228697215430", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a18122e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a18122e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep reinforcement learning (DRL) has shown incredible performance in \nlearning various tasks to the human level. However, unlike human perception, \ncurrent DRL models connect the entire low-level sensory input to the \nstate-action values rather than exploiting the relationship between and among \nentities that constitute the sensory input. Because of this difference, DRL \nneeds vast amount of experience samples to learn. In this paper, we propose a \nMulti-focus Attention Network (MANet) which mimics human ability to spatially \nabstract the low-level sensory input into multiple entities and attend to them \nsimultaneously. The proposed method first divides the low-level input into \nseveral segments which we refer to as partial states. After this segmentation, \nparallel attention layers attend to the partial states relevant to solving the \ntask. Our model estimates state-action values using these attended partial \nstates. In our experiments, MANet attains highest scores with significantly \nless experience samples. Additionally, the model shows higher performance \ncompared to the Deep Q-network and the single attention model as benchmarks. \nFurthermore, we extend our model to attentive communication model for \nperforming multi-agent cooperative tasks. In multi-agent cooperative task \nexperiments, our model shows 20% faster learning than existing state-of-the-art \nmodel. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b15", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04603"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Igor Halperin", "title": "Inverse Reinforcement Learning for Marketing. (arXiv:1712.04612v1 [q-fin.CP])", "alternate": [{"href": "http://arxiv.org/abs/1712.04612", "type": "text/html"}], "timestampUsec": "1513228697215429", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a1cfddd\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a1cfddd&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Learning customer preferences from an observed behaviour is an important \ntopic in the marketing literature. Structural models typically model \nforward-looking customers or firms as utility-maximizing agents whose utility \nis estimated using methods of Stochastic Optimal Control. We suggest an \nalternative approach to study dynamic consumer demand, based on Inverse \nReinforcement Learning (IRL). We develop a version of the Maximum Entropy IRL \nthat leads to a highly tractable model formulation that amounts to \nlow-dimensional convex optimization in the search for optimal model parameters. \nUsing simulations of consumer demand, we show that observational noise for \nidentical customers can be easily confused with an apparent consumer \nheterogeneity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04612"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Subhash Kak", "title": "Reasoning in Systems with Elements that Randomly Switch Characteristics. (arXiv:1712.04909v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04909", "type": "text/html"}], "timestampUsec": "1513228697215428", "comments": [], "summary": {"content": "<p>We examine the issue of stability of probability in reasoning about complex \nsystems with uncertainty in structure. Normally, propositions are viewed as \nprobability functions on an abstract random graph where it is implicitly \nassumed that the nodes of the graph have stable properties. But what if some of \nthe nodes change their characteristics? This is a situation that cannot be \ncovered by abstractions of either static or dynamic sets when these changes \ntake place at regular intervals. We propose the use of sets with elements that \nchange, and modular forms are proposed to account for one type of such change. \nAn expression for the dependence of the mean on the probability of the \nswitching elements has been determined. The system is also analyzed from the \nperspective of decision between different hypotheses. Such sets are likely to \nbe of use in complex system queries and in analysis of surveys. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b2b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04909"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Scott Garrabrant, Tsvi Benson-Tilsen, Andrew Critch, Nate Soares, Jessica Taylor", "title": "Logical Induction. (arXiv:1609.03543v4 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1609.03543", "type": "text/html"}], "timestampUsec": "1513228697215427", "comments": [], "summary": {"content": "<p>We present a computable algorithm that assigns probabilities to every logical \nstatement in a given formal language, and refines those probabilities over \ntime. For instance, if the language is Peano arithmetic, it assigns \nprobabilities to all arithmetical statements, including claims about the twin \nprime conjecture, the outputs of long-running computations, and its own \nprobabilities. We show that our algorithm, an instance of what we call a \nlogical inductor, satisfies a number of intuitive desiderata, including: (1) it \nlearns to predict patterns of truth and falsehood in logical statements, often \nlong before having the resources to evaluate the statements, so long as the \npatterns can be written down in polynomial time; (2) it learns to use \nappropriate statistical summaries to predict sequences of statements whose \ntruth values appear pseudorandom; and (3) it learns to have accurate beliefs \nabout its own current beliefs, in a manner that avoids the standard paradoxes \nof self-reference. For example, if a given computer program only ever produces \noutputs in a certain range, a logical inductor learns this fact in a timely \nmanner; and if late digits in the decimal expansion of $\\pi$ are difficult to \npredict, then a logical inductor learns to assign $\\approx 10\\%$ probability to \n\"the $n$th digit of $\\pi$ is a 7\" for large $n$. Logical inductors also learn \nto trust their future beliefs more than their current beliefs, and their \nbeliefs are coherent in the limit (whenever $\\phi \\implies \\psi$, \n$\\mathbb{P}_\\infty(\\phi) \\le \\mathbb{P}_\\infty(\\psi)$, and so on); and logical \ninductors strictly dominate the universal semimeasure in the limit. \n</p> \n<p>These properties and many others all follow from a single logical induction \ncriterion, which is motivated by a series of stock trading analogies. Roughly \nspeaking, each logical sentence $\\phi$ is associated with a stock that is worth \n\\$1 per share if [...] \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b37", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1609.03543"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nick Cheney, Josh Bongard, Vytas SunSpiral, Hod Lipson", "title": "Scalable Co-Optimization of Morphology and Control in Embodied Machines. (arXiv:1706.06133v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.06133", "type": "text/html"}], "timestampUsec": "1513228697215426", "comments": [], "summary": {"content": "<p>Evolution sculpts both the body plans and nervous systems of agents together \nover time. In contrast, in AI and robotics, a robot's body plan is usually \ndesigned by hand, and control policies are then optimized for that fixed \ndesign. The task of simultaneously co-optimizing the morphology and controller \nof an embodied robot has remained a challenge. In psychology, the theory of \nembodied cognition posits that behavior arises from a close coupling between \nbody plan and sensorimotor control, which suggests why co-optimizing these two \nsubsystems is so difficult: most evolutionary changes to morphology tend to \nadversely impact sensorimotor control, leading to an overall decrease in \nbehavioral performance. Here, we further examine this hypothesis and \ndemonstrate a technique for \"morphological innovation protection\", which \ntemporarily reduces selection pressure on recently morphologically-changed \nindividuals, thus enabling evolution some time to \"readapt\" to the new \nmorphology with subsequent control policy mutations. We show the potential for \nthis method to avoid local optima and converge to similar highly fit \nmorphologies across widely varying initial conditions, while sustaining fitness \nimprovements further into optimization. While this technique is admittedly only \nthe first of many steps that must be taken to achieve scalable optimization of \nembodied machines, we hope that theoretical insight into the cause of \nevolutionary stagnation in current methods will help to enable the automation \nof robot design and behavioral training -- while simultaneously providing a \ntestbed to investigate the theory of embodied cognition. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b3b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.06133"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jeremy Morton, Tim A. Wheeler, Mykel J. Kochenderfer", "title": "Closed-Loop Policies for Operational Tests of Safety-Critical Systems. (arXiv:1707.08234v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.08234", "type": "text/html"}], "timestampUsec": "1513228697215425", "comments": [], "summary": {"content": "<p>Manufacturers of safety-critical systems must make the case that their \nproduct is sufficiently safe for public deployment. Much of this case often \nrelies upon critical event outcomes from real-world testing, requiring \nmanufacturers to be strategic about how they allocate testing resources in \norder to maximize their chances of demonstrating system safety. This work \nframes the partially observable and belief-dependent problem of test scheduling \nas a Markov decision process, which can be solved efficiently to yield \nclosed-loop manufacturer testing policies. By solving for policies over a wide \nrange of problem formulations, we are able to provide high-level guidance for \nmanufacturers and regulators on issues relating to the testing of \nsafety-critical systems. This guidance spans an array of topics, including \ncircumstances under which manufacturers should continue testing despite \nobserved incidents, when manufacturers should test aggressively, and when \nregulators should increase or reduce the real-world testing requirements for an \nautonomous vehicle. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b41", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.08234"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jakob N. Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, Igor Mordatch", "title": "Learning with Opponent-Learning Awareness. (arXiv:1709.04326v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04326", "type": "text/html"}], "timestampUsec": "1513228697215424", "comments": [], "summary": {"content": "<p>Multi-agent settings are quickly gathering importance in machine learning. \nBeyond a plethora of recent work on deep multi-agent reinforcement learning, \nhierarchical reinforcement learning, generative adversarial networks and \ndecentralized optimization can all be seen as instances of this setting. \nHowever, the presence of multiple learning agents in these settings renders the \ntraining problem non-stationary and often leads to unstable training or \nundesired final results. We present Learning with Opponent-Learning Awareness \n(LOLA), a method that reasons about the anticipated learning of the other \nagents. The LOLA learning rule includes an additional term that accounts for \nthe impact of the agent's policy on the anticipated parameter update of the \nother agents. We show that the LOLA update rule can be efficiently calculated \nusing an extension of the likelihood ratio policy gradient update, making the \nmethod suitable for model-free RL. This method thus scales to large parameter \nand input spaces and nonlinear function approximators. Preliminary results show \nthat the encounter of two LOLA agents leads to the emergence of tit-for-tat and \ntherefore cooperation in the iterated prisoners' dilemma (IPD), while \nindependent learning does not. In this domain, LOLA also receives higher \npayouts compared to a naive learner, and is robust against exploitation by \nhigher order gradient-based methods. Applied to infinitely repeated matching \npennies, LOLA agents converge to the Nash equilibrium. In a round robin \ntournament we show that LOLA agents can successfully shape the learning of a \nrange of multi-agent learning algorithms from literature, resulting in the \nhighest average returns on the IPD. We also apply LOLA to a grid world task \nwith an embedded social dilemma using deep recurrent policies. Again, by \nconsidering the learning of the other agent, LOLA agents learn to cooperate out \nof selfish interests. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04326"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Brendan Fong, David I. Spivak, R&#xe9;my Tuy&#xe9;ras", "title": "Backprop as Functor: A compositional perspective on supervised learning. (arXiv:1711.10455v2 [math.CT] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10455", "type": "text/html"}], "timestampUsec": "1513228697215422", "comments": [], "summary": {"content": "<p>A supervised learning algorithm searches over a set of functions $A \\to B$ \nparametrised by a space $P$ to find the best approximation to some ideal \nfunction $f\\colon A \\to B$. It does this by taking examples $(a,f(a)) \\in \nA\\times B$, and updating the parameter according to some rule. We define a \ncategory where these update rules may be composed, and show that gradient \ndescent---with respect to a fixed step size and an error function satisfying a \ncertain property---defines a monoidal functor from a category of parametrised \nfunctions to this category of update rules. This provides a structural \nperspective on backpropagation, as well as a broad generalisation of neural \nnetworks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b64", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10455"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Arun Venkitaraman, Dave Zachariah", "title": "Learning Sparse Graphs for Prediction and Filtering of Multivariate Data Processes. (arXiv:1712.04542v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04542", "type": "text/html"}], "timestampUsec": "1513228697215421", "comments": [], "summary": {"content": "<p>We address the problem of prediction and filtering of multivariate data \nprocess using an underlying graph model. We develop a method that learns a \nsparse partial correlation graph in a tuning-free and computationally efficient \nmanner. Specifically, the graph structure is learned recursively without the \nneed for cross-validation or parameter tuning by building upon a \nhyperparameter-free framework. Experiments using real-world datasets show that \nthe proposed method offers significant performance gains in prediction and \nfiltering tasks, in comparison with the graphs frequently associated with these \ndatasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b6a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04542"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Seokhyun Chung, Young Woong Park, Taesu Cheong", "title": "A Mathematical Programming Approach for Integrated Multiple Linear Regression Subset Selection and Validation. (arXiv:1712.04543v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04543", "type": "text/html"}], "timestampUsec": "1513228697215420", "comments": [], "summary": {"content": "<p>Subset selection for multiple linear regression aims to construct a \nregression model that minimizes errors by selecting a small number of \nexplanatory variables. Once a model is built, various statistical tests and \ndiagnostics are conducted to validate the model and to determine whether \nregression assumptions are met. Most traditional approaches require human \ndecisions at this step, for example, the user adding or removing a variable \nuntil a satisfactory model is obtained. However, this trial-and-error strategy \ncannot guarantee that a subset that minimizes the errors while satisfying all \nregression assumptions will be found. In this paper, we propose a fully \nautomated model building procedure for multiple linear regression subset \nselection that integrates model building and validation based on mathematical \nprogramming. The proposed model minimizes mean squared errors while ensuring \nthat the majority of the important regression assumptions are met. When no \nsubset satisfies all of the considered regression assumptions, our model \nprovides an alternative subset that satisfies most of these assumptions. \nComputational results show that our model yields better solutions (i.e., \nsatisfying more regression assumptions) compared to benchmark models while \nmaintaining similar explanatory power. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b71", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04543"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ruben Martinez-Cantin, Kevin Tee, Michael McCourt", "title": "Practical Bayesian optimization in the presence of outliers. (arXiv:1712.04567v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04567", "type": "text/html"}], "timestampUsec": "1513228697215419", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a1d005e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a1d005e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Inference in the presence of outliers is an important field of research as \noutliers are ubiquitous and may arise across a variety of problems and domains. \nBayesian optimization is method that heavily relies on probabilistic inference. \nThis allows outstanding sample efficiency because the probabilistic machinery \nprovides a memory of the whole optimization process. However, that virtue \nbecomes a disadvantage when the memory is populated with outliers, inducing \nbias in the estimation. In this paper, we present an empirical evaluation of \nBayesian optimization methods in the presence of outliers. The empirical \nevidence shows that Bayesian optimization with robust regression often produces \nsuboptimal results. We then propose a new algorithm which combines robust \nregression (a Gaussian process with Student-t likelihood) with outlier \ndiagnostics to classify data points as outliers or inliers. By using an \nscheduler for the classification of outliers, our method is more efficient and \nhas better convergence over the standard robust regression. Furthermore, we \nshow that even in controlled situations with no expected outliers, our method \nis able to produce better results. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b78", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04567"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Branislav Kveton, Csaba Szepesvari, Anup Rao, Zheng Wen, Yasin Abbasi-Yadkori, S. Muthukrishnan", "title": "Stochastic Low-Rank Bandits. (arXiv:1712.04644v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04644", "type": "text/html"}], "timestampUsec": "1513228697215418", "comments": [], "summary": {"content": "<p>Many problems in computer vision and recommender systems involve low-rank \nmatrices. In this work, we study the problem of finding the maximum entry of a \nstochastic low-rank matrix from sequential observations. At each step, a \nlearning agent chooses pairs of row and column arms, and receives the noisy \nproduct of their latent values as a reward. The main challenge is that the \nlatent values are unobserved. We identify a class of non-negative matrices \nwhose maximum entry can be found statistically efficiently and propose an \nalgorithm for finding them, which we call LowRankElim. We derive a \n$\\DeclareMathOperator{\\poly}{poly} O((K + L) \\poly(d) \\Delta^{-1} \\log n)$ \nupper bound on its $n$-step regret, where $K$ is the number of rows, $L$ is the \nnumber of columns, $d$ is the rank of the matrix, and $\\Delta$ is the minimum \ngap. The bound depends on other problem-specific constants that clearly do not \ndepend $K L$. To the best of our knowledge, this is the first such result in \nthe literature. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b92", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04644"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "D. Belomestny, L. Iosipoi, N. Zhivotovskiy", "title": "Variance reduction via empirical variance minimization: convergence and complexity. (arXiv:1712.04667v1 [math.NA])", "alternate": [{"href": "http://arxiv.org/abs/1712.04667", "type": "text/html"}], "timestampUsec": "1513228697215417", "comments": [], "summary": {"content": "<p>In this paper we propose and study a generic variance reduction approach. The \nproposed method is based on minimization of the empirical variance over a \nsuitable class of zero mean control functionals. We present the corresponding \nconvergence analysis and analyze complexity. Finally some numerical results \nshowing efficiency of the proposed approach are presented. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96ba4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04667"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "George Philipp, Seunghak Lee, Eric P. Xing", "title": "Stability Selection for Structured Variable Selection. (arXiv:1712.04688v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04688", "type": "text/html"}], "timestampUsec": "1513228697215416", "comments": [], "summary": {"content": "<p>In variable or graph selection problems, finding a right-sized model or \ncontrolling the number of false positives is notoriously difficult. Recently, a \nmeta-algorithm called Stability Selection was proposed that can provide \nreliable finite-sample control of the number of false positives. Its benefits \nwere demonstrated when used in conjunction with the lasso and orthogonal \nmatching pursuit algorithms. \n</p> \n<p>In this paper, we investigate the applicability of stability selection to \nstructured selection algorithms: the group lasso and the structured \ninput-output lasso. We find that using stability selection often increases the \npower of both algorithms, but that the presence of complex structure reduces \nthe reliability of error control under stability selection. We give strategies \nfor setting tuning parameters to obtain a good model size under stability \nselection, and highlight its strengths and weaknesses compared to competing \nmethods screen and clean and cross-validation. We give guidelines about when to \nuse which error control method. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bb4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04688"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hideyuki Miyahara, Yuki Sughiyama", "title": "A Quantum Extension of Variational Bayes Inference. (arXiv:1712.04709v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04709", "type": "text/html"}], "timestampUsec": "1513228697215415", "comments": [], "summary": {"content": "<p>Variational Bayes (VB) inference is one of the most important algorithms in \nmachine learning and widely used in engineering and industry. However, VB is \nknown to suffer from the problem of local optima. In this Letter, we generalize \nVB by using quantum mechanics, and propose a new algorithm, which we call \nquantum annealing variational Bayes (QAVB) inference. We then show that QAVB \ndrastically improve the performance of VB by applying them to a clustering \nproblem described by a Gaussian mixture model. Finally, we discuss an intuitive \nunderstanding on how QAVB works well. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bbd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04709"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Loucas Pillaud-Vivien (SIERRA), Alessandro Rudi (SIERRA), Francis Bach (SIERRA)", "title": "Exponential convergence of testing error for stochastic gradient methods. (arXiv:1712.04755v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04755", "type": "text/html"}], "timestampUsec": "1513228697215414", "comments": [], "summary": {"content": "<p>We consider binary classification problems with positive definite kernels and \nsquare loss, and study the convergence rates of stochastic gradient methods. We \nshow that while the excess testing loss (squared loss) converges slowly to zero \nas the number of observations (and thus iterations) goes to infinity, the \ntesting error (classification error) converges exponentially fast if low-noise \nconditions are assumed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bc4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04755"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Cl&#xe9;mentine Barreyre, B&#xe9;atrice Laurent (IMT), Jean-Michel Loubes (IMT), Bertrand Cabon, Lo&#xef;c Boussouf", "title": "Multiple testing for outlier detection in functional data. (arXiv:1712.04775v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04775", "type": "text/html"}], "timestampUsec": "1513228697215413", "comments": [], "summary": {"content": "<p>We propose a novel procedure for outlier detection in functional data, in a \nsemi-supervised framework. As the data is functional, we consider the \ncoefficients obtained after projecting the observations onto orthonormal bases \n(wavelet, PCA). A multiple testing procedure based on the two-sample test is \ndefined in order to highlight the levels of the coefficients on which the \noutliers appear as significantly different to the normal data. The selected \ncoefficients are then called features for the outlier detection, on which we \ncompute the Local Outlier Factor to highlight the outliers. This procedure to \nselect the features is applied on simulated data that mimic the behaviour of \nspace telemetries, and compared with existing dimension reduction techniques. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bc9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04775"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Victor Chernozhukov, Mert Demirer, Esther Duflo, Ivan Fernandez-Val", "title": "Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments. (arXiv:1712.04802v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04802", "type": "text/html"}], "timestampUsec": "1513228697215412", "comments": [], "summary": {"content": "<p>We propose strategies to estimate and make inference on key features of \nheterogeneous effects in randomized experiments. These key features include \nbest linear predictors of the effects using machine learning proxies, average \neffects sorted by impact groups, and average characteristics of most and least \nimpacted units. The approach is valid in high dimensional settings, where the \neffects are proxied by machine learning methods. We post-process these proxies \ninto the estimates of the key features. Our approach is agnostic about the \nproperties of the machine learning estimators used to produce proxies, and it \ncompletely avoids making any strong assumption. Estimation and inference relies \non repeated data splitting to avoid overfitting and achieve validity. Our \nvariational inference method is shown to be uniformly valid and quantifies the \nuncertainty coming from both parameter estimation and data splitting. In \nessence, we take medians of p-values and medians of confidence intervals, \nresulting from many different data splits, and then adjust their nominal level \nto guarantee uniform validity. The inference method could be of substantial \nindependent interest in many machine learning applications. Empirical \napplications illustrate the use of the approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bd2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04802"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tom Hope, Dafna Shahaf", "title": "Ballpark Crowdsourcing: The Wisdom of Rough Group Comparisons. (arXiv:1712.04828v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04828", "type": "text/html"}], "timestampUsec": "1513228697215411", "comments": [], "summary": {"content": "<p>Crowdsourcing has become a popular method for collecting labeled training \ndata. However, in many practical scenarios traditional labeling can be \ndifficult for crowdworkers (for example, if the data is high-dimensional or \nunintuitive, or the labels are continuous). \n</p> \n<p>In this work, we develop a novel model for crowdsourcing that can complement \nstandard practices by exploiting people's intuitions about groups and relations \nbetween them. We employ a recent machine learning setting, called Ballpark \nLearning, that can estimate individual labels given only coarse, aggregated \nsignal over groups of data points. To address the important case of continuous \nlabels, we extend the Ballpark setting (which focused on classification) to \nregression problems. We formulate the problem as a convex optimization problem \nand propose fast, simple methods with an innate robustness to outliers. \n</p> \n<p>We evaluate our methods on real-world datasets, demonstrating how useful \nconstraints about groups can be harnessed from a crowd of non-experts. Our \nmethods can rival supervised models trained on many true labels, and can obtain \nconsiderably better results from the crowd than a standard label-collection \nprocess (for a lower price). By collecting rough guesses on groups of instances \nand using machine learning to infer the individual labels, our lightweight \nframework is able to address core crowdsourcing challenges and train machine \nlearning models in a cost-effective way. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bda", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04828"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sheng Lin, Ning Liu, Mahdi Nazemi, Hongjia Li, Caiwen Ding, Yanzhi Wang, Massoud Pedram", "title": "FFT-Based Deep Learning Deployment in Embedded Systems. (arXiv:1712.04910v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04910", "type": "text/html"}], "timestampUsec": "1513228697215410", "comments": [], "summary": {"content": "<p>Deep learning has delivered its powerfulness in many application domains, \nespecially in image and speech recognition. As the backbone of deep learning, \ndeep neural networks (DNNs) consist of multiple layers of various types with \nhundreds to thousands of neurons. Embedded platforms are now becoming essential \nfor deep learning deployment due to their portability, versatility, and energy \nefficiency. The large model size of DNNs, while providing excellent accuracy, \nalso burdens the embedded platforms with intensive computation and storage. \nResearchers have investigated on reducing DNN model size with negligible \naccuracy loss. This work proposes a Fast Fourier Transform (FFT)-based DNN \ntraining and inference model suitable for embedded platforms with reduced \nasymptotic complexity of both computation and storage, making our approach \ndistinguished from existing approaches. We develop the training and inference \nalgorithms based on FFT as the computing kernel and deploy the FFT-based \ninference model on embedded platforms achieving extraordinary processing speed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96be6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04910"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xinkun Nie, Stefan Wager", "title": "Learning Objectives for Treatment Effect Estimation. (arXiv:1712.04912v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04912", "type": "text/html"}], "timestampUsec": "1513228697215409", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a1d035e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a1d035e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We develop a general class of two-step algorithms for heterogeneous treatment \neffect estimation in observational studies. We first estimate marginal effects \nand treatment propensities to form an objective function that isolates the \nheterogeneous treatment effects, and then optimize the learned objective. This \napproach has several advantages over existing methods. From a practical \nperspective, our method is very flexible and easy to use: In both steps, we can \nuse any method of our choice, e.g., penalized regression, a deep net, or \nboosting; moreover, these methods can be fine-tuned by cross-validating on the \nlearned objective. Meanwhile, in the case of penalized kernel regression, we \nshow that our method has a quasi-oracle property, whereby even if our pilot \nestimates for marginal effects and treatment propensities are not particularly \naccurate, we achieve the same regret bounds as an oracle who has a-priori \nknowledge of these nuisance components. We implement variants of our method \nbased on both penalized regression and convolutional neural networks, and find \npromising performance relative to existing baselines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96be8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04912"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, James Robins", "title": "Double/Debiased Machine Learning for Treatment and Causal Parameters. (arXiv:1608.00060v6 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1608.00060", "type": "text/html"}], "timestampUsec": "1513228697215408", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a2363c8\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a2363c8&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Most modern supervised statistical/machine learning (ML) methods are \nexplicitly designed to solve prediction problems very well. Achieving this goal \ndoes not imply that these methods automatically deliver good estimators of \ncausal parameters. Examples of such parameters include individual regression \ncoefficients, average treatment effects, average lifts, and demand or supply \nelasticities. In fact, estimates of such causal parameters obtained via naively \nplugging ML estimators into estimating equations for such parameters can behave \nvery poorly due to the regularization bias. Fortunately, this regularization \nbias can be removed by solving auxiliary prediction problems via ML tools. \nSpecifically, we can form an orthogonal score for the target low-dimensional \nparameter by combining auxiliary and main ML predictions. The score is then \nused to build a de-biased estimator of the target parameter which typically \nwill converge at the fastest possible 1/root(n) rate and be approximately \nunbiased and normal, and from which valid confidence intervals for these \nparameters of interest may be constructed. The resulting method thus could be \ncalled a \"double ML\" method because it relies on estimating primary and \nauxiliary predictive models. In order to avoid overfitting, our construction \nalso makes use of the K-fold sample splitting, which we call cross-fitting. \nThis allows us to use a very broad set of ML predictive methods in solving the \nauxiliary and main prediction problems, such as random forest, lasso, ridge, \ndeep neural nets, boosted trees, as well as various hybrids and aggregators of \nthese methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1608.00060"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andreas Svensson, Thomas B. Sch&#xf6;n, Fredrik Lindsten", "title": "Learning of state-space models with highly informative observations: a tempered Sequential Monte Carlo solution. (arXiv:1702.01618v2 [stat.CO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1702.01618", "type": "text/html"}], "timestampUsec": "1513228697215407", "comments": [], "summary": {"content": "<p>Probabilistic (or Bayesian) modeling and learning offers interesting \npossibilities for systematic representation of uncertainty using probability \ntheory. However, probabilistic learning often leads to computationally \nchallenging problems. Some problems of this type that were previously \nintractable can now be solved on standard personal computers thanks to recent \nadvances in Monte Carlo methods. In particular, for learning of unknown \nparameters in nonlinear state-space models, methods based on the particle \nfilter (a Monte Carlo method) have proven very useful. A notoriously \nchallenging problem, however, still occurs when the observations in the \nstate-space model are highly informative, i.e. when there is very little or no \nmeasurement noise present, relative to the amount of process noise. The \nparticle filter will then struggle in estimating one of the basic components \nfor probabilistic learning, namely the likelihood $p($data$|$parameters$)$. To \nthis end we suggest an algorithm which initially assumes that there is \nsubstantial amount of artificial measurement noise present. The variance of \nthis noise is sequentially decreased in an adaptive fashion such that we, in \nthe end, recover the original problem or possibly a very close approximation of \nit. The main component in our algorithm is a sequential Monte Carlo (SMC) \nsampler, which gives our proposed method a clear resemblance to the SMC^2 \nmethod. Another natural link is also made to the ideas underlying the \napproximate Bayesian computation (ABC). We illustrate it with numerical \nexamples, and in particular show promising results for a challenging \nWiener-Hammerstein benchmark problem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bf1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1702.01618"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xingguo Li, Lin F. Yang, Jason Ge, Jarvis Haupt, Tong Zhang, Tuo Zhao", "title": "On Quadratic Convergence of DC Proximal Newton Algorithm for Nonconvex Sparse Learning in High Dimensions. (arXiv:1706.06066v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.06066", "type": "text/html"}], "timestampUsec": "1513228697215406", "comments": [], "summary": {"content": "<p>We propose a DC proximal Newton algorithm for solving nonconvex regularized \nsparse learning problems in high dimensions. Our proposed algorithm integrates \nthe proximal Newton algorithm with multi-stage convex relaxation based on the \ndifference of convex (DC) programming, and enjoys both strong computational and \nstatistical guarantees. Specifically, by leveraging a sophisticated \ncharacterization of sparse modeling structures/assumptions (i.e., local \nrestricted strong convexity and Hessian smoothness), we prove that within each \nstage of convex relaxation, our proposed algorithm achieves (local) quadratic \nconvergence, and eventually obtains a sparse approximate local optimum with \noptimal statistical properties after only a few convex relaxations. Numerical \nexperiments are provided to support our theory. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bf6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.06066"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Marie Chavent, Vanessa Kuentz-Simonet, Amaury Labenne, J&#xe9;r&#xf4;me Saracco", "title": "ClustGeo: an R package for hierarchical clustering with spatial constraints. (arXiv:1707.03897v2 [stat.CO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.03897", "type": "text/html"}], "timestampUsec": "1513228697215405", "comments": [], "summary": {"content": "<p>In this paper, we propose a Ward-like hierarchical clustering algorithm \nincluding spatial/geographical constraints. Two dissimilarity matrices $D_0$ \nand $D_1$ are inputted, along with a mixing parameter $\\alpha \\in [0,1]$. The \ndissimilarities can be non-Euclidean and the weights of the observations can be \nnon-uniform. The first matrix gives the dissimilarities in the \"feature space\" \nand the second matrix gives the dissimilarities in the \"constraint space\". The \ncriterion minimized at each stage is a convex combination of the homogeneity \ncriterion calculated with $D_0$ and the homogeneity criterion calculated with \n$D_1$. The idea is then to determine a value of $\\alpha$ which increases the \nspatial contiguity without deteriorating too much the quality of the solution \nbased on the variables of interest i.e. those of the feature space. This \nprocedure is illustrated on a real dataset using the R package ClustGeo. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96c23", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.03897"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Thanh V. Nguyen, Raymond K. W. Wong, Chinmay Hegde", "title": "Provably Accurate Double-Sparse Coding. (arXiv:1711.03638v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.03638", "type": "text/html"}], "timestampUsec": "1513228697215404", "comments": [], "summary": {"content": "<p>Sparse coding is a crucial subroutine in algorithms for various signal \nprocessing, deep learning, and other machine learning applications. The central \ngoal is to learn an overcomplete dictionary that can sparsely represent a given \ninput dataset. However, a key challenge is that storage, transmission, and \nprocessing of the learned dictionary can be untenably high if the data \ndimension is high. In this paper, we consider the double-sparsity model \nintroduced by Rubinstein et al. (2010b) where the dictionary itself is the \nproduct of a fixed, known basis and a data-adaptive sparse component. First, we \nintroduce a simple algorithm for double-sparse coding that can be amenable to \nefficient implementation via neural architectures. Second, we theoretically \nanalyze its performance and demonstrate asymptotic sample complexity and \nrunning time benefits over existing (provable) approaches for sparse coding. To \nour knowledge, our work introduces the first computationally efficient \nalgorithm for double-sparse coding that enjoys rigorous statistical guarantees. \nFinally, we support our analysis via several numerical experiments on simulated \ndata, confirming that our method can indeed be useful in problem sizes \nencountered in practical applications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96c42", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.03638"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jihun Hamm", "title": "Machine vs Machine: Minimax-Optimal Defense Against Adversarial Examples. (arXiv:1711.04368v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.04368", "type": "text/html"}], "timestampUsec": "1513228697215403", "comments": [], "summary": {"content": "<p>Recently, researchers have discovered that the state-of-the-art object \nclassifiers can be fooled easily by small perturbations in the input \nunnoticeable to human eyes. It is known that an attacker can generate strong \nadversarial examples if she knows the classifier parameters. Conversely, a \ndefender can robustify the classifier by retraining if she has the adversarial \nexamples. The cat-and-mouse game nature of attacks and defenses raises the \nquestion of the presence of equilibria in the dynamics. In this paper, we \npresent a neural-network based attack class to approximate a larger but \nintractable class of attacks, and formulate the attacker-defender interaction \nas a zero-sum leader-follower game. We present sensitivity-penalized \noptimization algorithms to find minimax solutions, which are the best \nworst-case defenses against whitebox attacks. Advantages of the learning-based \nattacks and defenses compared to gradient-based attacks and defenses are \ndemonstrated with MNIST and CIFAR-10. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96c52", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.04368"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Robert Geirhos, David H. J. Janssen, Heiko H. Sch&#xfc;tt, Jonas Rauber, Matthias Bethge, Felix A. Wichmann", "title": "Comparing deep neural networks against humans: object recognition when the signal gets weaker. (arXiv:1706.06969v1 [cs.CV] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1706.06969", "type": "text/html"}], "timestampUsec": "1513228697215401", "comments": [], "summary": {"content": "<p>Human visual object recognition is typically rapid and seemingly effortless, \nas well as largely independent of viewpoint and object orientation. Until very \nrecently, animate visual systems were the only ones capable of this remarkable \ncomputational feat. This has changed with the rise of a class of computer \nvision algorithms called deep neural networks (DNNs) that achieve human-level \nclassification performance on object recognition tasks. Furthermore, a growing \nnumber of studies report similarities in the way DNNs and the human visual \nsystem process objects, suggesting that current DNNs may be good models of \nhuman visual object recognition. Yet there clearly exist important \narchitectural and processing differences between state-of-the-art DNNs and the \nprimate visual system. The potential behavioural consequences of these \ndifferences are not well understood. We aim to address this issue by comparing \nhuman and DNN generalisation abilities towards image degradations. We find the \nhuman visual system to be more robust to image manipulations like contrast \nreduction, additive noise or novel eidolon-distortions. In addition, we find \nprogressively diverging classification error-patterns between man and DNNs when \nthe signal gets weaker, indicating that there may still be marked differences \nin the way humans and current DNNs perform visual object recognition. We \nenvision that our findings as well as our carefully measured and freely \navailable behavioural datasets provide a new useful benchmark for the computer \nvision community to improve the robustness of DNNs and a motivation for \nneuroscientists to search for mechanisms in the brain that could facilitate \nthis robustness. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96c57", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.06969"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Maryam Fazel-Zarandi, Shang-Wen Li, Jin Cao, Jared Casale, Peter Henderson, David Whitney, Alborz Geramifard", "title": "Learning Robust Dialog Policies in Noisy Environments. (arXiv:1712.04034v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.04034", "type": "text/html"}], "timestampUsec": "1513142281080184", "comments": [], "summary": {"content": "<p>Modern virtual personal assistants provide a convenient interface for \ncompleting daily tasks via voice commands. An important consideration for these \nassistants is the ability to recover from automatic speech recognition (ASR) \nand natural language understanding (NLU) errors. In this paper, we focus on \nlearning robust dialog policies to recover from these errors. To this end, we \ndevelop a user simulator which interacts with the assistant through voice \ncommands in realistic scenarios with noisy audio, and use it to learn dialog \npolicies through deep reinforcement learning. We show that dialogs generated by \nour simulator are indistinguishable from human generated dialogs, as determined \nby human evaluators. Furthermore, preliminary experimental results show that \nthe learned policies in noisy environments achieve the same execution success \nrate with fewer dialog turns compared to fixed rule-based policies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513142281080", "annotations": [], "published": 1513142281, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034613311b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04034"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Haitao Zhao", "title": "Neural Component Analysis for Fault Detection. (arXiv:1712.04118v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04118", "type": "text/html"}], "timestampUsec": "1513141858490941", "comments": [], "summary": {"content": "<p>Principal component analysis (PCA) is largely adopted for chemical process \nmonitoring and numerous PCA-based systems have been developed to solve various \nfault detection and diagnosis problems. Since PCA-based methods assume that the \nmonitored process is linear, nonlinear PCA models, such as autoencoder models \nand kernel principal component analysis (KPCA), has been proposed and applied \nto nonlinear process monitoring. However, KPCA-based methods need to perform \neigen-decomposition (ED) on the kernel Gram matrix whose dimensions depend on \nthe number of training data. Moreover, prefixed kernel parameters cannot be \nmost effective for different faults which may need different parameters to \nmaximize their respective detection performances. Autoencoder models lack the \nconsideration of orthogonal constraints which is crucial for PCA-based \nalgorithms. To address these problems, this paper proposes a novel nonlinear \nmethod, called neural component analysis (NCA), which intends to train a \nfeedforward neural work with orthogonal constraints such as those used in PCA. \nNCA can adaptively learn its parameters through backpropagation and the \ndimensionality of the nonlinear features has no relationship with the number of \ntraining samples. Extensive experimental results on the Tennessee Eastman (TE) \nbenchmark process show the superiority of NCA in terms of missed detection rate \n(MDR) and false alarm rate (FAR). The source code of NCA can be found in \nhttps://github.com/haitaozhao/Neural-Component-Analysis.git. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127d8a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04118"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Daniel Hein, Steffen Udluft, Thomas A. Runkler", "title": "Interpretable Policies for Reinforcement Learning by Genetic Programming. (arXiv:1712.04170v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04170", "type": "text/html"}], "timestampUsec": "1513141858490940", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a236717\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a236717&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The search for interpretable reinforcement learning policies is of high \nacademic and industrial interest. Especially for industrial systems, domain \nexperts are more likely to deploy autonomously learned controllers if they are \nunderstandable and convenient to evaluate. Basic algebraic equations are \nsupposed to meet these requirements, as long as they are restricted to an \nadequate complexity. Here we introduce the genetic programming for \nreinforcement learning (GPRL) approach based on model-based batch reinforcement \nlearning and genetic programming, which autonomously learns policy equations \nfrom pre-existing default state-action trajectory samples. GPRL is compared to \na straight-forward method which utilizes genetic programming for symbolic \nregression, yielding policies imitating an existing well-performing, but \nnon-interpretable policy. Experiments on three reinforcement learning \nbenchmarks, i.e., mountain car, cart-pole balancing, and industrial benchmark, \ndemonstrate the superiority of our GPRL approach compared to the symbolic \nregression method. GPRL is capable of producing well-performing interpretable \nreinforcement learning policies from pre-existing default trajectory data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127d8e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04170"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "V.I. Avrutskiy", "title": "Backpropagation generalized for output derivatives. (arXiv:1712.04185v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.04185", "type": "text/html"}], "timestampUsec": "1513141858490939", "comments": [], "summary": {"content": "<p>Backpropagation algorithm is the cornerstone for neural network analysis. \nPaper extends it for training any derivatives of neural network's output with \nrespect to its input. By the dint of it feedforward networks can be used to \nsolve or verify solutions of partial or simple, linear or nonlinear \ndifferential equations. This method vastly differs from traditional ones like \nfinite differences on a mesh. It contains no approximations, but rather an \nexact form of differential operators. Algorithm is built to train a feed \nforward network with any number of hidden layers and any kind of sufficiently \nsmooth activation functions. It's presented in a form of matrix-vector products \nso highly parallel implementation is readily possible. First part derives the \nmethod for 2D case with first and second order derivatives, second part extends \nit to N-dimensional case with any derivatives. All necessary expressions for \nusing this method to solve most applied PDE can be found in Appendix D. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127d98", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04185"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yoshihiro Nagano, Ryo Karakida, Masato Okada", "title": "Concept Formation and Dynamics of Repeated Inference in Deep Generative Models. (arXiv:1712.04195v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04195", "type": "text/html"}], "timestampUsec": "1513141858490938", "comments": [], "summary": {"content": "<p>Deep generative models are reported to be useful in broad applications \nincluding image generation. Repeated inference between data space and latent \nspace in these models can denoise cluttered images and improve the quality of \ninferred results. However, previous studies only qualitatively evaluated image \noutputs in data space, and the mechanism behind the inference has not been \ninvestigated. The purpose of the current study is to numerically analyze \nchanges in activity patterns of neurons in the latent space of a deep \ngenerative model called a \"variational auto-encoder\" (VAE). What kinds of \ninference dynamics the VAE demonstrates when noise is added to the input data \nare identified. The VAE embeds a dataset with clear cluster structures in the \nlatent space and the center of each cluster of multiple correlated data points \n(memories) is referred as the concept. Our study demonstrated that transient \ndynamics of inference first approaches a concept, and then moves close to a \nmemory. Moreover, the VAE revealed that the inference dynamics approaches a \nmore abstract concept to the extent that the uncertainty of input data \nincreases due to noise. It was demonstrated that by increasing the number of \nthe latent variables, the trend of the inference dynamics to approach a concept \ncan be enhanced, and the generalization ability of the VAE can be improved. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127d9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04195"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wieland Brendel, Jonas Rauber, Matthias Bethge", "title": "Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models. (arXiv:1712.04248v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04248", "type": "text/html"}], "timestampUsec": "1513141858490937", "comments": [], "summary": {"content": "<p>Many machine learning algorithms are vulnerable to almost imperceptible \nperturbations of their inputs. So far it was unclear how much risk adversarial \nperturbations carry for the safety of real-world machine learning applications \nbecause most methods used to generate such perturbations rely either on \ndetailed model information (gradient-based attacks) or on confidence scores \nsuch as class probabilities (score-based attacks), neither of which are \navailable in most real-world scenarios. In many such cases one currently needs \nto retreat to transfer-based attacks which rely on cumbersome substitute \nmodels, need access to the training data and can be defended against. Here we \nemphasise the importance of attacks which solely rely on the final model \ndecision. Such decision-based attacks are (1) applicable to real-world \nblack-box models such as autonomous cars, (2) need less knowledge and are \neasier to apply than transfer-based attacks and (3) are more robust to simple \ndefences than gradient- or score-based attacks. Previous attacks in this \ncategory were limited to simple models or simple datasets. Here we introduce \nthe Boundary Attack, a decision-based attack that starts from a large \nadversarial perturbation and then seeks to reduce the perturbation while \nstaying adversarial. The attack is conceptually simple, requires close to no \nhyperparameter tuning, does not rely on substitute models and is competitive \nwith the best gradient-based attacks in standard computer vision tasks like \nImageNet. We apply the attack on two black-box algorithms from Clarifai.com. \nThe Boundary Attack in particular and the class of decision-based attacks in \ngeneral open new avenues to study the robustness of machine learning models and \nraise new questions regarding the safety of deployed machine learning systems. \nAn implementation of the attack is available as part of Foolbox at \nhttps://github.com/bethgelab/foolbox . \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127da1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04248"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nicola Milano, Paolo Pagliuca, Stefano Nolfi", "title": "Robustness, Evolvability and Phenotypic Complexity: Insights from Evolving Digital Circuits. (arXiv:1712.04254v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.04254", "type": "text/html"}], "timestampUsec": "1513141858490936", "comments": [], "summary": {"content": "<p>We show how the characteristics of the evolutionary algorithm influence the \nevolvability of candidate solutions, i.e. the propensity of evolving \nindividuals to generate better solutions as a result of genetic variation. More \nspecifically, (1+{\\lambda}) evolutionary strategies largely outperform \n({\\mu}+1) evolutionary strategies in the context of the evolution of digital \ncircuits --- a domain characterized by a high level of neutrality. This \ndifference is due to the fact that the competition for robustness to mutations \namong the circuits evolved with ({\\mu}+1) evolutionary strategies leads to the \nselection of phenotypically simple but low evolvable circuits. These circuits \nachieve robustness by minimizing the number of functional genes rather than by \nrelying on redundancy or degeneracy to buffer the effects of mutations. The \nanalysis of these factors enabled us to design a new evolutionary algorithm, \nnamed Parallel Stochastic Hill Climber (PSHC), which outperforms the other two \nmethods considered. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127da4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04254"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Albert Gatt, Emiel Krahmer", "title": "Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation. (arXiv:1703.09902v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.09902", "type": "text/html"}], "timestampUsec": "1513141858490935", "comments": [], "summary": {"content": "<p>This paper surveys the current state of the art in Natural Language \nGeneration (NLG), defined as the task of generating text or speech from \nnon-linguistic input. A survey of NLG is timely in view of the changes that the \nfield has undergone over the past decade or so, especially in relation to new \n(usually data-driven) methods, as well as new applications of NLG technology. \nThis survey therefore aims to (a) give an up-to-date synthesis of research on \nthe core tasks in NLG and the architectures adopted in which such tasks are \norganised; (b) highlight a number of relatively recent research topics that \nhave arisen partly as a result of growing synergies between NLG and other areas \nof artificial intelligence; (c) draw attention to the challenges in NLG \nevaluation, relating them to similar challenges faced in other areas of Natural \nLanguage Processing, with an emphasis on different evaluation methods and the \nrelationships between them. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127dbb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.09902"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Vedran Dunjko, Yi-Kai Liu, Xingyao Wu, Jacob M. Taylor", "title": "Super-polynomial and exponential improvements for quantum-enhanced reinforcement learning. (arXiv:1710.11160v2 [quant-ph] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.11160", "type": "text/html"}], "timestampUsec": "1513141858490934", "comments": [], "summary": {"content": "<p>Recent work on quantum machine learning has demonstrated that quantum \ncomputers can offer dramatic improvements over classical devices for data \nmining, prediction and classification. However, less is known about the \nadvantages using quantum computers may bring in the more general setting of \nreinforcement learning, where learning is achieved via interaction with a task \nenvironment that provides occasional rewards. Reinforcement learning can \nincorporate data-analysis-oriented learning settings as special cases, but also \nincludes more complex situations where, e.g., reinforcing feedback is delayed. \nIn a few recent works, Grover-type amplification has been utilized to construct \nquantum agents that achieve up-to-quadratic improvements in learning \nefficiency. These encouraging results have left open the key question of \nwhether super-polynomial improvements in learning times are possible for \ngenuine reinforcement learning problems, that is problems that go beyond the \nother more restricted learning paradigms. In this work, we provide a family of \nsuch genuine reinforcement learning tasks. We construct quantum-enhanced \nlearners which learn super-polynomially, and even exponentially faster than any \nclassical reinforcement learning model, and we discuss the potential impact our \nresults may have on future technologies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127de5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.11160"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Daniel George, E. A. Huerta", "title": "Deep Learning for Real-time Gravitational Wave Detection and Parameter Estimation with LIGO Data. (arXiv:1711.07966v2 [gr-qc] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07966", "type": "text/html"}], "timestampUsec": "1513141858490933", "comments": [], "summary": {"content": "<p>The recent Nobel-prize-winning detections of gravitational waves from merging \nblack holes and the subsequent detection of the collision of two neutron stars \nin coincidence with electromagnetic observations have inaugurated a new era of \nmultimessenger astrophysics. To enhance the scope of this emergent science, we \nproposed the use of deep convolutional neural networks for the detection and \ncharacterization of gravitational wave signals in real-time. This method, Deep \nFiltering, was initially demonstrated using simulated LIGO noise. In this \narticle, we present the extension of Deep Filtering using real data from the \nfirst observing run of LIGO, for both detection and parameter estimation of \ngravitational waves from binary black hole mergers with continuous data streams \nfrom multiple LIGO detectors. We show for the first time that machine learning \ncan detect and estimate the true parameters of a real GW event observed by \nLIGO. Our comparisons show that Deep Filtering is far more computationally \nefficient than matched-filtering, while retaining similar sensitivity and lower \nerrors, allowing real-time processing of weak time-series signals in \nnon-stationary non-Gaussian noise, with minimal resources, and also enables the \ndetection of new classes of gravitational wave sources that may go unnoticed \nwith existing detection algorithms. This approach is uniquely suited to enable \ncoincident detection campaigns of gravitational waves and their multimessenger \ncounterparts in real-time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127df7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07966"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, Aleksander Madry", "title": "A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations. (arXiv:1712.02779v2 [cs.LG] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02779", "type": "text/html"}], "timestampUsec": "1513141858490931", "comments": [], "summary": {"content": "<p>Recent work has shown that neural network-based vision classifiers exhibit a \nsignificant vulnerability to misclassifications caused by imperceptible but \nadversarial perturbations of their inputs. These perturbations, however, are \npurely pixel-wise and built out of loss function gradients of either the \nattacked model or its surrogate. As a result, they tend to be contrived and \nlook pretty artificial. This might suggest that such vulnerability to slight \ninput perturbations can only arise in a truly adversarial setting and thus is \nunlikely to be an issue in more \"natural\" contexts. \n</p> \n<p>In this paper, we provide evidence that such belief might be incorrect. We \ndemonstrate that significantly simpler, and more likely to occur naturally, \ntransformations of the input - namely, rotations and translations alone, \nsuffice to significantly degrade the classification performance of neural \nnetwork-based vision models across a spectrum of datasets. This remains to be \nthe case even when these models are trained using appropriate data \naugmentation. Finding such \"fooling\" transformations does not require having \nany special access to the model - just trying out a small number of random \nrotation and translation combinations already has a significant effect. These \nfindings suggest that our current neural network-based vision models might not \nbe as reliable as we tend to assume. \n</p> \n<p>Finally, we consider a new class of perturbations that combines rotations and \ntranslations with the standard pixel-wise attacks. We observe that these two \ntypes of input transformations are, in a sense, orthogonal to each other. Their \neffect on the performance of the model seems to be additive, while robustness \nto one type does not seem to affect the robustness to the other type. This \nsuggests that this combined class of transformations is a more complete notion \nof similarity in the context of adversarial robustness of vision models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e17", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02779"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1515043164, "author": "Michael Bernico, Yuntao Li, Dingchao Zhang", "title": "Investigating the Impact of Data Volume and Domain Similarity on Transfer Learning Applications. (arXiv:1712.04008v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.04008", "type": "text/html"}], "timestampUsec": "1513141858490930", "comments": [], "summary": {"content": "<p>Transfer Learning helps to build a system to recognize and apply knowledge \nand experience learned in previous tasks (source task) to new tasks or new \ndomains (target task), which share some commonality. The two important factors \nthat impact the performance of transfer learning models are: (a) the size of \nthe target dataset and (b) the similarity in distribution between source and \ntarget domains. Thus far there has been little investigation into just how \nimportant these factors are. In this paper, we investigated the impact of \ntarget dataset size and source/target domain similarity on model performance \nthrough a series of experiments. We found that more data is always beneficial, \nand that model performance improved linearly with the log of data size, until \nwe were out of data. As source/target domains differ, more data is required and \nfine tuning will render better performance than feature extraction. When \nsource/target domains are similar and data size is small, fine tuning and \nfeature extraction renders equivalent performance. We hope that our study \ninspires further work in transfer learning, which continues to be a very \nimportant technique for developing practical machine learning applications in \nbusiness domains. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1515043164, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04008"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Roman V. Yampolskiy", "title": "Detecting Qualia in Natural and Artificial Agents. (arXiv:1712.04020v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04020", "type": "text/html"}], "timestampUsec": "1513141858490929", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a236968\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a236968&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The Hard Problem of consciousness has been dismissed as an illusion. By \nshowing that computers are capable of experiencing, we show that they are at \nleast rudimentarily conscious with potential to eventually reach \nsuperconsciousness. The main contribution of the paper is a test for confirming \ncertain subjective experiences in a tested agent. We follow with analysis of \nbenefits and problems with conscious machines and implications of such \ncapability on future of computing, machine rights and artificial intelligence \nsafety. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04020"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Miao Liu, Marlos C. Machado, Gerald Tesauro, Murray Campbell", "title": "The Eigenoption-Critic Framework. (arXiv:1712.04065v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04065", "type": "text/html"}], "timestampUsec": "1513141858490928", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a28b786\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a28b786&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Eigenoptions (EOs) have been recently introduced as a promising idea for \ngenerating a diverse set of options through the graph Laplacian, having been \nshown to allow efficient exploration. Despite its initial promising results, a \ncouple of issues in current algorithms limit its application, namely: (1) EO \nmethods require two separate steps (eigenoption discovery and reward \nmaximization) to learn a control policy, which can incur a significant amount \nof storage and computation; (2) EOs are only defined for problems with discrete \nstate-spaces and; (3) it is not easy to take the environment's reward function \ninto consideration when discovering EOs. To addresses these issues, we \nintroduce an algorithm termed eigenoption-critic (EOC) based on the \nOption-critic (OC) framework [Bacon17], a general hierarchical reinforcement \nlearning (RL) algorithm that allows learning the intra-option policies \nsimultaneously with the policy over options. We also propose a generalization \nof EOC to problems with continuous state-spaces through the Nystr\\\"om \napproximation. EOC can also be seen as extending OC to nonstationary settings, \nwhere the discovered options are not tailored for a single task. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e44", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04065"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Thomas Bartz-Beielstein, Lorenzo Gentile, Martin Zaefferer", "title": "In a Nutshell: Sequential Parameter Optimization. (arXiv:1712.04076v1 [cs.MS])", "alternate": [{"href": "http://arxiv.org/abs/1712.04076", "type": "text/html"}], "timestampUsec": "1513141858490927", "comments": [], "summary": {"content": "<p>The performance of optimization algorithms relies crucially on their \nparameterizations. Finding good parameter settings is called algorithm tuning. \nUsing a simple simulated annealing algorithm, we will demonstrate how \noptimization algorithms can be tuned using the sequential parameter \noptimization toolbox (SPOT). SPOT provides several tools for automated and \ninteractive tuning. The underling concepts of the SPOT approach are explained. \nThis includes key techniques such as exploratory fitness landscape analysis and \nresponse surface methodology. Many examples illustrate how SPOT can be used for \nunderstanding the performance of algorithms and gaining insight into \nalgorithm's behavior. Furthermore, we demonstrate how SPOT can be used as an \noptimizer and how a sophisticated ensemble approach is able to combine several \nmeta models via stacking. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e49", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04076"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, Zhangyang Wang", "title": "RESIDE: A Benchmark for Single Image Dehazing. (arXiv:1712.04143v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.04143", "type": "text/html"}], "timestampUsec": "1513141858490926", "comments": [], "summary": {"content": "<p>In this paper, we present a comprehensive study and evaluation of existing \nsingle image dehazing algorithms, using a new large-scale benchmark consisting \nof both synthetic and real-world hazy images, called REalistic Single Image \nDEhazing (RESIDE). RESIDE highlights diverse data sources and image contents, \nand is divided into five subsets, each serving different training or evaluation \npurposes. We further provide a rich variety of criteria for dehazing algorithm \nevaluation, ranging from full-reference metrics, to no-reference metrics, to \nsubjective evaluation and the novel task-driven evaluation. Experiments on \nRESIDE sheds light on the comparisons and limitations of state-of-the-art \ndehazing algorithms, and suggest promising future directions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04143"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jingyi Wang, Jun Sun, Yifan Jia", "title": "Toward `verifying' a Water Treatment System. (arXiv:1712.04155v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04155", "type": "text/html"}], "timestampUsec": "1513141858490925", "comments": [], "summary": {"content": "<p>Modeling and verifying real-world cyber-physical systems are challenging, \nespecially so for complex systems where manually modeling is infeasible. In \nthis work, we report our experience on combining model learning and abstraction \nrefinement to analyze a challenging system, i.e., a real-world Secure Water \nTreatment (SWaT) system. Given a set of safety requirements, the objective is \nto either show that the system is safe with a high probability (so that a \nsystem shutdown is rarely triggered due to safety violation) or otherwise. As \nthe system is too complicated to be manually modelled, we apply latest \nautomatic model learning techniques to construct a set of Markov chains through \nabstraction and refinement, based on two long system execution logs (one for \ntraining and the other for testing). For each probabilistic property, we either \nreport it does not hold with a certain level of probabilistic confidence, or \nreport that it holds by showing the evidence in the form of an abstract Markov \nchain. The Markov chains can subsequently be implemented as runtime monitors in \nSWaT. This is the first case study of applying model learning techniques to a \nreal-world system as far as we know. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e53", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04155"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Niek Tax, Marlon Dumas", "title": "Mining Non-Redundant Sets of Generalizing Patterns from Sequence Databases. (arXiv:1712.04159v1 [cs.DS])", "alternate": [{"href": "http://arxiv.org/abs/1712.04159", "type": "text/html"}], "timestampUsec": "1513141858490924", "comments": [], "summary": {"content": "<p>Sequential pattern mining techniques extract patterns corresponding to \nfrequent subsequences from a sequence database. A practical limitation of these \ntechniques is that they overload the user with too many patterns. Local Process \nModel (LPM) mining is an alternative approach coming from the field of process \nmining. While in traditional sequential pattern mining, a pattern describes one \nsubsequence, an LPM captures a set of subsequences. Also, while traditional \nsequential patterns only match subsequences that are observed in the sequence \ndatabase, an LPM may capture subsequences that are not explicitly observed, but \nthat are related to observed subsequences. In other words, LPMs generalize the \nbehavior observed in the sequence database. These properties make it possible \nfor a set of LPMs to cover the behavior of a much larger set of sequential \npatterns. Yet, existing LPM mining techniques still suffer from the pattern \nexplosion problem because they produce sets of redundant LPMs. In this paper, \nwe propose several heuristics to mine a set of non-redundant LPMs either from a \nset of redundant LPMs or from a set of sequential patterns. We empirically \ncompare the proposed heuristics between them and against existing (local) \nprocess mining techniques in terms of coverage, precision, and complexity of \nthe produced sets of LPMs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e5b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04159"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yueh-Hua Wu, Shou-De Lin", "title": "A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents. (arXiv:1712.04172v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04172", "type": "text/html"}], "timestampUsec": "1513141858490923", "comments": [], "summary": {"content": "<p>This paper proposes a low-cost, easily realizable strategy to equip a \nreinforcement learning (RL) agent the capability of behaving ethically. Our \nmodel allows the designers of RL agents to solely focus on the task to achieve, \nwithout having to worry about the implementation of multiple trivial ethical \npatterns to follow. Based on the assumption that the majority of human \nbehavior, regardless which goals they are achieving, is ethical, our design \nintegrates human policy with the RL policy to achieve the target objective with \nless chance of violating the ethical code that human beings normally obey. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e62", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04172"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wenpin Jiao", "title": "Contradiction-Centricity: A Uniform Model for Formation of Swarm Intelligence and its Simulations. (arXiv:1712.04182v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04182", "type": "text/html"}], "timestampUsec": "1513141858490922", "comments": [], "summary": {"content": "<p>It is a grand challenge to model the emergence of swarm intelligence and many \nprinciples or models had been proposed. However, existing models do not catch \nthe nature of swarm intelligence and they are not generic enough to describe \nvarious types of emergence phenomena. In this work, we propose a \ncontradiction-centric model for emergence of swarm intelligence, in which \nindividuals' contradictions dominate their appearances whilst they are \nassociated and interacting to update their contradictions. This model \nhypothesizes that 1) the emergence of swarm intelligence is rooted in the \ndevelopment of contradictions of individuals and the interactions among \nassociated individuals and 2) swarm intelligence is essentially a combinative \nreflection of the configurations of contradictions inside individuals and the \ndistributions of contradictions among individuals. To verify the feasibility of \nthe model, we simulate four types of swarm intelligence. As the simulations \nshow, our model is truly generic and can describe the emergence of a variety of \nswarm intelligence, and it is also very simple and can be easily applied to \ndemonstrate the emergence of swarm intelligence without needing complicated \ncomputations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e69", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04182"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mehdi Mohammadi, Ala Al-Fuqaha, Sameh Sorour, Mohsen Guizani", "title": "Deep Learning for IoT Big Data and Streaming Analytics: A Survey. (arXiv:1712.04301v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04301", "type": "text/html"}], "timestampUsec": "1513141858490921", "comments": [], "summary": {"content": "<p>In the era of the Internet of Things (IoT), an enormous amount of sensing \ndevices collect and/or generate various sensory data over time for a wide range \nof fields and applications. Based on the nature of the application, these \ndevices will result in big or fast/real-time data streams. Applying analytics \nover such data streams to discover new information, predict future insights, \nand make control decisions is a crucial process that makes IoT a worthy \nparadigm for businesses and a quality-of-life improving technology. In this \npaper, we provide a thorough overview on using a class of advanced machine \nlearning techniques, namely Deep Learning (DL), to facilitate the analytics and \nlearning in the IoT domain. We start by articulating IoT data characteristics \nand identifying two major treatments for IoT data from a machine learning \nperspective, namely IoT big data analytics and IoT streaming data analytics. We \nalso discuss why DL is a promising approach to achieve the desired analytics in \nthese types of data and applications. The potential of using emerging DL \ntechniques for IoT data analytics are then discussed, and its promises and \nchallenges are introduced. We present a comprehensive background on different \nDL architectures and algorithms. We also analyze and summarize major reported \nresearch attempts that leveraged DL in the IoT domain. The smart IoT devices \nthat have incorporated DL in their intelligence background are also discussed. \nDL implementation approaches on the fog and cloud centers in support of IoT \napplications are also surveyed. Finally, we shed light on some challenges and \npotential directions for future research. At the end of each section, we \nhighlight the lessons learned based on our experiments and review of the recent \nliterature. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e71", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04301"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mihai Nadin", "title": "In folly ripe. In reason rotten. Putting machine theology to rest. (arXiv:1712.04306v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04306", "type": "text/html"}], "timestampUsec": "1513141858490920", "comments": [], "summary": {"content": "<p>Computation has changed the world more than any previous expressions of \nknowledge. In its particular algorithmic embodiment, it offers a perspective, \nwithin which the digital computer (one of many possible) exercises a role \nreminiscent of theology. Since it is closed to meaning, algorithmic digital \ncomputation can at most mimic the creative aspects of life. AI, in the \nperspective of time, proved to be less an acronym for artificial intelligence \nand more of automating tasks associated with intelligence. The entire \ndevelopment led to the hypostatized role of the machine: outputting nothing \nelse but reality, including that of the humanity that made the machine happen. \nThe convergence machine called deep learning is only the latest form through \nwhich the deterministic theology of the machine claims more than what extremely \neffective data processing actually is. A new understanding of complexity, as \nwell as the need to distinguish between the reactive nature of the artificial \nand the anticipatory nature of the living are suggested as practical responses \nto the challenges posed by machine theology. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e75", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04306"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gopal P. Sarma, Nick J. Hay, Adam Safron", "title": "AI Safety and Reproducibility: Establishing Robust Foundations for the Neuroscience of Human Values. (arXiv:1712.04307v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04307", "type": "text/html"}], "timestampUsec": "1513141858490919", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a28babb\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a28babb&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose the creation of a systematic effort to identify and replicate key \nfindings in neuroscience and allied fields related to understanding human \nvalues. Our aim is to ensure that research underpinning the value alignment \nproblem of artificial intelligence has been sufficiently validated to play a \nrole in the design of AI systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e7d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04307"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Claudio Gallicchio, Alessio Micheli", "title": "Deep Echo State Network (DeepESN): A Brief Survey. (arXiv:1712.04323v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04323", "type": "text/html"}], "timestampUsec": "1513141858490918", "comments": [], "summary": {"content": "<p>The study of deep recurrent neural networks (RNNs) and, in particular, of \ndeep Reservoir Computing (RC) is gaining an increasing research attention in \nthe neural networks community. The recently introduced deep Echo State Network \n(deepESN) model opened the way to an extremely efficient approach for designing \ndeep neural networks for temporal data. At the same time, the study of deepESNs \nallowed to shed light on the intrinsic properties of state dynamics developed \nby hierarchical compositions of recurrent layers, i.e. on the bias of depth in \nRNNs architectural design. In this paper, we summarize the advancements in the \ndevelopment, analysis and applications of deepESNs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e84", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04323"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Patrick Klose, Rudolf Mester", "title": "Simulated Autonomous Driving on Realistic Road Networks using Deep Reinforcement Learning. (arXiv:1712.04363v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04363", "type": "text/html"}], "timestampUsec": "1513141858490917", "comments": [], "summary": {"content": "<p>Using Deep Reinforcement Learning (DRL) can be a promising approach to handle \ntasks in the field of (simulated) autonomous driving, whereby recent \npublications only consider learning in unusual driving environments. This paper \noutlines a developed software, which instead can be used for evaluating DRL \nalgorithms based on realistic road networks and therefore in more usual driving \nenvironments. Furthermore, we identify difficulties when DRL algorithms are \napplied to tasks, in which it is not only important to reach a goal, but also \nhow this goal is reached. We conclude this paper by presenting the results of \nan application of a new DRL algorithm, which can partly solve these problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e8a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04363"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Amrita Gupta, Mehrdad Farajtabar, Bistra Dilkina, Hongyuan Zha", "title": "Hawkes Processes for Invasive Species Modeling and Management. (arXiv:1712.04386v1 [q-bio.PE])", "alternate": [{"href": "http://arxiv.org/abs/1712.04386", "type": "text/html"}], "timestampUsec": "1513141858490916", "comments": [], "summary": {"content": "<p>The spread of invasive species to new areas threatens the stability of \necosystems and causes major economic losses in agriculture and forestry. We \npropose a novel approach to minimizing the spread of an invasive species given \na limited intervention budget. We first model invasive species propagation \nusing Hawkes processes, and then derive closed-form expressions for \ncharacterizing the effect of an intervention action on the invasion process. We \nuse this to obtain an optimal intervention plan based on an integer programming \nformulation, and compare the optimal plan against several \necologically-motivated heuristic strategies used in practice. We present an \nempirical study of two variants of the invasive control problem: minimizing the \nfinal rate of invasions, and minimizing the number of invasions at the end of a \ngiven time horizon. Our results show that the optimized intervention achieves \nnearly the same level of control that would be attained by completely \neradicating the species, with a 20% cost saving. Additionally, we design a \nheuristic intervention strategy based on a combination of the density and life \nstage of the invasive individuals, and find that it comes surprisingly close to \nthe optimized strategy, suggesting that this could serve as a good rule of \nthumb in invasive species management. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e95", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04386"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ignacio Mart&#xed;n, Jos&#xe9; Alberto Hern&#xe1;ndez, Alfonso Mu&#xf1;oz, Antonio Guzm&#xe1;n", "title": "Android Malware Characterization using Metadata and Machine Learning Techniques. (arXiv:1712.04402v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1712.04402", "type": "text/html"}], "timestampUsec": "1513141858490915", "comments": [], "summary": {"content": "<p>Android Malware has emerged as a consequence of the increasing popularity of \nsmartphones and tablets. While most previous work focuses on inherent \ncharacteristics of Android apps to detect malware, this study analyses indirect \nfeatures and meta-data to identify patterns in malware applications. Our \nexperiments show that: (1) the permissions used by an application offer only \nmoderate performance results; (2) other features publicly available at Android \nMarkets are more relevant in detecting malware, such as the application \ndeveloper and certificate issuer, and (3) compact and efficient classifiers can \nbe constructed for the early detection of malware applications prior to code \ninspection or sandboxing. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e9b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04402"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhe Wu, Bharat Singh, Larry S. Davis, V. S. Subrahmanian", "title": "Deception Detection in Videos. (arXiv:1712.04415v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04415", "type": "text/html"}], "timestampUsec": "1513141858490914", "comments": [], "summary": {"content": "<p>We present a system for covert automated deception detection in real-life \ncourtroom trial videos. We study the importance of different modalities like \nvision, audio and text for this task. On the vision side, our system uses \nclassifiers trained on low level video features which predict human \nmicro-expressions. We show that predictions of high-level micro-expressions can \nbe used as features for deception prediction. Surprisingly, IDT (Improved Dense \nTrajectory) features which have been widely used for action recognition, are \nalso very good at predicting deception in videos. We fuse the score of \nclassifiers trained on IDT features and high-level micro-expressions to improve \nperformance. MFCC (Mel-frequency Cepstral Coefficients) features from the audio \ndomain also provide a significant boost in performance, while information from \ntranscripts is not very beneficial for our system. Using various classifiers, \nour automated system obtains an AUC of 0.877 (10-fold cross-validation) when \nevaluated on subjects which were not part of the training set. Even though \nstate-of-the-art methods use human annotations of micro-expressions for \ndeception detection, our fully automated approach outperforms them by 5%. When \ncombined with human annotations of micro-expressions, our AUC improves to \n0.922. We also present results of a user-study to analyze how well do average \nhumans perform on this task, what modalities they use for deception detection \nand how they perform if only one modality is accessible. Our project page can \nbe found at \\url{https://doubaibai.github.io/DARE/}. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ea4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04415"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Konstantinos Chatzilygeroudis, Vassilis Vassiliades, Jean-Baptiste Mouret", "title": "Reset-free Trial-and-Error Learning for Robot Damage Recovery. (arXiv:1610.04213v4 [cs.RO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.04213", "type": "text/html"}], "timestampUsec": "1513141858490913", "comments": [], "summary": {"content": "<p>The high probability of hardware failures prevents many advanced robots \n(e.g., legged robots) from being confidently deployed in real-world situations \n(e.g., post-disaster rescue). Instead of attempting to diagnose the failures, \nrobots could adapt by trial-and-error in order to be able to complete their \ntasks. In this situation, damage recovery can be seen as a Reinforcement \nLearning (RL) problem. However, the best RL algorithms for robotics require the \nrobot and the environment to be reset to an initial state after each episode, \nthat is, the robot is not learning autonomously. In addition, most of the RL \nmethods for robotics do not scale well with complex robots (e.g., walking \nrobots) and either cannot be used at all or take too long to converge to a \nsolution (e.g., hours of learning). In this paper, we introduce a novel \nlearning algorithm called \"Reset-free Trial-and-Error\" (RTE) that (1) breaks \nthe complexity by pre-generating hundreds of possible behaviors with a dynamics \nsimulator of the intact robot, and (2) allows complex robots to quickly recover \nfrom damage while completing their tasks and taking the environment into \naccount. We evaluate our algorithm on a simulated wheeled robot, a simulated \nsix-legged robot, and a real six-legged walking robot that are damaged in \nseveral ways (e.g., a missing leg, a shortened leg, faulty motor, etc.) and \nwhose objective is to reach a sequence of targets in an arena. Our experiments \nshow that the robots can recover most of their locomotion abilities in an \nenvironment with obstacles, and without any human intervention. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127eab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.04213"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hanul Shin, Jung Kwon Lee, Jaehong Kim, Jiwon Kim", "title": "Continual Learning with Deep Generative Replay. (arXiv:1705.08690v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08690", "type": "text/html"}], "timestampUsec": "1513141858490912", "comments": [], "summary": {"content": "<p>Attempts to train a comprehensive artificial intelligence capable of solving \nmultiple tasks have been impeded by a chronic problem called catastrophic \nforgetting. Although simply replaying all previous data alleviates the problem, \nit requires large memory and even worse, often infeasible in real world \napplications where the access to past data is limited. Inspired by the \ngenerative nature of hippocampus as a short-term memory system in primate \nbrain, we propose the Deep Generative Replay, a novel framework with a \ncooperative dual model architecture consisting of a deep generative model \n(\"generator\") and a task solving model (\"solver\"). With only these two models, \ntraining data for previous tasks can easily be sampled and interleaved with \nthose for a new task. We test our methods in several sequential learning \nsettings involving image classification tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127eb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08690"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shai Shalev-Shwartz, Shaked Shammah, Amnon Shashua", "title": "On a Formal Model of Safe and Scalable Self-driving Cars. (arXiv:1708.06374v3 [cs.RO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.06374", "type": "text/html"}], "timestampUsec": "1513141858490911", "comments": [], "summary": {"content": "<p>In recent years, car makers and tech companies have been racing towards self \ndriving cars. It seems that the main parameter in this race is who will have \nthe first car on the road. The goal of this paper is to add to the equation two \nadditional crucial parameters. The first is standardization of safety assurance \n--- what are the minimal requirements that every self-driving car must satisfy, \nand how can we verify these requirements. The second parameter is scalability \n--- engineering solutions that lead to unleashed costs will not scale to \nmillions of cars, which will push interest in this field into a niche academic \ncorner, and drive the entire field into a \"winter of autonomous driving\". In \nthe first part of the paper we propose a white-box, interpretable, mathematical \nmodel for safety assurance, which we call Responsibility-Sensitive Safety \n(RSS). In the second part we describe a design of a system that adheres to our \nsafety assurance requirements and is scalable to millions of cars. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127eb9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.06374"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Seunghyun Yoon, Pablo Estrada, Kyomin Jung", "title": "Synonym Discovery with Etymology-based Word Embeddings. (arXiv:1709.10445v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.10445", "type": "text/html"}], "timestampUsec": "1513141858490910", "comments": [], "summary": {"content": "<p>We propose a novel approach to learn word embeddings based on an extended \nversion of the distributional hypothesis. Our model derives word embedding \nvectors using the etymological composition of words, rather than the context in \nwhich they appear. It has the strength of not requiring a large text corpus, \nbut instead it requires reliable access to etymological roots of words, making \nit specially fit for languages with logographic writing systems. The model \nconsists on three steps: (1) building an etymological graph, which is a \nbipartite network of words and etymological roots, (2) obtaining the \nbiadjacency matrix of the etymological graph and reducing its dimensionality, \n(3) using columns/rows of the resulting matrices as embedding vectors. We test \nour model in the Chinese and Sino-Korean vocabularies. Our graphs are formed by \na set of 117,000 Chinese words, and a set of 135,000 Sino-Korean words. In both \ncases we show that our model performs well in the task of synonym discovery. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ec1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.10445"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sima Behpour, Kris M. Kitani, Brian D. Ziebart", "title": "ADA: A Game-Theoretic Perspective on Data Augmentation for Object Detection. (arXiv:1710.07735v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.07735", "type": "text/html"}], "timestampUsec": "1513141858490909", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a28bdd6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a28bdd6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The use of random perturbations of ground truth data, such as random \ntranslation or scaling of bounding boxes, is a common heuristic used for data \naugmentation that has been shown to prevent overfitting and improve \ngeneralization. Since the design of data augmentation is largely guided by \nreported best practices, it is difficult to understand if those design choices \nare optimal. To provide a more principled perspective, we develop a \ngame-theoretic interpretation of data augmentation in the context of object \ndetection. We aim to find an optimal adversarial perturbations of the ground \ntruth data (i.e., the worst case perturbations) that forces the object bounding \nbox predictor to learn from the hardest distribution of perturbed examples for \nbetter test-time performance. We establish that the game theoretic solution, \nthe Nash equilibrium, provides both an optimal predictor and optimal data \naugmentation distribution. We show that our adversarial method of training a \npredictor can significantly improve test time performance for the task of \nobject detection. On the ImageNet object detection task, our adversarial \napproach improves performance by over 16\\% compared to the best performing data \naugmentation method \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ec7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.07735"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kevin T. Feigelis, Blue Sheffer, Daniel L. K. Yamins", "title": "Modular Continual Learning in a Unified Visual Environment. (arXiv:1711.07425v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07425", "type": "text/html"}], "timestampUsec": "1513141858490908", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a2d3167\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a2d3167&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A core aspect of human intelligence is the ability to learn new tasks quickly \nand switch between them flexibly. Here, we describe a modular continual \nreinforcement learning paradigm inspired by these abilities. We first introduce \na visual interaction environment that allows many types of tasks to be unified \nin a single framework. We then describe a reward map prediction scheme that \nlearns new tasks robustly in the very large state and action spaces required by \nsuch an environment. We investigate how properties of module architecture \ninfluence efficiency of task learning, showing that a module motif \nincorporating specific design principles (e.g. early bottlenecks, low-order \npolynomial nonlinearities, and symmetry) significantly outperforms more \nstandard neural network motifs, needing fewer training examples and fewer \nneurons to achieve high levels of performance. Finally, we present a \nmeta-controller architecture for task switching based on a dynamic neural \nvoting scheme, which allows new modules to use information learned from \npreviously-seen tasks to substantially improve their own learning efficiency. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ecd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07425"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "John P Dickerson, Karthik A Sankararaman, Aravind Srinivasan, Pan Xu", "title": "Allocation Problems in Ride-Sharing Platforms: Online Matching with Offline Reusable Resources. (arXiv:1711.08345v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08345", "type": "text/html"}], "timestampUsec": "1513141858490907", "comments": [], "summary": {"content": "<p>Bipartite matching markets pair agents on one side of a market with agents, \nitems, or contracts on the opposing side. Prior work addresses online bipartite \nmatching markets, where agents arrive over time and are dynamically matched to \na known set of disposable resources. In this paper, we propose a new model, \nOnline Matching with (offline) Reusable Resources under Known Adversarial \nDistributions (OM-RR-KAD), in which resources on the offline side are reusable \ninstead of disposable; that is, once matched, resources become available again \nat some point in the future. We show that our model is tractable by presenting \nan LP-based adaptive algorithm that achieves an online competitive ratio of 1/2 \n- eps for any given eps greater than 0. We also show that no non-adaptive \nalgorithm can achieve a ratio of 1/2 + o(1) based on the same benchmark LP. \nThrough a data-driven analysis on a massive openly-available dataset, we show \nour model is robust enough to capture the application of taxi dispatching \nservices and ride-sharing systems. We also present heuristics that perform well \nin practice. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ed5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08345"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christopher Streiffer, Huan Chen, Theophilus Benson, Asim Kadav", "title": "DeepConfig: Automating Data Center Network Topologies Management with Machine Learning. (arXiv:1712.03890v1 [cs.NI] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1712.03890", "type": "text/html"}], "timestampUsec": "1513141858490906", "comments": [], "summary": {"content": "<p>In recent years, many techniques have been developed to improve the \nperformance and efficiency of data center networks. While these techniques \nprovide high accuracy, they are often designed using heuristics that leverage \ndomain-specific properties of the workload or hardware. \n</p> \n<p>In this vision paper, we argue that many data center networking techniques, \ne.g., routing, topology augmentation, energy savings, with diverse goals \nactually share design and architectural similarity. We present a design for \ndeveloping general intermediate representations of network topologies using \ndeep learning that is amenable to solving classes of data center problems. We \ndevelop a framework, DeepConfig, that simplifies the processing of configuring \nand training deep learning agents that use the intermediate representation to \nlearns different tasks. To illustrate the strength of our approach, we \nconfigured, implemented, and evaluated a DeepConfig-Agent that tackles the data \ncenter topology augmentation problem. Our initial results are promising --- \nDeepConfig performs comparably to the optimal. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127edb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03890"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Brian Dolhansky, Cristian Canton Ferrer", "title": "Eye In-Painting with Exemplar Generative Adversarial Networks. (arXiv:1712.03999v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.03999", "type": "text/html"}], "timestampUsec": "1513141858490905", "comments": [], "summary": {"content": "<p>This paper introduces a novel approach to in-painting where the identity of \nthe object to remove or change is preserved and accounted for at inference \ntime: Exemplar GANs (ExGANs). ExGANs are a type of conditional GAN that utilize \nexemplar information to produce high-quality, personalized in painting results. \nWe propose using exemplar information in the form of a reference image of the \nregion to in-paint, or a perceptual code describing that object. Unlike \nprevious conditional GAN formulations, this extra information can be inserted \nat multiple points within the adversarial network, thus increasing its \ndescriptive power. We show that ExGANs can produce photo-realistic personalized \nin-painting results that are both perceptually and semantically plausible by \napplying them to the task of closed to-open eye in-painting in natural \npictures. A new benchmark dataset is also introduced for the task of eye \nin-painting for future comparisons. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ee4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03999"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jason Poulos, Rafael Valle", "title": "Attention networks for image-to-text. (arXiv:1712.04046v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.04046", "type": "text/html"}], "timestampUsec": "1513141858490904", "comments": [], "summary": {"content": "<p>The paper approaches the problem of image-to-text with attention-based \nencoder-decoder networks that are trained to handle sequences of characters \nrather than words. We experiment on lines of text from a popular handwriting \ndatabase with different attention mechanisms for the decoder. The model trained \nwith softmax attention achieves the lowest test error, outperforming several \nother RNN-based models. Our results show that softmax attention is able to \nlearn a linear alignment whereas the alignment generated by sigmoid attention \nis linear but much less precise. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127eeb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04046"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zinan Lin, Ashish Khetan, Giulia Fanti, Sewoong Oh", "title": "PacGAN: The power of two samples in generative adversarial networks. (arXiv:1712.04086v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04086", "type": "text/html"}], "timestampUsec": "1513141858490903", "comments": [], "summary": {"content": "<p>Generative adversarial networks (GANs) are innovative techniques for learning \ngenerative models of complex data distributions from samples. Despite \nremarkable recent improvements in generating realistic images, one of their \nmajor shortcomings is the fact that in practice, they tend to produce samples \nwith little diversity, even when trained on diverse datasets. This phenomenon, \nknown as mode collapse, has been the main focus of several recent advances in \nGANs. Yet there is little understanding of why mode collapse happens and why \nexisting approaches are able to mitigate mode collapse. We propose a principled \napproach to handling mode collapse, which we call packing. The main idea is to \nmodify the discriminator to make decisions based on multiple samples from the \nsame class, either real or artificially generated. We borrow analysis tools \nfrom binary hypothesis testing---in particular the seminal result of Blackwell \n[Bla53]---to prove a fundamental connection between packing and mode collapse. \nWe show that packing naturally penalizes generators with mode collapse, thereby \nfavoring generator distributions with less mode collapse during the training \nprocess. Numerical experiments on benchmark datasets suggests that packing \nprovides significant improvements in practice as well. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ef6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04086"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alex Lamb, Devon Hjelm, Yaroslav Ganin, Joseph Paul Cohen, Aaron Courville, Yoshua Bengio", "title": "GibbsNet: Iterative Adversarial Inference for Deep Graphical Models. (arXiv:1712.04120v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04120", "type": "text/html"}], "timestampUsec": "1513141858490902", "comments": [], "summary": {"content": "<p>Directed latent variable models that formulate the joint distribution as \n$p(x,z) = p(z) p(x \\mid z)$ have the advantage of fast and exact sampling. \nHowever, these models have the weakness of needing to specify $p(z)$, often \nwith a simple fixed prior that limits the expressiveness of the model. \nUndirected latent variable models discard the requirement that $p(z)$ be \nspecified with a prior, yet sampling from them generally requires an iterative \nprocedure such as blocked Gibbs-sampling that may require many steps to draw \nsamples from the joint distribution $p(x, z)$. We propose a novel approach to \nlearning the joint distribution between the data and a latent code which uses \nan adversarially learned iterative procedure to gradually refine the joint \ndistribution, $p(x, z)$, to better match with the data distribution on each \nstep. GibbsNet is the best of both worlds both in theory and in practice. \nAchieving the speed and simplicity of a directed latent variable model, it is \nguaranteed (assuming the adversarial game reaches the virtual training criteria \nglobal minimum) to produce samples from $p(x, z)$ with only a few sampling \niterations. Achieving the expressiveness and flexibility of an undirected \nlatent variable model, GibbsNet does away with the need for an explicit $p(z)$ \nand has the ability to do attribute prediction, class-conditional generation, \nand joint image-attribute modeling in a single model which is not trained for \nany of these specific tasks. We show empirically that GibbsNet is able to learn \na more complex $p(z)$ and show that this leads to improved inpainting and \niterative refinement of $p(x, z)$ for dozens of steps and stable generation \nwithout collapse for thousands of steps, despite being trained on only a few \nsteps. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f04", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04120"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Utkarsh Porwal, Smruthi Mukund", "title": "Outlier Detection by Consistent Data Selection Method. (arXiv:1712.04129v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04129", "type": "text/html"}], "timestampUsec": "1513141858490901", "comments": [], "summary": {"content": "<p>Often the challenge associated with tasks like fraud and spam detection[1] is \nthe lack of all likely patterns needed to train suitable supervised learning \nmodels. In order to overcome this limitation, such tasks are attempted as \noutlier or anomaly detection tasks. We also hypothesize that out- liers have \nbehavioral patterns that change over time. Limited data and continuously \nchanging patterns makes learning significantly difficult. In this work we are \nproposing an approach that detects outliers in large data sets by relying on \ndata points that are consistent. The primary contribution of this work is that \nit will quickly help retrieve samples for both consistent and non-outlier data \nsets and is also mindful of new outlier patterns. No prior knowledge of each \nset is required to extract the samples. The method consists of two phases, in \nthe first phase, consistent data points (non- outliers) are retrieved by an \nensemble method of unsupervised clustering techniques and in the second phase a \none class classifier trained on the consistent data point set is ap- plied on \nthe remaining sample set to identify the outliers. The approach is tested on \nthree publicly available data sets and the performance scores are competitive. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f0e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04129"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aidin Ferdowsi, Ursula Challita, Walid Saad", "title": "Deep Learning for Reliable Mobile Edge Analytics in Intelligent Transportation Systems. (arXiv:1712.04135v1 [cs.IT])", "alternate": [{"href": "http://arxiv.org/abs/1712.04135", "type": "text/html"}], "timestampUsec": "1513141858490900", "comments": [], "summary": {"content": "<p>Intelligent transportation systems (ITSs) will be a major component of \ntomorrow's smart cities. However, realizing the true potential of ITSs requires \nultra-low latency and reliable data analytics solutions that can combine, in \nreal-time, a heterogeneous mix of data stemming from the ITS network and its \nenvironment. Such data analytics capabilities cannot be provided by \nconventional cloud-centric data processing techniques whose communication and \ncomputing latency can be high. Instead, edge-centric solutions that are \ntailored to the unique ITS environment must be developed. In this paper, an \nedge analytics architecture for ITSs is introduced in which data is processed \nat the vehicle or roadside smart sensor level in order to overcome the ITS \nlatency and reliability challenges. With a higher capability of passengers' \nmobile devices and intra-vehicle processors, such a distributed edge computing \narchitecture can leverage deep learning techniques for reliable mobile sensing \nin ITSs. In this context, the ITS mobile edge analytics challenges pertaining \nto heterogeneous data, autonomous control, vehicular platoon control, and \ncyber-physical security are investigated. Then, different deep learning \nsolutions for such challenges are proposed. The proposed deep learning \nsolutions will enable ITS edge analytics by endowing the ITS devices with \npowerful computer vision and signal processing functions. Preliminary results \nshow that the proposed edge analytics architecture, coupled with the power of \ndeep learning algorithms, can provide a reliable, secure, and truly smart \ntransportation environment. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04135"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Song Cheng, Jing Chen, Lei Wang", "title": "Information Perspective to Probabilistic Modeling: Boltzmann Machines versus Born Machines. (arXiv:1712.04144v1 [physics.data-an])", "alternate": [{"href": "http://arxiv.org/abs/1712.04144", "type": "text/html"}], "timestampUsec": "1513141858490899", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a2d33ed\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a2d33ed&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We compare and contrast the statistical physics and quantum physics inspired \napproaches for unsupervised generative modeling of classical data. The two \napproaches represent probabilities of observed data using energy-based models \nand quantum states respectively.Classical and quantum information patterns of \nthe target datasets therefore provide principled guidelines for structural \ndesign and learning in these two approaches. Taking the restricted Boltzmann \nmachines (RBM) as an example, we analyze the information theoretical bounds of \nthe two approaches. We verify our reasonings by comparing the performance of \nRBMs of various architectures on the standard MNIST datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f31", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04144"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sho Sonoda, Noboru Murata", "title": "Transportation analysis of denoising autoencoders: a novel method for analyzing deep neural networks. (arXiv:1712.04145v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04145", "type": "text/html"}], "timestampUsec": "1513141858490898", "comments": [], "summary": {"content": "<p>The feature map obtained from the denoising autoencoder (DAE) is investigated \nby determining transportation dynamics of the DAE, which is a cornerstone for \ndeep learning. Despite the rapid development in its application, deep neural \nnetworks remain analytically unexplained, because the feature maps are nested \nand parameters are not faithful. In this paper, we address the problem of the \nformulation of nested complex of parameters by regarding the feature map as a \ntransport map. Even when a feature map has different dimensions between input \nand output, we can regard it as a transportation map by considering that both \nthe input and output spaces are embedded in a common high-dimensional space. In \naddition, the trajectory is a geometric object and thus, is independent of \nparameterization. In this manner, transportation can be regarded as a universal \ncharacter of deep neural networks. By determining and analyzing the \ntransportation dynamics, we can understand the behavior of a deep neural \nnetwork. In this paper, we investigate a fundamental case of deep neural \nnetworks: the DAE. We derive the transport map of the DAE, and reveal that the \ninfinitely deep DAE transports mass to decrease a certain quantity, such as \nentropy, of the data distribution. These results though analytically simple, \nshed light on the correspondence between deep neural networks and the \nWasserstein gradient flows. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f38", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04145"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Salman Salloum, Yulin He, Joshua Zhexue Huang, Xiaoliang Zhang, Tamer Z. Emara", "title": "A Random Sample Partition Data Model for Big Data Analysis. (arXiv:1712.04146v1 [cs.DC])", "alternate": [{"href": "http://arxiv.org/abs/1712.04146", "type": "text/html"}], "timestampUsec": "1513141858490897", "comments": [], "summary": {"content": "<p>Big data sets must be carefully partitioned into statistically similar data \nsubsets that can be used as representative samples for big data analysis tasks. \nIn this paper, we propose the random sample partition (RSP) to represent a big \ndata set as a set of non-overlapping data subsets, i.e. RSP data blocks, where \neach RSP data block has the same probability distribution with the whole big \ndata set. Then, the block-based sampling is used to directly select \nrepresentative samples for a variety of data analysis tasks. We show how RSP \ndata blocks can be employed to estimate statistics and build models which are \nequivalent (or approximate) to those from the whole big data set. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04146"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Irene Teinemaa, Marlon Dumas, Anna Leontjeva, Fabrizio Maria Maggi", "title": "Temporal Stability in Predictive Process Monitoring. (arXiv:1712.04165v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04165", "type": "text/html"}], "timestampUsec": "1513141858490896", "comments": [], "summary": {"content": "<p>Predictive business process monitoring is concerned with the analysis of \nevents produced during the execution of a business process in order to predict \nas early as possible the final outcome of an ongoing case. Traditionally, \npredictive process monitoring methods are optimized with respect to accuracy. \nHowever, in environments where users make decisions and take actions in \nresponse to the predictions they receive, it is equally important to optimize \nthe stability of the successive predictions made for each case. To this end, \nthis paper defines a notion of temporal stability for predictive process \nmonitoring and evaluates existing methods with respect to both temporal \nstability and accuracy. We find that methods based on XGBoost and LSTM neural \nnetworks exhibit the highest temporal stability. We then show that temporal \nstability can be enhanced by hyperparameter-optimizing random forests and \nXGBoost classifiers with respect to inter-run stability. Finally, we show that \ntime series smoothing techniques can further enhance temporal stability at the \nexpense of slightly lower accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f4f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04165"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hiroki Mori, Keisuke Kawano, Hiroki Yokoyama", "title": "Causal Patterns: Extraction of multiple causal relationships by Mixture of Probabilistic Partial Canonical Correlation Analysis. (arXiv:1712.04221v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.04221", "type": "text/html"}], "timestampUsec": "1513141858490895", "comments": [], "summary": {"content": "<p>In this paper, we propose a mixture of probabilistic partial canonical \ncorrelation analysis (MPPCCA) that extracts the Causal Patterns from two \nmultivariate time series. Causal patterns refer to the signal patterns within \ninteractions of two elements having multiple types of mutually causal \nrelationships, rather than a mixture of simultaneous correlations or the \nabsence of presence of a causal relationship between the elements. In \nmultivariate statistics, partial canonical correlation analysis (PCCA) \nevaluates the correlation between two multivariates after subtracting the \neffect of the third multivariate. PCCA can calculate the Granger Causal- ity \nIndex (which tests whether a time-series can be predicted from an- other \ntime-series), but is not applicable to data containing multiple partial \ncanonical correlations. After introducing the MPPCCA, we propose an \nexpectation-maxmization (EM) algorithm that estimates the parameters and latent \nvariables of the MPPCCA. The MPPCCA is expected to ex- tract multiple partial \ncanonical correlations from data series without any supervised signals to split \nthe data as clusters. The method was then eval- uated in synthetic data \nexperiments. In the synthetic dataset, our method estimated the multiple \npartial canonical correlations more accurately than the existing method. To \ndetermine the types of patterns detectable by the method, experiments were also \nconducted on real datasets. The method estimated the communication patterns In \nmotion-capture data. The MP- PCCA is applicable to various type of signals such \nas brain signals, human communication and nonlinear complex multibody systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f58", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04221"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Soumitro Chakrabarty, Emanu&#xeb;l A. P. Habets", "title": "Multi-Speaker Localization Using Convolutional Neural Network Trained with Noise. (arXiv:1712.04276v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1712.04276", "type": "text/html"}], "timestampUsec": "1513141858490894", "comments": [], "summary": {"content": "<p>The problem of multi-speaker localization is formulated as a multi-class \nmulti-label classification problem, which is solved using a convolutional \nneural network (CNN) based source localization method. Utilizing the common \nassumption of disjoint speaker activities, we propose a novel method to train \nthe CNN using synthesized noise signals. The proposed localization method is \nevaluated for two speakers and compared to a well-known steered response power \nmethod. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f5e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04276"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chuang Wang, Jonathan Mattingly, Yue M. Lu", "title": "Scaling Limit: Exact and Tractable Analysis of Online Learning Algorithms with Applications to Regularized Regression and PCA. (arXiv:1712.04332v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04332", "type": "text/html"}], "timestampUsec": "1513141858490893", "comments": [], "summary": {"content": "<p>We present a framework for analyzing the exact dynamics of a class of online \nlearning algorithms in the high-dimensional scaling limit. Our results are \napplied to two concrete examples: online regularized linear regression and \nprincipal component analysis. As the ambient dimension tends to infinity, and \nwith proper time scaling, we show that the time-varying joint empirical \nmeasures of the target feature vector and its estimates provided by the \nalgorithms will converge weakly to a deterministic measured-valued process that \ncan be characterized as the unique solution of a nonlinear PDE. Numerical \nsolutions of this PDE can be efficiently obtained. These solutions lead to \nprecise predictions of the performance of the algorithms, as many practical \nperformance metrics are linear functionals of the joint empirical measures. In \naddition to characterizing the dynamic performance of online learning \nalgorithms, our asymptotic analysis also provides useful insights. In \nparticular, in the high-dimensional limit, and due to exchangeability, the \noriginal coupled dynamics associated with the algorithms will be asymptotically \n\"decoupled\", with each coordinate independently solving a 1-D effective \nminimization problem via stochastic gradient descent. Exploiting this insight \nfor nonconvex optimization problems may prove an interesting line of future \nresearch. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f65", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04332"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Luis Perez", "title": "Predicting Yelp Star Reviews Based on Network Structure with Deep Learning. (arXiv:1712.04350v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04350", "type": "text/html"}], "timestampUsec": "1513141858490892", "comments": [], "summary": {"content": "<p>In this paper, we tackle the real-world problem of predicting Yelp \nstar-review rating based on business features (such as images, descriptions), \nuser features (average previous ratings), and, of particular interest, network \nproperties (which businesses has a user rated before). We compare multiple \nmodels on different sets of features -- from simple linear regression on \nnetwork features only to deep learning models on network and item features. \n</p> \n<p>In recent years, breakthroughs in deep learning have led to increased \naccuracy in common supervised learning tasks, such as image classification, \ncaptioning, and language understanding. However, the idea of combining deep \nlearning with network feature and structure appears to be novel. While the \nproblem of predicting future interactions in a network has been studied at \nlength, these approaches have often ignored either node-specific data or global \nstructure. \n</p> \n<p>We demonstrate that taking a mixed approach combining both node-level \nfeatures and network information can effectively be used to predict Yelp-review \nstar ratings. We evaluate on the Yelp dataset by splitting our data along the \ntime dimension (as would naturally occur in the real-world) and comparing our \nmodel against others which do no take advantage of the network structure and/or \ndeep learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f70", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04350"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Farshid Rayhan, Sajid Ahmed, Asif Mahbub, Md. Rafsan Jani, Swakkhar Shatabda, Dewan Md. Farid", "title": "CUSBoost: Cluster-based Under-sampling with Boosting for Imbalanced Classification. (arXiv:1712.04356v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04356", "type": "text/html"}], "timestampUsec": "1513141858490891", "comments": [], "summary": {"content": "<p>Class imbalance classification is a challenging research problem in data \nmining and machine learning, as most of the real-life datasets are often \nimbalanced in nature. Existing learning algorithms maximise the classification \naccuracy by correctly classifying the majority class, but misclassify the \nminority class. However, the minority class instances are representing the \nconcept with greater interest than the majority class instances in real-life \napplications. Recently, several techniques based on sampling methods \n(under-sampling of the majority class and over-sampling the minority class), \ncost-sensitive learning methods, and ensemble learning have been used in the \nliterature for classifying imbalanced datasets. In this paper, we introduce a \nnew clustering-based under-sampling approach with boosting (AdaBoost) \nalgorithm, called CUSBoost, for effective imbalanced classification. The \nproposed algorithm provides an alternative to RUSBoost (random under-sampling \nwith AdaBoost) and SMOTEBoost (synthetic minority over-sampling with AdaBoost) \nalgorithms. We evaluated the performance of CUSBoost algorithm with the \nstate-of-the-art methods based on ensemble learning like AdaBoost, RUSBoost, \nSMOTEBoost on 13 imbalance binary and multi-class datasets with various \nimbalance ratios. The experimental results show that the CUSBoost is a \npromising and effective approach for dealing with highly imbalanced datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f78", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04356"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexander Sage, Eirikur Agustsson, Radu Timofte, Luc Van Gool", "title": "Logo Synthesis and Manipulation with Clustered Generative Adversarial Networks. (arXiv:1712.04407v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.04407", "type": "text/html"}], "timestampUsec": "1513141858490890", "comments": [], "summary": {"content": "<p>Designing a logo for a new brand is a lengthy and tedious back-and-forth \nprocess between a designer and a client. In this paper we explore to what \nextent machine learning can solve the creative task of the designer. For this, \nwe build a dataset -- LLD -- of 600k+ logos crawled from the world wide web. \nTraining Generative Adversarial Networks (GANs) for logo synthesis on such \nmulti-modal data is not straightforward and results in mode collapse for some \nstate-of-the-art methods. We propose the use of synthetic labels obtained \nthrough clustering to disentangle and stabilize GAN training. We are able to \ngenerate a high diversity of plausible logos and we demonstrate latent space \nexploration techniques to ease the logo design task in an interactive manner. \nMoreover, we validate the proposed clustered GAN training on CIFAR 10, \nachieving state-of-the-art Inception scores when using synthetic labels \nobtained via clustering the features of an ImageNet classifier. GANs can cope \nwith multi-modal data by means of synthetic labels achieved through clustering, \nand our results show the creative potential of such techniques for logo \nsynthesis and manipulation. Our dataset and models will be made publicly \navailable at https://data.vision.ee.ethz.ch/cvl/lld/. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f7e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04407"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1515390605, "author": "Amir Gholami, Ariful Azad, Peter Jin, Kurt Keutzer, Aydin Buluc", "title": "Integrated Model, Batch and Domain Parallelism in Training Neural Networks. (arXiv:1712.04432v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.04432", "type": "text/html"}], "timestampUsec": "1513141858490889", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a2d363f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a2d363f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a new integrated method of exploiting model, batch and domain \nparallelism for the training of deep neural networks (DNNs) on large \ndistributed-memory computers using minibatch stochastic gradient descent (SGD). \nOur goal is to find an efficient parallelization strategy for a fixed batch \nsize using $P$ processes. Our method is inspired by the communication-avoiding \nalgorithms in numerical linear algebra. We see $P$ processes as logically \ndivided into a $P_r \\times P_c$ grid where the $P_r$ dimension is implicitly \nresponsible for model/domain parallelism and the $P_c$ dimension is implicitly \nresponsible for batch parallelism. In practice, the integrated matrix-based \nparallel algorithm encapsulates these types of parallelism automatically. We \nanalyze the communication complexity and analytically demonstrate that the \nlowest communication costs are often achieved neither with pure model nor with \npure data parallelism. We also show the positive effect of our approach in the \ncomputational performance of SGD based DNN training where the reduced number of \nprocesses responsible for data parallelism result in \"fatter\" matrices that \nenable higher-throughput matrix multiplication. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f85", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04432"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Matthieu Geist, Bilal Piot, Olivier Pietquin", "title": "Is the Bellman residual a bad proxy?. (arXiv:1606.07636v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1606.07636", "type": "text/html"}], "timestampUsec": "1513141858490888", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a321ef0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a321ef0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper aims at theoretically and empirically comparing two standard \noptimization criteria for Reinforcement Learning: i) maximization of the mean \nvalue and ii) minimization of the Bellman residual. For that purpose, we place \nourselves in the framework of policy search algorithms, that are usually \ndesigned to maximize the mean value, and derive a method that minimizes the \nresidual $\\|T_* v_\\pi - v_\\pi\\|_{1,\\nu}$ over policies. A theoretical analysis \nshows how good this proxy is to policy optimization, and notably that it is \nbetter than its value-based counterpart. We also propose experiments on \nrandomly generated generic Markov decision processes, specifically designed for \nstudying the influence of the involved concentrability coefficient. They show \nthat the Bellman residual is generally a bad proxy to policy optimization and \nthat directly maximizing the mean value is much better, despite the current \nlack of deep theoretical analysis. This might seem obvious, as directly \naddressing the problem of interest is usually better, but given the prevalence \nof (projected) Bellman residual minimization in value-based reinforcement \nlearning, we believe that this question is worth to be considered. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f8c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1606.07636"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Luca Franceschi, Michele Donini, Paolo Frasconi, Massimiliano Pontil", "title": "Forward and Reverse Gradient-Based Hyperparameter Optimization. (arXiv:1703.01785v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.01785", "type": "text/html"}], "timestampUsec": "1513141858490887", "comments": [], "summary": {"content": "<p>We study two procedures (reverse-mode and forward-mode) for computing the \ngradient of the validation error with respect to the hyperparameters of any \niterative learning algorithm such as stochastic gradient descent. These \nprocedures mirror two methods of computing gradients for recurrent neural \nnetworks and have different trade-offs in terms of running time and space \nrequirements. Our formulation of the reverse-mode procedure is linked to \nprevious work by Maclaurin et al. [2015] but does not require reversible \ndynamics. The forward-mode procedure is suitable for real-time hyperparameter \nupdates, which may significantly speed up hyperparameter optimization on large \ndatasets. We present experiments on data cleaning and on learning task \ninteractions. We also present one large-scale experiment where the use of \nprevious gradient-based methods would be prohibitive. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f9b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.01785"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Soumitro Chakrabarty, Emanu&#xeb;l. A. P. Habets", "title": "Broadband DOA estimation using Convolutional neural networks trained with noise signals. (arXiv:1705.00919v2 [cs.SD] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.00919", "type": "text/html"}], "timestampUsec": "1513141858490886", "comments": [], "summary": {"content": "<p>A convolution neural network (CNN) based classification method for broadband \nDOA estimation is proposed, where the phase component of the short-time Fourier \ntransform coefficients of the received microphone signals are directly fed into \nthe CNN and the features required for DOA estimation are learnt during \ntraining. Since only the phase component of the input is used, the CNN can be \ntrained with synthesized noise signals, thereby making the preparation of the \ntraining data set easier compared to using speech signals. Through experimental \nevaluation, the ability of the proposed noise trained CNN framework to \ngeneralize to speech sources is demonstrated. In addition, the robustness of \nthe system to noise, small perturbations in microphone positions, as well as \nits ability to adapt to different acoustic conditions is investigated using \nexperiments with simulated and real data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fa6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.00919"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexej Gossmann, Pascal Zille, Vince Calhoun, Yu-Ping Wang", "title": "FDR-Corrected Sparse Canonical Correlation Analysis with Applications to Imaging Genomics. (arXiv:1705.04312v3 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.04312", "type": "text/html"}], "timestampUsec": "1513141858490885", "comments": [], "summary": {"content": "<p>Reducing the number of false positive discoveries is presently one of the \nmost pressing issues in the life sciences. It is of especially great importance \nfor many applications in neuroimaging and genomics, where datasets are \ntypically high-dimensional, which means that the number of explanatory \nvariables exceeds the sample size. The false discovery rate (FDR) is a \ncriterion that can be employed to address that issue. Thus it has gained great \npopularity as a tool for testing multiple hypotheses. Canonical correlation \nanalysis (CCA) is a statistical technique that is used to make sense of the \ncross-correlation of two sets of measurements collected on the same set of \nsamples (e.g., brain imaging and genomic data for the same mental illness \npatients), and sparse CCA extends the classical method to high-dimensional \nsettings. Here we propose a way of applying the FDR concept to sparse CCA, and \na method to control the FDR. The proposed FDR correction directly influences \nthe sparsity of the solution, adapting it to the unknown true sparsity level. \nTheoretical derivation as well as simulation studies show that our procedure \nindeed keeps the FDR of the canonical vectors below a user-specified target \nlevel. We apply the proposed method to an imaging genomics dataset from the \nPhiladelphia Neurodevelopmental Cohort. Our results link the brain connectivity \nprofiles derived from brain activity during an emotion identification task, as \nmeasured by functional magnetic resonance imaging (fMRI), to the corresponding \nsubjects' genomic data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127faf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.04312"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lin F. Yang, Vladimir Braverman, Tuo Zhao, Mengdi Wang", "title": "Online Factorization and Partition of Complex Networks From Random Walks. (arXiv:1705.07881v4 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.07881", "type": "text/html"}], "timestampUsec": "1513141858490884", "comments": [], "summary": {"content": "<p>Finding the reduced-dimensional structure is critical to understanding \ncomplex networks. Existing approaches such as spectral clustering are \napplicable only when the full network is explicitly observed. In this paper, we \nfocus on the online factorization and partition of implicit large-scale \nnetworks based on observations from an associated random walk. We formulate \nthis into a nonconvex stochastic factorization problem and propose an efficient \nand scalable stochastic generalized Hebbian algorithm. The algorithm is able to \nprocess dependent state-transition data dynamically generated by the underlying \nnetwork and learn a low-dimensional representation for each vertex. By applying \na diffusion approximation analysis, we show that the continuous-time limiting \nprocess of the stochastic algorithm converges globally to the \"principal \ncomponents\" of the Markov chain and achieves a nearly optimal sample \ncomplexity. Once given the learned low-dimensional representations, we further \napply clustering techniques to recover the network partition. We show that when \nthe associated Markov process is lumpable, one can recover the partition \nexactly with high probability. We apply the proposed approach to model the \ntraffic flow of Manhattan as city-wide random walks. By using our algorithm to \nanalyze the taxi trip data, we discover a latent partition of the Manhattan \ncity that closely matches the traffic dynamics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fb9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.07881"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hao Wu, Frank No&#xe9;", "title": "Variational approach for learning Markov processes from time series data. (arXiv:1707.04659v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.04659", "type": "text/html"}], "timestampUsec": "1513141858490883", "comments": [], "summary": {"content": "<p>Inference, prediction and control of complex dynamical systems from time \nseries is important in many areas, including financial markets, power grid \nmanagement, climate and weather modeling, or molecular dynamics. The analysis \nof such highly nonlinear dynamical systems is facilitated by the fact that we \ncan often find a (generally nonlinear) transformation of the system coordinates \nto features in which the dynamics can be excellently approximated by a linear \nMarkovian model. Moreover, the large number of system variables often change \ncollectively on large time- and length-scales, facilitating a low-dimensional \nanalysis in feature space. In this paper, we introduce a variational approach \nfor Markov processes (VAMP) that allows us to find optimal feature mappings and \noptimal Markovian models of the dynamics from given time series data. The key \ninsight is that the best linear model can be obtained from the top singular \ncomponents of the Koopman operator. This leads to the definition of a family of \nscore functions called VAMP-r which can be calculated from data, and can be \nemployed to optimize a Markovian model. In addition, based on the relationship \nbetween the variational scores and approximation errors of Koopman operators, \nwe propose a new VAMP-E score, which can be applied to cross-validation for \nhyper-parameter optimization and model selection in VAMP. VAMP is valid for \nboth reversible and nonreversible processes and for stationary and \nnon-stationary processes or realizations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fbf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.04659"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tianyi Lin, Linbo Qiao, Teng Zhang, Jiashi Feng, Bofeng Zhang", "title": "Stochastic Primal-Dual Proximal ExtraGradient Descent for Compositely Regularized Optimization. (arXiv:1708.05978v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.05978", "type": "text/html"}], "timestampUsec": "1513141858490882", "comments": [], "summary": {"content": "<p>We consider a wide range of regularized stochastic minimization problems with \ntwo regularization terms, one of which is composed with a linear function. This \noptimization model abstracts a number of important applications in artificial \nintelligence and machine learning, such as fused Lasso, fused logistic \nregression, and a class of graph-guided regularized minimization. The \ncomputational challenges of this model are in two folds. On one hand, the \nclosed-form solution of the proximal mapping associated with the composed \nregularization term or the expected objective function is not available. On the \nother hand, the calculation of the full gradient of the expectation in the \nobjective is very expensive when the number of input data samples is \nconsiderably large. To address these issues, we propose a stochastic variant of \nextra-gradient type methods, namely \\textsf{Stochastic Primal-Dual Proximal \nExtraGradient descent (SPDPEG)}, and analyze its convergence property for both \nconvex and strongly convex objectives. For general convex objectives, the \nuniformly average iterates generated by \\textsf{SPDPEG} converge in expectation \nwith $O(1/\\sqrt{t})$ rate. While for strongly convex objectives, the uniformly \nand non-uniformly average iterates generated by \\textsf{SPDPEG} converge with \n$O(\\log(t)/t)$ and $O(1/t)$ rates, respectively. The order of the rate of the \nproposed algorithm is known to match the best convergence rate for first-order \nstochastic algorithms. Experiments on fused logistic regression and \ngraph-guided regularized logistic regression problems show that the proposed \nalgorithm performs very efficiently and consistently outperforms other \ncompeting algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fc8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.05978"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Agnieszka Sitko, Przemyslaw Biecek", "title": "The Merging Path Plot: adaptive fusing of k-groups with likelihood-based model selection. (arXiv:1709.04412v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04412", "type": "text/html"}], "timestampUsec": "1513141858490881", "comments": [], "summary": {"content": "<p>There are many statistical tests that verify the null hypothesis: the \nvariable of interest has the same distribution among k-groups. But once the \nnull hypothesis is rejected, how to present the structure of dissimilarity \nbetween groups? In this article, we introduce The Merging Path Plot - a \nmethodology, and factorMerger - an R package, for exploration and visualization \nof k-group dissimilarities. Comparison of k-groups is one of the most important \nissues in exploratory analyses and it has zillions of applications. The \nclassical solution is to test a~null hypothesis that observations from all \ngroups come from the same distribution. If the global null hypothesis is \nrejected, a~more detailed analysis of differences among pairs of groups is \nperformed. The traditional approach is to use pairwise post hoc tests in order \nto verify which groups differ significantly. However, this approach fails with \na large number of groups in both interpretation and visualization layer. \nThe~Merging Path Plot methodology solves this problem by using an \neasy-to-understand description of dissimilarity among groups based on \nLikelihood Ratio Test (LRT) statistic. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fd1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04412"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang", "title": "Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec. (arXiv:1710.02971v3 [cs.SI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.02971", "type": "text/html"}], "timestampUsec": "1513141858490880", "comments": [], "summary": {"content": "<p>Since the invention of word2vec, the skip-gram model has significantly \nadvanced the research of network embedding, such as the recent emergence of the \nDeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of \nthe aforementioned models with negative sampling can be unified into the matrix \nfactorization framework with closed forms. Our analysis and proofs reveal that: \n(1) DeepWalk empirically produces a low-rank transformation of a network's \nnormalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk \nwhen the size of vertices' context is set to one; (3) As an extension of LINE, \nPTE can be viewed as the joint factorization of multiple networks' Laplacians; \n(4) node2vec is factorizing a matrix related to the stationary distribution and \ntransition probability tensor of a 2nd-order random walk. We further provide \nthe theoretical connections between skip-gram based network embedding \nalgorithms and the theory of graph Laplacian. Finally, we present the NetMF \nmethod as well as its approximation algorithm for computing network embedding. \nOur method offers significant improvements over DeepWalk and LINE for \nconventional network mining tasks. This work lays the theoretical foundation \nfor skip-gram based network embedding methods, leading to a better \nunderstanding of latent network representation learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fd8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.02971"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christina Heinze-Deml, Nicolai Meinshausen", "title": "Grouping-By-ID: Guarding Against Adversarial Domain Shifts. (arXiv:1710.11469v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.11469", "type": "text/html"}], "timestampUsec": "1513141858490879", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a322339\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a322339&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>When training a deep network for image classification, one can broadly \ndistinguish between two types of latent features that will drive the \nclassification. Following Gong et al. (2016), we can divide features into (i) \n\"core\" features $X^{ci}$ whose distribution $P(X^{ci} | Y)$ does not change \nsubstantially across domains and (ii) \"style\" or \"orthogonal\" features \n$X^\\perp$ whose distribution $P(X^\\perp | Y)$ can change substantially across \ndomains. These latter orthogonal features would generally include features such \nas position or brightness but also more complex ones like hair color or posture \nfor images of persons. We try to guard against future adversarial domain shifts \nby ideally just using the \"core\" features for classification. In contrast to \nprevious work, we assume that the domain itself is not observed and hence a \nlatent variable, i.e. we cannot directly see the distributional change of \nfeatures across different domains. We do assume, however, that we can sometimes \nobserve a so-called ID variable. E.g. we might know that two images show the \nsame person, with ID referring to the identity of the person. The method \nrequires only a small fraction of images to have an ID variable. We provide a \ncausal framework for the problem by adding the ID variable to the model of Gong \net al. (2016). If two or more samples share the same class and identifier, then \nwe treat those samples as counterfactuals under different interventions on the \northogonal features. Using this grouping-by-ID approach, we regularize the \nnetwork to provide near constant output across samples that share the same ID \nby penalizing with an appropriate graph Laplacian. This substantially improves \nperformance in settings where domains change in terms of image quality, \nbrightness, color or posture and movement. We show links to questions of \ninterpretability, fairness, transfer learning and adversarial examples. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fdf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.11469"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Felix Last, Georgios Douzas, Fernando Bacao", "title": "Oversampling for Imbalanced Learning Based on K-Means and SMOTE. (arXiv:1711.00837v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.00837", "type": "text/html"}], "timestampUsec": "1513141858490878", "comments": [], "summary": {"content": "<p>Learning from class-imbalanced data continues to be a common and challenging \nproblem in supervised learning as standard classification algorithms are \ndesigned to handle balanced class distributions. While different strategies \nexist to tackle this problem, methods which generate artificial data to achieve \na balanced class distribution are more versatile than modifications to the \nclassification algorithm. Such techniques, called oversamplers, modify the \ntraining data, allowing any classifier to be used with class-imbalanced \ndatasets. Many algorithms have been proposed for this task, but most are \ncomplex and tend to generate unnecessary noise. This work presents a simple and \neffective oversampling method based on k-means clustering and SMOTE \noversampling, which avoids the generation of noise and effectively overcomes \nimbalances between and within classes. Empirical results of extensive \nexperiments with 71 datasets show that training data oversampled with the \nproposed method improves classification results. Moreover, k-means SMOTE \nconsistently outperforms other popular oversampling methods. An implementation \nis made available in the python programming language. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fe3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.00837"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Saki Shinoda, Daniel E. Worrall, Gabriel J. Brostow", "title": "Virtual Adversarial Ladder Networks For Semi-supervised Learning. (arXiv:1711.07476v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07476", "type": "text/html"}], "timestampUsec": "1513141858490877", "comments": [], "summary": {"content": "<p>Semi-supervised learning (SSL) partially circumvents the high cost of \nlabeling data by augmenting a small labeled dataset with a large and relatively \ncheap unlabeled dataset drawn from the same distribution. This paper offers a \nnovel interpretation of two deep learning-based SSL approaches, ladder networks \nand virtual adversarial training (VAT), as applying distributional smoothing to \ntheir respective latent spaces. We propose a class of models that fuse these \napproaches. We achieve near-supervised accuracy with high consistency on the \nMNIST dataset using just 5 labels per class: our best model, ladder with \nlayer-wise virtual adversarial noise (LVAN-LW), achieves 1.42% +/- 0.12 average \nerror rate on the MNIST test set, in comparison with 1.62% +/- 0.65 reported \nfor the ladder network. On adversarial examples generated with L2-normalized \nfast gradient method, LVAN-LW trained with 5 examples per class achieves \naverage error rate 2.4% +/- 0.3 compared to 68.6% +/- 6.5 for the ladder \nnetwork and 9.9% +/- 7.5 for VAT. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fe9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07476"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wataru Kumagai", "title": "Regret Analysis for Continuous Dueling Bandit. (arXiv:1711.07693v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07693", "type": "text/html"}], "timestampUsec": "1513141858490876", "comments": [], "summary": {"content": "<p>The dueling bandit is a learning framework wherein the feedback information \nin the learning process is restricted to a noisy comparison between a pair of \nactions. In this research, we address a dueling bandit problem based on a cost \nfunction over a continuous space. We propose a stochastic mirror descent \nalgorithm and show that the algorithm achieves an $O(\\sqrt{T\\log T})$-regret \nbound under strong convexity and smoothness assumptions for the cost function. \nSubsequently, we clarify the equivalence between regret minimization in dueling \nbandit and convex optimization for the cost function. Moreover, when \nconsidering a lower bound in convex optimization, our algorithm is shown to \nachieve the optimal convergence rate in convex optimization and the optimal \nregret in dueling bandit except for a logarithmic factor. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ff0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07693"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Maria De-Arteaga, William Herlands", "title": "Proceedings of NIPS 2017 Workshop on Machine Learning for the Developing World. (arXiv:1711.09522v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09522", "type": "text/html"}], "timestampUsec": "1513141858490875", "comments": [], "summary": {"content": "<p>This is the Proceedings of NIPS 2017 Workshop on Machine Learning for the \nDeveloping World, held in Long Beach, California, USA on December 8, 2017 \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ff6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09522"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Boyang Deng, Junjie Yan, Dahua Lin", "title": "Peephole: Predicting Network Performance Before Training. (arXiv:1712.03351v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03351", "type": "text/html"}], "timestampUsec": "1513055249337172", "comments": [], "summary": {"content": "<p>The quest for performant networks has been a significant force that drives \nthe advancements of deep learning in recent years. While rewarding, improving \nnetwork design has never been an easy journey. The large design space combined \nwith the tremendous cost required for network training poses a major obstacle \nto this endeavor. In this work, we propose a new approach to this problem, \nnamely, predicting the performance of a network before training, based on its \narchitecture. Specifically, we develop a unified way to encode individual \nlayers into vectors and bring them together to form an integrated description \nvia LSTM. Taking advantage of the recurrent network's strong expressive power, \nthis method can reliably predict the performances of various network \narchitectures. Our empirical studies showed that it not only achieved accurate \npredictions but also produced consistent rankings across datasets -- a key \ndesideratum in performance prediction. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcee2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03351"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Abien Fred Agarap", "title": "An Architecture Combining Convolutional Neural Network (CNN) and Support Vector Machine (SVM) for Image Classification. (arXiv:1712.03541v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.03541", "type": "text/html"}], "timestampUsec": "1513055249337171", "comments": [], "summary": {"content": "<p>Convolutional neural networks (CNNs) are similar to \"ordinary\" neural \nnetworks in the sense that they are made up of hidden layers consisting of \nneurons with \"learnable\" parameters. These neurons receive inputs, performs a \ndot product, and then follows it with a non-linearity. The whole network \nexpresses the mapping between raw image pixels and their class scores. \nConventionally, the Softmax function is the classifier used at the last layer \nof this network. However, there have been studies (Alalshekmubarak and Smith, \n2013; Agarap, 2017; Tang, 2013) conducted to challenge this norm. The cited \nstudies introduce the usage of linear support vector machine (SVM) in an \nartificial neural network architecture. This project is yet another take on the \nsubject, and is inspired by (Tang, 2013). Empirical data has shown that the \nCNN-SVM model was able to achieve a test accuracy of ~99.04% using the MNIST \ndataset (LeCun, Cortes, and Burges, 2010). On the other hand, the CNN-Softmax \nwas able to achieve a test accuracy of ~99.23% using the same dataset. Both \nmodels were also tested on the recently-published Fashion-MNIST dataset (Xiao, \nRasul, and Vollgraf, 2017), which is suppose to be a more difficult image \nclassification dataset than MNIST (Zalandoresearch, 2017). This proved to be \nthe case as CNN-SVM reached a test accuracy of ~90.72%, while the CNN-Softmax \nreached a test accuracy of ~91.86%. The said results may be improved if data \npreprocessing techniques were employed on the datasets, and if the base CNN \nmodel was a relatively more sophisticated than the one used in this study. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcee5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03541"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Naveen Kodali, Jacob Abernethy, James Hays, Zsolt Kira", "title": "On Convergence and Stability of GANs. (arXiv:1705.07215v5 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.07215", "type": "text/html"}], "timestampUsec": "1513055249337170", "comments": [], "summary": {"content": "<p>We propose studying GAN training dynamics as regret minimization, which is in \ncontrast to the popular view that there is consistent minimization of a \ndivergence between real and generated distributions. We analyze the convergence \nof GAN training from this new point of view to understand why mode collapse \nhappens. We hypothesize the existence of undesirable local equilibria in this \nnon-convex game to be responsible for mode collapse. We observe that these \nlocal equilibria often exhibit sharp gradients of the discriminator function \naround some real data points. We demonstrate that these degenerate local \nequilibria can be avoided with a gradient penalty scheme called DRAGAN. We show \nthat DRAGAN enables faster training, achieves improved stability with fewer \nmode collapses, and leads to generator networks with better modeling \nperformance across a variety of architectures and objective functions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcee8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.07215"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhen Li, Zuoqiang Shi", "title": "A Flow Model of Neural Networks. (arXiv:1708.06257v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.06257", "type": "text/html"}], "timestampUsec": "1513055249337169", "comments": [], "summary": {"content": "<p>Based on a natural connection between ResNet and transport equation or its \ncharacteristic equation, we propose a continuous flow model for both ResNet and \nplain net. Through this continuous model, a ResNet can be explicitly \nconstructed as a refinement of a plain net. The flow model provides an \nalternative perspective to understand phenomena in deep neural networks, such \nas why it is necessary and sufficient to use 2-layer blocks in ResNets, why \ndeeper is better, and why ResNets are even deeper, and so on. It also opens a \ngate to bring in more tools from the huge area of differential equations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcef1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.06257"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Majdi Mafarja, Seyedali Mirjalili", "title": "S-Shaped vs. V-Shaped Transfer Functions for Antlion Optimization Algorithm in Feature Selection Problems. (arXiv:1712.03223v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.03223", "type": "text/html"}], "timestampUsec": "1513055249337168", "comments": [], "summary": {"content": "<p>Feature selection is an important preprocessing step for classification \nproblems. It deals with selecting near optimal features in the original \ndataset. Feature selection is an NP-hard problem, so meta-heuristics can be \nmore efficient than exact methods. In this work, Ant Lion Optimizer (ALO), \nwhich is a recent metaheuristic algorithm, is employed as a wrapper feature \nselection method. Six variants of ALO are proposed where each employ a transfer \nfunction to map a continuous search space to a discrete search space. The \nperformance of the proposed approaches is tested on eighteen UCI datasets and \ncompared to a number of existing approaches in the literature: Particle Swarm \nOptimization, Gravitational Search Algorithm, and two existing ALO-based \napproaches. Computational experiments show that the proposed approaches \nefficiently explore the feature space and select the most informative features, \nwhich help to improve the classification accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcef6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03223"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Florian Krebs, Bruno Lubascher, Tobias Moers, Pieter Schaap, Gerasimos Spanakis", "title": "Social Emotion Mining Techniques for Facebook Posts Reaction Prediction. (arXiv:1712.03249v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.03249", "type": "text/html"}], "timestampUsec": "1513055249337167", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a322a3f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a322a3f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>As of February 2016 Facebook allows users to express their experienced \nemotions about a post by using five so-called `reactions'. This research paper \nproposes and evaluates alternative methods for predicting these reactions to \nuser posts on public pages of firms/companies (like supermarket chains). For \nthis purpose, we collected posts (and their reactions) from Facebook pages of \nlarge supermarket chains and constructed a dataset which is available for other \nresearches. In order to predict the distribution of reactions of a new post, \nneural network architectures (convolutional and recurrent neural networks) were \ntested using pretrained word embeddings. Results of the neural networks were \nimproved by introducing a bootstrapping approach for sentiment and emotion \nmining on the comments for each post. The final model (a combination of neural \nnetwork and a baseline emotion miner) is able to predict the reaction \ndistribution on Facebook posts with a mean squared error (or misclassification \nrate) of 0.135. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcefd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03249"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ben Parr, Deepak Dilipkumar, Yuan Liu", "title": "Nintendo Super Smash Bros. Melee: An \"Untouchable\" Agent. (arXiv:1712.03280v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.03280", "type": "text/html"}], "timestampUsec": "1513055249337166", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a36b0e2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a36b0e2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Nintendo's Super Smash Bros. Melee fighting game can be emulated on modern \nhardware allowing us to inspect internal memory states, such as character \npositions. We created an AI that avoids being hit by training using these \ninternal memory states and outputting controller button presses. After training \non a month's worth of Melee matches, our best agent learned to avoid the \ntoughest AI built into the game for a full minute 74.6% of the time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03280"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Heejin Jeong, Daniel D. Lee", "title": "Bayesian Q-learning with Assumed Density Filtering. (arXiv:1712.03333v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03333", "type": "text/html"}], "timestampUsec": "1513055249337165", "comments": [], "summary": {"content": "<p>While off-policy temporal difference methods have been broadly used in \nreinforcement learning due to their efficiency and simple implementation, their \nBayesian counterparts have been relatively understudied. This is mainly because \nthe max operator in the Bellman optimality equation brings non-linearity and \ninconsistent distributions over value function. In this paper, we introduce a \nnew Bayesian approach to off-policy TD methods using Assumed Density Filtering, \ncalled ADFQ, which updates beliefs on action-values (Q) through an online \nBayesian inference method. Uncertainty measures in the beliefs not only are \nused in exploration but they provide a natural regularization in the belief \nupdates. We also present a connection between ADFQ and Q-learning. Our \nempirical results show the proposed ADFQ algorithms outperform comparing \nalgorithms in several task domains. Moreover, our algorithms improve general \ndrawbacks in BRL such as computational complexity, usage of uncertainty, and \nnonlinearity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf07", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03333"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Konda Reddy Mopuri, Utkarsh Ojha, Utsav Garg, R. Venkatesh Babu", "title": "NAG: Network for Adversary Generation. (arXiv:1712.03390v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.03390", "type": "text/html"}], "timestampUsec": "1513055249337164", "comments": [], "summary": {"content": "<p>Adversarial perturbations can pose a serious threat for deploying machine \nlearning systems. Recent works have shown existence of image-agnostic \nperturbations that can fool classifiers over most natural images. Existing \nmethods present optimization approaches that solve for a fooling objective with \nan imperceptibility constraint to craft the perturbations. However, for a given \nclassifier, they generate one perturbation at a time, which is a single \ninstance from the manifold of adversarial perturbations. Also, in order to \nbuild robust models, it is essential to explore the manifold of adversarial \nperturbations. In this paper, we propose for the first time, a generative \napproach to model the distribution of adversarial perturbations. The \narchitecture of the proposed model is inspired from that of GANs and is trained \nusing fooling and diversity objectives. Our trained generator network attempts \nto capture the distribution of adversarial perturbations for a given classifier \nand readily generates a wide variety of such perturbations. Our experimental \nevaluation demonstrates that perturbations crafted by our model (i) achieve \nstate-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver \nexcellent cross model generalizability. Our work can be deemed as an important \nstep in the process of inferring about the complex manifolds of adversarial \nperturbations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf0c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03390"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chanwoo Kim, Ehsan Variani, Arun Narayanan, Michiel Bacchiani", "title": "Efficient Implementation of the Room Simulator for Training Deep Neural Network Acoustic Models. (arXiv:1712.03439v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1712.03439", "type": "text/html"}], "timestampUsec": "1513055249337163", "comments": [], "summary": {"content": "<p>In this paper, we describe how to efficiently implement an acoustic room \nsimulator to generate large-scale simulated data for training deep neural \nnetworks. Even though Google Room Simulator in [1] was shown to be quite \neffective in reducing the Word Error Rates (WERs) for far-field applications by \ngenerating simulated far-field training sets, it requires a very large number \nof Fast Fourier Transforms (FFTs) of large size. Room Simulator in [1] used \napproximately 80 percent of Central Processing Unit (CPU) usage in our CPU + \nGraphics Processing Unit (GPU) training architecture [2]. In this work, we \nimplement an efficient OverLap Addition (OLA) based filtering using the \nopen-source FFTW3 library. Further, we investigate the effects of the Room \nImpulse Response (RIR) lengths. Experimentally, we conclude that we can cut the \ntail portions of RIRs whose power is less than 20 dB below the maximum power \nwithout sacrificing the speech recognition accuracy. However, we observe that \ncutting RIR tail more than this threshold harms the speech recognition accuracy \nfor rerecorded test sets. Using these approaches, we were able to reduce CPU \nusage for the room simulator portion down to 9.69 percent in CPU/GPU training \narchitecture. Profiling result shows that we obtain 22.4 times speed-up on a \nsingle machine and 37.3 times speed up on Google's distributed training \ninfrastructure. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf12", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03439"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexander Gruenstein, Raziel Alvarez, Chris Thornton, Mohammadali Ghodrat", "title": "A Cascade Architecture for Keyword Spotting on Mobile Devices. (arXiv:1712.03603v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1712.03603", "type": "text/html"}], "timestampUsec": "1513055249337162", "comments": [], "summary": {"content": "<p>We present a cascade architecture for keyword spotting with speaker \nverification on mobile devices. By pairing a small computational footprint with \nspecialized digital signal processing (DSP) chips, we are able to achieve low \npower consumption while continuously listening for a keyword. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf15", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03603"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Philipp Oettershagen, Florian Achermann, Benjamin M&#xfc;ller, Daniel Schneider, Roland Siegwart", "title": "Towards Fully Environment-Aware UAVs: Real-Time Path Planning with Online 3D Wind Field Prediction in Complex Terrain. (arXiv:1712.03608v1 [cs.RO])", "alternate": [{"href": "http://arxiv.org/abs/1712.03608", "type": "text/html"}], "timestampUsec": "1513055249337161", "comments": [], "summary": {"content": "<p>Today, low-altitude fixed-wing Unmanned Aerial Vehicles (UAVs) are largely \nlimited to primitively follow user-defined waypoints. To allow fully-autonomous \nremote missions in complex environments, real-time environment-aware navigation \nis required both with respect to terrain and strong wind drafts. This paper \npresents two relevant initial contributions: First, the literature's first-ever \n3D wind field prediction method which can run in real time onboard a UAV is \npresented. The approach retrieves low-resolution global weather data, and uses \npotential flow theory to adjust the wind field such that terrain boundaries, \nmass conservation, and the atmospheric stratification are observed. A \ncomparison with 1D LIDAR data shows an overall wind error reduction of 23% with \nrespect to the zero-wind assumption that is mostly used for UAV path planning \ntoday. However, given that the vertical winds are not resolved accurately \nenough further research is required and identified. Second, a sampling-based \npath planner that considers the aircraft dynamics in non-uniform wind \niteratively via Dubins airplane paths is presented. Performance optimizations, \ne.g. obstacle-aware sampling and fast 2.5D-map collision checks, render the \nplanner 50% faster than the Open Motion Planning Library (OMPL) implementation. \nTest cases in Alpine terrain show that the wind-aware planning performs up to \n50x less iterations than shortest-path planning and is thus slower in low \nwinds, but that it tends to deliver lower-cost paths in stronger winds. More \nimportantly, in contrast to the shortest-path planner, it always delivers \ncollision-free paths. Overall, our initial research demonstrates the \nfeasibility of 3D wind field prediction from a UAV and the advantages of \nwind-aware planning. This paves the way for follow-up research on \nfully-autonomous environment-aware navigation of UAVs in real-life missions and \ncomplex terrain. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03608"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, Girish Chowdhary", "title": "Robust Deep Reinforcement Learning with Adversarial Attacks. (arXiv:1712.03632v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03632", "type": "text/html"}], "timestampUsec": "1513055249337160", "comments": [], "summary": {"content": "<p>This paper proposes adversarial attacks for Reinforcement Learning (RL) and \nthen improves the robustness of Deep Reinforcement Learning algorithms (DRL) to \nparameter uncertainties with the help of these attacks. We show that even a \nnaively engineered attack successfully degrades the performance of DRL \nalgorithm. We further improve the attack using gradient information of an \nengineered loss function which leads to further degradation in performance. \nThese attacks are then leveraged during training to improve the robustness of \nRL within robust control framework. We show that this adversarial training of \nDRL algorithms like Deep Double Q learning and Deep Deterministic Policy \nGradients leads to significant increase in robustness to parameter variations \nfor RL benchmarks such as Cart-pole, Mountain Car, Hopper and Half Cheetah \nenvironment. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf28", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03632"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zlatan Ajanovic, Michael Stolz, Martin Horn", "title": "Novel model-based heuristics for energy optimal motion planning of an autonomous vehicle using A*. (arXiv:1712.03719v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1712.03719", "type": "text/html"}], "timestampUsec": "1513055249337159", "comments": [], "summary": {"content": "<p>Predictive motion planning is the key to achieve energy-efficient driving, \nwhich is one of the main benefits of automated driving. Researchers have been \nstudying the planning of velocity trajectories, a simpler form of motion \nplanning, for over a decade now and many different methods are available. \nDynamic programming has shown to be the most common choice due to its numerical \nbackground and ability to include nonlinear constraints and models. Although \nplanning of optimal trajectory is done in a systematic way, dynamic programming \ndoesn't use any knowledge about the considered problem to guide the exploration \nand therefore explores all possible trajectories. A* is an algorithm which \nenables using knowledge about the problem to guide the exploration to the most \npromising solutions first. Knowledge has to be represented in a form of a \nheuristic function, which gives an optimistic estimate of cost for \ntransitioning between two states, which is not a straightforward task. This \npaper presents a novel heuristics incorporating air drag and auxiliary power as \nwell as operational costs of the vehicle, besides kinetic and potential energy \nand rolling resistance known in the literature. Furthermore, optimal cruising \nvelocity, which depends on vehicle aerodynamic properties and auxiliary power, \nis derived. Results are compared for different variants of heuristic functions \nand dynamic programming as well. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf30", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03719"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rakesh R Pimplikar, Kushal Mukherjee, Gyana Parija, Harit Vishwakarma, Ramasuri Narayanam, Sarthak Ahuja, Rohith D Vallam, Ritwik Chaudhuri, Joydeep Mondal", "title": "Cogniculture: Towards a Better Human-Machine Co-evolution. (arXiv:1712.03724v1 [cs.CY])", "alternate": [{"href": "http://arxiv.org/abs/1712.03724", "type": "text/html"}], "timestampUsec": "1513055249337158", "comments": [], "summary": {"content": "<p>Research in Artificial Intelligence is breaking technology barriers every \nday. New algorithms and high performance computing are making things possible \nwhich we could only have imagined earlier. Though the enhancements in AI are \nmaking life easier for human beings day by day, there is constant fear that AI \nbased systems will pose a threat to humanity. People in AI community have \ndiverse set of opinions regarding the pros and cons of AI mimicking human \nbehavior. Instead of worrying about AI advancements, we propose a novel idea of \ncognitive agents, including both human and machines, living together in a \ncomplex adaptive ecosystem, collaborating on human computation for producing \nessential social goods while promoting sustenance, survival and evolution of \nthe agents' life cycle. We highlight several research challenges and technology \nbarriers in achieving this goal. We propose a governance mechanism around this \necosystem to ensure ethical behaviors of all cognitive agents. Along with a \nnovel set of use-cases of Cogniculture, we discuss the road map ahead for this \njourney. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03724"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Bin Yu, Karl Kumbier", "title": "Artificial Intelligence and Statistics. (arXiv:1712.03779v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03779", "type": "text/html"}], "timestampUsec": "1513055249337157", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a36b53a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a36b53a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Artificial intelligence (AI) is intrinsically data-driven. It calls for the \napplication of statistical concepts through human-machine collaboration during \ngeneration of data, development of algorithms, and evaluation of results. This \npaper discusses how such human-machine collaboration can be approached through \nthe statistical concepts of population, question of interest, \nrepresentativeness of training data, and scrutiny of results (PQRS). The PQRS \nworkflow provides a conceptual framework for integrating statistical ideas with \nhuman input into AI products and research. These ideas include experimental \ndesign principles of randomization and local control as well as the principle \nof stability to gain reproducibility and interpretability of algorithms and \ndata results. We discuss the use of these principles in the contexts of \nself-driving cars, automated medical diagnoses, and examples from the authors' \ncollaborative research. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf3b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03779"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Eunwoo Kim, Chanho Ahn, Songhwai Oh", "title": "Learning Nested Sparse Structures in Deep Neural Networks. (arXiv:1712.03781v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.03781", "type": "text/html"}], "timestampUsec": "1513055249337156", "comments": [], "summary": {"content": "<p>Recently, there have been increasing demands to construct compact deep \narchitectures to remove unnecessary redundancy and to improve the inference \nspeed. While many recent works focus on reducing the redundancy by eliminating \nunneeded weight parameters, it is not possible to apply a single deep \narchitecture for multiple devices with different resources. When a new device \nor circumstantial condition requires a new deep architecture, it is necessary \nto construct and train a new network from scratch. In this work, we propose a \nnovel deep learning framework, called a nested sparse network, which exploits \nan n-in-1-type nested structure in a neural network. A nested sparse network \nconsists of multiple levels of networks with a different sparsity ratio \nassociated with each level, and higher level networks share parameters with \nlower level networks to enable stable nested learning. The proposed framework \nrealizes a resource-aware versatile architecture as the same network can meet \ndiverse resource requirements. Moreover, the proposed nested network can learn \ndifferent forms of knowledge in its internal networks at different levels, \nenabling multiple tasks using a single network, such as coarse-to-fine \nhierarchical classification. In order to train the proposed nested sparse \nnetwork, we propose efficient weight connection learning and channel and layer \nscheduling strategies. We evaluate our network in multiple tasks, including \nadaptive deep compression, knowledge distillation, and learning class \nhierarchy, and demonstrate that nested sparse networks perform competitively, \nbut more efficiently, than existing methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf41", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03781"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Manolis Savva, Angel X. Chang, Alexey Dosovitskiy, Thomas Funkhouser, Vladlen Koltun", "title": "MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments. (arXiv:1712.03931v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03931", "type": "text/html"}], "timestampUsec": "1513055249337155", "comments": [], "summary": {"content": "<p>We present MINOS, a simulator designed to support the development of \nmultisensory models for goal-directed navigation in complex indoor \nenvironments. The simulator leverages large datasets of complex 3D environments \nand supports flexible configuration of multimodal sensor suites. We use MINOS \nto benchmark deep-learning-based navigation methods, to analyze the influence \nof environmental complexity on navigation performance, and to carry out a \ncontrolled study of multimodality in sensorimotor learning. The experiments \nshow that current deep reinforcement learning approaches fail in large \nrealistic environments. The experiments also indicate that multimodality is \nbeneficial in learning to navigate cluttered scenes. MINOS is released \nopen-source to the research community at <a href=\"http://minosworld.org\">this http URL</a> . A video that \nshows MINOS can be found at https://youtu.be/c0mL9K64q84 \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03931"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gaurav Bhatt, Aman Sharma, Shivam Sharma, Ankush Nagpal, Balasubramanian Raman, Ankush Mittal", "title": "On the Benefit of Combining Neural, Statistical and External Features for Fake News Identification. (arXiv:1712.03935v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.03935", "type": "text/html"}], "timestampUsec": "1513055249337154", "comments": [], "summary": {"content": "<p>Identifying the veracity of a news article is an interesting problem while \nautomating this process can be a challenging task. Detection of a news article \nas fake is still an open question as it is contingent on many factors which the \ncurrent state-of-the-art models fail to incorporate. In this paper, we explore \na subtask to fake news identification, and that is stance detection. Given a \nnews article, the task is to determine the relevance of the body and its claim. \nWe present a novel idea that combines the neural, statistical and external \nfeatures to provide an efficient solution to this problem. We compute the \nneural embedding from the deep recurrent model, statistical features from the \nweighted n-gram bag-of-words model and handcrafted external features with the \nhelp of feature engineering heuristics. Finally, using deep neural layer all \nthe features are combined, thereby classifying the headline-body news pair as \nagree, disagree, discuss, or unrelated. We compare our proposed technique with \nthe current state-of-the-art models on the fake news challenge dataset. Through \nextensive experiments, we find that the proposed model outperforms all the \nstate-of-the-art techniques including the submissions to the fake news \nchallenge. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf4f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03935"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gerhard Brewka, Stefan Ellmauthaler, Ricardo Gon&#xe7;alves, Matthias Knorr, Jo&#xe3;o Leite, J&#xf6;rg P&#xfc;hrer", "title": "Reactive Multi-Context Systems: Heterogeneous Reasoning in Dynamic Environments. (arXiv:1609.03438v3 [cs.LO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1609.03438", "type": "text/html"}], "timestampUsec": "1513055249337153", "comments": [], "summary": {"content": "<p>Managed multi-context systems (mMCSs) allow for the integration of \nheterogeneous knowledge sources in a modular and very general way. They were, \nhowever, mainly designed for static scenarios and are therefore not well-suited \nfor dynamic environments in which continuous reasoning over such heterogeneous \nknowledge with constantly arriving streams of data is necessary. In this paper, \nwe introduce reactive multi-context systems (rMCSs), a framework for reactive \nreasoning in the presence of heterogeneous knowledge sources and data streams. \nWe show that rMCSs are indeed well-suited for this purpose by illustrating how \nseveral typical problems arising in the context of stream reasoning can be \nhandled using them, by showing how inconsistencies possibly occurring in the \nintegration of multiple knowledge sources can be handled, and by arguing that \nthe potential non-determinism of rMCSs can be avoided if needed using an \nalternative, more skeptical well-founded semantics instead with beneficial \ncomputational properties. We also investigate the computational complexity of \nvarious reasoning problems related to rMCSs. Finally, we discuss related work, \nand show that rMCSs do not only generalize mMCSs to dynamic settings, but also \ncapture/extend relevant approaches w.r.t. dynamics in knowledge representation \nand stream reasoning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf54", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1609.03438"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Maxime Bucher (1), St&#xe9;phane Herbin (1), Fr&#xe9;d&#xe9;ric Jurie ((1) Palaiseau)", "title": "Generating Visual Representations for Zero-Shot Classification. (arXiv:1708.06975v3 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.06975", "type": "text/html"}], "timestampUsec": "1513055249337151", "comments": [], "summary": {"content": "<p>This paper addresses the task of learning an image clas-sifier when some \ncategories are defined by semantic descriptions only (e.g. visual attributes) \nwhile the others are defined by exemplar images as well. This task is often \nreferred to as the Zero-Shot classification task (ZSC). Most of the previous \nmethods rely on learning a common embedding space allowing to compare visual \nfeatures of unknown categories with semantic descriptions. This paper argues \nthat these approaches are limited as i) efficient discrimi-native classifiers \ncan't be used ii) classification tasks with seen and unseen categories \n(Generalized Zero-Shot Classification or GZSC) can't be addressed efficiently. \nIn contrast , this paper suggests to address ZSC and GZSC by i) learning a \nconditional generator using seen classes ii) generate artificial training \nexamples for the categories without exemplars. ZSC is then turned into a \nstandard supervised learning problem. Experiments with 4 generative models and \n5 datasets experimentally validate the approach, giving state-of-the-art \nresults on both ZSC and GZSC. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf61", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.06975"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Stephane Le Roux, Guillermo A. Perez", "title": "The Complexity of Graph-Based Reductions for Reachability in Markov Decision Processes. (arXiv:1710.07903v2 [cs.LO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.07903", "type": "text/html"}], "timestampUsec": "1513055249337150", "comments": [], "summary": {"content": "<p>We study the never-worse relation (NWR) for Markov decision processes with an \ninfinite-horizon reachability objective. A state q is never worse than a state \np if the maximal probability of reaching the target set of states from p is at \nmost the same value from q, regardless of the probabilities labelling the \ntransitions. Extremal-probability states, end components, and essential states \nare all special cases of the equivalence relation induced by the NWR. Using the \nNWR, states in the same equivalence class can be collapsed. Then, actions \nleading to sub-optimal states can be removed. We show the natural decision \nproblem associated to computing the NWR is coNP-complete. Finally, we extend a \nknown incomplete polynomial-time iterative algorithm to under-approximate the \nNWR. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf66", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.07903"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ting Chen, Martin Renqiang Min, Yizhou Sun", "title": "Learning K-way D-dimensional Discrete Code For Compact Embedding Representations. (arXiv:1711.03067v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.03067", "type": "text/html"}], "timestampUsec": "1513055249337149", "comments": [], "summary": {"content": "<p>Embedding methods such as word embedding have become pillars for many \napplications containing discrete structures. Conventional embedding methods \ndirectly associate each symbol with a continuous embedding vector, which is \nequivalent to applying linear transformation based on \"one-hot\" encoding of the \ndiscrete symbols. Despite its simplicity, such approach yields number of \nparameters that grows linearly with the vocabulary size and can lead to \noverfitting. In this work we propose a much more compact K-way D-dimensional \ndiscrete encoding scheme to replace the \"one-hot\" encoding. In \"KD encoding\", \neach symbol is represented by a $D$-dimensional code, and each of its dimension \nhas a cardinality of $K$. The final symbol embedding vector can be generated by \ncomposing the code embedding vectors. To learn the semantically meaningful \ncode, we derive a relaxed discrete optimization technique based on stochastic \ngradient descent. By adopting the new coding system, the efficiency of \nparameterization can be significantly improved (from linear to logarithmic), \nand this can also mitigate the over-fitting problem. In our experiments with \nlanguage modeling, the number of embedding parameters can be reduced by 97\\% \nwhile achieving similar or better performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf6e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.03067"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aleksandar Zeljic, Peter Backeman, Christoph M. Wintersteiger, Philipp Ruemmer", "title": "Exploring Approximations for Floating-Point Arithmetic using UppSAT. (arXiv:1711.08859v2 [cs.LO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08859", "type": "text/html"}], "timestampUsec": "1513055249337148", "comments": [], "summary": {"content": "<p>We consider the problem of solving floating-point constraints obtained from \nsoftware verification. We present UppSAT --- a new implementation of a \nsystematic approximation refinement framework [ZWR17] as an abstract SMT \nsolver. Provided with an approximation and a decision procedure (implemented in \nan off-the-shelf SMT solver), UppSAT yields an approximating SMT solver. \nAdditionally, UppSAT includes a library of predefined approximation components \nwhich can be combined and extended to define new encodings, orderings and \nsolving strategies. We propose that UppSAT can be used as a sandbox for easy \nand flexible exploration of new approximations. To substantiate this, we \nexplore several approximations of floating-point arithmetic. Approximations can \nbe viewed as a composition of an encoding into a target theory, a precision \nordering, and a number of strategies for model reconstruction and precision (or \napproximation) refinement. We present encodings of floating-point arithmetic \ninto reduced precision floating-point arithmetic, real-arithmetic, and \nfixed-point arithmetic (encoded in the theory of bit-vectors). In an \nexperimental evaluation, we compare the advantages and disadvantages of \napproximating solvers obtained by combining various encodings and decision \nprocedures (based on existing state-of-the-art SMT solvers for floating-point, \nreal, and bit-vector arithmetic). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf76", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08859"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mohammadreza Soltani, Chinmay Hegde", "title": "Fast Low-Rank Matrix Estimation without the Condition Number. (arXiv:1712.03281v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03281", "type": "text/html"}], "timestampUsec": "1513055249337145", "comments": [], "summary": {"content": "<p>In this paper, we study the general problem of optimizing a convex function \n$F(L)$ over the set of $p \\times p$ matrices, subject to rank constraints on \n$L$. However, existing first-order methods for solving such problems either are \ntoo slow to converge, or require multiple invocations of singular value \ndecompositions. On the other hand, factorization-based non-convex algorithms, \nwhile being much faster, require stringent assumptions on the \\emph{condition \nnumber} of the optimum. In this paper, we provide a novel algorithmic framework \nthat achieves the best of both worlds: asymptotically as fast as factorization \nmethods, while requiring no dependency on the condition number. \n</p> \n<p>We instantiate our general framework for three important matrix estimation \nproblems that impact several practical applications; (i) a \\emph{nonlinear} \nvariant of affine rank minimization, (ii) logistic PCA, and (iii) precision \nmatrix estimation in probabilistic graphical model learning. We then derive \nexplicit bounds on the sample complexity as well as the running time of our \napproach, and show that it achieves the best possible bounds for both cases. We \nalso provide an extensive range of experimental results, and demonstrate that \nour algorithm provides a very attractive tradeoff between estimation accuracy \nand running time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf83", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03281"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shankar Krishnan, Ying Xiao, Rif A. Saurous", "title": "Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks. (arXiv:1712.03298v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03298", "type": "text/html"}], "timestampUsec": "1513055249337144", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a36b9ed\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a36b9ed&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Progress in deep learning is slowed by the days or weeks it takes to train \nlarge models. The natural solution of using more hardware is limited by \ndiminishing returns, and leads to inefficient use of additional resources. In \nthis paper, we present a large batch, stochastic optimization algorithm that is \nboth faster than widely used algorithms for fixed amounts of computation, and \nalso scales up substantially better as more computational resources become \navailable. Our algorithm implicitly computes the inverse Hessian of each \nmini-batch to produce descent directions; we do so without either an explicit \napproximation to the Hessian or Hessian-vector products. We demonstrate the \neffectiveness of our algorithm by successfully training large ImageNet models \n(Inception-V3, Resnet-50, Resnet-101 and Inception-Resnet-V2) with mini-batch \nsizes of up to 32000 with no loss in validation error relative to current \nbaselines, and no increase in the total number of steps. At smaller mini-batch \nsizes, our optimizer improves the validation error in these models by 0.8-0.9%. \nAlternatively, we can trade off this accuracy to reduce the number of training \nsteps needed by roughly 10-30%. Our work is practical and easily usable by \nothers -- only one hyperparameter (learning rate) needs tuning, and \nfurthermore, the algorithm is as computationally cheap as the commonly used \nAdam optimizer. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf91", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03298"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chihao Zhang, Shihua Zhang", "title": "Bayesian Joint Matrix Decomposition for Data Integration with Heterogeneous Noise. (arXiv:1712.03337v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.03337", "type": "text/html"}], "timestampUsec": "1513055249337143", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a3bdad4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a3bdad4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Matrix decomposition is a popular and fundamental approach in machine \nlearning and data mining. It has been successfully applied into various fields. \nMost matrix decomposition methods focus on decomposing a data matrix from one \nsingle source. However, it is common that data are from different sources with \nheterogeneous noise. A few of matrix decomposition methods have been extended \nfor such multi-view data integration and pattern discovery. While only few \nmethods were designed to consider the heterogeneity of noise in such multi-view \ndata for data integration explicitly. To this end, we propose a joint matrix \ndecomposition framework (BJMD), which models the heterogeneity of noise by \nGaussian distribution in a Bayesian framework. We develop two algorithms to \nsolve this model: one is a variational Bayesian inference algorithm, which \nmakes full use of the posterior distribution; and another is a maximum a \nposterior algorithm, which is more scalable and can be easily paralleled. \nExtensive experiments on synthetic and real-world datasets demonstrate that \nBJMD considering the heterogeneity of noise is superior or competitive to the \nstate-of-the-art methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf98", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03337"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Adam McCarthy, Blanca Rodriguez, Ana Minchole", "title": "Variational Inference over Non-differentiable Cardiac Simulators using Bayesian Optimization. (arXiv:1712.03353v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03353", "type": "text/html"}], "timestampUsec": "1513055249337142", "comments": [], "summary": {"content": "<p>Performing inference over simulators is generally intractable as their \nruntime means we cannot compute a marginal likelihood. We develop a \nlikelihood-free inference method to infer parameters for a cardiac simulator, \nwhich replicates electrical flow through the heart to the body surface. We \nimprove the fit of a state-of-the-art simulator to an electrocardiogram (ECG) \nrecorded from a real patient. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf9f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03353"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Huiming Zhang, Jinzhu Jia", "title": "Elastic-net regularized High-dimensional Negative Binomial Regression: Consistency and Weak Signals Detection. (arXiv:1712.03412v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03412", "type": "text/html"}], "timestampUsec": "1513055249337141", "comments": [], "summary": {"content": "<p>We study sparse high-dimensional negative binomial regression problem for \ncount data regression by showing non-asymptotic merits of the Elastic-net \nregularized estimator. With the KKT conditions, we derive two types of \nnon-asymptotic oracle inequalities for the elastic net estimates of negative \nbinomial regression by utilizing Compatibility factor and Stabil Condition, \nrespectively. Based on oracle inequalities we proposed, we firstly show the \nsign consistency property of the Elastic-net estimators provided that the \nnon-zero components in sparse true vector are large than a proper choice of the \nweakest signal detection threshold, and the second application is that we give \nan oracle inequality for bounding the grouping effect with high probability, \nthirdly, under some assumptions of design matrix, we can recover the true \nvariable set with high probability if the weakest signal detection threshold is \nlarge than 3 times the value of turning parameter, at last, we briefly discuss \nthe de-biased Elastic-net estimator. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfa5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03412"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Matteo Pirotta, Marcello Restelli", "title": "Cost-Sensitive Approach to Batch Size Adaptation for Gradient Descent. (arXiv:1712.03428v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03428", "type": "text/html"}], "timestampUsec": "1513055249337140", "comments": [], "summary": {"content": "<p>In this paper, we propose a novel approach to automatically determine the \nbatch size in stochastic gradient descent methods. The choice of the batch size \ninduces a trade-off between the accuracy of the gradient estimate and the cost \nin terms of samples of each update. We propose to determine the batch size by \noptimizing the ratio between a lower bound to a linear or quadratic Taylor \napproximation of the expected improvement and the number of samples used to \nestimate the gradient. The performance of the proposed approach is empirically \ncompared with related methods on popular classification tasks. \n</p> \n<p>The work was presented at the NIPS workshop on Optimizing the Optimizers. \nBarcelona, Spain, 2016. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfa9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03428"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zahra Shakeri, Anand D. Sarwate, Waheed U. Bajwa", "title": "Identifiability of Kronecker-structured Dictionaries for Tensor Data. (arXiv:1712.03471v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03471", "type": "text/html"}], "timestampUsec": "1513055249337139", "comments": [], "summary": {"content": "<p>This paper derives sufficient conditions for reliable recovery of coordinate \ndictionaries comprising a Kronecker-structured dictionary that is used for \nrepresenting $K$th-order tensor data. Tensor observations are generated by a \nKronecker-structured dictionary and sparse coefficient tensors that follow the \nseparable sparsity model. This work provides sufficient conditions on the \nunderlying coordinate dictionaries, coefficient and noise distributions, and \nnumber of samples that guarantee recovery of the individual coordinate \ndictionaries up to a specified error with high probability. In particular, the \nsample complexity to recover $K$ coordinate dictionaries with dimensions \n$m_k\\times p_k$ up to estimation error $r_k$ is shown to be $\\max_{k \\in \n[K]}\\mathcal{O}(m_kp_k^3r_k^{-2})$. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfaf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03471"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Edgar Xi, Selina Bing, Yang Jin", "title": "Capsule Network Performance on Complex Data. (arXiv:1712.03480v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03480", "type": "text/html"}], "timestampUsec": "1513055249337138", "comments": [], "summary": {"content": "<p>In recent years, convolutional neural networks (CNN) have played an important \nrole in the field of deep learning. Variants of CNN's have proven to be very \nsuccessful in classification tasks across different domains. However, there are \ntwo big drawbacks to CNN's: their failure to take into account of important \nspatial hierarchies between features, and their lack of rotational invariance. \nAs long as certain key features of an object are present in the test data, \nCNN's classify the test data as the object, disregarding features' relative \nspatial orientation to each other. This causes false positives. The lack of \nrotational invariance in CNN's would cause the network to incorrectly assign \nthe object another label, causing false negatives. To address this concern, \nHinton et al. propose a novel type of neural network using the concept of \ncapsules in a recent paper. With the use of dynamic routing and reconstruction \nregularization, the capsule network model would be both rotation invariant and \nspatially aware. The capsule network has shown its potential by achieving a \nstate-of-the-art result of 0.25% test error on MNIST without data augmentation \nsuch as rotation and scaling, better than the previous baseline of 0.39%. To \nfurther test out the application of capsule networks on data with higher \ndimensionality, we attempt to find the best set of configurations that yield \nthe optimal test error on CIFAR10 dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03480"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pedro Silva, Sepehr Akhavan-Masouleh, Li Li", "title": "Improving Malware Detection Accuracy by Extracting Icon Information. (arXiv:1712.03483v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1712.03483", "type": "text/html"}], "timestampUsec": "1513055249337137", "comments": [], "summary": {"content": "<p>Detecting PE malware files is now commonly approached using statistical and \nmachine learning models. While these models commonly use features extracted \nfrom the structure of PE files, we propose that icons from these files can also \nhelp better predict malware. We propose an innovative machine learning approach \nto extract information from icons. Our proposed approach consists of two steps: \n1) extracting icon features using summary statics, histogram of gradients \n(HOG), and a convolutional autoencoder, 2) clustering icons based on the \nextracted icon features. Using publicly available data and by using machine \nlearning experiments, we show our proposed icon clusters significantly boost \nthe efficacy of malware prediction models. In particular, our experiments show \nan average accuracy increase of 10% when icon clusters are used in the \nprediction model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfb8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03483"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jason Poulos", "title": "Causal Inference for Observational Time-Series with Encoder-Decoder Networks. (arXiv:1712.03553v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03553", "type": "text/html"}], "timestampUsec": "1513055249337136", "comments": [], "summary": {"content": "<p>This paper proposes a method for estimating the causal effect of a discrete \nintervention in observational time-series data using encoder-decoder recurrent \nneural networks (RNNs). Encoder-decoder networks, which are special class of \nRNNs suitable for handling variable-length sequential data, are used to predict \na counterfactual time-series of treated unit outcomes. The proposed method does \nnot rely on pretreatment covariates and encoder-decoder networks are capable of \nlearning nonconvex combinations of control unit outcomes to construct a \ncounterfactual. To demonstrate the proposed method, I extend a field experiment \nstudying the effect of radio advertisements on electoral competition to \nobservational time-series. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfc1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03553"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Bo Wu, Yang Liu, Bo Lang, Lei Huang", "title": "DGCNN: Disordered Graph Convolutional Neural Network Based on the Gaussian Mixture Model. (arXiv:1712.03563v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03563", "type": "text/html"}], "timestampUsec": "1513055249337135", "comments": [], "summary": {"content": "<p>Convolutional neural networks (CNNs) can be applied to graph similarity \nmatching, in which case they are called graph CNNs. Graph CNNs are attracting \nincreasing attention due to their effectiveness and efficiency. However, the \nexisting convolution approaches focus only on regular data forms and require \nthe transfer of the graph or key node neighborhoods of the graph into the same \nfixed form. During this transfer process, structural information of the graph \ncan be lost, and some redundant information can be incorporated. To overcome \nthis problem, we propose the disordered graph convolutional neural network \n(DGCNN) based on the mixed Gaussian model, which extends the CNN by adding a \npreprocessing layer called the disordered graph convolutional layer (DGCL). The \nDGCL uses a mixed Gaussian function to realize the mapping between the \nconvolution kernel and the nodes in the neighborhood of the graph. The output \nof the DGCL is the input of the CNN. We further implement a \nbackward-propagation optimization process of the convolutional layer by which \nwe incorporate the feature-learning model of the irregular node neighborhood \nstructure into the network. Thereafter, the optimization of the convolution \nkernel becomes part of the neural network learning process. The DGCNN can \naccept arbitrary scaled and disordered neighborhood graph structures as the \nreceptive fields of CNNs, which reduces information loss during graph \ntransformation. Finally, we perform experiments on multiple standard graph \ndatasets. The results show that the proposed method outperforms the \nstate-of-the-art methods in graph classification and retrieval. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfc5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03563"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Stefan Depeweg, Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato, Steffen Udluft, Thomas Runkler", "title": "Sensitivity Analysis for Predictive Uncertainty in Bayesian Neural Networks. (arXiv:1712.03605v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03605", "type": "text/html"}], "timestampUsec": "1513055249337134", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a3bdd00\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a3bdd00&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We derive a novel sensitivity analysis of input variables for predictive \nepistemic and aleatoric uncertainty. We use Bayesian neural networks with \nlatent variables as a model class and illustrate the usefulness of our \nsensitivity analysis on real-world datasets. Our method increases the \ninterpretability of complex black-box probabilistic models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfcd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03605"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Robert Kwiatkowski, Oscar Chang", "title": "Gradient Normalization & Depth Based Decay For Deep Learning. (arXiv:1712.03607v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03607", "type": "text/html"}], "timestampUsec": "1513055249337133", "comments": [], "summary": {"content": "<p>In this paper we introduce a novel method of gradient normalization and decay \nwith respect to depth. Our method leverages the simple concept of normalizing \nall gradients in a deep neural network, and then decaying said gradients with \nrespect to their depth in the network. Our proposed normalization and decay \ntechniques can be used in conjunction with most current state of the art \noptimizers and are a very simple addition to any network. This method, although \nsimple, showed improvements in convergence time on state of the art networks \nsuch as DenseNet and ResNet on image classification tasks, as well as on an \nLSTM for natural language processing tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfd2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03607"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christos Thrampoulidis, Ankit Singh Rawat", "title": "The PhaseLift for Non-quadratic Gaussian Measurements. (arXiv:1712.03638v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03638", "type": "text/html"}], "timestampUsec": "1513055249337132", "comments": [], "summary": {"content": "<p>We study the problem of recovering a structured signal $\\mathbf{x}_0$ from \nhigh-dimensional measurements of the form $y=f(\\mathbf{a}^T\\mathbf{x}_0)$ for \nsome nonlinear function $f$. When the measurement vector $\\mathbf a$ is iid \nGaussian, Brillinger observed in his 1982 paper that $\\mu_\\ell\\cdot\\mathbf{x}_0 \n= \\min_{\\mathbf{x}}\\mathbb{E}(y - \\mathbf{a}^T\\mathbf{x})^2$, where \n$\\mu_\\ell=\\mathbb{E}_{\\gamma}[\\gamma f(\\gamma)]$ with $\\gamma$ being a standard \nGaussian random variable. Based on this simple observation, he showed that, in \nthe classical statistical setting, the least-squares method is consistent. More \nrecently, Plan \\&amp; Vershynin extended this result to the high-dimensional \nsetting and derived error bounds for the generalized Lasso. Unfortunately, both \nleast-squares and the Lasso fail to recover $\\mathbf{x}_0$ when $\\mu_\\ell=0$. \nFor example, this includes all even link functions. We resolve this issue by \nproposing and analyzing an appropriate generic semidefinite-optimization based \nmethod. In a nutshell, our idea is to treat such link functions as if they were \nlinear in a lifted space of higher-dimension. An appealing feature of our error \nanalysis is that it captures the effect of the nonlinearity in a few simple \nsummary parameters, which can be particularly useful in system design. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfda", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03638"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mustafa Hajij, Basem Assiri, Paul Rosen", "title": "Distributed Mapper. (arXiv:1712.03660v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.03660", "type": "text/html"}], "timestampUsec": "1513055249337131", "comments": [], "summary": {"content": "<p>The construction of Mapper has emerged in the last decade as a powerful and \neffective topological data analysis tool that approximates and generalizes \nother topological summaries, such as the Reeb graph, the contour tree, split, \nand joint trees. In this paper we study the parallel analysis of the \nconstruction of Mapper. We give a provably correct algorithm to distribute \nMapper on a set of processors and discuss the performance results that compare \nour approach to a reference sequential Mapper implementation. We report the \nperformance experiments that demonstrate the efficiency of our method. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfe6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03660"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Luiz G A Alves, Haroldo V Ribeiro, Francisco A Rodrigues", "title": "Crime prediction through urban metrics and statistical learning. (arXiv:1712.03834v1 [physics.soc-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.03834", "type": "text/html"}], "timestampUsec": "1513055249337130", "comments": [], "summary": {"content": "<p>Understanding the causes of crime is a longstanding issue in researcher's \nagenda. While it is a hard task to extract causality from data, several linear \nmodels have been proposed to predict crime through the existing correlations \nbetween crime and urban metrics. However, because of non-Gaussian distributions \nand multicollinearity in urban indicators, it is common to find controversial \nconclusions about the influence of some urban indicators on crime. Machine \nlearning ensemble-based algorithms can handle well such problems. Here, we use \na random forest regressor to predict crime and quantify the influence of urban \nindicators on homicides. Our approach can have up to $97\\%$ of accuracy on \ncrime prediction and the importance of urban indicators is ranked and clustered \nin groups of equal influence, which are robust under slightly changes in the \ndata sample analyzed. Our results determine the rank of importance of urban \nindicators to predict crime, unveiling that unemployment and illiteracy are the \nmost important variables for describing homicides in Brazilian cities. We \nfurther believe that our approach helps in producing more robust conclusions \nregarding the effects of urban indicators on crime, having potential \napplications for guiding public policies for crime control. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcffa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03834"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ferenc Husz&#xe1;r", "title": "On Quadratic Penalties in Elastic Weight Consolidation. (arXiv:1712.03847v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03847", "type": "text/html"}], "timestampUsec": "1513055249337129", "comments": [], "summary": {"content": "<p>Elastic weight consolidation (EWC, Kirkpatrick et al, 2017) is a novel \nalgorithm designed to safeguard against catastrophic forgetting in neural \nnetworks. EWC can be seen as an approximation to Laplace propagation (Eskin et \nal, 2004), and this view is consistent with the motivation given by Kirkpatrick \net al (2017). In this note, I present an extended derivation that covers the \ncase when there are more than two tasks. I show that the quadratic penalties in \nEWC are inconsistent with this derivation and might lead to double-counting \ndata from earlier tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd005", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03847"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gundeep Arora, Vinay Kumar Verma, Ashish Mishra, Piyush Rai", "title": "Generalized Zero-Shot Learning via Synthesized Examples. (arXiv:1712.03878v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03878", "type": "text/html"}], "timestampUsec": "1513055249337128", "comments": [], "summary": {"content": "<p>We present a generative framework for generalized zero-shot learning where \nthe training and test classes are not necessarily disjoint. Built upon a \nvariational autoencoder based architecture, consisting of a probabilistic \nencoder and a probabilistic conditional decoder, our model can generate novel \nexemplars from seen/unseen classes, given their respective class attributes. \nThese exemplars can subsequently be used to train any off-the-shelf \nclassification model. One of the key aspects of our encoder-decoder \narchitecture is a feedback-driven mechanism in which a discriminator (a \nmultivariate regressor) learns to map the generated exemplars to the \ncorresponding class attribute vectors, leading to an improved generator. Our \nmodel's ability to generate and leverage examples from unseen classes to train \nthe classification model naturally helps to mitigate the bias towards \npredicting seen classes in generalized zero-shot learning settings. Through a \ncomprehensive set of experiments, we show that our model outperforms several \nstate-of-the-art methods, on several benchmark datasets, for both standard as \nwell as generalized zero-shot learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd00a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03878"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Akinori Tanaka, Akio Tomiya", "title": "Towards reduction of autocorrelation in HMC by machine learning. (arXiv:1712.03893v1 [hep-lat])", "alternate": [{"href": "http://arxiv.org/abs/1712.03893", "type": "text/html"}], "timestampUsec": "1513055249337127", "comments": [], "summary": {"content": "<p>In this paper we propose new algorithm to reduce autocorrelation in Markov \nchain Monte-Carlo algorithms for euclidean field theories on the lattice. Our \nproposing algorithm is the Hybrid Monte-Carlo algorithm (HMC) with restricted \nBoltzmann machine. We examine the validity of the algorithm by employing the \nphi-fourth theory in three dimension. We observe reduction of the \nautocorrelation both in symmetric and broken phase as well. Our proposing \nalgorithm provides consistent central values of expectation values of the \naction density and one-point Green's function with ones from the original HMC \nin both the symmetric phase and broken phase within the statistical error. On \nthe other hand, two-point Green's functions have slight difference between one \ncalculated by the HMC and one by our proposing algorithm in the symmetric \nphase. Furthermore, near the criticality, the distribution of the one-point \nGreen's function differs from the one from HMC. We discuss the origin of \ndiscrepancies and its improvement. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd013", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03893"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Stefano Trac&#xe0;, Cynthia Rudin", "title": "Regulating Greed Over Time. (arXiv:1505.05629v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1505.05629", "type": "text/html"}], "timestampUsec": "1513055249337126", "comments": [], "summary": {"content": "<p>In retail, there are predictable yet dramatic time-dependent patterns in \ncustomer behavior, such as periodic changes in the number of visitors, or \nincreases in visitors just before major holidays. The current paradigm of \nmulti-armed bandit analysis does not take these known patterns into account. \nThis means that for applications in retail, where prices are fixed for periods \nof time, current bandit algorithms will not suffice. This work provides a \nremedy that takes the time-dependent patterns into account, and we show how \nthis remedy is implemented in the UCB and {\\epsilon}-greedy methods and we \nintroduce a new policy called the variable arm pool method. In the corrected \nmethods, exploitation (greed) is regulated over time, so that more exploitation \noccurs during higher reward periods, and more exploration occurs in periods of \nlow reward. In order to understand why regret is reduced with the corrected \nmethods, we present a set of bounds that provide insight into why we would want \nto exploit during periods of high reward, and discuss the impact on regret. Our \nproposed methods perform well in experiments, and were inspired by a \nhigh-scoring entry in the Exploration and Exploitation 3 contest using data \nfrom Yahoo! Front Page. That entry heavily used time-series methods to regulate \ngreed over time, which was substantially more effective than other contextual \nbandit methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd01e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1505.05629"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Matthias Bauer, Mateo Rojas-Carulla, Jakub Bart&#x142;omiej &#x15a;wi&#x105;tkowski, Bernhard Sch&#xf6;lkopf, Richard E. Turner", "title": "Discriminative k-shot learning using probabilistic models. (arXiv:1706.00326v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.00326", "type": "text/html"}], "timestampUsec": "1513055249337125", "comments": [], "summary": {"content": "<p>This paper introduces a probabilistic framework for k-shot image \nclassification. The goal is to generalise from an initial large-scale \nclassification task to a separate task comprising new classes and small numbers \nof examples. The new approach not only leverages the feature-based \nrepresentation learned by a neural network from the initial task \n(representational transfer), but also information about the classes (concept \ntransfer). The concept information is encapsulated in a probabilistic model for \nthe final layer weights of the neural network which acts as a prior for \nprobabilistic k-shot learning. We show that even a simple probabilistic model \nachieves state-of-the-art on a standard k-shot learning dataset by a large \nmargin. Moreover, it is able to accurately model uncertainty, leading to well \ncalibrated classifiers, and is easily extensible and flexible, unlike many \nrecent approaches to k-shot learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd027", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.00326"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David W Dreisigmeyer", "title": "Tight Semi-Nonnegative Matrix Factorization. (arXiv:1709.04395v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04395", "type": "text/html"}], "timestampUsec": "1513055249337124", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a3bdece\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a3bdece&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The nonnegative matrix factorization is a widely used, flexible matrix \ndecomposition, finding applications in biology, image and signal processing and \ninformation retrieval, among other areas. Here we present a related matrix \nfactorization. A multi-objective optimization problem finds conical \ncombinations of templates that approximate a given data matrix. The templates \nare chosen so that as far as possible only the initial data set can be \nrepresented this way. However, the templates are not required to be nonnegative \nnor convex combinations of the original data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd031", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04395"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Arunselvan Ramaswamy, Shalabh Bhatnagar", "title": "Conditions for Stability and Convergence of Set-Valued Stochastic Approximations: Applications to Approximate Value and Fixed point Iterations. (arXiv:1709.04673v2 [cs.SY] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04673", "type": "text/html"}], "timestampUsec": "1513055249337123", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a422d4e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a422d4e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The main aim of this paper is the development of easily verifiable sufficient \nconditions for stability (almost sure boundedness) and convergence of \nstochastic approximation algorithms (SAAs) with set-valued mean-fields, a class \nof model-free algorithms that have become important in recent times. In this \npaper we provide a complete analysis of such algorithms under three different, \nyet related sets of sufficient conditions, based on the existence of an \nassociated global/local Lyapunov function. Unlike previous Lyapunov function \nbased approaches, we provide a simple recipe for explicitly constructing the \nLyapunov function, needed for analysis. Our work builds on the works of \nAbounadi, Bertsekas and Borkar (2002), Munos (2005), and Ramaswamy and \nBhatnagar (2016). An important motivation for the flavor of our assumptions \ncomes from the need to understand dynamic programming and reinforcement \nlearning algorithms, that use deep neural networks (DNNs) for function \napproximations and parameterizations. These algorithms are popularly known as \ndeep learning algorithms. As an important application of our theory, we provide \na complete analysis of the stochastic approximation counterpart of approximate \nvalue iteration (AVI), an important dynamic programming method designed to \ntackle Bellman's curse of dimensionality. Further, the assumptions involved are \nsignificantly weaker, easily verifiable and truly model-free. The theory \npresented in this paper is also used to develop and analyze the first SAA for \nfinding fixed points of contractive set-valued maps. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd03c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04673"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anderson Y. Zhang, Harrison H. Zhou", "title": "Theoretical and Computational Guarantees of Mean Field Variational Inference for Community Detection. (arXiv:1710.11268v3 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.11268", "type": "text/html"}], "timestampUsec": "1513055249337122", "comments": [], "summary": {"content": "<p>The mean field variational Bayes method is becoming increasingly popular in \nstatistics and machine learning. Its iterative Coordinate Ascent Variational \nInference algorithm has been widely applied to large scale Bayesian inference. \nSee Blei et al. (2017) for a recent comprehensive review. Despite the \npopularity of the mean field method there exist remarkably little fundamental \ntheoretical justifications. To the best of our knowledge, the iterative \nalgorithm has never been investigated for any high dimensional and complex \nmodel. In this paper, we study the mean field method for community detection \nunder the Stochastic Block Model. For an iterative Batch Coordinate Ascent \nVariational Inference algorithm, we show that it has a linear convergence rate \nand converges to the minimax rate within $\\log n$ iterations. This complements \nthe results of Bickel et al. (2013) which studied the global minimum of the \nmean field variational Bayes and obtained asymptotic normal estimation of \nglobal model parameters. In addition, we obtain similar optimality results for \nGibbs sampling and an iterative procedure to calculate maximum likelihood \nestimation, which can be of independent interest. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd04a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.11268"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Therese Anders, Hong Xu, Cheng Cheng, T. K. Satish Kumar", "title": "Measuring Territorial Control in Civil Wars Using Hidden Markov Models: A Data Informatics-Based Approach. (arXiv:1711.06786v2 [stat.AP] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.06786", "type": "text/html"}], "timestampUsec": "1513055249337121", "comments": [], "summary": {"content": "<p>Territorial control is a key aspect shaping the dynamics of civil war. \nDespite its importance, we lack data on territorial control that are \nfine-grained enough to account for subnational spatio-temporal variation and \nthat cover a large set of conflicts. To resolve this issue, we propose a \ntheoretical model of the relationship between territorial control and tactical \nchoice in civil war and outline how Hidden Markov Models (HMMs) are suitable to \ncapture theoretical intuitions and estimate levels of territorial control. We \ndiscuss challenges of using HMMs in this application and mitigation strategies \nfor future work. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd04f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.06786"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Daniel George, Hongyu Shen, E. A. Huerta", "title": "Glitch Classification and Clustering for LIGO with Deep Transfer Learning. (arXiv:1711.07468v2 [astro-ph.IM] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07468", "type": "text/html"}], "timestampUsec": "1513055249337120", "comments": [], "summary": {"content": "<p>The detection of gravitational waves with LIGO and Virgo requires a detailed \nunderstanding of the response of these instruments in the presence of \nenvironmental and instrumental noise. Of particular interest is the study of \nanomalous non-Gaussian noise transients known as glitches, since their high \noccurrence rate in LIGO/Virgo data can obscure or even mimic true gravitational \nwave signals. Therefore, successfully identifying and excising glitches is of \nutmost importance to detect and characterize gravitational waves. In this \narticle, we present the first application of Deep Learning combined with \nTransfer Learning for glitch classification, using real data from LIGO's first \ndiscovery campaign labeled by Gravity Spy, showing that knowledge from \npre-trained models for real-world object recognition can be transferred for \nclassifying spectrograms of glitches. We demonstrate that this method enables \nthe optimal use of very deep convolutional neural networks for glitch \nclassification given small unbalanced training datasets, significantly reduces \nthe training time, and achieves state-of-the-art accuracy above 98.8%. Once \ntrained via transfer learning, we show that the networks can be truncated and \nused as feature extractors for unsupervised clustering to automatically group \ntogether new classes of glitches and anomalies. This novel capability is of \ncritical importance to identify and remove new types of glitches which will \noccur as the LIGO/Virgo detectors gradually attain design sensitivity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd060", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07468"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Subhadip Dey, Sawon Pratiher, Saon Banerjee, Chanchal Kumar Mukherjee", "title": "SolarisNet: A Deep Regression Network for Solar Radiation Prediction. (arXiv:1711.08413v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08413", "type": "text/html"}], "timestampUsec": "1513055249337119", "comments": [], "summary": {"content": "<p>Effective utilization of photovoltaic (PV) plants requires weather \nvariability robust global solar radiation (GSR) forecasting models. Random \nweather turbulence phenomena coupled with assumptions of clear sky model as \nsuggested by Hottel pose significant challenges to parametric &amp; non-parametric \nmodels in GSR conversion rate estimation. Also, a decent GSR estimate requires \ncostly high-tech radiometer and expert dependent instrument handling and \nmeasurements, which are subjective. As such, a computer aided monitoring (CAM) \nsystem to evaluate PV plant operation feasibility by employing smart grid past \ndata analytics and deep learning is developed. Our algorithm, SolarisNet is a \n6-layer deep neural network trained on data collected at two weather stations \nlocated near Kalyani metrological site, West Bengal, India. The daily GSR \nprediction performance using SolarisNet outperforms the existing state of art \nand its efficacy in inferring past GSR data insights to comprehend daily and \nseasonal GSR variability along with its competence for short term forecasting \nis discussed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd069", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08413"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kartik Audhkhasi, Brian Kingsbury, Bhuvana Ramabhadran, George Saon, Michael Picheny", "title": "Building competitive direct acoustics-to-word models for English conversational speech recognition. (arXiv:1712.03133v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.03133", "type": "text/html"}], "timestampUsec": "1512969981619087", "comments": [], "summary": {"content": "<p>Direct acoustics-to-word (A2W) models in the end-to-end paradigm have \nreceived increasing attention compared to conventional sub-word based automatic \nspeech recognition models using phones, characters, or context-dependent hidden \nMarkov model states. This is because A2W models recognize words from speech \nwithout any decoder, pronunciation lexicon, or externally-trained language \nmodel, making training and decoding with such models simple. Prior work has \nshown that A2W models require orders of magnitude more training data in order \nto perform comparably to conventional models. Our work also showed this \naccuracy gap when using the English Switchboard-Fisher data set. This paper \ndescribes a recipe to train an A2W model that closes this gap and is at-par \nwith state-of-the-art sub-word based models. We achieve a word error rate of \n8.8%/13.9% on the Hub5-2000 Switchboard/CallHome test sets without any decoder \nor language model. We find that model initialization, training data order, and \nregularization have the most impact on the A2W model performance. Next, we \npresent a joint word-character A2W model that learns to first spell the word \nand then recognize it. This model provides a rich output to the user instead of \nsimple word hypotheses, making it especially useful in the case of words unseen \nor rarely-seen during training. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902761", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03133"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wei Chen, YingYing Cao, Yifei Sun, Qunfeng Liu, Yun Li", "title": "Improving Brain Storm Optimization Algorithm via Simplex Search. (arXiv:1712.03166v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.03166", "type": "text/html"}], "timestampUsec": "1512969981619086", "comments": [], "summary": {"content": "<p>Through modeling human's brainstorming process, the brain storm optimization \n(BSO) algorithm has become a promising population based evolution algorithm. \nHowever, BSO is often good at global exploration but not good enough at local \nexploitation, just like most global optimization algorithms. In this paper, the \nNelder-Mead's Simplex (NMS) method is adopted in a simple version of BSO. Our \ngoal is to combine BSO's exploration ability and NMS's exploitation ability \ntogether, and develop an enhanced BSO via a better balance between global \nexploration and local exploitation. Large number of experimental results are \nreported, and the proposed algorithm is shown to perform better than both BSO \nand NMS. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902782", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03166"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Basant Agarwal, Heri Ramampiaro, Helge Langseth, Massimiliano Ruocco", "title": "A Deep Network Model for Paraphrase Detection in Short Text Messages. (arXiv:1712.02820v1 [cs.IR])", "alternate": [{"href": "http://arxiv.org/abs/1712.02820", "type": "text/html"}], "timestampUsec": "1512969981619084", "comments": [], "summary": {"content": "<p>This paper is concerned with paraphrase detection. The ability to detect \nsimilar sentences written in natural language is crucial for several \napplications, such as text mining, text summarization, plagiarism detection, \nauthorship authentication and question answering. Given two sentences, the \nobjective is to detect whether they are semantically identical. An important \ninsight from this work is that existing paraphrase systems perform well when \napplied on clean texts, but they do not necessarily deliver good performance \nagainst noisy texts. Challenges with paraphrase detection on user generated \nshort texts, such as Twitter, include language irregularity and noise. To cope \nwith these challenges, we propose a novel deep neural network-based approach \nthat relies on coarse-grained sentence modeling using a convolutional neural \nnetwork and a long short-term memory model, combined with a specific \nfine-grained word-level similarity matching model. Our experimental results \nshow that the proposed approach outperforms existing state-of-the-art \napproaches on user-generated noisy social media data, such as Twitter texts, \nand achieves highly competitive performance on a cleaner corpus. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449027ab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02820"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Li Zhou, Kevin Small, Oleg Rokhlenko, Charles Elkan", "title": "End-to-End Offline Goal-Oriented Dialog Policy Learning via Policy Gradient. (arXiv:1712.02838v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.02838", "type": "text/html"}], "timestampUsec": "1512969981619083", "comments": [], "summary": {"content": "<p>Learning a goal-oriented dialog policy is generally performed offline with \nsupervised learning algorithms or online with reinforcement learning (RL). \nAdditionally, as companies accumulate massive quantities of dialog transcripts \nbetween customers and trained human agents, encoder-decoder methods have gained \npopularity as agent utterances can be directly treated as supervision without \nthe need for utterance-level annotations. However, one potential drawback of \nsuch approaches is that they myopically generate the next agent utterance \nwithout regard for dialog-level considerations. To resolve this concern, this \npaper describes an offline RL method for learning from unannotated corpora that \ncan optimize a goal-oriented policy at both the utterance and dialog level. We \nintroduce a novel reward function and use both on-policy and off-policy policy \ngradient to learn a policy offline without requiring online user interaction or \nan explicit state space definition. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449027b4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02838"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aditya Ganeshan", "title": "Per-Pixel Feedback for improving Semantic Segmentation. (arXiv:1712.02861v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.02861", "type": "text/html"}], "timestampUsec": "1512969981619082", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a422fd5\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a422fd5&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Semantic segmentation is the task of assigning a label to each pixel in the \nimage.In recent years, deep convolutional neural networks have been driving \nadvances in multiple tasks related to cognition. Although, DCNNs have resulted \nin unprecedented visual recognition performances, they offer little \ntransparency. To understand how DCNN based models work at the task of semantic \nsegmentation, we try to analyze the DCNN models in semantic segmentation. We \ntry to find the importance of global image information for labeling pixels. \n</p> \n<p>Based on the experiments on discriminative regions, and modeling of \nfixations, we propose a set of new training loss functions for fine-tuning DCNN \nbased models. The proposed training regime has shown improvement in performance \nof DeepLab Large FOV(VGG-16) Segmentation model for PASCAL VOC 2012 dataset. \nHowever, further test remains to conclusively evaluate the benefits due to the \nproposed loss functions across models, and data-sets. \n</p> \n<p>Submitted in part fulfillment of the requirements for the degree of \nIntegrated Masters of Science in Applied Mathematics. \n</p> \n<p>Update: Further Experiment showed minimal benefits. \n</p> \n<p>Code Available [here](https://github.com/BardOfCodes/Seg-Unravel). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449027c8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02861"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Harold Boley, Gen Zou", "title": "Perspectival Knowledge in PSOA RuleML: Representation, Model Theory, and Translation. (arXiv:1712.02869v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.02869", "type": "text/html"}], "timestampUsec": "1512969981619081", "comments": [], "summary": {"content": "<p>In Positional-Slotted Object-Applicative (PSOA) RuleML, a predicate \napplication (atom) can have an Object IDentifier (OID) and descriptors that may \nbe positional arguments (tuples) or attribute-value pairs (slots). PSOA RuleML \n1.0 specifies for each descriptor whether it is to be interpreted under the \nperspective of the predicate in whose scope it occurs. This perspectivity \ndimension refines the space between oidless, positional atoms (relationships) \nand oidful, slotted atoms (frames): While relationships use only a \npredicate-scope-sensitive (predicate-dependent) tuple and frames use only \npredicate-scope-insensitive (predicate-independent) slots, PSOA RuleML 1.0 uses \na systematics of orthogonal constructs also permitting atoms with \n(predicate-)independent tuples and atoms with (predicate-)dependent slots. This \nsupports data and knowledge representation where a slot attribute can have \ndifferent values depending on the predicate. PSOA thus extends object-oriented \nmulti-membership and multiple inheritance. Based on objectification, PSOA laws \nare given: Besides unscoping and centralization, the semantic restriction and \ntransformation of describution permits rescoping of one atom's independent \ndescriptors to another atom with the same OID but a different predicate. For \ninheritance, default descriptors are realized by rules. On top of a metamodel \nand a Grailog visualization, PSOA's atom systematics for facts, queries, and \nrules is explained. The presentation and (XML-)serialization syntaxes of PSOA \nRuleML 1.0 are introduced. Its model-theoretic semantics is formalized by \nextending the earlier interpretation functions for dependent descriptors. The \nopen-source PSOATransRun 1.3 system realizes PSOA RuleML 1.0 by a translator to \nruntime predicates, including for dependent tuples (prdtupterm) and slots \n(prdsloterm). Our tests show efficiency advantages of dependent and tupled \nmodeling. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449027ea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02869"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chen Zhu, Hengshu Zhu, Hui Xiong, Pengliang Ding, Fang Xie", "title": "Recruitment Market Trend Analysis with Sequential Latent Variable Models. (arXiv:1712.02975v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.02975", "type": "text/html"}], "timestampUsec": "1512969981619080", "comments": [], "summary": {"content": "<p>Recruitment market analysis provides valuable understanding of \nindustry-specific economic growth and plays an important role for both \nemployers and job seekers. With the rapid development of online recruitment \nservices, massive recruitment data have been accumulated and enable a new \nparadigm for recruitment market analysis. However, traditional methods for \nrecruitment market analysis largely rely on the knowledge of domain experts and \nclassic statistical models, which are usually too general to model large-scale \ndynamic recruitment data, and have difficulties to capture the fine-grained \nmarket trends. To this end, in this paper, we propose a new research paradigm \nfor recruitment market analysis by leveraging unsupervised learning techniques \nfor automatically discovering recruitment market trends based on large-scale \nrecruitment data. Specifically, we develop a novel sequential latent variable \nmodel, named MTLVM, which is designed for capturing the sequential dependencies \nof corporate recruitment states and is able to automatically learn the latent \nrecruitment topics within a Bayesian generative framework. In particular, to \ncapture the variability of recruitment topics over time, we design hierarchical \ndirichlet processes for MTLVM. These processes allow to dynamically generate \nthe evolving recruitment topics. Finally, we implement a prototype system to \nempirically evaluate our approach based on real-world recruitment data in \nChina. Indeed, by visualizing the results from MTLVM, we can successfully \nreveal many interesting findings, such as the popularity of LBS related jobs \nreached the peak in the 2nd half of 2014, and decreased in 2015. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449027ff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02975"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Farnood Salehi, Patrick Thiran, L. Elisa Celis", "title": "Stochastic Dual Coordinate Descent with Bandit Sampling. (arXiv:1712.03010v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03010", "type": "text/html"}], "timestampUsec": "1512969981619079", "comments": [], "summary": {"content": "<p>Coordinate descent methods minimize a cost function by updating a single \ndecision variable (corresponding to one coordinate) at a time. Ideally, one \nwould update the decision variable that yields the largest marginal decrease in \nthe cost function. However, finding this coordinate would require checking all \nof them, which is not computationally practical. We instead propose a new \nadaptive method for coordinate descent. First, we define a lower bound on the \ndecrease of the cost function when a coordinate is updated and, instead of \ncalculating this lower bound for all coordinates, we use a multi-armed bandit \nalgorithm to learn which coordinates result in the largest marginal decrease \nwhile simultaneously performing coordinate descent. We show that our approach \nimproves the convergence of the coordinate methods (including parallel \nversions) both theoretically and experimentally. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902809", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03010"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zengkun Li", "title": "A Heuristic Search Algorithm Using the Stability of Learning Algorithms as the Fitness Function in Certain Scenarios: An Artificial General Intelligence Engineering Approach. (arXiv:1712.03043v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.03043", "type": "text/html"}], "timestampUsec": "1512969981619078", "comments": [], "summary": {"content": "<p>This paper presents a non-manual design engineering method based on heuristic \nsearch algorithm to search for candidate agents in the solution space which \nformed by artificial intelligence agents modeled on the base of \nbionics.Compared with the artificial design method represented by meta-learning \nand the bionics method represented by the neural architecture chip,this method \nis more feasible for realizing artificial general intelligence,and it has a \nmuch better interaction with cognitive neuroscience;at the same time,the \nengineering method is based on the theoretical hypothesis that the final \nlearning algorithm is stable in certain scenarios,and has generalization \nability in various scenarios.The paper discusses the theory preliminarily and \nproposes the possible correlation between the theory and the fixed-point \ntheorem in the field of mathematics.Limited by the author's knowledge \nlevel,this correlation is proposed only as a kind of conjecture. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902814", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03043"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mayank Kejriwal, Jiayuan Ding, Runqi Shao, Anoop Kumar, Pedro Szekely", "title": "FlagIt: A System for Minimally Supervised Human Trafficking Indicator Mining. (arXiv:1712.03086v1 [cs.CY])", "alternate": [{"href": "http://arxiv.org/abs/1712.03086", "type": "text/html"}], "timestampUsec": "1512969981619077", "comments": [], "summary": {"content": "<p>In this paper, we describe and study the indicator mining problem in the \nonline sex advertising domain. We present an in-development system, FlagIt \n(Flexible and adaptive generation of Indicators from text), which combines the \nbenefits of both a lightweight expert system and classical semi-supervision \n(heuristic re-labeling) with recently released state-of-the-art unsupervised \ntext embeddings to tag millions of sentences with indicators that are highly \ncorrelated with human trafficking. The FlagIt technology stack is open source. \nOn preliminary evaluations involving five indicators, FlagIt illustrates \npromising performance compared to several alternatives. The system is being \nactively developed, refined and integrated into a domain-specific search system \nused by over 200 law enforcement agencies to combat human trafficking, and is \nbeing aggressively extended to mine at least six more indicators with minimal \nprogramming effort. FlagIt is a good example of a system that operates in \nlimited label settings, and that requires creative combinations of established \nmachine learning techniques to produce outputs that could be used by real-world \nnon-technical analysts. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902818", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03086"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Charles A. Johnson, Enoch Yeung", "title": "A Class of Logistic Functions for Approximating State-Inclusive Koopman Operators. (arXiv:1712.03132v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03132", "type": "text/html"}], "timestampUsec": "1512969981619076", "comments": [], "summary": {"content": "<p>An outstanding challenge in nonlinear systems theory is identification or \nlearning of a given nonlinear system's Koopman operator directly from data or \nmodels. Advances in extended dynamic mode decomposition approaches and machine \nlearning methods have enabled data-driven discovery of Koopman operators, for \nboth continuous and discrete-time systems. Since Koopman operators are often \ninfinite-dimensional, they are approximated in practice using \nfinite-dimensional systems. The fidelity and convergence of a given \nfinite-dimensional Koopman approximation is a subject of ongoing research. In \nthis paper we introduce a class of Koopman observable functions that confer an \napproximate closure property on their corresponding finite-dimensional \napproximations of the Koopman operator. We derive error bounds for the fidelity \nof this class of observable functions, as well as identify two key learning \nparameters which can be used to tune performance. We illustrate our approach on \ntwo classical nonlinear system models: the Van Der Pol oscillator and the \nbistable toggle switch. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902822", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03132"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sylwia Polberg, Anthony Hunter", "title": "Empirical Evaluation of Abstract Argumentation: Supporting the Need for Bipolar and Probabilistic Approaches. (arXiv:1707.09324v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.09324", "type": "text/html"}], "timestampUsec": "1512969981619075", "comments": [], "summary": {"content": "<p>In dialogical argumentation it is often assumed that the involved parties \nalways correctly identify the intended statements posited by each other, \nrealize all of the associated relations, conform to the three acceptability \nstates (accepted, rejected, undecided), adjust their views when new and correct \ninformation comes in, and that a framework handling only attack relations is \nsufficient to represent their opinions. Although it is natural to make these \nassumptions as a starting point for further research, removing them or even \nacknowledging that such removal should happen is more challenging for some of \nthese concepts than for others. Probabilistic argumentation is one of the \napproaches that can be harnessed for more accurate user modelling. The \nepistemic approach allows us to represent how much a given argument is believed \nby a given person, offering us the possibility to express more than just three \nagreement states. It is equipped with a wide range of postulates, including \nthose that do not make any restrictions concerning how initial arguments should \nbe viewed, thus potentially being more adequate for handling beliefs of the \npeople that have not fully disclosed their opinions in comparison to Dung's \nsemantics. The constellation approach can be used to represent the views of \ndifferent people concerning the structure of the framework we are dealing with, \nincluding cases in which not all relations are acknowledged or when they are \nseen differently than intended. Finally, bipolar argumentation frameworks can \nbe used to express both positive and negative relations between arguments. In \nthis paper we describe the results of an experiment in which participants \njudged dialogues in terms of agreement and structure. We compare our findings \nwith the aforementioned assumptions as well as with the constellation and \nepistemic approaches to probabilistic argumentation and bipolar argumentation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902832", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.09324"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexander Peysakhovich, Adam Lerer", "title": "Prosocial learning agents solve generalized Stag Hunts better than selfish ones. (arXiv:1709.02865v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.02865", "type": "text/html"}], "timestampUsec": "1512969981619074", "comments": [], "summary": {"content": "<p>Deep reinforcement learning has become an important paradigm for constructing \nagents that can enter complex multi-agent situations and improve their policies \nthrough experience. One commonly used technique is reactive training - applying \nstandard RL methods while treating other agents as a part of the learner's \nenvironment. It is known that in general-sum games reactive training can lead \ngroups of agents to converge to inefficient outcomes. We focus on one such \nclass of environments: Stag Hunt games. Here agents either choose a risky \ncooperative policy (which leads to high payoffs if both choose it but low \npayoffs to an agent who attempts it alone) or a safe one (which leads to a safe \npayoff no matter what). We ask how we can change the learning rule of a single \nagent to improve its outcomes in Stag Hunts that include other reactive \nlearners. We extend existing work on reward-shaping in multi-agent \nreinforcement learning and show that that making a single agent prosocial, that \nis, making them care about the rewards of their partners can increase the \nprobability that groups converge to good outcomes. Thus, even if we control a \nsingle agent in a group making that agent prosocial can increase our agent's \nlong-run payoff. We show experimentally that this result carries over to a \nvariety of more complex environments with Stag Hunt-like dynamics including \nones where agents must learn from raw input pixels. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902846", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.02865"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, Jun Wang", "title": "Long Text Generation via Adversarial Training with Leaked Information. (arXiv:1709.08624v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.08624", "type": "text/html"}], "timestampUsec": "1512969981619073", "comments": [], "summary": {"content": "<p>Automatically generating coherent and semantically meaningful text has many \napplications in machine translation, dialogue systems, image captioning, etc. \nRecently, by combining with policy gradient, Generative Adversarial Nets (GAN) \nthat use a discriminative model to guide the training of the generative model \nas a reinforcement learning policy has shown promising results in text \ngeneration. However, the scalar guiding signal is only available after the \nentire text has been generated and lacks intermediate information about text \nstructure during the generative process. As such, it limits its success when \nthe length of the generated text samples is long (more than 20 words). In this \npaper, we propose a new framework, called LeakGAN, to address the problem for \nlong text generation. We allow the discriminative net to leak its own \nhigh-level extracted features to the generative net to further help the \nguidance. The generator incorporates such informative signals into all \ngeneration steps through an additional Manager module, which takes the \nextracted features of current generated words and outputs a latent vector to \nguide the Worker module for next-word generation. Our extensive experiments on \nsynthetic data and various real-world tasks with Turing test demonstrate that \nLeakGAN is highly effective in long text generation and also improves the \nperformance in short text generation scenarios. More importantly, without any \nsupervision, LeakGAN would be able to implicitly learn sentence structures only \nthrough the interaction between Manager and Worker. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902858", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.08624"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Enoch Yeung, Soumya Kundu, Nathan Hodas", "title": "Learning Deep Neural Network Representations for Koopman Operators of Nonlinear Dynamical Systems. (arXiv:1708.06850v2 [cs.LG] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1708.06850", "type": "text/html"}], "timestampUsec": "1512969981619072", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a423270\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a423270&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The Koopman operator has recently garnered much attention for its value in \ndynamical systems analysis and data-driven model discovery. However, its \napplication has been hindered by the computational complexity of extended \ndynamic mode decomposition; this requires a combinatorially large basis set to \nadequately describe many nonlinear systems of interest, e.g. cyber-physical \ninfrastructure systems, biological networks, social systems, and fluid \ndynamics. Often the dictionaries generated for these problems are manually \ncurated, requiring domain-specific knowledge and painstaking tuning. In this \npaper we introduce a deep learning framework for learning Koopman operators of \nnonlinear dynamical systems. We show that this novel method automatically \nselects efficient deep dictionaries, outperforming state-of-the-art methods. We \nbenchmark this method on partially observed nonlinear systems, including the \nglycolytic oscillator and show it is able to predict quantitatively 100 steps \ninto the future, using only a single timepoint, and qualitative oscillatory \nbehavior 400 steps into the future. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902869", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.06850"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Seyed Mehran Kazemi, David Poole", "title": "RelNN: A Deep Neural Model for Relational Learning. (arXiv:1712.02831v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02831", "type": "text/html"}], "timestampUsec": "1512969981619071", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a47caba\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a47caba&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Statistical relational AI (StarAI) aims at reasoning and learning in noisy \ndomains described in terms of objects and relationships by combining \nprobability with first-order logic. With huge advances in deep learning in the \ncurrent years, combining deep networks with first-order logic has been the \nfocus of several recent studies. Many of the existing attempts, however, only \nfocus on relations and ignore object properties. The attempts that do consider \nobject properties are limited in terms of modelling power or scalability. In \nthis paper, we develop relational neural networks (RelNNs) by adding hidden \nlayers to relational logistic regression (the relational counterpart of \nlogistic regression). We learn latent properties for objects both directly and \nthrough general rules. Back-propagation is used for training these models. A \nmodular, layer-wise architecture facilitates utilizing the techniques developed \nwithin deep learning community to our architecture. Initial experiments on \neight tasks over three real-world datasets show that RelNNs are promising \nmodels for relational learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902883", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02831"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lukas Mosser, Olivier Dubrule, Martin J. Blunt", "title": "Stochastic reconstruction of an oolitic limestone by generative adversarial networks. (arXiv:1712.02854v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.02854", "type": "text/html"}], "timestampUsec": "1512969981619070", "comments": [], "summary": {"content": "<p>Stochastic image reconstruction is a key part of modern digital rock physics \nand materials analysis that aims to create numerous representative samples of \nmaterial micro-structures for upscaling, numerical computation of effective \nproperties and uncertainty quantification. We present a method of \nthree-dimensional stochastic image reconstruction based on generative \nadversarial neural networks (GANs). GANs represent a framework of unsupervised \nlearning methods that require no a priori inference of the probability \ndistribution associated with the training data. Using a fully convolutional \nneural network allows fast sampling of large volumetric images.We apply a GAN \nbased workflow of network training and image generation to an oolitic Ketton \nlimestone micro-CT dataset. Minkowski functionals, effective permeability as \nwell as velocity distributions of simulated flow within the acquired images are \ncompared with the synthetic reconstructions generated by the deep neural \nnetwork. While our results show that GANs allow a fast and accurate \nreconstruction of the evaluated image dataset, we address a number of open \nquestions and challenges involved in the evaluation of generative network-based \nmethods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902893", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02854"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Valerio Perrone, Rodolphe Jenatton, Matthias Seeger, Cedric Archambeau", "title": "Multiple Adaptive Bayesian Linear Regression for Scalable Bayesian Optimization with Warm Start. (arXiv:1712.02902v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02902", "type": "text/html"}], "timestampUsec": "1512969981619069", "comments": [], "summary": {"content": "<p>Bayesian optimization (BO) is a model-based approach for gradient-free \nblack-box function optimization. Typically, BO is powered by a Gaussian process \n(GP), whose algorithmic complexity is cubic in the number of evaluations. \nHence, GP-based BO cannot leverage large amounts of past or related function \nevaluations, for example, to warm start the BO procedure. We develop a multiple \nadaptive Bayesian linear regression model as a scalable alternative whose \ncomplexity is linear in the number of observations. The multiple Bayesian \nlinear regression models are coupled through a shared feedforward neural \nnetwork, which learns a joint representation and transfers knowledge across \nmachine learning problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449028b6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02902"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Panagiotis A. Traganitis, Alba Pag&#xe8;s-Zamora, Georgios B. Giannakis", "title": "Blind Multi-class Ensemble Learning with Unequally Reliable Classifiers. (arXiv:1712.02903v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02903", "type": "text/html"}], "timestampUsec": "1512969981619068", "comments": [], "summary": {"content": "<p>The rising interest in pattern recognition and data analytics has spurred the \ndevelopment of innovative machine learning algorithms and tools. However, as \neach algorithm has its strengths and limitations, one is motivated to \njudiciously fuse multiple algorithms in order to find the \"best\" performing \none, for a given dataset. Ensemble learning aims at such high-performance \nmeta-algorithm, by combining the outputs from multiple algorithms. The present \nwork introduces a blind scheme for learning from ensembles of classifiers, \nusing a moment matching method that leverages joint tensor and matrix \nfactorization. Blind refers to the combiner who has no knowledge of the \nground-truth labels that each classifier has been trained on. A rigorous \nperformance analysis is derived and the proposed scheme is evaluated on \nsynthetic and real datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449028cb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02903"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Casey Chu, Andrey Zhmoginov, Mark Sandler", "title": "CycleGAN: a Master of Steganography. (arXiv:1712.02950v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.02950", "type": "text/html"}], "timestampUsec": "1512969981619067", "comments": [], "summary": {"content": "<p>CycleGAN is one of the latest successful approaches to learn a correspondence \nbetween two image distributions. In a series of experiments, we demonstrate an \nintriguing property of the model: CycleGAN learns to \"hide\" information about a \nsource image inside the generated image in nearly imperceptible, high-frequency \nnoise. This trick ensures that the complementary generator can recover the \noriginal sample and thus satisfy the cyclic consistency requirement, but the \ngenerated image remains realistic. We connect this phenomenon with adversarial \nattacks by viewing CycleGAN's training procedure as training a generator of \nadversarial examples, thereby showing that adversarial attacks are not limited \nto classifiers but also may target generative models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449028d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02950"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xue Lu, Niall Adams, Nikolas Kantas", "title": "On Adaptive Estimation for Dynamic Bernoulli Bandits. (arXiv:1712.03134v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03134", "type": "text/html"}], "timestampUsec": "1512969981619066", "comments": [], "summary": {"content": "<p>The multi-armed bandit (MAB) problem is a classic example of the \nexploration-exploitation dilemma. It is concerned with maximising the total \nrewards for a gambler by sequentially pulling an arm from a multi-armed slot \nmachine where each arm is associated with a reward distribution. In static \nMABs, the reward distributions do not change over time, while in dynamic MABs, \neach arm's reward distribution can change, and the optimal arm can switch over \ntime. Motivated by many real applications where rewards are binary counts, we \nfocus on dynamic Bernoulli bandits. Standard methods like $\\epsilon$-Greedy and \nUpper Confidence Bound (UCB), which rely on the sample mean estimator, often \nfail to track the changes in underlying reward for dynamic problems. In this \npaper, we overcome the shortcoming of slow response to change by deploying \nadaptive estimation in the standard methods and propose a new family of \nalgorithms, which are adaptive versions of $\\epsilon$-Greedy, UCB, and Thompson \nsampling. These new methods are simple and easy to implement. Moreover, they do \nnot require any prior knowledge about the data, which is important for real \napplications. We examine the new algorithms numerically in different scenarios \nand find out that the results show solid improvements of our algorithms in \ndynamic environments. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449028dc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03134"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexander Novikov, Mikhail Trofimov, Ivan Oseledets", "title": "Exponential Machines. (arXiv:1605.03795v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1605.03795", "type": "text/html"}], "timestampUsec": "1512969981619065", "comments": [], "summary": {"content": "<p>Modeling interactions between features improves the performance of machine \nlearning solutions in many domains (e.g. recommender systems or sentiment \nanalysis). In this paper, we introduce Exponential Machines (ExM), a predictor \nthat models all interactions of every order. The key idea is to represent an \nexponentially large tensor of parameters in a factorized format called Tensor \nTrain (TT). The Tensor Train format regularizes the model and lets you control \nthe number of underlying parameters. To train the model, we develop a \nstochastic Riemannian optimization procedure, which allows us to fit tensors \nwith 2^160 entries. We show that the model achieves state-of-the-art \nperformance on synthetic data with high-order interactions and that it works on \npar with high-order factorization machines on a recommender system dataset \nMovieLens 100K. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449028e9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1605.03795"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yuxin Chen, Emmanuel Candes", "title": "The Projected Power Method: An Efficient Algorithm for Joint Alignment from Pairwise Differences. (arXiv:1609.05820v3 [cs.IT] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1609.05820", "type": "text/html"}], "timestampUsec": "1512969981619064", "comments": [], "summary": {"content": "<p>Various applications involve assigning discrete label values to a collection \nof objects based on some pairwise noisy data. Due to the discrete---and hence \nnonconvex---structure of the problem, computing the optimal assignment \n(e.g.~maximum likelihood assignment) becomes intractable at first sight. This \npaper makes progress towards efficient computation by focusing on a concrete \njoint alignment problem---that is, the problem of recovering $n$ discrete \nvariables $x_i \\in \\{1,\\cdots, m\\}$, $1\\leq i\\leq n$ given noisy observations \nof their modulo differences $\\{x_i - x_j~\\mathsf{mod}~m\\}$. We propose a \nlow-complexity and model-free procedure, which operates in a lifted space by \nrepresenting distinct label values in orthogonal directions, and which attempts \nto optimize quadratic functions over hypercubes. Starting with a first guess \ncomputed via a spectral method, the algorithm successively refines the iterates \nvia projected power iterations. We prove that for a broad class of statistical \nmodels, the proposed projected power method makes no error---and hence \nconverges to the maximum likelihood estimate---in a suitable regime. Numerical \nexperiments have been carried out on both synthetic and real data to \ndemonstrate the practicality of our algorithm. We expect this algorithmic \nframework to be effective for a broad range of discrete assignment problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449028f1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1609.05820"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lanlan Liu, Jia Deng", "title": "Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs by Selective Execution. (arXiv:1701.00299v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1701.00299", "type": "text/html"}], "timestampUsec": "1512969981619063", "comments": [], "summary": {"content": "<p>We introduce Dynamic Deep Neural Networks (D2NN), a new type of feed-forward \ndeep neural network that allows selective execution. Given an input, only a \nsubset of D2NN neurons are executed, and the particular subset is determined by \nthe D2NN itself. By pruning unnecessary computation depending on input, D2NNs \nprovide a way to improve computational efficiency. To achieve dynamic selective \nexecution, a D2NN augments a feed-forward deep neural network (directed acyclic \ngraph of differentiable modules) with controller modules. Each controller \nmodule is a sub-network whose output is a decision that controls whether other \nmodules can execute. A D2NN is trained end to end. Both regular and controller \nmodules in a D2NN are learnable and are jointly trained to optimize both \naccuracy and efficiency. Such training is achieved by integrating \nbackpropagation with reinforcement learning. With extensive experiments of \nvarious D2NN architectures on image classification tasks, we demonstrate that \nD2NNs are general and flexible, and can effectively optimize \naccuracy-efficiency trade-offs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449028f9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1701.00299"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yizhen Wang, Somesh Jha, Kamalika Chaudhuri", "title": "Analyzing the Robustness of Nearest Neighbors to Adversarial Examples. (arXiv:1706.03922v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.03922", "type": "text/html"}], "timestampUsec": "1512969981619062", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a47ce6b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a47ce6b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Motivated by applications such as autonomous vehicles, test-time attacks via \nadversarial examples have received a great deal of recent attention. In this \nsetting, an adversary is capable of making queries to a classifier, and \nperturbs a test example by a small amount in order to force the classifier to \nreport an incorrect label. While a long line of work has explored a number of \nattacks, not many reliable defenses are known, and there is an overall lack of \ngeneral understanding about the foundations of designing machine learning \nalgorithms robust to adversarial examples. \n</p> \n<p>In this paper, we take a step towards addressing this challenging question by \nintroducing a new theoretical framework, analogous to bias-variance theory, \nwhich we can use to tease out the causes of vulnerability. We apply our \nframework to a simple classification algorithm: nearest neighbors, and analyze \nits robustness to adversarial examples. Motivated by our analysis, we propose a \nmodified version of the nearest neighbor algorithm, and demonstrate both \ntheoretically and empirically that it has superior robustness to standard \nnearest neighbors. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902907", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.03922"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Long Chen, Fajie Yuan, Joemon M. Jose, Weinan Zhang", "title": "Improving Negative Sampling for Word Representation using Self-embedded Features. (arXiv:1710.09805v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.09805", "type": "text/html"}], "timestampUsec": "1512969981619061", "comments": [], "summary": {"content": "<p>Although the word-popularity based negative sampler has shown superb \nperformance in the skip-gram model, the theoretical motivation behind \noversampling popular (non-observed) words as negative samples is still not well \nunderstood. In this paper, we start from an investigation of the gradient \nvanishing issue in the skip-gram model without a proper negative sampler. By \nperforming an insightful analysis from the stochastic gradient descent (SGD) \nlearning perspective, we demonstrate that, both theoretically and intuitively, \nnegative samples with larger inner product scores are more informative than \nthose with lower scores for the SGD learner in terms of both convergence rate \nand accuracy. Understanding this, we propose an alternative sampling algorithm \nthat dynamically selects informative negative samples during each SGD update. \nMore importantly, the proposed sampler accounts for multi-dimensional \nself-embedded features during the sampling process, which essentially makes it \nmore effective than the original popularity-based (one-dimensional) sampler. \nEmpirical experiments further verify our observations, and show that our \nfine-grained samplers gain significant improvement over the existing ones \nwithout increasing computational complexity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034490290c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.09805"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Geng Ji, Robert Bamler, Erik B. Sudderth, Stephan Mandt", "title": "Bayesian Paragraph Vectors. (arXiv:1711.03946v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.03946", "type": "text/html"}], "timestampUsec": "1512969981619060", "comments": [], "summary": {"content": "<p>Word2vec (Mikolov et al., 2013) has proven to be successful in natural \nlanguage processing by capturing the semantic relationships between different \nwords. Built on top of single-word embeddings, paragraph vectors (Le and \nMikolov, 2014) find fixed-length representations for pieces of text with \narbitrary lengths, such as documents, paragraphs, and sentences. In this work, \nwe propose a novel interpretation for neural-network-based paragraph vectors by \ndeveloping an unsupervised generative model whose maximum likelihood solution \ncorresponds to traditional paragraph vectors. This probabilistic formulation \nallows us to go beyond point estimates of parameters and to perform Bayesian \nposterior inference. We find that the entropy of paragraph vectors decreases \nwith the length of documents, and that information about posterior uncertainty \nimproves performance in supervised learning tasks such as sentiment analysis \nand paraphrase detection. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902913", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.03946"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rhicheek Patra, Egor Samosvat, Michael Roizner, Andrei Mishchenko", "title": "BoostJet: Towards Combining Statistical Aggregates with Neural Embeddings for Recommendations. (arXiv:1711.05828v2 [cs.IR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05828", "type": "text/html"}], "timestampUsec": "1512969981619059", "comments": [], "summary": {"content": "<p>Recommenders have become widely popular in recent years because of their \nbroader applicability in many e-commerce applications. These applications rely \non recommenders for generating advertisements for various offers or providing \ncontent recommendations. However, the quality of the generated recommendations \ndepends on user features (like demography, temporality), offer features (like \npopularity, price), and user-offer features (like implicit or explicit \nfeedback). Current state-of-the-art recommenders do not explore such diverse \nfeatures concurrently while generating the recommendations. \n</p> \n<p>In this paper, we first introduce the notion of Trackers which enables us to \ncapture the above-mentioned features and thus incorporate users' online \nbehaviour through statistical aggregates of different features (demography, \ntemporality, popularity, price). We also show how to capture offer-to-offer \nrelations, based on their consumption sequence, leveraging neural embeddings \nfor offers in our Offer2Vec algorithm. We then introduce BoostJet, a novel \nrecommender which integrates the Trackers along with the neural embeddings \nusing MatrixNet, an efficient distributed implementation of gradient boosted \ndecision tree, to improve the recommendation quality significantly. We provide \nan in-depth evaluation of BoostJet on Yandex's dataset, collecting online \nbehaviour from tens of millions of online users, to demonstrate the \npracticality of BoostJet in terms of recommendation quality as well as \nscalability. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902917", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05828"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rachid Guerraoui, Erwan Le Merrer, Rhicheek Patra, Jean-Ronan Vigouroux", "title": "Sequences, Items And Latent Links: Recommendation With Consumed Item Packs. (arXiv:1711.06100v2 [cs.IR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.06100", "type": "text/html"}], "timestampUsec": "1512969981619058", "comments": [], "summary": {"content": "<p>Recommenders personalize the web content by typically using collaborative \nfiltering to relate users (or items) based on explicit feedback, e.g., ratings. \nThe difficulty of collecting this feedback has recently motivated to consider \nimplicit feedback (e.g., item consumption along with the corresponding time). \n</p> \n<p>In this paper, we introduce the notion of consumed item pack (CIP) which \nenables to link users (or items) based on their implicit analogous consumption \nbehavior. Our proposal is generic, and we show that it captures three novel \nimplicit recommenders: a user-based (CIP-U), an item-based (CIP-I), and a word \nembedding-based (DEEPCIP), as well as a state-of-the-art technique using \nimplicit feedback (FISM). We show that our recommenders handle incremental \nupdates incorporating freshly consumed items. We demonstrate that all three \nrecommenders provide a recommendation quality that is competitive with \nstate-of-the-art ones, including one incorporating both explicit and implicit \nfeedback. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902938", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.06100"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Haoye Cai, Chunyan Bai, Yu-Wing Tai, Chi-Keung Tang", "title": "Deep Video Generation, Prediction and Completion of Human Action Sequences. (arXiv:1711.08682v3 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08682", "type": "text/html"}], "timestampUsec": "1512969981619057", "comments": [], "summary": {"content": "<p>Current deep learning results on video generation are limited while there are \nonly a few first results on video prediction and no relevant significant \nresults on video completion. This is due to the severe ill-posedness inherent \nin these three problems. In this paper, we focus on human action videos, and \npropose a general, two-stage deep framework to generate human action videos \nwith no constraints or arbitrary number of constraints, which uniformly address \nthe three problems: video generation given no input frames, video prediction \ngiven the first few frames, and video completion given the first and last \nframes. To make the problem tractable, in the first stage we train a deep \ngenerative model that generates a human pose sequence from random noise. In the \nsecond stage, a skeleton-to-image network is trained, which is used to generate \na human action video given the complete human pose sequence generated in the \nfirst stage. By introducing the two-stage strategy, we sidestep the original \nill-posed problems while producing for the first time high-quality video \ngeneration/prediction/completion results of much longer duration. We present \nquantitative and qualitative evaluation to show that our two-stage approach \noutperforms state-of-the-art methods in video generation, prediction and video \ncompletion. Our video result demonstration can be viewed at \nhttps://iamacewhite.github.io/supp/index.html \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902944", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08682"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513315289, "author": "Chen Huang, Chen Kong, Simon Lucey", "title": "CNNs are Globally Optimal Given Multi-Layer Support. (arXiv:1712.02501v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02501", "type": "text/html"}], "timestampUsec": "1512709730983976", "comments": [], "summary": {"content": "<p>Stochastic Gradient Descent (SGD) is the central workhorse for training \nmodern CNNs. Although giving impressive empirical performance it can be slow to \nconverge. In this paper we explore a novel strategy for training a CNN using an \nalternation strategy that offers substantial speedups during training. We make \nthe following contributions: (i) replace the ReLU non-linearity within a CNN \nwith positive hard-thresholding, (ii) reinterpret this non-linearity as a \nbinary state vector making the entire CNN linear if the multi-layer support is \nknown, and (iii) demonstrate that under certain conditions a global optima to \nthe CNN can be found through local descent. We then employ a novel alternation \nstrategy (between weights and support) for CNN training that leads to \nsubstantially faster convergence rates, nice theoretical properties, and \nachieving state of the art results across large scale datasets (e.g. ImageNet) \nas well as other standard benchmarks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7c7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02501"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Federico Raue, Andreas Dengel, Thomas M. Breuel, Marcus Liwicki", "title": "Symbol Grounding Association in Multimodal Sequences with Missing Elements. (arXiv:1511.04401v5 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1511.04401", "type": "text/html"}], "timestampUsec": "1512709730983975", "comments": [], "summary": {"content": "<p>In this paper, we extend a symbolic association framework for being able to \nhandle missing elements in multimodal sequences. The general scope of the work \nis the symbolic associations of object-word mappings as it happens in language \ndevelopment in infants. In other words, two different representations of the \nsame abstract concepts can associate in both directions. This scenario has been \nlong interested in Artificial Intelligence, Psychology, and Neuroscience. In \nthis work, we extend a recent approach for multimodal sequences (visual and \naudio) to also cope with missing elements in one or both modalities. Our method \nuses two parallel Long Short-Term Memories (LSTMs) with a learning rule based \non EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We \npropose to include an extra step for the combination with the max operation for \nexploiting the common elements between both sequences. The motivation behind is \nthat the combination acts as a condition selector for choosing the best \nrepresentation from both LSTMs. We evaluated the proposed extension in the \nfollowing scenarios: missing elements in one modality (visual or audio) and \nmissing elements in both modalities (visual and sound). The performance of our \nextension reaches better results than the original model and similar results to \nindividual LSTM trained in each modality. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7cb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1511.04401"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kieran Greer", "title": "New Ideas for Brain Modelling 3. (arXiv:1612.00369v6 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1612.00369", "type": "text/html"}], "timestampUsec": "1512709730983974", "comments": [], "summary": {"content": "<p>This paper considers a process for the creation and subsequent firing of \nsequences of neuronal patterns, as might be found in the human brain. The scale \nis one of larger patterns emerging from an ensemble mass, possibly through some \ntype of energy equation and a reduction procedure. The links between the \npatterns can be formed naturally, as a residual effect of the pattern creation \nitself. This paper follows-on closely from the earlier research, including two \nearlier papers in the series and uses the ideas of entropy and cohesion. With a \nsmall addition, it is possible to show how the inter-pattern links can be \ndetermined. A new compact Grid form of an earlier Counting Mechanism is also \ndemonstrated. It is possible to explain how a very basic repeating structure \ncan form the arbitrary patterns and activation sequences between them, and a \nkey question of how nodes synchronise may even be answerable. The paper \nfinishes with an implementation architecture, for the realisation and storage \nof knowledge and memory, as part of a general design, based on distributed \nneural components. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7d1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.00369"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andrea Soltoggio, Kenneth O. Stanley, Sebastian Risi", "title": "Born to Learn: the Inspiration, Progress, and Future of Evolved Plastic Artificial Neural Networks. (arXiv:1703.10371v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.10371", "type": "text/html"}], "timestampUsec": "1512709730983973", "comments": [], "summary": {"content": "<p>Biological plastic neural networks are systems of extraordinary computational \ncapabilities shaped by evolution, development, and lifetime learning. The \ninterplay of these elements leads to the emergence of adaptive behavior and \nintelligence. Inspired by such intricate natural phenomena, Evolved Plastic \nArtificial Neural Networks (EPANNs) use simulated evolution in-silico to breed \nplastic neural networks with a large variety of dynamics, architectures, and \nplasticity rules: these artificial systems are composed of inputs, outputs, and \nplastic components that change in response to experiences in an environment. \nThese systems may autonomously discover novel adaptive algorithms, and lead to \nhypotheses on the emergence of biological adaptation. EPANNs have seen \nconsiderable progress over the last two decades. Current scientific and \ntechnological advances in artificial neural networks are now setting the \nconditions for radically new approaches and results. In particular, the \nlimitations of hand-designed networks could be overcome by more flexible and \ninnovative solutions. This paper brings together a variety of inspiring ideas \nthat define the field of EPANNs. The main methods and results are reviewed. \nFinally, new opportunities and developments are presented. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7d6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.10371"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jesus Lago, Fjo De Ridder, Peter Vrancx, Bart De Schutter", "title": "Forecasting day-ahead electricity prices in Europe: the importance of considering market integration. (arXiv:1708.07061v3 [q-fin.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.07061", "type": "text/html"}], "timestampUsec": "1512709730983972", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a47d17b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a47d17b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Motivated by the increasing integration among electricity markets, in this \npaper we propose two different methods to incorporate market integration in \nelectricity price forecasting and to improve the predictive performance. First, \nwe propose a deep neural network that considers features from connected markets \nto improve the predictive accuracy in a local market. To measure the importance \nof these features, we propose a novel feature selection algorithm that, by \nusing Bayesian optimization and functional analysis of variance, evaluates the \neffect of the features on the algorithm performance. In addition, using market \nintegration, we propose a second model that, by simultaneously predicting \nprices from two markets, improves the forecasting accuracy even further. As a \ncase study, we consider the electricity market in Belgium and the improvements \nin forecasting accuracy when using various French electricity features. We show \nthat the two proposed models lead to improvements that are statistically \nsignificant. Particularly, due to market integration, the predictive accuracy \nis improved from 15.7% to 12.5% sMAPE (symmetric mean absolute percentage \nerror). In addition, we show that the proposed feature selection algorithm is \nable to perform a correct assessment, i.e. to discard the irrelevant features. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7db", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.07061"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Robin Tibor Schirrmeister, Lukas Gemein, Katharina Eggensperger, Frank Hutter, Tonio Ball", "title": "Deep learning with convolutional neural networks for decoding and visualization of EEG pathology. (arXiv:1708.08012v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.08012", "type": "text/html"}], "timestampUsec": "1512709730983971", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a4cb9b2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a4cb9b2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We apply convolutional neural networks (ConvNets) to the task of \ndistinguishing pathological from normal EEG recordings in the Temple University \nHospital EEG Abnormal Corpus. We use two basic, shallow and deep ConvNet \narchitectures recently shown to decode task-related information from EEG at \nleast as well as established algorithms designed for this purpose. In decoding \nEEG pathology, both ConvNets reached substantially better accuracies (about 6% \nbetter, ~85% vs. ~79%) than the only published result for this dataset, and \nwere still better when using only 1 minute of each recording for training and \nonly six seconds of each recording for testing. We used automated methods to \noptimize architectural hyperparameters and found intriguingly different ConvNet \narchitectures, e.g., with max pooling as the only nonlinearity. Visualizations \nof the ConvNet decoding behavior showed that they used spectral power changes \nin the delta (0-4 Hz) and theta (4-8 Hz) frequency range, possibly alongside \nother features, consistent with expectations derived from spectral analysis of \nthe EEG data and from the textual medical reports. Analysis of the textual \nmedical reports also highlighted the potential for accuracy increases by \nintegrating contextual information, such as the age of subjects. In summary, \nthe ConvNets and visualization techniques used in this study constitute a next \nstep towards clinically useful automated EEG diagnosis and establish a new \nbaseline for future work on this topic. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7e1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.08012"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Juncai Xu, Zhenzhong Shen, Qingwen Ren, Xin Xie, Zhengyu Yang", "title": "Geometric Semantic Genetic Programming Algorithm and Slump Prediction. (arXiv:1709.06114v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.06114", "type": "text/html"}], "timestampUsec": "1512709730983970", "comments": [], "summary": {"content": "<p>Research on the performance of recycled concrete as building material in the \ncurrent world is an important subject. Given the complex composition of \nrecycled concrete, conventional methods for forecasting slump scarcely obtain \nsatisfactory results. Based on theory of nonlinear prediction method, we \npropose a recycled concrete slump prediction model based on geometric semantic \ngenetic programming (GSGP) and combined it with recycled concrete features. \nTests show that the model can accurately predict the recycled concrete slump by \nusing the established prediction model to calculate the recycled concrete slump \nwith different mixing ratios in practical projects and by comparing the \npredicted values with the experimental values. By comparing the model with \nseveral other nonlinear prediction models, we can conclude that GSGP has higher \naccuracy and reliability than conventional methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7e5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.06114"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, Jaap Kamps", "title": "Avoiding Your Teacher's Mistakes: Training Neural Networks with Controlled Weak Supervision. (arXiv:1711.00313v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.00313", "type": "text/html"}], "timestampUsec": "1512709730983969", "comments": [], "summary": {"content": "<p>Training deep neural networks requires massive amounts of training data, but \nfor many tasks only limited labeled data is available. This makes weak \nsupervision attractive, using weak or noisy signals like the output of \nheuristic methods or user click-through data for training. In a semi-supervised \nsetting, we can use a large set of data with weak labels to pretrain a neural \nnetwork and then fine-tune the parameters with a small amount of data with true \nlabels. This feels intuitively sub-optimal as these two independent stages \nleave the model unaware about the varying label quality. What if we could \nsomehow inform the model about the label quality? In this paper, we propose a \nsemi-supervised learning method where we train two neural networks in a \nmulti-task fashion: a \"target network\" and a \"confidence network\". The target \nnetwork is optimized to perform a given task and is trained using a large set \nof unlabeled data that are weakly annotated. We propose to weight the gradient \nupdates to the target network using the scores provided by the second \nconfidence network, which is trained on a small amount of supervised data. Thus \nwe avoid that the weight updates computed from noisy labels harm the quality of \nthe target network model. We evaluate our learning strategy on two different \ntasks: document ranking and sentiment classification. The results demonstrate \nthat our approach not only enhances the performance compared to the baselines \nbut also speeds up the learning process from weak labels. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7eb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.00313"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jiajun Lu, Hussein Sibai, Evan Fabry", "title": "Adversarial Examples that Fool Detectors. (arXiv:1712.02494v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.02494", "type": "text/html"}], "timestampUsec": "1512709730983967", "comments": [], "summary": {"content": "<p>An adversarial example is an example that has been adjusted to produce a \nwrong label when presented to a system at test time. To date, adversarial \nexample constructions have been demonstrated for classifiers, but not for \ndetectors. If adversarial examples that could fool a detector exist, they could \nbe used to (for example) maliciously create security hazards on roads populated \nwith smart vehicles. In this paper, we demonstrate a construction that \nsuccessfully fools two standard detectors, Faster RCNN and YOLO. The existence \nof such examples is surprising, as attacking a classifier is very different \nfrom attacking a detector, and that the structure of detectors - which must \nsearch for their own bounding box, and which cannot estimate that box very \naccurately - makes it quite likely that adversarial patterns are strongly \ndisrupted. We show that our construction produces adversarial examples that \ngeneralize well across sequences digitally, even though large perturbations are \nneeded. We also show that our construction yields physical objects that are \nadversarial. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7f2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02494"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas", "title": "ChemNet: A Transferable and Generalizable Deep Neural Network for Small-Molecule Property Prediction. (arXiv:1712.02734v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02734", "type": "text/html"}], "timestampUsec": "1512709730983966", "comments": [], "summary": {"content": "<p>With access to large datasets, deep neural networks (DNN) have achieved \nhuman-level accuracy in image and speech recognition tasks. However, in \nchemistry, availability of large standardized and labelled datasets is scarce, \nand many chemical properties of research interest, chemical data is inherently \nsmall and fragmented. In this work, we explore transfer learning techniques in \nconjunction with the existing Chemception CNN model, to create a transferable \nand generalizable deep neural network for small-molecule property prediction. \nOur latest model, ChemNet learns in a semi-supervised manner from inexpensive \nlabels computed from the ChEMBL database. When fine-tuned to the Tox21, HIV and \nFreeSolv dataset, which are 3 separate chemical properties that ChemNet was not \noriginally trained on, we demonstrate that ChemNet exceeds the performance of \nexisting Chemception models and other contemporary DNN models. Furthermore, as \nChemNet has been pre-trained on a large diverse chemical database, it can be \nused as a general-purpose plug-and-play deep neural network for the prediction \nof novel small-molecule chemical properties. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02734"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lucas Bechberger, Kai-Uwe K&#xfc;hnberger", "title": "Measuring Relations Between Concepts In Conceptual Spaces. (arXiv:1707.02292v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.02292", "type": "text/html"}], "timestampUsec": "1512709730983965", "comments": [], "summary": {"content": "<p>The highly influential framework of conceptual spaces provides a geometric \nway of representing knowledge. Instances are represented by points in a \nhigh-dimensional space and concepts are represented by regions in this space. \nOur recent mathematical formalization of this framework is capable of \nrepresenting correlations between different domains in a geometric way. In this \npaper, we extend our formalization by providing quantitative mathematical \ndefinitions for the notions of concept size, subsethood, implication, \nsimilarity, and betweenness. This considerably increases the representational \npower of our formalization by introducing measurable ways of describing \nrelations between concepts. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c802", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.02292"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Dianmu Zhang, Blake Hannaford", "title": "IKBT: solving closed-form Inverse Kinematics with Behavior Tree. (arXiv:1711.05412v3 [cs.RO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05412", "type": "text/html"}], "timestampUsec": "1512709730983964", "comments": [], "summary": {"content": "<p>Serial robot arms have complicated kinematic equations which must be solved \nto write effective arm planning and control software (the Inverse Kinematics \nProblem). Existing software packages for inverse kinematics often rely on \nnumerical methods which have significant shortcomings. Here we report a new \nsymbolic inverse kinematics solver which overcomes the limitations of numerical \nmethods, and the shortcomings of previous symbolic software packages. We \nintegrate Behavior Trees, an execution planning framework previously used for \ncontrolling intelligent robot behavior, to organize the equation solving \nprocess, and a modular architecture for each solution technique. The system \nsuccessfully solved, generated a LaTex report, and generated a Python code \ntemplate for 18 out of 19 example robots of 4-6 DOF. The system is readily \nextensible, maintainable, and multi-platform with few dependencies. The \ncomplete package is available with a Modified BSD license on Github. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c807", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05412"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Georgios Theocharous, Zheng Wen, Yasin Abbasi-Yadkori, Nikos Vlassis", "title": "Posterior Sampling for Large Scale Reinforcement Learning. (arXiv:1711.07979v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07979", "type": "text/html"}], "timestampUsec": "1512709730983963", "comments": [], "summary": {"content": "<p>Posterior sampling for reinforcement learning (PSRL) is a popular algorithm \nfor learning to control an unknown Markov decision process (MDP). PSRL \nmaintains a distribution over MDP parameters and in an episodic fashion samples \nMDP parameters, computes the optimal policy for them and executes it. A special \ncase of PSRL is where at the end of each episode the MDP resets to the initial \nstate distribution. Extensions of this idea to general MDPs without state \nresetting has so far produced non-practical algorithms and in some cases buggy \ntheoretical analysis. This is due to the difficulty of analyzing regret under \nepisode switching schedules that depend on random variables of the true \nunderlying model. We propose a solution to this problem that involves using a \ndeterministic, model-independent episode switching schedule, and establish a \nBayes regret bound under mild assumptions. Our algorithm termed deterministic \nschedule PSRL (DS-PSRL) is efficient in terms of time, sample, and space \ncomplexity. Our result is more generally applicable to continuous state action \nproblems. We demonstrate how this algorithm is well suited for sequential \nrecommendation problems such as points of interest (POI). We derive a general \nprocedure for parameterizing the underlying MDPs, to create action condition \ndynamics from passive data, that do not contain actions. We prove that such \nparameterization satisfies the assumptions of our analysis. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c812", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07979"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1514178135, "author": "Lirong Xue, Samory Kpotufe", "title": "Achieving the time of $1$-NN, but the accuracy of $k$-NN. (arXiv:1712.02369v2 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02369", "type": "text/html"}], "timestampUsec": "1512709730983961", "comments": [], "summary": {"content": "<p>We propose a simple approach which, given distributed computing resources, \ncan nearly achieve the accuracy of $k$-NN prediction, while matching (or \nimproving) the faster prediction time of $1$-NN. The approach consists of \naggregating denoised $1$-NN predictors over a small number of distributed \nsubsamples. We show, both theoretically and experimentally, that small \nsubsample sizes suffice to attain similar performance as $k$-NN, without \nsacrificing the computational efficiency of $1$-NN. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c815", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02369"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Guodong Zhang, Shengyang Sun, David Duvenaud, Roger Grosse", "title": "Noisy Natural Gradient as Variational Inference. (arXiv:1712.02390v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.02390", "type": "text/html"}], "timestampUsec": "1512709730983960", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a4cbbe4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a4cbbe4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Combining the flexibility of deep learning with Bayesian uncertainty \nestimation has long been a goal in our field, and many modern approaches are \nbased on variational Bayes. Unfortunately, one is forced to choose between \noverly simplistic variational families (e.g. fully factorized) or expensive and \ncomplicated inference procedures. We show that natural gradient ascent with \nadaptive weight noise can be interpreted as fitting a variational posterior to \nmaximize the evidence lower bound (ELBO). This insight allows us to train full \ncovariance, fully factorized, and matrix variate Gaussian variational \nposteriors using noisy versions of natural gradient, Adam, and K-FAC, \nrespectively. On standard regression benchmarks, our noisy K-FAC algorithm \nmakes better predictions and matches HMC's predictive variances better than \nexisting methods. Its improved uncertainty estimates lead to more efficient \nexploration in the settings of active learning and intrinsic motivation for \nreinforcement learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c81e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02390"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Guo Yu, Jacob Bien", "title": "Estimating the error variance in a high-dimensional linear model. (arXiv:1712.02412v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.02412", "type": "text/html"}], "timestampUsec": "1512709730983959", "comments": [], "summary": {"content": "<p>The lasso has been studied extensively as a tool for estimating the \ncoefficient vector in the high-dimensional linear model; however, considerably \nless is known about estimating the error variance. Indeed, most well-known \ntheoretical properties of the lasso, including recent advances in selective \ninference with the lasso, are established under the assumption that the \nunderlying error variance is known. Yet the error variance in practice is, of \ncourse, unknown. In this paper, we propose the natural lasso estimator for the \nerror variance, which maximizes a penalized likelihood objective. A key aspect \nof the natural lasso is that the likelihood is expressed in terms of the \nnatural parameterization of the multiparameter exponential family of a Gaussian \nwith unknown mean and variance. The result is a remarkably simple estimator \nwith provably good performance in terms of mean squared error. These \ntheoretical results do not require placing any assumptions on the design matrix \nor the true regression coefficients. We also propose a companion estimator, \ncalled the organic lasso, which theoretically does not require tuning of the \nregularization parameter. Both estimators do well compared to preexisting \nmethods, especially in settings where successful recovery of the true support \nof the coefficient vector is hard. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c824", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02412"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lorenzo Boninsegna, Feliks N&#xfc;ske, Cecilia Clementi", "title": "Sparse learning of stochastic dynamic equations. (arXiv:1712.02432v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02432", "type": "text/html"}], "timestampUsec": "1512709730983958", "comments": [], "summary": {"content": "<p>With the rapid increase of available data for complex systems, there is great \ninterest in the extraction of physically relevant information from massive \ndatasets. Recently, a framework called Sparse Identification of Nonlinear \nDynamics (SINDy) has been introduced to identify the governing equations of \ndynamical systems from simulation data. In this study, we extend SINDy to \nstochastic dynamical systems, which are frequently used to model biophysical \nprocesses. We prove the asymptotic correctness of stochastics SINDy in the \ninfinite data limit, both in the original and projected variables. We discuss \nalgorithms to solve the sparse regression problem arising from the practical \nimplementation of SINDy, and show that cross validation is an essential tool to \ndetermine the right level of sparsity. We demonstrate the proposed methodology \non two test systems, namely, the diffusion in a one-dimensional potential, and \nthe projected dynamics of a two-dimensional diffusion process. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c829", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02432"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Farzaneh S. Fard, Thomas P. Trappenberg", "title": "A Novel Model for Arbitration between Planning and Habitual Control Systems. (arXiv:1712.02441v1 [cs.SY])", "alternate": [{"href": "http://arxiv.org/abs/1712.02441", "type": "text/html"}], "timestampUsec": "1512709730983957", "comments": [], "summary": {"content": "<p>It is well established that humans decision making and instrumental control \nuses multiple systems, some which use habitual action selection and some which \nrequire deliberate planning. Deliberate planning systems use predictions of \naction-outcomes using an internal model of the agent's environment, while \nhabitual action selection systems learn to automate by repeating previously \nrewarded actions. Habitual control is computationally efficient but may be \ninflexible in changing environments. Conversely, deliberate planning may be \ncomputationally expensive, but flexible in dynamic environments. This paper \nproposes a general architecture comprising both control paradigms by \nintroducing an arbitrator that controls which subsystem is used at any time. \nThis system is implemented for a target-reaching task with a simulated \ntwo-joint robotic arm that comprises a supervised internal model and deep \nreinforcement learning. Through permutation of target-reaching conditions, we \ndemonstrate that the proposed is capable of rapidly learning kinematics of the \nsystem without a priori knowledge, and is robust to (A) changing environmental \nreward and kinematics, and (B) occluded vision. The arbitrator model is \ncompared to exclusive deliberate planning with the internal model and exclusive \nhabitual control instances of the model. The results show how such a model can \nharness the benefits of both systems, using fast decisions in reliable \ncircumstances while optimizing performance in changing environments. In \naddition, the proposed model learns very fast. Finally, the system which \nincludes internal models is able to reach the target under the visual \nocclusion, while the pure habitual system is unable to operate sufficiently \nunder such conditions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c82e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02441"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yunpeng Li, Ivan Kiskin, Davide Zilli, Marianne Sinka, Henry Chan, Kathy Willis, Stephen Roberts", "title": "Cost-sensitive detection with variational autoencoders for environmental acoustic sensing. (arXiv:1712.02488v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02488", "type": "text/html"}], "timestampUsec": "1512709730983956", "comments": [], "summary": {"content": "<p>Environmental acoustic sensing involves the retrieval and processing of audio \nsignals to better understand our surroundings. While large-scale acoustic data \nmake manual analysis infeasible, they provide a suitable playground for machine \nlearning approaches. Most existing machine learning techniques developed for \nenvironmental acoustic sensing do not provide flexible control of the trade-off \nbetween the false positive rate and the false negative rate. This paper \npresents a cost-sensitive classification paradigm, in which the \nhyper-parameters of classifiers and the structure of variational autoencoders \nare selected in a principled Neyman-Pearson framework. We examine the \nperformance of the proposed approach using a dataset from the HumBug project \nwhich aims to detect the presence of mosquitoes using sound collected by simple \nembedded devices. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c834", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02488"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lucas Roberts, Leo Razoumov, Lin Su, Yuyang Wang", "title": "Gini-regularized Optimal Transport with an Application to Spatio-Temporal Forecasting. (arXiv:1712.02512v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02512", "type": "text/html"}], "timestampUsec": "1512709730983955", "comments": [], "summary": {"content": "<p>Rapidly growing product lines and services require a finer-granularity \nforecast that considers geographic locales. However the open question remains, \nhow to assess the quality of a spatio-temporal forecast? In this manuscript we \nintroduce a metric to evaluate spatio-temporal forecasts. This metric is based \non an Opti- mal Transport (OT) problem. The metric we propose is a constrained \nOT objec- tive function using the Gini impurity function as a regularizer. We \ndemonstrate through computer experiments both the qualitative and the \nquantitative charac- teristics of the Gini regularized OT problem. Moreover, we \nshow that the Gini regularized OT problem converges to the classical OT \nproblem, when the Gini regularized problem is considered as a function of \n{\\lambda}, the regularization parame-ter. The convergence to the classical OT \nsolution is faster than the state-of-the-art Entropic-regularized OT[Cuturi, \n2013] and results in a numerically more stable algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c83c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02512"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513315291, "author": "Fengshuo Zhang, Chao Gao", "title": "Convergence Rates of Variational Posterior Distributions. (arXiv:1712.02519v2 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02519", "type": "text/html"}], "timestampUsec": "1512709730983954", "comments": [], "summary": {"content": "<p>We study convergence rates of variational posterior distributions for \nnonparametric and high-dimensional inference. We formulate general conditions \non prior, likelihood, and variational class that characterize the convergence \nrates. Under similar \"prior mass and testing\" conditions considered in the \nliterature, the rate is found to be the sum of two terms. The first term stands \nfor the convergence rate of the true posterior distribution, and the second \nterm is contributed by the variational approximation error. For a class of \npriors that admit the structure of a mixture of product measures, we propose a \nnovel prior mass condition, under which the variational approximation error of \nthe generalized mean-field class is dominated by convergence rate of the true \nposterior. We demonstrate the applicability of our general results for various \nmodels, prior distributions and variational classes by deriving convergence \nrates of the corresponding variational posteriors. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c845", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02519"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jianqiao Wangni, Jingwei Zhuo, Jun Zhu", "title": "Learning Random Fourier Features by Hybrid Constrained Optimization. (arXiv:1712.02527v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02527", "type": "text/html"}], "timestampUsec": "1512709730983953", "comments": [], "summary": {"content": "<p>The kernel embedding algorithm is an important component for adapting kernel \nmethods to large datasets. Since the algorithm consumes a major computation \ncost in the testing phase, we propose a novel teacher-learner framework of \nlearning computation-efficient kernel embeddings from specific data. In the \nframework, the high-precision embeddings (teacher) transfer the data \ninformation to the computation-efficient kernel embeddings (learner). We \njointly select informative embedding functions and pursue an orthogonal \ntransformation between two embeddings. We propose a novel approach of \nconstrained variational expectation maximization (CVEM), where the alternate \ndirection method of multiplier (ADMM) is applied over a nonconvex domain in the \nmaximization step. We also propose two specific formulations based on the \nprevalent Random Fourier Feature (RFF), the masked and blocked version of \nComputation-Efficient RFF (CERF), by imposing a random binary mask or a block \nstructure on the transformation matrix. By empirical studies of several \napplications on different real-world datasets, we demonstrate that the CERF \nsignificantly improves the performance of kernel methods upon the RFF, under \ncertain arithmetic operation requirements, and suitable for structured matrix \nmultiplication in Fastfood type algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c84b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02527"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Carles Roger Riera Molina, Oriol Pujol Vila", "title": "Solving internal covariate shift in deep learning with linked neurons. (arXiv:1712.02609v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02609", "type": "text/html"}], "timestampUsec": "1512709730983952", "comments": [], "summary": {"content": "<p>This work proposes a novel solution to the problem of internal covariate \nshift and dying neurons using the concept of linked neurons. We define the \nneuron linkage in terms of two constraints: first, all neuron activations in \nthe linkage must have the same operating point. That is to say, all of them \nshare input weights. Secondly, a set of neurons is linked if and only if there \nis at least one member of the linkage that has a non-zero gradient in regard to \nthe input of the activation function. This means that for any input in the \nactivation function, there is at least one member of the linkage that operates \nin a non-flat and non-zero area. This simple change has profound implications \nin the network learning dynamics. In this article we explore the consequences \nof this proposal and show that by using this kind of units, internal covariate \nshift is implicitly solved. As a result of this, the use of linked neurons \nallows to train arbitrarily large networks without any architectural or \nalgorithmic trick, effectively removing the need of using re-normalization \nschemes such as Batch Normalization, which leads to halving the required \ntraining time. It also solves the problem of the need for standarized input \ndata. Results show that the units using the linkage not only do effectively \nsolve the aforementioned problems, but are also a competitive alternative with \nrespect to state-of-the-art with very promising results. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c853", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02609"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513247526, "author": "Beyza Ermis, Ali Taylan Cemgil", "title": "Differentially Private Variational Dropout. (arXiv:1712.02629v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02629", "type": "text/html"}], "timestampUsec": "1512709730983951", "comments": [], "summary": {"content": "<p>Deep neural networks with their large number of parameters are highly \nflexible learning systems. The high flexibility in such networks brings with \nsome serious problems such as overfitting, and regularization is used to \naddress this problem. A currently popular and effective regularization \ntechnique for controlling the overfitting is dropout. Often, large data \ncollections required for neural networks contain sensitive information such as \nthe medical histories of patients, and the privacy of the training data should \nbe protected. In this paper, we modify the recently proposed variational \ndropout technique which provided an elegant Bayesian interpretation to dropout, \nand show that the intrinsic noise in the variational dropout can be exploited \nto obtain a degree of differential privacy. The iterative nature of training \nneural networks presents a challenge for privacy-preserving estimation since \nmultiple iterations increase the amount of noise added. We overcome this by \nusing a relaxed notion of differential privacy, called concentrated \ndifferential privacy, which provides tighter estimates on the overall privacy \nloss. We demonstrate the accuracy of our privacy-preserving variational dropout \nalgorithm on benchmark datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1513247526, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c85a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02629"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alain Virouleau, Agathe Guilloux, St&#xe9;phane Ga&#xef;ffas, Malgorzata Bogdan", "title": "High-dimensional robust regression and outliers detection with SLOPE. (arXiv:1712.02640v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02640", "type": "text/html"}], "timestampUsec": "1512709730983950", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a4cbdb0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a4cbdb0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The problems of outliers detection and robust regression in a \nhigh-dimensional setting are fundamental in statistics, and have numerous \napplications. Following a recent set of works providing methods for \nsimultaneous robust regression and outliers detection, we consider in this \npaper a model of linear regression with individual intercepts, in a \nhigh-dimensional setting. We introduce a new procedure for simultaneous \nestimation of the linear regression coefficients and intercepts, using two \ndedicated sorted-$\\ell_1$ penalizations, also called SLOPE. We develop a \ncomplete theory for this problem: first, we provide sharp upper bounds on the \nstatistical estimation error of both the vector of individual intercepts and \nregression coefficients. Second, we give an asymptotic control on the False \nDiscovery Rate (FDR) and statistical power for support selection of the \nindividual intercepts. As a consequence, this paper is the first to introduce a \nprocedure with guaranteed FDR and statistical power control for outliers \ndetection under the mean-shift model. Numerical illustrations, with a \ncomparison to recent alternative approaches, are provided on both simulated and \nseveral real-world datasets. Experiments are conducted using an open-source \nsoftware written in Python and C++. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c85f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02640"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ga&#xeb;lle Loosli, Hattoibe Aboubacar", "title": "Using SVDD in SimpleMKL for 3D-Shapes Filtering. (arXiv:1712.02658v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02658", "type": "text/html"}], "timestampUsec": "1512709730983949", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a529f6b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a529f6b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper proposes the adaptation of Support Vector Data Description (SVDD) \nto the multiple kernel case (MK-SVDD), based on SimpleMKL. It also introduces a \nvariant called Slim-MK-SVDD that is able to produce a tighter frontier around \nthe data. For the sake of comparison, the equivalent methods are also developed \nfor One-Class SVM, known to be very similar to SVDD for certain shapes of \nkernels. \n</p> \n<p>Those algorithms are illustrated in the context of 3D-shapes filtering and \noutliers detection. For the 3D-shapes problem, the objective is to be able to \nselect a sub-category of 3D-shapes, each sub-category being learned with our \nalgorithm in order to create a filter. For outliers detection, we apply the \nproposed algorithms for unsupervised outliers detection as well as for the \nsupervised case. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c864", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02658"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andreas Svensson, Dave Zachariah, Thomas B. Sch&#xf6;n", "title": "Is My Model Flexible Enough? Information-Theoretic Model Check. (arXiv:1712.02675v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02675", "type": "text/html"}], "timestampUsec": "1512709730983948", "comments": [], "summary": {"content": "<p>The choice of model class is fundamental in statistical learning and system \nidentification, no matter whether the class is derived from physical principles \nor is a generic black-box. We develop a method to evaluate the specified model \nclass by assessing its capability of reproducing data that is similar to the \nobserved data record. This model check is based on the information-theoretic \nproperties of models viewed as data generators and is applicable to e.g. \nsequential data and nonlinear dynamical models. The method can be understood as \na specific two-sided posterior predictive test. We apply the \ninformation-theoretic model check to both synthetic and real data and compare \nit with a classical whiteness test. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c86d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02675"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chia-Yu Chen, Jungwook Choi, Daniel Brand, Ankur Agrawal, Wei Zhang, Kailash Gopalakrishnan", "title": "AdaComp : Adaptive Residual Gradient Compression for Data-Parallel Distributed Training. (arXiv:1712.02679v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.02679", "type": "text/html"}], "timestampUsec": "1512709730983947", "comments": [], "summary": {"content": "<p>Highly distributed training of Deep Neural Networks (DNNs) on future compute \nplatforms (offering 100 of TeraOps/s of computational capacity) is expected to \nbe severely communication constrained. To overcome this limitation, new \ngradient compression techniques are needed that are computationally friendly, \napplicable to a wide variety of layers seen in Deep Neural Networks and \nadaptable to variations in network architectures as well as their \nhyper-parameters. In this paper we introduce a novel technique - the Adaptive \nResidual Gradient Compression (AdaComp) scheme. AdaComp is based on localized \nselection of gradient residues and automatically tunes the compression rate \ndepending on local activity. We show excellent results on a wide spectrum of \nstate of the art Deep Learning models in multiple domains (vision, speech, \nlanguage), datasets (MNIST, CIFAR10, ImageNet, BN50, Shakespeare), optimizers \n(SGD with momentum, Adam) and network parameters (number of learners, \nminibatch-size etc.). Exploiting both sparsity and quantization, we demonstrate \nend-to-end compression rates of ~200X for fully-connected and recurrent layers, \nand ~40X for convolutional layers, without any noticeable degradation in model \naccuracies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c873", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02679"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Thomas Hehn, Fred A. Hamprecht", "title": "End-to-end Learning of Deterministic Decision Trees. (arXiv:1712.02743v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02743", "type": "text/html"}], "timestampUsec": "1512709730983946", "comments": [], "summary": {"content": "<p>Conventional decision trees have a number of favorable properties, including \ninterpretability, a small computational footprint and the ability to learn from \nlittle training data. However, they lack a key quality that has helped fuel the \ndeep learning revolution: that of being end-to-end trainable, and to learn from \nscratch those features that best allow to solve a given supervised learning \nproblem. Recent work (Kontschieder 2015) has addressed this deficit, but at the \ncost of losing a main attractive trait of decision trees: the fact that each \nsample is routed along a small subset of tree nodes only. We here propose a \nmodel and Expectation-Maximization training scheme for decision trees that are \nfully probabilistic at train time, but after a deterministic annealing process \nbecome deterministic at test time. We also analyze the learned oblique split \nparameters on image datasets and show that Neural Networks can be trained at \neach split node. In summary, we present the first end-to-end learning scheme \nfor deterministic decision trees and present results on par with or superior to \npublished standard oblique decision tree algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c879", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02743"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Abdolreza Mohammadi, Ernst C. Wit", "title": "BDgraph: An R Package for Bayesian Structure Learning in Graphical Models. (arXiv:1501.05108v5 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1501.05108", "type": "text/html"}], "timestampUsec": "1512709730983945", "comments": [], "summary": {"content": "<p>Graphical models provide powerful tools to uncover complicated patterns in \nmultivariate data and are commonly used in Bayesian statistics and machine \nlearning. In this paper, we introduce an R package BDgraph which performs \nBayesian structure learning for general undirected graphical models with either \ncontinuous or discrete variables. The package efficiently implements recent \nimprovements in the Bayesian literature. To speed up computations, the \ncomputationally intensive tasks have been implemented in C++ and interfaced \nwith R. In addition, the package contains several functions for simulation and \nvisualization, as well as two multivariate datasets taken from the literature \nand are used to describe the package capabilities. The paper includes a brief \noverview of the statistical methods which have been implemented in the package. \nThe main body of the paper explains how to use the package. Furthermore, we \nillustrate the package's functionality in both real and artificial examples, as \nwell as in an extensive simulation study. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c881", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1501.05108"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Peter D. Gr&#xfc;nwald, Nishant A. Mehta", "title": "Fast Rates for General Unbounded Loss Functions: from ERM to Generalized Bayes. (arXiv:1605.00252v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1605.00252", "type": "text/html"}], "timestampUsec": "1512709730983944", "comments": [], "summary": {"content": "<p>We present new excess risk bounds for general unbounded loss functions \nincluding log loss and squared loss, where the distribution of the losses may \nbe heavy-tailed. The bounds hold for general estimators, but they are optimized \nwhen applied to $\\eta$-generalized Bayesian, MDL, and ERM estimators. When \napplied with log loss, the bounds imply convergence rates for generalized \nBayesian inference under misspecification in terms of a generalization of the \nHellinger metric as long as the learning rate $\\eta$ is set correctly. For \ngeneral loss functions, our bounds rely on two separate conditions: the \n$v$-GRIP (generalized reversed information projection) conditions, which \ncontrol the lower tail of the excess loss; and the newly introduced witness \ncondition, which controls the upper tail. The parameter $v$ in the $v$-GRIP \nconditions determines the achievable rate and is akin to the exponent in the \nwell-known Tsybakov margin condition and the Bernstein condition for bounded \nlosses, which the $v$-GRIP conditions generalize; favorable $v$ in combination \nwith small model complexity leads to $\\tilde{O}(1/n)$ rates. The witness \ncondition allows us to connect the excess risk to an 'annealed' version \nthereof, by which we generalize several previous results connecting Hellinger \nand R\\'enyi divergence to KL divergence. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c887", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1605.00252"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Luca Ambrogioni, Eric Maris", "title": "Complex-valued Gaussian Process Regression for Time Series Analysis. (arXiv:1611.10073v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.10073", "type": "text/html"}], "timestampUsec": "1512709730983943", "comments": [], "summary": {"content": "<p>The construction of synthetic complex-valued signals from real-valued \nobservations is an important step in many time series analysis techniques. The \nmost widely used approach is based on the Hilbert transform, which maps the \nreal-valued signal into its quadrature component. In this paper, we define a \nprobabilistic generalization of this approach. We model the observable \nreal-valued signal as the real part of a latent complex-valued Gaussian \nprocess. In order to obtain the appropriate statistical relationship between \nits real and imaginary parts, we define two new classes of complex-valued \ncovariance functions. Through an analysis of simulated chirplets and stochastic \noscillations, we show that the resulting Gaussian process complex-valued signal \nprovides a better estimate of the instantaneous amplitude and frequency than \nthe established approaches. Furthermore, the complex-valued Gaussian process \nregression allows to incorporate prior information about the structure in \nsignal and noise and thereby to tailor the analysis to the features of the \nsignal. As a example, we analyze the non-stationary dynamics of brain \noscillations in the alpha band, as measured using magneto-encephalography. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c88b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.10073"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andee Kaplan, Daniel Nordman, Stephen Vardeman", "title": "Properties and Bayesian fitting of restricted Boltzmann machines. (arXiv:1612.01158v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1612.01158", "type": "text/html"}], "timestampUsec": "1512709730983942", "comments": [], "summary": {"content": "<p>A restricted Boltzmann machine (RBM) is an undirected graphical model \nconstructed for discrete or continuous random variables, with two layers, one \nhidden and one visible, and no conditional dependency within a layer. In recent \nyears, RBMs have risen to prominence due to their connection to deep learning. \nBy treating a hidden layer of one RBM as the visible layer in a second RBM, a \ndeep architecture can be created. RBMs are thought to thereby have the ability \nto encode very complex and rich structures in data, making them attractive for \nsupervised learning. However, the generative behavior of RBMs is largely \nunexplored. In this paper, we discuss the relationship between RBM parameter \nspecification in the binary case and model properties such as degeneracy, \ninstability and uninterpretability. We also describe the difficulties that \narise in likelihood-based and Bayes fitting of such (highly flexible) models, \nespecially as Gibbs sampling (quasi-Bayes) methods are often advocated for the \nRBM model structure. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c896", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.01158"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Martin Arjovsky, Soumith Chintala, L&#xe9;on Bottou", "title": "Wasserstein GAN. (arXiv:1701.07875v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1701.07875", "type": "text/html"}], "timestampUsec": "1512709730983941", "comments": [], "summary": {"content": "<p>We introduce a new algorithm named WGAN, an alternative to traditional GAN \ntraining. In this new model, we show that we can improve the stability of \nlearning, get rid of problems like mode collapse, and provide meaningful \nlearning curves useful for debugging and hyperparameter searches. Furthermore, \nwe show that the corresponding optimization problem is sound, and provide \nextensive theoretical work highlighting the deep connections to other distances \nbetween distributions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c8a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1701.07875"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Luca Ambrogioni, Eric Maris", "title": "Integral Transforms from Finite Data: An Application of Gaussian Process Regression to Fourier Analysis. (arXiv:1704.02828v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.02828", "type": "text/html"}], "timestampUsec": "1512709730983940", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a52a1c2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a52a1c2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Computing accurate estimates of the Fourier transform of analog signals from \ndiscrete data points is important in many fields of science and engineering. \nThe conventional approach of performing the discrete Fourier transform of the \ndata implicitly assumes periodicity and bandlimitedness of the signal. In this \npaper, we use Gaussian process regression to estimate the Fourier transform (or \nany other integral transform) without making these assumptions. This is \npossible because the posterior expectation of Gaussian process regression maps \na finite set of samples to a function defined on the whole real line, expressed \nas a linear combination of covariance functions. We estimate the covariance \nfunction from the data using an appropriately designed gradient ascent method \nthat constrains the solution to a linear combination of tractable kernel \nfunctions. This procedure results in a posterior expectation of the analog \nsignal whose Fourier transform can be obtained analytically by exploiting \nlinearity. Our simulations show that the new method leads to sharper and more \nprecise estimation of the spectral density both in noise-free and \nnoise-corrupted signals. We further validate the method in two real-world \napplications: the analysis of the yearly fluctuation in atmospheric CO2 level \nand the analysis of the spectral content of brain signals. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c8b8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.02828"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yossi Arjevani", "title": "Limitations on Variance-Reduction and Acceleration Schemes for Finite Sum Optimization. (arXiv:1706.01686v2 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.01686", "type": "text/html"}], "timestampUsec": "1512709730983939", "comments": [], "summary": {"content": "<p>We study the conditions under which one is able to efficiently apply \nvariance-reduction and acceleration schemes on finite sum optimization \nproblems. First, we show that, perhaps surprisingly, the finite sum structure \nby itself, is not sufficient for obtaining a complexity bound of \n$\\tilde{\\cO}((n+L/\\mu)\\ln(1/\\epsilon))$ for $L$-smooth and $\\mu$-strongly \nconvex individual functions - one must also know which individual function is \nbeing referred to by the oracle at each iteration. Next, we show that for a \nbroad class of first-order and coordinate-descent finite sum algorithms \n(including, e.g., SDCA, SVRG, SAG), it is not possible to get an `accelerated' \ncomplexity bound of $\\tilde{\\cO}((n+\\sqrt{n L/\\mu})\\ln(1/\\epsilon))$, unless \nthe strong convexity parameter is given explicitly. Lastly, we show that when \nthis class of algorithms is used for minimizing $L$-smooth and convex finite \nsums, the optimal complexity bound is $\\tilde{\\cO}(n+L/\\epsilon)$, assuming \nthat (on average) the same update rule is used in every iteration, and \n$\\tilde{\\cO}(n+\\sqrt{nL/\\epsilon})$, otherwise. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c8ca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.01686"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "R&#xe9;mi Gribonval (PANAMA), Gilles Blanchard, Nicolas Keriven (PANAMA, UR1), Yann Traonmilin (PANAMA)", "title": "Compressive Statistical Learning with Random Feature Moments. (arXiv:1706.07180v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.07180", "type": "text/html"}], "timestampUsec": "1512709730983938", "comments": [], "summary": {"content": "<p>We describe a general framework --compressive statistical learning-- for \nresource-efficient large-scale learning: the training collection is compressed \nin one pass into a low-dimensional sketch (a vector of random empirical \ngeneralized moments) that captures the information relevant to the considered \nlearning task. A near-minimizer of the risk is computed from the sketch through \nthe solution of a nonlinear least squares problem. We investigate sufficient \nsketch sizes to control the generalization error of this procedure. The \nframework is illustrated on compressive clustering, compressive Gaussian \nmixture Modeling with fixed known variance, and compressive PCA. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c8d9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.07180"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "George Barmpalias, Frank Stephan", "title": "Algorithmic learning of probability distributions from random data in the limit. (arXiv:1710.11303v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.11303", "type": "text/html"}], "timestampUsec": "1512709730983937", "comments": [], "summary": {"content": "<p>We study the problem of identifying a probability distribution for some given \nrandomly sampled data in the limit, in the context of algorithmic learning \ntheory as proposed recently by Vinanyi and Chater. We show that there exists a \ncomputable partial learner for the computable probability measures, while by \nBienvenu, Monin and Shen it is known that there is no computable learner for \nthe computable probability measures. Our main result is the characterization of \nthe oracles that compute explanatory learners for the computable (continuous) \nprobability measures as the high oracles. This provides an analogue of a \nwell-known result of Adleman and Blum in the context of learning computable \nprobability distributions. We also discuss related learning notions such as \nbehaviorally correct learning and orther variations of explanatory learning, in \nthe context of learning probability distributions from data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c8e4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.11303"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Marco Federici, Karen Ullrich, Max Welling", "title": "Improved Bayesian Compression. (arXiv:1711.06494v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.06494", "type": "text/html"}], "timestampUsec": "1512709730983936", "comments": [], "summary": {"content": "<p>Compression of Neural Networks (NN) has become a highly studied topic in \nrecent years. The main reason for this is the demand for industrial scale usage \nof NNs such as deploying them on mobile devices, storing them efficiently, \ntransmitting them via band-limited channels and most importantly doing \ninference at scale. In this work, we propose to join the Soft-Weight Sharing \nand Variational Dropout approaches that show strong results to define a new \nstate-of-the-art in terms of model compression. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c8f3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.06494"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Fabian Offert", "title": "\"I know it when I see it\". Visualization and Intuitive Interpretability. (arXiv:1711.08042v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08042", "type": "text/html"}], "timestampUsec": "1512709730983935", "comments": [], "summary": {"content": "<p>Most research on the interpretability of machine learning systems focuses on \nthe development of a more rigorous notion of interpretability. I suggest that a \nbetter understanding of the deficiencies of the intuitive notion of \ninterpretability is needed as well. I show that visualization enables but also \nimpedes intuitive interpretability, as it presupposes two levels of technical \npre-interpretation: dimensionality reduction and regularization. Furthermore, I \nargue that the use of positive concepts to emulate the distributed semantic \nstructure of machine learning models introduces a significant human bias into \nthe model. As a consequence, I suggest that, if intuitive interpretability is \nneeded, singular representations of internal model states should be avoided. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c8f7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08042"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nand Sharma", "title": "Single-trial P300 Classification using PCA with LDA, QDA and Neural Networks. (arXiv:1712.01977v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.01977", "type": "text/html"}], "timestampUsec": "1512624568381662", "comments": [], "summary": {"content": "<p>The P300 event-related potential (ERP), evoked in scalp-recorded \nelectroencephalography (EEG) by external stimuli, has proven to be a reliable \nresponse for controlling a BCI. The P300 component of an event related \npotential is thus widely used in brain-computer interfaces to translate the \nsubjects' intent by mere thoughts into commands to control artificial devices. \nThe main challenge in the classification of P300 trials in \nelectroencephalographic (EEG) data is the low signal-to-noise ratio (SNR) of \nthe P300 response. To overcome the low SNR of individual trials, it is common \npractice to average together many consecutive trials, which effectively \ndiminishes the random noise. Unfortunately, when more repeated trials are \nrequired for applications such as the P300 speller, the communication rate is \ngreatly reduced. This has resulted in a need for better methods to improve \nsingle-trial classification accuracy of P300 response. In this work, we use \nPrincipal Component Analysis (PCA) as a preprocessing method and use Linear \nDiscriminant Analysis (LDA)and neural networks for classification. The results \nshow that a combination of PCA with these methods provided as high as 13\\% \naccuracy gain for single-trial classification while using only 3 to 4 principal \ncomponents. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c37", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01977"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kyeong Soo Kim, Sanghyuk Lee, Kaizhu Huang", "title": "A Scalable Deep Neural Network Architecture for Multi-Building and Multi-Floor Indoor Localization Based on Wi-Fi Fingerprinting. (arXiv:1712.01990v1 [cs.NI])", "alternate": [{"href": "http://arxiv.org/abs/1712.01990", "type": "text/html"}], "timestampUsec": "1512624568381661", "comments": [], "summary": {"content": "<p>One of the key technologies for future large-scale location-aware services \ncovering a complex of multi-story buildings --- e.g., a big shopping mall and a \nuniversity campus --- is a scalable indoor localization technique. In this \npaper, we report the current status of our investigation on the use of deep \nneural networks (DNNs) for scalable building/floor classification and \nfloor-level position estimation based on Wi-Fi fingerprinting. Exploiting the \nhierarchical nature of the building/floor estimation and floor-level \ncoordinates estimation of a location, we propose a new DNN architecture \nconsisting of a stacked autoencoder for the reduction of feature space \ndimension and a feed-forward classifier for multi-label classification of \nbuilding/floor/location, on which the multi-building and multi-floor indoor \nlocalization system based on Wi-Fi fingerprinting is built. Experimental \nresults for the performance of building/floor estimation and floor-level \ncoordinates estimation of a given location demonstrate the feasibility of the \nproposed DNN-based indoor localization system, which can provide near \nstate-of-the-art performance using a single DNN, for the implementation with \nlower complexity and energy consumption at mobile devices. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c43", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01990"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mahdi Namazifar", "title": "Named Entity Sequence Classification. (arXiv:1712.02316v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.02316", "type": "text/html"}], "timestampUsec": "1512624568381660", "comments": [], "summary": {"content": "<p>Named Entity Recognition (NER) aims at locating and classifying named \nentities in text. In some use cases of NER, including cases where detected \nnamed entities are used in creating content recommendations, it is crucial to \nhave a reliable confidence level for the detected named entities. In this work \nwe study the problem of finding confidence levels for detected named entities. \nWe refer to this problem as Named Entity Sequence Classification (NESC). We \nframe NESC as a binary classification problem and we use NER as well as \nrecurrent neural networks to find the probability of candidate named entity is \na real named entity. We apply this approach to Tweet texts and we show how we \ncould find named entities with high confidence levels from Tweets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c49", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02316"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Omid Poursaeed, Isay Katsman, Bicheng Gao, Serge Belongie", "title": "Generative Adversarial Perturbations. (arXiv:1712.02328v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.02328", "type": "text/html"}], "timestampUsec": "1512624568381659", "comments": [], "summary": {"content": "<p>In this paper, we propose novel generative models for creating adversarial \nexamples, slightly perturbed images resembling natural images but maliciously \ncrafted to fool pre-trained models. We present trainable deep neural networks \nfor transforming images to adversarial perturbations. Our proposed models can \nproduce image-agnostic and image-dependent perturbations for both targeted and \nnon-targeted attacks. We also demonstrate that similar architectures can \nachieve impressive results in fooling classification and semantic segmentation \nmodels, obviating the need for hand-crafting attack methods for each task. \nUsing extensive experiments on challenging high-resolution datasets such as \nImageNet and Cityscapes, we show that our perturbations achieve high fooling \nrates with small perturbation norms. Moreover, our attacks are considerably \nfaster than current iterative methods at inference time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02328"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kyriaki Kalimeri, Mariano G. Beiro, Matteo Delfino, Robert Raleigh, Ciro Cattuto", "title": "Predicting Demographics, Moral Foundations, and Human Values from Digital Behaviors. (arXiv:1712.01930v1 [cs.CY])", "alternate": [{"href": "http://arxiv.org/abs/1712.01930", "type": "text/html"}], "timestampUsec": "1512624568381658", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a52a37e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a52a37e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Personal electronic devices such as smartphones give access to a broad range \nof behavioral signals that can be used to learn about the characteristics and \npreferences of individuals. In this study we explore the connection between \ndemographic and psychological attributes and digital records for a cohort of \n7,633 people, closely representative of the US population with respect to \ngender, age, geographical distribution, education, and income. We collected \nself-reported assessments on validated psychometric questionnaires based on \nboth the Moral Foundations and Basic Human Values theories, and combined this \ninformation with passively-collected multi-modal digital data from web browsing \nbehavior, smartphone usage and demographic data. Then, we designed a machine \nlearning framework to infer both the demographic and psychological attributes \nfrom the behavioral data. In a cross-validated setting, our model is found to \npredict demographic attributes with good accuracy (weighted AUC scores of 0.90 \nfor gender, 0.71 for age, 0.74 for ethnicity). Our weighted AUC scores for \nMoral Foundation attributes (0.66) and Human Values attributes (0.60) suggest \nthat accurate prediction of complex psychometric attributes is more challenging \nbut feasible. This connection might prove useful for designing personalized \nservices, communication strategies, and interventions, and can be used to \nsketch a portrait of people with similar worldviews. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c55", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01930"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yantian Zha, Yikang Li, Sriram Gopalakrishnan, Baoxin Li, Subbarao Kambhampati", "title": "Recognizing Plans by Learning Embeddings from Observed Action Distributions. (arXiv:1712.01949v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.01949", "type": "text/html"}], "timestampUsec": "1512624568381657", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a62e5fe\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a62e5fe&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Recent advances in visual activity recognition have raised the possibility of \napplications such as automated video surveillance. Effective approaches for \nsuch problems however require the ability to recognize the plans of the agents \nfrom video information. Although traditional plan recognition algorithms depend \non access to sophisticated domain models, one recent promising direction \ninvolves learning shallow models directly from the observed activity sequences, \nand using them to recognize/predict plans. One limitation of such approaches is \nthat they expect observed action sequences as training data. In many cases \ninvolving vision or sensing from raw data, there is considerably uncertainty \nabout the specific action at any given time point. The most we can expect in \nsuch cases is probabilistic information about the action at that point. The \ntraining data will then be sequences of such observed action distributions. In \nthis paper, we focus on doing effective plan recognition with such uncertain \nobservations. Our contribution is a novel extension of word vector embedding \ntechniques to directly handle such observation distributions as input. This \ninvolves computing embeddings by minimizing the distance between distributions \n(measured as KL-divergence). We will show that our approach has superior \nperformance when the perception error rate (PER) is higher, and competitive \nperformance when the PER is lower. We will also explore the possibility of \nusing importance sampling techniques to handle observed action distributions \nwith traditional word vector embeddings. We will show that although such \napproaches can give good recognition accuracy, they take significantly longer \ntraining time and their performance will degrade significantly at higher \nperception error rate. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c5c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01949"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N. Sainath, Zhifeng Chen, Rohit Prabhavalkar", "title": "An analysis of incorporating an external language model into a sequence-to-sequence model. (arXiv:1712.01996v1 [eess.AS])", "alternate": [{"href": "http://arxiv.org/abs/1712.01996", "type": "text/html"}], "timestampUsec": "1512624568381656", "comments": [], "summary": {"content": "<p>Attention-based sequence-to-sequence models for automatic speech recognition \njointly train an acoustic model, language model, and alignment mechanism. Thus, \nthe language model component is only trained on transcribed audio-text pairs. \nThis leads to the use of shallow fusion with an external language model at \ninference time. Shallow fusion refers to log-linear interpolation with a \nseparately trained language model at each step of the beam search. In this \nwork, we investigate the behavior of shallow fusion across a range of \nconditions: different types of language models, different decoding units, and \ndifferent tasks. On Google Voice Search, we demonstrate that the use of shallow \nfusion with a neural LM with wordpieces yields a 9.1% relative word error rate \nreduction (WERR) over our competitive attention-based sequence-to-sequence \nmodel, obviating the need for second-pass rescoring. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c65", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01996"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Garrett B. Goh, Nathan O. Hodas, Charles Siegel, Abhinav Vishnu", "title": "SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for Predicting Chemical Properties. (arXiv:1712.02034v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02034", "type": "text/html"}], "timestampUsec": "1512624568381655", "comments": [], "summary": {"content": "<p>Chemical databases store information in text representations, and the SMILES \nformat is a universal standard used in many cheminformatics software. Encoded \nin each SMILES string is structural information that can be used to predict \ncomplex chemical properties. In this work, we develop SMILES2Vec, a deep RNN \nthat automatically learns features from SMILES strings to predict chemical \nproperties, without the need for additional explicit chemical information, or \nthe \"grammar\" of how SMILES encode structural data. Using Bayesian optimization \nmethods to tune the network architecture, we show that an optimized SMILES2Vec \nmodel can serve as a general-purpose neural network for learning a range of \ndistinct chemical properties including toxicity, activity, solubility and \nsolvation energy, while outperforming contemporary MLP networks that uses \nengineered features. Furthermore, we demonstrate proof-of-concept of \ninterpretability by developing an explanation mask that localizes on the most \nimportant characters used in making a prediction. When tested on the solubility \ndataset, this localization identifies specific parts of a chemical that is \nconsistent with established first-principles knowledge of solubility with an \naccuracy of 88%, demonstrating that neural networks can learn technically \naccurate chemical concepts. The fact that SMILES2Vec validates established \nchemical facts, while providing state-of-the-art accuracy, makes it a potential \ntool for widespread adoption of interpretable deep learning by the chemistry \ncommunity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c6f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02034"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Borui Wang, Geoffrey Gordon", "title": "Learning General Latent-Variable Graphical Models with Predictive Belief Propagation and Hilbert Space Embeddings. (arXiv:1712.02046v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.02046", "type": "text/html"}], "timestampUsec": "1512624568381654", "comments": [], "summary": {"content": "<p>In this paper, we propose a new algorithm for learning general \nlatent-variable probabilistic graphical models using the techniques of \npredictive state representation, instrumental variable regression, and \nreproducing-kernel Hilbert space embeddings of distributions. Under this new \nlearning framework, we first convert latent-variable graphical models into \ncorresponding latent-variable junction trees, and then reduce the hard \nparameter learning problem into a pipeline of supervised learning problems, \nwhose results will then be used to perform predictive belief propagation over \nthe latent junction tree during the actual inference procedure. We then give \nproofs of our algorithm's correctness, and demonstrate its good performance in \nexperiments on one synthetic dataset and two real-world tasks from \ncomputational biology and computer vision - classifying DNA splice junctions \nand recognizing human actions in videos. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c7b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02046"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jinbae Im, Sungzoon Cho", "title": "Distance-based Self-Attention Network for Natural Language Inference. (arXiv:1712.02047v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.02047", "type": "text/html"}], "timestampUsec": "1512624568381653", "comments": [], "summary": {"content": "<p>Attention mechanism has been used as an ancillary means to help RNN or CNN. \nHowever, the Transformer (Vaswani et al., 2017) recently recorded the \nstate-of-the-art performance in machine translation with a dramatic reduction \nin training time by solely using attention. Motivated by the Transformer, \nDirectional Self Attention Network (Shen et al., 2017), a fully attention-based \nsentence encoder, was proposed. It showed good performance with various data by \nusing forward and backward directional information in a sentence. But in their \nstudy, not considered at all was the distance between words, an important \nfeature when learning the local dependency to help understand the context of \ninput text. We propose Distance-based Self-Attention Network, which considers \nthe word distance by using a simple distance mask in order to model the local \ndependency without losing the ability of modeling global dependency which \nattention has inherent. Our model shows good performance with NLI data, and it \nrecords the new state-of-the-art result with SNLI data. Additionally, we show \nthat our model has a strength in long sentences or documents. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c82", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02047"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Luca Pappalardo, Paolo Cintia, Dino Pedreschi, Fosca Giannotti, Albert-Laszlo Barabasi", "title": "Human Perception of Performance. (arXiv:1712.02224v1 [physics.soc-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.02224", "type": "text/html"}], "timestampUsec": "1512624568381652", "comments": [], "summary": {"content": "<p>Humans are routinely asked to evaluate the performance of other individuals, \nseparating success from failure and affecting outcomes from science to \neducation and sports. Yet, in many contexts, the metrics driving the human \nevaluation process remain unclear. Here we analyse a massive dataset capturing \nplayers' evaluations by human judges to explore human perception of performance \nin soccer, the world's most popular sport. We use machine learning to design an \nartificial judge which accurately reproduces human evaluation, allowing us to \ndemonstrate how human observers are biased towards diverse contextual features. \nBy investigating the structure of the artificial judge, we uncover the aspects \nof the players' behavior which attract the attention of human judges, \ndemonstrating that human evaluation is based on a noticeability heuristic where \nonly feature values far from the norm are considered to rate an individual's \nperformance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c89", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02224"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xuelin Qian, Yanwei Fu, Wenxuan Wang, Tao Xiang, Yang Wu, Yu-Gang Jiang, Xiangyang Xue", "title": "Pose-Normalized Image Generation for Person Re-identification. (arXiv:1712.02225v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.02225", "type": "text/html"}], "timestampUsec": "1512624568381651", "comments": [], "summary": {"content": "<p>Person Re-identification (re-id) faces two major challenges: the lack of \ncross-view paired training data and learning discriminative identity-sensitive \nand view-invariant features in the presence of large pose variations. In this \nwork, we address both problems by proposing a novel deep person image \ngeneration model for synthesizing realistic person images conditional on pose. \nThe model is based on a generative adversarial network (GAN) and used \nspecifically for pose normalization in re-id, thus termed pose-normalization \nGAN (PN-GAN). With the synthesized images, we can learn a new type of deep \nre-id feature free of the influence of pose variations. We show that this \nfeature is strong on its own and highly complementary to features learned with \nthe original images. Importantly, we now have a model that generalizes to any \nnew re-id dataset without the need for collecting any training data for model \nfine-tuning, thus making a deep re-id model truly scalable. Extensive \nexperiments on five benchmarks show that our model outperforms the \nstate-of-the-art models, often significantly. In particular, the features \nlearned on Market-1501 can achieve a Rank-1 accuracy of 68.67% on VIPeR without \nany model fine-tuning, beating almost all existing models fine-tuned on the \ndataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c94", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02225"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Daniel Golovin, Andreas Krause", "title": "Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization. (arXiv:1003.3967v5 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1003.3967", "type": "text/html"}], "timestampUsec": "1512624568381650", "comments": [], "summary": {"content": "<p>Solving stochastic optimization problems under partial observability, where \none needs to adaptively make decisions with uncertain outcomes, is a \nfundamental but notoriously difficult challenge. In this paper, we introduce \nthe concept of adaptive submodularity, generalizing submodular set functions to \nadaptive policies. We prove that if a problem satisfies this property, a simple \nadaptive greedy algorithm is guaranteed to be competitive with the optimal \npolicy. In addition to providing performance guarantees for both stochastic \nmaximization and coverage, adaptive submodularity can be exploited to \ndrastically speed up the greedy algorithm by using lazy evaluations. We \nillustrate the usefulness of the concept by giving several examples of adaptive \nsubmodular objectives arising in diverse applications including sensor \nplacement, viral marketing and active learning. Proving adaptive submodularity \nfor these problems allows us to recover existing results in these applications \nas special cases, improve approximation guarantees and handle natural \ngeneralizations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c9c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1003.3967"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhaohan Daniel Guo, Philip S. Thomas, Emma Brunskill", "title": "Using Options and Covariance Testing for Long Horizon Off-Policy Policy Evaluation. (arXiv:1703.03453v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.03453", "type": "text/html"}], "timestampUsec": "1512624568381649", "comments": [], "summary": {"content": "<p>Evaluating a policy by deploying it in the real world can be risky and \ncostly. Off-policy policy evaluation (OPE) algorithms use historical data \ncollected from running a previous policy to evaluate a new policy, which \nprovides a means for evaluating a policy without requiring it to ever be \ndeployed. Importance sampling is a popular OPE method because it is robust to \npartial observability and works with continuous states and actions. However, \nthe amount of historical data required by importance sampling can scale \nexponentially with the horizon of the problem: the number of sequential \ndecisions that are made. We propose using policies over temporally extended \nactions, called options, and show that combining these policies with importance \nsampling can significantly improve performance for long-horizon problems. In \naddition, we can take advantage of special cases that arise due to \noptions-based policies to further improve the performance of importance \nsampling. We further generalize these special cases to a general covariance \ntesting rule that can be used to decide which weights to drop in an IS \nestimate, and derive a new IS algorithm called Incremental Importance Sampling \nthat can provide significantly more accurate estimates for a broad class of \ndomains. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7ca7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.03453"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ting-Hao &#x27;Kenneth&#x27; Huang, Yun-Nung Chen, Jeffrey P. Bigham", "title": "Real-time On-Demand Crowd-powered Entity Extraction. (arXiv:1704.03627v2 [cs.HC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.03627", "type": "text/html"}], "timestampUsec": "1512624568381648", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a62e828\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a62e828&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Output-agreement mechanisms such as ESP Game have been widely used in human \ncomputation to obtain reliable human-generated labels. In this paper, we argue \nthat a \"time-limited\" output-agreement mechanism can be used to create a fast \nand robust crowd-powered component in interactive systems, particularly \ndialogue systems, to extract key information from user utterances on the fly. \nOur experiments on Amazon Mechanical Turk using the Airline Travel Information \nSystem (ATIS) dataset showed that the proposed approach achieves high-quality \nresults with an average response time shorter than 9 seconds. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.03627"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations. (arXiv:1704.04683v5 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.04683", "type": "text/html"}], "timestampUsec": "1512624568381647", "comments": [], "summary": {"content": "<p>We present RACE, a new dataset for benchmark evaluation of methods in the \nreading comprehension task. Collected from the English exams for middle and \nhigh school Chinese students in the age range between 12 to 18, RACE consists \nof near 28,000 passages and near 100,000 questions generated by human experts \n(English instructors), and covers a variety of topics which are carefully \ndesigned for evaluating the students' ability in understanding and reasoning. \nIn particular, the proportion of questions that requires reasoning is much \nlarger in RACE than that in other benchmark datasets for reading comprehension, \nand there is a significant gap between the performance of the state-of-the-art \nmodels (43%) and the ceiling human performance (95%). We hope this new dataset \ncan serve as a valuable resource for research and evaluation in machine \ncomprehension. The dataset is freely available at \n<a href=\"http://www.cs.cmu.edu/~glai1/data/race/\">this http URL</a> and the code is available at \nhttps://github.com/qizhex/RACE_AR_baselines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.04683"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Riccardo Polvara, Massimiliano Patacchiola, Sanjay Sharma, Jian Wan, Andrew Manning, Robert Sutton, Angelo Cangelosi", "title": "Autonomous Quadrotor Landing using Deep Reinforcement Learning. (arXiv:1709.03339v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.03339", "type": "text/html"}], "timestampUsec": "1512624568381646", "comments": [], "summary": {"content": "<p>Landing an unmanned aerial vehicle (UAV) on a ground marker is an open \nproblem despite the effort of the research community. Previous attempts mostly \nfocused on the analysis of hand-crafted geometric features and the use of \nexternal sensors in order to allow the vehicle to approach the land-pad. In \nthis article, we propose a method based on deep reinforcement learning that \nonly requires low-resolution images taken from a down-looking camera in order \nto identify the position of the marker and land the UAV on it. The proposed \napproach is based on a hierarchy of Deep Q-Networks (DQNs) used as high-level \ncontrol policy for the navigation toward the marker. We implemented different \ntechnical solutions, such as the combination of vanilla and double DQNs trained \nusing a partitioned buffer replay.The results show that policies trained on \nuniform textures can accomplish autonomous landing on a large variety of \nsimulated environments. The overall performance is comparable with a \nstate-of-the-art algorithm and human pilots. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cbb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.03339"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Behzad Tabibian, Utkarsh Upadhyay, Abir De, Ali Zarezade, Bernhard Schoelkopf, Manuel Gomez-Rodriguez", "title": "Optimizing Human Learning. (arXiv:1712.01856v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01856", "type": "text/html"}], "timestampUsec": "1512624568381644", "comments": [], "summary": {"content": "<p>Spaced repetition is a technique for efficient memorization which uses \nrepeated, spaced review of content to improve long-term retention. Can we find \nthe optimal reviewing schedule to maximize the benefits of spaced repetition? \nIn this paper, we introduce a novel, flexible representation of spaced \nrepetition using the framework of marked temporal point processes and then \naddress the above question as an optimal control problem for stochastic \ndifferential equations with jumps. For two well-known human memory models, we \nshow that the optimal reviewing schedule is given by the recall probability of \nthe content to be learned. As a result, we can then develop a simple, scalable \nonline algorithm, Memorize, to sample the optimal reviewing times. Experiments \non both synthetic and real data gathered from Duolingo, a popular \nlanguage-learning online platform, show that our algorithm may be able to help \nlearners memorize more effectively than alternatives. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cbf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01856"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tara N. Sainath, Rohit Prabhavalkar, Shankar Kumar, Seungji Lee, Anjuli Kannan, David Rybach, Vlad Schogol, Patrick Nguyen, Bo Li, Yonghui Wu, Zhifeng Chen, Chung-Cheng Chiu", "title": "No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models. (arXiv:1712.01864v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.01864", "type": "text/html"}], "timestampUsec": "1512624568381643", "comments": [], "summary": {"content": "<p>For decades, context-dependent phonemes have been the dominant sub-word unit \nfor conventional acoustic modeling systems. This status quo has begun to be \nchallenged recently by end-to-end models which seek to combine acoustic, \npronunciation, and language model components into a single neural network. Such \nsystems, which typically predict graphemes or words, simplify the recognition \nprocess since they remove the need for a separate expert-curated pronunciation \nlexicon to map from phoneme-based units to words. However, there has been \nlittle previous work comparing phoneme-based versus grapheme-based sub-word \nunits in the end-to-end modeling framework, to determine whether the gains from \nsuch approaches are primarily due to the new probabilistic model, or from the \njoint learning of the various components with grapheme-based units. \n</p> \n<p>In this work, we conduct detailed experiments which are aimed at quantifying \nthe value of phoneme-based pronunciation lexica in the context of end-to-end \nmodels. We examine phoneme-based end-to-end models, which are contrasted \nagainst grapheme-based ones on a large vocabulary English Voice-search task, \nwhere we find that graphemes do indeed outperform phonemes. We also compare \ngrapheme and phoneme-based approaches on a multi-dialect English task, which \nonce again confirm the superiority of graphemes, greatly simplifying the system \nfor recognizing multiple dialects. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cc7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01864"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally", "title": "Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training. (arXiv:1712.01887v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01887", "type": "text/html"}], "timestampUsec": "1512624568381642", "comments": [], "summary": {"content": "<p>Large-scale distributed training requires significant communication bandwidth \nfor gradient exchange that limits the scalability of multi-node training, and \nrequires expensive high-bandwidth network infrastructure. The situation gets \neven worse with distributed training on mobile devices (federated learning), \nwhich suffers from higher latency, lower throughput, and intermittent poor \nconnections. In this paper, we find 99.9% of the gradient exchange in \ndistributed SGD is redundant, and propose Deep Gradient Compression (DGC) to \ngreatly reduce the communication bandwidth. To preserve accuracy during \ncompression, DGC employs four methods: momentum correction, local gradient \nclipping, momentum factor masking, and warm-up training. We have applied Deep \nGradient Compression to image classification, speech recognition, and language \nmodeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and \nLibrispeech Corpus. On these scenarios, Deep Gradient Compression achieves a \ngradient compression ratio from 270x to 600x without losing accuracy, cutting \nthe gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from \n488MB to 0.74MB. Deep gradient compression enables large-scale distributed \ntraining on inexpensive commodity 1Gbps Ethernet and facilitates distributed \ntraining on mobile. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cd1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01887"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gilles Blanchard, Oleksandr Zadorozhnyi", "title": "Concentration of weakly dependent Banach-valued sums and applications to kernel learning methods. (arXiv:1712.01934v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01934", "type": "text/html"}], "timestampUsec": "1512624568381641", "comments": [], "summary": {"content": "<p>We obtain a new Bernstein-type inequality for sums of Banach-valued random \nvariables satisfying a weak dependence assumption of general type and under \ncertain smoothness assumptions of the underlying Banach norm. We use this \ninequality in order to investigate in asymptotical regime the error upper \nbounds for the broad family of spectral regularization methods for reproducing \nkernel decision rules, when trained on a sample coming from a $\\tau-$mixing \nprocess. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cdd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01934"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sujayam Saha, Adityanand Guntuboyina", "title": "On the nonparametric maximum likelihood estimator for Gaussian location mixture densities with application to Gaussian denoising. (arXiv:1712.02009v1 [math.ST])", "alternate": [{"href": "http://arxiv.org/abs/1712.02009", "type": "text/html"}], "timestampUsec": "1512624568381640", "comments": [], "summary": {"content": "<p>We study the Nonparametric Maximum Likelihood Estimator (NPMLE) for \nestimating Gaussian location mixture densities in $d$-dimensions from \nindependent observations. Unlike usual likelihood-based methods for fitting \nmixtures, NPMLEs are based on convex optimization. We prove finite sample \nresults on the Hellinger accuracy of every NPMLE. Our results imply, in \nparticular, that every NPMLE achieves near parametric risk (up to logarithmic \nmultiplicative factors) when the true density is a discrete Gaussian mixture \nwithout any prior information on the number of mixture components. NPMLEs can \nnaturally be used to yield empirical Bayes estimates of the Oracle Bayes \nestimator in the Gaussian denoising problem. We prove bounds for the accuracy \nof the empirical Bayes estimate as an approximation to the Oracle Bayes \nestimator. Here our results imply that the empirical Bayes estimator performs \nat nearly the optimal level (up to logarithmic multiplicative factors) for \ndenoising in clustering situations without any prior knowledge of the number of \nclusters. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cf1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02009"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aditya Devarakonda, Maxim Naumov, Michael Garland", "title": "AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks. (arXiv:1712.02029v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.02029", "type": "text/html"}], "timestampUsec": "1512624568381639", "comments": [], "summary": {"content": "<p>Training deep neural networks with Stochastic Gradient Descent, or its \nvariants, requires careful choice of both learning rate and batch size. While \nsmaller batch sizes generally converge in fewer training epochs, larger batch \nsizes offer more parallelism and hence better computational efficiency. We have \ndeveloped a new training approach that, rather than statically choosing a \nsingle batch size for all epochs, adaptively increases the batch size during \nthe training process. Our method delivers the convergence rate of small batch \nsizes while achieving performance similar to large batch sizes. We analyse our \napproach using the standard AlexNet, ResNet, and VGG networks operating on the \npopular CIFAR-10, CIFAR-100, and ImageNet datasets. Our results demonstrate \nthat learning with adaptive batch sizes can improve performance by factors of \nup to 6.25 on 4 NVIDIA Tesla P100 GPUs while changing accuracy by less than 1% \nrelative to training with fixed batch sizes. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cf7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02029"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David Barmherzig, Ju Sun", "title": "A Local Analysis of Block Coordinate Descent for Gaussian Phase Retrieval. (arXiv:1712.02083v1 [cs.IT])", "alternate": [{"href": "http://arxiv.org/abs/1712.02083", "type": "text/html"}], "timestampUsec": "1512624568381638", "comments": [], "summary": {"content": "<p>While convergence of the Alternating Direction Method of Multipliers (ADMM) \non convex problems is well studied, convergence on nonconvex problems is only \npartially understood. In this paper, we consider the Gaussian phase retrieval \nproblem, formulated as a linear constrained optimization problem with a \nbiconvex objective. The particular structure allows for a novel application of \nthe ADMM. It can be shown that the dual variable is zero at the global \nminimizer. This motivates the analysis of a block coordinate descent algorithm, \nwhich is equivalent to the ADMM with the dual variable fixed to be zero. We \nshow that the block coordinate descent algorithm converges to the global \nminimizer at a linear rate, when starting from a deterministically achievable \ninitialization point. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02083"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sebastian Stabinger, Antonio Rodriguez-Sanchez", "title": "Guided Labeling using Convolutional Neural Networks. (arXiv:1712.02154v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.02154", "type": "text/html"}], "timestampUsec": "1512624568381637", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a62ea6a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a62ea6a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Over the last couple of years, deep learning and especially convolutional \nneural networks have become one of the work horses of computer vision. One \nlimiting factor for the applicability of supervised deep learning to more areas \nis the need for large, manually labeled datasets. In this paper we propose an \neasy to implement method we call guided labeling, which automatically \ndetermines which samples from an unlabeled dataset should be labeled. We show \nthat using this procedure, the amount of samples that need to be labeled is \nreduced considerably in comparison to labeling images arbitrarily. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d06", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02154"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512709735, "author": "Chaopeng Shen", "title": "A trans-disciplinary review of deep learning research for water resources scientists. (arXiv:1712.02162v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02162", "type": "text/html"}], "timestampUsec": "1512624568381636", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a694ee4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a694ee4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep learning (DL), a new-generation artificial neural network research, has \nmade profound strides in recent years. This review paper is intended to provide \nwater resources scientists with a simple technical overview, trans-disciplinary \nprogress update, and potentially inspirations about DL. Effective \narchitectures, more accessible data, advances in regularization, and new \ncomputing power enabled the success of DL. A trans-disciplinary review reveals \nthat DL is rapidly transforming myriad scientific disciplines including \nhigh-energy physics, astronomy, chemistry, genomics and remote sensing, where \nsystematic DL toolkits, innovative customizations, and sub-disciplines have \nemerged. However, with a few exceptions, its adoption in hydrology has so far \nbeen gradual. The literature suggests that novel regularization techniques can \neffectively prevent high-capacity deep networks from overfitting. As a result, \nin most scientific disciplines, DL models demonstrated superior predictive and \ngeneralization performance to conventional methods. Meanwhile, less noticed is \nthat DL may also serve as a scientific exploratory tool. A new area termed \"AI \nneuroscience\", has been born. This budding sub-discipline is accumulating a \nsignificant body of work, e.g., distilling knowledge obtained in DL networks to \ninterpretable models, attributing decisions to inputs via back-propagation of \nrelevance, or visualization of activations. These methods are designed to \ninterpret the decision process of deep networks and derive insights. While \nscientists so far have mostly been using customized, ad-hoc methods for \ninterpretation, vast opportunities await for DL to propel advancement in water \nscience. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d14", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02162"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alejandro Murua, Ranjan Maitra", "title": "Fast spatial inference in the homogeneous Ising model. (arXiv:1712.02195v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.02195", "type": "text/html"}], "timestampUsec": "1512624568381635", "comments": [], "summary": {"content": "<p>The Ising model is important in statistical modeling and inference in many \napplications, however its normalizing constant, mean number of active vertices \nand mean spin interaction are intractable. We provide accurate approximations \nthat make it possible to calculate these quantities numerically. Simulation \nstudies indicate good performance when compared to Markov Chain Monte Carlo \nmethods and at a tiny fraction of the time. The methodology is also used to \nperform Bayesian inference in a functional Magnetic Resonance Imaging \nactivation detection experiment. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d1f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02195"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nicolas Thiebaut, Antoine Simoulin, Karl Neuberger, Issam Ibnoushein, Nicolas Bousquet, Nathalie Reix, S&#xe9;bastien Moli&#xe8;re, Carole Mathelin", "title": "An innovative solution for breast cancer textual big data analysis. (arXiv:1712.02259v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02259", "type": "text/html"}], "timestampUsec": "1512624568381634", "comments": [], "summary": {"content": "<p>The digitalization of stored information in hospitals now allows for the \nexploitation of medical data in text format, as electronic health records \n(EHRs), initially gathered for other purposes than epidemiology. Manual search \nand analysis operations on such data become tedious. In recent years, the use \nof natural language processing (NLP) tools was highlighted to automatize the \nextraction of information contained in EHRs, structure it and perform \nstatistical analysis on this structured information. The main difficulties with \nthe existing approaches is the requirement of synonyms or ontology \ndictionaries, that are mostly available in English only and do not include \nlocal or custom notations. In this work, a team composed of oncologists as \ndomain experts and data scientists develop a custom NLP-based system to process \nand structure textual clinical reports of patients suffering from breast \ncancer. The tool relies on the combination of standard text mining techniques \nand an advanced synonym detection method. It allows for a global analysis by \nretrieval of indicators such as medical history, tumor characteristics, \ntherapeutic responses, recurrences and prognosis. The versatility of the method \nallows to obtain easily new indicators, thus opening up the way for \nretrospective studies with a substantial reduction of the amount of manual \nwork. With no need for biomedical annotators or pre-defined ontologies, this \nlanguage-agnostic method reached an good extraction accuracy for several \nconcepts of interest, according to a comparison with a manually structured \nfile, without requiring any existing corpus with local or new notations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d2d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02259"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiaoyong Pan, Junchi Yan", "title": "Attention based convolutional neural network for predicting RNA-protein binding sites. (arXiv:1712.02270v1 [q-bio.GN])", "alternate": [{"href": "http://arxiv.org/abs/1712.02270", "type": "text/html"}], "timestampUsec": "1512624568381633", "comments": [], "summary": {"content": "<p>RNA-binding proteins (RBPs) play crucial roles in many biological processes, \ne.g. gene regulation. Computational identification of RBP binding sites on RNAs \nare urgently needed. In particular, RBPs bind to RNAs by recognizing sequence \nmotifs. Thus, fast locating those motifs on RNA sequences is crucial and \ntime-efficient for determining whether the RNAs interact with the RBPs or not. \nIn this study, we present an attention based convolutional neural network, \niDeepA, to predict RNA-protein binding sites from raw RNA sequences. We first \nencode RNA sequences into one-hot encoding. Next, we design a deep learning \nmodel with a convolutional neural network (CNN) and an attention mechanism, \nwhich automatically search for important positions, e.g. binding motifs, to \nlearn discriminant high-level features for predicting RBP binding sites. We \nevaluate iDeepA on publicly gold-standard RBP binding sites derived from \nCLIP-seq data. The results demonstrate iDeepA achieves comparable performance \nwith other state-of-the-art methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d3a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02270"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Victor Veitch, Ekansh Sharma, Zacharie Naulet, Daniel M. Roy", "title": "Exchangeable modelling of relational data: checking sparsity, train-test splitting, and sparse exchangeable Poisson matrix factorization. (arXiv:1712.02311v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02311", "type": "text/html"}], "timestampUsec": "1512624568381632", "comments": [], "summary": {"content": "<p>A variety of machine learning tasks---e.g., matrix factorization, topic \nmodelling, and feature allocation---can be viewed as learning the parameters of \na probability distribution over bipartite graphs. Recently, a new class of \nmodels for networks, the sparse exchangeable graphs, have been introduced to \nresolve some important pathologies of traditional approaches to statistical \nnetwork modelling; most notably, the inability to model sparsity (in the \nasymptotic sense). The present paper explains some practical insights arising \nfrom this work. We first show how to check if sparsity is relevant for \nmodelling a given (fixed size) dataset by using network subsampling to identify \na simple signature of sparsity. We discuss the implications of the (sparse) \nexchangeable subsampling theory for test-train dataset splitting; we argue \ncommon approaches can lead to biased results, and we propose a principled \nalternative. Finally, we study sparse exchangeable Poisson matrix factorization \nas a worked example. In particular, we show how to adapt mean field variational \ninference to the sparse exchangeable setting, allowing us to scale inference to \nhuge datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d42", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02311"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tatjana Chavdarova, Fran&#xe7;ois Fleuret", "title": "SGAN: An Alternative Training of Generative Adversarial Networks. (arXiv:1712.02330v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02330", "type": "text/html"}], "timestampUsec": "1512624568381631", "comments": [], "summary": {"content": "<p>The Generative Adversarial Networks (GANs) have demonstrated impressive \nperformance for data synthesis, and are now used in a wide range of computer \nvision tasks. In spite of this success, they gained a reputation for being \ndifficult to train, what results in a time-consuming and human-involved \ndevelopment process to use them. \n</p> \n<p>We consider an alternative training process, named SGAN, in which several \nadversarial \"local\" pairs of networks are trained independently so that a \n\"global\" supervising pair of networks can be trained against them. The goal is \nto train the global pair with the corresponding ensemble opponent for improved \nperformances in terms of mode coverage. This approach aims at increasing the \nchances that learning will not stop for the global pair, preventing both to be \ntrapped in an unsatisfactory local minimum, or to face oscillations often \nobserved in practice. To guarantee the latter, the global pair never affects \nthe local ones. \n</p> \n<p>The rules of SGAN training are thus as follows: the global generator and \ndiscriminator are trained using the local discriminators and generators, \nrespectively, whereas the local networks are trained with their fixed local \nopponent. \n</p> \n<p>Experimental results on both toy and real-world problems demonstrate that \nthis approach outperforms standard training in terms of better mitigating mode \ncollapse, stability while converging and that it surprisingly, increases the \nconvergence speed as well. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d4c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02330"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Dmitrii Marin, Meng Tang, Ismail Ben Ayed, Yuri Boykov", "title": "Kernel clustering: density biases and solutions. (arXiv:1705.05950v5 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.05950", "type": "text/html"}], "timestampUsec": "1512624568381630", "comments": [], "summary": {"content": "<p>Kernel methods are popular in clustering due to their generality and \ndiscriminating power. However, we show that many kernel clustering criteria \nhave density biases theoretically explaining some practically significant \nartifacts empirically observed in the past. For example, we provide conditions \nand formally prove the density mode isolation bias in kernel K-means for a \ncommon class of kernels. We call it Breiman's bias due to its similarity to the \nhistogram mode isolation previously discovered by Breiman in decision tree \nlearning with Gini impurity. We also extend our analysis to other popular \nkernel clustering methods, e.g. average/normalized cut or dominant sets, where \ndensity biases can take different forms. For example, splitting isolated points \nby cut-based criteria is essentially the sparsest subset bias, which is the \nopposite of the density mode bias. Our findings suggest that a principled \nsolution for density biases in kernel clustering should directly address data \ninhomogeneity. We show that density equalization can be implicitly achieved \nusing either locally adaptive weights or locally adaptive kernels. Moreover, \ndensity equalization makes many popular kernel clustering objectives \nequivalent. Our synthetic and real data experiments illustrate density biases \nand proposed solutions. We anticipate that theoretical understanding of kernel \nclustering limitations and their principled solutions will be important for a \nbroad spectrum of data analysis applications across the disciplines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d52", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.05950"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andrew J. Wagenmaker, Brian E. Moore, Raj Rao Nadakuditi", "title": "Robust Photometric Stereo via Dictionary Learning. (arXiv:1710.08873v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.08873", "type": "text/html"}], "timestampUsec": "1512624568381629", "comments": [], "summary": {"content": "<p>Photometric stereo is a method that seeks to reconstruct the normal vectors \nof an object from a set of images of the object illuminated under different \nlight sources. While effective in some situations, classical photometric stereo \nrelies on a diffuse surface model that cannot handle objects with complex \nreflectance patterns, and it is sensitive to non-idealities in the images. In \nthis work, we propose a novel approach to photometric stereo that relies on \ndictionary learning to produce robust normal vector reconstructions. \nSpecifically, we develop three formulations for applying dictionary learning to \nphotometric stereo. We propose a preprocessing step that utilizes dictionary \nlearning to denoise the images. We also present a model that applies dictionary \nlearning to regularize and reconstruct the normal vectors from the images under \nthe classic Lambertian reflectance model. Finally, we generalize the latter \nmodel to explicitly model non-Lambertian objects. We investigate all three \napproaches through extensive experimentation on synthetic and real benchmark \ndatasets and observe state-of-the-art performance compared to existing robust \nphotometric stereo methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d5c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.08873"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, Michael I. Jordan", "title": "Stochastic Cubic Regularization for Fast Nonconvex Optimization. (arXiv:1711.02838v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.02838", "type": "text/html"}], "timestampUsec": "1512624568381628", "comments": [], "summary": {"content": "<p>This paper proposes a stochastic variant of a classic algorithm---the \ncubic-regularized Newton method [Nesterov and Polyak 2006]. The proposed \nalgorithm efficiently escapes saddle points and finds approximate local minima \nfor general smooth, nonconvex functions in only \n$\\mathcal{\\tilde{O}}(\\epsilon^{-3.5})$ stochastic gradient and stochastic \nHessian-vector product evaluations. The latter can be computed as efficiently \nas stochastic gradients. This improves upon the \n$\\mathcal{\\tilde{O}}(\\epsilon^{-4})$ rate of stochastic gradient descent. Our \nrate matches the best-known result for finding local minima without requiring \nany delicate acceleration or variance-reduction techniques. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d64", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.02838"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Vince Lyzinski, Keith Levin, Carey E. Priebe", "title": "On consistent vertex nomination schemes. (arXiv:1711.05610v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05610", "type": "text/html"}], "timestampUsec": "1512624568381627", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a69522f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a69522f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Given a vertex of interest in a network $G_1$, the vertex nomination problem \nseeks to find the corresponding vertex of interest (if it exists) in a second \nnetwork $G_2$. Although the vertex nomination problem and related tasks have \nattracted much attention in the machine learning literature, with applications \nto social and biological networks, the framework has so far been confined to a \ncomparatively small class of network models, and the concept of statistically \nconsistent vertex nomination schemes has been only shallowly explored. In this \npaper, we extend the vertex nomination problem to a very general statistical \nmodel of graphs. Further, drawing inspiration from the long-established \nclassification framework in the pattern recognition literature, we provide \ndefinitions for the key notions of Bayes optimality and consistency in our \nextended vertex nomination framework, including a derivation of the Bayes \noptimal vertex nomination scheme. In addition, we prove that no universally \nconsistent vertex nomination schemes exist. Illustrative examples are provided \nthroughout. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d6e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05610"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yunpeng Li, Davide Zilli, Henry Chan, Ivan Kiskin, Marianne Sinka, Stephen Roberts, Kathy Willis", "title": "Mosquito detection with low-cost smartphones: data acquisition for malaria research. (arXiv:1711.06346v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.06346", "type": "text/html"}], "timestampUsec": "1512624568381626", "comments": [], "summary": {"content": "<p>Mosquitoes are a major vector for malaria, causing hundreds of thousands of \ndeaths in the developing world each year. Not only is the prevention of \nmosquito bites of paramount importance to the reduction of malaria transmission \ncases, but understanding in more forensic detail the interplay between malaria, \nmosquito vectors, vegetation, standing water and human populations is crucial \nto the deployment of more effective interventions. Typically the presence and \ndetection of malaria-vectoring mosquitoes is only quantified by hand-operated \ninsect traps or signified by the diagnosis of malaria. If we are to gather \ntimely, large-scale data to improve this situation, we need to automate the \nprocess of mosquito detection and classification as much as possible. In this \npaper, we present a candidate mobile sensing system that acts as both a \nportable early warning device and an automatic acoustic data acquisition \npipeline to help fuel scientific inquiry and policy. The machine learning \nalgorithm that powers the mobile system achieves excellent off-line \nmulti-species detection performance while remaining computationally efficient. \nFurther, we have conducted preliminary live mosquito detection tests using \nlow-cost mobile phones and achieved promising results. The deployment of this \nsystem for field usage in Southeast Asia and Africa is planned in the near \nfuture. In order to accelerate processing of field recordings and labelling of \ncollected data, we employ a citizen science platform in conjunction with \nautomated methods, the former implemented using the Zooniverse platform, \nallowing crowdsourcing on a grand scale. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d78", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.06346"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan, Xilin Chen", "title": "Arbitrary Facial Attribute Editing: Only Change What You Want. (arXiv:1711.10678v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10678", "type": "text/html"}], "timestampUsec": "1512624568381625", "comments": [], "summary": {"content": "<p>Facial attribute editing aims to modify either single or multiple attributes \non a face image. Since it is practically infeasible to collect images with \narbitrarily specified attributes for each person, the generative adversarial \nnet (GAN) and the encoder-decoder architecture are usually incorporated to \nhandle this task. With the encoder-decoder architecture, arbitrary attribute \nediting can then be conducted by decoding the latent representation of the face \nimage conditioned on the specified attributes. A few existing methods attempt \nto establish attribute-independent latent representation for arbitrarily \nchanging the attributes. However, since the attributes portray the \ncharacteristics of the face image, the attribute-independent constraint on the \nlatent representation is excessive. Such constraint may result in information \nloss and unexpected distortion on the generated images (e.g. over-smoothing), \nespecially for those identifiable attributes such as gender, race etc. Instead \nof imposing the attribute-independent constraint on the latent representation, \nwe introduce an attribute classification constraint on the generated image, \njust requiring the correct change of the attributes. Meanwhile, reconstruction \nlearning is introduced in order to guarantee the preservation of all other \nattribute-excluding details on the generated image, and adversarial learning is \nemployed for visually realistic generation. Moreover, our method can be \nnaturally extended to attribute intensity manipulation. Experiments on the \nCelebA dataset show that our method outperforms the state-of-the-arts on \ngenerating realistic attribute editing results with facial details well \npreserved. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d83", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10678"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Joon Kyung Kim, Vikas Chandra, Hadi Esmaeilzadeh", "title": "Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks. (arXiv:1712.01507v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.01507", "type": "text/html"}], "timestampUsec": "1512537801473962", "comments": [], "summary": {"content": "<p>Hardware acceleration of Deep Neural Networks (DNNs) aims to tame their \nenormous compute intensity. Fully realizing the potential of acceleration in \nthis domain requires understanding and leveraging algorithmic properties of \nDNNs. This paper builds upon the algorithmic insight that bitwidth of \noperations in DNNs can be reduced without compromising their accuracy. However, \nto prevent accuracy loss, the bitwidth varies significantly across DNNs and it \nmay even be adjusted for each layer individually. Thus, a fixed-bitwidth \naccelerator would either offer limited benefits to accommodate the worst-case \nbitwidth, or inevitably lead to a degradation in final accuracy. To alleviate \nthese deficiencies, this work introduces dynamic bit-level fusion/decomposition \nas a new dimension in the design of DNN accelerators. We explore this dimension \nby designing Bit Fusion, a bit-flexible accelerator, that constitutes an array \nof bit-level processing elements that dynamically fuse to match the bitwidth of \nindividual DNN layers. This flexibility in the architecture minimizes the \ncomputation and the communication at the finest granularity possible with no \nloss in accuracy. We evaluate the benefits of Bit Fusion using eight real-world \nfeed-forward and recurrent DNNs. The proposed microarchitecture is implemented \nin Verilog and synthesized in 45 nm technology. Using the synthesis results and \ncycle accurate simulation, we compare the benefits of Bit Fusion to two \nstate-of-the-art DNN accelerators, Eyeriss and Stripes. In the same area, \nfrequency, and technology node, Bit Fusion offers 4.3x speedup and 9.6x energy \nsavings over Eyeriss. Bit Fusion provides 2.4x speedup and 4.1x energy \nreduction over Stripes at 45 nm node when Bit Fusion area and frequency are set \nto those of Stripes. Compared to Jetson-TX2, Bit Fusion offers 4.3x speedup and \nalmost matches the performance of TitanX, which is 4.6x faster than TX2. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394e91", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01507"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pierre-Yves Oudeyer (Flowers)", "title": "Autonomous development and learning in artificial intelligence and robotics: Scaling up deep learning to human--like learning. (arXiv:1712.01626v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.01626", "type": "text/html"}], "timestampUsec": "1512537801473961", "comments": [], "summary": {"content": "<p>Autonomous lifelong development and learning is a fundamental capability of \nhumans, differentiating them from current deep learning systems. However, other \nbranches of artificial intelligence have designed crucial ingredients towards \nautonomous learning: curiosity and intrinsic motivation, social learning and \nnatural interaction with peers, and embodiment. These mechanisms guide \nexploration and autonomous choice of goals, and integrating them with deep \nlearning opens stimulating perspectives. Deep learning (DL) approaches made \ngreat advances in artificial intelligence, but are still far away from human \nlearning. As argued convincingly by Lake et al., differences include human \ncapabilities to learn causal models of the world from very little data, \nleveraging compositional representations and priors like intuitive physics and \npsychology. However, there are other fundamental differences between current DL \nsystems and human learning, as well as technical ingredients to fill this gap, \nthat are either superficially, or not adequately, discussed by Lake et al. \nThese fundamental mechanisms relate to autonomous development and learning. \nThey are bound to play a central role in artificial intelligence in the future. \nCurrent DL systems require engineers to manually specify a task-specific \nobjective function for every new task, and learn through off-line processing of \nlarge training databases. On the contrary, humans learn autonomously open-ended \nrepertoires of skills, deciding for themselves which goals to pursue or value, \nand which skills to explore, driven by intrinsic motivation/curiosity and \nsocial learning through natural interaction with peers. Such learning processes \nare incremental, online, and progressive. Human child development involves a \nprogressive increase of complexity in a curriculum of learning where skills are \nexplored, acquired, and built on each other, through particular ordering and \ntiming. Finally, human learning happens in the physical world, and through \nbodily and physical experimentation, under severe constraints on energy, time, \nand computational resources. In the two last decades, the field of \nDevelopmental and Cognitive Robotics (Cangelosi and Schlesinger, 2015, Asada et \nal., 2009), in strong interaction with developmental psychology and \nneuroscience, has achieved significant advances in computational \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394e9d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01626"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hojjat Salehinejad, Shahrokh Valaee, Tim Dowdell, Errol Colak, Joseph Barfett", "title": "Generalization of Deep Neural Networks for Chest Pathology Classification in X-Rays Using Generative Adversarial Networks. (arXiv:1712.01636v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01636", "type": "text/html"}], "timestampUsec": "1512537801473960", "comments": [], "summary": {"content": "<p>Medical datasets are often highly imbalanced, over representing common \nmedical problems, and sparsely representing rare problems. We propose \nsimulation of pathology in images to overcome the above limitations. Using \nchest Xrays as a model medical image, we implement a generative adversarial \nnetwork (GAN) to create artificial images based upon a modest sized labeled \ndataset. We employ a combination of real and artificial images to train a deep \nconvolutional neural network (DCNN) to detect pathology across five classes of \ndisease. We furthermore demonstrate that augmenting the original imbalanced \ndataset with GAN generated images improves performance of chest pathology \nclassification using the proposed DCNN in comparison to the same DCNN trained \nwith the original dataset alone. This improved performance is largely \nattributed to balancing of the dataset using GAN generated images, where image \nclasses that are lacking in example images are preferentially augmented. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394ea3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01636"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Maciej J. Mrowinski, Piotr Fronczak, Agata Fronczak, Marcel Ausloos, Olgica Nedic", "title": "Artificial intelligence in peer review: How can evolutionary computation support journal editors?. (arXiv:1712.01682v1 [cs.DL])", "alternate": [{"href": "http://arxiv.org/abs/1712.01682", "type": "text/html"}], "timestampUsec": "1512537801473959", "comments": [], "summary": {"content": "<p>With the volume of manuscripts submitted for publication growing every year, \nthe deficiencies of peer review (e.g. long review times) are becoming more \napparent. Editorial strategies, sets of guidelines designed to speed up the \nprocess and reduce editors workloads, are treated as trade secrets by \npublishing houses and are not shared publicly. To improve the effectiveness of \ntheir strategies, editors in small publishing groups are faced with undertaking \nan iterative trial-and-error approach. We show that Cartesian Genetic \nProgramming, a nature-inspired evolutionary algorithm, can dramatically improve \neditorial strategies. The artificially evolved strategy reduced the duration of \nthe peer review process by 30%, without increasing the pool of reviewers (in \ncomparison to a typical human-developed strategy). Evolutionary computation has \ntypically been used in technological processes or biological ecosystems. Our \nresults demonstrate that genetic programs can improve real-world social systems \nthat are usually much harder to understand and control than physical systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394eab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01682"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wellington Pinheiro dos Santos, Francisco Marcos de Assis, Ricardo Emmanuel de Souza, Priscilla B. Mendes, Henrique S. S. Monteiro, Havana Diogo Alves", "title": "Fuzzy-Based Dialectical Non-Supervised Image Classification and Clustering. (arXiv:1712.01694v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01694", "type": "text/html"}], "timestampUsec": "1512537801473958", "comments": [], "summary": {"content": "<p>The materialist dialectical method is a philosophical investigative method to \nanalyze aspects of reality. These aspects are viewed as complex processes \ncomposed by basic units named poles, which interact with each other. Dialectics \nhas experienced considerable progress in the 19th century, with Hegel's \ndialectics and, in the 20th century, with the works of Marx, Engels, and \nGramsci, in Philosophy and Economics. The movement of poles through their \ncontradictions is viewed as a dynamic process with intertwined phases of \nevolution and revolutionary crisis. In order to build a computational process \nbased on dialectics, the interaction between poles can be modeled using fuzzy \nmembership functions. Based on this assumption, we introduce the Objective \nDialectical Classifier (ODC), a non-supervised map for classification based on \nmaterialist dialectics and designed as an extension of fuzzy c-means \nclassifier. As a case study, we used ODC to classify 181 magnetic resonance \nsynthetic multispectral images composed by proton density, $T_1$- and \n$T_2$-weighted synthetic brain images. Comparing ODC to k-means, fuzzy c-means, \nand Kohonen's self-organized maps, concerning with image fidelity indexes as \nestimatives of quantization distortion, we proved that ODC can reach almost the \nsame quantization performance as optimal non-supervised classifiers like \nKohonen's self-organized maps. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394eae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01694"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Higor Neto Lima, Wellington Pinheiro dos Santos, M&#xea;user Jorge Silva Valen&#xe7;a", "title": "Triagem virtual de imagens de imuno-histoqu\\'imica usando redes neurais artificiais e espectro de padr\\~oes. (arXiv:1712.01695v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01695", "type": "text/html"}], "timestampUsec": "1512537801473957", "comments": [], "summary": {"content": "<p>The importance of organizing medical images according to their nature, \napplication and relevance is increasing. Furhermore, a previous selection of \nmedical images can be useful to accelerate the task of analysis by \npathologists. Herein this work we propose an image classifier to integrate a \nCBIR (Content-Based Image Retrieval) selection system. This classifier is based \non pattern spectra and neural networks. Feature selection is performed using \npattern spectra and principal component analysis, whilst image classification \nis based on multilayer perceptrons and a composition of self-organizing maps \nand learning vector quantization. These methods were applied for content \nselection of immunohistochemical images of placenta and newdeads lungs. Results \ndemonstrated that this approach can reach reasonable classification \nperformance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394ebb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01695"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wellington Pinheiro dos Santos, Francisco Marcos de Assis, Ricardo Emmanuel de Souza, Pl&#xed;nio Batista dos Santos Filho, Fernando Buarque de Lima Neto", "title": "Dialectical Multispectral Classification of Diffusion-Weighted Magnetic Resonance Images as an Alternative to Apparent Diffusion Coefficients Maps to Perform Anatomical Analysis. (arXiv:1712.01697v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01697", "type": "text/html"}], "timestampUsec": "1512537801473956", "comments": [], "summary": {"content": "<p>Multispectral image analysis is a relatively promising field of research with \napplications in several areas, such as medical imaging and satellite \nmonitoring. A considerable number of current methods of analysis are based on \nparametric statistics. Alternatively, some methods in Computational \nIntelligence are inspired by biology and other sciences. Here we claim that \nPhilosophy can be also considered as a source of inspiration. This work \nproposes the Objective Dialectical Method (ODM): a method for classification \nbased on the Philosophy of Praxis. ODM is instrumental in assembling evolvable \nmathematical tools to analyze multispectral images. In the case study described \nin this paper, multispectral images are composed of diffusion-weighted (DW) \nmagnetic resonance (MR) images. The results are compared to ground-truth images \nproduced by polynomial networks using a morphological similarity index. The \nclassification results are used to improve the usual analysis of the apparent \ndiffusion coefficient map. Such results proved that gray and white matter can \nbe distinguished in DW-MR multispectral analysis and, consequently, DW-MR \nimages can also be used to furnish anatomical information. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394ec8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01697"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Manuele Rusci, Lukas Cavigelli, Luca Benini", "title": "Design Automation for Binarized Neural Networks: A Quantum Leap Opportunity?. (arXiv:1712.01743v1 [cs.OH])", "alternate": [{"href": "http://arxiv.org/abs/1712.01743", "type": "text/html"}], "timestampUsec": "1512537801473955", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a69551b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a69551b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Design automation in general, and in particular logic synthesis, can play a \nkey role in enabling the design of application-specific Binarized Neural \nNetworks (BNN). This paper presents the hardware design and synthesis of a \npurely combinational BNN for ultra-low power near-sensor processing. We \nleverage the major opportunities raised by BNN models, which consist mostly of \nlogical bit-wise operations and integer counting and comparisons, for pushing \nultra-low power deep learning circuits close to the sensor and coupling it with \nbinarized mixed-signal image sensor data. We analyze area, power and energy \nmetrics of BNNs synthesized as combinational networks. Our synthesis results in \nGlobalFoundries 22nm SOI technology shows a silicon area of 2.61mm2 for \nimplementing a combinational BNN with 32x32 binary input sensor receptive field \nand weight parameters fixed at design time. This is 2.2x smaller than a \nsynthesized network with re-configurable parameters. With respect to other \ncomparable techniques for deep learning near-sensor processing, our approach \nfeatures a 10x higher energy efficiency. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394ecd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01743"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1516686009, "author": "Patrick McClure, Nikolaus Kriegeskorte", "title": "Robustly representing uncertainty in deep neural networks through sampling. (arXiv:1611.01639v7 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.01639", "type": "text/html"}], "timestampUsec": "1512537801473954", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a6e5e88\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a6e5e88&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>As deep neural networks (DNNs) are applied to increasingly challenging \nproblems, they will need to be able to represent their own uncertainty. \nModeling uncertainty is one of the key features of Bayesian methods. Using \nBernoulli dropout with sampling at prediction time has recently been proposed \nas an efficient and well performing variational inference method for DNNs. \nHowever, sampling from other multiplicative noise based variational \ndistributions has not been investigated in depth. We evaluated Bayesian DNNs \ntrained with Bernoulli or Gaussian multiplicative masking of either the units \n(dropout) or the weights (dropconnect). We tested the calibration of the \nprobabilistic predictions of Bayesian convolutional neural networks (CNNs) on \nMNIST and CIFAR-10. Sampling at prediction time increased the calibration of \nthe DNNs' probabalistic predictions. Sampling weights, whether Gaussian or \nBernoulli, led to more robust representation of uncertainty compared to \nsampling of units. However, using either Gaussian or Bernoulli dropout led to \nincreased test set classification accuracy. Based on these findings we used \nboth Bernoulli dropout and Gaussian dropconnect concurrently, which we show \napproximates the use of a spike-and-slab variational distribution without \nincreasing the number of learned parameters. We found that spike-and-slab \nsampling had higher test set performance than Gaussian dropconnect and more \nrobustly represented its uncertainty compared to Bernoulli dropout. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1516686008, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394edb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.01639"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yan Duan, Marcin Andrychowicz, Bradly C. Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, Wojciech Zaremba", "title": "One-Shot Imitation Learning. (arXiv:1703.07326v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.07326", "type": "text/html"}], "timestampUsec": "1512537801473953", "comments": [], "summary": {"content": "<p>Imitation learning has been commonly applied to solve different tasks in \nisolation. This usually requires either careful feature engineering, or a \nsignificant number of samples. This is far from what we desire: ideally, robots \nshould be able to learn from very few demonstrations of any given task, and \ninstantly generalize to new situations of the same task, without requiring \ntask-specific engineering. In this paper, we propose a meta-learning framework \nfor achieving such capability, which we call one-shot imitation learning. \n</p> \n<p>Specifically, we consider the setting where there is a very large set of \ntasks, and each task has many instantiations. For example, a task could be to \nstack all blocks on a table into a single tower, another task could be to place \nall blocks on a table into two-block towers, etc. In each case, different \ninstances of the task would consist of different sets of blocks with different \ninitial states. At training time, our algorithm is presented with pairs of \ndemonstrations for a subset of all tasks. A neural net is trained that takes as \ninput one demonstration and the current state (which initially is the initial \nstate of the other demonstration of the pair), and outputs an action with the \ngoal that the resulting sequence of states and actions matches as closely as \npossible with the second demonstration. At test time, a demonstration of a \nsingle instance of a new task is presented, and the neural net is expected to \nperform well on new instances of this new task. The use of soft attention \nallows the model to generalize to conditions and tasks unseen in the training \ndata. We anticipate that by training this model on a much greater variety of \ntasks and settings, we will obtain a general system that can turn any \ndemonstrations into robust policies that can accomplish an overwhelming variety \nof tasks. \n</p> \n<p>Videos available at https://bit.ly/nips2017-oneshot . \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394ee4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.07326"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Peter Bartlett, Dylan J. Foster, Matus Telgarsky", "title": "Spectrally-normalized margin bounds for neural networks. (arXiv:1706.08498v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.08498", "type": "text/html"}], "timestampUsec": "1512537801473952", "comments": [], "summary": {"content": "<p>This paper presents a margin-based multiclass generalization bound for neural \nnetworks that scales with their margin-normalized \"spectral complexity\": their \nLipschitz constant, meaning the product of the spectral norms of the weight \nmatrices, times a certain correction factor. This bound is empirically \ninvestigated for a standard AlexNet network trained with SGD on the mnist and \ncifar10 datasets, with both original and random labels; the bound, the \nLipschitz constants, and the excess risks are all in direct correlation, \nsuggesting both that SGD selects predictors whose complexity scales with the \ndifficulty of the learning task, and secondly that the presented bound is \nsensitive to this complexity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394ef0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.08498"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Juncai Xu, Zhenzhong Shen, Qingwen Ren, Xin Xie, Zhengyu Yang", "title": "Slope Stability Analysis with Geometric Semantic Genetic Programming. (arXiv:1708.09116v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.09116", "type": "text/html"}], "timestampUsec": "1512537801473951", "comments": [], "summary": {"content": "<p>Genetic programming has been widely used in the engineering field. Compared \nwith the conventional genetic programming and artificial neural network, \ngeometric semantic genetic programming (GSGP) is superior in astringency and \ncomputing efficiency. In this paper, GSGP is adopted for the classification and \nregression analysis of a sample dataset. Furthermore, a model for slope \nstability analysis is established on the basis of geometric semantics. \nAccording to the results of the study based on GSGP, the method can analyze \nslope stability objectively and is highly precise in predicting slope stability \nand safety factors. Hence, the predicted results can be used as a reference for \nslope safety design. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394efb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.09116"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yong-Siang Shih, Kai-Yueh Chang, Hsuan-Tien Lin, Min Sun", "title": "Compatibility Family Learning for Item Recommendation and Generation. (arXiv:1712.01262v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01262", "type": "text/html"}], "timestampUsec": "1512537801473950", "comments": [], "summary": {"content": "<p>Compatibility between items, such as clothes and shoes, is a major factor \namong customer's purchasing decisions. However, learning \"compatibility\" is \nchallenging due to (1) broader notions of compatibility than those of \nsimilarity, (2) the asymmetric nature of compatibility, and (3) only a small \nset of compatible and incompatible items are observed. We propose an end-to-end \ntrainable system to embed each item into a latent vector and project a query \nitem into K compatible prototypes in the same space. These prototypes reflect \nthe broad notions of compatibility. We refer to both the embedding and \nprototypes as \"Compatibility Family\". In our learned space, we introduce a \nnovel Projected Compatibility Distance (PCD) function which is differentiable \nand ensures diversity by aiming for at least one prototype to be close to a \ncompatible item, whereas none of the prototypes are close to an incompatible \nitem. We evaluate our system on a toy dataset, two Amazon product datasets, and \nPolyvore outfit dataset. Our method consistently achieves state-of-the-art \nperformance. Finally, we show that we can visualize the candidate compatible \nprototypes using a Metric-regularized Conditional Generative Adversarial \nNetwork (MrCGAN), where the input is a projected prototype and the output is a \ngenerated image of a compatible item. We ask human evaluators to judge the \nrelative compatibility between our generated images and images generated by \nCGANs conditioned directly on query items. Our generated images are \nsignificantly preferred, with roughly twice the number of votes as others. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f14", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01262"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shangtong Zhang, Richard S. Sutton", "title": "A Deeper Look at Experience Replay. (arXiv:1712.01275v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01275", "type": "text/html"}], "timestampUsec": "1512537801473949", "comments": [], "summary": {"content": "<p>Experience replay plays an important role in the success of deep \nreinforcement learning (RL) by helping stabilize the neural networks. It has \nbecome a new norm in deep RL algorithms. In this paper, however, we showcase \nthat varying the size of the experience replay buffer can hurt the performance \neven in very simple tasks. The size of the replay buffer is actually a \nhyper-parameter which needs careful tuning. Moreover, our study of experience \nreplay leads to the formulation of the Combined DQN algorithm, which can \nsignificantly outperform primitive DQN in some tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f21", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01275"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rakshit Agrawal, Anwar Habeeb, Chih-Hsin Hsueh", "title": "Learning User Intent from Action Sequences on Interactive Systems. (arXiv:1712.01328v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.01328", "type": "text/html"}], "timestampUsec": "1512537801473948", "comments": [], "summary": {"content": "<p>Interactive systems have taken over the web and mobile space with increasing \nparticipation from users. Applications across every marketing domain can now be \naccessed through mobile or web where users can directly perform certain actions \nand reach a desired outcome. Actions of user on a system, though, can be \nrepresentative of a certain intent. Ability to learn this intent through user's \nactions can help draw certain insight into the behavior of users on a system. \n</p> \n<p>In this paper, we present models to optimize interactive systems by learning \nand analyzing user intent through their actions on the system. We present a \nfour phased model that uses time-series of interaction actions sequentially \nusing a Long Short-Term Memory (LSTM) based sequence learning system that helps \nbuild a model for intent recognition. Our system then provides an objective \nspecific maximization followed by analysis and contrasting methods in order to \nidentify spaces of improvement in the interaction system. We discuss deployment \nscenarios for such a system and present results from evaluation on an online \nmarketplace using user clickstream data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f28", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01328"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mircea Mironenco, Dana Kianfar, Ke Tran, Evangelos Kanoulas, Efstratios Gavves", "title": "Examining Cooperation in Visual Dialog Models. (arXiv:1712.01329v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01329", "type": "text/html"}], "timestampUsec": "1512537801473947", "comments": [], "summary": {"content": "<p>In this work we propose a blackbox intervention method for visual dialog \nmodels, with the aim of assessing the contribution of individual linguistic or \nvisual components. Concretely, we conduct structured or randomized \ninterventions that aim to impair an individual component of the model, and \nobserve changes in task performance. We reproduce a state-of-the-art visual \ndialog model and demonstrate that our methodology yields surprising insights, \nnamely that both dialog and image information have minimal contributions to \ntask performance. The intervention method presented here can be applied as a \nsanity check for the strength and robustness of each component in visual dialog \nsystems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f33", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01329"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhiqian Chen, Xuchao Zhang, Arnold P. Boedihardjo, Jing Dai, Chang-Tien Lu", "title": "Multimodal Storytelling via Generative Adversarial Imitation Learning. (arXiv:1712.01455v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.01455", "type": "text/html"}], "timestampUsec": "1512537801473946", "comments": [], "summary": {"content": "<p>Deriving event storylines is an effective summarization method to succinctly \norganize extensive information, which can significantly alleviate the pain of \ninformation overload. The critical challenge is the lack of widely recognized \ndefinition of storyline metric. Prior studies have developed various approaches \nbased on different assumptions about users' interests. These works can extract \ninteresting patterns, but their assumptions do not guarantee that the derived \npatterns will match users' preference. On the other hand, their exclusiveness \nof single modality source misses cross-modality information. This paper \nproposes a method, multimodal imitation learning via generative adversarial \nnetworks(MIL-GAN), to directly model users' interests as reflected by various \ndata. In particular, the proposed model addresses the critical challenge by \nimitating users' demonstrated storylines. Our proposed model is designed to \nlearn the reward patterns given user-provided storylines and then applies the \nlearned policy to unseen data. The proposed approach is demonstrated to be \ncapable of acquiring the user's implicit intent and outperforming competing \nmethods by a substantial margin with a user study. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f3f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01455"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tomer Libal (Inria, Paris), Xaviera Steele (American University of Paris)", "title": "Determinism in the Certification of UNSAT Proofs. (arXiv:1712.01488v1 [cs.LO])", "alternate": [{"href": "http://arxiv.org/abs/1712.01488", "type": "text/html"}], "timestampUsec": "1512537801473945", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a6e60d8\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a6e60d8&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The search for increased trustworthiness of SAT solvers is very active and \nuses various methods. Some of these methods obtain a proof from the provers \nthen check it, normally by replicating the search based on the proof's \ninformation. Because the certification process involves another nontrivial \nproof search, the trust we can place in it is decreased. Some attempts to amend \nthis use certifiers which have been verified by proofs assistants such as \nIsabelle/HOL and Coq. Our approach is different because it is based on an \nextremely simplified certifier. This certifier enjoys a very high level of \ntrust but is very inefficient. In this paper, we experiment with this approach \nand conclude that by placing some restrictions on the formats, one can mostly \neliminate the need for search and in principle, can certify proofs of arbitrary \nsize. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01488"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Qingxiang Feng, Yicong Zhou", "title": "Discriminant Projection Representation-based Classification for Vision Recognition. (arXiv:1712.01643v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01643", "type": "text/html"}], "timestampUsec": "1512537801473944", "comments": [], "summary": {"content": "<p>Representation-based classification methods such as sparse \nrepresentation-based classification (SRC) and linear regression classification \n(LRC) have attracted a lot of attentions. In order to obtain the better \nrepresentation, a novel method called projection representation-based \nclassification (PRC) is proposed for image recognition in this paper. PRC is \nbased on a new mathematical model. This model denotes that the 'ideal \nprojection' of a sample point $x$ on the hyper-space $H$ may be gained by \niteratively computing the projection of $x$ on a line of hyper-space $H$ with \nthe proper strategy. Therefore, PRC is able to iteratively approximate the \n'ideal representation' of each subject for classification. Moreover, the \ndiscriminant PRC (DPRC) is further proposed, which obtains the discriminant \ninformation by maximizing the ratio of the between-class reconstruction error \nover the within-class reconstruction error. Experimental results on five \ntypical databases show that the proposed PRC and DPRC are effective and \noutperform other state-of-the-art methods on several vision recognition tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f50", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01643"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shun Miao, Sebastien Piat, Peter Fischer, Ahmet Tuysuzoglu, Philip Mewes, Tommaso Mansi, Rui Liao", "title": "Dilated FCN for Multi-Agent 2D/3D Medical Image Registration. (arXiv:1712.01651v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01651", "type": "text/html"}], "timestampUsec": "1512537801473943", "comments": [], "summary": {"content": "<p>2D/3D image registration to align a 3D volume and 2D X-ray images is a \nchallenging problem due to its ill-posed nature and various artifacts presented \nin 2D X-ray images. In this paper, we propose a multi-agent system with an auto \nattention mechanism for robust and efficient 2D/3D image registration. \nSpecifically, an individual agent is trained with dilated Fully Convolutional \nNetwork (FCN) to perform registration in a Markov Decision Process (MDP) by \nobserving a local region, and the final action is then taken based on the \nproposals from multiple agents and weighted by their corresponding confidence \nlevels. The contributions of this paper are threefold. First, we formulate \n2D/3D registration as a MDP with observations, actions, and rewards properly \ndefined with respect to X-ray imaging systems. Second, to handle various \nartifacts in 2D X-ray images, multiple local agents are employed efficiently \nvia FCN-based structures, and an auto attention mechanism is proposed to favor \nthe proposals from regions with more reliable visual cues. Third, a dilated \nFCN-based training mechanism is proposed to significantly reduce the Degree of \nFreedom in the simulation of registration environment, and drastically improve \ntraining efficiency by an order of magnitude compared to standard CNN-based \ntraining method. We demonstrate that the proposed method achieves high \nrobustness on both spine cone beam Computed Tomography data with a low \nsignal-to-noise ratio and data from minimally invasive spine surgery where \nsevere image artifacts and occlusions are presented due to metal screws and \nguide wires, outperforming other state-of-the-art methods (single agent-based \nand optimization-based) by a large margin. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f54", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01651"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Siyu Yu, Nanning Zheng, Yongqiang Ma, Hao Wu, Badong Chen", "title": "A Novel Brain Decoding Method: a Correlation Network Framework for Revealing Brain Connections. (arXiv:1712.01668v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01668", "type": "text/html"}], "timestampUsec": "1512537801473942", "comments": [], "summary": {"content": "<p>Brain decoding is a hot spot in cognitive science, which focuses on \nreconstructing perceptual images from brain activities. Analyzing the \ncorrelations of collected data from human brain activities and representing \nactivity patterns are two problems in brain decoding based on functional \nmagnetic resonance imaging (fMRI) signals. However, existing correlation \nanalysis methods mainly focus on the strength information of voxel, which \nreveals functional connectivity in the cerebral cortex. They tend to neglect \nthe structural information that implies the intracortical or intrinsic \nconnections; that is, structural connectivity. Hence, the effective \nconnectivity inferred by these methods is relatively unilateral. Therefore, we \nproposed a correlation network (CorrNet) framework that could be flexibly \ncombined with diverse pattern representation models. In the CorrNet framework, \nthe topological correlation was introduced to reveal structural information. \nRich correlations were obtained, which contributed to specifying the underlying \neffective connectivity. We also combined the CorrNet framework with a linear \nsupport vector machine (SVM) and a dynamic evolving spike neuron network (SNN) \nfor pattern representation separately, thus providing a novel method for \ndecoding cognitive activity patterns. Experimental results verified the \nreliability and robustness of our CorrNet framework and demonstrated that the \nnew method achieved significant improvement in brain decoding over comparable \nmethods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f5b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01668"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis", "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm. (arXiv:1712.01815v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.01815", "type": "text/html"}], "timestampUsec": "1512537801473941", "comments": [], "summary": {"content": "<p>The game of chess is the most widely-studied domain in the history of \nartificial intelligence. The strongest programs are based on a combination of \nsophisticated search techniques, domain-specific adaptations, and handcrafted \nevaluation functions that have been refined by human experts over several \ndecades. In contrast, the AlphaGo Zero program recently achieved superhuman \nperformance in the game of Go, by tabula rasa reinforcement learning from games \nof self-play. In this paper, we generalise this approach into a single \nAlphaZero algorithm that can achieve, tabula rasa, superhuman performance in \nmany challenging domains. Starting from random play, and given no domain \nknowledge except the game rules, AlphaZero achieved within 24 hours a \nsuperhuman level of play in the games of chess and shogi (Japanese chess) as \nwell as Go, and convincingly defeated a world-champion program in each case. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f5f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01815"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Marko Horvat, Anton Grbin, Gordan Gledec", "title": "WNtags: A Web-Based Tool For Image Labeling And Retrieval With Lexical Ontologies. (arXiv:1302.2223v2 [cs.IR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1302.2223", "type": "text/html"}], "timestampUsec": "1512537801473940", "comments": [], "summary": {"content": "<p>Ever growing number of image documents available on the Internet continuously \nmotivates research in better annotation models and more efficient retrieval \nmethods. Formal knowledge representation of objects and events in pictures, \ntheir interaction as well as context complexity becomes no longer an option for \na quality image repository, but a necessity. We present an ontology-based \nonline image annotation tool WNtags and demonstrate its usefulness in several \ntypical multimedia retrieval tasks using International Affective Picture System \nemotionally annotated image database. WNtags is built around WordNet lexical \nontology but considers Suggested Upper Merged Ontology as the preferred \nlabeling formalism. WNtags uses sets of weighted WordNet synsets as high-level \nimage semantic descriptors and query matching is performed with word stemming \nand node distance metrics. We also elaborate our near future plans to expand \nimage content description with induced affect as in stimuli for research of \nhuman emotion and attention. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f6b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1302.2223"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ivan Brugere, Brian Gallagher, Tanya Y. Berger-Wolf", "title": "Network Structure Inference, A Survey: Motivations, Methods, and Applications. (arXiv:1610.00782v3 [cs.SI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.00782", "type": "text/html"}], "timestampUsec": "1512537801473939", "comments": [], "summary": {"content": "<p>Networks represent relationships between entities in many complex systems, \nspanning from online social interactions to biological cell development and \nbrain connectivity. In many cases, relationships between entities are \nunambiguously known: are two users 'friends' in a social network? Do two \nresearchers collaborate on a published paper? Do two road segments in a \ntransportation system intersect? These are directly observable in the system in \nquestion. In most cases, relationship between nodes are not directly observable \nand must be inferred: does one gene regulate the expression of another? Do two \nanimals who physically co-locate have a social bond? Who infected whom in a \ndisease outbreak in a population? Existing approaches for inferring networks \nfrom data are found across many application domains, and use specialized \nknowledge to infer and measure the quality of inferred network for a specific \ntask or hypothesis. However, current research lacks a rigorous methodology \nwhich employs standard statistical validation on inferred models. In this \nsurvey, we examine (1) how network representations are constructed from \nunderlying data, (2) the variety of questions and tasks on these \nrepresentations over several domains, and (3) validation strategies for \nmeasuring the inferred network's capability of answering questions on the \nsystem of interest. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f7b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.00782"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman, Filip De Turck, Pieter Abbeel", "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning. (arXiv:1611.04717v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.04717", "type": "text/html"}], "timestampUsec": "1512537801473938", "comments": [], "summary": {"content": "<p>Count-based exploration algorithms are known to perform near-optimally when \nused in conjunction with tabular reinforcement learning (RL) methods for \nsolving small discrete Markov decision processes (MDPs). It is generally \nthought that count-based methods cannot be applied in high-dimensional state \nspaces, since most states will only occur once. Recent deep RL exploration \nstrategies are able to deal with high-dimensional continuous state spaces \nthrough complex heuristics, often relying on optimism in the face of \nuncertainty or intrinsic motivation. In this work, we describe a surprising \nfinding: a simple generalization of the classic count-based approach can reach \nnear state-of-the-art performance on various high-dimensional and/or continuous \ndeep RL benchmarks. States are mapped to hash codes, which allows to count \ntheir occurrences with a hash table. These counts are then used to compute a \nreward bonus according to the classic count-based exploration theory. We find \nthat simple hash functions can achieve surprisingly good results on many \nchallenging tasks. Furthermore, we show that a domain-dependent learned hash \ncode may further improve these results. Detailed analysis reveals important \naspects of a good hash function: 1) having appropriate granularity and 2) \nencoding information relevant to solving the MDP. This exploration strategy \nachieves near state-of-the-art performance on both continuous control tasks and \nAtari 2600 games, hence providing a simple yet powerful baseline for solving \nMDPs that require considerable exploration. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f83", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.04717"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anagha Kulkarni, Yantian Zha, Tathagata Chakraborti, Satya Gautam Vadlamudi, Yu Zhang, Subbarao Kambhampati", "title": "Explicablility as Minimizing Distance from Expected Behavior. (arXiv:1611.05497v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.05497", "type": "text/html"}], "timestampUsec": "1512537801473937", "comments": [], "summary": {"content": "<p>In order to have effective human AI collaboration, it is not simply enough to \naddress the question of autonomy; an equally important question is, how the \nAI's behavior is being perceived by their human counterparts. When AI agent's \ntask plans are generated without such considerations, they may often \ndemonstrate inexplicable behavior from the human's point of view. This problem \narises due to the human's partial or inaccurate understanding of the agent's \nplanning process and/or the model. This may have serious implications on \nhuman-AI collaboration, from increased cognitive load and reduced trust in the \nagent, to more serious concerns of safety in interactions with physical agent. \nIn this paper, we address this issue by modeling the notion of plan \nexplicability as a function of the distance between a plan that agent makes and \nthe plan that human expects it to make. To this end, we learn a distance \nfunction based on different plan distance measures that can accurately model \nthis notion of plan explicability, and develop an anytime search algorithm that \ncan use this distance as a heuristic to come up with progressively explicable \nplans. We evaluate the effectiveness of our approach in a simulated autonomous \ncar domain and a physical service robot domain. We provide empirical \nevaluations that demonstrate the usefulness of our approach in making the \nplanning process of an autonomous agent conform to human expectations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f92", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.05497"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "TonTon Hsien-De Huang, Hung-Yu Kao", "title": "R2-D2: ColoR-inspired Convolutional NeuRal Network (CNN)-based AndroiD Malware Detections. (arXiv:1705.04448v4 [cs.CR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.04448", "type": "text/html"}], "timestampUsec": "1512537801473936", "comments": [], "summary": {"content": "<p>Machine Learning (ML) has found it particularly useful in malware detection. \nHowever, as the malware evolves very fast, the stability of the feature \nextracted from malware serves as a critical issue in malware detection. Recent \nsuccess of deep learning in image recognition, natural language processing, and \nmachine translation indicate a potential solution for stabilizing the malware \ndetection effectiveness. We present a coloR-inspired convolutional neuRal \nnetwork-based AndroiD malware Detection (R2-D2), which can detect malware \nwithout extracting pre-selected features (e.g., the control-flow of op-code, \nclasses, methods of functions and the timing they are invoked etc.) from \nAndroid apps. In particular, we develop a color representation for translating \nAndroid apps into RGB color code and transform them to a fixed-sized encoded \nimage. After that, the encoded image is fed to convolutional neural network for \nautomatic feature extraction and learning, reducing the expert's intervention. \nWe have collected over 1 million malware samples and 1 million benign samples \naccording to the data provided by Leopard Mobile Inc. from its core product \nSecurity Master (which has 623 million monthly active users and 10k new malware \nsamples per day). It is shown that R2-D2 can effectively detect the malware. \nFurthermore, we keep our research results and release experiment material on \n<a href=\"http://R2D2.TWMAN.ORG\">this http URL</a> if there is any update. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fa3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.04448"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yihui He, Xiaobo Ma, Xiapu Luo, Jianfeng Li, Mengchen Zhao, Bo An, Xiaohong Guan", "title": "Vehicle Traffic Driven Camera Placement for Better Metropolis Security Surveillance. (arXiv:1705.08508v3 [cs.CY] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08508", "type": "text/html"}], "timestampUsec": "1512537801473935", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a6e62c2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a6e62c2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Security surveillance is one of the most important issues in smart cities, \nespecially in an era of terrorism. Deploying a number of (video) cameras is a \ncommon surveillance approach. Given the never-ending power offered by vehicles \nto metropolises, exploiting vehicle traffic to design camera placement \nstrategies could potentially facilitate security surveillance. This article \nconstitutes the first effort toward building the linkage between vehicle \ntraffic and security surveillance, which is a critical problem for smart \ncities. We expect our study could influence the decision making of surveillance \ncamera placement, and foster more research of principled ways of security \nsurveillance beneficial to our physical-world life. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08508"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Abhishek Kumar, Prasanna Sattigeri, P. Thomas Fletcher", "title": "Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference. (arXiv:1705.08850v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08850", "type": "text/html"}], "timestampUsec": "1512537801473934", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a740176\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a740176&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Semi-supervised learning methods using Generative Adversarial Networks (GANs) \nhave shown promising empirical success recently. Most of these methods use a \nshared discriminator/classifier which discriminates real examples from fake \nwhile also predicting the class label. Motivated by the ability of the GANs \ngenerator to capture the data manifold well, we propose to estimate the tangent \nspace to the data manifold using GANs and employ it to inject invariances into \nthe classifier. In the process, we propose enhancements over existing methods \nfor learning the inverse mapping (i.e., the encoder) which greatly improves in \nterms of semantic similarity of the reconstructed sample with the input sample. \nWe observe considerable empirical gains in semi-supervised learning over \nbaselines, particularly in the cases when the number of labeled examples is \nlow. We also provide insights into how fake examples influence the \nsemi-supervised learning procedure. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fb5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08850"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1515390605, "author": "Haoshu Fang, Yuanlu Xu, Wenguan Wang, Xiaobai Liu, Song-Chun Zhu", "title": "Learning Pose Grammar to Encode Human Body Configuration for 3D Pose Estimation. (arXiv:1710.06513v6 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.06513", "type": "text/html"}], "timestampUsec": "1512537801473933", "comments": [], "summary": {"content": "<p>In this paper, we propose a pose grammar to tackle the problem of 3D human \npose estimation. Our model directly takes 2D pose as input and learns a \ngeneralized 2D-3D mapping function. The proposed model consists of a base \nnetwork which efficiently captures pose-aligned features and a hierarchy of \nBi-directional RNNs (BRNN) on the top to explicitly incorporate a set of \nknowledge regarding human body configuration (i.e., kinematics, symmetry, motor \ncoordination). The proposed model thus enforces high-level constraints over \nhuman poses. In learning, we develop a pose sample simulator to augment \ntraining samples in virtual camera views, which further improves our model \ngeneralizability. We validate our method on public 3D human pose benchmarks and \npropose a new evaluation protocol working on cross-view setting to verify the \ngeneralization capability of different methods. We empirically observe that \nmost state-of-the-art methods encounter difficulty under such setting while our \nmethod can well handle such challenges. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1515390604, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fb8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.06513"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Cyprien Gilet, Marie Deprez, Jean-Baptiste Caillau, Michel Barlaud", "title": "Clustering with feature selection using alternating minimization, Application to computational biology. (arXiv:1711.02974v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.02974", "type": "text/html"}], "timestampUsec": "1512537801473932", "comments": [], "summary": {"content": "<p>This paper deals with unsupervised clustering with feature selection. The \nproblem is to estimate both labels and a sparse projection matrix of weights. \nTo address this combinatorial non-convex problem maintaining a strict control \non the sparsity of the matrix of weights, we propose an alternating \nminimization of the Frobenius norm criterion. We provide a new efficient \nalgorithm named K-sparse which alternates k-means with projection-gradient \nminimization. The projection-gradient step is a method of splitting type, with \nexact projection on the $\\ell^1$ ball to promote sparsity. The convergence of \nthe gradient-projection step is addressed, and a preliminary analysis of the \nalternating minimization is made. The Frobenius norm criterion converges as the \nnumber of iterates in Algorithm K-sparse goes to infinity. Experiments on \nSingle Cell RNA sequencing datasets show that our method significantly improves \nthe results of PCA k-means, spectral clustering, SIMLR, and Sparcl methods, and \nachieves a relevant selection of genes. The complexity of K-sparse is linear in \nthe number of samples (cells), so that the method scales up to large datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fc1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.02974"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Igor Melnyk, Cicero Nogueira dos Santos, Kahini Wadhawan, Inkit Padhi, Abhishek Kumar", "title": "Improved Neural Text Attribute Transfer with Non-parallel Data. (arXiv:1711.09395v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09395", "type": "text/html"}], "timestampUsec": "1512537801473931", "comments": [], "summary": {"content": "<p>Text attribute transfer using non-parallel data requires methods that can \nperform disentanglement of content and linguistic attributes. In this work, we \npropose multiple improvements over the existing approaches that enable the \nencoder-decoder framework to cope with the text attribute transfer from \nnon-parallel data. We perform experiments on the sentiment transfer task using \ntwo datasets. For both datasets, our proposed method outperforms a strong \nbaseline in two of the three employed evaluation metrics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fc6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09395"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christos Louizos, Max Welling, Diederik P. Kingma", "title": "Learning Sparse Neural Networks through $L_0$ Regularization. (arXiv:1712.01312v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01312", "type": "text/html"}], "timestampUsec": "1512537801473928", "comments": [], "summary": {"content": "<p>We propose a practical method for $L_0$ norm regularization for neural \nnetworks: pruning the network during training by encouraging weights to become \nexactly zero. Such regularization is interesting since (1) it can greatly speed \nup training and inference, and (2) it can improve generalization. AIC and BIC, \nwell-known model selection criteria, are special cases of $L_0$ regularization. \nHowever, since the $L_0$ norm of weights is non-differentiable, we cannot \nincorporate it directly as a regularization term in the objective function. We \npropose a solution through the inclusion of a collection of non-negative \nstochastic gates, which collectively determine which weights to set to zero. We \nshow that, somewhat surprisingly, for certain distributions over the gates, the \nexpected $L_0$ norm of the resulting gated weights is differentiable with \nrespect to the distribution parameters. We further propose the \\emph{hard \nconcrete} distribution for the gates, which is obtained by \"stretching\" a \nbinary concrete distribution and then transforming its samples with a \nhard-sigmoid. The parameters of the distribution over the gates can then be \njointly optimized with the original network parameters. As a result our method \nallows for straightforward and efficient learning of model structures with \nstochastic gradient descent and allows for conditional computation in a \nprincipled way. We perform various experiments to demonstrate the effectiveness \nof the resulting approach and regularizer. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fd3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01312"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Katja Ried, Thomas M&#xfc;ller, Hans J. Briegel", "title": "Modelling collective motion based on the principle of agency. (arXiv:1712.01334v1 [q-bio.PE])", "alternate": [{"href": "http://arxiv.org/abs/1712.01334", "type": "text/html"}], "timestampUsec": "1512537801473927", "comments": [], "summary": {"content": "<p>Collective motion is an intriguing phenomenon, especially considering that it \narises from a set of simple rules governing local interactions between \nindividuals. In theoretical models, these rules are normally \\emph{assumed} to \ntake a particular form, possibly constrained by heuristic arguments. We propose \na new class of models, which describe the individuals as \\emph{agents}, capable \nof deciding for themselves how to act and learning from their experiences. The \nlocal interaction rules do not need to be postulated in this model, since they \n\\emph{emerge} from the learning process. We apply this ansatz to a concrete \nscenario involving marching locusts, in order to model the phenomenon of \ndensity-dependent alignment. We show that our learning agent-based model can \naccount for a Fokker-Planck equation that describes the collective motion and, \nmost notably, that the agents can learn the appropriate local interactions, \nrequiring no strong previous assumptions on their form. These results suggest \nthat learning agent-based models are a powerful tool for studying a broader \nclass of problems involving collective motion and animal agency in general. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fd7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01334"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Samuel E. Otto, Clarence W. Rowley", "title": "Linearly-Recurrent Autoencoder Networks for Learning Dynamics. (arXiv:1712.01378v1 [math.DS])", "alternate": [{"href": "http://arxiv.org/abs/1712.01378", "type": "text/html"}], "timestampUsec": "1512537801473926", "comments": [], "summary": {"content": "<p>This paper describes a method for learning low-dimensional approximations of \nnonlinear dynamical systems, based on neural-network approximations of the \nunderlying Koopman operator. Extended Dynamic Mode Decomposition (EDMD) \nprovides a useful data-driven approximation of the Koopman operator for \nanalyzing dynamical systems. This paper addresses a fundamental problem \nassociated with EDMD: a trade-off between representational capacity of the \ndictionary and over-fitting due to insufficient data. A new neural network \narchitecture combining an autoencoder with linear recurrent dynamics in the \nencoded state is used to learn a low-dimensional and highly informative \nKoopman-invariant subspace of observables. A method is also presented for \nbalanced model reduction of over-specified EDMD systems in feature space. \nNonlinear reconstruction using partially linear multi-kernel regression aims to \nimprove reconstruction accuracy from the low-dimensional state when the data \nhas complex but intrinsically low-dimensional structure. The techniques \ndemonstrate the ability to identify Koopman eigenfunctions of the unforced \nDuffing equation, create accurate low-dimensional models of an unstable \ncylinder wake flow, and make short-time predictions of the chaotic \nKuramoto-Sivashinsky equation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fdd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01378"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shubhanshu Shekhar, Tara Javidi", "title": "Gaussian Process bandits with adaptive discretization. (arXiv:1712.01447v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01447", "type": "text/html"}], "timestampUsec": "1512537801473925", "comments": [], "summary": {"content": "<p>In this paper, the problem of maximizing a black-box function $f:\\mathcal{X} \n\\to \\mathbb{R}$ is studied in the Bayesian framework with a Gaussian Process \n(GP) prior. In particular, a new algorithm for this problem is proposed, and \nhigh probability bounds on its simple and cumulative regret are established. \nThe query point selection rule in most existing methods involves an exhaustive \nsearch over an increasingly fine sequence of uniform discretizations of \n$\\mathcal{X}$. The proposed algorithm, in contrast, adaptively refines \n$\\mathcal{X}$ which leads to a lower computational complexity, particularly \nwhen $\\mathcal{X}$ is a subset of a high dimensional Euclidean space. In \naddition to the computational gains, sufficient conditions are identified under \nwhich the regret bounds of the new algorithm improve upon the known results. \nFinally an extension of the algorithm to the case of contextual bandits is \nproposed, and high probability bounds on the contextual regret are presented. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fe4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01447"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Thomas Laurent, James von Brecht", "title": "Deep linear neural networks with arbitrary loss: All local minima are global. (arXiv:1712.01473v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01473", "type": "text/html"}], "timestampUsec": "1512537801473924", "comments": [], "summary": {"content": "<p>We consider deep linear networks with arbitrary differentiable loss. We \nprovide a short and elementary proof of the following fact: all local minima \nare global minima if each hidden layer is wider than either the input or output \nlayer. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394feb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01473"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhanli Chen, Rashid Ansari, Diana J. Wilkie", "title": "Learning Pain from Action Unit Combinations: A Weakly Supervised Approach via Multiple Instance Learning. (arXiv:1712.01496v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01496", "type": "text/html"}], "timestampUsec": "1512537801473923", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a740465\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a740465&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Facial pain expression is an important modality for assessing pain, \nespecially when a patient's verbal ability to communicate is impaired. A set of \neight facial muscle based action units (AUs), which are defined by the Facial \nAction Coding System (FACS), have been widely studied and are highly reliable \nfor pain detection through facial expressions. However, using FACS is a very \ntime consuming task that makes its clinical use prohibitive. An automated \nfacial expression recognition system (AFER) reliably detecting pain-related AUs \nwould be highly beneficial for efficient and practical pain monitoring. \nAutomated pain detection under clinical settings is viewed as a weakly \nsupervised problem, which is not suitable general AFER system that trained on \nwell labeled data. Existing pain oriented AFER research either focus on the \nindividual pain-related AU recognition or bypassing the AU detection procedure \nby training a binary pain classifier from pain intensity data. In this paper, \nwe decouple pain detection into two consecutive tasks: the AFER based AU \nlabeling at video frame level and a probabilistic measure of pain at sequence \nlevel from AU combination scores. Our work is distinguished in the following \naspects, 1) State of the art AFER tools Emotient is applied on pain oriented \ndata sets for single AU labeling. 2) Two different data structures are proposed \nto encode AU combinations from single AU scores, which forms low-dimensional \nfeature vectors for the learning framework. 3) Two weakly supervised learning \nframeworks namely multiple instance learning and multiple clustered instance \nlearning are employed corresponding to each feature structure to learn pain \nfrom video sequences. The results shows 87% pain recognition accuracy with 0.94 \nAUC on UNBC-McMaster dataset. Tests on Wilkie's dataset suggests the potential \nvalue of the proposed system for pain monitoring task under clinical settings. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394ff0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01496"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wei Xiao", "title": "An Online Algorithm for Nonparametric Correlations. (arXiv:1712.01521v1 [stat.AP])", "alternate": [{"href": "http://arxiv.org/abs/1712.01521", "type": "text/html"}], "timestampUsec": "1512537801473922", "comments": [], "summary": {"content": "<p>Nonparametric correlations such as Spearman's rank correlation and Kendall's \ntau correlation are widely applied in scientific and engineering fields. This \npaper investigates the problem of computing nonparametric correlations on the \nfly for streaming data. Standard batch algorithms are generally too slow to \nhandle real-world big data applications. They also require too much memory \nbecause all the data need to be stored in the memory before processing. This \npaper proposes a novel online algorithm for computing nonparametric \ncorrelations. The algorithm has O(1) time complexity and O(1) memory cost and \nis quite suitable for edge devices, where only limited memory and processing \npower are available. You can seek a balance between speed and accuracy by \nchanging the number of cutpoints specified in the algorithm. The online \nalgorithm can compute the nonparametric correlations 10 to 1,000 times faster \nthan the corresponding batch algorithm, and it can compute them based either on \nall past observations or on fixed-size sliding windows. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034139500b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01521"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhiwu Huang, Jiqing Wu, Luc Van Gool", "title": "Manifold-valued Image Generation with Wasserstein Adversarial Networks. (arXiv:1712.01551v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01551", "type": "text/html"}], "timestampUsec": "1512537801473921", "comments": [], "summary": {"content": "<p>Unsupervised image generation has recently received an increasing amount of \nattention thanks to the great success of generative adversarial networks \n(GANs), particularly Wasserstein GANs. Inspired by the paradigm of real-valued \nimage generation, this paper makes the first attempt to formulate the problem \nof generating manifold-valued images, which are frequently encountered in \nreal-world applications. For the study, we specially exploit three typical \nmanifold-valued image generation tasks: hue-saturation-value (HSV) color image \ngeneration, chromaticity-brightness (CB) color image generation, and \ndiffusion-tensor (DT) image generation. In order to produce such kinds of \nimages as realistic as possible, we generalize the state-of-the-art technique \nof Wasserstein GANs to the manifold context with exploiting Riemannian \ngeometry. For the proposed manifold-valued image generation problem, we \nrecommend three benchmark datasets that are CIFAR-10 HSV/CB color images, \nImageNet HSV/CB color images, UCL DT image datasets. On the three datasets, we \nexperimentally demonstrate the proposed manifold-aware Wasserestein GAN can \ngenerate high quality manifold-valued images. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034139506d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01551"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Stefan Klus, Ingmar Schuster, Krikamol Muandet", "title": "Eigendecompositions of Transfer Operators in Reproducing Kernel Hilbert Spaces. (arXiv:1712.01572v1 [math.DS])", "alternate": [{"href": "http://arxiv.org/abs/1712.01572", "type": "text/html"}], "timestampUsec": "1512537801473920", "comments": [], "summary": {"content": "<p>Transfer operators such as the Perron-Frobenius or Koopman operator play an \nimportant role in the global analysis of complex dynamical systems. The \neigenfunctions of these operators can be used to detect metastable sets, to \nproject the dynamics onto the dominant slow processes, or to separate \nsuperimposed signals. We extend transfer operator theory to reproducing kernel \nHilbert spaces and show that these operators are related to Hilbert space \nrepresentations of conditional distributions, known as conditional mean \nembeddings in the machine learning community. Moreover, numerical methods to \ncompute empirical estimates of these embeddings are akin to data-driven methods \nfor the approximation of transfer operators such as extended dynamic mode \ndecomposition and its variants. In fact, most of the existing methods can be \nderived from our framework, providing a unifying view on the approximation of \ntransfer operators. One main benefit of the presented kernel-based approaches \nis that these methods can be applied to any domain where a similarity measure \ngiven by a kernel is available. We illustrate the results with the aid of \nguiding examples and highlight potential applications in molecular dynamics as \nwell as video and text data analysis. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341395083", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01572"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David Janz, Jos van der Westhuizen, Brooks Paige, Matt J. Kusner, Jose Miguel Hernandez-Labato", "title": "Learning a Generative Model for Validity in Complex Discrete Structures. (arXiv:1712.01664v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01664", "type": "text/html"}], "timestampUsec": "1512537801473919", "comments": [], "summary": {"content": "<p>Deep generative models have been successfully used to learn representations \nfor high-dimensional discrete spaces by representing discrete objects as \nsequences, for which powerful sequence-based deep models can be employed. \nUnfortunately, these techniques are significantly hindered by the fact that \nthese generative models often produce invalid sequences: sequences which do not \nrepresent any underlying discrete structure. As a step towards solving this \nproblem, we propose to learn a deep recurrent validator model, which can \nestimate whether a partial sequence can function as the beginning of a full, \nvalid sequence. This model not only discriminates between valid and invalid \nsequences, but also provides insight as to how individual sequence elements \ninfluence the validity of the overall sequence, and the existence of a \ncorresponding discrete object. To learn this model we propose a reinforcement \nlearning approach, where an oracle which can evaluate validity of complete \nsequences provides a sparse reward signal. We believe this is a key step toward \nlearning generative models that faithfully produce valid sequences which \nrepresent discrete objects. We demonstrate its effectiveness in evaluating the \nvalidity of Python 3 source code for mathematical expressions, and improving \nthe ability of a variational autoencoder trained on SMILES strings to decode \nvalid molecular structures. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034139509e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01664"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Beyza Ermis, Ali Taylan Cemgil", "title": "Differentially Private Dropout. (arXiv:1712.01665v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01665", "type": "text/html"}], "timestampUsec": "1512537801473918", "comments": [], "summary": {"content": "<p>Large data collections required for the training of neural networks often \ncontain sensitive information such as the medical histories of patients, and \nthe privacy of the training data must be preserved. In this paper, we introduce \na dropout technique that provides an elegant Bayesian interpretation to \ndropout, and show that the intrinsic noise added, with the primary goal of \nregularization, can be exploited to obtain a degree of differential privacy. \nThe iterative nature of training neural networks presents a challenge for \nprivacy-preserving estimation since multiple iterations increase the amount of \nnoise added. We overcome this by using a relaxed notion of differential \nprivacy, called concentrated differential privacy, which provides tighter \nestimates on the overall privacy loss. We demonstrate the accuracy of our \nprivacy-preserving dropout algorithm on benchmark datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003413950b2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01665"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jos&#xe9; Lezama, Qiang Qiu, Pablo Mus&#xe9;, Guillermo Sapiro", "title": "OL\\'E: Orthogonal Low-rank Embedding, A Plug and Play Geometric Loss for Deep Learning. (arXiv:1712.01727v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01727", "type": "text/html"}], "timestampUsec": "1512537801473917", "comments": [], "summary": {"content": "<p>Deep neural networks trained using a softmax layer at the top and the \ncross-entropy loss are ubiquitous tools for image classification. Yet, this \ndoes not naturally enforce intra-class similarity nor inter-class margin of the \nlearned deep representations. To simultaneously achieve these two goals, \ndifferent solutions have been proposed in the literature, such as the pairwise \nor triplet losses. However, such solutions carry the extra task of selecting \npairs or triplets, and the extra computational burden of computing and learning \nfor many combinations of them. In this paper, we propose a plug-and-play loss \nterm for deep networks that explicitly reduces intra-class variance and \nenforces inter-class margin simultaneously, in a simple and elegant geometric \nmanner. For each class, the deep features are collapsed into a learned linear \nsubspace, or union of them, and inter-class subspaces are pushed to be as \northogonal as possible. Our proposed Orthogonal Low-rank Embedding (OL\\'E) does \nnot require carefully crafting pairs or triplets of samples for training, and \nworks standalone as a classification loss, being the first reported deep metric \nlearning framework of its kind. Because of the improved margin between features \nof different classes, the resulting deep networks generalize better, are more \ndiscriminative, and more robust. We demonstrate improved classification \nperformance in general object recognition, plugging the proposed loss term into \nexisting off-the-shelf architectures. In particular, we show the advantage of \nthe proposed loss in the small data/model scenario, and we significantly \nadvance the state-of-the-art on the Stanford STL-10 benchmark. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003413950bc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01727"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tara N. Sainath, Chung-Cheng Chiu, Rohit Prabhavalkar, Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Zhifeng Chen", "title": "Improving the Performance of Online Neural Transducer Models. (arXiv:1712.01807v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.01807", "type": "text/html"}], "timestampUsec": "1512537801473916", "comments": [], "summary": {"content": "<p>Having a sequence-to-sequence model which can operate in an online fashion is \nimportant for streaming applications such as Voice Search. Neural transducer is \na streaming sequence-to-sequence model, but has shown a significant degradation \nin performance compared to non-streaming models such as Listen, Attend and \nSpell (LAS). In this paper, we present various improvements to NT. \nSpecifically, we look at increasing the window over which NT computes \nattention, mainly by looking backwards in time so the model still remains \nonline. In addition, we explore initializing a NT model from a LAS-trained \nmodel so that it is guided with a better alignment. Finally, we explore \nincluding stronger language models such as using wordpiece models, and applying \nan external LM during the beam search. On a Voice Search task, we find with \nthese improvements we can get NT to match the performance of LAS. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003413950cd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01807"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David M. Blei, Alp Kucukelbir, Jon D. McAuliffe", "title": "Variational Inference: A Review for Statisticians. (arXiv:1601.00670v7 [stat.CO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1601.00670", "type": "text/html"}], "timestampUsec": "1512537801473915", "comments": [], "summary": {"content": "<p>One of the core problems of modern statistics is to approximate \ndifficult-to-compute probability densities. This problem is especially \nimportant in Bayesian statistics, which frames all inference about unknown \nquantities as a calculation involving the posterior density. In this paper, we \nreview variational inference (VI), a method from machine learning that \napproximates probability densities through optimization. VI has been used in \nmany applications and tends to be faster than classical methods, such as Markov \nchain Monte Carlo sampling. The idea behind VI is to first posit a family of \ndensities and then to find the member of that family which is close to the \ntarget. Closeness is measured by Kullback-Leibler divergence. We review the \nideas behind mean-field variational inference, discuss the special case of VI \napplied to exponential family models, present a full example with a Bayesian \nmixture of Gaussians, and derive a variant that uses stochastic optimization to \nscale up to massive data. We discuss modern research in VI and highlight \nimportant open problems. VI is powerful, but it is not yet well understood. Our \nhope in writing this paper is to catalyze statistical research on this class of \nalgorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003413950da", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1601.00670"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513669231, "author": "Justin Sirignano, Konstantinos Spiliopoulos", "title": "DGM: A deep learning algorithm for solving partial differential equations. (arXiv:1708.07469v3 [q-fin.MF] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.07469", "type": "text/html"}], "timestampUsec": "1512537801473914", "comments": [], "summary": {"content": "<p>High-dimensional PDEs have been a longstanding computational challenge. We \npropose to solve high-dimensional PDEs by approximating the solution with a \ndeep neural network which is trained to satisfy the differential operator, \ninitial condition, and boundary conditions. We prove that the neural network \nconverges to the solution of the partial differential equation as the number of \nhidden units increases. Our algorithm is meshfree, which is key since meshes \nbecome infeasible in higher dimensions. Instead of forming a mesh, the neural \nnetwork is trained on batches of randomly sampled time and space points. We \nimplement the approach for American options (a type of free-boundary PDE which \nis widely used in finance) in up to $200$ dimensions. We call the algorithm a \n\"Deep Galerkin Method (DGM)\" since it is similar in spirit to Galerkin methods, \nwith the solution approximated by a neural network instead of a linear \ncombination of basis functions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003413950ff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.07469"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Xiaolin Hu, Jianguo Li, Jun Zhu", "title": "Boosting Adversarial Attacks with Momentum. (arXiv:1710.06081v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.06081", "type": "text/html"}], "timestampUsec": "1512537801473913", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a74069f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a74069f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep neural networks are vulnerable to adversarial examples, which poses \nsecurity concerns on these algorithms due to the potentially severe \nconsequences. Adversarial attacks serve as an important surrogate to evaluate \nthe robustness of deep learning models before they are deployed. However, most \nof the existing adversarial attacks can only fool a black-box model with a low \nsuccess rate because of the coupling of the attack ability and the \ntransferability. To address this issue, we propose a broad class of \nmomentum-based iterative algorithms to boost adversarial attacks. By \nintegrating the momentum term into the iterative process for attacks, our \nmethods can stabilize update directions and escape from poor local maxima \nduring the iterations, resulting in more transferable adversarial examples. To \nfurther improve the success rates for black-box attacks, we apply momentum \niterative algorithms to an ensemble of models, and show that the adversarially \ntrained models with a strong defense ability are also vulnerable to our \nblack-box attacks. We hope that the proposed methods will serve as a benchmark \nfor evaluating the robustness of various deep models and defense methods. We \nwon the first places in NIPS 2017 Non-targeted Adversarial Attack and Targeted \nAdversarial Attack competitions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341395110", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.06081"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jungtaek Kim, Saehoon Kim, Seungjin Choi", "title": "Learning to Warm-Start Bayesian Hyperparameter Optimization. (arXiv:1710.06219v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.06219", "type": "text/html"}], "timestampUsec": "1512537801473912", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a7b75ee\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a7b75ee&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Hyperparameter optimization undergoes extensive evaluations of validation \nerrors in order to find its best configuration. Bayesian optimization is now \npopular for hyperparameter optimization, since it reduces the number of \nvalidation error evaluations required. Suppose that we are given a collection \nof datasets on which hyperparameters are already tuned by either humans with \ndomain expertise or extensive trials of cross-validation. When a model is \napplied to a new dataset, it is desirable to let Bayesian optimization start \nfrom configurations that were successful on similar datasets. To this end, we \nconstruct a Siamese network with convolutional layers followed by \nbi-directional LSTM layers, to learn meta-features over image datasets. Learned \nmeta-features are used to select a few datasets that are similar to the new \ndataset, so that a set of configurations in similar datasets is adopted as \ninitialization to warm-start Bayesian hyperparameter optimization. Experiments \non image datasets demonstrate that our learned meta-features are useful in \noptimizing hyperparameters in deep residual networks for image classification. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341395124", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.06219"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anqi Wu, Oluwasanmi Koyejo, Jonathan W. Pillow", "title": "Dependent relevance determination for smooth and structured sparse regression. (arXiv:1711.10058v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10058", "type": "text/html"}], "timestampUsec": "1512537801473911", "comments": [], "summary": {"content": "<p>In many problem settings, parameter vectors are not merely sparse, but \ndependent in such a way that non-zero coefficients tend to cluster together. We \nrefer to this form of dependency as \"region sparsity\". Classical sparse \nregression methods, such as the lasso and automatic relevance determination \n(ARD), which model parameters as independent a priori, and therefore do not \nexploit such dependencies. Here we introduce a hierarchical model for smooth, \nregion-sparse weight vectors and tensors in a linear regression setting. Our \napproach represents a hierarchical extension of the relevance determination \nframework, where we add a transformed Gaussian process to model the \ndependencies between the prior variances of regression weights. We combine this \nwith a structured model of the prior variances of Fourier coefficients, which \neliminates unnecessary high frequencies. The resulting prior encourages weights \nto be region-sparse in two different bases simultaneously. We develop Laplace \napproximation and Monte Carlo Markov Chain (MCMC) sampling to provide efficient \ninference for the posterior. Furthermore, a two-stage convex relaxation of the \nLaplace approximation approach is also provided to relax the inevitable \nnon-convexity during the optimization. We finally show substantial improvements \nover comparable methods for both simulated and real datasets from brain \nimaging. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034139513b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10058"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1513318279, "author": "Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Katya Gonina, Navdeep Jaitly, Bo Li, Jan Chorowski, Michiel Bacchiani", "title": "State-of-the-art Speech Recognition With Sequence-to-Sequence Models. (arXiv:1712.01769v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.01769", "type": "text/html"}], "timestampUsec": "1512536858997297", "comments": [], "summary": {"content": "<p>Attention-based encoder-decoder architectures such as Listen, Attend, and \nSpell (LAS), subsume the acoustic, pronunciation and language model components \nof a traditional automatic speech recognition (ASR) system into a single neural \nnetwork. In our previous work, we have shown that such architectures are \ncomparable to state-of-the-art ASR systems on dictation tasks, but it was not \nclear if such architectures would be practical for more challenging tasks such \nas voice search. In this work, we explore a variety of structural and \noptimization improvements to our LAS model which significantly improve \nperformance. On the structural side, we show that word piece models can be used \ninstead of graphemes. We introduce a novel multi-head attention architecture, \nwhich offers improvements over the commonly-used single-head attention. On the \noptimization side, we explore techniques such as synchronous training, \nscheduled sampling, label smoothing, and applying minimum word error rate \noptimization, which are all shown to improve accuracy. We present results with \na unidirectional LSTM encoder for streaming recognition. On a 12,500~hour voice \nsearch task, we find that the proposed changes improve the WER of the LAS \nsystem from 9.2% to 5.6%, which corresponds to a 16% relative improvement over \nthe best conventional system which achieves 6.7% WER. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512536858997", "annotations": [], "published": 1513318279, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034137931c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01769"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Rohit Prabhavalkar, Tara N. Sainath, Yonghui Wu, Patrick Nguyen, Zhifeng Chen, Chung-Cheng Chiu, Anjuli Kannan", "title": "Minimum Word Error Rate Training for Attention-based Sequence-to-Sequence Models. (arXiv:1712.01818v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.01818", "type": "text/html"}], "timestampUsec": "1512536858997296", "comments": [], "summary": {"content": "<p>Sequence-to-sequence models, such as attention-based models in automatic \nspeech recognition (ASR), are typically trained to optimize the cross-entropy \ncriterion which corresponds to improving the log-likelihood of the data. \nHowever, system performance is usually measured in terms of word error rate \n(WER), not log-likelihood. Traditional ASR systems benefit from discriminative \nsequence training which optimizes criteria such as the state-level minimum \nBayes risk (sMBR) which are more closely related to WER. In the present work, \nwe explore techniques to train attention-based models to directly minimize \nexpected word error rate. We consider two loss functions which approximate the \nexpected number of word errors: either by sampling from the model, or by using \nN-best lists of decoded hypotheses, which we find to be more effective than the \nsampling-based method. In experimental evaluations, we find that the proposed \ntraining procedure improves performance by up to 8.2% relative to the baseline \nsystem. This allows us to train grapheme-based, uni-directional attention-based \nmodels which match the performance of a traditional, state-of-the-art, \ndiscriminative sequence-trained system on a mobile voice-search task. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512536858997", "annotations": [], "published": 1512536859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341379320", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01818"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512767377, "author": "Benjamin Doerr", "title": "An Elementary Analysis of the Probability That a Binomial Random Variable Exceeds Its Expectation. (arXiv:1712.00519v2 [math.PR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00519", "type": "text/html"}], "timestampUsec": "1512450843841025", "comments": [], "summary": {"content": "<p>We give an elementary proof of the fact that a binomial random variable $X$ \nwith parameters $n$ and $0.29/n \\le p &lt; 1$ with probability at least $1/4$ \nstrictly exceeds its expectation. We also show that for $1/n \\le p &lt; 1 - 1/n$, \n$X$ exceeds its expectation by more than one with probability at least \n$0.0370$. Both probabilities approach $1/2$ when $np$ and $n(1-p)$ tend to \ninfinity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512767377, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763912", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00519"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aaron Tuor, Ryan Baerwolf, Nicolas Knowles, Brian Hutchinson, Nicole Nichols, Rob Jasper", "title": "Recurrent Neural Network Language Models for Open Vocabulary Event-Level Cyber Anomaly Detection. (arXiv:1712.00557v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.00557", "type": "text/html"}], "timestampUsec": "1512450843841024", "comments": [], "summary": {"content": "<p>Automated analysis methods are crucial aids for monitoring and defending a \nnetwork to protect the sensitive or confidential data it hosts. This work \nintroduces a flexible, powerful, and unsupervised approach to detecting \nanomalous behavior in computer and network logs, one that largely eliminates \ndomain-dependent feature engineering employed by existing methods. By treating \nsystem logs as threads of interleaved \"sentences\" (event log lines) to train \nonline unsupervised neural network language models, our approach provides an \nadaptive model of normal network behavior. We compare the effectiveness of both \nstandard and bidirectional recurrent neural network language models at \ndetecting malicious activity within network log data. Extending these models, \nwe introduce a tiered recurrent architecture, which provides context by \nmodeling sequences of users' actions over time. Compared to Isolation Forest \nand Principal Components Analysis, two popular anomaly detection algorithms, we \nobserve superior performance on the Los Alamos National Laboratory Cyber \nSecurity dataset. For log-line-level red team detection, our best performing \ncharacter-based model provides test set area under the receiver operator \ncharacteristic curve of 0.98, demonstrating the strong fine-grained anomaly \ndetection performance of this approach on open vocabulary logging sources. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763916", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00557"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wellington Pinheiro dos Santos, Ricardo Emmanuel de Souza, Pl&#xed;nio B. dos Santos Filho", "title": "Evaluation of Alzheimer's Disease by Analysis of MR Images using Multilayer Perceptrons and Kohonen SOM Classifiers as an Alternative to the ADC Maps. (arXiv:1712.00712v1 [eess.IV])", "alternate": [{"href": "http://arxiv.org/abs/1712.00712", "type": "text/html"}], "timestampUsec": "1512450843841023", "comments": [], "summary": {"content": "<p>Alzheimer's disease is the most common cause of dementia, yet hard to \ndiagnose precisely without invasive techniques, particularly at the onset of \nthe disease. This work approaches image analysis and classification of \nsynthetic multispectral images composed by diffusion-weighted magnetic \nresonance (MR) cerebral images for the evaluation of cerebrospinal fluid area \nand measuring the advance of Alzheimer's disease. A clinical 1.5 T MR imaging \nsystem was used to acquire all images presented. The classification methods are \nbased on multilayer perceptrons and Kohonen Self-Organized Map classifiers. We \nassume the classes of interest can be separated by hyperquadrics. Therefore, a \n2-degree polynomial network is used to classify the original image, generating \nthe ground truth image. The classification results are used to improve the \nusual analysis of the apparent diffusion coefficient map. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034076391b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00712"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lyudmila Grigoryeva, Juan-Pablo Ortega", "title": "Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems. (arXiv:1712.00754v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.00754", "type": "text/html"}], "timestampUsec": "1512450843841022", "comments": [], "summary": {"content": "<p>A new class of non-homogeneous state-affine systems is introduced. Sufficient \nconditions are identified that guarantee first, that the associated reservoir \ncomputers with linear readouts are causal, time-invariant, and satisfy the \nfading memory property and second, that a subset of this class is universal in \nthe category of fading memory filters with stochastic almost surely bounded \ninputs. This means that any discrete-time filter that satisfies the fading \nmemory property with random inputs of that type can be uniformly approximated \nby elements in the non-homogeneous state-affine family. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763922", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00754"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Valter Augusto de Freitas Barbosa, Reiga Ramalho Ribeiro, Allan Rivalles Souza Feitosa, Victor Luiz Bezerra Ara&#xfa;jo da Silva, Arthur Diego Dias Rocha, Rafaela Covello de Freitas, Ricardo Emmanuel de Souza, Wellington Pinheiro dos Santos", "title": "Reconstruction of Electrical Impedance Tomography Using Fish School Search, Non-Blind Search, and Genetic Algorithm. (arXiv:1712.00789v1 [physics.med-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.00789", "type": "text/html"}], "timestampUsec": "1512450843841021", "comments": [], "summary": {"content": "<p>Electrical Impedance Tomography (EIT) is a noninvasive imaging technique that \ndoes not use ionizing radiation, with application both in environmental \nsciences and in health. Image reconstruction is performed by solving an inverse \nproblem and ill-posed. Evolutionary Computation and Swarm Intelligence have \nbecome a source of methods for solving inverse problems. Fish School Search \n(FSS) is a promising search and optimization method, based on the dynamics of \nschools of fish. In this article the authors present a method for \nreconstruction of EIT images based on FSS and Non-Blind Search (NBS). The \nmethod was evaluated using numerical phantoms consisting of electrical \nconductivity images with subjects in the center, between the center and the \nedge and on the edge of a circular section, with meshes of 415 finite elements. \nThe authors performed 20 simulations for each configuration. Results showed \nthat both FSS and FSS-NBS were able to converge faster than genetic algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763928", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00789"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andrew Levy, Robert Platt, Kate Saenko", "title": "Hierarchical Actor-Critic. (arXiv:1712.00948v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00948", "type": "text/html"}], "timestampUsec": "1512450843841020", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a7b78c6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a7b78c6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present a novel approach to hierarchical reinforcement learning called \nHierarchical Actor-Critic (HAC). HAC aims to make learning tasks with sparse \nbinary rewards more efficient by enabling agents to learn how to break down \ntasks from scratch. The technique uses of a set of actor-critic networks that \nlearn to decompose tasks into a hierarchy of subgoals. We demonstrate that HAC \nsignificantly improves sample efficiency in a series of tasks that involve \nsparse binary rewards and require behavior over a long time horizon. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763933", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00948"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Johannes Lengler", "title": "Drift Analysis. (arXiv:1712.00964v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.00964", "type": "text/html"}], "timestampUsec": "1512450843841019", "comments": [], "summary": {"content": "<p>Drift analysis is one of the major tools for analysing evolutionary \nalgorithms and nature-inspired search heuristics. In this chapter we give an \nintroduction to drift analysis and give some examples of how to use it for the \nanalysis of evolutionary algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763941", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00964"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Paolo Meloni, Alessandro Capotondi, Gianfranco Deriu, Michele Brian, Francesco Conti, Davide Rossi, Luigi Raffo, Luca Benini", "title": "NEURAghe: Exploiting CPU-FPGA Synergies for Efficient and Flexible CNN Inference Acceleration on Zynq SoCs. (arXiv:1712.00994v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.00994", "type": "text/html"}], "timestampUsec": "1512450843841018", "comments": [], "summary": {"content": "<p>Deep convolutional neural networks (CNNs) obtain outstanding results in tasks \nthat require human-level understanding of data, like image or speech \nrecognition. However, their computational load is significant, motivating the \ndevelopment of CNN-specialized accelerators. This work presents NEURAghe, a \nflexible and efficient hardware/software solution for the acceleration of CNNs \non Zynq SoCs. NEURAghe leverages the synergistic usage of Zynq ARM cores and of \na powerful and flexible Convolution-Specific Processor deployed on the \nreconfigurable logic. The Convolution-Specific Processor embeds both a \nconvolution engine and a programmable soft core, releasing the ARM processors \nfrom most of the supervision duties and allowing the accelerator to be \ncontrolled by software at an ultra-fine granularity. This methodology opens the \nway for cooperative heterogeneous computing: while the accelerator takes care \nof the bulk of the CNN workload, the ARM cores can seamlessly execute \nhard-to-accelerate parts of the computational graph, taking advantage of the \nNEON vector engines to further speed up computation. Through the companion \nNeuDNN SW stack, NEURAghe supports end-to-end CNN-based classification with a \npeak performance of 169 Gops/s, and an energy efficiency of 17 Gops/W. Thanks \nto our heterogeneous computing model, our platform improves upon the \nstate-of-the-art, achieving a frame rate of 5.5 fps on the end-to-end execution \nof VGG-16, and 6.6 fps on ResNet-18. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034076394a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00994"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ga&#xe9;tan Marceau-Caron, Yann Ollivier", "title": "Natural Langevin Dynamics for Neural Networks. (arXiv:1712.01076v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01076", "type": "text/html"}], "timestampUsec": "1512450843841017", "comments": [], "summary": {"content": "<p>One way to avoid overfitting in machine learning is to use model parameters \ndistributed according to a Bayesian posterior given the data, rather than the \nmaximum likelihood estimator. Stochastic gradient Langevin dynamics (SGLD) is \none algorithm to approximate such Bayesian posteriors for large models and \ndatasets. SGLD is a standard stochastic gradient descent to which is added a \ncontrolled amount of noise, specifically scaled so that the parameter converges \nin law to the posterior distribution [WT11, TTV16]. The posterior predictive \ndistribution can be approximated by an ensemble of samples from the trajectory. \n</p> \n<p>Choice of the variance of the noise is known to impact the practical behavior \nof SGLD: for instance, noise should be smaller for sensitive parameter \ndirections. Theoretically, it has been suggested to use the inverse Fisher \ninformation matrix of the model as the variance of the noise, since it is also \nthe variance of the Bayesian posterior [PT13, AKW12, GC11]. But the Fisher \nmatrix is costly to compute for large- dimensional models. \n</p> \n<p>Here we use the easily computed Fisher matrix approximations for deep neural \nnetworks from [MO16, Oll15]. The resulting natural Langevin dynamics combines \nthe advantages of Amari's natural gradient descent and Fisher-preconditioned \nLangevin dynamics for large neural networks. \n</p> \n<p>Small-scale experiments on MNIST show that Fisher matrix preconditioning \nbrings SGLD close to dropout as a regularizing technique. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763952", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01076"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513141860, "author": "Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, Neoklis Polyzotis", "title": "The Case for Learned Index Structures. (arXiv:1712.01208v2 [cs.DB] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.01208", "type": "text/html"}], "timestampUsec": "1512450843841016", "comments": [], "summary": {"content": "<p>Indexes are models: a B-Tree-Index can be seen as a model to map a key to the \nposition of a record within a sorted array, a Hash-Index as a model to map a \nkey to a position of a record within an unsorted array, and a BitMap-Index as a \nmodel to indicate if a data record exists or not. In this exploratory research \npaper, we start from this premise and posit that all existing index structures \ncan be replaced with other types of models, including deep-learning models, \nwhich we term learned indexes. The key idea is that a model can learn the sort \norder or structure of lookup keys and use this signal to effectively predict \nthe position or existence of records. We theoretically analyze under which \nconditions learned indexes outperform traditional index structures and describe \nthe main challenges in designing learned index structures. Our initial results \nshow, that by using neural nets we are able to outperform cache-optimized \nB-Trees by up to 70% in speed while saving an order-of-magnitude in memory over \nseveral real-world data sets. More importantly though, we believe that the idea \nof replacing core components of a data management system through learned models \nhas far reaching implications for future systems designs and that this work \njust provides a glimpse of what might be possible. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763964", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01208"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tim Rockt&#xe4;schel, Sebastian Riedel", "title": "End-to-End Differentiable Proving. (arXiv:1705.11040v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.11040", "type": "text/html"}], "timestampUsec": "1512450843841015", "comments": [], "summary": {"content": "<p>We introduce neural networks for end-to-end differentiable proving of queries \nto knowledge bases by operating on dense vector representations of symbols. \nThese neural networks are constructed recursively by taking inspiration from \nthe backward chaining algorithm as used in Prolog. Specifically, we replace \nsymbolic unification with a differentiable computation on vector \nrepresentations of symbols using a radial basis function kernel, thereby \ncombining symbolic reasoning with learning subsymbolic vector representations. \nBy using gradient descent, the resulting neural network can be trained to infer \nfacts from a given incomplete knowledge base. It learns to (i) place \nrepresentations of similar symbols in close proximity in a vector space, (ii) \nmake use of such similarities to prove queries, (iii) induce logical rules, and \n(iv) use provided and induced logical rules for multi-hop reasoning. We \ndemonstrate that this architecture outperforms ComplEx, a state-of-the-art \nneural link prediction model, on three out of four benchmark knowledge bases \nwhile at the same time inducing interpretable function-free first-order logic \nrules. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763969", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.11040"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Abhinav Gupta, Yajie Miao, Leonardo Neves, Florian Metze", "title": "Visual Features for Context-Aware Speech Recognition. (arXiv:1712.00489v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.00489", "type": "text/html"}], "timestampUsec": "1512450843841014", "comments": [], "summary": {"content": "<p>Automatic transcriptions of consumer-generated multi-media content such as \n\"Youtube\" videos still exhibit high word error rates. Such data typically \noccupies a very broad domain, has been recorded in challenging conditions, with \ncheap hardware and a focus on the visual modality, and may have been \npost-processed or edited. In this paper, we extend our earlier work on adapting \nthe acoustic model of a DNN-based speech recognition system to an RNN language \nmodel and show how both can be adapted to the objects and scenes that can be \nautomatically detected in the video. We are working on a corpus of \"how-to\" \nvideos from the web, and the idea is that an object that can be seen (\"car\"), \nor a scene that is being detected (\"kitchen\") can be used to condition both \nmodels on the \"context\" of the recording, thereby reducing perplexity and \nimproving transcription. We achieve good improvements in both cases and compare \nand analyze the respective reductions in word error rate. We expect that our \nresults can be used for any type of speech processing in which \"context\" \ninformation is available, for example in robotics, man-machine interaction, or \nwhen indexing large audio-visual archives, and should ultimately help to bring \ntogether the \"video-to-text\" and \"speech-to-text\" communities. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763970", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00489"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512537804, "author": "Tim Miller, Piers Howe, Liz Sonenberg", "title": "Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences. (arXiv:1712.00547v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00547", "type": "text/html"}], "timestampUsec": "1512450843841013", "comments": [], "summary": {"content": "<p>In his seminal book `The Inmates are Running the Asylum: Why High-Tech \nProducts Drive Us Crazy And How To Restore The Sanity' [2004, Sams \nIndianapolis, IN, USA], Alan Cooper argues that a major reason why software is \noften poorly designed (from a user perspective) is that programmers are in \ncharge of design decisions, rather than interaction designers. As a result, \nprogrammers design software for themselves, rather than for their target \naudience, a phenomenon he refers to as the `inmates running the asylum'. This \npaper argues that explainable AI risks a similar fate. While the re-emergence \nof explainable AI is positive, this paper argues most of us as AI researchers \nare building explanatory agents for ourselves, rather than for the intended \nusers. But explainable AI is more likely to succeed if researchers and \npractitioners understand, adopt, implement, and improve models from the vast \nand valuable bodies of research in philosophy, psychology, and cognitive \nscience, and if evaluation of these models is focused more on people than on \ntechnology. From a light scan of literature, we demonstrate that there is \nconsiderable scope to infuse more results from the social and behavioural \nsciences into explainable AI, and present some key results from these fields \nthat are relevant to explainable AI. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763978", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00547"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yan Zhu, Shaoting Zhang, Dimitris Metaxas", "title": "Interactive Reinforcement Learning for Object Grounding via Self-Talking. (arXiv:1712.00576v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00576", "type": "text/html"}], "timestampUsec": "1512450843841012", "comments": [], "summary": {"content": "<p>Humans are able to identify a referred visual object in a complex scene via a \nfew rounds of natural language communications. Success communication requires \nboth parties to engage and learn to adapt for each other. In this paper, we \nintroduce an interactive training method to improve the natural language \nconversation system for a visual grounding task. During interactive training, \nboth agents are reinforced by the guidance from a common reward function. The \nparametrized reward function also cooperatively updates itself via \ninteractions, and contribute to accomplishing the task. We evaluate the method \non GuessWhat?! visual grounding task, and significantly improve the task \nsuccess rate. However, we observe language drifting problem during training and \npropose to use reward engineering to improve the interpretability for the \ngenerated conversations. Our result also indicates evaluating goal-ended visual \nconversation tasks require semantic relevant metrics beyond task success rate. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034076397e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00576"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lianmin Zheng, Jiacheng Yang, Han Cai, Weinan Zhang, Jun Wang, Yong Yu", "title": "MAgent: A Many-Agent Reinforcement Learning Platform for Artificial Collective Intelligence. (arXiv:1712.00600v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00600", "type": "text/html"}], "timestampUsec": "1512450843841011", "comments": [], "summary": {"content": "<p>We introduce MAgent, a platform to support research and development of \nmany-agent reinforcement learning. Unlike previous research platforms on single \nor multi-agent reinforcement learning, MAgent focuses on supporting the tasks \nand the applications that require hundreds to millions of agents. Within the \ninteractions among a population of agents, it enables not only the study of \nlearning algorithms for agents' optimal polices, but more importantly, the \nobservation and understanding of individual agent's behaviors and social \nphenomena emerging from the AI society, including communication languages, \nleaderships, altruism. MAgent is highly scalable and can host up to one million \nagents on a single GPU server. MAgent also provides flexible configurations for \nAI researchers to design their customized environments and agents. In this \ndemo, we present three environments designed on MAgent and show emerged \ncollective intelligence by learning from scratch. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034076399a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00600"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Stefan Richthofer, Laurenz Wiskott", "title": "PFAx: Predictable Feature Analysis to Perform Control. (arXiv:1712.00634v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00634", "type": "text/html"}], "timestampUsec": "1512450843841010", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a7b7ae1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a7b7ae1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Predictable Feature Analysis (PFA) (Richthofer, Wiskott, ICMLA 2015) is an \nalgorithm that performs dimensionality reduction on high dimensional input \nsignal. It extracts those subsignals that are most predictable according to a \ncertain prediction model. We refer to these extracted signals as predictable \nfeatures. \n</p> \n<p>In this work we extend the notion of PFA to take supplementary information \ninto account for improving its predictions. Such information can be a \nmultidimensional signal like the main input to PFA, but is regarded external. \nThat means it won't participate in the feature extraction - no features get \nextracted or composed of it. Features will be exclusively extracted from the \nmain input such that they are most predictable based on themselves and the \nsupplementary information. We refer to this enhanced PFA as PFAx (PFA \nextended). \n</p> \n<p>Even more important than improving prediction quality is to observe the \neffect of supplementary information on feature selection. PFAx transparently \nprovides insight how the supplementary information adds to prediction quality \nand whether it is valuable at all. Finally we show how to invert that relation \nand can generate the supplementary information such that it would yield a \ncertain desired outcome of the main signal. \n</p> \n<p>We apply this to a setting inspired by reinforcement learning and let the \nalgorithm learn how to control an agent in an environment. With this method it \nis feasible to locally optimize the agent's state, i.e. reach a certain goal \nthat is near enough. We are preparing a follow-up paper that extends this \nmethod such that also global optimization is feasible. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003407639bc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00634"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Eyke H&#xfc;llermeier", "title": "From knowledge-based to data-driven modeling of fuzzy rule-based systems: A critical reflection. (arXiv:1712.00646v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00646", "type": "text/html"}], "timestampUsec": "1512450843841009", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a8190e3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a8190e3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper briefly elaborates on a development in (applied) fuzzy logic that \nhas taken place in the last couple of decades, namely, the complementation or \neven replacement of the traditional knowledge-based approach to fuzzy \nrule-based systems design by a data-driven one. It is argued that the classical \nrule-based modeling paradigm is actually more amenable to the knowledge-based \napproach, for which it has originally been conceived, while being less apt to \ndata-driven model design. An important reason that prevents fuzzy (rule-based) \nsystems from being leveraged in large-scale applications is the flat structure \nof rule bases, along with the local nature of fuzzy rules and their limited \nability to express complex dependencies between variables. This motivates \nalternative approaches to fuzzy systems modeling, in which functional \ndependencies can be represented more flexibly and more compactly in terms of \nhierarchical structures. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003407639d1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00646"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alper Kose, Berke Aral Sonmez, Metin Balaban", "title": "Simulated Annealing Algorithm for Graph Coloring. (arXiv:1712.00709v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00709", "type": "text/html"}], "timestampUsec": "1512450843841008", "comments": [], "summary": {"content": "<p>The goal of this Random Walks project is to code and experiment the Markov \nChain Monte Carlo (MCMC) method for the problem of graph coloring. In this \nreport, we present the plots of cost function \\(\\mathbf{H}\\) by varying the \nparameters like \\(\\mathbf{q}\\) (Number of colors that can be used in coloring) \nand \\(\\mathbf{c}\\) (Average node degree). The results are obtained by using \nsimulated annealing scheme, where the temperature (inverse of \n\\(\\mathbf{\\beta}\\)) parameter in the MCMC is lowered progressively. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003407639e6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00709"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512767377, "author": "Laura Graesser, Abhinav Gupta, Lakshay Sharma, Evelina Bakhturina", "title": "Sentiment Classification using Images and Label Embeddings. (arXiv:1712.00725v1 [cs.CL] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00725", "type": "text/html"}], "timestampUsec": "1512450843841007", "comments": [], "summary": {"content": "<p>In this project we analysed how much semantic information images carry, and \nhow much value image data can add to sentiment analysis of the text associated \nwith the images. To better understand the contribution from images, we compared \nmodels which only made use of image data, models which only made use of text \ndata, and models which combined both data types. We also analysed if this \napproach could help sentiment classifiers generalize to unknown sentiments. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512767377, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003407639fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00725"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Simon S. Du, Jason D. Lee, Yuandong Tian, Barnabas Poczos, Aarti Singh", "title": "Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of Spurious Local Minima. (arXiv:1712.00779v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00779", "type": "text/html"}], "timestampUsec": "1512450843841006", "comments": [], "summary": {"content": "<p>We consider the problem of learning a one-hidden-layer neural network with \nnon-overlapping convolutional layer and ReLU activation function, i.e., \n$f(\\mathbf{Z}; \\mathbf{w}, \\mathbf{a}) = \\sum_j \na_j\\sigma(\\mathbf{w}^\\top\\mathbf{Z}_j)$, in which both the convolutional \nweights $\\mathbf{w}$ and the output weights $\\mathbf{a}$ are parameters to be \nlearned. We prove that with Gaussian input $\\mathbf{Z}$, there is a spurious \nlocal minimum that is not a global mininum. Surprisingly, in the presence of \nlocal minimum, starting from randomly initialized weights, gradient descent \nwith weight normalization can still be proven to recover the true parameters \nwith constant probability (which can be boosted to arbitrarily high accuracy \nwith multiple restarts). We also show that with constant probability, the same \nprocedure could also converge to the spurious local minimum, showing that the \nlocal minimum plays a non-trivial role in the dynamics of gradient descent. \nFurthermore, a quantitative analysis shows that the gradient descent dynamics \nhas two phases: it starts off slow, but converges much faster after several \niterations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a13", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00779"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jakob Suchan, Mehul Bhatt, Przemys&#x142;aw Wa&#x142;&#x119;ga, Carl Schultz", "title": "Visual Explanation by High-Level Abduction: On Answer-Set Programming Driven Reasoning about Moving Objects. (arXiv:1712.00840v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00840", "type": "text/html"}], "timestampUsec": "1512450843841005", "comments": [], "summary": {"content": "<p>We propose a hybrid architecture for systematically computing robust visual \nexplanation(s) encompassing hypothesis formation, belief revision, and default \nreasoning with video data. The architecture consists of two tightly integrated \nsynergistic components: (1) (functional) answer set programming based abductive \nreasoning with space-time tracklets as native entities; and (2) a visual \nprocessing pipeline for detection based object tracking and motion analysis. \n</p> \n<p>We present the formal framework, its general implementation as a \n(declarative) method in answer set programming, and an example application and \nevaluation based on two diverse video datasets: the MOTChallenge benchmark \ndeveloped by the vision community, and a recently developed Movie Dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a24", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00840"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kyle Hundman, Thamme Gowda, Mayank Kejriwal, Benedikt Boecking", "title": "Always Lurking: Understanding and Mitigating Bias in Online Human Trafficking Detection. (arXiv:1712.00846v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00846", "type": "text/html"}], "timestampUsec": "1512450843841004", "comments": [], "summary": {"content": "<p>Web-based human trafficking activity has increased in recent years but it \nremains sparsely dispersed among escort advertisements and difficult to \nidentify due to its often-latent nature. The use of intelligent systems to \ndetect trafficking can thus have a direct impact on investigative resource \nallocation and decision-making, and, more broadly, help curb a widespread \nsocial problem. Trafficking detection involves assigning a normalized score to \na set of escort advertisements crawled from the Web -- a higher score indicates \na greater risk of trafficking-related (involuntary) activities. In this paper, \nwe define and study the problem of trafficking detection and present a \ntrafficking detection pipeline architecture developed over three years of \nresearch within the DARPA Memex program. Drawing on multi-institutional data, \nsystems, and experiences collected during this time, we also conduct post hoc \nbias analyses and present a bias mitigation plan. Our findings show that, while \nautomatic trafficking detection is an important application of AI for social \ngood, it also provides cautionary lessons for deploying predictive machine \nlearning algorithms without appropriate de-biasing. This ultimately led to \nintegration of an interpretable solution into a search system that contains \nover 100 million advertisements and is used by over 200 law enforcement \nagencies to investigate leads. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a2a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00846"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Catherine Dubois, Bruno Woltzenlogel Paleo", "title": "Proceedings of the Fifth Workshop on Proof eXchange for Theorem Proving. (arXiv:1712.00898v1 [cs.LO])", "alternate": [{"href": "http://arxiv.org/abs/1712.00898", "type": "text/html"}], "timestampUsec": "1512450843841003", "comments": [], "summary": {"content": "<p>This volume of EPTCS contains the proceedings of the Fifth Workshop on Proof \nExchange for Theorem Proving (PxTP 2017), held on September 23-24, 2017 as part \nof the Tableaux, FroCoS and ITP conferences in Brasilia, Brazil. The PxTP \nworkshop series brings together researchers working on various aspects of \ncommunication, integration, and cooperation between reasoning systems and \nformalisms, with a special focus on proofs. The progress in computer-aided \nreasoning, both automated and interactive, during the past decades, made it \npossible to build deduction tools that are increasingly more applicable to a \nwider range of problems and are able to tackle larger problems progressively \nfaster. In recent years, cooperation between such tools in larger systems has \ndemonstrated the potential to reduce the amount of manual intervention. \nCooperation between reasoning systems relies on availability of theoretical \nformalisms and practical tools to exchange problems, proofs, and models. The \nPxTP workshop series strives to encourage such cooperation by inviting \ncontributions on all aspects of cooperation between reasoning tools, whether \nautomatic or interactive. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a30", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00898"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jaejun Yoo, Sohail Sabir, Duchang Heo, Kee Hyun Kim, Abdul Wahab, Yoonseok Choi, Seul-I Lee, Eun Young Chae, Hak Hee Kim, Young Min Bae, Young-wook Choi, Seungryong Cho, Jong Chul Ye", "title": "Deep Learning Can Reverse Photon Migration for Diffuse Optical Tomography. (arXiv:1712.00912v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.00912", "type": "text/html"}], "timestampUsec": "1512450843841002", "comments": [], "summary": {"content": "<p>Can artificial intelligence (AI) learn complicated non-linear physics? Here \nwe propose a novel deep learning approach that learns non-linear photon \nscattering physics and obtains accurate 3D distribution of optical anomalies. \nIn contrast to the traditional black-box deep learning approaches to inverse \nproblems, our deep network learns to invert the Lippmann-Schwinger integral \nequation which describes the essential physics of photon migration of diffuse \nnear-infrared (NIR) photons in turbid media. As an example for clinical \nrelevance, we applied the method to our prototype diffuse optical tomography \n(DOT). We show that our deep neural network, trained with only simulation data, \ncan accurately recover the location of anomalies within biomimetic phantoms and \nlive animals without the use of an exogenous contrast agent. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a37", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00912"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tomoaki Nakamura, Takayuki Nagai, Tadahiro Taniguchi", "title": "SERKET: An Architecture For Connecting Stochastic Models to Realize a Large-Scale Cognitive Model. (arXiv:1712.00929v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00929", "type": "text/html"}], "timestampUsec": "1512450843841001", "comments": [], "summary": {"content": "<p>To realize human-like robot intelligence, a large-scale cognitive \narchitecture is required for robots to understand the environment through a \nvariety of sensors with which they are equipped. In this paper, we propose a \nnovel framework named Serket that enables the construction of a large-scale \ngenerative model and its inference easily by connecting sub-modules to allow \nthe robots to acquire various capabilities through interaction with their \nenvironments and others. We consider that large-scale cognitive models can be \nconstructed by connecting smaller fundamental models hierarchically while \nmaintaining their programmatic independence. Moreover, connected modules are \ndependent on each other, and parameters are required to be optimized as a \nwhole. Conventionally, the equations for parameter estimation have to be \nderived and implemented depending on the models. However, it becomes harder to \nderive and implement those of a larger scale model. To solve these problems, in \nthis paper, we propose a method for parameter estimation by communicating the \nminimal parameters between various modules while maintaining their programmatic \nindependence. Therefore, Serket makes it easy to construct large-scale models \nand estimate their parameters via the connection of modules. Experimental \nresults demonstrated that the model can be constructed by connecting modules, \nthe parameters can be optimized as a whole, and they are comparable with the \noriginal models that we have proposed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a46", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00929"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sachin Pawar, Pushpak Bhattacharya, Girish K. Palshikar", "title": "End-to-End Relation Extraction using Markov Logic Networks. (arXiv:1712.00988v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00988", "type": "text/html"}], "timestampUsec": "1512450843841000", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a819332\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a819332&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The task of end-to-end relation extraction consists of two sub-tasks: i) \nidentifying entity mentions along with their types and ii) recognizing semantic \nrelations among the entity mention pairs. %Identifying entity mentions along \nwith their types and recognizing semantic relations among the entity mentions, \nare two very important problems in Information Extraction. It has been shown \nthat for better performance, it is necessary to address these two sub-tasks \njointly. We propose an approach for simultaneous extraction of entity mentions \nand relations in a sentence, by using inference in Markov Logic Networks (MLN). \nWe learn three different classifiers : i) local entity classifier, ii) local \nrelation classifier and iii) \"pipeline\" relation classifier which uses \npredictions of the local entity classifier. Predictions of these classifiers \nmay be inconsistent with each other. We represent these predictions along with \nsome domain knowledge using weighted first-order logic rules in an MLN and \nperform joint inference over the MLN to obtain a global output with minimum \ninconsistencies. Experiments on the ACE (Automatic Content Extraction) 2004 \ndataset demonstrate that our approach of joint extraction using MLNs \noutperforms the baselines of individual classifiers. Our end-to-end relation \nextraction performance is better than 2 out of 3 previous results reported on \nthe ACE 2004 dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a58", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00988"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Girish Keshav Palshikar, Sachin Pawar, Saheb Chourasia, Nitin Ramrakhiyani", "title": "Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals. (arXiv:1712.00991v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.00991", "type": "text/html"}], "timestampUsec": "1512450843840999", "comments": [], "summary": {"content": "<p>Performance appraisal (PA) is an important HR process to periodically measure \nand evaluate every employee's performance vis-a-vis the goals established by \nthe organization. A PA process involves purposeful multi-step multi-modal \ncommunication between employees, their supervisors and their peers, such as \nself-appraisal, supervisor assessment and peer feedback. Analysis of the \nstructured data and text produced in PA is crucial for measuring the quality of \nappraisals and tracking actual improvements. In this paper, we apply text \nmining techniques to produce insights from PA text. First, we perform sentence \nclassification to identify strengths, weaknesses and suggestions of \nimprovements found in the supervisor assessments and then use clustering to \ndiscover broad categories among them. Next we use multi-class multi-label \nclassification techniques to match supervisor assessments to predefined broad \nperspectives on performance. Finally, we propose a short-text summarization \ntechnique to produce a summary of peer feedback comments for a given employee \nand compare it with manual summaries. All techniques are illustrated using a \nreal-life dataset of supervisor assessment and peer feedback text produced \nduring the PA of 4528 employees in a large multi-national IT company. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a65", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00991"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Leopoldo Bertossi", "title": "Characterizing and Computing Causes for Query Answers in Databases from Database Repairs and Repair Programs. (arXiv:1712.01001v1 [cs.DB])", "alternate": [{"href": "http://arxiv.org/abs/1712.01001", "type": "text/html"}], "timestampUsec": "1512450843840998", "comments": [], "summary": {"content": "<p>A correspondence between database tuples as causes for query answers in \ndatabases and tuple-based repairs of inconsistent databases with respect to \ndenial constraints has already been established. In this work, answer-set \nprograms that specify repairs of databases are used as a basis for solving \ncomputational and reasoning problems about causes. Here, causes are also \nintroduced at the attribute level by appealing to a both null-based and \nattribute-based repair semantics. The corresponding repair programs are \npresented, and they are used as a basis for computation and reasoning about \nattribute-level causes. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a6c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01001"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christoph Adami", "title": "The mind as a computational system. (arXiv:1712.01093v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.01093", "type": "text/html"}], "timestampUsec": "1512450843840997", "comments": [], "summary": {"content": "<p>The present document is an excerpt of an essay that I wrote as part of my \napplication material to graduate school in Computer Science (with a focus on \nArtificial Intelligence), in 1986. I was not invited by any of the schools that \nreceived it, so I became a theoretical physicist instead. The essay's full \ntitle was \"Some Topics in Philosophy and Computer Science\". I am making this \ntext (unchanged from 1985, preserving the typesetting as much as possible) \navailable now in memory of Jerry Fodor, whose writings had influenced me \nsignificantly at the time (even though I did not always agree). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a74", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01093"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David Isele, Akansel Cosgun", "title": "Transferring Autonomous Driving Knowledge on Simulated and Real Intersections. (arXiv:1712.01106v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01106", "type": "text/html"}], "timestampUsec": "1512450843840996", "comments": [], "summary": {"content": "<p>We view intersection handling on autonomous vehicles as a reinforcement \nlearning problem, and study its behavior in a transfer learning setting. We \nshow that a network trained on one type of intersection generally is not able \nto generalize to other intersections. However, a network that is pre-trained on \none intersection and fine-tuned on another performs better on the new task \ncompared to training in isolation. This network also retains knowledge of the \nprior task, even though some forgetting occurs. Finally, we show that the \nbenefits of fine-tuning hold when transferring simulated intersection handling \nknowledge to a real autonomous vehicle. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a7c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01106"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Abhinav Jauhri, Carlee Joe-Wong, John Paul Shen", "title": "On the Real-time Vehicle Placement Problem. (arXiv:1712.01235v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.01235", "type": "text/html"}], "timestampUsec": "1512450843840995", "comments": [], "summary": {"content": "<p>Motivated by ride-sharing platforms' efforts to reduce their riders' wait \ntimes for a vehicle, this paper introduces a novel problem of placing vehicles \nto fulfill real-time pickup requests in a spatially and temporally changing \nenvironment. The real-time nature of this problem makes it fundamentally \ndifferent from other placement and scheduling problems, as it requires not only \nreal-time placement decisions but also handling real-time request dynamics, \nwhich are influenced by human mobility patterns. We use a dataset of ten \nmillion ride requests from four major U.S. cities to show that the requests \nexhibit significant self-similarity. We then propose distributed online \nlearning algorithms for the real-time vehicle placement problem and bound their \nexpected performance under this observed self-similarity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a86", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01235"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wei Ma, Jun Lu", "title": "An Equivalence of Fully Connected Layer and Convolutional Layer. (arXiv:1712.01252v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01252", "type": "text/html"}], "timestampUsec": "1512450843840994", "comments": [], "summary": {"content": "<p>This article demonstrates that convolutional operation can be converted to \nmatrix multiplication, which has the same calculation way with fully connected \nlayer. The article is helpful for the beginners of the neural network to \nunderstand how fully connected layer and the convolutional layer work in the \nbackend. To be concise and to make the article more readable, we only consider \nthe linear case. It can be extended to the non-linear case easily through \nplugging in a non-linear encapsulation to the values like this $\\sigma(x)$ \ndenoted as $x^{\\prime}$. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a8e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01252"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Masataro Asai, Alex Fukunaga", "title": "Classical Planning in Deep Latent Space: Bridging the Subsymbolic-Symbolic Boundary. (arXiv:1705.00154v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.00154", "type": "text/html"}], "timestampUsec": "1512450843840993", "comments": [], "summary": {"content": "<p>Current domain-independent, classical planners require symbolic models of the \nproblem domain and instance as input, resulting in a knowledge acquisition \nbottleneck. Meanwhile, although deep learning has achieved significant success \nin many fields, the knowledge is encoded in a subsymbolic representation which \nis incompatible with symbolic systems such as planners. We propose LatPlan, an \nunsupervised architecture combining deep learning and classical planning. Given \nonly an unlabeled set of image pairs showing a subset of transitions allowed in \nthe environment (training inputs), and a pair of images representing the \ninitial and the goal states (planning inputs), LatPlan finds a plan to the goal \nstate in a symbolic latent space and returns a visualized plan execution. The \ncontribution of this paper is twofold: (1) State Autoencoder, which finds a \npropositional state representation of the environment using a Variational \nAutoencoder. It generates a discrete latent vector from the images, based on \nwhich a PDDL model can be constructed and then solved by an off-the-shelf \nplanner. (2) Action Autoencoder / Discriminator, a neural architecture which \njointly finds the action symbols and the implicit action models \n(preconditions/effects), and provides a successor function for the implicit \ngraph search. We evaluate LatPlan using image-based versions of 3 planning \ndomains: 8-puzzle, Towers of Hanoi and LightsOut. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a96", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.00154"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Thomas Anthony, Zheng Tian, David Barber", "title": "Thinking Fast and Slow with Deep Learning and Tree Search. (arXiv:1705.08439v4 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08439", "type": "text/html"}], "timestampUsec": "1512450843840992", "comments": [], "summary": {"content": "<p>Sequential decision making problems, such as structured prediction, robotic \ncontrol, and game playing, require a combination of planning policies and \ngeneralisation of those plans. In this paper, we present Expert Iteration \n(ExIt), a novel reinforcement learning algorithm which decomposes the problem \ninto separate planning and generalisation tasks. Planning new policies is \nperformed by tree search, while a deep neural network generalises those plans. \nSubsequently, tree search is improved by using the neural network policy to \nguide search, increasing the strength of new plans. In contrast, standard deep \nReinforcement Learning algorithms rely on a neural network not only to \ngeneralise plans, but to discover them too. We show that ExIt outperforms \nREINFORCE for training a neural network to play the board game Hex, and our \nfinal tree search agent, trained tabula rasa, defeats MoHex 1.0, the most \nrecent Olympiad Champion player to be publicly released. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763aa0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08439"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, Sergey Levine", "title": "Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning. (arXiv:1708.02596v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.02596", "type": "text/html"}], "timestampUsec": "1512450843840991", "comments": [], "summary": {"content": "<p>Model-free deep reinforcement learning algorithms have been shown to be \ncapable of learning a wide range of robotic skills, but typically require a \nvery large number of samples to achieve good performance. Model-based \nalgorithms, in principle, can provide for much more efficient learning, but \nhave proven difficult to extend to expressive, high-capacity models such as \ndeep neural networks. In this work, we demonstrate that medium-sized neural \nnetwork models can in fact be combined with model predictive control (MPC) to \nachieve excellent sample complexity in a model-based reinforcement learning \nalgorithm, producing stable and plausible gaits to accomplish various complex \nlocomotion tasks. We also propose using deep neural network dynamics models to \ninitialize a model-free learner, in order to combine the sample efficiency of \nmodel-based approaches with the high task-specific performance of model-free \nmethods. We empirically demonstrate on MuJoCo locomotion tasks that our pure \nmodel-based approach trained on just random action data can follow arbitrary \ntrajectories with excellent sample efficiency, and that our hybrid algorithm \ncan accelerate model-free learning on high-speed benchmark tasks, achieving \nsample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents. \nVideos can be found at https://sites.google.com/view/mbmf \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763aa7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.02596"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513315289, "author": "Tongzhou Wang, Yi Wu, David A. Moore, Stuart J. Russell", "title": "Neural Block Sampling. (arXiv:1708.06040v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.06040", "type": "text/html"}], "timestampUsec": "1512450843840990", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a8194ee\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a8194ee&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Efficient Monte Carlo inference often requires manual construction of \nmodel-specific proposals. We propose an approach to automated proposal \nconstruction by training neural networks to provide fast approximations to \nblock Gibbs conditionals. The learned proposals generalize to occurrences of \ncommon structural motifs both within a given model and across models, allowing \nfor the construction of a library of learned inference primitives that can \naccelerate inference on unseen models with no model-specific training required. \nWe explore several applications including open-universe Gaussian mixture \nmodels, in which our learned proposals outperform a hand-tuned sampler, and a \nreal-world named entity recognition task, in which our sampler's ability to \nescape local modes yields higher final F1 scores than single-site Gibbs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763aaf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.06040"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xinchen Yan, Jasmine Hsu, Mohi Khansari, Yunfei Bai, Arkanath Pathak, Abhinav Gupta, James Davidson, Honglak Lee", "title": "Learning 6-DOF Grasping Interaction with Deep Geometry-aware 3D Representations. (arXiv:1708.07303v3 [cs.RO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.07303", "type": "text/html"}], "timestampUsec": "1512450843840989", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a85ed4f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a85ed4f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper focuses on the problem of learning 6-DOF grasping with a parallel \njaw gripper in simulation. We propose the notion of a geometry-aware \nrepresentation in grasping based on the assumption that knowledge of 3D \ngeometry is at the heart of interaction. Our key idea is constraining and \nregularizing grasping interaction learning through 3D geometry prediction. \nSpecifically, we formulate the learning of deep geometry-aware grasping model \nin two steps: First, we learn to build mental geometry-aware representation by \nreconstructing the scene (i.e., 3D occupancy grid) from RGBD input via \ngenerative 3D shape modeling. Second, we learn to predict grasping outcome with \nits internal geometry-aware representation. The learned outcome prediction \nmodel is used to sequentially propose grasping solutions via \nanalysis-by-synthesis optimization. Our contributions are fourfold: (1) To best \nof our knowledge, we are presenting for the first time a method to learn a \n6-DOF grasping net from RGBD input; (2) We build a grasping dataset from \ndemonstrations in virtual reality with rich sensory and interaction \nannotations. This dataset includes 101 everyday objects spread across 7 \ncategories, additionally, we propose a data augmentation strategy for effective \nlearning; (3) We demonstrate that the learned geometry-aware representation \nleads to about 10 percent relative performance improvement over the baseline \nCNN on grasping objects from our dataset. (4) We further demonstrate that the \nmodel generalizes to novel viewpoints and object instances. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763ab8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.07303"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wengong Jin, Connor W. Coley, Regina Barzilay, Tommi Jaakkola", "title": "Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network. (arXiv:1709.04555v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04555", "type": "text/html"}], "timestampUsec": "1512450843840988", "comments": [], "summary": {"content": "<p>The prediction of organic reaction outcomes is a fundamental problem in \ncomputational chemistry. Since a reaction may involve hundreds of atoms, fully \nexploring the space of possible transformations is intractable. The current \nsolution utilizes reaction templates to limit the space, but it suffers from \ncoverage and efficiency issues. In this paper, we propose a template-free \napproach to efficiently explore the space of product molecules by first \npinpointing the reaction center -- the set of nodes and edges where graph edits \noccur. Since only a small number of atoms contribute to reaction center, we can \ndirectly enumerate candidate products. The generated candidates are scored by a \nWeisfeiler-Lehman Difference Network that models high-order interactions \nbetween changes occurring at nodes across the molecule. Our framework \noutperforms the top-performing template-based approach with a 10\\% margin, \nwhile running orders of magnitude faster. Finally, we demonstrate that the \nmodel accuracy rivals the performance of domain experts. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763abc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04555"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yuanduo He, Xu Chu, Juguang Peng, Jingyue Gao, Yasha Wang", "title": "Motif-based Rule Discovery for Predicting Real-valued Time Series. (arXiv:1709.04763v4 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04763", "type": "text/html"}], "timestampUsec": "1512450843840987", "comments": [], "summary": {"content": "<p>Time series prediction is of great significance in many applications and has \nattracted extensive attention from the data mining community. Existing work \nsuggests that for many problems, the shape in the current time series may \ncorrelate an upcoming shape in the same or another series. Therefore, it is a \npromising strategy to associate two recurring patterns as a rule's antecedent \nand consequent: the occurrence of the antecedent can foretell the occurrence of \nthe consequent, and the learned shape of consequent will give accurate \npredictions. Earlier work employs symbolization methods, but the symbolized \nrepresentation maintains too little information of the original series to mine \nvalid rules. The state-of-the-art work, though directly manipulating the \nseries, fails to segment the series precisely for seeking \nantecedents/consequents, resulting in inaccurate rules in common scenarios. In \nthis paper, we propose a novel motif-based rule discovery method, which \nutilizes motif discovery to accurately extract frequently occurring consecutive \nsubsequences, i.e. motifs, as antecedents/consequents. It then investigates the \nunderlying relationships between motifs by matching motifs as rule candidates \nand ranking them based on the similarities. Experimental results on real open \ndatasets show that the proposed approach outperforms the baseline method by \n23.9%. Furthermore, it extends the applicability from single time series to \nmultiple ones. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763ac2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04763"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Fang Zhang, Xiaochen Wang, Jingfei Han, Jie Tang, Shiyin Wang, Marie-Francine Moens", "title": "Fast Top-k Area Topics Extraction with Knowledge Base. (arXiv:1710.04822v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.04822", "type": "text/html"}], "timestampUsec": "1512450843840986", "comments": [], "summary": {"content": "<p>What are the most popular research topics in Artificial Intelligence (AI)? We \nformulate the problem as extracting top-$k$ topics that can best represent a \ngiven area with the help of knowledge base. We theoretically prove that the \nproblem is NP-hard and propose an optimization model, FastKATE, to address this \nproblem by combining both explicit and latent representations for each topic. \nWe leverage a large-scale knowledge base (Wikipedia) to generate topic \nembeddings using neural networks and use this kind of representations to help \ncapture the representativeness of topics for given areas. We develop a fast \nheuristic algorithm to efficiently solve the problem with a provable error \nbound. We evaluate the proposed model on three real-world datasets. \nExperimental results demonstrate our model's effectiveness, robustness, \nreal-timeness (return results in $&lt;1$s), and its superiority over several \nalternative methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763ace", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.04822"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aixiang Chen, Bingchuan Chen, Xiaolong Chai, Rui Bian, Hengguang Li", "title": "A Novel Stochastic Stratified Average Gradient Method: Convergence Rate and Its Complexity. (arXiv:1710.07783v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.07783", "type": "text/html"}], "timestampUsec": "1512450843840985", "comments": [], "summary": {"content": "<p>SGD (Stochastic Gradient Descent) is a popular algorithm for large scale \noptimization problems due to its low iterative cost. However, SGD can not \nachieve linear convergence rate as FGD (Full Gradient Descent) because of the \ninherent gradient variance. To attack the problem, mini-batch SGD was proposed \nto get a trade-off in terms of convergence rate and iteration cost. In this \npaper, a general CVI (Convergence-Variance Inequality) equation is presented to \nstate formally the interaction of convergence rate and gradient variance. Then \na novel algorithm named SSAG (Stochastic Stratified Average Gradient) is \nintroduced to reduce gradient variance based on two techniques, stratified \nsampling and averaging over iterations that is a key idea in SAG (Stochastic \nAverage Gradient). Furthermore, SSAG can achieve linear convergence rate of \n$\\mathcal {O}((1-\\frac{\\mu}{8CL})^k)$ at smaller storage and iterative costs, \nwhere $C\\geq 2$ is the category number of training data. This convergence rate \ndepends mainly on the variance between classes, but not on the variance within \nthe classes. In the case of $C\\ll N$ ($N$ is the training data size), SSAG's \nconvergence rate is much better than SAG's convergence rate of $\\mathcal \n{O}((1-\\frac{\\mu}{8NL})^k)$. Our experimental results show SSAG outperforms SAG \nand many other algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763ad2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.07783"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Weichao Zhou, Wenchao Li", "title": "Safety-Aware Apprenticeship Learning. (arXiv:1710.07983v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.07983", "type": "text/html"}], "timestampUsec": "1512450843840984", "comments": [], "summary": {"content": "<p>Apprenticeship learning (AL) is a class of \"learning from demonstrations\" \ntechniques where the reward function of a Markov Decision Process (MDP) is \nunknown to the learning agent and the agent has to derive a good policy by \nobserving an expert's demonstrations. In this paper, we study the problem of \nhow to make AL algorithms inherently safe while still meeting its learning \nobjective. We consider a setting where the unknown reward function is assumed \nto be a linear combination of a set of state features, and the safety property \nis specified in Probabilistic Computation Tree Logic (PCTL). By embedding \nprobabilistic model checking inside AL, we propose a novel \ncounterexample-guided approach that can ensure both safety and performance of \nthe learned policy. We demonstrate the effectiveness of our approach on several \nchallenging AL scenarios where safety is essential. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763adb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.07983"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chong Huang, Peter Kairouz, Xiao Chen, Lalitha Sankar, Ram Rajagopal", "title": "Context-Aware Generative Adversarial Privacy. (arXiv:1710.09549v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.09549", "type": "text/html"}], "timestampUsec": "1512450843840983", "comments": [], "summary": {"content": "<p>Preserving the utility of published datasets while simultaneously providing \nprovable privacy guarantees is a well-known challenge. On the one hand, \ncontext-free privacy solutions, such as differential privacy, provide strong \nprivacy guarantees, but often lead to a significant reduction in utility. On \nthe other hand, context-aware privacy solutions, such as information theoretic \nprivacy, achieve an improved privacy-utility tradeoff, but assume that the data \nholder has access to dataset statistics. We circumvent these limitations by \nintroducing a novel context-aware privacy framework called generative \nadversarial privacy (GAP). GAP leverages recent advancements in generative \nadversarial networks (GANs) to allow the data holder to learn privatization \nschemes from the dataset itself. Under GAP, learning the privacy mechanism is \nformulated as a constrained minimax game between two players: a privatizer that \nsanitizes the dataset in a way that limits the risk of inference attacks on the \nindividuals' private variables, and an adversary that tries to infer the \nprivate variables from the sanitized dataset. To evaluate GAP's performance, we \ninvestigate two simple (yet canonical) statistical dataset models: (a) the \nbinary data model, and (b) the binary Gaussian mixture model. For both models, \nwe derive game-theoretically optimal minimax privacy mechanisms, and show that \nthe privacy mechanisms learned from data (in a generative adversarial fashion) \nmatch the theoretically optimal ones. This demonstrates that our framework can \nbe easily applied in practice, even in the absence of dataset statistics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763ae3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.09549"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anna Harutyunyan, Peter Vrancx, Pierre-Luc Bacon, Doina Precup, Ann Nowe", "title": "Learning with Options that Terminate Off-Policy. (arXiv:1711.03817v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.03817", "type": "text/html"}], "timestampUsec": "1512450843840982", "comments": [], "summary": {"content": "<p>A temporally abstract action, or an option, is specified by a policy and a \ntermination condition: the policy guides option behavior, and the termination \ncondition roughly determines its length. Generally, learning with longer \noptions (like learning with multi-step returns) is known to be more efficient. \nHowever, if the option set for the task is not ideal, and cannot express the \nprimitive optimal policy exactly, shorter options offer more flexibility and \ncan yield a better solution. Thus, the termination condition puts learning \nefficiency at odds with solution quality. We propose to resolve this dilemma by \ndecoupling the behavior and target terminations, just like it is done with \npolicies in off-policy learning. To this end, we give a new algorithm, \nQ(\\beta), that learns the solution with respect to any termination condition, \nregardless of how the options actually terminate. We derive Q(\\beta) by casting \nlearning with options into a common framework with well-studied multi-step \noff-policy learning. We validate our algorithm empirically, and show that it \nholds up to its motivating claims. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763ae9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.03817"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kory Wallace Mathewson, Piotr Mirowski", "title": "Improvised Comedy as a Turing Test. (arXiv:1711.08819v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08819", "type": "text/html"}], "timestampUsec": "1512450843840981", "comments": [], "summary": {"content": "<p>The best improvisational theatre actors can make any scene partner, of any \nskill level or ability, appear talented and proficient in the art form, and \nthus \"make them shine\". To challenge this improvisational paradigm, we built an \nartificial intelligence (AI) trained to perform live shows alongside human \nactors for human audiences. Over the course of 30 performances to a combined \naudience of almost 3000 people, we have refined theatrical games which involve \ncombinations of human and (at times, adversarial) AI actors. We have developed \nspecific scene structures to include audience participants in interesting ways. \nFinally, we developed a complete show structure that submitted the audience to \na Turing test and observed their suspension of disbelief, which we believe is \nkey for human/non-human theatre co-creation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763af2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08819"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "YoungJoon Yoo, Seonguk Park, Junyoung Choi, Sangdoo Yun, Nojun Kwak", "title": "Butterfly Effect: Bidirectional Control of Classification Performance by Small Additive Perturbation. (arXiv:1711.09681v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09681", "type": "text/html"}], "timestampUsec": "1512450843840980", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a85f100\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a85f100&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper proposes a new algorithm for controlling classification results by \ngenerating a small additive perturbation without changing the classifier \nnetwork. Our work is inspired by existing works generating adversarial \nperturbation that worsens classification performance. In contrast to the \nexisting methods, our work aims to generate perturbations that can enhance \noverall classification performance. To solve this performance enhancement \nproblem, we newly propose a perturbation generation network (PGN) influenced by \nthe adversarial learning strategy. In our problem, the information in a large \nexternal dataset is summarized by a small additive perturbation, which helps to \nimprove the performance of the classifier trained with the target dataset. In \naddition to this performance enhancement problem, we show that the proposed PGN \ncan be adopted to solve the classical adversarial problem without utilizing the \ninformation on the target classifier. The mentioned characteristics of our \nmethod are verified through extensive experiments on publicly available visual \ndatasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763af9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09681"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Clemente Rubio-Manzano, Tomas Lermanda Senoceain", "title": "How linguistic descriptions of data can help to the teaching-learning process in higher education, case of study: artificial intelligence. (arXiv:1711.09744v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09744", "type": "text/html"}], "timestampUsec": "1512450843840979", "comments": [], "summary": {"content": "<p>Artificial Intelligence is a central topic in the computer science \ncurriculum. From the year 2011 a project-based learning methodology based on \ncomputer games has been designed and implemented into the intelligence \nartificial course at the University of the Bio-Bio. The project aims to develop \nsoftware-controlled agents (bots) which are programmed by using heuristic \nalgorithms seen during the course. This methodology allows us to obtain good \nlearning results, however several challenges have been founded during its \nimplementation. \n</p> \n<p>In this paper we show how linguistic descriptions of data can help to provide \nstudents and teachers with technical and personalized feedback about the \nlearned algorithms. Algorithm behavior profile and a new Turing test for \ncomputer games bots based on linguistic modelling of complex phenomena are also \nproposed in order to deal with such challenges. \n</p> \n<p>In order to show and explore the possibilities of this new technology, a web \nplatform has been designed and implemented by one of authors and its \nincorporation in the process of assessment allows us to improve the teaching \nlearning process. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b00", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09744"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mohsen Ahmadi Fahandar, Eyke H&#xfc;llermeier", "title": "Learning to Rank based on Analogical Reasoning. (arXiv:1711.10207v1 [stat.ML] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10207", "type": "text/html"}], "timestampUsec": "1512450843840977", "comments": [], "summary": {"content": "<p>Object ranking or \"learning to rank\" is an important problem in the realm of \npreference learning. On the basis of training data in the form of a set of \nrankings of objects represented as feature vectors, the goal is to learn a \nranking function that predicts a linear order of any new set of objects. In \nthis paper, we propose a new approach to object ranking based on principles of \nanalogical reasoning. More specifically, our inference pattern is formalized in \nterms of so-called analogical proportions and can be summarized as follows: \nGiven objects $A,B,C,D$, if object $A$ is known to be preferred to $B$, and $C$ \nrelates to $D$ as $A$ relates to $B$, then $C$ is (supposedly) preferred to \n$D$. Our method applies this pattern as a main building block and combines it \nwith ideas and techniques from instance-based learning and rank aggregation. \nBased on first experimental results for data sets from various domains (sports, \neducation, tourism, etc.), we conclude that our approach is highly competitive. \nIt appears to be specifically interesting in situations in which the objects \nare coming from different subdomains, and which hence require a kind of \nknowledge transfer. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b08", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10207"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Samaneh Nasiri Ghosheh Bolagh, Gari. D. Clifford", "title": "Subject Selection on a Riemannian Manifold for Unsupervised Cross-subject Seizure Detection. (arXiv:1712.00465v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00465", "type": "text/html"}], "timestampUsec": "1512450843840976", "comments": [], "summary": {"content": "<p>Inter-subject variability between individuals poses a challenge in \ninter-subject brain signal analysis problems. A new algorithm for \nsubject-selection based on clustering covariance matrices on a Riemannian \nmanifold is proposed. After unsupervised selection of the subsets of relevant \nsubjects, data in a cluster is mapped to a tangent space at the mean point of \ncovariance matrices in that cluster and an SVM classifier on labeled data from \nrelevant subjects is trained. Experiment on an EEG seizure database shows that \nthe proposed method increases the accuracy over state-of-the-art from 86.83% to \n89.84% and specificity from 87.38% to 89.64% while reducing the false positive \nrate/hour from 0.8/hour to 0.77/hour. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b11", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00465"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hasham Ul Haq, Rameel Ahmad, Sibt Ul Hussain", "title": "Intelligent EHRs: Predicting Procedure Codes From Diagnosis Codes. (arXiv:1712.00481v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00481", "type": "text/html"}], "timestampUsec": "1512450843840975", "comments": [], "summary": {"content": "<p>In order to submit a claim to insurance companies, a doctor needs to code a \npatient encounter with both the diagnosis (ICDs) and procedures performed \n(CPTs) in an Electronic Health Record (EHR). Identifying and applying relevant \nprocedures code is a cumbersome and time-consuming task as a doctor has to \nchoose from around 13,000 procedure codes with no predefined one-to-one \nmapping. In this paper, we propose a state-of-the-art deep learning method for \nautomatic and intelligent coding of procedures (CPTs) from the diagnosis codes \n(ICDs) entered by the doctor. Precisely, we cast the learning problem as a \nmulti-label classification problem and use distributed representation to learn \nthe input mapping of high-dimensional sparse ICDs codes. Our final model \ntrained on 2.3 million claims is able to outperform existing rule-based \nprobabilistic and association-rule mining based methods and has a recall of \n90@3. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b1f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00481"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Michael C. Hughes, Gabriel Hope, Leah Weiner, Thomas H. McCoy, Roy H. Perlis, Erik B. Sudderth, Finale Doshi-Velez", "title": "Prediction-Constrained Topic Models for Antidepressant Recommendation. (arXiv:1712.00499v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00499", "type": "text/html"}], "timestampUsec": "1512450843840974", "comments": [], "summary": {"content": "<p>Supervisory signals can help topic models discover low-dimensional data \nrepresentations that are more interpretable for clinical tasks. We propose a \nframework for training supervised latent Dirichlet allocation that balances two \ngoals: faithful generative explanations of high-dimensional data and accurate \nprediction of associated class labels. Existing approaches fail to balance \nthese goals by not properly handling a fundamental asymmetry: the intended task \nis always predicting labels from data, not data from labels. Our new \nprediction-constrained objective trains models that predict labels from heldout \ndata well while also producing good generative likelihoods and interpretable \ntopic-word parameters. In a case study on predicting depression medications \nfrom electronic health records, we demonstrate improved recommendations \ncompared to previous supervised topic models and high- dimensional logistic \nregression from words alone. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b29", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00499"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rui Luo, Weinan Zhang, Xiaojun Xu, Jun Wang", "title": "A Neural Stochastic Volatility Model. (arXiv:1712.00504v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00504", "type": "text/html"}], "timestampUsec": "1512450843840973", "comments": [], "summary": {"content": "<p>In this paper, we show that the recent integration of statistical models with \ndeep recurrent neural networks provides a new way of formulating volatility \n(the degree of variation of time series) models that have been widely used in \ntime series analysis and prediction in finance. The model comprises a pair of \ncomplementary stochastic recurrent neural networks: the generative network \nmodels the joint distribution of the stochastic volatility process; the \ninference network approximates the conditional distribution of the latent \nvariables given the observables. Our focus here is on the formulation of \ntemporal dynamics of volatility over time under a stochastic recurrent neural \nnetwork framework. Experiments on real-world stock price datasets demonstrate \nthat the proposed model generates a better volatility estimation and prediction \nthat outperforms stronge baseline methods, including the deterministic models, \nsuch as GARCH and its variants, and the stochastic MCMC-based models, and the \nGaussian-process-based, on the average negative log-likelihood measure. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00504"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sunho Park, Tae Hyun Hwang", "title": "Bayesian Semi-nonnegative Tri-matrix Factorization to Identify Pathways Associated with Cancer Types. (arXiv:1712.00520v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00520", "type": "text/html"}], "timestampUsec": "1512450843840972", "comments": [], "summary": {"content": "<p>Identifying altered pathways that are associated with specific cancer types \ncan potentially bring a significant impact on cancer patient treatment. \nAccurate identification of such key altered pathways information can be used to \ndevelop novel therapeutic agents as well as to understand the molecular \nmechanisms of various types of cancers better. Tri-matrix factorization is an \nefficient tool to learn associations between two different entities (e.g., \ncancer types and pathways in our case) from data. To successfully apply \ntri-matrix factorization methods to biomedical problems, biological prior \nknowledge such as pathway databases or protein-protein interaction (PPI) \nnetworks, should be taken into account in the factorization model. However, it \nis not straightforward in the Bayesian setting even though Bayesian methods are \nmore appealing than point estimate methods, such as a maximum likelihood or a \nmaximum posterior method, in the sense that they calculate distributions over \nvariables and are robust against overfitting. We propose a Bayesian \n(semi-)nonnegative matrix factorization model for human cancer genomic data, \nwhere the biological prior knowledge represented by a pathway database and a \nPPI network is taken into account in the factorization model through a finite \ndependent Beta-Bernoulli prior. We tested our method on The Cancer Genome Atlas \n(TCGA) dataset and found that the pathways identified by our method can be used \nas a prognostic biomarkers for patient subgroup identification. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b38", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00520"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "George H. Chen, Jeremy C. Weiss", "title": "Survival-Supervised Topic Modeling with Anchor Words: Characterizing Pancreatitis Outcomes. (arXiv:1712.00535v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00535", "type": "text/html"}], "timestampUsec": "1512450843840971", "comments": [], "summary": {"content": "<p>We introduce a new approach for topic modeling that is supervised by survival \nanalysis. Specifically, we build on recent work on unsupervised topic modeling \nwith so-called anchor words by providing supervision through an elastic-net \nregularized Cox proportional hazards model. In short, an anchor word being \npresent in a document provides strong indication that the document is partially \nabout a specific topic. For example, by seeing \"gallstones\" in a document, we \nare fairly certain that the document is partially about medicine. Our proposed \nmethod alternates between learning a topic model and learning a survival model \nto find a local minimum of a block convex optimization problem. We apply our \nproposed approach to predicting how long patients with pancreatitis admitted to \nan intensive care unit (ICU) will stay in the ICU. Our approach is as accurate \nas the best of a variety of baselines while being more interpretable than any \nof the baselines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b41", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00535"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chen Fang, Panuwat Janwattanapong, Chunfei Li, Malek Adjouadi", "title": "A global feature extraction model for the effective computer aided diagnosis of mild cognitive impairment using structural MRI images. (arXiv:1712.00556v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00556", "type": "text/html"}], "timestampUsec": "1512450843840970", "comments": [], "summary": {"content": "<p>Multiple modalities of biomarkers have been proved to be very sensitive in \nassessing the progression of Alzheimer's disease (AD), and using these \nmodalities and machine learning algorithms, several approaches have been \nproposed to assist in the early diagnosis of AD. Among the recent investigated \nstate-of-the-art approaches, Gaussian discriminant analysis (GDA)-based \napproaches have been demonstrated to be more effective and accurate in the \nclassification of AD, especially for delineating its prodromal stage of mild \ncognitive impairment (MCI). Moreover, among those binary classification \ninvestigations, the local feature extraction methods were mostly used, which \nmade them hardly be applied to a practical computer aided diagnosis system. \nTherefore, this study presents a novel global feature extraction model taking \nadvantage of the recent proposed GDA-based dual high-dimensional decision \nspaces, which can significantly improve the early diagnosis performance \ncomparing to those local feature extraction methods. In the true test using 20% \nheld-out data, for discriminating the most challenging MCI group from the \ncognitively normal control (CN) group, an F1 score of 91.06%, an accuracy of \n88.78%, a sensitivity of 91.80%, and a specificity of 83.78% were achieved that \ncan be considered as the best performance obtained so far. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b53", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00556"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chanh Nguyen, Georgi Georgiev, Yujie Ji, Ting Wang", "title": "Where Classification Fails, Interpretation Rises. (arXiv:1712.00558v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00558", "type": "text/html"}], "timestampUsec": "1512450843840969", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a85f443\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a85f443&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>An intriguing property of deep neural networks is their inherent \nvulnerability to adversarial inputs, which significantly hinders their \napplication in security-critical domains. Most existing detection methods \nattempt to use carefully engineered patterns to distinguish adversarial inputs \nfrom their genuine counterparts, which however can often be circumvented by \nadaptive adversaries. In this work, we take a completely different route by \nleveraging the definition of adversarial inputs: while deceiving for deep \nneural networks, they are barely discernible for human visions. Building upon \nrecent advances in interpretable models, we construct a new detection framework \nthat contrasts an input's interpretation against its classification. We \nvalidate the efficacy of this framework through extensive experiments using \nbenchmark datasets and attacks. We believe that this work opens a new direction \nfor designing adversarial input detection methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b5b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00558"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, Kevin Murphy", "title": "Progressive Neural Architecture Search. (arXiv:1712.00559v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.00559", "type": "text/html"}], "timestampUsec": "1512450843840968", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a8ad722\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a8ad722&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a method for learning CNN structures that is more efficient than \nprevious approaches: instead of using reinforcement learning (RL) or genetic \nalgorithms (GA), we use a sequential model-based optimization (SMBO) strategy, \nin which we search for architectures in order of increasing complexity, while \nsimultaneously learning a surrogate function to guide the search, similar to A* \nsearch. On the CIFAR-10 dataset, our method finds a CNN structure with the same \nclassification accuracy (3.41% error rate) as the RL method of Zoph et al. \n(2017), but 2 times faster (in terms of number of models evaluated). It also \noutperforms the GA method of Liu et al. (2017), which finds a model with worse \nperformance (3.63% error rate), and takes 5 times longer. Finally we show that \nthe model we learned on CIFAR also works well at the task of ImageNet \nclassification. In particular, we match the state-of-the-art performance of \n82.9% top-1 and 96.1% top-5 accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b64", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00559"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gabriel Erion, Hugh Chen, Scott M. Lundberg, Su-In Lee", "title": "Anesthesiologist-level forecasting of hypoxemia with only SpO2 data using deep learning. (arXiv:1712.00563v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00563", "type": "text/html"}], "timestampUsec": "1512450843840967", "comments": [], "summary": {"content": "<p>We use a deep learning model trained only on a patient's blood oxygenation \ndata (measurable with an inexpensive fingertip sensor) to predict impending \nhypoxemia (low blood oxygen) more accurately than trained anesthesiologists \nwith access to all the data recorded in a modern operating room. We also \nprovide a simple way to visualize the reason why a patient's risk is low or \nhigh by assigning weight to the patient's past blood oxygen values. This work \nhas the potential to provide cutting-edge clinical decision support in \nlow-resource settings, where rates of surgical complication and death are \nsubstantially greater than in high-resource areas. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b7d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00563"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zihao Hu, Xiyi Luo, Hongtao Lu, Yong Yu", "title": "Supervised Hashing based on Energy Minimization. (arXiv:1712.00573v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00573", "type": "text/html"}], "timestampUsec": "1512450843840966", "comments": [], "summary": {"content": "<p>Recently, supervised hashing methods have attracted much attention since they \ncan optimize retrieval speed and storage cost while preserving semantic \ninformation. Because hashing codes learning is NP-hard, many methods resort to \nsome form of relaxation technique. But the performance of these methods can \neasily deteriorate due to the relaxation. Luckily, many supervised hashing \nformulations can be viewed as energy functions, hence solving hashing codes is \nequivalent to learning marginals in the corresponding conditional random field \n(CRF). By minimizing the KL divergence between a fully factorized distribution \nand the Gibbs distribution of this CRF, a set of consistency equations can be \nobtained, but updating them in parallel may not yield a local optimum since the \nvariational lower bound is not guaranteed to increase. In this paper, we use a \nlinear approximation of the sigmoid function to convert these consistency \nequations to linear systems, which have a closed-form solution. By applying \nthis novel technique to two classical hashing formulations KSH and SPLH, we \nobtain two new methods called EM (energy minimizing based)-KSH and EM-SPLH. \nExperimental results on three datasets show the superiority of our methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b8c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00573"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Maggie Makar, Marzyeh Ghassemi, David Cutler, Ziad Obermeyer", "title": "Short-term Mortality Prediction for Elderly Patients Using Medicare Claims Data. (arXiv:1712.00644v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00644", "type": "text/html"}], "timestampUsec": "1512450843840965", "comments": [], "summary": {"content": "<p>Risk prediction is central to both clinical medicine and public health. While \nmany machine learning models have been developed to predict mortality, they are \nrarely applied in the clinical literature, where classification tasks typically \nrely on logistic regression. One reason for this is that existing machine \nlearning models often seek to optimize predictions by incorporating features \nthat are not present in the databases readily available to providers and policy \nmakers, limiting generalizability and implementation. Here we tested a number \nof machine learning classifiers for prediction of six-month mortality in a \npopulation of elderly Medicare beneficiaries, using an administrative claims \ndatabase of the kind available to the majority of health care payers and \nproviders. We show that machine learning classifiers substantially outperform \ncurrent widely-used methods of risk prediction but only when used with an \nimproved feature set incorporating insights from clinical medicine, developed \nfor this study. Our work has applications to supporting patient and provider \ndecision making at the end of life, as well as population health-oriented \nefforts to identify patients at high risk of poor outcomes. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b96", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00644"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xuanqing Liu, Minhao Cheng, Huan Zhang, Cho-Jui Hsieh", "title": "Towards Robust Neural Networks via Random Self-ensemble. (arXiv:1712.00673v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00673", "type": "text/html"}], "timestampUsec": "1512450843840964", "comments": [], "summary": {"content": "<p>Recent studies have revealed the vulnerability of deep neural networks - A \nsmall adversarial perturbation that is imperceptible to human can easily make a \nwell-trained deep neural network mis-classify. This makes it unsafe to apply \nneural networks in security-critical applications. In this paper, we propose a \nnew defensive algorithm called Random Self-Ensemble (RSE) by combining two \nimportant concepts: ${\\bf randomness}$ and ${\\bf ensemble}$. To protect a \ntargeted model, RSE adds random noise layers to the neural network to prevent \nfrom state-of-the-art gradient-based attacks, and ensembles the prediction over \nrandom noises to stabilize the performance. We show that our algorithm is \nequivalent to ensemble an infinite number of noisy models $f_\\epsilon$ without \nany additional memory overhead, and the proposed training procedure based on \nnoisy stochastic gradient descent can ensure the ensemble model has good \npredictive capability. Our algorithm significantly outperforms previous defense \ntechniques on real datasets. For instance, on CIFAR-10 with VGG network (which \nhas $92\\%$ accuracy without any attack), under the state-of-the-art C&amp;W attack \nwithin a certain distortion tolerance, the accuracy of unprotected model drops \nto less than $10\\%$, the best previous defense technique has $48\\%$ accuracy, \nwhile our method still has $86\\%$ prediction accuracy under the same level of \nattack. Finally, our method is simple and easy to integrate into any neural \nnetwork. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00673"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513669231, "author": "Frans A. Oliehoek, Rahul Savani, Jose Gallego-Posada, Elise van der Pol, Edwin D. de Jong, Roderich Gross", "title": "GANGs: Generative Adversarial Network Games. (arXiv:1712.00679v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00679", "type": "text/html"}], "timestampUsec": "1512450843840963", "comments": [], "summary": {"content": "<p>Generative Adversarial Networks (GAN) have become one of the most successful \nframeworks for unsupervised generative modeling. As GANs are difficult to train \nmuch research has focused on this. However, very little of this research has \ndirectly exploited game-theoretic techniques. We introduce Generative \nAdversarial Network Games (GANGs), which explicitly model a finite zero-sum \ngame between a generator ($G$) and classifier ($C$) that use mixed strategies. \nThe size of these games precludes exact solution methods, therefore we define \nresource-bounded best responses (RBBRs), and a resource-bounded Nash \nEquilibrium (RB-NE) as a pair of mixed strategies such that neither $G$ or $C$ \ncan find a better RBBR. The RB-NE solution concept is richer than the notion of \n`local Nash equilibria' in that it captures not only failures of escaping local \noptima of gradient descent, but applies to any approximate best response \ncomputations, including methods with random restarts. To validate our approach, \nwe solve GANGs with the Parallel Nash Memory algorithm, which provably \nmonotonically converges to an RB-NE. We compare our results to standard GAN \nsetups, and demonstrate that our method deals well with typical GAN problems \nsuch as mode collapse, partial mode coverage and forgetting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763baa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00679"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rajeev Ranjan, Swami Sankaranarayanan, Carlos D. Castillo, Rama Chellappa", "title": "Improving Network Robustness against Adversarial Attacks with Compact Convolution. (arXiv:1712.00699v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00699", "type": "text/html"}], "timestampUsec": "1512450843840962", "comments": [], "summary": {"content": "<p>Though Convolutional Neural Networks (CNNs) have surpassed human-level \nperformance on tasks such as object classification and face verification, they \ncan easily be fooled by adversarial attacks. These attacks add a small \nperturbation to the input image that causes the network to mis-classify the \nsample. In this paper, we focus on neutralizing adversarial attacks by \nexploring the effect of different loss functions such as CenterLoss and \nL2-Softmax Loss for enhanced robustness to adversarial perturbations. \nAdditionally, we propose power convolution, a novel method of convolution that \nwhen incorporated in conventional CNNs improve their robustness. Power \nconvolution ensures that features at every layer are bounded and close to each \nother. Extensive experiments show that Power Convolutional Networks (PCNs) \nneutralize multiple types of attacks, and perform better than existing methods \nfor defending adversarial attacks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763bc5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00699"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Qing Qu, Yuqian Zhang, Yonina C. Eldar, John Wright", "title": "Convolutional Phase Retrieval via Gradient Descent. (arXiv:1712.00716v1 [stat.CO])", "alternate": [{"href": "http://arxiv.org/abs/1712.00716", "type": "text/html"}], "timestampUsec": "1512450843840961", "comments": [], "summary": {"content": "<p>We study the convolutional phase retrieval problem, which considers \nrecovering an unknown signal $\\mathbf x \\in \\mathbb C^n $ from $m$ measurements \nconsisting of the magnitude of its cyclic convolution with a known kernel \n$\\mathbf a \\in \\mathbb C^m $. This model is motivated by applications such as \nchannel estimation, optics, and underwater acoustic communication, where the \nsignal of interest is acted on by a given channel/filter, and phase information \nis difficult or impossible to acquire. We show that when $\\mathbf a$ is random \nand the sample number $m$ is sufficiently large, with high probability $\\mathbf \nx$ can be efficiently recovered up to a global phase using a combination of \nspectral initialization and generalized gradient descent. The main challenge is \ncoping with dependencies in the measurement operator. We overcome this \nchallenge by using ideas from decoupling theory, suprema of chaos processes and \nthe restricted isometry property of random circulant matrices, and recent \nanalysis for alternating minimization methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763bd3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00716"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hongwei Wang, Jia Wang, Miao Zhao, Jiannong Cao, Minyi Guo", "title": "Joint Topic-Semantic-aware Social Recommendation for Online Voting. (arXiv:1712.00731v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00731", "type": "text/html"}], "timestampUsec": "1512450843840960", "comments": [], "summary": {"content": "<p>Online voting is an emerging feature in social networks, in which users can \nexpress their attitudes toward various issues and show their unique interest. \nOnline voting imposes new challenges on recommendation, because the propagation \nof votings heavily depends on the structure of social networks as well as the \ncontent of votings. In this paper, we investigate how to utilize these two \nfactors in a comprehensive manner when doing voting recommendation. First, due \nto the fact that existing text mining methods such as topic model and semantic \nmodel cannot well process the content of votings that is typically short and \nambiguous, we propose a novel Topic-Enhanced Word Embedding (TEWE) method to \nlearn word and document representation by jointly considering their topics and \nsemantics. Then we propose our Joint Topic-Semantic-aware social Matrix \nFactorization (JTS-MF) model for voting recommendation. JTS-MF model calculates \nsimilarity among users and votings by combining their TEWE representation and \nstructural information of social networks, and preserves this \ntopic-semantic-social similarity during matrix factorization. To evaluate the \nperformance of TEWE representation and JTS-MF model, we conduct extensive \nexperiments on real online voting dataset. The results prove the efficacy of \nour approach against several state-of-the-art baselines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763bd8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00731"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hongwei Wang, Fuzheng Zhang, Min Hou, Xing Xie, Minyi Guo, Qi Liu", "title": "SHINE: Signed Heterogeneous Information Network Embedding for Sentiment Link Prediction. (arXiv:1712.00732v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00732", "type": "text/html"}], "timestampUsec": "1512450843840959", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a8ad963\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a8ad963&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In online social networks people often express attitudes towards others, \nwhich forms massive sentiment links among users. Predicting the sign of \nsentiment links is a fundamental task in many areas such as personal \nadvertising and public opinion analysis. Previous works mainly focus on textual \nsentiment classification, however, text information can only disclose the \"tip \nof the iceberg\" about users' true opinions, of which the most are unobserved \nbut implied by other sources of information such as social relation and users' \nprofile. To address this problem, in this paper we investigate how to predict \npossibly existing sentiment links in the presence of heterogeneous information. \nFirst, due to the lack of explicit sentiment links in mainstream social \nnetworks, we establish a labeled heterogeneous sentiment dataset which consists \nof users' sentiment relation, social relation and profile knowledge by \nentity-level sentiment extraction method. Then we propose a novel and flexible \nend-to-end Signed Heterogeneous Information Network Embedding (SHINE) framework \nto extract users' latent representations from heterogeneous networks and \npredict the sign of unobserved sentiment links. SHINE utilizes multiple deep \nautoencoders to map each user into a low-dimension feature space while \npreserving the network structure. We demonstrate the superiority of SHINE over \nstate-of-the-art baselines on link prediction and node recommendation in two \nreal-world datasets. The experimental results also prove the efficacy of SHINE \nin cold start scenario. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763be1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00732"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wenqi Wang, Vaneet Aggarwal, Shuchin Aeron", "title": "Tensor Train Neighborhood Preserving Embedding. (arXiv:1712.00828v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00828", "type": "text/html"}], "timestampUsec": "1512450843840958", "comments": [], "summary": {"content": "<p>In this paper, we propose a Tensor Train Neighborhood Preserving Embedding \n(TTNPE) to embed multi-dimensional tensor data into low dimensional tensor \nsubspace. Novel approaches to solve the optimization problem in TTNPE are \nproposed. For this embedding, we evaluate novel trade-off gain among \nclassification, computation, and dimensionality reduction (storage) for \nsupervised learning. It is shown that compared to the state-of-the-arts tensor \nembedding methods, TTNPE achieves superior trade-off in classification, \ncomputation, and dimensionality reduction in MNIST handwritten digits and \nWeizmann face datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763bf6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00828"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Matthew Kupilik, Frank Witmer, Euan-Angus MacLeod, Caixia Wang, Tom Ravens", "title": "Gaussian Process Regression for Arctic Coastal Erosion Forecasting. (arXiv:1712.00867v1 [physics.geo-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.00867", "type": "text/html"}], "timestampUsec": "1512450843840957", "comments": [], "summary": {"content": "<p>Arctic coastal morphology is governed by multiple factors, many of which are \naffected by climatological changes. As the season length for shorefast ice \ndecreases and temperatures warm permafrost soils, coastlines are more \nsusceptible to erosion from storm waves. Such coastal erosion is a concern, \nsince the majority of the population centers and infrastructure in the Arctic \nare located near the coasts. Stakeholders and decision makers increasingly need \nmodels capable of scenario-based predictions to assess and mitigate the effects \nof coastal morphology on infrastructure and land use. Our research uses \nGaussian process models to forecast Arctic coastal erosion along the Beaufort \nSea near Drew Point, AK. Gaussian process regression is a data-driven modeling \nmethodology capable of extracting patterns and trends from data-sparse \nenvironments such as remote Arctic coastlines. To train our model, we use \nannual coastline positions and near-shore summer temperature averages from \nexisting datasets and extend these data by extracting additional coastlines \nfrom satellite imagery. We combine our calibrated models with future climate \nmodels to generate a range of plausible future erosion scenarios. Our results \nshow that the Gaussian process methodology substantially improves yearly \npredictions compared to linear and nonlinear least squares methods, and is \ncapable of generating detailed forecasts suitable for use by decision makers. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763c11", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00867"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512537806, "author": "Mostafa Rahmani, George Atia", "title": "Data Dropout in Arbitrary Basis for Deep Network Regularization. (arXiv:1712.00891v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00891", "type": "text/html"}], "timestampUsec": "1512450843840956", "comments": [], "summary": {"content": "<p>An important problem in training deep networks with high capacity is to \nensure that the trained network works well when presented with new inputs \noutside the training dataset. Dropout is an effective regularization technique \nto boost the network generalization in which a random subset of the elements of \nthe given data and the extracted features are set to zero during the training \nprocess. In this paper, a new randomized regularization technique in which we \nwithhold a random part of the data without necessarily turning off the \nneurons/data-elements is proposed. In the proposed method, of which the \nconventional dropout is shown to be a special case, random data dropout is \nperformed in an arbitrary basis, hence the designation Generalized Dropout. We \nalso present a framework whereby the proposed technique can be applied \nefficiently to convolutional neural networks. The presented numerical \nexperiments demonstrate that the proposed technique yields notable performance \ngain. Generalized Dropout provides new insight into the idea of dropout, shows \nthat we can achieve different performance gains by using different bases \nmatrices, and opens up a new research question as of how to choose optimal \nbases matrices that achieve maximal performance gain. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763c29", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00891"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513098583, "author": "Giambattista Parascandolo, Mateo Rojas-Carulla, Niki Kilbertus, Bernhard Sch&#xf6;lkopf", "title": "Learning Independent Causal Mechanisms. (arXiv:1712.00961v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00961", "type": "text/html"}], "timestampUsec": "1512450843840955", "comments": [], "summary": {"content": "<p>Independent causal mechanisms are a central concept in the study of causality \nwith implications for machine learning tasks. In this work we develop an \nalgorithm to recover a set of (inverse) independent mechanisms relating a \ndistribution transformed by the mechanisms to a reference distribution. The \napproach is fully unsupervised and based on a set of experts that compete for \ndata to specialize and extract the mechanisms. We test and analyze the proposed \nmethod on a series of experiments based on image transformations. Each expert \nsuccessfully maps a subset of the transformed data to the original domain, and \nthe learned mechanisms generalize to other domains. We discuss implications for \ndomain transfer and links to recent trends in generative modeling. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1513098583, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763c3f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00961"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Emanuele Pesce, Petros-Pavlos Ypsilantis, Samuel Withey, Robert Bakewell, Vicky Goh, Giovanni Montana", "title": "Learning to detect chest radiographs containing lung nodules using visual attention networks. (arXiv:1712.00996v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00996", "type": "text/html"}], "timestampUsec": "1512450843840954", "comments": [], "summary": {"content": "<p>Machine learning approaches hold great potential for the automated detection \nof lung nodules in chest radiographs, but training the algorithms requires vary \nlarge amounts of manually annotated images, which are difficult to obtain. Weak \nlabels indicating whether a radiograph is likely to contain pulmonary nodules \nare typically easier to obtain at scale by parsing historical free-text \nradiological reports associated to the radiographs. Using a repositotory of \nover 700,000 chest radiographs, in this study we demonstrate that promising \nnodule detection performance can be achieved using weak labels through \nconvolutional neural networks for radiograph classification. We propose two \nnetwork architectures for the classification of images likely to contain \npulmonary nodules using both weak labels and manually-delineated bounding \nboxes, when these are available. Annotated nodules are used at training time to \ndeliver a visual attention mechanism informing the model about its localisation \nperformance. The first architecture extracts saliency maps from high-level \nconvolutional layers and compares the estimated position of a nodule against \nthe ground truth, when this is available. A corresponding localisation error is \nthen back-propagated along with the softmax classification error. The second \napproach consists of a recurrent attention model that learns to observe a short \nsequence of smaller image portions through reinforcement learning. When a \nnodule annotation is available at training time, the reward function is \nmodified accordingly so that exploring portions of the radiographs away from a \nnodule incurs a larger penalty. Our empirical results demonstrate the potential \nadvantages of these architectures in comparison to competing methodologies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763c4b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00996"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yi Xu, Rong Jin, Tianbao Yang", "title": "NEON+: Accelerated Gradient Methods for Extracting Negative Curvature for Non-Convex Optimization. (arXiv:1712.01033v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1712.01033", "type": "text/html"}], "timestampUsec": "1512450843840953", "comments": [], "summary": {"content": "<p>Accelerated gradient (AG) methods are breakthroughs in convex optimization, \nimproving the convergence rate of the gradient descent method for optimization \nwith smooth functions. However, the analysis of AG methods for non-convex \noptimization is still limited. It remains an open question whether AG methods \nfrom convex optimization can accelerate the convergence of the gradient descent \nmethod for finding local minimum of non-convex optimization problems. This \npaper provides an affirmative answer to this question. In particular, we \nanalyze two renowned variants of AG methods (namely Polyak's Heavy Ball method \nand Nesterov's Accelerated Gradient method) for extracting the negative \ncurvature from random noise, which is central to escaping from saddle points. \nBy leveraging the proposed AG methods for extracting the negative curvature, we \npresent a new AG algorithm with double loops for non-convex \noptimization~\\footnote{this is in contrast to a single-loop AG algorithm \nproposed in a recent manuscript~\\citep{AGNON}, which directly analyzed the \nNesterov's AG method for non-convex optimization and appeared online on \nNovember 29, 2017. However, we emphasize that our work is an independent work, \nwhich is inspired by our earlier work~\\citep{NEON17} and is based on a \ndifferent novel analysis.}, which converges to second-order stationary point \n$\\x$ such that $\\|\\nabla f(\\x)\\|\\leq \\epsilon$ and $\\nabla^2 f(\\x)\\geq \n-\\sqrt{\\epsilon} I$ with $\\widetilde O(1/\\epsilon^{1.75})$ iteration \ncomplexity, improving that of gradient descent method by a factor of \n$\\epsilon^{-0.25}$ and matching the best iteration complexity of second-order \nHessian-free methods for non-convex optimization. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763c74", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01033"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mohammad Emtiyaz Khan, Zuozhu Liu, Voot Tangkaratt, Yarin Gal", "title": "Vprop: Variational Inference using RMSprop. (arXiv:1712.01038v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01038", "type": "text/html"}], "timestampUsec": "1512450843840952", "comments": [], "summary": {"content": "<p>Many computationally-efficient methods for Bayesian deep learning rely on \ncontinuous optimization algorithms, but the implementation of these methods \nrequires significant changes to existing code-bases. In this paper, we propose \nVprop, a method for Gaussian variational inference that can be implemented with \ntwo minor changes to the off-the-shelf RMSprop optimizer. Vprop also reduces \nthe memory requirements of Black-Box Variational Inference by half. We derive \nVprop using the conjugate-computation variational inference method, and \nestablish its connections to Newton's method, natural-gradient methods, and \nextended Kalman filters. Overall, this paper presents Vprop as a principled, \ncomputationally-efficient, and easy-to-implement method for Bayesian deep \nlearning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763c94", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01038"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, Pascal Frossard", "title": "Adaptive Quantization for Deep Neural Network. (arXiv:1712.01048v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01048", "type": "text/html"}], "timestampUsec": "1512450843840951", "comments": [], "summary": {"content": "<p>In recent years Deep Neural Networks (DNNs) have been rapidly developed in \nvarious applications, together with increasingly complex architectures. The \nperformance gain of these DNNs generally comes with high computational costs \nand large memory consumption, which may not be affordable for mobile platforms. \nDeep model quantization can be used for reducing the computation and memory \ncosts of DNNs, and deploying complex DNNs on mobile equipment. In this work, we \npropose an optimization framework for deep model quantization. First, we \npropose a measurement to estimate the effect of parameter quantization errors \nin individual layers on the overall model prediction accuracy. Then, we propose \nan optimization process based on this measurement for finding optimal \nquantization bit-width for each layer. This is the first work that \ntheoretically analyse the relationship between parameter quantization errors of \nindividual layers and model accuracy. Our new quantization algorithm \noutperforms previous quantization optimization methods, and achieves 20-40% \nhigher compression rate compared to equal bit-width quantization at the same \nmodel prediction accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763ca9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01048"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Muhammad Raza Khan, Joshua Blumenstock", "title": "Determinants of Mobile Money Adoption in Pakistan. (arXiv:1712.01081v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01081", "type": "text/html"}], "timestampUsec": "1512450843840950", "comments": [], "summary": {"content": "<p>In this work, we analyze the problem of adoption of mobile money in Pakistan \nby using the call detail records of a major telecom company as our input. Our \nresults highlight the fact that different sections of the society have \ndifferent patterns of adoption of digital financial services but user mobility \nrelated features are the most important one when it comes to adopting and using \nmobile money services. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763cbe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01081"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Dieter Hendricks, Adam Cobb, Richard Everett, Jonathan Downing, Stephen J. Roberts", "title": "Inferring agent objectives at different scales of a complex adaptive system. (arXiv:1712.01137v1 [q-fin.TR])", "alternate": [{"href": "http://arxiv.org/abs/1712.01137", "type": "text/html"}], "timestampUsec": "1512450843840949", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a8adb37\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a8adb37&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We introduce a framework to study the effective objectives at different time \nscales of financial market microstructure. The financial market can be regarded \nas a complex adaptive system, where purposeful agents collectively and \nsimultaneously create and perceive their environment as they interact with it. \nIt has been suggested that multiple agent classes operate in this system, with \na non-trivial hierarchy of top-down and bottom-up causation classes with \ndifferent effective models governing each level. We conjecture that agent \nclasses may in fact operate at different time scales and thus act differently \nin response to the same perceived market state. Given scale-specific temporal \nstate trajectories and action sequences estimated from aggregate market \nbehaviour, we use Inverse Reinforcement Learning to compute the effective \nreward function for the aggregate agent class at each scale, allowing us to \nassess the relative attractiveness of feature vectors across different scales. \nDifferences in reward functions for feature vectors may indicate different \nobjectives of market participants, which could assist in finding the scale \nboundary for agent classes. This has implications for learning algorithms \noperating in this domain. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763cd5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01137"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Abdul-Saboor Sheikh, Kashif Rasul, Andreas Merentitis, Urs Bergmann", "title": "Stochastic Maximum Likelihood Optimization via Hypernetworks. (arXiv:1712.01141v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01141", "type": "text/html"}], "timestampUsec": "1512450843840948", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a90ec15\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a90ec15&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This work explores maximum likelihood optimization of neural networks through \nhypernetworks. A hypernetwork initializes the weights of another network, which \nin turn can be employed for typical functional tasks such as regression and \nclassification. We optimize hypernetworks to directly maximize the conditional \nlikelihood of target variables given input. Using this approach we obtain \ncompetitive empirical results on regression and classification benchmarks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763cdf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01141"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mohsen Ahmadi Fahandar, Eyke H&#xfc;llermeier, In&#xe9;s Couso", "title": "Statistical Inference for Incomplete Ranking Data: The Case of Rank-Dependent Coarsening. (arXiv:1712.01158v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01158", "type": "text/html"}], "timestampUsec": "1512450843840947", "comments": [], "summary": {"content": "<p>We consider the problem of statistical inference for ranking data, \nspecifically rank aggregation, under the assumption that samples are incomplete \nin the sense of not comprising all choice alternatives. In contrast to most \nexisting methods, we explicitly model the process of turning a full ranking \ninto an incomplete one, which we call the coarsening process. To this end, we \npropose the concept of rank-dependent coarsening, which assumes that incomplete \nrankings are produced by projecting a full ranking to a random subset of ranks. \nFor a concrete instantiation of our model, in which full rankings are drawn \nfrom a Plackett-Luce distribution and observations take the form of pairwise \npreferences, we study the performance of various rank aggregation methods. In \naddition to predictive accuracy in the finite sample setting, we address the \ntheoretical question of consistency, by which we mean the ability to recover a \ntarget ranking when the sample size goes to infinity, despite a potential bias \nin the observations caused by the (unknown) coarsening. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763cf6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01158"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David G. Nagy, Gerg&#x151; Orb&#xe1;n", "title": "Episodic memory for continual model learning. (arXiv:1712.01169v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01169", "type": "text/html"}], "timestampUsec": "1512450843840946", "comments": [], "summary": {"content": "<p>Both the human brain and artificial learning agents operating in real-world \nor comparably complex environments are faced with the challenge of online model \nselection. In principle this challenge can be overcome: hierarchical Bayesian \ninference provides a principled method for model selection and it converges on \nthe same posterior for both off-line (i.e. batch) and online learning. However, \nmaintaining a parameter posterior for each model in parallel has in general an \neven higher memory cost than storing the entire data set and is consequently \nclearly unfeasible. Alternatively, maintaining only a limited set of models in \nmemory could limit memory requirements. However, sufficient statistics for one \nmodel will usually be insufficient for fitting a different kind of model, \nmeaning that the agent loses information with each model change. We propose \nthat episodic memory can circumvent the challenge of limited memory-capacity \nonline model selection by retaining a selected subset of data points. We design \na method to compute the quantities necessary for model selection even when the \ndata is discarded and only statistics of one (or few) learnt models are \navailable. We demonstrate on a simple model that a limited-sized episodic \nmemory buffer, when the content is optimised to retain data with statistics not \nmatching the current representation, can resolve the fundamental challenge of \nonline model selection. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763cfd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01169"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Madhav Nimishakavi, Pratik Jawanpuria, Bamdev Mishra", "title": "A Dual Framework for Low-rank Tensor Completion. (arXiv:1712.01193v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01193", "type": "text/html"}], "timestampUsec": "1512450843840945", "comments": [], "summary": {"content": "<p>We propose a novel formulation of the low-rank tensor completion problem that \nis based on the duality theory and a particular choice of low-rank regularizer. \nThis low-rank regularizer along with the dual perspective provides a simple \ncharacterization of the solution to the tensor completion problem. Motivated by \nlarge-scale setting, we next derive a rank-constrained reformulation of the \nproposed optimization problem, which is shown to lie on the Riemannian \nspectrahedron manifold. We exploit the versatile Riemannian optimization \nframework to develop computationally efficient conjugate gradient and \ntrust-region algorithms. The experiments confirm the benefits of our choice of \nregularization and the proposed algorithms outperform state-of-the-art \nalgorithms on several real-world data sets in different applications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01193"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christopher Nowzohour, Marloes H. Maathuis, Robin J. Evans, Peter B&#xfc;hlmann", "title": "Distributional Equivalence and Structure Learning for Bow-free Acyclic Path Diagrams. (arXiv:1508.01717v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1508.01717", "type": "text/html"}], "timestampUsec": "1512450843840944", "comments": [], "summary": {"content": "<p>We consider the problem of structure learning for bow-free acyclic path \ndiagrams (BAPs). BAPs can be viewed as a generalization of linear Gaussian DAG \nmodels that allow for certain hidden variables. We present a first method for \nthis problem using a greedy score-based search algorithm. We also prove some \nnecessary and some sufficient conditions for distributional equivalence of BAPs \nwhich are used in an algorithmic ap- proach to compute (nearly) equivalent \nmodel structures. This allows us to infer lower bounds of causal effects. We \nalso present applications to real and simulated datasets using our publicly \navailable R-package. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d27", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1508.01717"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sandrine Dallaporta, Yohann De Castro", "title": "Sparse Recovery Guarantees from Extreme Eigenvalues Small Deviations. (arXiv:1604.01171v3 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1604.01171", "type": "text/html"}], "timestampUsec": "1512450843840943", "comments": [], "summary": {"content": "<p>This article provides a new toolbox to derive sparse recovery guarantees from \nsmall deviations on extreme singular values or extreme eigenvalues obtained in \nRandom Matrix Theory. This work is based on Restricted Isometry Constants \n(RICs) which are a pivotal notion in Compressed Sensing and High-Dimensional \nStatistics as these constants finely assess how a linear operator is \nconditioned on the set of sparse vectors and hence how it performs in SRSR. \nWhile it is an open problem to construct deterministic matrices with apposite \nRICs, one can prove that such matrices exist using random matrices models. In \nthis paper, we show upper bounds on RICs for Gaussian and Rademacher matrices \nusing state-of-the-art small deviation estimates on their extreme eigenvalues. \nThis allows us to derive a lower bound on the probability of getting SRSR. One \nbenefit of this paper is a direct and explicit derivation of upper bounds on \nRICs and lower bounds on SRSR from small deviations on the extreme eigenvalues \ngiven by Random Matrix theory. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1604.01171"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nicholas Boyd, Trevor Hastie, Stephen Boyd, Benjamin Recht, Michael Jordan", "title": "Saturating Splines and Feature Selection. (arXiv:1609.06764v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1609.06764", "type": "text/html"}], "timestampUsec": "1512450843840942", "comments": [], "summary": {"content": "<p>We extend the adaptive regression spline model by incorporating saturation, \nthe natural requirement that a function extend as a constant outside a certain \nrange. We fit saturating splines to data using a convex optimization problem \nover a space of measures, which we solve using an efficient algorithm based on \nthe conditional gradient method. Unlike many existing approaches, our algorithm \nsolves the original infinite-dimensional (for splines of degree at least two) \noptimization problem without pre-specified knot locations. We then adapt our \nalgorithm to fit generalized additive models with saturating splines as \ncoordinate functions and show that the saturation requirement allows our model \nto simultaneously perform feature selection and nonlinear function fitting. \nFinally, we briefly sketch how the method can be extended to higher order \nsplines and to different requirements on the extension outside the data range. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d42", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1609.06764"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ryan J. Gallagher, Kyle Reing, David Kale, Greg Ver Steeg", "title": "Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge. (arXiv:1611.10277v3 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.10277", "type": "text/html"}], "timestampUsec": "1512450843840941", "comments": [], "summary": {"content": "<p>While generative models such as Latent Dirichlet Allocation (LDA) have proven \nfruitful in topic modeling, they often require detailed assumptions and careful \nspecification of hyperparameters. Such model complexity issues only compound \nwhen trying to generalize generative models to incorporate human input. We \nintroduce Correlation Explanation (CorEx), an alternative approach to topic \nmodeling that does not assume an underlying generative model, and instead \nlearns maximally informative topics through an information-theoretic framework. \nThis framework naturally generalizes to hierarchical and semi-supervised \nextensions with no additional modeling assumptions. In particular, word-level \ndomain knowledge can be flexibly incorporated within CorEx through anchor \nwords, allowing topic separability and representation to be promoted with \nminimal human intervention. Across a variety of datasets, metrics, and \nexperiments, we demonstrate that CorEx produces topics that are comparable in \nquality to those produced by unsupervised and semi-supervised variants of LDA. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d46", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.10277"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yuheng Bu, Shaofeng Zou, Venugopal V. Veeravalli", "title": "Linear-Complexity Exponentially-Consistent Tests for Universal Outlying Sequence Detection. (arXiv:1701.06084v4 [cs.IT] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1701.06084", "type": "text/html"}], "timestampUsec": "1512450843840940", "comments": [], "summary": {"content": "<p>The problem of universal outlying sequence detection is studied, where the \ngoal is to detect outlying sequences among $M$ sequences of samples. A sequence \nis considered as outlying if the observations therein are generated by a \ndistribution different from those generating the observations in the majority \nof the sequences. In the universal setting, we are interested in identifying \nall the outlying sequences without knowing the underlying generating \ndistributions. In this paper, a class of tests based on distribution clustering \nis proposed. These tests are shown to be exponentially consistent with linear \ntime complexity in $M$. Numerical results demonstrate that our clustering-based \ntests achieve similar performance to existing tests, while being considerably \nmore computationally efficient. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1701.06084"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sitao Xiang, Hao Li", "title": "On the Effects of Batch and Weight Normalization in Generative Adversarial Networks. (arXiv:1704.03971v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.03971", "type": "text/html"}], "timestampUsec": "1512450843840939", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a90eeeb\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a90eeeb&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Generative adversarial networks (GANs) are highly effective unsupervised \nlearning frameworks that can generate very sharp data, even for data such as \nimages with complex, highly multimodal distributions. However GANs are known to \nbe very hard to train, suffering from problems such as mode collapse and \ndisturbing visual artifacts. Batch normalization (BN) techniques have been \nintroduced to address the training. Though BN accelerates the training in the \nbeginning, our experiments show that the use of BN can be unstable and \nnegatively impact the quality of the trained model. The evaluation of BN and \nnumerous other recent schemes for improving GAN training is hindered by the \nlack of an effective objective quality measure for GAN models. To address these \nissues, we first introduce a weight normalization (WN) approach for GAN \ntraining that significantly improves the stability, efficiency and the quality \nof the generated samples. To allow a methodical evaluation, we introduce \nsquared Euclidean reconstruction error on a test set as a new objective \nmeasure, to assess training performance in terms of speed, stability, and \nquality of generated samples. Our experiments with a standard DCGAN \narchitecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10) \nindicate that training using WN is generally superior to BN for GANs, achieving \n10% lower mean squared loss for reconstruction and significantly better \nqualitative results than BN. We further demonstrate the stability of WN on a \n21-layer ResNet trained with the CelebA data set. The code for this paper is \navailable at https://github.com/stormraiser/gan-weightnorm-resnet \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d51", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.03971"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Cem M. Deniz, Siyuan Xiang, Spencer Hallyburton, Arakua Welbeck, Stephen Honig, Kyunghyun Cho, Gregory Chang", "title": "Segmentation of the Proximal Femur from MR Images using Deep Convolutional Neural Networks. (arXiv:1704.06176v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.06176", "type": "text/html"}], "timestampUsec": "1512450843840938", "comments": [], "summary": {"content": "<p>Magnetic resonance imaging (MRI) has been proposed as a complimentary method \nto measure bone quality and assess fracture risk. However, manual segmentation \nof MR images of bone is time-consuming, limiting the use of MRI measurements in \nthe clinical practice. The purpose of this paper is to present an automatic \nproximal femur segmentation method that is based on deep convolutional neural \nnetworks (CNNs). This study had institutional review board approval and written \ninformed consent was obtained from all subjects. A dataset of volumetric \nstructural MR images of the proximal femur from 86 subject were \nmanually-segmented by an expert. We performed experiments by training two \ndifferent CNN architectures with multiple number of initial feature maps and \nlayers, and tested their segmentation performance against the gold standard of \nmanual segmentations using four-fold cross-validation. Automatic segmentation \nof the proximal femur achieved a high dice similarity score of 0.94$\\pm$0.05 \nwith precision = 0.95$\\pm$0.02, and recall = 0.94$\\pm$0.08 using a CNN \narchitecture based on 3D convolution exceeding the performance of 2D CNNs. The \nhigh segmentation accuracy provided by CNNs has the potential to help bring the \nuse of structural MRI measurements of bone quality into clinical practice for \nmanagement of osteoporosis. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d63", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.06176"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Crist&#xf3;bal Esteban, Stephanie L. Hyland, Gunnar R&#xe4;tsch", "title": "Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs. (arXiv:1706.02633v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.02633", "type": "text/html"}], "timestampUsec": "1512450843840937", "comments": [], "summary": {"content": "<p>Generative Adversarial Networks (GANs) have shown remarkable success as a \nframework for training models to produce realistic-looking data. In this work, \nwe propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to \nproduce realistic real-valued multi-dimensional time series, with an emphasis \non their application to medical data. RGANs make use of recurrent neural \nnetworks in the generator and the discriminator. In the case of RCGANs, both of \nthese RNNs are conditioned on auxiliary information. We demonstrate our models \nin a set of toy datasets, where we show visually and quantitatively (using \nsample likelihood and maximum mean discrepancy) that they can successfully \ngenerate realistic time-series. We also describe novel evaluation methods for \nGANs, where we generate a synthetic labelled training dataset, and evaluate on \na real test set the performance of a model trained on the synthetic data, and \nvice-versa. We illustrate with these metrics that RCGANs can generate \ntime-series data useful for supervised training, with only minor degradation in \nperformance on real test data. This is demonstrated on digit classification \nfrom 'serialised' MNIST and by training an early warning system on a medical \ndataset of 17,000 patients from an intensive care unit. We further discuss and \nanalyse the privacy concerns that may arise when using RCGANs to generate \nrealistic synthetic medical time series data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d70", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.02633"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shiv Shankar, Sunita Sarawagi", "title": "Labeled Memory Networks for Online Model Adaptation. (arXiv:1707.01461v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.01461", "type": "text/html"}], "timestampUsec": "1512450843840936", "comments": [], "summary": {"content": "<p>Augmenting a neural network with memory that can grow without growing the \nnumber of trained parameters is a recent powerful concept with many exciting \napplications. We propose a design of memory augmented neural networks (MANNs) \ncalled Labeled Memory Networks (LMNs) suited for tasks requiring online \nadaptation in classification models. LMNs organize the memory with classes as \nthe primary key.The memory acts as a second boosted stage following a regular \nneural network thereby allowing the memory and the primary network to play \ncomplementary roles. Unlike existing MANNs that write to memory for every \ninstance and use LRU based memory replacement, LMNs write only for instances \nwith non-zero loss and use label-based memory replacement. We demonstrate \nsignificant accuracy gains on various tasks including word-modelling and \nfew-shot learning. In this paper, we establish their potential in online \nadapting a batch trained neural network to domain-relevant labeled data at \ndeployment time. We show that LMNs are better than other MANNs designed for \nmeta-learning. We also found them to be more accurate and faster than \nstate-of-the-art methods of retuning model parameters for adapting to \ndomain-specific labeled data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d79", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.01461"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Peter Mills", "title": "Accelerating Kernel Classifiers Through Borders Mapping. (arXiv:1708.05917v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.05917", "type": "text/html"}], "timestampUsec": "1512450843840935", "comments": [], "summary": {"content": "<p>Support vector machines (SVM) and other kernel techniques represent a family \nof powerful statistical classification methods with high accuracy and broad \napplicability. Because they use all or a significant portion of the training \ndata, however, they can be slow, especially for large problems. Piecewise \nlinear classifiers are similarly versatile, yet have the additional advantages \nof simplicity, ease of interpretation and, if the number of component linear \nclassifiers is not too large, speed. Here we show how a simple, piecewise \nlinear classifier can be trained from a kernel-based classifier in order to \nimprove the classification speed. The method works by finding the root of the \ndifference in conditional probabilities between pairs of opposite classes to \nbuild up a representation of the decision boundary. When tested on 17 different \ndatasets, it succeeded in improving the classification speed of a SVM for 9 of \nthem by factors as high as 88 times or more. The method is best suited to \nproblems with continuum features data and smooth probability functions. Because \nthe component linear classifiers are built up individually from an existing \nclassifier, rather than through a simultaneous optimization procedure, the \nclassifier is also fast to train. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d81", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.05917"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Naoki Hayashi, Sumio Watanabe", "title": "Asymptotic Bayesian Generalization Error in a General Stochastic Matrix Factorization for Markov Chain and Bayesian Network. (arXiv:1709.04212v3 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04212", "type": "text/html"}], "timestampUsec": "1512450843840934", "comments": [], "summary": {"content": "<p>Stochastic matrix factorization (SMF) can be regarded as a restriction of \nnon-negative matrix factorization (NMF). SMF is useful for inference of topic \nmodels, NMF for binary matrices data, Markov chains, and Bayesian networks. \nHowever, SMF needs strong assumptions to reach a unique factorization and its \ntheoretical prediction accuracy has not yet been clarified. In this paper, we \nstudy the maximum the pole of zeta function (real log canonical threshold) of a \ngeneral SMF and derive an upper bound of the generalization error in Bayesian \ninference. The results give a foundation for a widely applicable and rigorous \nfactorization method of SMF and mean that the generalization error in SMF \nbecomes smaller than regular statistical models by Bayesian inference. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d8e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04212"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tsz Kit Lau, Yuan Yao", "title": "Accelerated Block Coordinate Proximal Gradients with Applications in High Dimensional Statistics. (arXiv:1710.05338v7 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.05338", "type": "text/html"}], "timestampUsec": "1512450843840933", "comments": [], "summary": {"content": "<p>Nonconvex optimization problems arise in different research fields and arouse \nlots of attention in signal processing, statistics and machine learning. In \nthis work, we explore the accelerated proximal gradient method and some of its \nvariants which have been shown to converge under nonconvex context recently. We \nshow that a novel variant proposed here, which exploits adaptive momentum and \nblock coordinate update with specific update rules, further improves the \nperformance of a broad class of nonconvex problems. In applications to sparse \nlinear regression with regularizations like Lasso, grouped Lasso, capped \n$\\ell_1$ and SCAP, the proposed scheme enjoys provable local linear \nconvergence, with experimental justification. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.05338"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Urs K&#xf6;ster, Tristan J. Webb, Xin Wang, Marcel Nassar, Arjun K. Bansal, William H. Constable, O&#x11f;uz H. Elibol, Scott Gray, Stewart Hall, Luke Hornof, Amir Khosrowshahi, Carey Kloss, Ruby J. Pai, Naveen Rao", "title": "Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks. (arXiv:1711.02213v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.02213", "type": "text/html"}], "timestampUsec": "1512450843840932", "comments": [], "summary": {"content": "<p>Deep neural networks are commonly developed and trained in 32-bit floating \npoint format. Significant gains in performance and energy efficiency could be \nrealized by training and inference in numerical formats optimized for deep \nlearning. Despite advances in limited precision inference in recent years, \ntraining of neural networks in low bit-width remains a challenging problem. \nHere we present the Flexpoint data format, aiming at a complete replacement of \n32-bit floating point format training and inference, designed to support modern \ndeep network topologies without modifications. Flexpoint tensors have a shared \nexponent that is dynamically adjusted to minimize overflows and maximize \navailable dynamic range. We validate Flexpoint by training AlexNet, a deep \nresidual network and a generative adversarial network, using a simulator \nimplemented with the neon deep learning framework. We demonstrate that 16-bit \nFlexpoint closely matches 32-bit floating point in training all three models, \nwithout any need for tuning of model hyperparameters. Our results suggest \nFlexpoint as a promising numerical format for future hardware for training and \ninference. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763db0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.02213"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Bogdan Kulynych, Carmela Troncoso", "title": "Feature importance scores and lossless feature pruning using Banzhaf power indices. (arXiv:1711.04992v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.04992", "type": "text/html"}], "timestampUsec": "1512450843840931", "comments": [], "summary": {"content": "<p>Understanding the influence of features in machine learning is crucial to \ninterpreting models and selecting the best features for classification. In this \nwork we propose the use of principles from coalitional game theory to reason \nabout importance of features. In particular, we propose the use of the Banzhaf \npower index as a measure of influence of features on the outcome of a \nclassifier. We show that features having Banzhaf power index of zero can be \nlosslessly pruned without damage to classifier accuracy. Computing the power \nindices does not require having access to data samples. However, if samples are \navailable, the indices can be empirically estimated. We compute Banzhaf power \nindices for a neural network classifier on real-life data, and compare the \nresults with gradient-based feature saliency, and coefficients of a logistic \nregression model with $L_1$ regularization. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763dc5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.04992"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Seong Jin Cho, Sunghun Kang, Chang D. Yoo", "title": "A Resizable Mini-batch Gradient Descent based on a Randomized Weighted Majority. (arXiv:1711.06424v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.06424", "type": "text/html"}], "timestampUsec": "1512450843840930", "comments": [], "summary": {"content": "<p>Determining the appropriate batch size for mini-batch gradient descent is \nalways time consuming as it often relies on grid search. This paper considers a \nresizable mini-batch gradient descent (RMGD) algorithm-inspired by the \nrandomized weighted majority algorithm-for achieving best performance in grid \nsearch by selecting an appropriate batch size at each epoch with a probability \ndefined as a function of its previous success/failure and the validation error. \nThis probability encourages exploration of different batch size and then later \nexploitation of batch size with history of success. At each epoch, the RMGD \nsamples a batch size from its probability distribution, then uses the selected \nbatch size for mini-batch gradient descent. After obtaining the validation \nerror at each epoch, the probability distribution is updated to incorporate the \neffectiveness of the sampled batch size. The RMGD essentially assists the \nlearning process to explore the possible domain of the batch size and exploit \nsuccessful batch size. Experimental results show that the RMGD achieves \nperformance better than the best performing single batch size. Furthermore, it \nattains this performance in a shorter amount of time than that of the best \nperforming. It is surprising that the RMGD achieves better performance than \ngrid search. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763dd0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.06424"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Carlos X. Hern&#xe1;ndez, Hannah K. Wayment-Steele, Mohammad M. Sultan, Brooke E. Husic, Vijay S. Pande", "title": "Variational Encoding of Complex Dynamics. (arXiv:1711.08576v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08576", "type": "text/html"}], "timestampUsec": "1512450843840929", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a90f0cc\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a90f0cc&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Often the analysis of time-dependent chemical and biophysical systems \nproduces high-dimensional time-series data for which it can be difficult to \ninterpret which individual features are most salient. While recent work from \nour group and others has demonstrated the utility of time-lagged co-variate \nmodels to study such systems, linearity assumptions can limit the compression \nof inherently nonlinear dynamics into just a few characteristic components. \nRecent work in the field of deep learning has led to the development of \nvariational autoencoders (VAE), which are able to compress complex datasets \ninto simpler manifolds. We present the use of a time-lagged VAE, or variational \ndynamics encoder (VDE), to reduce complex, nonlinear processes to a single \nembedding with high fidelity to the underlying dynamics. We demonstrate how the \nVDE is able to capture nontrivial dynamics in a variety of examples, including \nBrownian dynamics and atomistic protein folding. Additionally, we demonstrate a \nmethod for analyzing the VDE model, inspired by saliency mapping, to determine \nwhat features are selected by the VDE model to describe dynamics. The VDE \npresents an important step in applying techniques from deep learning to more \naccurately model and interpret complex biophysics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763ddf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08576"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiyu Yu, Tongliang Liu, Mingming Gong, Dacheng Tao", "title": "Learning with Biased Complementary Labels. (arXiv:1711.09535v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09535", "type": "text/html"}], "timestampUsec": "1512450843840928", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a973593\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a973593&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper we study the classification problem in which we have access to \neasily obtainable surrogate for the true labels, namely complementary labels, \nwhich specify classes that observations do \\textbf{not} belong to. For example, \nif one is familiar with monkeys but not meerkats, a meerkat is easily \nidentified as not a monkey, so \"monkey\" is annotated to the meerkat as a \ncomplementary label. Specifically, let $Y$ and $\\bar{Y}$ be the true and \ncomplementary labels, respectively. We first model the annotation of \ncomplementary labels via the transition probabilities $P(\\bar{Y}=i|Y=j), i\\neq \nj\\in\\{1,\\cdots,c\\}$, where $c$ is the number of classes. All the previous \nmethods implicitly assume that the transition probabilities $P(\\bar{Y}=i|Y=j)$ \nare identical, which is far from true in practice because humans are biased \ntoward their own experience. For example, if a person is more familiar with \nmonkey than prairie dog when providing complementary labels for meerkats, \nhe/she is more likely to employ \"monkey\" as a complementary label. We therefore \nreason that the transition probabilities will be different. In this paper, we \naddress three fundamental problems raised by learning with biased complementary \nlabels. (1) How to estimate the transition probabilities? (2) How to modify the \ntraditional loss functions and extend standard deep neural network classifiers \nto learn with biased complementary labels? (3) Does the classifier learned from \nexamples with complementary labels by our proposed method converge to the \noptimal one learned from examples with true labels? Comprehensive experiments \non MNIST, CIFAR10, CIFAR100, and Tiny ImageNet empirically validate the \nsuperiority of the proposed method to the current state-of-the-art methods with \naccuracy gains of over 10\\%. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763df3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09535"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Santiago Hern&#xe1;ndez-Orozco, Narsis A. Kiani, Hector Zenil", "title": "Algorithmically probable mutations reproduce aspects of evolution such as convergence rate, genetic memory, modularity, diversity explosions, and mass extinction. (arXiv:1709.00268v5 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.00268", "type": "text/html"}], "timestampUsec": "1512364308369392", "comments": [], "summary": {"content": "<p>Natural selection explains how life has evolved over millions of years from \nmore primitive forms. The speed at which this happens, however, has sometimes \ndefied explanations based on random (uniformly distributed) mutations. Here we \ninvestigate the application of algorithmic mutations (no recombination) to \nbinary matrices drawn from numerical approximations to algorithmic probability \nin order to compare evolutionary convergence rates against the null hypothesis \n(uniformly distributed mutations). Results both on synthetic and a small \nbiological examples lead to an accelerated rate of convergence when using the \nalgorithmic probability. We also show that algorithmically evolved modularity \nprovides an advantage that produces a genetic memory. We demonstrate that \nregular structures are preserved and carried on when they first occur and can \nlead to an accelerated production of diversity and extinction, possibly \nexplaining naturally occurring phenomena such as diversity explosions (e.g. the \nCambrian) and massive extinctions (e.g. the End Triassic) whose causes have \neluded researchers and are a cause for debate. The approach introduced here \nappears to be a better approximation to biological evolution than models based \nexclusively upon random uniform mutations, and it also approaches better a \nformal version of open-ended evolution based on previous results. The results \nvalidate the motivations and results of Chaitin's Metabiology programme and \nprevious suggestions that computation may be an equally important driver of \nevolution together, and even before, the action and result of natural \nselection. We also show that inducing the method on problems of optimization, \nsuch as genetic algorithms, has the potential to significantly accelerate \nconvergence of artificial evolutionary algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb640d9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.00268"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiaojie Jin, Yingzhen Yang, Ning Xu, Jianchao Yang, Jiashi Feng, Shuicheng Yan", "title": "WSNet: Compact and Efficient Networks with Weight Sampling. (arXiv:1711.10067v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10067", "type": "text/html"}], "timestampUsec": "1512364308369391", "comments": [], "summary": {"content": "<p>We present a new approach and a novel architecture, termed WSNet, for \nlearning compact and efficient deep neural networks. Existing approaches \nconventionally learn full model parameters independently at first and then \ncompress them via ad hoc processing like model pruning or filter factorization. \nDifferent from them, WSNet proposes learning model parameters by sampling from \na compact set of learnable parameters, which naturally enforces parameter \nsharing throughout the learning process. We show that such novel weight \nsampling approach (and induced WSNet) promotes both weights and computation \nsharing favorably. It can more efficiently learn much smaller networks with \ncompetitive performance, compared to baseline networks with equal number of \nconvolution filters. Specifically, we consider learning compact and efficient \n1D convolutional neural networks for audio classification. Extensive \nexperiments on multiple audio classification datasets verify the effectiveness \nof WSNet. Combined with weight quantization, the resulted models are up to 180x \nsmaller and theoretically up to 16x faster than the well-established baselines, \nwithout noticeable performance drop. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64101", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10067"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Seungkyun Hong, Seongchan Kim, Minsu Joh, Sa-kwang Song", "title": "PSIque: Next Sequence Prediction of Satellite Images using a Convolutional Sequence-to-Sequence Network. (arXiv:1711.10644v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10644", "type": "text/html"}], "timestampUsec": "1512364308369390", "comments": [], "summary": {"content": "<p>Predicting unseen weather phenomena is an important issue for disaster \nmanagement. In this paper, we suggest a model for a convolutional \nsequence-to-sequence autoencoder for predicting undiscovered weather situations \nfrom previous satellite images. We also propose a symmetric skip connection \nbetween encoder and decoder modules to produce more comprehensive image \npredictions. To examine our model performance, we conducted experiments for \neach suggested model to predict future satellite images from historical \nsatellite images. A specific combination of skip connection and \nsequence-to-sequence autoencoder was able to generate closest prediction from \nthe ground truth image. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64116", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10644"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Martin Klissarov, Pierre-Luc Bacon, Jean Harb, Doina Precup", "title": "Learnings Options End-to-End for Continuous Action Tasks. (arXiv:1712.00004v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00004", "type": "text/html"}], "timestampUsec": "1512364308369389", "comments": [], "summary": {"content": "<p>We present new results on learning temporally extended actions for \ncontinuoustasks, using the options framework (Suttonet al.[1999b], Precup \n[2000]). In orderto achieve this goal we work with the option-critic \narchitecture (Baconet al.[2017])using a deliberation cost and train it with \nproximal policy optimization (Schulmanet al.[2017]) instead of vanilla policy \ngradient. Results on Mujoco domains arepromising, but lead to interesting \nquestions aboutwhena given option should beused, an issue directly connected to \nthe use of initiation sets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64134", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00004"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shangtong Zhang, Osmar R. Zaiane", "title": "Comparing Deep Reinforcement Learning and Evolutionary Methods in Continuous Control. (arXiv:1712.00006v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00006", "type": "text/html"}], "timestampUsec": "1512364308369388", "comments": [], "summary": {"content": "<p>Reinforcement learning and evolutionary strategy are two major approaches in \naddressing complicated control problems. Both have strong biological basis and \nthere have been recently many advanced techniques in both domains. In this \npaper, we present a thorough comparison between the state of the art techniques \nin both domains in complex continuous control tasks. We also formulate the \nparallelized version of the Proximal Policy Optimization method and the Deep \nDeterministic Policy Gradient method. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64164", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00006"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jason Bernard, Ian McQuillan", "title": "New Techniques for Inferring L-Systems Using Genetic Algorithm. (arXiv:1712.00180v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00180", "type": "text/html"}], "timestampUsec": "1512364308369387", "comments": [], "summary": {"content": "<p>Lindenmayer systems (L-systems) are a formal grammar system that iteratively \nrewrites all symbols of a string, in parallel. When visualized with a graphical \ninterpretation, the images have self-similar shapes that appear frequently in \nnature, and they have been particularly successful as a concise, reusable \ntechnique for simulating plants. The L-system inference problem is to find an \nL-system to simulate a given plant. This is currently done mainly by experts, \nbut this process is limited by the availability of experts, the complexity that \nmay be solved by humans, and time. This paper introduces the Plant Model \nInference Tool (PMIT) that infers deterministic context-free L-systems from an \ninitial sequence of strings generated by the system using a genetic algorithm. \nPMIT is able to infer more complex systems than existing approaches. Indeed, \nwhile existing approaches are limited to L-systems with a total sum of 20 \ncombined symbols in the productions, PMIT can infer almost all L-systems tested \nwhere the total sum is 140 symbols. This was validated using a test bed of 28 \npreviously developed L-system models, in addition to models created \nartificially by bootstrapping larger models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6418d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00180"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hee Jung Ryu, Margaret Mitchell, Hartwig Adam", "title": "Improving Smiling Detection with Race and Gender Diversity. (arXiv:1712.00193v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.00193", "type": "text/html"}], "timestampUsec": "1512364308369386", "comments": [], "summary": {"content": "<p>Recent progress in deep learning has been accompanied by a growing concern \nfor whether models are fair for users, with equally good performance across \ndifferent demographics. In computer vision research, such questions are \nrelevant to face detection and the related task of face attribute detection, \namong others. We measure race and gender inclusion in the context of smiling \ndetection, and introduce a method for improving smiling detection across \ndemographic groups. Our method introduces several modifications over existing \ndetection methods, leveraging twofold transfer learning to better model facial \ndiversity. Results show that this technique improves accuracy against strong \nbaselines for most demographic groups as well as overall. Our best-performing \nmodel defines a new state-of-the-art for smiling detection, reaching 91% on the \nFaces of the World dataset. The accompanying multi-head diversity classifier \nalso defines a new state-of-the-art for gender classification, reaching 93.87% \non the Faces of the World dataset. This research demonstrates the utility of \nmodeling race and gender to improve a face attribute detection task, using a \ntwofold transfer learning framework that allows for privacy towards individuals \nin a target dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb641ac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00193"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chong Di", "title": "A double competitive strategy based learning automata algorithm. (arXiv:1712.00222v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00222", "type": "text/html"}], "timestampUsec": "1512364308369385", "comments": [], "summary": {"content": "<p>Learning Automata (LA) are considered as one of the most powerful tools in \nthe field of reinforcement learning. The family of estimator algorithms is \nproposed to improve the convergence rate of LA and has made great achievements. \nHowever, the estimators perform poorly on estimating the reward probabilities \nof actions in the initial stage of the learning process of LA. In this \nsituation, a lot of rewards would be added to the probabilities of non-optimal \nactions. Thus, a large number of extra iterations are needed to compensate for \nthese wrong rewards. In order to improve the speed of convergence, we propose a \nnew P-model absorbing learning automaton by utilizing a double competitive \nstrategy which is designed for updating the action probability vector. In this \nway, the wrong rewards can be corrected instantly. Hence, the proposed Double \nCompetitive Algorithm overcomes the drawbacks of existing estimator algorithms. \nA refined analysis is presented to show the $\\epsilon-optimality$ of the \nproposed scheme. The extensive experimental results in benchmark environments \ndemonstrate that our proposed learning automata perform more efficiently than \nthe most classic LA $SE_{RI}$ and the current fastest LA $DGCPA^{*}$. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb641ba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00222"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aishwarya Agrawal, Dhruv Batra, Devi Parikh, Aniruddha Kembhavi", "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering. (arXiv:1712.00377v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.00377", "type": "text/html"}], "timestampUsec": "1512364308369384", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a97390e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a97390e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A number of studies have found that today's Visual Question Answering (VQA) \nmodels are heavily driven by superficial correlations in the training data and \nlack sufficient image grounding. To encourage development of models geared \ntowards the latter, we propose a new setting for VQA where for every question \ntype, train and test sets have different prior distributions of answers. \nSpecifically, we present new splits of the VQA v1 and VQA v2 datasets, which we \ncall Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 \nrespectively). First, we evaluate several existing VQA models under this new \nsetting and show that their performance degrades significantly compared to the \noriginal VQA setting. Second, we propose a novel Grounded Visual Question \nAnswering model (GVQA) that contains inductive biases and restrictions in the \narchitecture specifically designed to prevent the model from 'cheating' by \nprimarily relying on priors in the training data. Specifically, GVQA explicitly \ndisentangles the recognition of visual concepts present in the image from the \nidentification of plausible answer space for a given question, enabling the \nmodel to more robustly generalize across different distributions of answers. \nGVQA is built off an existing VQA model -- Stacked Attention Networks (SAN). \nOur experiments demonstrate that GVQA significantly outperforms SAN on both \nVQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more \npowerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in \nseveral cases. GVQA offers strengths complementary to SAN when trained and \nevaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more \ntransparent and interpretable than existing VQA models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb641d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00377"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Oliver Bent, Sekou L. Remy, Stephen Roberts, Aisha Walcott-Bryant", "title": "Novel Exploration Techniques (NETs) for Malaria Policy Interventions. (arXiv:1712.00428v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00428", "type": "text/html"}], "timestampUsec": "1512364308369383", "comments": [], "summary": {"content": "<p>The task of decision-making under uncertainty is daunting, especially for \nproblems which have significant complexity. Healthcare policy makers across the \nglobe are facing problems under challenging constraints, with limited tools to \nhelp them make data driven decisions. In this work we frame the process of \nfinding an optimal malaria policy as a stochastic multi-armed bandit problem, \nand implement three agent based strategies to explore the policy space. We \napply a Gaussian Process regression to the findings of each agent, both for \ncomparison and to account for stochastic results from simulating the spread of \nmalaria in a fixed population. The generated policy spaces are compared with \npublished results to give a direct reference with human expert decisions for \nthe same simulated population. Our novel approach provides a powerful resource \nfor policy makers, and a platform which can be readily extended to capture \nfuture more nuanced policy spaces. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb641e7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00428"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Marcello Balduccini, Yuliya Lierler", "title": "Constraint Answer Set Solver EZCSP and Why Integration Schemas Matter. (arXiv:1702.04047v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1702.04047", "type": "text/html"}], "timestampUsec": "1512364308369381", "comments": [], "summary": {"content": "<p>Researchers in answer set programming and constraint programming have spent \nsignificant efforts in the development of hybrid languages and solving \nalgorithms combining the strengths of these traditionally separate fields. \nThese efforts resulted in a new research area: constraint answer set \nprogramming. Constraint answer set programming languages and systems proved to \nbe successful at providing declarative, yet efficient solutions to problems \ninvolving hybrid reasoning tasks. One of the main contributions of this paper \nis the first comprehensive account of the constraint answer set language and \nsolver EZCSP, a mainstream representative of this research area that has been \nused in various successful applications. We also develop an extension of the \ntransition systems proposed by Nieuwenhuis et al. in 2006 to capture Boolean \nsatisfiability solvers. We use this extension to describe the EZCSP algorithm \nand prove formal claims about it. The design and algorithmic details behind \nEZCSP clearly demonstrate that the development of the hybrid systems of this \nkind is challenging. Many questions arise when one faces various design choices \nin an attempt to maximize system's benefits. One of the key decisions that a \ndeveloper of a hybrid solver makes is settling on a particular integration \nschema within its implementation. Thus, another important contribution of this \npaper is a thorough case study based on EZCSP, focused on the various \nintegration schemas that it provides. \n</p> \n<p>Under consideration in Theory and Practice of Logic Programming (TPLP). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64205", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1702.04047"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sirui Yao, Bert Huang", "title": "Beyond Parity: Fairness Objectives for Collaborative Filtering. (arXiv:1705.08804v2 [cs.IR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08804", "type": "text/html"}], "timestampUsec": "1512364308369380", "comments": [], "summary": {"content": "<p>We study fairness in collaborative-filtering recommender systems, which are \nsensitive to discrimination that exists in historical data. Biased data can \nlead collaborative-filtering methods to make unfair predictions for users from \nminority groups. We identify the insufficiency of existing fairness metrics and \npropose four new metrics that address different forms of unfairness. These \nfairness metrics can be optimized by adding fairness terms to the learning \nobjective. Experiments on synthetic and real data show that our new metrics can \nbetter measure fairness than the baseline, and that the fairness objectives \neffectively help reduce unfairness. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64241", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08804"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Joohyung Lee, Samidh Talsania, Yi Wang", "title": "Computing LPMLN Using ASP and MLN Solvers. (arXiv:1707.06325v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.06325", "type": "text/html"}], "timestampUsec": "1512364308369379", "comments": [], "summary": {"content": "<p>LPMLN is a recent addition to probabilistic logic programming languages. Its \nmain idea is to overcome the rigid nature of the stable model semantics by \nassigning a weight to each rule in a way similar to Markov Logic is defined. We \npresent two implementations of LPMLN, $\\text{LPMLN2ASP}$ and \n$\\text{LPMLN2MLN}$. System $\\text{LPMLN2ASP}$ translates LPMLN programs into \nthe input language of answer set solver $\\text{CLINGO}$, and using weak \nconstraints and stable model enumeration, it can compute most probable stable \nmodels as well as exact conditional and marginal probabilities. System \n$\\text{LPMLN2MLN}$ translates LPMLN programs into the input language of Markov \nLogic solvers, such as $\\text{ALCHEMY}$, $\\text{TUFFY}$, and $\\text{ROCKIT}$, \nand allows for performing approximate probabilistic inference on LPMLN \nprograms. We also demonstrate the usefulness of the LPMLN systems for computing \nother languages, such as ProbLog and Pearl's Causal Models, that are shown to \nbe translatable into LPMLN. (Under consideration for acceptance in TPLP) \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6425b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.06325"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gregory Kahn, Adam Villaflor, Bosen Ding, Pieter Abbeel, Sergey Levine", "title": "Self-supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation. (arXiv:1709.10489v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.10489", "type": "text/html"}], "timestampUsec": "1512364308369378", "comments": [], "summary": {"content": "<p>Enabling robots to autonomously navigate complex environments is essential \nfor real-world deployment. Prior methods approach this problem by having the \nrobot maintain an internal map of the world, and then use a localization and \nplanning method to navigate through the internal map. However, these approaches \noften include a variety of assumptions, are computationally intensive, and do \nnot learn from failures. In contrast, learning-based methods improve as the \nrobot acts in the environment, but are difficult to deploy in the real-world \ndue to their high sample complexity. To address the need to learn complex \npolicies with few samples, we propose a generalized computation graph that \nsubsumes value-based model-free methods and model-based methods, with specific \ninstantiations interpolating between model-free and model-based. We then \ninstantiate this graph to form a navigation model that learns from raw images \nand is sample efficient. Our simulated car experiments explore the design \ndecisions of our navigation model, and show our approach outperforms \nsingle-step and $N$-step double Q-learning. We also evaluate our approach on a \nreal-world RC car and show it can learn to navigate through a complex indoor \nenvironment with a few hours of fully autonomous, self-supervised training. \nVideos of the experiments and code can be found at github.com/gkahn13/gcg \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64271", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.10489"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mikael Henaff, Junbo Zhao, Yann LeCun", "title": "Prediction Under Uncertainty with Error-Encoding Networks. (arXiv:1711.04994v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.04994", "type": "text/html"}], "timestampUsec": "1512364308369377", "comments": [], "summary": {"content": "<p>In this work we introduce a new framework for performing temporal predictions \nin the presence of uncertainty. It is based on a simple idea of disentangling \ncomponents of the future state which are predictable from those which are \ninherently unpredictable, and encoding the unpredictable components into a \nlow-dimensional latent variable which is fed into a forward model. Our method \nuses a supervised training objective which is fast and easy to train. We \nevaluate it in the context of video prediction on multiple datasets and show \nthat it is able to consistently generate diverse predictions without the need \nfor alternating minimization over a latent space or adversarial training. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64282", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.04994"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Long Ouyang, Michael C. Frank", "title": "Pedagogical learning. (arXiv:1711.09401v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09401", "type": "text/html"}], "timestampUsec": "1512364308369376", "comments": [], "summary": {"content": "<p>A common assumption in machine learning is that training data are i.i.d. \nsamples from some distribution. Processes that generate i.i.d. samples are, in \na sense, uninformative---they produce data without regard to how good this data \nis for learning. By contrast, cognitive science research has shown that when \npeople generate training data for others (i.e., teaching), they deliberately \nselect examples that are helpful for learning. Because the data is more \ninformative, learning can require less data. Interestingly, such examples are \nmost effective when learners know that the data were pedagogically generated \n(as opposed to randomly generated). We call this pedagogical learning---when a \nlearner assumes that evidence comes from a helpful teacher. In this work, we \nask how pedagogical learning might work for machine learning algorithms. \nStudying this question requires understanding how people actually teach complex \nconcepts with examples, so we conducted a behavioral study examining how people \nteach regular expressions using example strings. We found that teachers' \nexamples contain powerful clustering structure that can greatly facilitate \nlearning. We then develop a model of teaching and show a proof of concept that \nusing this model inside of a learner can improve performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64291", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09401"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ahmad Chaddad, Behnaz Naisiri, Marco Pedersoli, Eric Granger, Christian Desrosiers, Matthew Toews", "title": "Modeling Information Flow Through Deep Neural Networks. (arXiv:1712.00003v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00003", "type": "text/html"}], "timestampUsec": "1512364308369374", "comments": [], "summary": {"content": "<p>This paper proposes a principled information theoretic analysis of \nclassification for deep neural network structures, e.g. convolutional neural \nnetworks (CNN). The output of convolutional filters is modeled as a random \nvariable Y conditioned on the object class C and network filter bank F. The \nconditional entropy (CENT) H(Y |C,F) is shown in theory and experiments to be a \nhighly compact and class-informative code, that can be computed from the filter \noutputs throughout an existing CNN and used to obtain higher classification \nresults than the original CNN itself. Experiments demonstrate the effectiveness \nof CENT feature analysis in two separate CNN classification contexts. 1) In the \nclassification of neurodegeneration due to Alzheimer's disease (AD) and natural \naging from 3D magnetic resonance image (MRI) volumes, 3 CENT features result in \nan AUC=94.6% for whole-brain AD classification, the highest reported accuracy \non the public OASIS dataset used and 12% higher than the softmax output of the \noriginal CNN trained for the task. 2) In the context of visual object \nclassification from 2D photographs, transfer learning based on a small set of \nCENT features identified throughout an existing CNN leads to AUC values \ncomparable to the 1000-feature softmax output of the original network when \nclassifying previously unseen object categories. The general information \ntheoretical analysis explains various recent CNN design successes, e.g. densely \nconnected CNN architectures, and provides insights for future research \ndirections in deep learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb642b4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00003"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "You Jin Kim (1), Yun-Geun Lee (1), Jeong Whun Kim (2), Jin Joo Park (2), Borim Ryu (2), Jung-Woo Ha (1) ((1) Clova AI Research, NAVER Corp., (2) Seoul National University Bundang Hospital)", "title": "Highrisk Prediction from Electronic Medical Records via Deep Attention Networks. (arXiv:1712.00010v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00010", "type": "text/html"}], "timestampUsec": "1512364308369373", "comments": [], "summary": {"content": "<p>Predicting highrisk vascular diseases is a significant issue in the medical \ndomain. Most predicting methods predict the prognosis of patients from \npathological and radiological measurements, which are expensive and require \nmuch time to be analyzed. Here we propose deep attention models that predict \nthe onset of the high risky vascular disease from symbolic medical histories \nsequence of hypertension patients such as ICD-10 and pharmacy codes only, \nMedical History-based Prediction using Attention Network (MeHPAN). We \ndemonstrate two types of attention models based on 1) bidirectional gated \nrecurrent unit (R-MeHPAN) and 2) 1D convolutional multilayer model (C-MeHPAN). \nTwo MeHPAN models are evaluated on approximately 50,000 hypertension patients \nwith respect to precision, recall, f1-measure and area under the curve (AUC). \nExperimental results show that our MeHPAN methods outperform standard \nclassification models. Comparing two MeHPANs, R-MeHPAN provides more better \ndiscriminative capability with respect to all metrics while C-MeHPAN presents \nmuch shorter training time with competitive accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb642c0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00010"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Genevieve Flaspohler, Nicholas Roy, Yogesh Girdhar", "title": "Feature discovery and visualization of robot mission data using convolutional autoencoders and Bayesian nonparametric topic models. (arXiv:1712.00028v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00028", "type": "text/html"}], "timestampUsec": "1512364308369372", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a973c7c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a973c7c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The gap between our ability to collect interesting data and our ability to \nanalyze these data is growing at an unprecedented rate. Recent algorithmic \nattempts to fill this gap have employed unsupervised tools to discover \nstructure in data. Some of the most successful approaches have used \nprobabilistic models to uncover latent thematic structure in discrete data. \nDespite the success of these models on textual data, they have not generalized \nas well to image data, in part because of the spatial and temporal structure \nthat may exist in an image stream. \n</p> \n<p>We introduce a novel unsupervised machine learning framework that \nincorporates the ability of convolutional autoencoders to discover features \nfrom images that directly encode spatial information, within a Bayesian \nnonparametric topic model that discovers meaningful latent patterns within \ndiscrete data. By using this hybrid framework, we overcome the fundamental \ndependency of traditional topic models on rigidly hand-coded data \nrepresentations, while simultaneously encoding spatial dependency in our topics \nwithout adding model complexity. We apply this model to the motivating \napplication of high-level scene understanding and mission summarization for \nexploratory marine robots. Our experiments on a seafloor dataset collected by a \nmarine robot show that the proposed hybrid framework outperforms current \nstate-of-the-art approaches on the task of unsupervised seafloor terrain \ncharacterization. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb642cf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00028"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xavier Roynard, Jean-Emmanuel Deschaud, Fran&#xe7;ois Goulette", "title": "Paris-Lille-3D: a large and high-quality ground truth urban point cloud dataset for automatic segmentation and classification. (arXiv:1712.00032v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00032", "type": "text/html"}], "timestampUsec": "1512364308369371", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a9d4faa\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a9d4faa&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper introduces a new Urban Point Cloud Dataset for Automatic \nSegmentation and Classification acquired by Mobile Laser Scanning (MLS). We \ndescribe how the dataset is obtained from acquisition to post-processing and \nlabeling. This dataset can be used to learn classification algorithm, however, \ngiven that a great attention has been paid to the split between the different \nobjects, this dataset can also be used to learn the segmentation. The dataset \nconsists of around 2km of MLS point cloud acquired in two cities. The number of \npoints and range of classes make us consider that it can be used to train \nDeep-Learning methods. Besides we show some results of automatic segmentation \nand classification. The dataset is available at: \n<a href=\"http://caor-mines-paristech.fr/fr/paris-lille-3d-dataset/\">this http URL</a> \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb642fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00032"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "I&#xf1;igo Urteaga, David J. Albers, Marija Vlajic Wheeler, Anna Druet, Hans Raffauf, No&#xe9;mie Elhadad", "title": "Towards Personalized Modeling of the Female Hormonal Cycle: Experiments with Mechanistic Models and Gaussian Processes. (arXiv:1712.00117v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00117", "type": "text/html"}], "timestampUsec": "1512364308369370", "comments": [], "summary": {"content": "<p>In this paper, we introduce a novel task for machine learning in healthcare, \nnamely personalized modeling of the female hormonal cycle. The motivation for \nthis work is to model the hormonal cycle and predict its phases in time, both \nfor healthy individuals and for those with disorders of the reproductive \nsystem. Because there are individual differences in the menstrual cycle, we are \nparticularly interested in personalized models that can account for individual \nidiosyncracies, towards identifying phenotypes of menstrual cycles. As a first \nstep, we consider the hormonal cycle as a set of observations through time. We \nuse a previously validated mechanistic model to generate realistic hormonal \npatterns, and experiment with Gaussian process regression to estimate their \nvalues over time. Specifically, we are interested in the feasibility of \npredicting menstrual cycle phases under varying learning conditions: number of \ncycles used for training, hormonal measurement noise and sampling rates, and \ninformed vs. agnostic sampling of hormonal measurements. Our results indicate \nthat Gaussian processes can help model the female menstrual cycle. We discuss \nthe implications of our experiments in the context of modeling the female \nmenstrual cycle. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6430e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00117"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zelun Luo, Yuliang Zou, Judy Hoffman, Li Fei-Fei", "title": "Label Efficient Learning of Transferable Representations across Domains and Tasks. (arXiv:1712.00123v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00123", "type": "text/html"}], "timestampUsec": "1512364308369369", "comments": [], "summary": {"content": "<p>We propose a framework that learns a representation transferable across \ndifferent domains and tasks in a label efficient manner. Our approach battles \ndomain shift with a domain adversarial loss, and generalizes the embedding to \nnovel task using a metric learning-based approach. Our model is simultaneously \noptimized on labeled source data and unlabeled or sparsely labeled data in the \ntarget domain. Our method shows compelling results on novel classes within a \nnew domain even when only a few labeled examples per class are available, \noutperforming the prevalent fine-tuning approach. In addition, we demonstrate \nthe effectiveness of our framework on the transfer learning task from image \nobject recognition to video action recognition. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6432a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00123"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512450854, "author": "Tammo Rukat, Dustin Lange, C&#xe9;dric Archambeau", "title": "An interpretable latent variable model for attribute applicability in the Amazon catalogue. (arXiv:1712.00126v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00126", "type": "text/html"}], "timestampUsec": "1512364308369368", "comments": [], "summary": {"content": "<p>Learning attribute applicability of products in the Amazon catalog (e.g., \npredicting that a shoe should have a value for size, but not for battery-type \nat scale is a challenge. The need for an interpretable model is contingent on \n(1) the lack of ground truth training data, (2) the need to utilise prior \ninformation about the underlying latent space and (3) the ability to understand \nthe quality of predictions on new, unseen data. To this end, we develop the \nMaxMachine, a probabilistic latent variable model that learns distributed \nbinary representations, associated to sets of features that are likely to \nco-occur in the data. Layers of MaxMachines can be stacked such that higher \nlayers encode more abstract information. Any set of variables can be clamped to \nencode prior information. We develop fast sampling based posterior inference. \nPreliminary results show that the model improves over the baseline in 17 out of \n19 product groups and provides qualitatively reasonable predictions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6434b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00126"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Muneki Yasuda, Kazuyuki Tanaka", "title": "Susceptibility Propagation by Using Diagonal Consistency. (arXiv:1712.00155v1 [cond-mat.stat-mech])", "alternate": [{"href": "http://arxiv.org/abs/1712.00155", "type": "text/html"}], "timestampUsec": "1512364308369367", "comments": [], "summary": {"content": "<p>A susceptibility propagation that is constructed by combining a belief \npropagation and a linear response method is used for approximate computation \nfor Markov random fields. Herein, we formulate a new, improved susceptibility \npropagation by using the concept of a diagonal matching method that is based on \nmean-field approaches to inverse Ising problems. The proposed susceptibility \npropagation is robust for various network structures, and it is reduced to the \nordinary susceptibility propagation and to the adaptive \nThouless-Anderson-Palmer equation in special cases. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64371", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00155"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexandre Yahi, Rami Vanguri, No&#xe9;mie Elhadad, Nicholas P. Tatonetti", "title": "Generative Adversarial Networks for Electronic Health Records: A Framework for Exploring and Evaluating Methods for Predicting Drug-Induced Laboratory Test Trajectories. (arXiv:1712.00164v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00164", "type": "text/html"}], "timestampUsec": "1512364308369366", "comments": [], "summary": {"content": "<p>Generative Adversarial Networks (GANs) represent a promising class of \ngenerative networks that combine neural networks with game theory. From \ngenerating realistic images and videos to assisting musical creation, GANs are \ntransforming many fields of arts and sciences. However, their application to \nhealthcare has not been fully realized, more specifically in generating \nelectronic health records (EHR) data. In this paper, we propose a framework for \nexploring the value of GANs in the context of continuous laboratory time series \ndata. We devise an unsupervised evaluation method that measures the predictive \npower of synthetic laboratory test time series. Further, we show that when it \ncomes to predicting the impact of drug exposure on laboratory test data, \nincorporating representation learning of the training cohorts prior to training \nGAN models is beneficial. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb643cd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00164"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wenbo Zhao, Yang Gao, Rita Singh, Ming Li", "title": "Speaker identification from the sound of the human breath. (arXiv:1712.00171v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1712.00171", "type": "text/html"}], "timestampUsec": "1512364308369365", "comments": [], "summary": {"content": "<p>This paper examines the speaker identification potential of breath sounds in \ncontinuous speech. Speech is largely produced during exhalation. In order to \nreplenish air in the lungs, speakers must periodically inhale. When inhalation \noccurs in the midst of continuous speech, it is generally through the mouth. \nIntra-speech breathing behavior has been the subject of much study, including \nthe patterns, cadence, and variations in energy levels. However, an often \nignored characteristic is the {\\em sound} produced during the inhalation phase \nof this cycle. Intra-speech inhalation is rapid and energetic, performed with \nopen mouth and glottis, effectively exposing the entire vocal tract to enable \nmaximum intake of air. This results in vocal tract resonances evoked by \nturbulence that are characteristic of the speaker's speech-producing apparatus. \nConsequently, the sounds of inhalation are expected to carry information about \nthe speaker's identity. Moreover, unlike other spoken sounds which are subject \nto active control, inhalation sounds are generally more natural and less \naffected by voluntary influences. The goal of this paper is to demonstrate that \nbreath sounds are indeed bio-signatures that can be used to identify speakers. \nWe show that these sounds by themselves can yield remarkably accurate speaker \nrecognition with appropriate feature representations and classification \nframeworks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb643f1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00171"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chris Wu, Tanay Tandon", "title": "Rapid point-of-care Hemoglobin measurement through low-cost optics and Convolutional Neural Network based validation. (arXiv:1712.00174v1 [physics.med-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.00174", "type": "text/html"}], "timestampUsec": "1512364308369364", "comments": [], "summary": {"content": "<p>A low-cost, robust, and simple mechanism to measure hemoglobin would play a \ncritical role in the modern health infrastructure. Consistent sample \nacquisition has been a long-standing technical hurdle for photometer-based \nportable hemoglobin detectors which rely on micro cuvettes and dry chemistry. \nAny particulates (e.g. intact red blood cells (RBCs), microbubbles, etc.) in a \ncuvette's sensing area drastically impact optical absorption profile, and \ncommercial hemoglobinometers lack the ability to automatically detect faulty \nsamples. We present the ground-up development of a portable, low-cost and open \nplatform with equivalent accuracy to medical-grade devices, with the addition \nof CNN-based image processing for rapid sample viability prechecks. The \ndeveloped platform has demonstrated precision to the nearest $0.18[g/dL]$ of \nhemoglobin, an R^2 = 0.945 correlation to hemoglobin absorption curves reported \nin literature, and a 97% detection accuracy of poorly-prepared samples. We see \nthe developed hemoglobin device/ML platform having massive implications in \nrural medicine, and consider it an excellent springboard for robust deep \nlearning optical spectroscopy: a currently untapped source of data for \ndetection of countless analytes. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64419", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00174"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kelly Peterson, Ognjen (Oggi) Rudovic, Ricardo Guerrero, Rosalind W. Picard", "title": "Personalized Gaussian Processes for Future Prediction of Alzheimer's Disease Progression. (arXiv:1712.00181v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00181", "type": "text/html"}], "timestampUsec": "1512364308369363", "comments": [], "summary": {"content": "<p>In this paper, we introduce the use of a personalized Gaussian Process model \n(pGP) to predict the key metrics of Alzheimer's Disease progression (MMSE, \nADAS-Cog13, CDRSB and CS) based on each patient's previous visits. We start by \nlearning a population-level model using multi-modal data from previously seen \npatients using the base Gaussian Process (GP) regression. Then, this model is \nadapted sequentially over time to a new patient using domain adaptive GPs to \nform the patient's pGP. We show that this new approach, together with an \nauto-regressive formulation, leads to significant improvements in forecasting \nfuture clinical status and cognitive scores for target patients when compared \nto modeling the population with traditional GPs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6442e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00181"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nikos Kargas, Nicholas D. Sidiropoulos, Xiao Fu", "title": "Tensors, Learning, and 'Kolmogorov Extension' for Finite-alphabet Random Vectors. (arXiv:1712.00205v1 [eess.SP])", "alternate": [{"href": "http://arxiv.org/abs/1712.00205", "type": "text/html"}], "timestampUsec": "1512364308369362", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a9d5312\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a9d5312&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Estimating the joint probability mass function (PMF) of a set of random \nvariables lies at the heart of statistical learning and signal processing. \nWithout structural assumptions, such as modeling the variables as a Markov \nchain, tree, or other graphical model, joint PMF estimation is often considered \nmission impossible - the number of unknowns grows exponentially with the number \nof variables. But who gives us the structural model? Is there a generic, \n'non-parametric' way to control joint PMF complexity without relying on a \npriori structural assumptions regarding the underlying probability model? Is it \npossible to discover the operational structure without biasing the analysis up \nfront? What if we only observe random subsets of the variables, can we still \nreliably estimate the joint PMF of all? This paper shows, perhaps surprisingly, \nthat if the joint PMF of any three variables can be estimated, then the joint \nPMF of all the variables can be provably recovered under relatively mild \nconditions. The result is reminiscent of Kolmogorov's extension theorem - \nconsistent specification of lower-order distributions induces a unique \nprobability measure for the entire process. The difference is that for \nprocesses of limited complexity (rank of the high-order PMF) it is possible to \nobtain complete characterization from only third-order distributions. In fact \nnot all third order PMFs are needed; and under more stringent conditions even \nsecond-order will do. Exploiting multilinear (tensor) algebra, this paper \nproves that such higher-order PMF completion can be guaranteed - several \npertinent identifiability results are derived. It also provides a practical and \nefficient algorithm to carry out the recovery task. Judiciously designed \nsimulations and real-data experiments on movie recommendation and data \nclassification are presented to showcase the effectiveness of the approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64443", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00205"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "C&#xe9;sar A. Uribe, Soomin Lee, Alexander Gasnikov, Angelia Nedi&#x107;", "title": "Optimal Algorithms for Distributed Optimization. (arXiv:1712.00232v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1712.00232", "type": "text/html"}], "timestampUsec": "1512364308369361", "comments": [], "summary": {"content": "<p>In this paper, we study the optimal convergence rate for distributed convex \noptimization problems in networks. We model the communication restrictions \nimposed by the network as a set of affine constraints and provide optimal \ncomplexity bounds for four different setups, namely: the function $F(\\xb) \n\\triangleq \\sum_{i=1}^{m}f_i(\\xb)$ is strongly convex and smooth, either \nstrongly convex or smooth or just convex. Our results show that Nesterov's \naccelerated gradient descent on the dual problem can be executed in a \ndistributed manner and obtains the same optimal rates as in the centralized \nversion of the problem (up to constant or logarithmic factors) with an \nadditional cost related to the spectral gap of the interaction matrix. Finally, \nwe discuss some extensions to the proposed setup such as proximal friendly \nfunctions, time-varying graphs, improvement of the condition numbers. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64465", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00232"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tycho Max Sylvester Tax, Jose Luis Diez Antich, Hendrik Purwins, Lars Maal&#xf8;e", "title": "Utilizing Domain Knowledge in End-to-End Audio Processing. (arXiv:1712.00254v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1712.00254", "type": "text/html"}], "timestampUsec": "1512364308369360", "comments": [], "summary": {"content": "<p>End-to-end neural network based approaches to audio modelling are generally \noutperformed by models trained on high-level data representations. In this \npaper we present preliminary work that shows the feasibility of training the \nfirst layers of a deep convolutional neural network (CNN) model to learn the \ncommonly-used log-scaled mel-spectrogram transformation. Secondly, we \ndemonstrate that upon initializing the first layers of an end-to-end CNN \nclassifier with the learned transformation, convergence and performance on the \nESC-50 environmental sound classification dataset are similar to a CNN-based \nmodel trained on the highly pre-processed log-scaled mel-spectrogram features. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64476", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00254"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nikolay Jetchev, Urs Bergmann, Calvin Seward", "title": "GANosaic: Mosaic Creation with Generative Texture Manifolds. (arXiv:1712.00269v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.00269", "type": "text/html"}], "timestampUsec": "1512364308369359", "comments": [], "summary": {"content": "<p>This paper presents a novel framework for generating texture mosaics with \nconvolutional neural networks. Our method is called GANosaic and performs \noptimization in the latent noise space of a generative texture model, which \nallows the transformation of a content image into a mosaic exhibiting the \nvisual properties of the underlying texture manifold. To represent that \nmanifold, we use a state-of-the-art generative adversarial method for texture \nsynthesis, which can learn expressive texture representations from data and \nproduce mosaic images with very high resolution. This fully convolutional model \ngenerates smooth (without any visible borders) mosaic images which morph and \nblend different textures locally. In addition, we develop a new type of \ndifferentiable statistical regularization appropriate for optimization over the \nprior noise space of the PSGAN model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64488", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00269"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Stefan Webb, Adam Golinski, Robert Zinkov, N. Siddharth, Yee Whye Teh, Frank Wood", "title": "Faithful Model Inversion Substantially Improves Auto-encoding Variational Inference. (arXiv:1712.00287v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00287", "type": "text/html"}], "timestampUsec": "1512364308369358", "comments": [], "summary": {"content": "<p>In learning deep generative models, the encoder for variational inference is \ntypically formed in an ad hoc manner with a structure and parametrization \nanalogous to the forward model. Our chief insight is that this results in \ncoarse approximations to the posterior, and that the d-separation properties of \nthe BN structure of the forward model should be used, in a principled way, to \nproduce ones that are faithful to the posterior, for which we introduce the \nnovel Compact Minimal I-map (CoMI) algorithm. Applying our method to common \nmodels reveals that standard encoder design choices lack many important edges, \nand through experiments we demonstrate that modelling these edges is important \nfor optimal learning. We show how using a faithful encoder is crucial when \nmodelling with continuous relaxations of categorical distributions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64494", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00287"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Thomas Brouwer, Pietro Lio&#x27;", "title": "Prior and Likelihood Choices for Bayesian Matrix Factorisation on Small Datasets. (arXiv:1712.00288v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00288", "type": "text/html"}], "timestampUsec": "1512364308369357", "comments": [], "summary": {"content": "<p>In this paper, we study the effects of different prior and likelihood choices \nfor Bayesian matrix factorisation, focusing on small datasets. These choices \ncan greatly influence the predictive performance of the methods. We identify \nfour groups of approaches: Gaussian-likelihood with real-valued priors, \nnonnegative priors, semi-nonnegative models, and finally Poisson-likelihood \napproaches. For each group we review several models from the literature, \nconsidering sixteen in total, and discuss the relations between different \npriors and matrix norms. We extensively compare these methods on eight \nreal-world datasets across three application areas, giving both inter- and \nintra-group comparisons. We measure convergence runtime speed, cross-validation \nperformance, sparse and noisy prediction performance, and model selection \nrobustness. We offer several insights into the trade-offs between prior and \nlikelihood choices for Bayesian matrix factorisation on small datasets - such \nas that Poisson models give poor predictions, and that nonnegative models are \nmore constrained than real-valued ones. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644a9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00288"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512537806, "author": "Jakub M. Tomczak, Maximilian Ilse, Max Welling", "title": "Deep Learning with Permutation-invariant Operator for Multi-instance Histopathology Classification. (arXiv:1712.00310v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00310", "type": "text/html"}], "timestampUsec": "1512364308369356", "comments": [], "summary": {"content": "<p>The computer-aided analysis of medical scans is a longstanding goal in the \nmedical imaging field. Currently, deep learning has became a dominant \nmethodology for supporting pathologists and radiologist. Deep learning \nalgorithms have been successfully applied to digital pathology and radiology, \nnevertheless, there are still practical issues that prevent these tools to be \nwidely used in practice. The main obstacles are low number of available cases \nand large size of images (a.k.a. the small n, large p problem in machine \nlearning), and a very limited access to annotation at a pixel level that can \nlead to severe overfitting and large computational requirements. We propose to \nhandle these issues by introducing a framework that processes a medical image \nas a collection of small patches using a single, shared neural network. The \nfinal diagnosis is provided by combining scores of individual patches using a \npermutation-invariant operator (combination). In machine learning community \nsuch approach is called a multi-instance learning (MIL). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644b3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00310"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Marc Oliu, Javier Selva, Sergio Escalera", "title": "Folded Recurrent Neural Networks for Future Video Prediction. (arXiv:1712.00311v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.00311", "type": "text/html"}], "timestampUsec": "1512364308369355", "comments": [], "summary": {"content": "<p>Future video prediction is an ill-posed Computer Vision problem that recently \nreceived much attention. Its main challenges are the high variability in video \ncontent, the propagation of errors through time, and the non-specificity of the \nfuture frames: given a sequence of past frames there is a continuous \ndistribution of possible futures. This work introduces bijective Gated \nRecurrent Units, a double mapping between the input and output of a GRU layer. \nThis allows for recurrent auto-encoders with state sharing between encoder and \ndecoder, stratifying the sequence representation and helping to prevent \ncapacity problems. We show how with this topology only the encoder or decoder \nneeds to be applied for input encoding and prediction, respectively. This \nreduces the computational cost and avoids re-encoding the predictions when \ngenerating a sequence of frames, mitigating the propagation of errors. \nFurthermore, it is possible to remove layers from an already trained model, \ngiving an insight to the role performed by each layer and making the model more \nexplainable. We evaluate our approach on three video datasets, outperforming \nstate of the art prediction results on MMNIST and UCF101, and obtaining \ncompetitive results on KTH with 2 and 3 times less memory usage and \ncomputational cost than the best scored approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644bf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00311"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hongbin Pei, Bo Yang, Jiming Liu, Lei Dong", "title": "Group Sparse Bayesian Learning for Active Surveillance on Epidemic Dynamics. (arXiv:1712.00328v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00328", "type": "text/html"}], "timestampUsec": "1512364308369354", "comments": [], "summary": {"content": "<p>Predicting epidemic dynamics is of great value in understanding and \ncontrolling diffusion processes, such as infectious disease spread and \ninformation propagation. This task is intractable, especially when surveillance \nresources are very limited. To address the challenge, we study the problem of \nactive surveillance, i.e., how to identify a small portion of system components \nas sentinels to effect monitoring, such that the epidemic dynamics of an entire \nsystem can be readily predicted from the partial data collected by such \nsentinels. We propose a novel measure, the gamma value, to identify the \nsentinels by modeling a sentinel network with row sparsity structure. We design \na flexible group sparse Bayesian learning algorithm to mine the sentinel \nnetwork suitable for handling both linear and non-linear dynamical systems by \nusing the expectation maximization method and variational approximation. The \nefficacy of the proposed algorithm is theoretically analyzed and empirically \nvalidated using both synthetic and real-world data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644c5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00328"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Adrien Lagrange, Mathieu Fauvel, St&#xe9;phane May, Nicolas Dobigeon", "title": "Hierarchical Bayesian image analysis: from low-level modeling to robust supervised learning. (arXiv:1712.00368v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.00368", "type": "text/html"}], "timestampUsec": "1512364308369353", "comments": [], "summary": {"content": "<p>Within a supervised classification framework, labeled data are used to learn \nclassifier parameters. Prior to that, it is generally required to perform \ndimensionality reduction via feature extraction. These preprocessing steps have \nmotivated numerous research works aiming at recovering latent variables in an \nunsupervised context. This paper proposes a unified framework to perform \nclassification and low-level modeling jointly. The main objective is to use the \nestimated latent variables as features for classification and to incorporate \nsimultaneously supervised information to help latent variable extraction. The \nproposed hierarchical Bayesian model is divided into three stages: a first \nlow-level modeling stage to estimate latent variables, a second stage \nclustering these features into statistically homogeneous groups and a last \nclassification stage exploiting the (possibly badly) labeled data. Performance \nof the model is assessed in the specific context of hyperspectral image \ninterpretation, unifying two standard analysis techniques, namely unmixing and \nclassification. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644ca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00368"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, Yanqi Zhou", "title": "Deep Learning Scaling is Predictable, Empirically. (arXiv:1712.00409v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00409", "type": "text/html"}], "timestampUsec": "1512364308369352", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32a9d55dc\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32a9d55dc&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep learning (DL) creates impactful advances following a virtuous recipe: \nmodel architecture search, creating large training data sets, and scaling \ncomputation. It is widely believed that growing training sets and models should \nimprove accuracy and result in better products. As DL application domains grow, \nwe would like a deeper understanding of the relationships between training set \nsize, computational scale, and model accuracy improvements to advance the \nstate-of-the-art. \n</p> \n<p>This paper presents a large scale empirical characterization of \ngeneralization error and model size growth as training sets grow. We introduce \na methodology for this measurement and test four machine learning domains: \nmachine translation, language modeling, image processing, and speech \nrecognition. Our empirical results show power-law generalization error scaling \nacross a breadth of factors, resulting in power-law exponents---the \"steepness\" \nof the learning curve---yet to be explained by theoretical work. Further, model \nimprovements only shift the error but do not appear to affect the power-law \nexponent. We also show that model size scales sublinearly with data size. These \nscaling relationships have significant implications on deep learning research, \npractice, and systems. They can assist model debugging, setting accuracy \ntargets, and decisions about data set growth. They can also guide computing \nsystem design and underscore the importance of continued computational scaling. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00409"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "James T. Wilson, Riccardo Moriconi, Frank Hutter, Marc Peter Deisenroth", "title": "The reparameterization trick for acquisition functions. (arXiv:1712.00424v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00424", "type": "text/html"}], "timestampUsec": "1512364308369351", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32aa2be97\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32aa2be97&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Bayesian optimization is a sample-efficient approach to solving global \noptimization problems. Along with a surrogate model, this approach relies on \ntheoretically motivated value heuristics (acquisition functions) to guide the \nsearch process. Maximizing acquisition functions yields the best performance; \nunfortunately, this ideal is difficult to achieve since optimizing acquisition \nfunctions per se is frequently non-trivial. This statement is especially true \nin the parallel setting, where acquisition functions are routinely non-convex, \nhigh-dimensional, and intractable. Here, we demonstrate how many popular \nacquisition functions can be formulated as Gaussian integrals amenable to the \nreparameterization trick and, ensuingly, gradient-based optimization. Further, \nwe use this reparameterized representation to derive an efficient Monte Carlo \nestimator for the upper confidence bound acquisition function in the context of \nparallel selection. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644db", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00424"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiaoyu Liu, Diyu Yang, Aly El Gamal", "title": "Deep Neural Network Architectures for Modulation Classification. (arXiv:1712.00443v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00443", "type": "text/html"}], "timestampUsec": "1512364308369350", "comments": [], "summary": {"content": "<p>In this work, we investigate the value of employing deep learning for the \ntask of wireless signal modulation recognition. Recently in [1], a framework \nhas been introduced by generating a dataset using GNU radio that mimics the \nimperfections in a real wireless channel, and uses 11 different modulation \ntypes. Further, a convolutional neural network (CNN) architecture was developed \nand shown to deliver performance that exceeds that of expert-based approaches. \nHere, we follow the framework of [1] and find deep neural network architectures \nthat deliver higher accuracy than the state of the art. We tested the \narchitecture of [1] and found it to achieve an accuracy of approximately 75% of \ncorrectly recognizing the modulation type. We first tune the CNN architecture \nof [1] and find a design with four convolutional layers and two dense layers \nthat gives an accuracy of approximately 83.8% at high SNR. We then develop \narchitectures based on the recently introduced ideas of Residual Networks \n(ResNet [2]) and Densely Connected Networks (DenseNet [3]) to achieve high SNR \naccuracies of approximately 83.5% and 86.6%, respectively. Finally, we \nintroduce a Convolutional Long Short-term Deep Neural Network (CLDNN [4]) to \nachieve an accuracy of approximately 88.5% at high SNR. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644e5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00443"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Michael Riis Andersen, Aki Vehtari, Ole Winther, Lars Kai Hansen", "title": "Bayesian inference for spatio-temporal spike-and-slab priors. (arXiv:1509.04752v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1509.04752", "type": "text/html"}], "timestampUsec": "1512364308369349", "comments": [], "summary": {"content": "<p>In this work, we address the problem of solving a series of underdetermined \nlinear inverse problems subject to a sparsity constraint. We generalize the \nspike-and-slab prior distribution to encode a priori correlation of the support \nof the solution in both space and time by imposing a transformed Gaussian \nprocess on the spike-and-slab probabilities. An expectation propagation (EP) \nalgorithm for posterior inference under the proposed model is derived. For \nlarge scale problems, the standard EP algorithm can be prohibitively slow. We \ntherefore introduce three different approximation schemes to reduce the \ncomputational complexity. Finally, we demonstrate the proposed model using \nnumerical experiments based on both synthetic and real data sets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644eb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1509.04752"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiaohan Yan, Jacob Bien", "title": "Hierarchical Sparse Modeling: A Choice of Two Group Lasso Formulations. (arXiv:1512.01631v4 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1512.01631", "type": "text/html"}], "timestampUsec": "1512364308369348", "comments": [], "summary": {"content": "<p>Demanding sparsity in estimated models has become a routine practice in \nstatistics. In many situations, we wish to require that the sparsity patterns \nattained honor certain problem-specific constraints. Hierarchical sparse \nmodeling (HSM) refers to situations in which these constraints specify that one \nset of parameters be set to zero whenever another is set to zero. In recent \nyears, numerous papers have developed convex regularizers for this form of \nsparsity structure, which arises in many areas of statistics including \ninteraction modeling, time series analysis, and covariance estimation. In this \npaper, we observe that these methods fall into two frameworks, the group lasso \n(GL) and latent overlapping group lasso (LOG), which have not been \nsystematically compared in the context of HSM. The purpose of this paper is to \nprovide a side-by-side comparison of these two frameworks for HSM in terms of \ntheir statistical properties and computational efficiency. We call special \nattention to GL's more aggressive shrinkage of parameters deep in the \nhierarchy, a property not shared by LOG. In terms of computation, we introduce \na finite-step algorithm that exactly solves the proximal operator of LOG for a \ncertain simple HSM structure; we later exploit this to develop a novel \npath-based block coordinate descent scheme for general HSM structures. Both \nalgorithms greatly improve the computational performance of LOG. Finally, we \ncompare the two methods in the context of covariance estimation, where we \nintroduce a new sparsely-banded estimator using LOG, which we show achieves the \nstatistical advantages of an existing GL-based method but is simpler to express \nand more efficient to compute. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1512.01631"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ravi Kiran Raman, Lav R. Varshney", "title": "Universal Joint Image Clustering and Registration using Partition Information. (arXiv:1701.02776v2 [cs.IT] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1701.02776", "type": "text/html"}], "timestampUsec": "1512364308369347", "comments": [], "summary": {"content": "<p>We consider the problem of universal joint clustering and registration of \nimages and define algorithms using multivariate information functionals. We \nfirst study registering two images using maximum mutual information and prove \nits asymptotic optimality. We then show the shortcomings of pairwise \nregistration in multi-image registration, and design an asymptotically optimal \nalgorithm based on multiinformation. Further, we define a novel multivariate \ninformation functional to perform joint clustering and registration of images, \nand prove consistency of the algorithm. Finally, we consider registration and \nclustering of numerous limited-resolution images, defining algorithms that are \norder-optimal in scaling of number of pixels in each image with the number of \nimages. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6451a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1701.02776"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jen-Cheng Hou, Syu-Siang Wang, Ying-Hui Lai, Yu Tsao, Hsiu-Wen Chang, Hsin-Min Wang", "title": "Audio-Visual Speech Enhancement based on Multimodal Deep Convolutional Neural Networks. (arXiv:1703.10893v4 [cs.SD] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.10893", "type": "text/html"}], "timestampUsec": "1512364308369346", "comments": [], "summary": {"content": "<p>Speech enhancement (SE) aims to reduce noise in speech signals. Most SE \ntechniques focus only on addressing audio information. In this work, inspired \nby multimodal learning, which utilizes data from different modalities, and the \nrecent success of convolutional neural networks (CNNs) in SE, we propose an \naudio-visual deep CNNs (AVDCNN) SE model, which incorporates audio and visual \nstreams into a unified network model. We also propose a multi-task learning \nframework for reconstructing audio and visual signals at the output layer. \nPrecisely speaking, the proposed AVDCNN model is structured as an audio-visual \nencoder-decoder network, in which audio and visual data are first processed \nusing individual CNNs, and then fused into a joint network to generate enhanced \nspeech (the primary task) and reconstructed images (the secondary task) at the \noutput layer. The model is trained in an end-to-end manner, and parameters are \njointly learned through back-propagation. We evaluate enhanced speech using \nfive instrumental criteria. Results show that the AVDCNN model yields a notably \nsuperior performance compared with an audio-only CNN-based SE model and two \nconventional SE approaches, confirming the effectiveness of integrating visual \ninformation into the SE process. In addition, the AVDCNN model also outperforms \nan existing audio-visual SE model, confirming its capability of effectively \ncombining audio and visual information in SE. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64538", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.10893"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, Cynthia Rudin", "title": "Learning Certifiably Optimal Rule Lists for Categorical Data. (arXiv:1704.01701v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.01701", "type": "text/html"}], "timestampUsec": "1512364308369345", "comments": [], "summary": {"content": "<p>We present the design and implementation of a custom discrete optimization \ntechnique for building rule lists over a categorical feature space. Our \nalgorithm produces rule lists with optimal training performance, according to \nthe regularized empirical risk, with a certificate of optimality. By leveraging \nalgorithmic bounds, efficient data structures, and computational reuse, we \nachieve several orders of magnitude speedup in time and a massive reduction of \nmemory consumption. We demonstrate that our approach produces optimal rule \nlists on practical problems in seconds. Our results indicate that it is \npossible to construct optimal sparse rule lists that are approximately as \naccurate as the COMPAS proprietary risk prediction tool on data from Broward \nCounty, Florida, but that are completely interpretable. This framework is a \nnovel alternative to CART and other decision tree methods for interpretable \nmodeling. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6455b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.01701"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kamil Ciosek, Shimon Whiteson", "title": "Expected Policy Gradients. (arXiv:1706.05374v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.05374", "type": "text/html"}], "timestampUsec": "1512364308369344", "comments": [], "summary": {"content": "<p>We propose expected policy gradients (EPG), which unify stochastic policy \ngradients (SPG) and deterministic policy gradients (DPG) for reinforcement \nlearning. Inspired by expected sarsa, EPG integrates across the action when \nestimating the gradient, instead of relying only on the action in the sampled \ntrajectory. We establish a new general policy gradient theorem, of which the \nstochastic and deterministic policy gradient theorems are special cases. We \nalso prove that EPG reduces the variance of the gradient estimates without \nrequiring deterministic policies and, for the Gaussian case, with no \ncomputational overhead. Finally, we show that it is optimal in a certain sense \nto explore with a Gaussian policy such that the covariance is proportional to \nthe exponential of the scaled Hessian of the critic with respect to the \nactions. We present empirical results confirming that this new form of \nexploration substantially outperforms DPG with the Ornstein-Uhlenbeck heuristic \nin four challenging MuJoCo domains. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6456e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.05374"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le", "title": "Learning Transferable Architectures for Scalable Image Recognition. (arXiv:1707.07012v3 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.07012", "type": "text/html"}], "timestampUsec": "1512364308369343", "comments": [], "summary": {"content": "<p>Developing neural network image classification models often requires \nsignificant architecture engineering. In this paper, we attempt to automate \nthis engineering process by learning the model architectures directly on the \ndataset of interest. As this approach is expensive when the dataset is large, \nwe propose to search for an architectural building block on a small dataset and \nthen transfer the block to a larger dataset. Our key contribution is the design \nof a new search space which enables transferability. In our experiments, we \nsearch for the best convolutional layer (or \"cell\") on the CIFAR-10 dataset and \nthen apply this cell to the ImageNet dataset by stacking together more copies \nof this cell, each with their own parameters. Although the cell is not searched \nfor directly on ImageNet, an architecture constructed from the best cell \nachieves, among the published works, state-of-the-art accuracy of 82.7% top-1 \nand 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than \nthe best human-invented architectures while having 9 billion fewer FLOPS -- a \nreduction of 28% in computational demand from the previous state-of-the-art \nmodel. When evaluated at different levels of computational cost, accuracies of \nour models exceed those of the state-of-the-art human-designed models. For \ninstance, a smaller network constructed from the best cell also achieves 74% \ntop-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art \nmodels for mobile platforms. On CIFAR-10, an architecture constructed from the \nbest cell achieves 2.4% error rate, which is also state-of-the-art. Finally, \nthe image features learned from image classification can also be transferred to \nother computer vision problems. On the task of object detection, the learned \nfeatures used with the Faster-RCNN framework surpass state-of-the-art by 4.0% \nachieving 43.1% mAP on the COCO dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64586", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.07012"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christian Donner, Manfred Opper", "title": "Inverse Ising problem in continuous time: A latent variable approach. (arXiv:1709.04495v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04495", "type": "text/html"}], "timestampUsec": "1512364308369342", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32aa2c2a7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32aa2c2a7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We consider the inverse Ising problem, i.e. the inference of network \ncouplings from observed spin trajectories for a model with continuous time \nGlauber dynamics. By introducing two sets of auxiliary latent random variables \nwe render the likelihood into a form, which allows for simple iterative \ninference algorithms with analytical updates. The variables are: (1) Poisson \nvariables to linearise an exponential term which is typical for point process \nlikelihoods and (2) P\\'olya-Gamma variables, which make the likelihood \nquadratic in the coupling parameters. Using the augmented likelihood, we derive \nan expectation-maximization (EM) algorithm to obtain the maximum likelihood \nestimate of network parameters. Using a third set of latent variables we extend \nthe EM algorithm to sparse couplings via L1 regularization. Finally, we develop \nan efficient approximate Bayesian inference algorithm using a variational \napproach. We demonstrate the performance of our algorithms on data simulated \nfrom an Ising model. For data which are simulated from a more biologically \nplausible network with spiking neurons, we show that the Ising model captures \nwell the low order statistics of the data and how the Ising couplings are \nrelated to the underlying synaptic structure of the simulated network. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6459d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04495"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Iqbal H. Sarker, Muhammad Ashad Kabir, Alan Colman, Jun Han", "title": "An Improved Naive Bayes Classifier-based Noise Detection Technique for Classifying User Phone Call Behavior. (arXiv:1710.04461v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.04461", "type": "text/html"}], "timestampUsec": "1512364308369341", "comments": [], "summary": {"content": "<p>The presence of noisy instances in mobile phone data is a fundamental issue \nfor classifying user phone call behavior (i.e., accept, reject, missed and \noutgoing), with many potential negative consequences. The classification \naccuracy may decrease and the complexity of the classifiers may increase due to \nthe number of redundant training samples. To detect such noisy instances from a \ntraining dataset, researchers use naive Bayes classifier (NBC) as it identifies \nmisclassified instances by taking into account independence assumption and \nconditional probabilities of the attributes. However, some of these \nmisclassified instances might indicate usages behavioral patterns of individual \nmobile phone users. Existing naive Bayes classifier based noise detection \ntechniques have not considered this issue and, thus, are lacking in \nclassification accuracy. In this paper, we propose an improved noise detection \ntechnique based on naive Bayes classifier for effectively classifying users' \nphone call behaviors. In order to improve the classification accuracy, we \neffectively identify noisy instances from the training dataset by analyzing the \nbehavioral patterns of individuals. We dynamically determine a noise threshold \naccording to individual's unique behavioral patterns by using both the naive \nBayes classifier and Laplace estimator. We use this noise threshold to identify \nnoisy instances. To measure the effectiveness of our technique in classifying \nuser phone call behavior, we employ the most popular classification algorithm \n(e.g., decision tree). Experimental results on the real phone call log dataset \nshow that our proposed technique more accurately identifies the noisy instances \nfrom the training datasets that leads to better classification accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb645b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.04461"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin", "title": "Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples. (arXiv:1711.09325v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09325", "type": "text/html"}], "timestampUsec": "1512364308369340", "comments": [], "summary": {"content": "<p>The problem of detecting whether a test sample is from in-distribution (i.e., \ntraining distribution by a classifier) or out-of-distribution sufficiently \ndifferent from it arises in many real-world machine learning applications. \nHowever, the state-of-art deep neural networks are known to be highly \noverconfident in their predictions, i.e., do not distinguish in- and \nout-of-distributions. Recently, to handle this issue, several threshold-based \ndetectors have been proposed given pre-trained neural classifiers. However, the \nperformance of prior works highly depends on how to train the classifiers since \nthey only focus on improving inference procedures. In this paper, we develop a \nnovel training method for classifiers so that such inference algorithms can \nwork better. In particular, we suggest two additional terms added to the \noriginal loss (e.g., cross entropy). The first one forces samples from \nout-of-distribution less confident by the classifier and the second one is for \n(implicitly) generating most effective training samples for the first one. In \nessence, our method jointly trains both classification and generative neural \nnetworks for out-of-distribution. We demonstrate its effectiveness using deep \nconvolutional neural networks on various popular image datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb645cd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09325"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Harish S. Bhat, Sidra J. Goldman-Mellor", "title": "Predicting Adolescent Suicide Attempts with Neural Networks. (arXiv:1711.10057v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10057", "type": "text/html"}], "timestampUsec": "1512364308369339", "comments": [], "summary": {"content": "<p>Though suicide is a major public health problem in the US, machine learning \nmethods are not commonly used to predict an individual's risk of \nattempting/committing suicide. In the present work, starting with an anonymized \ncollection of electronic health records for 522,056 unique, California-resident \nadolescents, we develop neural network models to predict suicide attempts. We \nframe the problem as a binary classification problem in which we use a \npatient's data from 2006-2009 to predict either the presence (1) or absence (0) \nof a suicide attempt in 2010. After addressing issues such as severely \nimbalanced classes and the variable length of a patient's history, we build \nneural networks with depths varying from two to eight hidden layers. For test \nset observations where we have at least five ED/hospital visits' worth of data \non a patient, our depth-4 model achieves a sensitivity of 0.703, specificity of \n0.980, and AUC of 0.958. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb645da", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10057"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sebastian Urban, Marcus Basalla, Patrick van der Smagt", "title": "Gaussian Process Neurons Learn Stochastic Activation Functions. (arXiv:1711.11059v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11059", "type": "text/html"}], "timestampUsec": "1512136966936910", "comments": [], "summary": {"content": "<p>We propose stochastic, non-parametric activation functions that are fully \nlearnable and individual to each neuron. Complexity and the risk of overfitting \nare controlled by placing a Gaussian process prior over these functions. The \nresult is the Gaussian process neuron, a probabilistic unit that can be used as \nthe basic building block for probabilistic graphical models that resemble the \nstructure of neural networks. The proposed model can intrinsically handle \nuncertainties in its inputs and self-estimate the confidence of its \npredictions. Using variational Bayesian inference and the central limit \ntheorem, a fully deterministic loss function is derived, allowing it to be \ntrained as efficiently as a conventional neural network using mini-batch \ngradient descent. The posterior distribution of activation functions is \ninferred from the training data alongside the weights of the network. \n</p> \n<p>The proposed model favorably compares to deep Gaussian processes, both in \nmodel complexity and efficiency of inference. It can be directly applied to \nrecurrent or convolutional network structures, allowing its use in audio and \nimage processing tasks. \n</p> \n<p>As an preliminary empirical evaluation we present experiments on regression \nand classification tasks, in which our model achieves performance comparable to \nor better than a Dropout regularized neural network with a fixed activation \nfunction. Experiments are ongoing and results will be added as they become \navailable. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2c9fea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11059"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yudong Cao, Gian Giacomo Guerreschi, Al&#xe1;n Aspuru-Guzik", "title": "Quantum Neuron: an elementary building block for machine learning on quantum computers. (arXiv:1711.11240v1 [quant-ph])", "alternate": [{"href": "http://arxiv.org/abs/1711.11240", "type": "text/html"}], "timestampUsec": "1512136966936909", "comments": [], "summary": {"content": "<p>Even the most sophisticated artificial neural networks are built by \naggregating substantially identical units called neurons. A neuron receives \nmultiple signals, internally combines them, and applies a non-linear function \nto the resulting weighted sum. Several attempts to generalize neurons to the \nquantum regime have been proposed, but all proposals collided with the \ndifficulty of implementing non-linear activation functions, which is essential \nfor classical neurons, due to the linear nature of quantum mechanics. Here we \npropose a solution to this roadblock in the form of a small quantum circuit \nthat naturally simulates neurons with threshold activation. Our quantum circuit \ndefines a building block, the \"quantum neuron\", that can reproduce a variety of \nclassical neural network constructions while maintaining the ability to process \nsuperpositions of inputs and preserve quantum coherence and entanglement. In \nthe construction of feedforward networks of quantum neurons, we provide \nnumerical evidence that the network not only can learn a function when trained \nwith superposition of inputs and the corresponding output, but that this \ntraining suffices to learn the function on all individual inputs separately. \nWhen arranged to mimic Hopfield networks, quantum neural networks exhibit \nproperties of associative memory. Patterns are encoded using the simple Hebbian \nrule for the weights and we demonstrate attractor dynamics from corrupted \ninputs. Finally, the fact that our quantum model closely captures (traditional) \nneural network dynamics implies that the vast body of literature and results on \nneural networks becomes directly relevant in the context of quantum machine \nlearning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2c9ff5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11240"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christopher Tegho, Pawe&#x142; Budzianowski, Milica Ga&#x161;i&#x107;", "title": "Uncertainty Estimates for Efficient Neural Network-based Dialogue Policy Optimisation. (arXiv:1711.11486v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11486", "type": "text/html"}], "timestampUsec": "1512136966936908", "comments": [], "summary": {"content": "<p>In statistical dialogue management, the dialogue manager learns a policy that \nmaps a belief state to an action for the system to perform. Efficient \nexploration is key to successful policy optimisation. Current deep \nreinforcement learning methods are very promising but rely on epsilon-greedy \nexploration, thus subjecting the user to a random choice of action during \nlearning. Alternative approaches such as Gaussian Process SARSA (GPSARSA) \nestimate uncertainties and are sample efficient, leading to better user \nexperience, but on the expense of a greater computational complexity. This \npaper examines approaches to extract uncertainty estimates from deep Q-networks \n(DQN) in the context of dialogue management. We perform an extensive benchmark \nof deep Bayesian methods to extract uncertainty estimates, namely \nBayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble and \nalpha-divergences, combining it with DQN algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca004", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11486"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513669230, "author": "Antti Tarvainen, Harri Valpola", "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. (arXiv:1703.01780v4 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.01780", "type": "text/html"}], "timestampUsec": "1512136966936907", "comments": [], "summary": {"content": "<p>The recently proposed Temporal Ensembling has achieved state-of-the-art \nresults in several semi-supervised learning benchmarks. It maintains an \nexponential moving average of label predictions on each training example, and \npenalizes predictions that are inconsistent with this target. However, because \nthe targets change only once per epoch, Temporal Ensembling becomes unwieldy \nwhen learning large datasets. To overcome this problem, we propose Mean \nTeacher, a method that averages model weights instead of label predictions. As \nan additional benefit, Mean Teacher improves test accuracy and enables training \nwith fewer labels than Temporal Ensembling. Without changing the network \narchitecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250 \nlabels, outperforming Temporal Ensembling trained with 1000 labels. We also \nshow that a good network architecture is crucial to performance. Combining Mean \nTeacher and Residual Networks, we improve the state of the art on CIFAR-10 with \n4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels \nfrom 35.24% to 9.11%. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca010", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.01780"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513055251, "author": "Rafa&#x142; Muszy&#x144;ski, Jun Wang", "title": "Happiness Pursuit: Personality Learning in a Society of Agents. (arXiv:1711.11068v2 [cs.MA] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.11068", "type": "text/html"}], "timestampUsec": "1512136966936906", "comments": [], "summary": {"content": "<p>Modeling personality is a challenging problem with applications spanning \ncomputer games, virtual assistants, online shopping and education. Many \ntechniques have been tried, ranging from neural networks to computational \ncognitive architectures. However, most approaches rely on examples with \nhand-crafted features and scenarios. Here, we approach learning a personality \nby training agents using a Deep Q-Network (DQN) model on rewards based on \npsychoanalysis, against hand-coded AI in the game of Pong. As a result, we \nobtain 4 agents, each with its own personality. Then, we define happiness of an \nagent, which can be seen as a measure of alignment with agent's objective \nfunction, and study it when agents play both against hand-coded AI, and against \neach other. We find that the agents that achieve higher happiness during \ntesting against hand-coded AI, have lower happiness when competing against each \nother. This suggests that higher happiness in testing is a sign of overfitting \nin learning to interact with hand-coded AI, and leads to worse performance \nagainst agents with different personalities. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca01f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11068"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Adit Krishnan, Ashish Sharma, Hari Sundaram", "title": "Improving Latent User Models in Online Social Media. (arXiv:1711.11124v1 [cs.SI])", "alternate": [{"href": "http://arxiv.org/abs/1711.11124", "type": "text/html"}], "timestampUsec": "1512136966936905", "comments": [], "summary": {"content": "<p>Modern social platforms are characterized by the presence of rich \nuser-behavior data associated with the publication, sharing and consumption of \ntextual content. Users interact with content and with each other in a complex \nand dynamic social environment while simultaneously evolving over time. In \norder to effectively characterize users and predict their future behavior in \nsuch a setting, it is necessary to overcome several challenges. Content \nheterogeneity and temporal inconsistency of behavior data result in severe \nsparsity at the user level. In this paper, we propose a novel \nmutual-enhancement framework to simultaneously partition and learn latent \nactivity profiles of users. We propose a flexible user partitioning approach to \neffectively discover rare behaviors and tackle user-level sparsity. We \nextensively evaluate the proposed framework on massive datasets from real-world \nplatforms including Q&amp;A networks and interactive online courses (MOOCs). Our \nresults indicate significant gains over state-of-the-art behavior models ( 15% \navg ) in a varied range of tasks and our gains are further magnified for users \nwith limited interaction data. The proposed algorithms are amenable to \nparallelization, scale linearly in the size of datasets, and provide \nflexibility to model diverse facets of user behavior. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca029", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11124"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang, William Yang Wang", "title": "Video Captioning via Hierarchical Reinforcement Learning. (arXiv:1711.11135v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1711.11135", "type": "text/html"}], "timestampUsec": "1512136966936904", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32aa2c57d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32aa2c57d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Video captioning is the task of automatically generating a textual \ndescription of the actions in a video. Although previous work (e.g. \nsequence-to-sequence model) has shown promising results in abstracting a coarse \ndescription of a short video, it is still very challenging to caption a video \ncontaining multiple fine-grained actions with a detailed description. This \npaper aims to address the challenge by proposing a novel hierarchical \nreinforcement learning framework for video captioning, where a high-level \nManager module learns to design sub-goals and a low-level Worker module \nrecognizes the primitive actions to fulfill the sub-goal. With this \ncompositional framework to reinforce video captioning at different levels, our \napproach significantly outperforms all the baseline methods on a newly \nintroduced large-scale dataset for fine-grained video captioning. Furthermore, \nour non-ensemble model has already achieved the state-of-the-art results on the \nwidely-used MSR-VTT dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca032", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11135"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, Guy Van den Broeck", "title": "A Semantic Loss Function for Deep Learning with Symbolic Knowledge. (arXiv:1711.11157v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1711.11157", "type": "text/html"}], "timestampUsec": "1512136966936903", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32aa8310b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32aa8310b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper develops a novel methodology for using symbolic knowledge in deep \nlearning. From first principles, we derive a semantic loss function that \nbridges between neural output vectors and logical constraints. This loss \nfunction captures how close the neural network is to satisfying the constraints \non its output. An experimental evaluation shows that our semantic loss function \neffectively guides the learner to achieve (near-)state-of-the-art results on \nsemi-supervised multi-class classification. Moreover, it significantly \nincreases the ability of the neural network to predict structured objects, such \nas rankings and paths. These discrete concepts are tremendously difficult to \nlearn, and benefit from a tight integration of deep learning and symbolic \nreasoning methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca03d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11157"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sahin Cem Geyik, Jianqiang Shen, Shahriar Shariat, Ali Dasdan, Santanu Kolay", "title": "Towards Data Quality Assessment in Online Advertising. (arXiv:1711.11175v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1711.11175", "type": "text/html"}], "timestampUsec": "1512136966936902", "comments": [], "summary": {"content": "<p>In online advertising, our aim is to match the advertisers with the most \nrelevant users to optimize the campaign performance. In the pursuit of \nachieving this goal, multiple data sources provided by the advertisers or \nthird-party data providers are utilized to choose the set of users according to \nthe advertisers' targeting criteria. In this paper, we present a framework that \ncan be applied to assess the quality of such data sources in large scale. This \nframework efficiently evaluates the similarity of a specific data source \ncategorization to that of the ground truth, especially for those cases when the \nground truth is accessible only in aggregate, and the user-level information is \nanonymized or unavailable due to privacy reasons. We propose multiple \nmethodologies within this framework, present some preliminary assessment \nresults, and evaluate how the methodologies compare to each other. We also \npresent two use cases where we can utilize the data quality assessment results: \nthe first use case is targeting specific user categories, and the second one is \nforecasting the desirable audiences we can reach for an online advertising \ncampaign with pre-set targeting criteria. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca04e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11175"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Dhaval Adjodah, Dan Calacci, Yan Leng, Peter Krafft, Esteban Moro, Alex Pentland", "title": "Improved Learning in Evolution Strategies via Sparser Inter-Agent Network Topologies. (arXiv:1711.11180v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1711.11180", "type": "text/html"}], "timestampUsec": "1512136966936901", "comments": [], "summary": {"content": "<p>We draw upon a previously largely untapped literature on human collective \nintelligence as a source of inspiration for improving deep learning. Implicit \nin many algorithms that attempt to solve Deep Reinforcement Learning (DRL) \ntasks is the network of processors along which parameter values are shared. So \nfar, existing approaches have implicitly utilized fully-connected networks, in \nwhich all processors are connected. However, the scientific literature on human \ncollective intelligence suggests that complete networks may not always be the \nmost effective information network structures for distributed search through \ncomplex spaces. Here we show that alternative topologies can improve deep \nneural network training: we find that sparser networks learn higher rewards \nfaster, leading to learning improvements at lower communication costs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca060", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11180"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hyunwoo Lee, Jooyoung Kim, Dojun Yang, Joon-Ho Kim", "title": "Embedded Real-Time Fall Detection Using Deep Learning For Elderly Care. (arXiv:1711.11200v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11200", "type": "text/html"}], "timestampUsec": "1512136966936900", "comments": [], "summary": {"content": "<p>This paper proposes a real-time embedded fall detection system using a \nDVS(Dynamic Vision Sensor) that has never been used for traditional fall \ndetection, a dataset for fall detection using that, and a DVS-TN(DVS-Temporal \nNetwork). The first contribution is building a DVS Falls Dataset, which made \nour network to recognize a much greater variety of falls than the existing \ndatasets that existed before and solved privacy issues using the DVS. Secondly, \nwe introduce the DVS-TN : optimized deep learning network to detect falls using \nDVS. Finally, we implemented a fall detection system which can run on \nlow-computing H/W with real-time, and tested on DVS Falls Dataset that takes \ninto account various falls situations. Our approach achieved 95.5% on the \nF1-score and operates at 31.25 FPS on NVIDIA Jetson TX1 board. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca07b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11200"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yunhao Tang, Alp Kucukelbir", "title": "Variational Deep Q Network. (arXiv:1711.11225v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1711.11225", "type": "text/html"}], "timestampUsec": "1512136966936899", "comments": [], "summary": {"content": "<p>We propose a framework that directly tackles the probability distribution of \nthe value function parameters in Deep Q Network (DQN), with powerful \nvariational inference subroutines to approximate the posterior of the \nparameters. We will establish the equivalence between our proposed surrogate \nobjective and variational inference loss. Our new algorithm achieves efficient \nexploration and performs well on large scale chain Markov Decision Process \n(MDP). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca094", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11225"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shu Guo, Quan Wang, Lihong Wang, Bin Wang, Li Guo", "title": "Knowledge Graph Embedding with Iterative Guidance from Soft Rules. (arXiv:1711.11231v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1711.11231", "type": "text/html"}], "timestampUsec": "1512136966936898", "comments": [], "summary": {"content": "<p>Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of \ncurrent research. Combining such an embedding model with logic rules has \nrecently attracted increasing attention. Most previous attempts made a one-time \ninjection of logic rules, ignoring the interactive nature between embedding \nlearning and logical inference. And they focused only on hard rules, which \nalways hold with no exception and usually require extensive manual effort to \ncreate or validate. In this paper, we propose Rule-Guided Embedding (RUGE), a \nnovel paradigm of KG embedding with iterative guidance from soft rules. RUGE \nenables an embedding model to learn simultaneously from 1) labeled triples that \nhave been directly observed in a given KG, 2) unlabeled triples whose labels \nare going to be predicted iteratively, and 3) soft rules with various \nconfidence levels extracted automatically from the KG. In the learning process, \nRUGE iteratively queries rules to obtain soft labels for unlabeled triples, and \nintegrates such newly labeled triples to update the embedding model. Through \nthis iterative procedure, knowledge embodied in logic rules may be better \ntransferred into the learned embeddings. We evaluate RUGE in link prediction on \nFreebase and YAGO. Experimental results show that: 1) with rule knowledge \ninjected iteratively, RUGE achieves significant and consistent improvements \nover state-of-the-art baselines; and 2) despite their uncertainties, \nautomatically extracted soft rules are highly beneficial to KG embedding, even \nthose with moderate confidence levels. The code and data used for this paper \ncan be obtained from https://github.com/iieir-km/RUGE. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca0a9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11231"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Himanshu Sahni, Saurabh Kumar, Farhan Tejani, Charles Isbell", "title": "Learning to Compose Skills. (arXiv:1711.11289v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1711.11289", "type": "text/html"}], "timestampUsec": "1512136966936897", "comments": [], "summary": {"content": "<p>We present a differentiable framework capable of learning a wide variety of \ncompositions of simple policies that we call skills. By recursively composing \nskills with themselves, we can create hierarchies that display complex \nbehavior. Skill networks are trained to generate skill-state embeddings that \nare provided as inputs to a trainable composition function, which in turn \noutputs a policy for the overall task. Our experiments on an environment \nconsisting of multiple collect and evade tasks show that this architecture is \nable to quickly build complex skills from simpler ones. Furthermore, the \nlearned composition function displays some transfer to unseen combinations of \nskills, allowing for zero-shot generalizations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca0bf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11289"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, Jaap Kamps", "title": "Learning to Learn from Weak Supervision by Full Supervision. (arXiv:1711.11383v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11383", "type": "text/html"}], "timestampUsec": "1512136966936896", "comments": [], "summary": {"content": "<p>In this paper, we propose a method for training neural networks when we have \na large set of data with weak labels and a small amount of data with true \nlabels. In our proposed model, we train two neural networks: a target network, \nthe learner and a confidence network, the meta-learner. The target network is \noptimized to perform a given task and is trained using a large set of unlabeled \ndata that are weakly annotated. We propose to control the magnitude of the \ngradient updates to the target network using the scores provided by the second \nconfidence network, which is trained on a small amount of supervised data. Thus \nwe avoid that the weight updates computed from noisy labels harm the quality of \nthe target network model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca0d2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11383"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pierre Stock, Moustapha Cisse", "title": "ConvNets and ImageNet Beyond Accuracy: Explanations, Bias Detection, Adversarial Examples and Model Criticism. (arXiv:1711.11443v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1711.11443", "type": "text/html"}], "timestampUsec": "1512136966936895", "comments": [], "summary": {"content": "<p>ConvNets and Imagenet have driven the recent success of deep learning for \nimage classification. However, the marked slowdown in performance improvement, \nthe recent studies on the lack of robustness of neural networks to adversarial \nexamples and their tendency to exhibit undesirable biases (e.g racial biases) \nquestioned the reliability and the sustained development of these methods. This \nwork investigates these questions from the perspective of the end-user by using \nhuman subject studies and explanations. We experimentally demonstrate that the \naccuracy and robustness of ConvNets measured on Imagenet are underestimated. We \nshow that explanations can mitigate the impact of misclassified adversarial \nexamples from the perspective of the end-user and we introduce a novel tool for \nuncovering the undesirable biases learned by a model. These contributions also \nshow that explanations are a promising tool for improving our understanding of \nConvNets' predictions and for designing more reliable models \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca0e2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11443"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ming Liu, Bo Lang, Zepeng Gu", "title": "Calculating Semantic Similarity between Academic Articles using Topic Event and Ontology. (arXiv:1711.11508v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1711.11508", "type": "text/html"}], "timestampUsec": "1512136966936894", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32aa8334b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32aa8334b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Determining semantic similarity between academic documents is crucial to many \ntasks such as plagiarism detection, automatic technical survey and semantic \nsearch. Current studies mostly focus on semantic similarity between concepts, \nsentences and short text fragments. However, document-level semantic matching \nis still based on statistical information in surface level, neglecting article \nstructures and global semantic meanings, which may cause the deviation in \ndocument understanding. In this paper, we focus on the document-level semantic \nsimilarity issue for academic literatures with a novel method. We represent \nacademic articles with topic events that utilize multiple information profiles, \nsuch as research purposes, methodologies and domains to integrally describe the \nresearch work, and calculate the similarity between topic events based on the \ndomain ontology to acquire the semantic similarity between articles. \nExperiments show that our approach achieves significant performance compared to \nstate-of-the-art methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca0fc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11508"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512364312, "author": "Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra", "title": "Embodied Question Answering. (arXiv:1711.11543v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.11543", "type": "text/html"}], "timestampUsec": "1512136966936893", "comments": [], "summary": {"content": "<p>We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where \nan agent is spawned at a random location in a 3D environment and asked a \nquestion (\"What color is the car?\"). In order to answer, the agent must first \nintelligently navigate to explore the environment, gather information through \nfirst-person (egocentric) vision, and then answer the question (\"orange\"). \n</p> \n<p>This challenging task requires a range of AI skills -- active perception, \nlanguage understanding, goal-driven navigation, commonsense reasoning, and \ngrounding of language into actions. In this work, we develop the environments, \nend-to-end-trained reinforcement learning agents, and evaluation protocols for \nEmbodiedQA. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca11a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11543"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Weipeng He, Petr Motlicek, Jean-Marc Odobez", "title": "Deep Neural Networks for Multiple Speaker Detection and Localization. (arXiv:1711.11565v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1711.11565", "type": "text/html"}], "timestampUsec": "1512136966936892", "comments": [], "summary": {"content": "<p>We propose to use neural networks (NNs) for simultaneous detection and \nlocalization of multiple sound sources in Human-Robot Interaction (HRI). Unlike \nconventional signal processing techniques, NN-based Sound Source Localization \n(SSL) methods are relatively straightforward and require no or fewer \nassumptions that hardly hold in real HRI scenarios. Previously, NN-based \nmethods have been successfully applied to single SSL problems, which do not \nextend to multiple sources in terms of detection and localization. In this \npaper, we thus propose a likelihood-based encoding of the network output, which \nnaturally allows the detection of an arbitrary number of sources. In addition, \nwe investigate the use of sub-band cross-correlation information as features \nfor better localization in sound mixtures, as well as three different NN \narchitectures based on different processing motivations. Experiments on real \ndata recorded from the robot show that our NN-based methods significantly \noutperform the popular spatial spectrum-based approaches. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca127", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11565"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Han Xiao, Minlie Huang, Xiaoyan Zhu", "title": "KSR: A Semantic Representation of Knowledge Graph within a Novel Unsupervised Paradigm. (arXiv:1608.07685v4 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1608.07685", "type": "text/html"}], "timestampUsec": "1512136966936891", "comments": [], "summary": {"content": "<p>Knowledge representation is a long-history topic in AI, which is very \nimportant. A variety of models have been proposed for knowledge graph \nembedding, which projects symbolic entities and relations into continuous \nvector space. However, most related methods merely focus on the data-fitting of \nknowledge graph, and ignore the interpretable semantic expression. Thus, \ntraditional embedding methods are not friendly for applications that require \nsemantic analysis, such as question answering and entity retrieval. To this \nend, this paper proposes a semantic representation method for knowledge graph \n\\textbf{(KSR)}, which imposes a two-level hierarchical generative process that \nglobally extracts many aspects and then locally assigns a specific category in \neach aspect for every triple. Since both aspects and categories are \nsemantics-relevant, the collection of categories in each aspect is treated as \nthe semantic representation of this triple. Extensive experiments show that our \nmodel outperforms other state-of-the-art baselines substantially. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca141", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1608.07685"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ke Li, Jitendra Malik", "title": "Learning to Optimize Neural Nets. (arXiv:1703.00441v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.00441", "type": "text/html"}], "timestampUsec": "1512136966936890", "comments": [], "summary": {"content": "<p>Learning to Optimize is a recently proposed framework for learning \noptimization algorithms using reinforcement learning. In this paper, we explore \nlearning an optimization algorithm for training shallow neural nets. Such \nhigh-dimensional stochastic optimization problems present interesting \nchallenges for existing reinforcement learning algorithms. We develop an \nextension that is suited to learning optimization algorithms in this setting \nand demonstrate that the learned optimization algorithm consistently \noutperforms other known optimization algorithms even on unseen tasks and is \nrobust to changes in stochasticity of gradients and the neural net \narchitecture. More specifically, we show that an optimization algorithm trained \nwith the proposed method on the problem of training a neural net on MNIST \ngeneralizes to the problems of training neural nets on the Toronto Faces \nDataset, CIFAR-10 and CIFAR-100. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca155", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.00441"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513055251, "author": "Gal Dalal, Bal&#xe1;zs Sz&#xf6;r&#xe9;nyi, Gugan Thoppe, Shie Mannor", "title": "Finite Sample Analyses for TD(0) with Function Approximation. (arXiv:1704.01161v4 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.01161", "type": "text/html"}], "timestampUsec": "1512136966936889", "comments": [], "summary": {"content": "<p>TD(0) is one of the most commonly used algorithms in reinforcement learning. \nDespite this, there is no existing finite sample analysis for TD(0) with \nfunction approximation, even for the linear case. Our work is the first to \nprovide such results. Existing convergence rates for Temporal Difference (TD) \nmethods apply only to somewhat modified versions, e.g., projected variants or \nones where stepsizes depend on unknown problem parameters. Our analyses obviate \nthese artificial alterations by exploiting strong properties of TD(0). We \nprovide convergence rates both in expectation and with high-probability. The \ntwo are obtained via different approaches that use relatively unknown, recently \ndeveloped stochastic approximation techniques. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca15e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.01161"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jo&#xe3;o M. Cunha, Jo&#xe3;o Gon&#xe7;alves, Pedro Martins, Penousal Machado, Am&#xed;lcar Cardoso", "title": "A Pig, an Angel and a Cactus Walk Into a Blender: A Descriptive Approach to Visual Blending. (arXiv:1706.09076v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.09076", "type": "text/html"}], "timestampUsec": "1512136966936888", "comments": [], "summary": {"content": "<p>A descriptive approach for automatic generation of visual blends is \npresented. The implemented system, the Blender, is composed of two components: \nthe Mapper and the Visual Blender. The approach uses structured visual \nrepresentations along with sets of visual relations which describe how the \nelements (in which the visual representation can be decomposed) relate among \neach other. Our system is a hybrid blender, as the blending process starts at \nthe Mapper (conceptual level) and ends at the Visual Blender (visual \nrepresentation level). The experimental results show that the Blender is able \nto create analogies from input mental spaces and produce well-composed blends, \nwhich follow the rules imposed by its base-analogy and its relations. The \nresulting blends are visually interesting and some can be considered as \nunexpected. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca174", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.09076"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tambet Matiisen, Avital Oliver, Taco Cohen, John Schulman", "title": "Teacher-Student Curriculum Learning. (arXiv:1707.00183v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.00183", "type": "text/html"}], "timestampUsec": "1512136966936887", "comments": [], "summary": {"content": "<p>We propose Teacher-Student Curriculum Learning (TSCL), a framework for \nautomatic curriculum learning, where the Student tries to learn a complex task \nand the Teacher automatically chooses subtasks from a given set for the Student \nto train on. We describe a family of Teacher algorithms that rely on the \nintuition that the Student should practice more those tasks on which it makes \nthe fastest progress, i.e. where the slope of the learning curve is highest. In \naddition, the Teacher algorithms address the problem of forgetting by also \nchoosing tasks where the Student's performance is getting worse. We demonstrate \nthat TSCL matches or surpasses the results of carefully hand-crafted curricula \nin two tasks: addition of decimal numbers with LSTM and navigation in \nMinecraft. Using our automatically generated curriculum enabled to solve a \nMinecraft maze that could not be solved at all when training directly on \nsolving the maze, and the learning was an order of magnitude faster than \nuniform sampling of subtasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca17f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.00183"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Felix Burget, Lukas Dominique Josef Fiederer, Daniel Kuhner, Martin V&#xf6;lker, Johannes Aldinger, Robin Tibor Schirrmeister, Chau Do, Joschka Boedecker, Bernhard Nebel, Tonio Ball, Wolfram Burgard", "title": "Acting Thoughts: Towards a Mobile Robotic Service Assistant for Users with Limited Communication Skills. (arXiv:1707.06633v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.06633", "type": "text/html"}], "timestampUsec": "1512136966936886", "comments": [], "summary": {"content": "<p>As autonomous service robots become more affordable and thus available also \nfor the general public, there is a growing need for user friendly interfaces to \ncontrol the robotic system. Currently available control modalities typically \nexpect users to be able to express their desire through either touch, speech or \ngesture commands. While this requirement is fulfilled for the majority of \nusers, paralyzed users may not be able to use such systems. In this paper, we \npresent a novel framework, that allows these users to interact with a robotic \nservice assistant in a closed-loop fashion, using only thoughts. The \nbrain-computer interface (BCI) system is composed of several interacting \ncomponents, i.e., non-invasive neuronal signal recording and decoding, \nhigh-level task planning, motion and manipulation planning as well as \nenvironment perception. In various experiments, we demonstrate its \napplicability and robustness in real world scenarios, considering \nfetch-and-carry tasks and tasks involving human-robot interaction. As our \nresults demonstrate, our system is capable of adapting to frequent changes in \nthe environment and reliably completing given tasks within a reasonable amount \nof time. Combined with high-level planning and autonomous robotic systems, \ninteresting new perspectives open up for non-invasive BCI-based human-robot \ninteractions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca18c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.06633"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rocco Tripodi, Stefano Li Pira", "title": "Analysis of Italian Word Embeddings. (arXiv:1707.08783v4 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.08783", "type": "text/html"}], "timestampUsec": "1512136966936885", "comments": [], "summary": {"content": "<p>In this work we analyze the performances of two of the most used word \nembeddings algorithms, skip-gram and continuous bag of words on Italian \nlanguage. These algorithms have many hyper-parameter that have to be carefully \ntuned in order to obtain accurate word representation in vectorial space. We \nprovide an accurate analysis and an evaluation, showing what are the best \nconfiguration of parameters for specific tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca1a1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.08783"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman, John Miller", "title": "Deep Voice 3: 2000-Speaker Neural Text-to-Speech. (arXiv:1710.07654v2 [cs.SD] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.07654", "type": "text/html"}], "timestampUsec": "1512136966936884", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32aa83517\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32aa83517&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present Deep Voice 3, a fully-convolutional attention-based neural \ntext-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural \nspeech synthesis systems in naturalness while training ten times faster. We \nscale Deep Voice 3 to data set sizes unprecedented for TTS, training on more \nthan eight hundred hours of audio from over two thousand speakers. In addition, \nwe identify common error modes of attention-based speech synthesis networks, \ndemonstrate how to mitigate them, and compare several different waveform \nsynthesis methods. We also describe how to scale inference to ten million \nqueries per day on one single-GPU server. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca1b4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.07654"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xin Wang, Geoffrey Oxholm, Da Zhang, Yuan-Fang Wang", "title": "Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer. (arXiv:1612.01895v2 [cs.CV] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1612.01895", "type": "text/html"}], "timestampUsec": "1512136966936883", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32aac8d07\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32aac8d07&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Transferring artistic styles onto everyday photographs has become an \nextremely popular task in both academia and industry. Recently, offline \ntraining has replaced on-line iterative optimization, enabling nearly real-time \nstylization. When those stylization networks are applied directly to \nhigh-resolution images, however, the style of localized regions often appears \nless similar to the desired artistic style. This is because the transfer \nprocess fails to capture small, intricate textures and maintain correct texture \nscales of the artworks. Here we propose a multimodal convolutional neural \nnetwork that takes into consideration faithful representations of both color \nand luminance channels, and performs stylization hierarchically with multiple \nlosses of increasing scales. Compared to state-of-the-art networks, our network \ncan also perform style transfer in nearly real-time by conducting much more \nsophisticated training offline. By properly handling style and texture cues at \nmultiple scales using several modalities, we can transfer not just large-scale, \nobvious style cues but also subtle, exquisite ones. That is, our scheme can \ngenerate results that are visually pleasing and more similar to multiple \ndesired artistic styles with color and texture cues at multiple scales. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca1c9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.01895"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lingfei Wang, Tom Michoel", "title": "Wisdom of the crowd from unsupervised dimension reduction. (arXiv:1711.11034v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11034", "type": "text/html"}], "timestampUsec": "1512136966936882", "comments": [], "summary": {"content": "<p>Wisdom of the crowd, the collective intelligence derived from responses of \nmultiple human or machine individuals to the same questions, can be more \naccurate than each individual, and improve social decision-making and \nprediction accuracy. This can also integrate multiple programs or datasets, \neach as an individual, for the same predictive questions. Crowd wisdom \nestimates each individual's independent error level arising from their limited \nknowledge, and finds the crowd consensus that minimizes the overall error. \nHowever, previous studies have merely built isolated, problem-specific models \nwith limited generalizability, and mainly for binary (yes/no) responses. Here \nwe show with simulation and real-world data that the crowd wisdom problem is \nanalogous to one-dimensional unsupervised dimension reduction in machine \nlearning. This provides a natural class of crowd wisdom solutions, such as \nprincipal component analysis and Isomap, which can handle binary and also \ncontinuous responses, like confidence levels, and consequently can be more \naccurate than existing solutions. They can even outperform \nsupervised-learning-based collective intelligence that is calibrated on \nhistorical performance of individuals, e.g. penalized linear regression and \nrandom forest. This study unifies crowd wisdom and unsupervised dimension \nreduction, and thereupon introduces a broad range of highly-performing and \nwidely-applicable crowd wisdom methods. As the costs for data acquisition and \nprocessing rapidly decrease, this study will promote and guide crowd wisdom \napplications in the social and natural sciences, including data fusion, \nmeta-analysis, crowd-sourcing, and committee decision making. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca1d6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11034"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy", "title": "A Multi-Horizon Quantile Recurrent Forecaster. (arXiv:1711.11053v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11053", "type": "text/html"}], "timestampUsec": "1512136966936881", "comments": [], "summary": {"content": "<p>We propose a framework for general probabilistic multi-step time series \nregression. Specifically, we exploit the expressiveness and temporal nature of \nRecurrent Neural Networks, the nonparametric nature of Quantile Regression and \nthe efficiency of Direct Multi-Horizon Forecasting. A new training scheme for \nrecurrent nets is designed to boost stability and performance. We show that the \napproach accommodates both temporal and static covariates, learning across \nmultiple related series, shifting seasonality, future planned event spikes and \ncold-starts in real life large-scale forecasting. The performance of the \nframework is demonstrated in an application to predict the future demand of \nitems sold on Amazon.com, and in a public probabilistic forecasting competition \nto predict electricity price and load. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca1ed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11053"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yen-Chi Chen, Y. Samuel Wang, Elena A. Erosheva", "title": "On the use of bootstrap with variational inference: Theory, interpretation, and a two-sample test example. (arXiv:1711.11057v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1711.11057", "type": "text/html"}], "timestampUsec": "1512136966936880", "comments": [], "summary": {"content": "<p>Variational inference is a general approach for approximating complex density \nfunctions, such as those arising in latent variable models, popular in machine \nlearning. It has been applied to approximate the maximum likelihood estimator \nand to carry out Bayesian inference, however, quantification of uncertainty \nwith variational inference remains challenging from both theoretical and \npractical perspectives. This paper is concerned with developing uncertainty \nmeasures for variational inference by using bootstrap procedures. We first \ndevelop two general bootstrap approaches for assessing the uncertainty of a \nvariational estimate and the study the underlying bootstrap theory in both \nfixed- and increasing-dimension settings. We then use the bootstrap approach \nand our theoretical results in the context of mixed membership modeling with \nmultivariate binary data on functional disability from the National Long Term \nCare Survey. We carry out a two-sample approach to test for changes in the \nrepeated measures of functional disability for the subset of individuals \npresent in 1984 and 1994 waves. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca1f9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11057"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Vinay Jethava, Devdatt Dubhashi", "title": "GANs for LIFE: Generative Adversarial Networks for Likelihood Free Inference. (arXiv:1711.11139v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1711.11139", "type": "text/html"}], "timestampUsec": "1512136966936879", "comments": [], "summary": {"content": "<p>We introduce a framework using Generative Adversarial Networks (GANs) for \nlikelihood--free inference (LFI) and Approximate Bayesian Computation (ABC). \nOur approach addresses both the key problems in likelihood--free inference, \nnamely how to compare distributions and how to efficiently explore the \nparameter space. Our framework allows one to use the simulator model as a black \nbox and leverage the power of deep networks to generate a rich set of features \nin a data driven fashion (as opposed to previous ad hoc approaches). Thereby it \nis a step towards a powerful alternative approach to LFI and ABC. On benchmark \ndata sets, our approach improves on others with respect to scalability, ability \nto handle high dimensional data and complex probability distributions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca1ff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11139"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chao Gao", "title": "Phase Transitions in Approximate Ranking. (arXiv:1711.11189v1 [math.ST])", "alternate": [{"href": "http://arxiv.org/abs/1711.11189", "type": "text/html"}], "timestampUsec": "1512136966936878", "comments": [], "summary": {"content": "<p>We study the problem of approximate ranking from observations of pairwise \ninteractions. The goal is to estimate the underlying ranks of $n$ objects from \ndata through interactions of comparison or collaboration. Under a general \nframework of approximate ranking models, we characterize the exact optimal \nstatistical error rates of estimating the underlying ranks. We discover \nimportant phase transition boundaries of the optimal error rates. Depending on \nthe value of the signal-to-noise ratio (SNR) parameter, the optimal rate, as a \nfunction of SNR, is either trivial, polynomial, exponential or zero. The four \ncorresponding regimes thus have completely different error behaviors. To the \nbest of our knowledge, this phenomenon, especially the phase transition between \nthe polynomial and the exponential rates, has not been discovered before. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca209", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11189"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chang Liu, Jun Zhu", "title": "Riemannian Stein Variational Gradient Descent for Bayesian Inference. (arXiv:1711.11216v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11216", "type": "text/html"}], "timestampUsec": "1512136966936877", "comments": [], "summary": {"content": "<p>We develop Riemannian Stein Variational Gradient Descent (RSVGD), a Bayesian \ninference method that generalizes Stein Variational Gradient Descent (SVGD) to \nRiemann manifold. The benefits are two-folds: (i) for inference tasks in \nEuclidean spaces, RSVGD has the advantage over SVGD of utilizing information \ngeometry, and (ii) for inference tasks on Riemann manifolds, RSVGD brings the \nunique advantages of SVGD to the Riemannian world. To appropriately transfer to \nRiemann manifolds, we conceive novel and non-trivial techniques for RSVGD, \nwhich are required by the intrinsically different characteristics of general \nRiemann manifolds from Euclidean spaces. We also discover Riemannian Stein's \nIdentity and Riemannian Kernelized Stein Discrepancy. Experimental results show \nthe advantages over SVGD of exploring distribution geometry and the advantages \nof particle-efficiency, iteration-effectiveness and approximation flexibility \nover other inference methods on Riemann manifolds. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca213", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11216"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Been Kim, Justin Gilmer, Fernanda Viegas, Ulfar Erlingsson, Martin Wattenberg", "title": "TCAV: Relative concept importance testing with Linear Concept Activation Vectors. (arXiv:1711.11279v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11279", "type": "text/html"}], "timestampUsec": "1512136966936876", "comments": [], "summary": {"content": "<p>Neural networks commonly offer high utility but remain difficult to \ninterpret. Developing methods to explain their decisions is challenging due to \ntheir large size, complex structure, and inscrutable internal representations. \nThis work argues that the language of explanations should be expanded from that \nof input features (e.g., assigning importance weightings to pixels) to include \nthat of higher-level, human-friendly concepts. For example, an understandable \nexplanation of why an image classifier outputs the label \"zebra\" would ideally \nrelate to concepts such as \"stripes\" rather than a set of particular pixel \nvalues. This paper introduces the \"concept activation vector\" (CAV) which \nallows quantitative analysis of a concept's relative importance to \nclassification, with a user-provided set of input data examples defining the \nconcept. CAVs may be easily used by non-experts, who need only provide \nexamples, and with CAVs the high-dimensional structure of neural networks turns \ninto an aid to interpretation, rather than an obstacle. Using the domain of \nimage classification as a testing ground, we describe how CAVs may be used to \ntest hypotheses about classifiers and also generate insights into the \ndeficiencies and correlations in training data. CAVs also provide us a directed \napproach to choose the combinations of neurons to visualize with the DeepDream \ntechnique, which traditionally has chosen neurons or linear combinations of \nneurons at random to visualize. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca21d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11279"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiaofan Lin, Cong Zhao, Wei Pan", "title": "Towards Accurate Binary Convolutional Neural Network. (arXiv:1711.11294v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1711.11294", "type": "text/html"}], "timestampUsec": "1512136966936875", "comments": [], "summary": {"content": "<p>We introduce a novel scheme to train binary convolutional neural networks \n(CNNs) -- CNNs with weights and activations constrained to {-1,+1} at run-time. \nIt has been known that using binary weights and activations drastically reduce \nmemory size and accesses, and can replace arithmetic operations with more \nefficient bitwise operations, leading to much faster test-time inference and \nlower power consumption. However, previous works on binarizing CNNs usually \nresult in severe prediction accuracy degradation. In this paper, we address \nthis issue with two major innovations: (1) approximating full-precision weights \nwith the linear combination of multiple binary weight bases; (2) employing \nmultiple binary activations to alleviate information loss. The implementation \nof the resulting binary CNN, denoted as ABC-Net, is shown to achieve much \ncloser performance to its full-precision counterpart, and even reach the \ncomparable prediction accuracy on ImageNet and forest trail datasets, given \nadequate binary weight bases and activations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca228", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11294"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512364319, "author": "Kerem C. Tezcan, Christian F. Baumgartner, Ender Konukoglu", "title": "MR image reconstruction using the learned data distribution as prior. (arXiv:1711.11386v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.11386", "type": "text/html"}], "timestampUsec": "1512136966936874", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32aac9072\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32aac9072&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>MR image reconstruction from undersampled data exploits priors to compensate \nfor missing k-space data. This has previously been achieved by using \nregularization methods, such as TV and wavelets, or data adaptive methods, such \nas dictionary learning. We propose to explicitly learn the probability \ndistribution of MR image patches and to constrain patches to have a high \nprobability according to this distribution in reconstruction, effectively \nemploying it as the prior. \n</p> \n<p>We use variational autoencoders (VAE) to learn the distribution of MR image \npatches. This high dimensional distribution is modelled by a latent parameter \nmodel of lower dimensions in a non-linear fashion. We develop a reconstruction \nalgorithm that uses the learned prior in a Maximum-A-Posteriori estimation \nformulation. We evaluate the proposed method with T1 weighted images and \ncompare it to existing alternatives. We also apply our method on images with \nwhite matter lesions. \n</p> \n<p>Visual evaluation of the samples drawn from the learned model showed that the \nVAE algorithm was able to approximate the distribution of MR image patches. \nFurthermore, the reconstruction algorithm using the approximate distribution \nproduced qualitatively better results. The proposed technique achieved RMSE, \nCNR and CN values of 2.77%, 0.43, 0.11 and 4.29%, 0.43, 0.11 for undersampling \nratios of 2 and 3, respectively. It outperformed other evaluated methods in \nterms of used metrics. In the experiments on images with white matter lesions, \nthe method faithfully reconstructed the lesions. \n</p> \n<p>We introduced a novel method for MR reconstruction, which takes a new \nperspective on regularization by learning priors. Results suggest the method \ncompares favorably against TV and dictionary based methods as well as the \nneural-network based ADMM-Net in terms of the RMSE, CNR and CN and perceptual \nimage quality and can reconstruct lesions as well. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca231", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11386"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Burim Ramosaj, Markus Pauly", "title": "Who wins the Miss Contest for Imputation Methods? Our Vote for Miss BooPF. (arXiv:1711.11394v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11394", "type": "text/html"}], "timestampUsec": "1512136966936873", "comments": [], "summary": {"content": "<p>Missing data is an expected issue when large amounts of data is collected, \nand several imputation techniques have been proposed to tackle this problem. \nBeneath classical approaches such as MICE, the application of Machine Learning \ntechniques is tempting. Here, the recently proposed missForest imputation \nmethod has shown high imputation accuracy under the Missing (Completely) at \nRandom scheme with various missing rates. In its core, it is based on a random \nforest for classification and regression, respectively. In this paper we study \nwhether this approach can even be enhanced by other methods such as the \nstochastic gradient tree boosting method, the C5.0 algorithm or modified random \nforest procedures. In particular, other resampling strategies within the random \nforest protocol are suggested. In an extensive simulation study, we analyze \ntheir performances for continuous, categorical as well as mixed-type data. \nTherein, MissBooPF, a combination of the stochastic gradient tree boosting \nmethod together with the parametrically bootstrapped random forest method, \nappeared to be promising. Finally, an empirical analysis focusing on credit \ninformation and Facebook data is conducted. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca240", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11394"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Daniel Chicharro, Giuseppe Pica, Stefano Panzeri", "title": "The identity of information: how deterministic dependencies constrain information synergy and redundancy. (arXiv:1711.11408v1 [q-bio.NC])", "alternate": [{"href": "http://arxiv.org/abs/1711.11408", "type": "text/html"}], "timestampUsec": "1512136966936872", "comments": [], "summary": {"content": "<p>Understanding how different information sources together transmit information \nis crucial in many domains. For example, understanding the neural code requires \ncharacterizing how different neurons contribute unique, redundant, or \nsynergistic pieces of information about sensory or behavioral variables. \nWilliams and Beer (2010) proposed a partial information decomposition (PID) \nwhich separates the mutual information that a set of sources contains about a \nset of targets into nonnegative terms interpretable as these pieces. \nQuantifying redundancy requires assigning an identity to different information \npieces, to assess when information is common across sources. Harder et al. \n(2013) proposed an identity axiom stating that there cannot be redundancy \nbetween two independent sources about a copy of themselves. However, \nBertschinger et al. (2012) showed that with a deterministically related \nsources-target copy this axiom is incompatible with ensuring PID nonnegativity. \nHere we study systematically the effect of deterministic target-sources \ndependencies. We introduce two synergy stochasticity axioms that generalize the \nidentity axiom, and we derive general expressions separating stochastic and \ndeterministic PID components. Our analysis identifies how negative terms can \noriginate from deterministic dependencies and shows how different assumptions \non information identity, implicit in the stochasticity and identity axioms, \ndetermine the PID structure. The implications for studying neural coding are \ndiscussed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca24d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11408"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ibrahim El Khalil Harrane, R&#xe9;mi Flamary, C&#xe9;dric Richard", "title": "On reducing the communication cost of the diffusion LMS algorithm. (arXiv:1711.11423v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11423", "type": "text/html"}], "timestampUsec": "1512136966936871", "comments": [], "summary": {"content": "<p>The rise of digital and mobile communications has recently made the world \nmore connected and networked, resulting in an unprecedented volume of data \nflowing between sources, data centers, or processes. While these data may be \nprocessed in a centralized manner, it is often more suitable to consider \ndistributed strategies such as diffusion as they are scalable and can handle \nlarge amounts of data by distributing tasks over networked agents. Although it \nis relatively simple to implement diffusion strategies over a cluster, it \nappears to be challenging to deploy them in an ad-hoc network with limited \nenergy budget for communication. In this paper, we introduce a diffusion LMS \nstrategy that significantly reduces communication costs without compromising \nthe performance. Then, we analyze the proposed algorithm in the mean and \nmean-square sense. Next, we conduct numerical experiments to confirm the \ntheoretical findings. Finally, we perform large scale simulations to test the \nalgorithm efficiency in a scenario where energy is limited. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca25a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11423"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Francisco J. Valverde-Albacete, Carmen Pel&#xe1;ez-Moreno", "title": "The Channel Multivariate Entropy Triangle and Balance Equation. (arXiv:1711.11510v1 [cs.IT])", "alternate": [{"href": "http://arxiv.org/abs/1711.11510", "type": "text/html"}], "timestampUsec": "1512136966936870", "comments": [], "summary": {"content": "<p>In this paper we use information-theoretic measures to provide a theory and \ntools to analyze the flow of information from a discrete, multivariate source \nof information $\\overline X$ to a discrete, multivariate sink of information \n$\\overline Y$ joined by a distribution $P_{\\overline X \\overline Y}$. The first \ncontribution is a decomposition of the maximal potential entropy of $(\\overline \nX, \\overline Y)$ that we call a balance equation, that can also be split into \ndecompositions for the entropies of $\\overline X$ and $\\overline Y$ \nrespectively. Such balance equations accept normalizations that allow them to \nbe represented in de Finetti entropy diagrams, our second contribution. The \nmost important of these, the aggregate Channel Multivariate Entropy Triangle \nCMET is an exploratory tool to assess the efficiency of multivariate channels. \nWe also present a practical contribution in the application of these balance \nequations and diagrams to the assessment of information transfer efficiency for \nPCA and ICA as feature transformation and selection procedures in machine \nlearning applications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca26d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11510"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rui Luo, Yaodong Yang, Jun Wang, Yuanyuan Liu", "title": "Thermostat-assisted Continuous-tempered Hamiltonian Monte Carlo for Multimodal Posterior Sampling on Large Datasets. (arXiv:1711.11511v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11511", "type": "text/html"}], "timestampUsec": "1512136966936869", "comments": [], "summary": {"content": "<p>In this paper, we propose a new sampling method named as the \nthermostat-assisted continuous-tempered Hamiltonian Monte Carlo for multimodal \nposterior sampling on large datasets. It simulates a noisy system, which is \naugmented by a coupling tempering variable as well as a set of Nos\\'e-Hoover \nthermostats. This augmentation is devised to address two main issues of \nconcern: the first is to effectively generate i.i.d. samples from complex \nmultimodal posterior distributions; the second is to adaptively control the \nsystem dynamics in the presence of unknown noise that arises from the use of \nmini-batches. The experiment on synthetic distributions has been performed; the \nresult demonstrates the effectiveness of the proposed method. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca279", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11511"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513315291, "author": "Kshiteej Sheth, Dinesh Garg, Anirban Dasgupta", "title": "Improved Linear Embeddings via Lagrange Duality. (arXiv:1711.11527v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.11527", "type": "text/html"}], "timestampUsec": "1512136966936868", "comments": [], "summary": {"content": "<p>Near isometric orthogonal embeddings to lower dimensions are a fundamental \ntool in data science and machine learning. In this paper, we present the \nconstruction of such embeddings that minimizes the maximum distortion for a \ngiven set of points. We formulate the problem as a non convex constrained \noptimization problem. We first construct a primal relaxation and then use the \ntheory of Lagrange duality to create dual relaxation. We also suggest a \npolynomial time algorithm based on the theory of convex optimization to solve \nthe dual relaxation provably. We provide a theoretical upper bound on the \napproximation guarantees for our algorithm, which depends only on the spectral \nproperties of the dataset. We experimentally demonstrate the superiority of our \nalgorithm compared to baselines in terms of the scalability and the ability to \nachieve lower distortion. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca286", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11527"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexander G. Ororbia II, Patrick Haffner, David Reitter, C. Lee Giles", "title": "Learning to Adapt by Minimizing Discrepancy. (arXiv:1711.11542v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1711.11542", "type": "text/html"}], "timestampUsec": "1512136966936867", "comments": [], "summary": {"content": "<p>We explore whether useful temporal neural generative models can be learned \nfrom sequential data without back-propagation through time. We investigate the \nviability of a more neurocognitively-grounded approach in the context of \nunsupervised generative modeling of sequences. Specifically, we build on the \nconcept of predictive coding, which has gained influence in cognitive science, \nin a neural framework. To do so we develop a novel architecture, the Temporal \nNeural Coding Network, and its learning algorithm, Discrepancy Reduction. The \nunderlying directed generative model is fully recurrent, meaning that it \nemploys structural feedback connections and temporal feedback connections, \nyielding information propagation cycles that create local learning signals. \nThis facilitates a unified bottom-up and top-down approach for information \ntransfer inside the architecture. Our proposed algorithm shows promise on the \nbouncing balls generative modeling problem. Further experiments could be \nconducted to explore the strengths and weaknesses of our approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca28e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11542"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jason Jo, Yoshua Bengio", "title": "Measuring the tendency of CNNs to Learn Surface Statistical Regularities. (arXiv:1711.11561v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1711.11561", "type": "text/html"}], "timestampUsec": "1512136966936866", "comments": [], "summary": {"content": "<p>Deep CNNs are known to exhibit the following peculiarity: on the one hand \nthey generalize extremely well to a test set, while on the other hand they are \nextremely sensitive to so-called adversarial perturbations. The extreme \nsensitivity of high performance CNNs to adversarial examples casts serious \ndoubt that these networks are learning high level abstractions in the dataset. \nWe are concerned with the following question: How can a deep CNN that does not \nlearn any high level semantics of the dataset manage to generalize so well? The \ngoal of this article is to measure the tendency of CNNs to learn surface \nstatistical regularities of the dataset. To this end, we use Fourier filtering \nto construct datasets which share the exact same high level abstractions but \nexhibit qualitatively different surface statistical regularities. For the SVHN \nand CIFAR-10 datasets, we present two Fourier filtered variants: a low \nfrequency variant and a randomly filtered variant. Each of the Fourier \nfiltering schemes is tuned to preserve the recognizability of the objects. Our \nmain finding is that CNNs exhibit a tendency to latch onto the Fourier image \nstatistics of the training dataset, sometimes exhibiting up to a 28% \ngeneralization gap across the various test sets. Moreover, we observe that \nsignificantly increasing the depth of a network has a very marginal impact on \nclosing the aforementioned generalization gap. Thus we provide quantitative \nevidence supporting the hypothesis that deep CNNs tend to learn surface \nstatistical regularities in the dataset rather than higher-level abstract \nconcepts. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca29c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11561"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pravesh K. Kothari, David Steurer", "title": "Outlier-robust moment-estimation via sum-of-squares. (arXiv:1711.11581v1 [cs.DS])", "alternate": [{"href": "http://arxiv.org/abs/1711.11581", "type": "text/html"}], "timestampUsec": "1512136966936865", "comments": [], "summary": {"content": "<p>We develop efficient algorithms for estimating low-degree moments of unknown \ndistributions in the presence of adversarial outliers. The guarantees of our \nalgorithms improve in many cases significantly over the best previous ones, \nobtained in recent works of Diakonikolas et al, Lai et al, and Charikar et al. \nWe also show that the guarantees of our algorithms match information-theoretic \nlower-bounds for the class of distributions we consider. These improved \nguarantees allow us to give improved algorithms for independent component \nanalysis and learning mixtures of Gaussians in the presence of outliers. \n</p> \n<p>Our algorithms are based on a standard sum-of-squares relaxation of the \nfollowing conceptually-simple optimization problem: Among all distributions \nwhose moments are bounded in the same way as for the unknown distribution, find \nthe one that is closest in statistical distance to the empirical distribution \nof the adversarially-corrupted sample. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca2a9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11581"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, Eli Shechtman", "title": "Toward Multimodal Image-to-Image Translation. (arXiv:1711.11586v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1711.11586", "type": "text/html"}], "timestampUsec": "1512136966936864", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a6b32aac9353\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a6b32aac9353&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Many image-to-image translation problems are ambiguous, as a single input \nimage may correspond to multiple possible outputs. In this work, we aim to \nmodel a \\emph{distribution} of possible outputs in a conditional generative \nmodeling setting. The ambiguity of the mapping is distilled in a \nlow-dimensional latent vector, which can be randomly sampled at test time. A \ngenerator learns to map the given input, combined with this latent code, to the \noutput. We explicitly encourage the connection between output and the latent \ncode to be invertible. This helps prevent a many-to-one mapping from the latent \ncode to the output during training, also known as the problem of mode collapse, \nand produces more diverse results. We explore several variants of this approach \nby employing different training objectives, network architectures, and methods \nof injecting the latent code. Our proposed method encourages bijective \nconsistency between the latent encoding and output modes. We present a \nsystematic comparison of our method and other variants on both perceptual \nrealism and diversity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca2b7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11586"}]}]}