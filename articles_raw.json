{"all_articles": [{"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224163", "id": "tag:google.com,2005:reader/item/00000003684d3dbd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Dual Recurrent Attention Units for Visual Question Answering. (arXiv:1802.00209v1 [cs.AI])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00209"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00209", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a742d3cbf8de\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a742d3cbf8de&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose an architecture for VQA which utilizes recurrent layers to \ngenerate visual and textual attention. The memory characteristic of the \nproposed recurrent attention units offers a rich joint embedding of visual and \ntextual features and enables the model to reason relations between several \nparts of the image and question. Our single model outperforms the first place \nwinner on the VQA 1.0 dataset, performs within margin to the current \nstate-of-the-art ensemble model. We also experiment with replacing attention \nmechanisms in other state-of-the-art models with our implementation and show \nincreased accuracy. In both cases, our recurrent attention mechanism improves \nperformance in tasks requiring sequential or relational reasoning on the VQA \ndataset. \n</p>"}, "author": "Ahmed Osman, Wojciech Samek", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224161", "id": "tag:google.com,2005:reader/item/00000003684d3ddf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deceptive Games. (arXiv:1802.00048v1 [cs.AI])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00048"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00048", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deceptive games are games where the reward structure or other aspects of the \ngame are designed to lead the agent away from a globally optimal policy. While \nmany games are already deceptive to some extent, we designed a series of games \nin the Video Game Description Language (VGDL) implementing specific types of \ndeception, classified by the cognitive biases they exploit. VGDL games can be \nrun in the General Video Game Artificial Intelligence (GVGAI) Framework, making \nit possible to test a variety of existing AI agents that have been submitted to \nthe GVGAI Competition on these deceptive games. Our results show that all \ntested agents are vulnerable to several kinds of deception, but that different \nagents have different weaknesses. This suggests that we can use deception to \nunderstand the capabilities of a game-playing algorithm, and game-playing \nalgorithms to characterize the deception displayed by a game. \n</p>"}, "author": "Damien Anderson, Matthew Stephenson, Julian Togelius, Christian Salge, John Levine, Jochen Renz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224160", "id": "tag:google.com,2005:reader/item/00000003684d3dfb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Recursive Feature Generation for Knowledge-based Learning. (arXiv:1802.00050v1 [cs.AI])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00050"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00050", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>When humans perform inductive learning, they often enhance the process with \nbackground knowledge. With the increasing availability of well-formed \ncollaborative knowledge bases, the performance of learning algorithms could be \nsignificantly enhanced if a way were found to exploit these knowledge bases. In \nthis work, we present a novel algorithm for injecting external knowledge into \ninduction algorithms using feature generation. Given a feature, the algorithm \ndefines a new learning task over its set of values, and uses the knowledge base \nto solve the constructed learning task. The resulting classifier is then used \nas a new feature for the original problem. We have applied our algorithm to the \ndomain of text classification using large semantic knowledge bases. We have \nshown that the generated features significantly improve the performance of \nexisting learning algorithms. \n</p>"}, "author": "Lior Friedman, Shaul Markovitch", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224159", "id": "tag:google.com,2005:reader/item/00000003684d3e1a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Crowd Flow Prediction by Deep Spatio-Temporal Transfer Learning. (arXiv:1802.00386v1 [cs.AI])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00386"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00386", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Crowd flow prediction is a fundamental urban computing problem. Recently, \ndeep learning has been successfully applied to solve this problem, but it \nrelies on rich historical data. In reality, many cities may suffer from data \nscarcity issue when their targeted service or infrastructure is new. To \novercome this issue, this paper proposes a novel deep spatio-temporal transfer \nlearning framework, called RegionTrans, which can predict future crowd flow in \na data-scarce (target) city by transferring knowledge from a data-rich (source) \ncity. Leveraging social network check-ins, RegionTrans first links a region in \nthe target city to certain regions in the source city, expecting that these \ninter-city region pairs will share similar crowd flow dynamics. Then, we \npropose a deep spatio-temporal neural network structure, in which a hidden \nlayer is dedicated to keeping the region representation. A source city model is \nthen trained on its rich historical data with this network structure. Finally, \nwe propose a region-based cross-city transfer learning algorithm to learn the \ntarget city model from the source city model by minimizing the hidden \nrepresentation discrepancy between the inter-city region pairs previously \nlinked by check-ins. With experiments on real crowd flow, RegionTrans can \noutperform state-of-the-arts by reducing up to 10.7% prediction error. \n</p>"}, "author": "Leye Wang, Xu Geng, Xiaojuan Ma, Feng Liu, Qiang Yang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224158", "id": "tag:google.com,2005:reader/item/00000003684d3e47", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "3D Object Dense Reconstruction from a Single Depth View. (arXiv:1802.00411v1 [cs.CV])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00411"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00411", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a novel approach, 3D-RecGAN++, which reconstructs \nthe complete 3D structure of a given object from a single arbitrary depth view \nusing generative adversarial networks. Unlike existing work which typically \nrequires multiple views of the same object or class labels to recover the full \n3D geometry, the proposed 3D-RecGAN++ only takes the voxel grid representation \nof a depth view of the object as input, and is able to generate the complete 3D \noccupancy grid with a high resolution of 256^3 by recovering the \noccluded/missing regions. The key idea is to combine the generative \ncapabilities of autoencoders and the conditional Generative Adversarial \nNetworks (GAN) framework, to infer accurate and fine-grained 3D structures of \nobjects in high-dimensional voxel space. Extensive experiments on large \nsynthetic datasets and real-world Kinect datasets show that the proposed \n3D-RecGAN++ significantly outperforms the state of the art in single view 3D \nobject reconstruction, and is able to reconstruct unseen types of objects. \n</p>"}, "author": "Bo Yang, Stefano Rosa, Andrew Markham, Niki Trigoni, Hongkai Wen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224157", "id": "tag:google.com,2005:reader/item/00000003684d3e7a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "On Understanding and Machine Understanding. (arXiv:1103.5034v2 [cs.AI] UPDATED)", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1103.5034"}], "alternate": [{"href": "http://arxiv.org/abs/1103.5034", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In the present paper, we try to propose a self-similar network theory for the \nbasic understanding. By extending the natural languages to a kind of so called \nidealy sufficient language, we can proceed a few steps to the investigation of \nthe language searching and the language understanding of AI. \n</p> \n<p>Image understanding, and the familiarity of the brain to the surrounding \nenvironment are also discussed. Group effects are discussed by addressing the \nessense of the power of influences, and constructing the influence network of a \nsociety. We also give a discussion of inspirations. \n</p>"}, "author": "Tong Chern", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224156", "id": "tag:google.com,2005:reader/item/00000003684d3ea3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Reliable Decision Support using Counterfactual Models. (arXiv:1703.10651v4 [stat.ML] UPDATED)", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.10651"}], "alternate": [{"href": "http://arxiv.org/abs/1703.10651", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Decision-makers are faced with the challenge of estimating what is likely to \nhappen when they take an action. For instance, if I choose not to treat this \npatient, are they likely to die? Practitioners commonly use supervised learning \nalgorithms to fit predictive models that help decision-makers reason about \nlikely future outcomes, but we show that this approach is unreliable, and \nsometimes even dangerous. The key issue is that supervised learning algorithms \nare highly sensitive to the policy used to choose actions in the training data, \nwhich causes the model to capture relationships that do not generalize. We \npropose using a different learning objective that predicts counterfactuals \ninstead of predicting outcomes under an existing action policy as in supervised \nlearning. To support decision-making in temporal settings, we introduce the \nCounterfactual Gaussian Process (CGP) to predict the counterfactual future \nprogression of continuous-time trajectories under sequences of future actions. \nWe demonstrate the benefits of the CGP on two important decision-support tasks: \nrisk prediction and \"what if?\" reasoning for individualized treatment planning. \n</p>"}, "author": "Peter Schulam, Suchi Saria", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224154", "id": "tag:google.com,2005:reader/item/00000003684d3efb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "JointDNN: An Efficient Training and Inference Engine for Intelligent Mobile Cloud Computing Services. (arXiv:1801.08618v1 [cs.DC] CROSS LISTED)", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.08618"}], "alternate": [{"href": "http://arxiv.org/abs/1801.08618", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks are among the most influential architectures of deep \nlearning algorithms, being deployed in many mobile intelligent applications. \nEnd-side services, such as intelligent personal assistants (IPAs), autonomous \ncars, and smart home services often employ either simple local models or \ncomplex remote models on the cloud. Mobile-only and cloud-only computations are \ncurrently the status quo approaches. In this paper, we propose an efficient, \nadaptive, and practical engine, JointDNN, for collaborative computation between \na mobile device and cloud for DNNs in both inference and training phase. \nJointDNN not only provides an energy and performance efficient method of \nquerying DNNs for the mobile side, but also benefits the cloud server by \nreducing the amount of its workload and communications compared to the \ncloud-only approach. Given the DNN architecture, we investigate the efficiency \nof processing some layers on the mobile device and some layers on the cloud \nserver. We provide optimization formulations at layer granularity for forward \nand backward propagation in DNNs, which can adapt to mobile battery limitations \nand cloud server load constraints and quality of service. JointDNN achieves up \nto 18X and 32X reductions on the latency and mobile energy consumption of \nquerying DNNs, respectively. \n</p>"}, "author": "Amir Erfan Eshratifar, Mohammad Saeed Abrishami, Massoud Pedram", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224153", "id": "tag:google.com,2005:reader/item/00000003684d3f1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fusarium Damaged Kernels Detection Using Transfer Learning on Deep Neural Network Architecture. (arXiv:1802.00030v1 [cs.LG])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00030"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00030", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The present work shows the application of transfer learning for a pre-trained \ndeep neural network (DNN), using a small image dataset ($\\approx$ 12,000) on a \nsingle workstation with enabled NVIDIA GPU card that takes up to 1 hour to \ncomplete the training task and archive an overall average accuracy of $94.7\\%$. \nThe DNN presents a $20\\%$ score of misclassification for an external test \ndataset. The accuracy of the proposed methodology is equivalent to ones using \nHSI methodology $(81\\%-91\\%)$ used for the same task, but with the advantage of \nbeing independent on special equipment to classify wheat kernel for FHB \nsymptoms. \n</p>"}, "author": "M&#xe1;rcio Nicolau, M&#xe1;rcia Barrocas Moreira Pimentel, Casiane Salete Tibola, Jos&#xe9; Mauricio Cunha Fernandes, Willingthon Pavan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224152", "id": "tag:google.com,2005:reader/item/00000003684d3f30", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Incremental kernel PCA and the Nystr\\\"om method. (arXiv:1802.00043v1 [stat.ML])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00043"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00043", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a742d3cbfad4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a742d3cbfad4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Incremental versions of batch algorithms are often desired, for increased \ntime efficiency in the streaming data setting, or increased memory efficiency \nin general. In this paper we present a novel algorithm for incremental kernel \nPCA, based on rank one updates to the eigendecomposition of the kernel matrix, \nwhich is more computationally efficient than comparable existing algorithms. We \nextend our algorithm to incremental calculation of the Nystr\\\"om approximation \nto the kernel matrix, the first such algorithm proposed. Incremental \ncalculation of the Nystr\\\"om approximation leads to further gains in memory \nefficiency, and allows for empirical evaluation of when a subset of sufficient \nsize has been obtained. \n</p>"}, "author": "Fredrik Hallgren, Paul Northrop", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224151", "id": "tag:google.com,2005:reader/item/00000003684d3f57", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Modified Sigma-Pi-Sigma Neural Network with Adaptive Choice of Multinomials. (arXiv:1802.00123v1 [cs.LG])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00123"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00123", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Sigma-Pi-Sigma neural networks (SPSNNs) as a kind of high-order neural \nnetworks can provide more powerful mapping capability than the traditional \nfeedforward neural networks (Sigma-Sigma neural networks). In the existing \nliterature, in order to reduce the number of the Pi nodes in the Pi layer, a \nspecial multinomial P_s is used in SPSNNs. Each monomial in P_s is linear with \nrespect to each particular variable sigma_i when the other variables are taken \nas constants. Therefore, the monomials like sigma_i^n or sigma_i^n sigma_j with \nn&gt;1 are not included. This choice may be somehow intuitive, but is not \nnecessarily the best. We propose in this paper a modified Sigma-Pi-Sigma neural \nnetwork (MSPSNN) with an adaptive approach to find a better multinomial for a \ngiven problem. To elaborate, we start from a complete multinomial with a given \norder. Then we employ a regularization technique in the learning process for \nthe given problem to reduce the number of monomials used in the multinomial, \nand end up with a new SPSNN involving the same number of monomials (= the \nnumber of nodes in the Pi-layer) as in P_s. Numerical experiments on some \nbenchmark problems show that our MSPSNN behaves better than the traditional \nSPSNN with P_s. \n</p>"}, "author": "Feng Li, Yan Liu, Khidir Shaib Mohamed, Wei Wu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224150", "id": "tag:google.com,2005:reader/item/00000003684d3f6d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning with Data Dependent Implicit Activation Function. (arXiv:1802.00168v1 [cs.LG])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00168"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00168", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Though deep neural networks (DNNs) achieve remarkable performances in many \nartificial intelligence tasks, the lack of training instances remains a \nnotorious challenge. As the network goes deeper, the generalization accuracy \ndecays rapidly in the situation of lacking massive amounts of training data. In \nthis paper, we propose novel deep neural network structures that can be \ninherited from all existing DNNs with almost the same level of complexity, and \ndevelop simple training algorithms. We show our paradigm successfully resolves \nthe lack of data issue. Tests on the CIFAR10 and CIFAR100 image recognition \ndatasets show that the new paradigm leads to 20$\\%$ to $30\\%$ relative error \nrate reduction compared to their base DNNs. The intuition of our algorithms for \ndeep residual network stems from theories of the partial differential equation \n(PDE) control problems. Code will be made available. \n</p>"}, "author": "Bao Wang, Xiyang Luo, Zhen Li, Wei Zhu, Zuoqiang Shi, Stanley J. Osher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224149", "id": "tag:google.com,2005:reader/item/00000003684d3f8f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "One-class Collective Anomaly Detection based on Long Short-Term Memory Recurrent Neural Networks. (arXiv:1802.00324v1 [cs.LG])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00324"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00324", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Intrusion detection for computer network systems has been becoming one of the \nmost critical tasks for network administrators today. It has an important role \nfor organizations, governments and our society due to the valuable resources \nhosted on computer networks. Traditional misuse detection strategies are unable \nto detect new and unknown intrusion types. In contrast, anomaly detection in \nnetwork security aims to distinguish between illegal or malicious events and \nnormal behavior of network systems. Anomaly detection can be considered as a \nclassification problem where it builds models of normal network behavior, of \nwhich it uses to detect new patterns that significantly deviate from the model. \nMost of the current approaches on anomaly detection is based on the learning of \nnormal behavior and anomalous actions. They do not include memory that is they \ndo not take into account previous events classify new ones. In this paper, we \npropose a one class collective anomaly detection model based on neural network \nlearning. Normally a Long Short Term Memory Recurrent Neural Network (LSTM RNN) \nis trained only on normal data, and it is capable of predicting several time \nsteps ahead of an input. In our approach, a LSTM RNN is trained on normal time \nseries data before performing a prediction for each time step. Instead of \nconsidering each time-step separately, the observation of prediction errors \nfrom a certain number of time-steps is now proposed as a new idea for detecting \ncollective anomalies. The prediction errors of a certain number of the latest \ntime-steps above a threshold will indicate a collective anomaly. The model is \nevaluated on a time series version of the KDD 1999 dataset. The experiments \ndemonstrate that the proposed model is capable to detect collective anomaly \nefficiently \n</p>"}, "author": "Nga Nguyen Thi, Van Loi Cao, Nhien-An Le-Khac", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224148", "id": "tag:google.com,2005:reader/item/00000003684d3f99", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Boundary-Seeking Generative Adversarial Networks. (arXiv:1702.08431v3 [stat.ML] UPDATED)", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1702.08431"}], "alternate": [{"href": "http://arxiv.org/abs/1702.08431", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative adversarial networks (GANs) are a learning framework that rely on \ntraining a discriminator to estimate a measure of difference between a target \nand generated distributions. GANs, as normally formulated, rely on the \ngenerated samples being completely differentiable w.r.t. the generative \nparameters, and thus do not work for discrete data. We introduce a method for \ntraining GANs with discrete data that uses the estimated difference measure \nfrom the discriminator to compute importance weights for generated samples, \nthus providing a policy gradient for training the generator. The importance \nweights have a strong connection to the decision boundary of the discriminator, \nand we call our method boundary-seeking GANs (BGANs). We demonstrate the \neffectiveness of the proposed algorithm with discrete image and character-based \nnatural language generation. In addition, the boundary-seeking objective \nextends to continuous data, which can be used to improve stability of training. \n</p>"}, "author": "R Devon Hjelm, Athul Paul Jacob, Tong Che, Adam Trischler, Kyunghyun Cho, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224147", "id": "tag:google.com,2005:reader/item/00000003684d3fa1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting. (arXiv:1709.01604v3 [cs.CR] UPDATED)", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.01604"}], "alternate": [{"href": "http://arxiv.org/abs/1709.01604", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine learning algorithms, when applied to sensitive data, pose a distinct \nthreat to privacy. A growing body of prior work demonstrates that models \nproduced by these algorithms may leak specific private information in the \ntraining data to an attacker, either through the models' structure or their \nobservable behavior. However, the underlying cause of this privacy risk is not \nwell understood beyond a handful of anecdotal accounts that suggest overfitting \nand influence might play a role. \n</p> \n<p>This paper examines the effect that overfitting and influence have on the \nability of an attacker to learn information about the training data from \nmachine learning models, either through training set membership inference or \nattribute inference attacks. Using both formal and empirical analyses, we \nillustrate a clear relationship between these factors and the privacy risk that \narises in several popular machine learning algorithms. We find that overfitting \nis sufficient to allow an attacker to perform membership inference and, when \nthe target attribute meets certain conditions about its influence, attribute \ninference attacks. Interestingly, our formal analysis also shows that \noverfitting is not necessary for these attacks and begins to shed light on what \nother factors may be in play. Finally, we explore the connection between \nmembership inference and attribute inference, showing that there are deep \nconnections between the two that lead to effective new attacks. \n</p>"}, "author": "Samuel Yeom, Matt Fredrikson, Irene Giacomelli, Somesh Jha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224146", "id": "tag:google.com,2005:reader/item/00000003684d3fcd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting. (arXiv:1709.04875v3 [cs.LG] UPDATED)", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.04875"}], "alternate": [{"href": "http://arxiv.org/abs/1709.04875", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Timely accurate traffic forecast is crucial for urban traffic control and \nguidance. Due to the high nonlinearity and complexity of traffic flow, \ntraditional methods cannot satisfy the requirements of mid-and-long term \nprediction tasks and often neglect spatial and temporal dependencies. In this \npaper, we propose a novel deep learning framework, Spatio-Temporal Graph \nConvolutional Networks (STGCN), to tackle the time series prediction problem in \ntraffic domain. Instead of applying regular convolutional and recurrent units, \nwe formulate the problem on graphs and build the model with complete \nconvolutional structures, which enable much faster training speed with fewer \nparameters. Experiments show that our STGCN model effectively captures \ncomprehensive spatio-temporal correlations through modeling multi-scale traffic \nnetworks and consistently outperforms state-of-the-art baselines on various \nreal-world traffic datasets. \n</p>"}, "author": "Bing Yu, Haoteng Yin, Zhanxing Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166253", "id": "tag:google.com,2005:reader/item/00000003678215c9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multi-Layer Competitive-Cooperative Framework for Performance Enhancement of Differential Evolution. (arXiv:1801.10546v1 [cs.NE])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10546"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10546", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Differential Evolution (DE) is one of the most powerful optimizers in the \nevolutionary algorithm (EA) family. In recent years, many DE variants have been \nproposed to enhance performance. However, when compared with each other, \nsignificant differences in performances are seldomly observed. To meet this \nchallenge of a more significant improvement, this paper proposes a multi-layer \ncompetitive-cooperative (MLCC) framework to combine the advantages of multiple \nDEs. Existing multi-method strategies commonly use a multi-population based \nstructure, which classifies the entire population into several subpopulations \nand evolve individuals only in their corresponding subgroups. MLCC proposes to \nimplement a parallel structure with the entire population simultaneously \nmonitored by multiple DEs assigned in multiple layers. Each individual can \nstore, utilize and update its evolution information in different layers by \nusing a novel individual preference based layer selecting (IPLS) mechanism and \na computational resource allocation bias (RAB) mechanism. In IPLS, individuals \nonly connect to one favorite layer. While in RAB, high quality solutions are \nevolved by considering all the layers. In this way, the multiple layers work in \na competitive and cooperative manner. The proposed MLCC framework has been \nimplemented on several highly competitive DEs. Experimental studies show that \nMLCC variants significantly outperform the baseline DEs as well as several \nstate-of-the-art and up-to-date DEs on the CEC benchmark functions. \n</p>"}, "author": "Sheng Xin Zhang, Li Ming Zheng, Kit Sang Tang, Shao Yong Zheng, Wing Shing Chan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166252", "id": "tag:google.com,2005:reader/item/00000003678215d2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Reinforcement Learning for Programming Language Correction. (arXiv:1801.10467v1 [cs.AI])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10467"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10467", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Novice programmers often struggle with the formal syntax of programming \nlanguages. To assist them, we design a novel programming language correction \nframework amenable to reinforcement learning. The framework allows an agent to \nmimic human actions for text navigation and editing. We demonstrate that the \nagent can be trained through self-exploration directly from the raw input, that \nis, program text itself, without any knowledge of the formal syntax of the \nprogramming language. We leverage expert demonstrations for one tenth of the \ntraining data to accelerate training. The proposed technique is evaluated on \n6975 erroneous C programs with typographic errors, written by students during \nan introductory programming course. Our technique fixes 14% more programs and \n29% more compiler error messages relative to those fixed by a state-of-the-art \ntool, DeepFix, which uses a fully supervised neural machine translation \napproach. \n</p>"}, "author": "Rahul Gupta, Aditya Kanade, Shirish Shevade", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166251", "id": "tag:google.com,2005:reader/item/00000003678215dc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Fraternal Dropout. (arXiv:1711.00066v3 [stat.ML] UPDATED)", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00066"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00066", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent neural networks (RNNs) are important class of architectures among \nneural networks useful for language modeling and sequential prediction. \nHowever, optimizing RNNs is known to be harder compared to feed-forward neural \nnetworks. A number of techniques have been proposed in literature to address \nthis problem. In this paper we propose a simple technique called fraternal \ndropout that takes advantage of dropout to achieve this goal. Specifically, we \npropose to train two identical copies of an RNN (that share parameters) with \ndifferent dropout masks while minimizing the difference between their \n(pre-softmax) predictions. In this way our regularization encourages the \nrepresentations of RNNs to be invariant to dropout mask, thus being robust. We \nshow that our regularization term is upper bounded by the expectation-linear \ndropout objective which has been shown to address the gap due to the difference \nbetween the train and inference phases of dropout. We evaluate our model and \nachieve state-of-the-art results in sequence modeling tasks on two benchmark \ndatasets - Penn Treebank and Wikitext-2. We also show that our approach leads \nto performance improvement by a significant margin in image captioning \n(Microsoft COCO) and semi-supervised (CIFAR-10) tasks. \n</p>"}, "author": "Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166250", "id": "tag:google.com,2005:reader/item/00000003678215e9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Learning to Classify from Impure Samples. (arXiv:1801.10158v1 [hep-ph])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10158"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10158", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a742d3cbfcc4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a742d3cbfcc4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A persistent challenge in practical classification tasks is that labelled \ntraining sets are not always available. In particle physics, this challenge is \nsurmounted by the use of simulations. These simulations accurately reproduce \nmost features of data, but cannot be trusted to capture all of the complex \ncorrelations exploitable by modern machine learning methods. Recent work in \nweakly supervised learning has shown that simple, low-dimensional classifiers \ncan be trained using only the impure mixtures present in data. Here, we \ndemonstrate that complex, high-dimensional classifiers can also be trained on \nimpure mixtures using weak supervision techniques, with performance comparable \nto what could be achieved with pure samples. Using weak supervision will \ntherefore allow us to avoid relying exclusively on simulations for \nhigh-dimensional classification. This work opens the door to a new regime \nwhereby complex models are trained directly on data, providing direct access to \nprobe the underlying physics. \n</p>"}, "author": "Patrick T. Komiske, Eric M. Metodiev, Benjamin Nachman, Matthew D. Schwartz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166249", "id": "tag:google.com,2005:reader/item/00000003678215ef", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Kernel Distillation for Gaussian Processes. (arXiv:1801.10273v1 [stat.ML])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10273"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10273", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a742d3d0d035\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a742d3d0d035&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Gaussian processes (GPs) are flexible models that can capture complex \nstructure in large-scale dataset due to their non-parametric nature. However, \nthe usage of GPs in real-world application is limited due to their high \ncomputational cost at inference time. In this paper, we introduce a new \nframework, \\textit{kernel distillation}, for kernel matrix approximation. The \nidea adopts from knowledge distillation in deep learning community, where we \napproximate a fully trained teacher kernel matrix of size $n\\times n$ with a \nstudent kernel matrix. We combine inducing points method with sparse low-rank \napproximation in the distillation procedure. The distilled student kernel \nmatrix only cost $\\mathcal{O}(m^2)$ storage where $m$ is the number of inducing \npoints and $m \\ll n$. We also show that one application of kernel distillation \nis for fast GP prediction, where we demonstrate empirically that our \napproximation provide better balance between the prediction time and the \npredictive performance compared to the alternatives. \n</p>"}, "author": "Congzheng Song, Yiming Sun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166248", "id": "tag:google.com,2005:reader/item/00000003678215f4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Multi-view Learning to Rank. (arXiv:1801.10402v1 [cs.LG])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10402"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10402", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of learning to rank from multiple sources. Though \nmulti-view learning and learning to rank have been studied extensively leading \nto a wide range of applications, multi-view learning to rank as a synergy of \nboth topics has received little attention. The aim of the paper is to propose a \ncomposite ranking method while keeping a close correlation with the individual \nrankings simultaneously. We propose a multi-objective solution to ranking by \ncapturing the information of the feature mapping from both within each view as \nwell as across views using autoencoder-like networks. Moreover, a novel \nend-to-end solution is introduced to enhance the joint ranking with minimum \nview-specific ranking loss, so that we can achieve the maximum global view \nagreements within a single optimization process. The proposed method is \nvalidated on a wide variety of ranking problems, including university ranking, \nmulti-view lingual text ranking and image data ranking, providing superior \nresults. \n</p>"}, "author": "Guanqun Cao, Alexandros Iosifidis, Moncef Gabbouj, Vijay Raghavan, Raju Gottumukkala", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166247", "id": "tag:google.com,2005:reader/item/00000003678215fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Model compression for faster structural separation of macromolecules captured by Cellular Electron Cryo-Tomography. (arXiv:1801.10597v1 [q-bio.QM])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10597"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10597", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Electron Cryo-Tomography (ECT) enables 3D visualization of macromolecule \nstructure inside single cells. Macromolecule classification approaches based on \nconvolutional neural networks (CNN) were developed to separate millions of \nmacromolecules captured from ECT systematically. However, given the fast \naccumulation of ECT data, it will soon become necessary to use CNN models to \nefficiently and accurately separate substantially more macromolecules at the \nprediction stage, which requires additional computational costs. To speed up \nthe prediction, we compress classification models into compact neural networks \nwith little in accuracy for deployment. Specifically, we propose to perform \nmodel compression through knowledge distillation. Firstly, a complex teacher \nnetwork is trained to generate soft labels with better classification \nfeasibility followed by training of customized student networks with simple \narchitectures using the soft label to compress model complexity. Our tests \ndemonstrate that our compressed models significantly reduce the number of \nparameters and time cost while maintaining similar classification accuracy. \n</p>"}, "author": "Jialiang Guo, Bo Zhou, Xiangrui Zeng, Zachary Freyberg, Min Xu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166246", "id": "tag:google.com,2005:reader/item/0000000367821606", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Attacking Binarized Neural Networks. (arXiv:1711.00449v2 [cs.LG] UPDATED)", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00449"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00449", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks with low-precision weights and activations offer compelling \nefficiency advantages over their full-precision equivalents. The two most \nfrequently discussed benefits of quantization are reduced memory consumption, \nand a faster forward pass when implemented with efficient bitwise operations. \nWe propose a third benefit of very low-precision neural networks: improved \nrobustness against some adversarial attacks, and in the worst case, performance \nthat is on par with full-precision models. We focus on the very low-precision \ncase where weights and activations are both quantized to $\\pm$1, and note that \nstochastically quantizing weights in just one layer can sharply reduce the \nimpact of iterative attacks. We observe that non-scaled binary neural networks \nexhibit a similar effect to the original defensive distillation procedure that \nled to gradient masking, and a false notion of security. We address this by \nconducting both black-box and white-box experiments with binary models that do \nnot artificially mask gradients. \n</p>"}, "author": "Angus Galloway, Graham W. Taylor, Medhat Moussa", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893436", "id": "tag:google.com,2005:reader/item/0000000366c10d01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "ReNN: Rule-embedded Neural Networks. (arXiv:1801.09856v1 [cs.LG])", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09856"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09856", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The artificial neural network shows powerful ability of inference, but it is \nstill criticized for lack of interpretability and prerequisite needs of big \ndataset. This paper proposes the Rule-embedded Neural Network (ReNN) to \novercome the shortages. ReNN first makes local-based inferences to detect local \npatterns, and then uses rules based on domain knowledge about the local \npatterns to generate rule-modulated map. After that, ReNN makes global-based \ninferences that synthesizes the local patterns and the rule-modulated map. To \nsolve the optimization problem caused by rules, we use a two-stage optimization \nstrategy to train the ReNN model. By introducing rules into ReNN, we can \nstrengthen traditional neural networks with long-term dependencies which are \ndifficult to learn with limited empirical dataset, thus improving inference \naccuracy. The complexity of neural networks can be reduced since long-term \ndependencies are not modeled with neural connections, and thus the amount of \ndata needed to optimize the neural networks can be reduced. Besides, inferences \nfrom ReNN can be analyzed with both local patterns and rules, and thus have \nbetter interpretability. In this paper, ReNN has been validated with a \ntime-series detection problem. \n</p>"}, "author": "Hu Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893433", "id": "tag:google.com,2005:reader/item/0000000366c10d0c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Over-representation of Extreme Events in Decision-Making: A Rational Metacognitive Account. (arXiv:1801.09848v1 [q-bio.NC])", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09848"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09848", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Availability bias, manifested in the over-representation of extreme \neventualities in decision-making, is a well-known cognitive bias, and is \ngenerally taken as evidence of human irrationality. In this work, we present \nthe first rational, metacognitive account of the Availability bias, formally \narticulated at Marr's algorithmic level of analysis. Concretely, we present a \nnormative, metacognitive model of how a cognitive system should over-represent \nextreme eventualities, depending on the amount of time available at its \ndisposal for decision-making. Our model also accounts for two well-known \nframing effects in human decision-making under risk---the fourfold pattern of \nrisk preferences in outcome probability (Tversky &amp; Kahneman, 1992) and in \noutcome magnitude (Markovitz, 1952)---thereby providing the first \nmetacognitively-rational basis for those effects. Empirical evidence, \nfurthermore, confirms an important prediction of our model. Surprisingly, our \nmodel is unimaginably robust with respect to its focal parameter. We discuss \nthe implications of our work for studies on human decision-making, and conclude \nby presenting a counterintuitive prediction of our model, which, if confirmed, \nwould have intriguing implications for human decision-making under risk. To our \nknowledge, our model is the first metacognitive, resource-rational process \nmodel of cognitive biases in decision-making. \n</p>"}, "author": "Ardavan S. Nobandegani, Kevin da Silva Castanheira, A. Ross Otto, Thomas R. Shultz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893432", "id": "tag:google.com,2005:reader/item/0000000366c10d15", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimal Number of Choices in Rating Contexts. (arXiv:1605.06588v6 [cs.AI] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1605.06588"}], "alternate": [{"href": "http://arxiv.org/abs/1605.06588", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In many settings people must give numerical scores to entities from a small \ndiscrete set. For instance, rating physical attractiveness from 1--5 on dating \nsites, or papers from 1--10 for conference reviewing. We study the problem of \nunderstanding when using a different number of options is optimal. For \nconcreteness we assume the true underlying scores are integers from 1--100. We \nconsider the case when scores are uniform random and Gaussian. We study when \nusing 2, 3, 4, 5, and 10 options is optimal in these models. One may expect \nthat using more options would always improve performance in this model, but we \nshow that this is not necessarily the case, and that using fewer choices---even \njust two---can surprisingly be optimal in certain situations. While in theory \nfor this setting it would be optimal to use all 100 options, in practice this \nis prohibitive, and it is preferable to utilize a smaller number of options due \nto humans' limited computational resources. Our results suggest that using a \nsmaller number of options than is typical could be optimal in certain \nsituations. This would have many potential applications, as settings requiring \nentities to be ranked by humans are ubiquitous. \n</p>"}, "author": "Sam Ganzfried, Farzana Yusuf", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893431", "id": "tag:google.com,2005:reader/item/0000000366c10d1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Towards a Quantum World Wide Web. (arXiv:1703.06642v2 [cs.AI] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.06642"}], "alternate": [{"href": "http://arxiv.org/abs/1703.06642", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We elaborate a quantum model for the meaning associated with corpora of \nwritten documents, like the pages forming the World Wide Web. To that end, we \nare guided by how physicists constructed quantum theory for microscopic \nentities, which unlike classical objects cannot be fully represented in our \nspatial theater. We suggest that a similar construction needs to be carried out \nby linguists and computational scientists, to capture the full meaning carried \nby collections of documental entities. More precisely, we show how to associate \na quantum-like 'entity of meaning' to a 'language entity formed by printed \ndocuments', considering the latter as the collection of traces that are left by \nthe former, in specific results of search actions that we describe as \nmeasurements. In other words, we offer a perspective where a collection of \ndocuments, like the Web, is described as the space of manifestation of a more \ncomplex entity - the QWeb - which is the object of our modeling, drawing its \ninspiration from previous studies on operational-realistic approaches to \nquantum physics and quantum modeling of human cognition and decision-making. We \nemphasize that a consistent QWeb model needs to account for the observed \ncorrelations between words appearing in printed documents, e.g., \nco-occurrences, as the latter would depend on the 'meaning connections' \nexisting between the concepts that are associated with these words. In that \nrespect, we show that both 'context and interference (quantum) effects' are \nrequired to explain the probabilities calculated by counting the relative \nnumber of documents containing certain words and co-ocurrrences of words. \n</p>"}, "author": "Diederik Aerts, Jonito Aerts Arguelles, Lester Beltran, Lyneth Beltran, Isaac Distrito, Massimiliano Sassoli de Bianchi, Sandro Sozzo, Tomas Veloz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893430", "id": "tag:google.com,2005:reader/item/0000000366c10d22", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Towards Synthesizing Complex Programs from Input-Output Examples. (arXiv:1706.01284v2 [cs.LG] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.01284"}], "alternate": [{"href": "http://arxiv.org/abs/1706.01284", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In recent years, deep learning techniques have been developed to improve the \nperformance of program synthesis from input-output examples. Albeit its \nsignificant progress, the programs that can be synthesized by state-of-the-art \napproaches are still simple in terms of their complexity. In this work, we move \na significant step forward along this direction by proposing a new class of \nchallenging tasks in the domain of program synthesis from input-output \nexamples: learning a context-free parser from pairs of input programs and their \nparse trees. We show that this class of tasks are much more challenging than \npreviously studied tasks, and the test accuracy of existing approaches is \nalmost 0%. \n</p> \n<p>We tackle the challenges by developing three novel techniques inspired by \nthree novel observations, which reveal the key ingredients of using deep \nlearning to synthesize a complex program. First, the use of a \nnon-differentiable machine is the key to effectively restrict the search space. \nThus our proposed approach learns a neural program operating a domain-specific \nnon-differentiable machine. Second, recursion is the key to achieve \ngeneralizability. Thus, we bake-in the notion of recursion in the design of our \nnon-differentiable machine. Third, reinforcement learning is the key to learn \nhow to operate the non-differentiable machine, but it is also hard to train the \nmodel effectively with existing reinforcement learning algorithms from a cold \nboot. We develop a novel two-phase reinforcement learning-based search \nalgorithm to overcome this issue. In our evaluation, we show that using our \nnovel approach, neural parsing programs can be learned to achieve 100% test \naccuracy on test inputs that are 500x longer than the training samples. \n</p>"}, "author": "Xinyun Chen, Chang Liu, Dawn Song", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893429", "id": "tag:google.com,2005:reader/item/0000000366c10d2c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Bayesian Neural Networks. (arXiv:1801.07710v2 [cs.LG] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.07710"}], "alternate": [{"href": "http://arxiv.org/abs/1801.07710", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a742d3d0d201\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a742d3d0d201&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper describes and discusses Bayesian Neural Network (BNN). The paper \nshowcases a few different applications of them for classification and \nregression problems. BNNs are comprised of a Probabilistic Model and a Neural \nNetwork. The intent of such a design is to combine the strengths of Neural \nNetworks and Stochastic modeling. Neural Networks exhibit continuous function \napproximator capabilities. Stochastic models allow direct specification of a \nmodel with known interaction between parameters to generate data. During the \nprediction phase, stochastic models generate a complete posterior distribution \nand produce probabilistic guarantees on the predictions. Thus BNNs are a unique \ncombination of neural network and stochastic models with the stochastic model \nforming the core of this integration. BNNs can then produce probabilistic \nguarantees on it's predictions and also generate the distribution of parameters \nthat it has learnt from the observations. That means, in the parameter space, \none can deduce the nature and shape of the neural network's learnt parameters. \nThese two characteristics makes them highly attractive to theoreticians as well \nas practitioners. Recently there has been a lot of activity in this area, with \nthe advent of numerous probabilistic programming libraries such as: PyMC3, \nEdward, Stan etc. Further this area is rapidly gaining ground as a standard \nmachine learning approach for numerous problems \n</p>"}, "author": "Vikram Mullachery, Aniruddh Khera, Amir Husain", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893428", "id": "tag:google.com,2005:reader/item/0000000366c10d3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Cross-type Biomedical Named Entity Recognition with Deep Multi-Task Learning. (arXiv:1801.09851v1 [cs.IR])", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09851"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09851", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Motivation: Biomedical named entity recognition (BioNER) is the most \nfundamental task in biomedical text mining. State-of-the-art BioNER systems \noften require handcrafted features specifically designed for each type of \nbiomedical entities. This feature generation process requires intensive labors \nfrom biomedical and linguistic experts, and makes it difficult to adapt these \nsystems to new biomedical entity types. Although recent studies explored using \nneural network models for BioNER to free experts from manual feature \ngeneration, these models still require substantial human efforts to annotate \nmassive training data. \n</p> \n<p>Results: We propose a multi-task learning framework for BioNER that is based \non neural network models to save human efforts. We build a global model by \ncollectively training multiple models that share parameters, each model \ncapturing the characteristics of a different biomedical entity type. In \nexperiments on five BioNER benchmark datasets covering four major biomedical \nentity types, our model outperforms state-of-the-art systems and other neural \nnetwork models by a large margin, even when only limited training data are \navailable. Further analysis shows that the large performance gains come from \nsharing character- and word-level information between different biomedical \nentities. The approach creates new opportunities for text-mining approaches to \nhelp biomedical scientists better exploit knowledge in biomedical literature. \n</p>"}, "author": "Xuan Wang, Yu Zhang, Xiang Ren, Yuhao Zhang, Marinka Zitnik, Jingbo Shang, Curtis Langlotz, Jiawei Han", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893427", "id": "tag:google.com,2005:reader/item/0000000366c10d4b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Cardiac Arrhythmia Detection from ECG Combining Convolutional and Long Short-Term Memory Networks. (arXiv:1801.10033v1 [eess.SP])", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10033"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10033", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Objectives: Atrial fibrillation (AF) is a common heart rhythm disorder \nassociated with deadly and debilitating consequences including heart failure, \nstroke, poor mental health, reduced quality of life and death. Having an \nautomatic system that diagnoses various types of cardiac arrhythmias would \nassist cardiologists to initiate appropriate preventive measures and to improve \nthe analysis of cardiac disease. To this end, this paper introduces a new \napproach to detect and classify automatically cardiac arrhythmias in \nelectrocardiograms (ECG) recordings. \n</p> \n<p>Methods: The proposed approach used a combination of Convolution Neural \nNetworks (CNNs) and a sequence of Long Short-Term Memory (LSTM) units, with \npooling, dropout and normalization techniques to improve their accuracy. The \nnetwork predicted a classification at every 18th input sample and we selected \nthe final prediction for classification. Results were cross-validated on the \nPhysionet Challenge 2017 training dataset, which contains 8,528 single lead ECG \nrecordings lasting from 9s to just over 60s. \n</p> \n<p>Results: Using the proposed structure and no explicit feature selection, \n10-fold stratified cross-validation gave an overall F-measure of 0.83.10-0.015 \non the held-out test data (mean-standard deviation over all folds) and 0.80 on \nthe hidden dataset of the Challenge entry server. \n</p>"}, "author": "Philip Warrick (1), Masun Nabhan Homsi (2) ((1) PeriGen. Inc., Montreal, Canada, (2) Simon Bolivar University, Caracas, Venezuela)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893426", "id": "tag:google.com,2005:reader/item/0000000366c10d54", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian Nonparametric Kernel-Learning. (arXiv:1506.08776v2 [stat.ML] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1506.08776"}], "alternate": [{"href": "http://arxiv.org/abs/1506.08776", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Kernel methods are ubiquitous tools in machine learning. However, there is \noften little reason for the common practice of selecting a kernel a priori. \nEven if a universal approximating kernel is selected, the quality of the finite \nsample estimator may be greatly affected by the choice of kernel. Furthermore, \nwhen directly applying kernel methods, one typically needs to compute a $N \n\\times N$ Gram matrix of pairwise kernel evaluations to work with a dataset of \n$N$ instances. The computation of this Gram matrix precludes the direct \napplication of kernel methods on large datasets, and makes kernel learning \nespecially difficult. In this paper we introduce Bayesian nonparmetric \nkernel-learning (BaNK), a generic, data-driven framework for scalable learning \nof kernels. BaNK places a nonparametric prior on the spectral distribution of \nrandom frequencies allowing it to both learn kernels and scale to large \ndatasets. We show that this framework can be used for large scale regression \nand classification tasks. Furthermore, we show that BaNK outperforms several \nother scalable approaches for kernel learning on a variety of real world \ndatasets. \n</p>"}, "author": "Junier Oliva, Avinava Dubey, Andrew G. Wilson, Barnabas Poczos, Jeff Schneider, Eric P. Xing", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893425", "id": "tag:google.com,2005:reader/item/0000000366c10d5b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Ensemble Adversarial Training: Attacks and Defenses. (arXiv:1705.07204v3 [stat.ML] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.07204"}], "alternate": [{"href": "http://arxiv.org/abs/1705.07204", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Adversarial examples are perturbed inputs designed to fool machine learning \nmodels. Adversarial training injects such examples into training data to \nincrease robustness. To scale this technique to large datasets, perturbations \nare crafted using fast single-step methods that maximize a linear approximation \nof the model's loss. We show that this form of adversarial training converges \nto a degenerate global minimum, wherein small curvature artifacts near the data \npoints obfuscate a linear approximation of the loss. The model thus learns to \ngenerate weak perturbations, rather than defend against strong ones. As a \nresult, we find that adversarial training remains vulnerable to black-box \nattacks, where we transfer perturbations computed on undefended models, as well \nas to a powerful novel single-step attack that escapes the non-smooth vicinity \nof the input data via a small random step. We further introduce Ensemble \nAdversarial Training, a technique that augments training data with \nperturbations transferred from other models. On ImageNet, Ensemble Adversarial \nTraining yields models with strong robustness to black-box attacks. In \nparticular, our most robust model won the first round of the NIPS 2017 \ncompetition on Defenses against Adversarial Attacks. \n</p>"}, "author": "Florian Tram&#xe8;r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick McDaniel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893424", "id": "tag:google.com,2005:reader/item/0000000366c10d5e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "SEARNN: Training RNNs with Global-Local Losses. (arXiv:1706.04499v2 [cs.LG] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.04499"}], "alternate": [{"href": "http://arxiv.org/abs/1706.04499", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose SEARNN, a novel training algorithm for recurrent neural networks \n(RNNs) inspired by the \"learning to search\" (L2S) approach to structured \nprediction. RNNs have been widely successful in structured prediction \napplications such as machine translation or parsing, and are commonly trained \nusing maximum likelihood estimation (MLE). Unfortunately, this training loss is \nnot always an appropriate surrogate for the test error: by only maximizing the \nground truth probability, it fails to exploit the wealth of information offered \nby structured losses. Further, it introduces discrepancies between training and \npredicting (such as exposure bias) that may hurt test performance. Instead, \nSEARNN leverages test-alike search space exploration to introduce global-local \nlosses that are closer to the test error. We first demonstrate improved \nperformance over MLE on two different tasks: OCR and spelling correction. Then, \nwe propose a subsampling strategy to enable SEARNN to scale to large vocabulary \nsizes. This allows us to validate the benefits of our approach on a machine \ntranslation task. \n</p>"}, "author": "R&#xe9;mi Leblond, Jean-Baptiste Alayrac, Anton Osokin, Simon Lacoste-Julien", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893423", "id": "tag:google.com,2005:reader/item/0000000366c10d62", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards Neural Phrase-based Machine Translation. (arXiv:1706.05565v5 [cs.CL] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.05565"}], "alternate": [{"href": "http://arxiv.org/abs/1706.05565", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our \nmethod explicitly models the phrase structures in output sequences using \nSleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence \nmodeling method. To mitigate the monotonic alignment requirement of SWAN, we \nintroduce a new layer to perform (soft) local reordering of input sequences. \nDifferent from existing neural machine translation (NMT) approaches, NPMT does \nnot use attention-based decoding mechanisms. Instead, it directly outputs \nphrases in a sequential order and can decode in linear time. Our experiments \nshow that NPMT achieves superior performances on IWSLT 2014 \nGerman-English/English-German and IWSLT 2015 English-Vietnamese machine \ntranslation tasks compared with strong NMT baselines. We also observe that our \nmethod produces meaningful phrases in output languages. \n</p>"}, "author": "Po-Sen Huang, Chong Wang, Sitao Huang, Dengyong Zhou, Li Deng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893422", "id": "tag:google.com,2005:reader/item/0000000366c10d65", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multiresolution Kernel Approximation for Gaussian Process Regression. (arXiv:1708.02183v3 [stat.ML] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.02183"}], "alternate": [{"href": "http://arxiv.org/abs/1708.02183", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Gaussian process regression generally does not scale to beyond a few \nthousands data points without applying some sort of kernel approximation \nmethod. Most approximations focus on the high eigenvalue part of the spectrum \nof the kernel matrix, $K$, which leads to bad performance when the length scale \nof the kernel is small. In this paper we introduce Multiresolution Kernel \nApproximation (MKA), the first true broad bandwidth kernel approximation \nalgorithm. Important points about MKA are that it is memory efficient, and it \nis a direct method, which means that it also makes it easy to approximate \n$K^{-1}$ and $\\mathop{\\textrm{det}}(K)$. \n</p>"}, "author": "Yi Ding, Risi Kondor, Jonathan Eskreis-Winkler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893421", "id": "tag:google.com,2005:reader/item/0000000366c10d6a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields. (arXiv:1708.08819v3 [cs.LG] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.08819"}], "alternate": [{"href": "http://arxiv.org/abs/1708.08819", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative adversarial networks (GANs) evolved into one of the most \nsuccessful unsupervised techniques for generating realistic images. Even though \nit has recently been shown that GAN training converges, GAN models often end up \nin local Nash equilibria that are associated with mode collapse or otherwise \nfail to model the target distribution. We introduce Coulomb GANs, which pose \nthe GAN learning problem as a potential field of charged particles, where \ngenerated samples are attracted to training set samples but repel each other. \nThe discriminator learns a potential field while the generator decreases the \nenergy by moving its samples along the vector (force) field determined by the \ngradient of the potential field. Through decreasing the energy, the GAN model \nlearns to generate samples according to the whole target distribution and does \nnot only cover some of its modes. We prove that Coulomb GANs possess only one \nNash equilibrium which is optimal in the sense that the model distribution \nequals the target distribution. We show the efficacy of Coulomb GANs on a \nvariety of image datasets. On LSUN and celebA, Coulomb GANs set a new state of \nthe art and produce a previously unseen variety of different samples. \n</p>"}, "author": "Thomas Unterthiner, Bernhard Nessler, Calvin Seward, G&#xfc;nter Klambauer, Martin Heusel, Hubert Ramsauer, Sepp Hochreiter", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517289278066", "timestampUsec": "1517289278066053", "id": "tag:google.com,2005:reader/item/0000000365f48c52", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks. (arXiv:1801.09335v1 [cs.LG])", "published": 1517289278, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09335"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09335", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is desirable to train convolutional networks (CNNs) to run more \nefficiently during inference. In many cases however, the computational budget \nthat the system has for inference cannot be known beforehand during training, \nor the inference budget is dependent on the changing real-time resource \navailability. Thus, it is inadequate to train just inference-efficient CNNs, \nwhose inference costs are not adjustable and cannot adapt to varied inference \nbudgets. We propose a novel approach for cost-adjustable inference in CNNs - \nStochastic Downsampling Point (SDPoint). During training, SDPoint applies \nfeature map downsampling to a random point in the layer hierarchy, with a \nrandom downsampling ratio. The different stochastic downsampling configurations \nknown as SDPoint instances (of the same model) have computational costs \ndifferent from each other, while being trained to minimize the same prediction \nloss. Sharing network parameters across different instances provides \nsignificant regularization boost. During inference, one may handpick a SDPoint \ninstance that best fits the inference budget. The effectiveness of SDPoint, as \nboth a cost-adjustable inference approach and a regularizer, is validated \nthrough extensive experiments on image classification. \n</p>"}, "author": "Jason Kuen, Xiangfei Kong, Zhe Lin, Gang Wang, Jianxiong Yin, Simon See, Yap-Peng Tan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517289278066", "timestampUsec": "1517289278066052", "id": "tag:google.com,2005:reader/item/0000000365f48c58", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the Behavior of Convolutional Nets for Feature Extraction. (arXiv:1703.01127v4 [cs.NE] UPDATED)", "published": 1517289278, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.01127"}], "alternate": [{"href": "http://arxiv.org/abs/1703.01127", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a742d3d0d3e6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a742d3d0d3e6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep neural networks are representation learning techniques. During training, \na deep net is capable of generating a descriptive language of unprecedented \nsize and detail in machine learning. Extracting the descriptive language coded \nwithin a trained CNN model (in the case of image data), and reusing it for \nother purposes is a field of interest, as it provides access to the visual \ndescriptors previously learnt by the CNN after processing millions of images, \nwithout requiring an expensive training phase. Contributions to this field \n(commonly known as feature representation transfer or transfer learning) have \nbeen purely empirical so far, extracting all CNN features from a single layer \nclose to the output and testing their performance by feeding them to a \nclassifier. This approach has provided consistent results, although its \nrelevance is limited to classification tasks. In a completely different \napproach, in this paper we statistically measure the discriminative power of \nevery single feature found within a deep CNN, when used for characterizing \nevery class of 11 datasets. We seek to provide new insights into the behavior \nof CNN features, particularly the ones from convolutional layers, as this can \nbe relevant for their application to knowledge representation and reasoning. \nOur results confirm that low and middle level features may behave differently \nto high level features, but only under certain conditions. We find that all CNN \nfeatures can be used for knowledge representation purposes both by their \npresence or by their absence, doubling the information a single CNN feature may \nprovide. We also study how much noise these features may include, and propose a \nthresholding approach to discard most of it. All these insights have a direct \napplication to the generation of CNN embedding spaces. \n</p>"}, "author": "Dario Garcia-Gasulla, Ferran Par&#xe9;s, Armand Vilalta, Jonatan Moreno, Eduard Ayguad&#xe9;, Jes&#xfa;s Labarta, Ulises Cort&#xe9;s, Toyotaro Suzumura", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}]}