{"all_articles": [{"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148069", "id": "tag:google.com,2005:reader/item/000000033032b1e3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "On the incorporation of interval-valued fuzzy sets into the Bousi-Prolog system: declarative semantics, implementation and applications. (arXiv:1711.03147v1 [cs.AI])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03147"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03147", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3be548ff\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3be548ff&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper we analyse the benefits of incorporating interval-valued fuzzy \nsets into the Bousi-Prolog system. A syntax, declarative semantics and im- \nplementation for this extension is presented and formalised. We show, by using \npotential applications, that fuzzy logic programming frameworks enhanced with \nthem can correctly work together with lexical resources and ontologies in order \nto improve their capabilities for knowledge representation and reasoning. \n</p>"}, "author": "Clemente Rubio-Manzano, Martin Pereira-Fari&#xf1;a", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148068", "id": "tag:google.com,2005:reader/item/000000033032b1ec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Information Directed Sampling for Stochastic Bandits with Graph Feedback. (arXiv:1711.03198v1 [cs.LG])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03198"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03198", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider stochastic multi-armed bandit problems with graph feedback, where \nthe decision maker is allowed to observe the neighboring actions of the chosen \naction. We allow the graph structure to vary with time and consider both \ndeterministic and Erd\\H{o}s-R\\'enyi random graph models. For such a graph \nfeedback model, we first present a novel analysis of Thompson sampling that \nleads to tighter performance bound than existing work. Next, we propose new \nInformation Directed Sampling based policies that are graph-aware in their \ndecision making. Under the deterministic graph case, we establish a Bayesian \nregret bound for the proposed policies that scales with the clique cover number \nof the graph instead of the number of actions. Under the random graph case, we \nprovide a Bayesian regret bound for the proposed policies that scales with the \nratio of the number of actions over the expected number of observations per \niteration. To the best of our knowledge, this is the first analytical result \nfor stochastic bandits with random graph feedback. Finally, using numerical \nevaluations, we demonstrate that our proposed IDS policies outperform existing \napproaches, including adaptions of upper confidence bound, $\\epsilon$-greedy \nand Exp3 algorithms. \n</p>"}, "author": "Fang Liu, Swapna Buccapatnam, Ness Shroff", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148067", "id": "tag:google.com,2005:reader/item/000000033032b1fb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Large-scale Cloze Test Dataset Designed by Teachers. (arXiv:1711.03225v1 [cs.CL])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03225"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03225", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Cloze test is widely adopted in language exams to evaluate students' language \nproficiency. In this paper, we propose the first large-scale human-designed \ncloze test dataset CLOTH, in which the questions were used in middle-school and \nhigh-school language exams. With the missing blanks carefully created by \nteachers and candidate choices purposely designed to be confusing, CLOTH \nrequires a deeper language understanding and a wider attention span than \nprevious automatically generated cloze datasets. We show humans outperform \ndedicated designed baseline models by a significant margin, even when the model \nis trained on sufficiently large external data. We investigate the source of \nthe performance gap, trace model deficiencies to some distinct properties of \nCLOTH, and identify the limited ability of comprehending a long-term context to \nbe the key bottleneck. \n</p>"}, "author": "Qizhe Xie, Guokun Lai, Zihang Dai, Eduard Hovy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148066", "id": "tag:google.com,2005:reader/item/000000033032b221", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "CogSciK: Clustering for Cognitive Science Motivated Decision Making. (arXiv:1711.03237v1 [cs.AI])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03237"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03237", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Computational models of decisionmaking must contend with the variance of \ncontext and any number of possible decisions that a defined strategic actor can \nmake at a given time. Relying on cognitive science theory, the authors have \ncreated an algorithm that captures the orientation of the actor towards an \nobject and arrays the possible decisions available to that actor based on their \ngiven intersubjective orientation. This algorithm, like a traditional K-means \nclustering algorithm, relies on a core-periphery structure that gives the \nlikelihood of moves as those closest to the cluster's centroid. The result is \nan algorithm that enables unsupervised classification of an array of decision \npoints belonging to an actor's present state and deeply rooted in cognitive \nscience theory. \n</p>"}, "author": "Dr. W. A. Rivera, James C. Wu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148065", "id": "tag:google.com,2005:reader/item/000000033032b229", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning to select examples for program synthesis. (arXiv:1711.03243v1 [cs.AI])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03243"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03243", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Program synthesis is a class of regression problems where one seeks a \nsolution, in the form of a source-code program, mapping the inputs to their \ncorresponding outputs exactly. Due to its precise and combinatorial nature, it \nis commonly formulated as a constraint satisfaction problem, where input-output \nexamples are encoded as constraints and solved with a constraint solver. A key \nchallenge of this formulation is scalability: while constraint solvers work \nwell with few well-chosen examples, a large set of examples can incur \nsignificant overhead in both time and memory. We address this challenge by \nconstructing a representative subset of examples that is both small and able to \nconstrain the solver sufficiently. We build the subset one example at a time, \nusing a neural network to predict the probability of unchosen input-output \nexamples conditioned on the chosen input-output examples, and adding the least \nprobable example to the subset. Experiment on a diagram drawing domain shows \nour approach produces subsets of examples that are small and representative for \nthe constraint solver. \n</p>"}, "author": "Yewen Pu, Zachery Miranda, Armando Solar-Lezama, Leslie Pack Kaelbling", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148064", "id": "tag:google.com,2005:reader/item/000000033032b238", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Automated Distribution System Planning for Large-Scale Network Integration Studies. (arXiv:1711.03331v1 [cs.CE])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03331"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03331", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Network integration studies try to assess the impact of future developments, \nsuch as the increase of Renewable Energy Sources or the introduction of Smart \nGrid Technologies, on large-scale network areas. Goals can be to support \nstrategic alignment in the regulatory framework or to adapt the network \nplanning principles of Distribution System Operators. This study outlines an \napproach for the automated distribution system planning that can calculate \nnetwork reconfiguration, reinforcement and extension plans in a fully automated \nfashion. This allows the estimation of the expected cost in massive \nprobabilistic simulations of large numbers of real networks and constitutes a \ncore component of a framework for large-scale network integration studies. \nExemplary case study results are presented that were performed in cooperation \nwith different major distribution system operators. The case studies cover the \nestimation of expected network reinforcement costs, technical and economical \nassessment of smart grid technologies and structural network optimisation. \n</p>"}, "author": "Alexander Scheidler, Leon Thurner, Martin Braun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148063", "id": "tag:google.com,2005:reader/item/000000033032b242", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Repairing Ontologies via Axiom Weakening. (arXiv:1711.03430v1 [cs.AI])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03430"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03430", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Ontology engineering is a hard and error-prone task, in which small changes \nmay lead to errors, or even produce an inconsistent ontology. As ontologies \ngrow in size, the need for automated methods for repairing inconsistencies \nwhile preserving as much of the original knowledge as possible increases. Most \nprevious approaches to this task are based on removing a few axioms from the \nontology to regain consistency. We propose a new method based on weakening \nthese axioms to make them less restrictive, employing the use of refinement \noperators. We introduce the theoretical framework for weakening DL ontologies, \npropose algorithms to repair ontologies based on the framework, and provide an \nanalysis of the computational complexity. Through an empirical analysis made \nover real-life ontologies, we show that our approach preserves significantly \nmore of the original knowledge of the ontology than removing axioms. \n</p>"}, "author": "Nicolas Troquard, Roberto Confalonieri, Pietro Galliani, Rafael Penaloza, Daniele Porello, Oliver Kutz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148062", "id": "tag:google.com,2005:reader/item/000000033032b248", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Open-World Knowledge Graph Completion. (arXiv:1711.03438v1 [cs.AI])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03438"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03438", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Knowledge Graphs (KGs) have been applied to many tasks including Web search, \nlink prediction, recommendation, natural language processing, and entity \nlinking. However, most KGs are far from complete and are growing at a rapid \npace. To address these problems, Knowledge Graph Completion (KGC) has been \nproposed to improve KGs by filling in its missing connections. Unlike existing \nmethods which hold a closed-world assumption, i.e., where KGs are fixed and new \nentities cannot be easily added, in the present work we relax this assumption \nand propose a new open-world KGC task. As a first attempt to solve this task we \nintroduce an open-world KGC model called ConMask. This model learns embeddings \nof the entity's name and parts of its text-description to connect unseen \nentities to the KG. To mitigate the presence of noisy text descriptions, \nConMask uses a relationship-dependent content masking to extract relevant \nsnippets and then trains a fully convolutional neural network to fuse the \nextracted snippets with entities in the KG. Experiments on large data sets, \nboth old and new, show that ConMask performs well in the open-world KGC task \nand even outperforms existing KGC models on the standard closed-world KGC task. \n</p>"}, "author": "Baoxu Shi, Tim Weninger", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148061", "id": "tag:google.com,2005:reader/item/000000033032b255", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Worm-level Control through Search-based Reinforcement Learning. (arXiv:1711.03467v1 [cs.NE])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03467"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03467", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Through natural evolution, nervous systems of organisms formed near-optimal \nstructures to express behavior. Here, we propose an effective way to create \ncontrol agents, by \\textit{re-purposing} the function of biological neural \ncircuit models, to govern similar real world applications. We model the \ntap-withdrawal (TW) neural circuit of the nematode, \\textit{C. elegans}, a \ncircuit responsible for the worm's reflexive response to external mechanical \ntouch stimulations, and learn its synaptic and neural parameters as a policy \nfor controlling the inverted pendulum problem. For reconfiguration of the \npurpose of the TW neural circuit, we manipulate a search-based reinforcement \nlearning. We show that our neural policy performs as good as existing \ntraditional control theory and machine learning approaches. A video \ndemonstration of the performance of our method can be accessed at \n\\url{https://youtu.be/o-Ia5IVyff8}. \n</p>"}, "author": "Mathias Lechner, Radu Grosu, Ramin M. Hasani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148060", "id": "tag:google.com,2005:reader/item/000000033032b26d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Scalable Log Determinants for Gaussian Process Kernel Learning. (arXiv:1711.03481v1 [stat.ML])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03481"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03481", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3be54c6b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3be54c6b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>For applications as varied as Bayesian neural networks, determinantal point \nprocesses, elliptical graphical models, and kernel learning for Gaussian \nprocesses (GPs), one must compute a log determinant of an $n \\times n$ positive \ndefinite matrix, and its derivatives - leading to prohibitive \n$\\mathcal{O}(n^3)$ computations. We propose novel $\\mathcal{O}(n)$ approaches \nto estimating these quantities from only fast matrix vector multiplications \n(MVMs). These stochastic approximations are based on Chebyshev, Lanczos, and \nsurrogate models, and converge quickly even for kernel matrices that have \nchallenging spectra. We leverage these approximations to develop a scalable \nGaussian process approach to kernel learning. We find that Lanczos is generally \nsuperior to Chebyshev for kernel learning, and that a surrogate approach can be \nhighly efficient and accurate with popular kernels. \n</p>"}, "author": "Kun Dong, David Eriksson, Hannes Nickisch, David Bindel, Andrew Gordon Wilson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148059", "id": "tag:google.com,2005:reader/item/000000033032b27d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning Multi-Modal Word Representation Grounded in Visual Context. (arXiv:1711.03483v1 [cs.CL])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03483"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03483", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Representing the semantics of words is a long-standing problem for the \nnatural language processing community. Most methods compute word semantics \ngiven their textual context in large corpora. More recently, researchers \nattempted to integrate perceptual and visual features. Most of these works \nconsider the visual appearance of objects to enhance word representations but \nthey ignore the visual environment and context in which objects appear. We \npropose to unify text-based techniques with vision-based techniques by \nsimultaneously leveraging textual and visual context to learn multimodal word \nembeddings. We explore various choices for what can serve as a visual context \nand present an end-to-end method to integrate visual context elements in a \nmultimodal skip-gram model. We provide experiments and extensive analysis of \nthe obtained results. \n</p>"}, "author": "&#xc9;loi Zablocki, Benjamin Piwowarski, Laure Soulier, Patrick Gallinari", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148058", "id": "tag:google.com,2005:reader/item/000000033032b285", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Fast Meta-Learning for Adaptive Hierarchical Classifier Design. (arXiv:1711.03512v1 [cs.LG])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03512"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03512", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a new splitting criterion for a meta-learning approach to \nmulticlass classifier design that adaptively merges the classes into a \ntree-structured hierarchy of increasingly difficult binary classification \nproblems. The classification tree is constructed from empirical estimates of \nthe Henze-Penrose bounds on the pairwise Bayes misclassification rates that \nrank the binary subproblems in terms of difficulty of classification. The \nproposed empirical estimates of the Bayes error rate are computed from the \nminimal spanning tree (MST) of the samples from each pair of classes. Moreover, \na meta-learning technique is presented for quantifying the one-vs-rest Bayes \nerror rate for each individual class from a single MST on the entire dataset. \nExtensive simulations on benchmark datasets show that the proposed hierarchical \nmethod can often be learned much faster than competing methods, while achieving \ncompetitive accuracy. \n</p>"}, "author": "Gerrit J. J. van den Burg, Alfred O. Hero", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023778", "id": "tag:google.com,2005:reader/item/000000033006469d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "An asymptotic analysis of distributed nonparametric methods. (arXiv:1711.03149v1 [math.ST])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03149"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03149", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We investigate and compare the fundamental performance of several distributed \nlearning methods that have been proposed recently. We do this in the context of \na distributed version of the classical signal-in-Gaussian-white-noise model, \nwhich serves as a benchmark model for studying performance in this setting. The \nresults show how the design and tuning of a distributed method can have great \nimpact on convergence rates and validity of uncertainty quantification. \nMoreover, we highlight the difficulty of designing nonparametric distributed \nprocedures that automatically adapt to smoothness. \n</p>"}, "author": "Botond Szabo, Harry van Zanten", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023777", "id": "tag:google.com,2005:reader/item/00000003300646a4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Deep Hyperspherical Learning. (arXiv:1711.03189v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03189"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03189", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolution as inner product has been the founding basis of convolutional \nneural networks (CNNs) and the key to end-to-end visual representation \nlearning. Benefiting from deeper architectures, recent CNNs have demonstrated \nincreasingly strong representation abilities. Despite such improvement, the \nincreased depth and larger parameter space have also led to challenges in \nproperly training a network. In light of such challenges, we propose \nhyperspherical convolution (SphereConv), a novel learning framework that gives \nangular representations on hyperspheres. We introduce SphereNet, deep \nhyperspherical convolution networks that are distinct from conventional inner \nproduct based convolutional networks. In particular, SphereNet adopts \nSphereConv as its basic convolution operator and is supervised by generalized \nangular softmax loss - a natural loss formulation under SphereConv. We show \nthat SphereNet can effectively encode discriminative representation and \nalleviate training difficulty, leading to easier optimization, faster \nconvergence and comparable (even better) classification accuracy over \nconvolutional counterparts. We also provide some theoretical insights for the \nadvantages of learning on hyperspheres. In addition, we introduce the learnable \nSphereConv, i.e., a natural improvement over prefixed SphereConv, and \nSphereNorm, i.e., hyperspherical learning as a normalization method. \nExperiments have verified our conclusions. \n</p>"}, "author": "Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, Le Song", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023776", "id": "tag:google.com,2005:reader/item/00000003300646ae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Creating Credible Models. (arXiv:1711.03190v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03190"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03190", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In many settings, it is important that a model be capable of providing \nreasons for its predictions (i.e., the model must be interpretable). However, \nthe model's reasoning may not conform with well-established knowledge. In such \ncases, while interpretable, the model lacks \\textit{credibility}. In this work, \nwe formally define credibility in the linear setting and focus on techniques \nfor learning models that are both accurate and credible. In particular, we \npropose a regularization penalty, expert yielded estimates (EYE), that \nincorporates expert knowledge about well-known relationships among covariates \nand the outcome of interest. We give both theoretical and empirical results \ncomparing our proposed method to several other regularization techniques. \nAcross a range of settings, experiments on both synthetic and real data show \nthat models learned using the EYE penalty are significantly more credible than \nthose learned using other penalties. Applied to a large-scale patient risk \nstratification task, our proposed technique results in a model whose top \nfeatures overlap significantly with known clinical risk factors, while still \nachieving good predictive performance. \n</p>"}, "author": "Jiaxuan Wang, Jeeheh Oh, Jenna Wiens", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023775", "id": "tag:google.com,2005:reader/item/00000003300646b6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Long-Term Sequential Prediction Using Expert Advice. (arXiv:1711.03194v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03194"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03194", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>For the prediction with expert advice setting, we consider methods to \nconstruct forecasting algorithms that suffer loss not much more than of any \nexpert in the pool. In contrast to the standard approach, we investigate the \ncase of long-term interval forecasting of time series, that is, each expert \nissues a sequence of forecasts for a time interval ahead and the master \nalgorithm combines these forecasts into one aggregated sequence of forecasts. \nTwo new approaches for aggregating experts long-term interval predictions are \npresented. One is based on Vovk's aggregation algorithm and considers sliding \nexperts, the other applies the approach of Mixing Past Posteriors method to the \nlong-term prediction. The upper bounds for regret of these algorithms for \nadversarial case are obtained. We also present results of numerical experiments \nof time series long-term prediction. \n</p>"}, "author": "Eugeny Burnaev, Alexander Korotin, Vladimir V&#x27;yugin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023774", "id": "tag:google.com,2005:reader/item/00000003300646c8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Information Directed Sampling for Stochastic Bandits with Graph Feedback. (arXiv:1711.03198v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03198"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03198", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider stochastic multi-armed bandit problems with graph feedback, where \nthe decision maker is allowed to observe the neighboring actions of the chosen \naction. We allow the graph structure to vary with time and consider both \ndeterministic and Erd\\H{o}s-R\\'enyi random graph models. For such a graph \nfeedback model, we first present a novel analysis of Thompson sampling that \nleads to tighter performance bound than existing work. Next, we propose new \nInformation Directed Sampling based policies that are graph-aware in their \ndecision making. Under the deterministic graph case, we establish a Bayesian \nregret bound for the proposed policies that scales with the clique cover number \nof the graph instead of the number of actions. Under the random graph case, we \nprovide a Bayesian regret bound for the proposed policies that scales with the \nratio of the number of actions over the expected number of observations per \niteration. To the best of our knowledge, this is the first analytical result \nfor stochastic bandits with random graph feedback. Finally, using numerical \nevaluations, we demonstrate that our proposed IDS policies outperform existing \napproaches, including adaptions of upper confidence bound, $\\epsilon$-greedy \nand Exp3 algorithms. \n</p>"}, "author": "Fang Liu, Swapna Buccapatnam, Ness Shroff", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023773", "id": "tag:google.com,2005:reader/item/00000003300646cf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Crafting Adversarial Examples For Speech Paralinguistics Applications. (arXiv:1711.03280v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03280"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03280", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Computational paralinguistic analysis is increasingly being used in a wide \nrange of applications, including security-sensitive applications such as \nspeaker verification, deceptive speech detection, and medical diagnosis. While \nstate-of-the-art machine learning techniques, such as deep neural networks, can \nprovide robust and accurate speech analysis, they are susceptible to \nadversarial attacks. In this work, we propose a novel end-to-end scheme to \ngenerate adversarial examples by perturbing directly the raw waveform of an \naudio recording rather than specific acoustic features. Our experiments show \nthat the proposed adversarial perturbation can lead to a significant \nperformance drop of state-of-the-art deep neural networks, while only minimally \nimpairing the audio quality. \n</p>"}, "author": "Yuan Gong, Christian Poellabauer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023772", "id": "tag:google.com,2005:reader/item/00000003300646da", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "A Separation Principle for Control in the Age of Deep Learning. (arXiv:1711.03321v1 [stat.ML])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03321"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03321", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We review the problem of defining and inferring a \"state\" for a control \nsystem based on complex, high-dimensional, highly uncertain measurement streams \nsuch as videos. Such a state, or representation, should contain all and only \nthe information needed for control, and discount nuisance variability in the \ndata. It should also have finite complexity, ideally modulated depending on \navailable resources. This representation is what we want to store in memory in \nlieu of the data, as it \"separates\" the control task from the measurement \nprocess. For the trivial case with no dynamics, a representation can be \ninferred by minimizing the Information Bottleneck Lagrangian in a function \nclass realized by deep neural networks. The resulting representation has much \nhigher dimension than the data, already in the millions, but it is smaller in \nthe sense of information content, retaining only what is needed for the task. \nThis process also yields representations that are invariant to nuisance factors \nand having maximally independent components. We extend these ideas to the \ndynamic case, where the representation is the posterior density of the task \nvariable given the measurements up to the current time, which is in general \nmuch simpler than the prediction density maintained by the classical Bayesian \nfilter. Again this can be finitely-parametrized using a deep neural network, \nand already some applications are beginning to emerge. No explicit assumption \nof Markovianity is needed; instead, complexity trades off approximation of an \noptimal representation, including the degree of Markovianity. \n</p>"}, "author": "Alessandro Achille, Stefano Soatto", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023771", "id": "tag:google.com,2005:reader/item/00000003300646e0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Analysis of Dropout in Online Learning. (arXiv:1711.03343v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03343"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03343", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3be55034\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3be55034&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep learning is the state-of-the-art in fields such as visual object \nrecognition and speech recognition. This learning uses a large number of layers \nand a huge number of units and connections. Therefore, overfitting is a serious \nproblem with it, and the dropout which is a kind of regularization tool is \nused. However, in online learning, the effect of dropout is not well known. \nThis paper presents our investigation on the effect of dropout in online \nlearning. We analyzed the effect of dropout on convergence speed near the \nsingular point. Our results indicated that dropout is effective in online \nlearning. Dropout tends to avoid the singular point for convergence speed near \nthat point. \n</p>"}, "author": "Kazuyuki Hara", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023770", "id": "tag:google.com,2005:reader/item/00000003300646e6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Dimension Reduction of High-Dimensional Datasets Based on Stepwise SVM. (arXiv:1711.03346v1 [stat.AP])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03346"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03346", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3bec6016\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3bec6016&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The current study proposes a dimension reduction method, stepwise support \nvector machine (SVM), to reduce the dimensions of large p small n datasets. The \nproposed method is compared with other dimension reduction methods, namely, the \nPearson product difference correlation coefficient (PCCs), recursive feature \nelimination based on random forest (RF-RFE), and principal component analysis \n(PCA), by using five gene expression datasets. Additionally, the prediction \nperformance of the variables selected by our method is evaluated. The study \nfound that stepwise SVM can effectively select the important variables and \nachieve good prediction performance. Moreover, the predictions of stepwise SVM \nfor reduced datasets was better than those for the unreduced datasets. The \nperformance of stepwise SVM was more stable than that of PCA and RF-RFE, but \nthe performance difference with respect to PCCs was minimal. It is necessary to \nreduce the dimensions of large p small n datasets. We believe that stepwise SVM \ncan effectively eliminate noise in data and improve the prediction accuracy in \nany large p small n dataset. \n</p>"}, "author": "Elizabeth P. Chou, Tzu-Wei Ko", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023769", "id": "tag:google.com,2005:reader/item/00000003300646f3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Multi-Relevance Transfer Learning. (arXiv:1711.03361v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03361"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03361", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Transfer learning aims to faciliate learning tasks in a label-scarce target \ndomain by leveraging knowledge from a related source domain with plenty of \nlabeled data. Often times we may have multiple domains with little or no \nlabeled data as targets waiting to be solved. Most existing efforts tackle \ntarget domains separately by modeling the `source-target' pairs without \nexploring the relatedness between them, which would cause loss of crucial \ninformation, thus failing to achieve optimal capability of knowledge transfer. \nIn this paper, we propose a novel and effective approach called Multi-Relevance \nTransfer Learning (MRTL) for this purpose, which can simultaneously transfer \ndifferent knowledge from the source and exploits the shared common latent \nfactors between target domains. Specifically, we formulate the problem as an \noptimization task based on a collective nonnegative matrix tri-factorization \nframework. The proposed approach achieves both source-target transfer and \ntarget-target leveraging by sharing multiple decomposed latent subspaces. \nFurther, an alternative minimization learning algorithm is developed with \nconvergence guarantee. Empirical study validates the performance and \neffectiveness of MRTL compared to the state-of-the-art methods. \n</p>"}, "author": "Tianchun Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023768", "id": "tag:google.com,2005:reader/item/00000003300646fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "A random matrix analysis and improvement of semi-supervised learning for large dimensional data. (arXiv:1711.03404v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03404"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03404", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This article provides an original understanding of the behavior of a class of \ngraph-oriented semi-supervised learning algorithms in the limit of large and \nnumerous data. It is demonstrated that the intuition at the root of these \nmethods collapses in this limit and that, as a result, most of them become \ninconsistent. Corrective measures and a new data-driven parametrization scheme \nare proposed along with a theoretical analysis of the asymptotic performances \nof the resulting approach. A surprisingly close behavior between theoretical \nperformances on Gaussian mixture models and on real datasets is also \nillustrated throughout the article, thereby suggesting the importance of the \nproposed analysis for dealing with practical data. As a result, significant \nperformance gains are observed on practical data classification using the \nproposed parametrization. \n</p>"}, "author": "Xiaoyi Mai, Romain Couillet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023767", "id": "tag:google.com,2005:reader/item/0000000330064707", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Using Phone Sensors and an Artificial Neural Network to Detect Gait Changes During Drinking Episodes in the Natural Environment. (arXiv:1711.03410v1 [cs.CY])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03410"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03410", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Phone sensors could be useful in assessing changes in gait that occur with \nalcohol consumption. This study determined (1) feasibility of collecting \ngait-related data during drinking occasions in the natural environment, and (2) \nhow gait-related features measured by phone sensors relate to estimated blood \nalcohol concentration (eBAC). Ten young adult heavy drinkers were prompted to \ncomplete a 5-step gait task every hour from 8pm to 12am over four consecutive \nweekends. We collected 3-xis accelerometer, gyroscope, and magnetometer data \nfrom phone sensors, and computed 24 gait-related features using a sliding \nwindow technique. eBAC levels were calculated at each time point based on \nEcological Momentary Assessment (EMA) of alcohol use. We used an artificial \nneural network model to analyze associations between sensor features and eBACs \nin training (70% of the data) and validation and test (30% of the data) \ndatasets. We analyzed 128 data points where both eBAC and gait-related sensor \ndata was captured, either when not drinking (n=60), while eBAC was ascending \n(n=55) or eBAC was descending (n=13). 21 data points were captured at times \nwhen the eBAC was greater than the legal limit (0.08 mg/dl). Using a Bayesian \nregularized neural network, gait-related phone sensor features showed a high \ncorrelation with eBAC (Pearson's r &gt; 0.9), and &gt;95% of estimated eBAC would \nfall between -0.012 and +0.012 of actual eBAC. It is feasible to collect \ngait-related data from smartphone sensors during drinking occasions in the \nnatural environment. Sensor-based features can be used to infer gait changes \nassociated with elevated blood alcohol content. \n</p>"}, "author": "Brian Suffoletto, Pedram Gharani, Tammy Chung, Hassan Karimi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023766", "id": "tag:google.com,2005:reader/item/0000000330064711", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Can clustering scale sublinearly with its clusters? A variational EM acceleration of GMMs and $k$-means. (arXiv:1711.03431v1 [stat.ML])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03431"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03431", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>One iteration of $k$-means or EM for Gaussian mixture models (GMMs) scales \nlinearly with the number of data points $N$, the number of clusters $C$, and \nthe data dimensionality $D$. In this study, we explore whether one iteration of \n$k$-means or EM for GMMs can scale sublinearly with $C$ at run-time, while the \nincrease of the clustering objective remains effective. The tool we apply for \ncomplexity reduction is variational EM, which is typically applied to make \ntraining of generative models with exponentially many hidden states tractable. \nHere, we apply novel theoretical results on truncated variational EM to make \ntractable clustering algorithms more efficient. The basic idea is the use of a \npartial variational E-step which reduces the linear complexity of \n$\\mathcal{O}(NCD)$ required for a full E-step to a sublinear complexity. Our \nmain observation is that the linear dependency on $C$ can be reduced to a \ndependency on a much smaller parameter $G$, related to the cluster neighborhood \nrelationship. We focus on two versions of partial variational EM for \nclustering: variational GMM, scaling with $\\mathcal{O}(NG^2D)$, and variational \n$k$-means, scaling with $\\mathcal{O}(NGD)$ per iteration. Empirical results \nthen show that these algorithms still require comparable numbers of iterations \nto increase the clustering objective to the same values as $k$-means. For data \nwith many clusters, we consequently observe reductions of the net computational \ndemands between two and three orders of magnitude. More generally, our results \nprovide substantial empirical evidence in favor of clustering to scale \nsublinearly with $C$. \n</p>"}, "author": "Dennis Forster, J&#xf6;rg L&#xfc;cke", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023765", "id": "tag:google.com,2005:reader/item/0000000330064724", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex Optimization. (arXiv:1711.03439v1 [math.OC])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03439"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03439", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a new randomized coordinate descent method for a convex \noptimization template with broad applications. Our analysis relies on a novel \ncombination of four ideas applied to the primal-dual gap function: smoothing, \nacceleration, homotopy, and coordinate descent with non-uniform sampling. As a \nresult, our method features the first convergence rate guarantees among the \ncoordinate descent methods, that are the best-known under a variety of common \nstructure assumptions on the template. We provide numerical evidence to support \nthe theoretical results with a comparison to state-of-the-art algorithms. \n</p>"}, "author": "Ahmet Alacaoglu, Quoc Tran-Dinh, Olivier Fercoq, Volkan Cevher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023764", "id": "tag:google.com,2005:reader/item/0000000330064733", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning Non-overlapping Convolutional Neural Networks with Multiple Kernels. (arXiv:1711.03440v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03440"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03440", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we consider parameter recovery for non-overlapping \nconvolutional neural networks (CNNs) with multiple kernels. We show that when \nthe inputs follow Gaussian distribution and the sample size is sufficiently \nlarge, the squared loss of such CNNs is $\\mathit{~locally~strongly~convex}$ in \na basin of attraction near the global optima for most popular activation \nfunctions, like ReLU, Leaky ReLU, Squared ReLU, Sigmoid and Tanh. The required \nsample complexity is proportional to the dimension of the input and polynomial \nin the number of kernels and a condition number of the parameters. We also show \nthat tensor methods are able to initialize the parameters to the local strong \nconvex region. Hence, for most smooth activations, gradient descent following \ntensor initialization is guaranteed to converge to the global optimal with time \nthat is linear in input dimension, logarithmic in precision and polynomial in \nother factors. To the best of our knowledge, this is the first work that \nprovides recovery guarantees for CNNs with multiple kernels under polynomial \nsample and computational complexities. \n</p>"}, "author": "Kai Zhong, Zhao Song, Inderjit S. Dhillon", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023763", "id": "tag:google.com,2005:reader/item/000000033006473f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Scalable Log Determinants for Gaussian Process Kernel Learning. (arXiv:1711.03481v1 [stat.ML])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03481"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03481", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>For applications as varied as Bayesian neural networks, determinantal point \nprocesses, elliptical graphical models, and kernel learning for Gaussian \nprocesses (GPs), one must compute a log determinant of an $n \\times n$ positive \ndefinite matrix, and its derivatives - leading to prohibitive \n$\\mathcal{O}(n^3)$ computations. We propose novel $\\mathcal{O}(n)$ approaches \nto estimating these quantities from only fast matrix vector multiplications \n(MVMs). These stochastic approximations are based on Chebyshev, Lanczos, and \nsurrogate models, and converge quickly even for kernel matrices that have \nchallenging spectra. We leverage these approximations to develop a scalable \nGaussian process approach to kernel learning. We find that Lanczos is generally \nsuperior to Chebyshev for kernel learning, and that a surrogate approach can be \nhighly efficient and accurate with popular kernels. \n</p>"}, "author": "Kun Dong, David Eriksson, Hannes Nickisch, David Bindel, Andrew Gordon Wilson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023762", "id": "tag:google.com,2005:reader/item/000000033006474e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Fast Meta-Learning for Adaptive Hierarchical Classifier Design. (arXiv:1711.03512v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03512"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03512", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a new splitting criterion for a meta-learning approach to \nmulticlass classifier design that adaptively merges the classes into a \ntree-structured hierarchy of increasingly difficult binary classification \nproblems. The classification tree is constructed from empirical estimates of \nthe Henze-Penrose bounds on the pairwise Bayes misclassification rates that \nrank the binary subproblems in terms of difficulty of classification. The \nproposed empirical estimates of the Bayes error rate are computed from the \nminimal spanning tree (MST) of the samples from each pair of classes. Moreover, \na meta-learning technique is presented for quantifying the one-vs-rest Bayes \nerror rate for each individual class from a single MST on the entire dataset. \nExtensive simulations on benchmark datasets show that the proposed hierarchical \nmethod can often be learned much faster than competing methods, while achieving \ncompetitive accuracy. \n</p>"}, "author": "Gerrit J. J. van den Burg, Alfred O. Hero", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023745", "id": "tag:google.com,2005:reader/item/00000003300647e4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Finding Heavily-Weighted Features in Data Streams. (arXiv:1711.02305v1 [cs.LG] CROSS LISTED)", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02305"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02305", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3bec6259\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3bec6259&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We introduce a new sub-linear space data structure---the Weight-Median \nSketch---that captures the most heavily weighted features in linear classifiers \ntrained over data streams. This enables memory-limited execution of several \nstatistical analyses over streams, including online feature selection, \nstreaming data explanation, relative deltoid detection, and streaming \nestimation of pointwise mutual information. In contrast with related sketches \nthat capture the most commonly occurring features (or items) in a data stream, \nthe Weight-Median Sketch captures the features that are most discriminative of \none stream (or class) compared to another. The Weight-Median sketch adopts the \ncore data structure used in the Count-Sketch, but, instead of sketching counts, \nit captures sketched gradient updates to the model parameters. We provide a \ntheoretical analysis of this approach that establishes recovery guarantees in \nthe online learning setting, and demonstrate substantial empirical improvements \nin accuracy-memory trade-offs over alternatives, including count-based sketches \nand feature hashing. \n</p>"}, "author": "Kai Sheng Tai, Vatsal Sharan, Peter Bailis, Gregory Valiant", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510280106271", "timestampUsec": "1510280106270932", "id": "tag:google.com,2005:reader/item/000000032fb7308b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Deep Learning for Real-time Gravitational Wave Detection and Parameter Estimation: Results with Advanced LIGO Data. (arXiv:1711.03121v1 [gr-qc])", "published": 1510280107, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03121"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03121", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The recent Nobel-prize-winning detections of gravitational waves from merging \nblack holes and the subsequent detection of the collision of two neutron stars \nin coincidence with electromagnetic observations have inaugurated a new era of \nmultimessenger astrophysics. To enhance the scope of this emergent field of \nscience, we pioneered the use of deep learning with convolutional neural \nnetworks, that take time-series inputs, for rapid detection and \ncharacterization of gravitational wave signals. This approach, Deep Filtering, \nwas initially demonstrated using simulated LIGO noise. In this article, we \npresent the extension of Deep Filtering using real data from LIGO, for both \ndetection and parameter estimation of gravitational waves from binary black \nhole mergers using continuous data streams from multiple LIGO detectors. We \ndemonstrate for the first time that machine learning can detect and estimate \nthe true parameters of real events observed by LIGO. Our results show that Deep \nFiltering achieves similar sensitivities and lower errors compared to \nmatched-filtering while being far more computationally efficient and more \nresilient to glitches, allowing real-time processing of weak time-series \nsignals in non-stationary non-Gaussian noise with minimal resources, and also \nenables the detection of new classes of gravitational wave sources that may go \nunnoticed with existing detection algorithms. This unified framework for data \nanalysis is ideally suited to enable coincident detection campaigns of \ngravitational waves and their multimessenger counterparts in real-time. \n</p>"}, "author": "Daniel George, E. A. Huerta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510280106271", "timestampUsec": "1510280106270931", "id": "tag:google.com,2005:reader/item/000000032fb7308f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "MarrNet: 3D Shape Reconstruction via 2.5D Sketches. (arXiv:1711.03129v1 [cs.CV])", "published": 1510280107, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03129"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03129", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>3D object reconstruction from a single image is a highly under-determined \nproblem, requiring strong prior knowledge of plausible 3D shapes. This \nintroduces challenges for learning-based approaches, as 3D object annotations \nare scarce in real images. Previous work chose to train on synthetic data with \nground truth 3D information, but suffered from domain adaptation when tested on \nreal data. In this work, we propose MarrNet, an end-to-end trainable model that \nsequentially estimates 2.5D sketches and 3D object shape. Our disentangled, \ntwo-step formulation has three advantages. First, compared to full 3D shape, \n2.5D sketches are much easier to be recovered from a 2D image; models that \nrecover 2.5D sketches are also more likely to transfer from synthetic to real \ndata. Second, for 3D reconstruction from 2.5D sketches, systems can learn \npurely from synthetic data. This is because we can easily render realistic 2.5D \nsketches without modeling object appearance variations in real images, \nincluding lighting, texture, etc. This further relieves the domain adaptation \nproblem. Third, we derive differentiable projective functions from 3D shape to \n2.5D sketches; the framework is therefore end-to-end trainable on real images, \nrequiring no human annotations. Our model achieves state-of-the-art performance \non 3D shape reconstruction. \n</p>"}, "author": "Jiajun Wu, Yifan Wang, Tianfan Xue, Xingyuan Sun, William T Freeman, Joshua B Tenenbaum", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510280106271", "timestampUsec": "1510280106270930", "id": "tag:google.com,2005:reader/item/000000032fb7309a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Deep D-bar: Real time Electrical Impedance Tomography Imaging with Deep Neural Networks. (arXiv:1711.03180v1 [math.NA])", "published": 1510280107, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03180"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03180", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The mathematical problem for Electrical Impedance Tomography (EIT) is a \nhighly nonlinear ill-posed inverse problem requiring carefully designed \nreconstruction procedures to ensure reliable image generation. D-bar methods \nare based on a rigorous mathematical analysis and provide robust direct \nreconstructions by using a low-pass filtering of the associated nonlinear \nFourier data. Similarly to low-pass filtering of linear Fourier data, only \nusing low frequencies in the image recovery process results in blurred images \nlacking sharp features such as clear organ boundaries. Convolutional Neural \nNetworks provide a powerful framework for post-processing such convolved direct \nreconstructions. In this study, we demonstrate that these CNN techniques lead \nto sharp and reliable reconstructions even for the highly nonlinear inverse \nproblem of EIT. The network is trained on data sets of simulated examples and \nthen applied to experimental data without the need to perform an additional \ntransfer training. Results are presented on experimental EIT data from the ACT4 \nand KIT4 EIT systems. \n</p>"}, "author": "Sarah Jane Hamilton, Andreas Hauptmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510280106271", "timestampUsec": "1510280106270929", "id": "tag:google.com,2005:reader/item/000000032fb7309e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz. (arXiv:1711.03357v1 [cs.NE])", "published": 1510280107, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03357"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03357", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The goal of this paper is to demonstrate a method for tensorizing neural \nnetworks based upon an efficient way of approximating scale invariant quantum \nstates, the Multi-scale Entanglement Renormalization Ansatz (MERA). We employ \nMERA as a replacement for linear layers in a neural network and test this \nimplementation on the CIFAR-10 dataset. The proposed method outperforms \nfactorization using tensor trains, providing greater compression for the same \nlevel of accuracy and greater accuracy for the same level of compression. We \ndemonstrate MERA-layers with 3900 times fewer parameters and a reduction in \naccuracy of less than 1% compared to the equivalent fully connected layers. \n</p>"}, "author": "Andrew Hallam, Edward Grant, Vid Stojevic, Simone Severini, Andrew G. Green", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510280106271", "timestampUsec": "1510280106270928", "id": "tag:google.com,2005:reader/item/000000032fb730a6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "A Further Analysis of The Role of Heterogeneity in Coevolutionary Spatial Games. (arXiv:1711.03417v1 [physics.soc-ph])", "published": 1510280107, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03417"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03417", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Heterogeneity has been studied as one of the most common explanations of the \npuzzle of cooperation in social dilemmas. A large number of papers have been \npublished discussing the effects of increasing heterogeneity in structured \npopulations of agents, where it has been established that heterogeneity may \nfavour cooperative behaviour if it supports agents to locally coordinate their \nstrategies. In this paper, assuming an existing model of a heterogeneous \nweighted network, we aim to further this analysis by exploring the relationship \n(if any) between heterogeneity and cooperation. We adopt a weighted network \nwhich is fully populated by agents playing both the Prisoner's Dilemma or the \nOptional Prisoner's Dilemma games with coevolutionary rules, i.e., not only the \nstrategies but also the link weights evolve over time. Surprisingly, results \nshow that the heterogeneity of link weights (states) on their own does not \nalways promote cooperation; rather cooperation is actually favoured by the \nincrease in the number of overlapping states and not by the heterogeneity \nitself. We believe that these results can guide further research towards a more \naccurate analysis of the role of heterogeneity in social dilemmas. \n</p>"}, "author": "Marcos Cardinot, Josephine Griffith, Colm O&#x27;Riordan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510280106271", "timestampUsec": "1510280106270927", "id": "tag:google.com,2005:reader/item/000000032fb730ae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Worm-level Control through Search-based Reinforcement Learning. (arXiv:1711.03467v1 [cs.NE])", "published": 1510280107, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03467"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03467", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Through natural evolution, nervous systems of organisms formed near-optimal \nstructures to express behavior. Here, we propose an effective way to create \ncontrol agents, by \\textit{re-purposing} the function of biological neural \ncircuit models, to govern similar real world applications. We model the \ntap-withdrawal (TW) neural circuit of the nematode, \\textit{C. elegans}, a \ncircuit responsible for the worm's reflexive response to external mechanical \ntouch stimulations, and learn its synaptic and neural parameters as a policy \nfor controlling the inverted pendulum problem. For reconfiguration of the \npurpose of the TW neural circuit, we manipulate a search-based reinforcement \nlearning. We show that our neural policy performs as good as existing \ntraditional control theory and machine learning approaches. A video \ndemonstration of the performance of our method can be accessed at \n\\url{https://youtu.be/o-Ia5IVyff8}. \n</p>"}, "author": "Mathias Lechner, Radu Grosu, Ramin M. Hasani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605507", "id": "tag:google.com,2005:reader/item/000000032f758eca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Recurrent Autoregressive Networks for Online Multi-Object Tracking. (arXiv:1711.02741v1 [cs.CV])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02741"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02741", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The main challenge of online multi-object tracking is to reliably associate \nobject trajectories with detections in each video frame based on their tracking \nhistory. In this work, we propose the Recurrent Autoregressive Network (RAN), a \ntemporal generative modeling framework to characterize the appearance and \nmotion dynamics of multiple objects over time. The RAN couples an external \nmemory and an internal memory. The external memory explicitly stores previous \ninputs of each trajectory in a time window, while the internal memory learns to \nsummarize long-term tracking history and associate detections by processing the \nexternal memory. We conduct experiments on the MOT 2015 and 2016 datasets to \ndemonstrate the robustness of our tracking method in highly crowded and \noccluded scenes. Our method achieves top-ranked results on the two benchmarks. \n</p>"}, "author": "Kuan Fang, Yu Xiang, Silvio Savarese", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605506", "id": "tag:google.com,2005:reader/item/000000032f758ee9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Block-Sparse Recurrent Neural Networks. (arXiv:1711.02782v1 [cs.LG])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02782"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02782", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent Neural Networks (RNNs) are used in state-of-the-art models in \ndomains such as speech recognition, machine translation, and language \nmodelling. Sparsity is a technique to reduce compute and memory requirements of \ndeep learning models. Sparse RNNs are easier to deploy on devices and high-end \nserver processors. Even though sparse operations need less compute and memory \nrelative to their dense counterparts, the speed-up observed by using sparse \noperations is less than expected on different hardware platforms. In order to \naddress this issue, we investigate two different approaches to induce block \nsparsity in RNNs: pruning blocks of weights in a layer and using group lasso \nregularization to create blocks of weights with zeros. Using these techniques, \nwe demonstrate that we can create block-sparse RNNs with sparsity ranging from \n80% to 90% with small loss in accuracy. This allows us to reduce the model size \nby roughly 10x. Additionally, we can prune a larger dense network to recover \nthis loss in accuracy while maintaining high block sparsity and reducing the \noverall parameter count. Our technique works with a variety of block sizes up \nto 32x32. Block-sparse RNNs eliminate overheads related to data storage and \nirregular memory accesses while increasing hardware efficiency compared to \nunstructured sparsity. \n</p>"}, "author": "Sharan Narang, Eric Undersander, Gregory Diamos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605505", "id": "tag:google.com,2005:reader/item/000000032f758ef9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Faster Fuzzing: Reinitialization with Deep Neural Models. (arXiv:1711.02807v1 [cs.AI])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02807"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02807", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We improve the performance of the American Fuzzy Lop (AFL) fuzz testing \nframework by using Generative Adversarial Network (GAN) models to reinitialize \nthe system with novel seed files. We assess performance based on the temporal \nrate at which we produce novel and unseen code paths. We compare this approach \nto seed file generation from a random draw of bytes observed in the training \nseed files. The code path lengths and variations were not sufficiently diverse \nto fully replace AFL input generation. However, augmenting native AFL with \nthese additional code paths demonstrated improvements over AFL alone. \nSpecifically, experiments showed the GAN was faster and more effective than the \nLSTM and out-performed a random augmentation strategy, as measured by the \nnumber of unique code paths discovered. GAN helps AFL discover 14.23% more code \npaths than the random strategy in the same amount of CPU time, finds 6.16% more \nunique code paths, and finds paths that are on average 13.84% longer. Using GAN \nshows promise as a reinitialization strategy for AFL to help the fuzzer \nexercise deep paths in software. \n</p>"}, "author": "Nicole Nichols, Mark Raugas, Robert Jasper, Nathan Hilliard", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605504", "id": "tag:google.com,2005:reader/item/000000032f758f12", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Inverse Reward Design. (arXiv:1711.02827v1 [cs.AI])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02827"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02827", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3bec64a3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3bec64a3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Autonomous agents optimize the reward function we give them. What they don't \nknow is how hard it is for us to design a reward function that actually \ncaptures what we want. When designing the reward, we might think of some \nspecific training scenarios, and make sure that the reward will lead to the \nright behavior in those scenarios. Inevitably, agents encounter new scenarios \n(e.g., new types of terrain) where optimizing that same reward may lead to \nundesired behavior. Our insight is that reward functions are merely \nobservations about what the designer actually wants, and that they should be \ninterpreted in the context in which they were designed. We introduce inverse \nreward design (IRD) as the problem of inferring the true objective based on the \ndesigned reward and the training MDP. We introduce approximate methods for \nsolving IRD problems, and use their solution to plan risk-averse behavior in \ntest MDPs. Empirical results suggest that this approach can help alleviate \nnegative side effects of misspecified reward functions and mitigate reward \nhacking. \n</p>"}, "author": "Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell, Anca Dragan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605503", "id": "tag:google.com,2005:reader/item/000000032f758f2b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "SIMILARnet: Simultaneous Intelligent Localization and Recognition Network. (arXiv:1711.02831v1 [cs.CV])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02831"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02831", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3bf50acf\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3bf50acf&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Global Average Pooling (GAP) [4] has been used previously to generate class \nactivation for image classification tasks. The motivation behind SIMILARnet \ncomes from the fact that the convolutional filters possess position information \nof the essential features and hence, combination of the feature maps could help \nus locate the class instances in an image. We propose a biologically inspired \nmodel that is free of differential connections and doesn't require separate \ntraining thereby reducing computation overhead. Our novel architecture \ngenerates promising results and unlike existing methods, the model is not \nsensitive to the input image size, thus promising wider application. Codes for \nthe experiment and illustrations can be found at: \nhttps://github.com/brcsomnath/Advanced-GAP . \n</p>"}, "author": "Arna Ghosh, Biswarup Bhattacharya, Somnath Basu Roy Chowdhury", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605502", "id": "tag:google.com,2005:reader/item/000000032f758f3f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning Sparse Visual Representations with Leaky Capped Norm Regularizers. (arXiv:1711.02857v1 [cs.LG])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02857"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02857", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Sparsity inducing regularization is an important part for learning \nover-complete visual representations. Despite the popularity of $\\ell_1$ \nregularization, in this paper, we investigate the usage of non-convex \nregularizations in this problem. Our contribution consists of three parts. \nFirst, we propose the leaky capped norm regularization (LCNR), which allows \nmodel weights below a certain threshold to be regularized more strongly as \nopposed to those above, therefore imposes strong sparsity and only introduces \ncontrollable estimation bias. We propose a majorization-minimization algorithm \nto optimize the joint objective function. Second, our study over monocular 3D \nshape recovery and neural networks with LCNR outperforms $\\ell_1$ and other \nnon-convex regularizations, achieving state-of-the-art performance and faster \nconvergence. Third, we prove a theoretical global convergence speed on the 3D \nrecovery problem. To the best of our knowledge, this is the first convergence \nanalysis of the 3D recovery problem. \n</p>"}, "author": "Jianqiao Wangni, Dahua Lin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605501", "id": "tag:google.com,2005:reader/item/000000032f758f4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Un r\\'esultat intrigant en commande sans mod\\`ele. (arXiv:1711.02877v1 [cs.SY])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02877"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02877", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>An elementary mathematical example proves, thanks to the Routh-Hurwitz \ncriterion, a result that is intriguing with respect to today's practical \nunderstanding of model-free control, i.e., an \"intelligent\" proportional \ncontroller (iP) may turn to be more difficult to tune than an intelligent \nproportional-derivative one (iPD). The vast superiority of iPDs when compared \nto classic PIDs is shown via computer simulations. The introduction as well as \nthe conclusion analyse model-free control in the light of recent advances. \n</p>"}, "author": "C&#xe9;dric Join, Emmanuel Delaleau, Michel Fliess, Claude H. Moog", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605500", "id": "tag:google.com,2005:reader/item/000000032f758f5f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Clustering with feature selection using alternating minimization, Application to computational biology. (arXiv:1711.02974v1 [cs.LG])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02974"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02974", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper deals with unsupervised clustering with feature selection. The \nproblem is to estimate both labels and a sparse projection matrix of weights. \nTo address this combinatorial non-convex problem maintaining a strict control \non the sparsity of the matrix of weights, we propose an alternating \nminimization of the Frobenius norm criterion. We provide a new efficient \nalgorithm named K-sparse which alternates k-means with projection-gradient \nminimization. The projection-gradient step is a method of splitting type, with \nexact projection on the $\\ell^1$ ball to promote sparsity. The convergence of \nthe gradient-projection step is addressed, and a preliminary analysis of the \nalternating minimization is made. The Frobenius norm criterion converges as the \nnumber of iterates in Algorithm K-sparse goes to infinity. Experiments on \nSingle Cell RNA sequencing datasets show that our method significantly improves \nthe results of PCA k-means, spectral clustering, SIMLR, and Sparcl methods, and \nachieves a relevant selection of genes. The complexity of K-sparse is linear in \nthe number of samples (cells), so that the method scales up to large datasets. \n</p>"}, "author": "Cyprien Gilet, Marie Deprez, Jean-Baptiste Caillau, Michel Barlaud", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605499", "id": "tag:google.com,2005:reader/item/000000032f758f67", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Intelligent Fault Analysis in Electrical Power Grids. (arXiv:1711.03026v1 [cs.CE])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03026"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03026", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Power grids are one of the most important components of infrastructure in \ntoday's world. Every nation is dependent on the security and stability of its \nown power grid to provide electricity to the households and industries. A \nmalfunction of even a small part of a power grid can cause loss of \nproductivity, revenue and in some cases even life. Thus, it is imperative to \ndesign a system which can detect the health of the power grid and take \nprotective measures accordingly even before a serious anomaly takes place. To \nachieve this objective, we have set out to create an artificially intelligent \nsystem which can analyze the grid information at any given time and determine \nthe health of the grid through the usage of sophisticated formal models and \nnovel machine learning techniques like recurrent neural networks. Our system \nsimulates grid conditions including stimuli like faults, generator output \nfluctuations, load fluctuations using Siemens PSS/E software and this data is \ntrained using various classifiers like SVM, LSTM and subsequently tested. The \nresults are excellent with our methods giving very high accuracy for the data. \nThis model can easily be scaled to handle larger and more complex grid \narchitectures. \n</p>"}, "author": "Biswarup Bhattacharya, Abhishek Sinha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605498", "id": "tag:google.com,2005:reader/item/000000032f758f70", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning K-way D-dimensional Discrete Code For Compact Embedding Representations. (arXiv:1711.03067v1 [cs.LG])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03067"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03067", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Embedding methods such as word embedding have become pillars for many \napplications containing discrete structures. Conventional embedding methods \ndirectly associate each symbol with a continuous embedding vector, which is \nequivalent to applying linear transformation based on \"one-hot\" encoding of the \ndiscrete symbols. Despite its simplicity, such approach yields number of \nparameters that grows linearly with the vocabulary size and can lead to \noverfitting. In this work we propose a much more compact K-way D-dimensional \ndiscrete encoding scheme to replace the \"one-hot\" encoding. In \"KD encoding\", \neach symbol is represented by a $D$-dimensional code, and each of its dimension \nhas a cardinality of $K$. The final symbol embedding vector can be generated by \ncomposing the code embedding vectors. To learn the semantically meaningful \ncode, we derive a relaxed discrete optimization technique based on stochastic \ngradient descent. By adopting the new coding system, the efficiency of \nparameterization can be significantly improved (from linear to logarithmic), \nand this can also mitigate the over-fitting problem. In our experiments with \nlanguage modeling, the number of embedding parameters can be reduced by 97\\% \nwhile achieving similar or better performance. \n</p>"}, "author": "Ting Chen, Martin Renqiang Min, Yizhou Sun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605497", "id": "tag:google.com,2005:reader/item/000000032f758f85", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Exploration in NetHack with Secret Discovery. (arXiv:1711.03087v1 [cs.AI])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03087"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03087", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Roguelike games generally feature exploration problems as a critical, yet \noften repetitive element of gameplay. Automated approaches, however, face \nchallenges in terms of optimality, as well as due to incomplete information, \nsuch as from the presence of secret doors. This paper presents an algorithmic \napproach to exploration of roguelike dungeon environments. Our design aims to \nminimize exploration time, balancing coverage and discovery of secret areas \nwith resource cost. Our algorithm is based on the concept of occupancy maps \npopular in robotics, adapted to encourage efficient discovery of secret access \npoints. Through extensive experimentation on NetHack maps we show that this \ntechnique is significantly more efficient than simpler greedy approaches. We \nfurther investigate optimized parameterization for the algorithm through a \ncomprehensive data analysis. These results point towards better automation for \nplayers as well as heuristics applicable to fully automated gameplay. \n</p>"}, "author": "Jonathan C. Campbell (1), Clark Verbrugge (1) ((1) McGill University)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885321", "id": "tag:google.com,2005:reader/item/000000032f482b5a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Neural Variational Inference and Learning in Undirected Graphical Models. (arXiv:1711.02679v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02679"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02679", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many problems in machine learning are naturally expressed in the language of \nundirected graphical models. Here, we propose black-box learning and inference \nalgorithms for undirected models that optimize a variational approximation to \nthe log-likelihood of the model. Central to our approach is an upper bound on \nthe log-partition function parametrized by a function q that we express as a \nflexible neural network. Our bound makes it possible to track the partition \nfunction during learning, to speed-up sampling, and to train a broad class of \nhybrid directed/undirected models via a unified variational inference \nframework. We empirically demonstrate the effectiveness of our method on \nseveral popular generative modeling datasets. \n</p>"}, "author": "Volodymyr Kuleshov, Stefano Ermon", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885320", "id": "tag:google.com,2005:reader/item/000000032f482b72", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Tangent: Automatic Differentiation Using Source Code Transformation in Python. (arXiv:1711.02712v1 [cs.MS])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02712"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02712", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Automatic differentiation (AD) is an essential primitive for machine learning \nprogramming systems. Tangent is a new library that performs AD using source \ncode transformation (SCT) in Python. It takes numeric functions written in a \nsyntactic subset of Python and NumPy as input, and generates new Python \nfunctions which calculate a derivative. This approach to automatic \ndifferentiation is different from existing packages popular in machine \nlearning, such as TensorFlow and Autograd. Advantages are that Tangent \ngenerates gradient code in Python which is readable by the user, easy to \nunderstand and debug, and has no runtime overhead. Tangent also introduces \nabstractions for easily injecting logic into the generated gradient code, \nfurther improving usability. \n</p>"}, "author": "Bart van Merri&#xeb;nboer, Alexander B. Wiltschko, Dan Moldovan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885319", "id": "tag:google.com,2005:reader/item/000000032f482bbc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "On the Discrimination-Generalization Tradeoff in GANs. (arXiv:1711.02771v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02771"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02771", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3bf50d36\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3bf50d36&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Generative adversarial training can be generally understood as minimizing \ncertain moment matching loss defined by a set of discriminator functions, \ntypically neural networks. The discriminator set should be large enough to be \nable to uniquely identify the true distribution (discriminative), and also be \nsmall enough to go beyond memorizing samples (generalizable). In this paper, we \nshow that a discriminator set is guaranteed to be discriminative whenever its \nlinear span is dense in the set of bounded continuous functions. This is a very \nmild condition satisfied even by neural networks with a single neuron. Further, \nwe develop generalization bounds between the learned distribution and true \ndistribution under different evaluation metrics. When evaluated with neural or \nWasserstein distances, our bounds show that generalization is guaranteed as \nlong as the discriminator set is small enough, regardless of the size of the \ngenerator or hypothesis set. When evaluated with KL divergence, our bound \nprovides an explanation on the counter-intuitive behaviors of testing \nlikelihood in GAN training. Our analysis sheds lights on understanding the \npractical performance of GANs. \n</p>"}, "author": "Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, Xiaodong He", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885318", "id": "tag:google.com,2005:reader/item/000000032f482c21", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Block-Sparse Recurrent Neural Networks. (arXiv:1711.02782v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02782"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02782", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent Neural Networks (RNNs) are used in state-of-the-art models in \ndomains such as speech recognition, machine translation, and language \nmodelling. Sparsity is a technique to reduce compute and memory requirements of \ndeep learning models. Sparse RNNs are easier to deploy on devices and high-end \nserver processors. Even though sparse operations need less compute and memory \nrelative to their dense counterparts, the speed-up observed by using sparse \noperations is less than expected on different hardware platforms. In order to \naddress this issue, we investigate two different approaches to induce block \nsparsity in RNNs: pruning blocks of weights in a layer and using group lasso \nregularization to create blocks of weights with zeros. Using these techniques, \nwe demonstrate that we can create block-sparse RNNs with sparsity ranging from \n80% to 90% with small loss in accuracy. This allows us to reduce the model size \nby roughly 10x. Additionally, we can prune a larger dense network to recover \nthis loss in accuracy while maintaining high block sparsity and reducing the \noverall parameter count. Our technique works with a variety of block sizes up \nto 32x32. Block-sparse RNNs eliminate overheads related to data storage and \nirregular memory accesses while increasing hardware efficiency compared to \nunstructured sparsity. \n</p>"}, "author": "Sharan Narang, Eric Undersander, Gregory Diamos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885317", "id": "tag:google.com,2005:reader/item/000000032f482c6d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Approximate message passing for nonconvex sparse regularization with stability and asymptotic analysis. (arXiv:1711.02795v1 [stat.ML])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02795"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02795", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We analyze linear regression problem with a nonconvex regularization called \nsmoothly clipped absolute deviation (SCAD) under overcomplete Gaussian basis \nfor Gaussian random data. We develop a message passing algorithm SCAD-AMP and \nanalytically show that the stability condition is corresponding to the AT \ncondition in spin glass literature. As asymptotic analysis, we show the \ncorrespondence between density evolution of SCAD-AMP and replica symmetric \nsolution. Numerical experiments confirm that for sufficiently large system \nsize, SCAD-AMP achieves the optimal performance predicted by replica method. \nFrom replica analysis, phase transition between replica symmetric (RS) and \nreplica symmetry breaking (RSB) region is found in the parameter space of SCAD. \nThe appearance of RS region for nonconvex penalty is a great advantage which \nindicate the region of smooth landscape of the optimization problem. \nFurthermore, we analytically show that the statistical representation \nperformance of SCAD penalty is improved compared with $\\ell_1$-based methods, \nand the minimum representation error under RS assumption is obtained at the \nedge of RS/RSB phase. The correspondence between the convergence of the \nexisting coordinate descent algorithm and RS/RSB transition is also indicated. \n</p>"}, "author": "Ayaka Sakata, Yingying Xu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885316", "id": "tag:google.com,2005:reader/item/000000032f482c85", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Deep Fault Analysis and Subset Selection in Solar Power Grids. (arXiv:1711.02810v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02810"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02810", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Non-availability of reliable and sustainable electric power is a major \nproblem in the developing world. Renewable energy sources like solar are not \nvery lucrative in the current stage due to various uncertainties like weather, \nstorage, land use among others. There also exists various other issues like \nmis-commitment of power, absence of intelligent fault analysis, congestion, \netc. In this paper, we propose a novel deep learning-based system for \npredicting faults and selecting power generators optimally so as to reduce \ncosts and ensure higher reliability in solar power systems. The results are \nhighly encouraging and they suggest that the approaches proposed in this paper \nhave the potential to be applied successfully in the developing world. \n</p>"}, "author": "Biswarup Bhattacharya, Abhishek Sinha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885315", "id": "tag:google.com,2005:reader/item/000000032f482cab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Stochastic Cubic Regularization for Fast Nonconvex Optimization. (arXiv:1711.02838v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02838"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02838", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes a stochastic variant of a classic algorithm---the \ncubic-regularized Newton method [Nesterov and Polyak 2006]. The proposed \nalgorithm efficiently escapes saddle points and finds approximate local minima \nfor general smooth, nonconvex functions in only \n$\\mathcal{\\tilde{O}}(\\epsilon^{-3.5})$ stochastic gradient and stochastic \nHessian-vector product evaluations. The latter can be computed as efficiently \nas stochastic gradients. This improves upon the \n$\\mathcal{\\tilde{O}}(\\epsilon^{-4})$ rate of stochastic gradient descent. Our \nrate matches the best-known result for finding local minima without requiring \nany delicate acceleration or variance-reduction techniques. \n</p>"}, "author": "Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, Michael I. Jordan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885314", "id": "tag:google.com,2005:reader/item/000000032f482d5a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Intriguing Properties of Adversarial Examples. (arXiv:1711.02846v1 [stat.ML])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02846"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02846", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is becoming increasingly clear that many machine learning classifiers are \nvulnerable to adversarial examples. In attempting to explain the origin of \nadversarial examples, previous studies have typically focused on the fact that \nneural networks operate on high dimensional data, they overfit, or they are too \nlinear. Here we argue that the origin of adversarial examples is primarily due \nto an inherent uncertainty that neural networks have about their predictions. \nWe show that the functional form of this uncertainty is independent of \narchitecture, dataset, and training protocol; and depends only on the \nstatistics of the logit differences of the network, which do not change \nsignificantly during training. This leads to adversarial error having a \nuniversal scaling, as a power-law, with respect to the size of the adversarial \nperturbation. We show that this universality holds for a broad range of \ndatasets (MNIST, CIFAR10, ImageNet, and random data), models (including \nstate-of-the-art deep networks, linear models, adversarially trained networks, \nand networks trained on randomly shuffled labels), and attacks (FGSM, step \nl.l., PGD). Motivated by these results, we study the effects of reducing \nprediction entropy on adversarial robustness. Finally, we study the effect of \nnetwork architectures on adversarial sensitivity. To do this, we use neural \narchitecture search with reinforcement learning to find adversarially robust \narchitectures on CIFAR10. Our resulting architecture is more robust to white \n\\emph{and} black box attacks compared to previous attempts. \n</p>"}, "author": "Ekin D. Cubuk, Barret Zoph, Samuel S. Schoenholz, Quoc V. Le", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885313", "id": "tag:google.com,2005:reader/item/000000032f482d71", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning Sparse Visual Representations with Leaky Capped Norm Regularizers. (arXiv:1711.02857v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02857"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02857", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Sparsity inducing regularization is an important part for learning \nover-complete visual representations. Despite the popularity of $\\ell_1$ \nregularization, in this paper, we investigate the usage of non-convex \nregularizations in this problem. Our contribution consists of three parts. \nFirst, we propose the leaky capped norm regularization (LCNR), which allows \nmodel weights below a certain threshold to be regularized more strongly as \nopposed to those above, therefore imposes strong sparsity and only introduces \ncontrollable estimation bias. We propose a majorization-minimization algorithm \nto optimize the joint objective function. Second, our study over monocular 3D \nshape recovery and neural networks with LCNR outperforms $\\ell_1$ and other \nnon-convex regularizations, achieving state-of-the-art performance and faster \nconvergence. Third, we prove a theoretical global convergence speed on the 3D \nrecovery problem. To the best of our knowledge, this is the first convergence \nanalysis of the 3D recovery problem. \n</p>"}, "author": "Jianqiao Wangni, Dahua Lin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885312", "id": "tag:google.com,2005:reader/item/000000032f482e02", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Dimension Estimation Using Random Connection Models. (arXiv:1711.02876v1 [stat.ML])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02876"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02876", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Information about intrinsic dimension is crucial to perform dimensionality \nreduction, compress information, design efficient algorithms, and do \nstatistical adaptation. In this paper we propose an estimator for the intrinsic \ndimension of a data set. The estimator is based on binary neighbourhood \ninformation about the observations in the form of two adjacency matrices, and \ndoes not require any explicit distance information. The underlying graph is \nmodelled according to a subset of a specific random connection model, sometimes \nreferred to as the Poisson blob model. Computationally the estimator scales \nlike n log n, and we specify its asymptotic distribution and rate of \nconvergence. A simulation study on both real and simulated data shows that our \napproach compares favourably with some competing methods from the literature, \nincluding approaches that rely on distance information. \n</p>"}, "author": "Paulo Serra, Michel Mandjes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885311", "id": "tag:google.com,2005:reader/item/000000032f482e3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Universal consistency and minimax rates for online Mondrian Forests. (arXiv:1711.02887v1 [stat.ML])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02887"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02887", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We establish the consistency of an algorithm of Mondrian Forests, a \nrandomized classification algorithm that can be implemented online. First, we \namend the original Mondrian Forest algorithm, that considers a fixed lifetime \nparameter. Indeed, the fact that this parameter is fixed hinders the \nstatistical consistency of the original procedure. Our modified Mondrian Forest \nalgorithm grows trees with increasing lifetime parameters $\\lambda_n$, and uses \nan alternative updating rule, allowing to work also in an online fashion. \nSecond, we provide a theoretical analysis establishing simple conditions for \nconsistency. Our theoretical analysis also exhibits a surprising fact: our \nalgorithm achieves the minimax rate (optimal rate) for the estimation of a \nLipschitz regression function, which is a strong extension of previous results \nto an arbitrary dimension. \n</p>"}, "author": "Jaouad Mourtada, St&#xe9;phane Ga&#xef;ffas, Erwan Scornet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885310", "id": "tag:google.com,2005:reader/item/000000032f482e67", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Clustering with feature selection using alternating minimization, Application to computational biology. (arXiv:1711.02974v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02974"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02974", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper deals with unsupervised clustering with feature selection. The \nproblem is to estimate both labels and a sparse projection matrix of weights. \nTo address this combinatorial non-convex problem maintaining a strict control \non the sparsity of the matrix of weights, we propose an alternating \nminimization of the Frobenius norm criterion. We provide a new efficient \nalgorithm named K-sparse which alternates k-means with projection-gradient \nminimization. The projection-gradient step is a method of splitting type, with \nexact projection on the $\\ell^1$ ball to promote sparsity. The convergence of \nthe gradient-projection step is addressed, and a preliminary analysis of the \nalternating minimization is made. The Frobenius norm criterion converges as the \nnumber of iterates in Algorithm K-sparse goes to infinity. Experiments on \nSingle Cell RNA sequencing datasets show that our method significantly improves \nthe results of PCA k-means, spectral clustering, SIMLR, and Sparcl methods, and \nachieves a relevant selection of genes. The complexity of K-sparse is linear in \nthe number of samples (cells), so that the method scales up to large datasets. \n</p>"}, "author": "Cyprien Gilet, Marie Deprez, Jean-Baptiste Caillau, Michel Barlaud", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885309", "id": "tag:google.com,2005:reader/item/000000032f482e91", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Variational Gaussian Dropout is not Bayesian. (arXiv:1711.02989v1 [stat.ML])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02989"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02989", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3bf50f8a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3bf50f8a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Gaussian multiplicative noise is commonly used as a stochastic regularisation \ntechnique in training of deterministic neural networks. A recent paper \nreinterpreted the technique as a specific algorithm for approximate inference \nin Bayesian neural networks; several extensions ensued. We show that the \nlog-uniform prior used in all the above publications does not generally induce \na proper posterior, and thus Bayesian inference in such models is ill-posed. \nIndependent of the log-uniform prior, the correlated weight noise approximation \nhas further issues leading to either infinite objective or high risk of \noverfitting. The above implies that the reported sparsity of obtained solutions \ncannot be explained by Bayesian or the related minimum description length \narguments. We thus study the objective from a non-Bayesian perspective, provide \nits previously unknown analytical form which allows exact gradient evaluation, \nand show that the later proposed additive reparametrisation introduces minima \nnot present in the original multiplicative parametrisation. Implications and \nfuture research directions are discussed. \n</p>"}, "author": "Jiri Hron, Alexander G. de G. Matthews, Zoubin Ghahramani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885308", "id": "tag:google.com,2005:reader/item/000000032f482eab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Intelligent Fault Analysis in Electrical Power Grids. (arXiv:1711.03026v1 [cs.CE])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03026"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03026", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3bfbc339\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3bfbc339&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Power grids are one of the most important components of infrastructure in \ntoday's world. Every nation is dependent on the security and stability of its \nown power grid to provide electricity to the households and industries. A \nmalfunction of even a small part of a power grid can cause loss of \nproductivity, revenue and in some cases even life. Thus, it is imperative to \ndesign a system which can detect the health of the power grid and take \nprotective measures accordingly even before a serious anomaly takes place. To \nachieve this objective, we have set out to create an artificially intelligent \nsystem which can analyze the grid information at any given time and determine \nthe health of the grid through the usage of sophisticated formal models and \nnovel machine learning techniques like recurrent neural networks. Our system \nsimulates grid conditions including stimuli like faults, generator output \nfluctuations, load fluctuations using Siemens PSS/E software and this data is \ntrained using various classifiers like SVM, LSTM and subsequently tested. The \nresults are excellent with our methods giving very high accuracy for the data. \nThis model can easily be scaled to handle larger and more complex grid \narchitectures. \n</p>"}, "author": "Biswarup Bhattacharya, Abhishek Sinha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885307", "id": "tag:google.com,2005:reader/item/000000032f482f0c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Recency-weighted Markovian inference. (arXiv:1711.03038v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03038"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03038", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We describe a Markov latent state space (MLSS) model, where the latent state \ndistribution is a decaying mixture over multiple past states. We present a \nsimple sampling algorithm that allows to approximate such high-order MLSS with \nfixed time and memory costs. \n</p>"}, "author": "Kristjan Kalm", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885306", "id": "tag:google.com,2005:reader/item/000000032f482f23", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Matrix-normal models for fMRI analysis. (arXiv:1711.03058v1 [stat.ML])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03058"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03058", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Multivariate analysis of fMRI data has benefited substantially from advances \nin machine learning. Most recently, a range of probabilistic latent variable \nmodels applied to fMRI data have been successful in a variety of tasks, \nincluding identifying similarity patterns in neural data (Representational \nSimilarity Analysis and its empirical Bayes variant, RSA and BRSA; Intersubject \nFunctional Connectivity, ISFC), combining multi-subject datasets (Shared \nResponse Mapping; SRM), and mapping between brain and behavior (Joint \nModeling). Although these methods share some underpinnings, they have been \ndeveloped as distinct methods, with distinct algorithms and software tools. We \nshow how the matrix-variate normal (MN) formalism can unify some of these \nmethods into a single framework. In doing so, we gain the ability to reuse \nnoise modeling assumptions, algorithms, and code across models. Our primary \ntheoretical contribution shows how some of these methods can be written as \ninstantiations of the same model, allowing us to generalize them to flexibly \nmodeling structured noise covariances. Our formalism permits novel model \nvariants and improved estimation strategies: in contrast to SRM, the number of \nparameters for MN-SRM does not scale with the number of voxels or subjects; in \ncontrast to BRSA, the number of parameters for MN-RSA scales additively rather \nthan multiplicatively in the number of voxels. We empirically demonstrate \nadvantages of two new methods derived in the formalism: for MN-RSA, we show up \nto 10x improvement in runtime, up to 6x improvement in RMSE, and more \nconservative behavior under the null. For MN-SRM, our method grants a modest \nimprovement to out-of-sample reconstruction while relaxing the orthonormality \nconstraint of SRM. We also provide a software prototyping tool for MN models \nthat can flexibly reuse noise covariance assumptions and algorithms across \nmodels. \n</p>"}, "author": "Michael Shvartsman, Narayanan Sundaram, Mikio C. Aoi, Adam Charles, Theodore C. Wilke, Jonathan D. Cohen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885305", "id": "tag:google.com,2005:reader/item/000000032f482f66", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning K-way D-dimensional Discrete Code For Compact Embedding Representations. (arXiv:1711.03067v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03067"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03067", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Embedding methods such as word embedding have become pillars for many \napplications containing discrete structures. Conventional embedding methods \ndirectly associate each symbol with a continuous embedding vector, which is \nequivalent to applying linear transformation based on \"one-hot\" encoding of the \ndiscrete symbols. Despite its simplicity, such approach yields number of \nparameters that grows linearly with the vocabulary size and can lead to \noverfitting. In this work we propose a much more compact K-way D-dimensional \ndiscrete encoding scheme to replace the \"one-hot\" encoding. In \"KD encoding\", \neach symbol is represented by a $D$-dimensional code, and each of its dimension \nhas a cardinality of $K$. The final symbol embedding vector can be generated by \ncomposing the code embedding vectors. To learn the semantically meaningful \ncode, we derive a relaxed discrete optimization technique based on stochastic \ngradient descent. By adopting the new coding system, the efficiency of \nparameterization can be significantly improved (from linear to logarithmic), \nand this can also mitigate the over-fitting problem. In our experiments with \nlanguage modeling, the number of embedding parameters can be reduced by 97\\% \nwhile achieving similar or better performance. \n</p>"}, "author": "Ting Chen, Martin Renqiang Min, Yizhou Sun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885304", "id": "tag:google.com,2005:reader/item/000000032f482f9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Lower bounds over Boolean inputs for deep neural networks with ReLU gates. (arXiv:1711.03073v1 [cs.CC])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03073"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03073", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Motivated by the resurgence of neural networks in being able to solve complex \nlearning tasks we undertake a study of high depth networks using ReLU gates \nwhich implement the function $x \\mapsto \\max\\{0,x\\}$. We try to understand the \nrole of depth in such neural networks by showing size lowerbounds against such \nnetwork architectures in parameter regimes hitherto unexplored. In particular \nwe show the following two main results about neural nets computing Boolean \nfunctions of input dimension $n$, \n</p> \n<p>1. We use the method of random restrictions to show almost linear, \n$\\Omega(\\epsilon^{2(1-\\delta)}n^{1-\\delta})$, lower bound for completely weight \nunrestricted LTF-of-ReLU circuits to match the Andreev function on at least \n$\\frac{1}{2} +\\epsilon$ fraction of the inputs for $\\epsilon &gt; \n\\sqrt{2\\frac{\\log^{\\frac {2}{2-\\delta}}(n)}{n}}$ for any $\\delta \\in (0,\\frac 1 \n2)$ \n</p> \n<p>2. We use the method of sign-rank to show exponential in dimension lower \nbounds for ReLU circuits ending in a LTF gate and of depths upto $O(n^{\\xi})$ \nwith $\\xi &lt; \\frac{1}{8}$ with some restrictions on the weights in the bottom \nmost layer. All other weights in these circuits are kept unrestricted. This in \nturns also implies the same lowerbounds for LTF circuits with the same \narchitecture and the same weight restrictions on their bottom most layer. \n</p> \n<p>Along the way we also show that there exists a $\\mathbb{R}^ n\\rightarrow \n\\mathbb{R}$ Sum-of-ReLU-of-ReLU function which Sum-of-ReLU neural nets can \nnever represent no matter how large they are allowed to be. \n</p>"}, "author": "Anirbit Mukherjee, Amitabh Basu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885282", "id": "tag:google.com,2005:reader/item/000000032f4831ab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Machine Learning Approach to RF Transmitter Identification. (arXiv:1711.01559v2 [eess.SP] CROSS LISTED)", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01559"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01559", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With the development and widespread use of wireless devices in recent years \n(mobile phones, Internet of Things, Wi-Fi), the electromagnetic spectrum has \nbecome extremely crowded. In order to counter security threats posed by rogue \nor unknown transmitters, it is important to identify RF transmitters not by the \ndata content of the transmissions but based on the intrinsic physical \ncharacteristics of the transmitters. RF waveforms represent a particular \nchallenge because of the extremely high data rates involved and the potentially \nlarge number of transmitters present in a given location. These factors outline \nthe need for rapid fingerprinting and identification methods that go beyond the \ntraditional hand-engineered approaches. In this study, we investigate the use \nof machine learning (ML) strategies to the classification and identification \nproblems, and the use of wavelets to reduce the amount of data required. Four \ndifferent ML strategies are evaluated: deep neural nets (DNN), convolutional \nneural nets (CNN), support vector machines (SVM), and multi-stage training \n(MST) using accelerated Levenberg-Marquardt (A-LM) updates. The A-LM MST method \npreconditioned by wavelets was by far the most accurate, achieving 100% \nclassification accuracy of transmitters, as tested using data originating from \n12 different transmitters. We discuss strategies for extension of MST to a much \nlarger number of transmitters. \n</p>"}, "author": "K. Youssef, Louis-S. Bouchard, K.Z. Haigh, H. Krovi, J. Silovsky, C.P. Vander Valk", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885281", "id": "tag:google.com,2005:reader/item/000000032f4831ec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "From Multimodal to Unimodal Webpages for Developing Countries. (arXiv:1711.02068v1 [cs.HC] CROSS LISTED)", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02068"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02068", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The multimodal web elements such as text and images are associated with \ninherent memory costs to store and transfer over the Internet. With the limited \nnetwork connectivity in developing countries, webpage rendering gets delayed in \nthe presence of high-memory demanding elements such as images (relative to \ntext). To overcome this limitation, we propose a Canonical Correlation Analysis \n(CCA) based computational approach to replace high-cost modality with an \nequivalent low-cost modality. Our model learns a common subspace for low-cost \nand high-cost modalities that maximizes the correlation between their visual \nfeatures. The obtained common subspace is used for determining the low-cost \n(text) element of a given high-cost (image) element for the replacement. We \nanalyze the cost-saving performance of the proposed approach through an \neye-tracking experiment conducted on real-world webpages. Our approach reduces \nthe memory-cost by at least 83.35% by replacing images with text. \n</p>"}, "author": "Vidyapu Sandeep, V Vijaya Saradhi, Samit Bhattacharya", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510193354097", "timestampUsec": "1510193354096502", "id": "tag:google.com,2005:reader/item/000000032f0058c8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Fidelity-Weighted Learning. (arXiv:1711.02799v1 [cs.LG])", "published": 1510193354, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02799"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02799", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Training deep neural networks requires many training samples, but in practice \ntraining labels are expensive to obtain and may be of varying quality, as some \nmay be from trusted expert labelers while others might be from heuristics or \nother sources of weak supervision such as crowd-sourcing. This creates a \nfundamental quality versus-quantity trade-off in the learning process. Do we \nlearn from the small amount of high-quality data or the potentially large \namount of weakly-labeled data? We argue that if the learner could somehow know \nand take the label-quality into account when learning the data representation, \nwe could get the best of both worlds. To this end, we propose \n\"fidelity-weighted learning\" (FWL), a semi-supervised student-teacher approach \nfor training deep neural networks using weakly-labeled data. FWL modulates the \nparameter updates to a student network (trained on the task we care about) on a \nper-sample basis according to the posterior confidence of its label-quality \nestimated by a teacher (who has access to the high-quality labels). Both \nstudent and teacher are learned from the data. We evaluate FWL on two tasks in \ninformation retrieval and natural language processing where we outperform \nstate-of-the-art alternative semi-supervised methods, indicating that our \napproach makes better use of strong and weak labels, and leads to better \ntask-dependent data representations. \n</p>"}, "author": "Mostafa Dehghani, Arash Mehrjou, Stephan Gouws, Jaap Kamps, Bernhard Sch&#xf6;lkopf", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510193354097", "timestampUsec": "1510193354096501", "id": "tag:google.com,2005:reader/item/000000032f0058d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Traffic Prediction Based on Random Connectivity in Deep Learning with Long Short-Term Memory. (arXiv:1711.02833v1 [cs.NI])", "published": 1510193354, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02833"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02833", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Traffic prediction plays an important role in evaluating the performance of \ntelecommunication networks and attracts intense research interests. A \nsignificant number of algorithms and models have been proposed to learn \nknowledge from traffic data and improve the prediction accuracy. In the recent \nbig data era, the relevant research enthusiasm remains and deep learning has \nbeen exploited to extract the useful information in depth. In particular, Long \nShort-Term Memory (LSTM), one kind of Recurrent Neural Network (RNN) schemes, \nhas attracted significant attentions due to the long-range dependency embedded \nin the sequential traffic data. However, LSTM has considerable computational \ncost, which can not be tolerated in tasks with stringent latency requirement. \nIn this paper, we propose a deep learning model based on LSTM, called Random \nConnectivity LSTM (RCLSTM). Compared to the conventional LSTM, RCLSTM achieves \na significant breakthrough in the architecture formation of neural network, \nwhose connectivity is determined in a stochastic manner rather than full \nconnected. So, the neural network in RCLSTM can exhibit certain sparsity, which \nmeans many neural connections are absent (distinguished from the full \nconnectivity) and thus the number of parameters to be trained is reduced and \nmuch fewer computations are required. We apply the RCLSTM solution to predict \ntraffic and validate that the RCLSTM with even 35% neural connectivity still \nshows a strong capability in traffic prediction. Also, along with increasing \nthe number of training samples, the performance of RCLSTM becomes closer to the \nconventional LSTM. Moreover, the RCLSTM exhibits even superior prediction \naccuracy than the conventional LSTM when the length of input traffic sequences \nincreases. \n</p>"}, "author": "Yuxiu Hua, Zhifeng Zhao, Rongpeng Li, Xianfu Chen, Zhiming Liu, Honggang Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510193354097", "timestampUsec": "1510193354096500", "id": "tag:google.com,2005:reader/item/000000032f0058e4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Lower bounds over Boolean inputs for deep neural networks with ReLU gates. (arXiv:1711.03073v1 [cs.CC])", "published": 1510193354, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03073"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03073", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3bfbc674\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3bfbc674&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Motivated by the resurgence of neural networks in being able to solve complex \nlearning tasks we undertake a study of high depth networks using ReLU gates \nwhich implement the function $x \\mapsto \\max\\{0,x\\}$. We try to understand the \nrole of depth in such neural networks by showing size lowerbounds against such \nnetwork architectures in parameter regimes hitherto unexplored. In particular \nwe show the following two main results about neural nets computing Boolean \nfunctions of input dimension $n$, \n</p> \n<p>1. We use the method of random restrictions to show almost linear, \n$\\Omega(\\epsilon^{2(1-\\delta)}n^{1-\\delta})$, lower bound for completely weight \nunrestricted LTF-of-ReLU circuits to match the Andreev function on at least \n$\\frac{1}{2} +\\epsilon$ fraction of the inputs for $\\epsilon &gt; \n\\sqrt{2\\frac{\\log^{\\frac {2}{2-\\delta}}(n)}{n}}$ for any $\\delta \\in (0,\\frac 1 \n2)$ \n</p> \n<p>2. We use the method of sign-rank to show exponential in dimension lower \nbounds for ReLU circuits ending in a LTF gate and of depths upto $O(n^{\\xi})$ \nwith $\\xi &lt; \\frac{1}{8}$ with some restrictions on the weights in the bottom \nmost layer. All other weights in these circuits are kept unrestricted. This in \nturns also implies the same lowerbounds for LTF circuits with the same \narchitecture and the same weight restrictions on their bottom most layer. \n</p> \n<p>Along the way we also show that there exists a $\\mathbb{R}^ n\\rightarrow \n\\mathbb{R}$ Sum-of-ReLU-of-ReLU function which Sum-of-ReLU neural nets can \nnever represent no matter how large they are allowed to be. \n</p>"}, "author": "Anirbit Mukherjee, Amitabh Basu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510193354097", "timestampUsec": "1510193354096496", "id": "tag:google.com,2005:reader/item/000000032f0058f6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Machine Learning Approach to RF Transmitter Identification. (arXiv:1711.01559v2 [eess.SP] CROSS LISTED)", "published": 1510193354, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01559"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01559", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With the development and widespread use of wireless devices in recent years \n(mobile phones, Internet of Things, Wi-Fi), the electromagnetic spectrum has \nbecome extremely crowded. In order to counter security threats posed by rogue \nor unknown transmitters, it is important to identify RF transmitters not by the \ndata content of the transmissions but based on the intrinsic physical \ncharacteristics of the transmitters. RF waveforms represent a particular \nchallenge because of the extremely high data rates involved and the potentially \nlarge number of transmitters present in a given location. These factors outline \nthe need for rapid fingerprinting and identification methods that go beyond the \ntraditional hand-engineered approaches. In this study, we investigate the use \nof machine learning (ML) strategies to the classification and identification \nproblems, and the use of wavelets to reduce the amount of data required. Four \ndifferent ML strategies are evaluated: deep neural nets (DNN), convolutional \nneural nets (CNN), support vector machines (SVM), and multi-stage training \n(MST) using accelerated Levenberg-Marquardt (A-LM) updates. The A-LM MST method \npreconditioned by wavelets was by far the most accurate, achieving 100% \nclassification accuracy of transmitters, as tested using data originating from \n12 different transmitters. We discuss strategies for extension of MST to a much \nlarger number of transmitters. \n</p>"}, "author": "K. Youssef, Louis-S. Bouchard, K.Z. Haigh, H. Krovi, J. Silovsky, C.P. Vander Valk", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164296", "id": "tag:google.com,2005:reader/item/000000032eb5f0d3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Bounding and Counting Linear Regions of Deep Neural Networks. (arXiv:1711.02114v1 [cs.LG])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02114"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02114", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we study the representational power of deep neural networks \n(DNN) that belong to the family of piecewise-linear (PWL) functions, based on \nPWL activation units such as rectifier or maxout. We investigate the complexity \nof such networks by studying the number of linear regions of the PWL function. \nTypically, a PWL function from a DNN can be seen as a large family of linear \nfunctions acting on millions of such regions. We directly build upon the work \nof Montufar et al. (2014) and Raghu et al. (2017) by refining the upper and \nlower bounds on the number of linear regions for rectified and maxout networks. \nIn addition to achieving tighter bounds, we also develop a novel method to \nperform exact enumeration or counting of the number of linear regions with a \nmixed-integer linear formulation that maps the input space to output. We use \nthis new capability to visualize how the number of linear regions change while \ntraining DNNs. \n</p>"}, "author": "Thiago Serra, Christian Tjandraatmadja, Srikumar Ramalingam", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164295", "id": "tag:google.com,2005:reader/item/000000032eb5f0e1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Weighted Transformer Network for Machine Translation. (arXiv:1711.02132v1 [cs.AI])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02132"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02132", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>State-of-the-art results on neural machine translation often use attentional \nsequence-to-sequence models with some form of convolution or recursion. Vaswani \net al. (2017) propose a new architecture that avoids recurrence and convolution \ncompletely. Instead, it uses only self-attention and feed-forward layers. While \nthe proposed architecture achieves state-of-the-art results on several machine \ntranslation tasks, it requires a large number of parameters and training \niterations to converge. We propose Weighted Transformer, a Transformer with \nmodified attention layers, that not only outperforms the baseline network in \nBLEU score but also converges 15-40% faster. Specifically, we replace the \nmulti-head attention by multiple self-attention branches that the model learns \nto combine during the training process. Our model improves the state-of-the-art \nperformance by 0.5 BLEU points on the WMT 2014 English-to-German translation \ntask and by 0.4 on the English-to-French translation task. \n</p>"}, "author": "Karim Ahmed, Nitish Shirish Keskar, Richard Socher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164294", "id": "tag:google.com,2005:reader/item/000000032eb5f0ea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Adaptive Bayesian Sampling with Monte Carlo EM. (arXiv:1711.02159v1 [cs.LG])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02159"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02159", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a novel technique for learning the mass matrices in samplers \nobtained from discretized dynamics that preserve some energy function. Existing \nadaptive samplers use Riemannian preconditioning techniques, where the mass \nmatrices are functions of the parameters being sampled. This leads to \nsignificant complexities in the energy reformulations and resultant dynamics, \noften leading to implicit systems of equations and requiring inversion of \nhigh-dimensional matrices in the leapfrog steps. Our approach provides a \nsimpler alternative, by using existing dynamics in the sampling step of a Monte \nCarlo EM framework, and learning the mass matrices in the M step with a novel \nonline technique. We also propose a way to adaptively set the number of samples \ngathered in the E step, using sampling error estimates from the leapfrog \ndynamics. Along with a novel stochastic sampler based on Nos\\'{e}-Poincar\\'{e} \ndynamics, we use this framework with standard Hamiltonian Monte Carlo (HMC) as \nwell as newer stochastic algorithms such as SGHMC and SGNHT, and show strong \nperformance on synthetic and real high-dimensional sampling scenarios; we \nachieve sampling accuracies comparable to Riemannian samplers while being \nsignificantly faster. \n</p>"}, "author": "Anirban Roychowdhury, Srinivasan Parthasarathy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164293", "id": "tag:google.com,2005:reader/item/000000032eb5f0f2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Alpha-expansion is Exact on Stable Instances. (arXiv:1711.02195v1 [stat.ML])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02195"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02195", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Approximate algorithms for structured prediction problems---such as the \npopular alpha-expansion algorithm (Boykov et al. 2001) in computer \nvision---typically far exceed their theoretical performance guarantees on \nreal-world instances. These algorithms often find solutions that are very close \nto optimal. The goal of this paper is to partially explain the performance of \nalpha-expansion on MAP inference in Ferromagnetic Potts models (FPMs). Our main \nresults use the connection between energy minimization in FPMs and the Uniform \nMetric Labeling problem to give a stability condition under which the \nalpha-expansion algorithm provably recovers the optimal MAP solution. This \ntheoretical result complements the numerous empirical observations of \nalpha-expansion's performance. Additionally, we give a different stability \ncondition under which an LP-based algorithm recovers the optimal solution. \n</p>"}, "author": "Hunter Lang, David Sontag, Aravindan Vijayaraghavan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164292", "id": "tag:google.com,2005:reader/item/000000032eb5f100", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Visually-Aware Fashion Recommendation and Design with Generative Image Models. (arXiv:1711.02231v1 [cs.CV])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02231"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02231", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Building effective recommender systems for domains like fashion is \nchallenging due to the high level of subjectivity and the semantic complexity \nof the features involved (i.e., fashion styles). Recent work has shown that \napproaches to `visual' recommendation (e.g.~clothing, art, etc.) can be made \nmore accurate by incorporating visual signals directly into the recommendation \nobjective, using `off-the-shelf' feature representations derived from deep \nnetworks. Here, we seek to extend this contribution by showing that \nrecommendation performance can be significantly improved by learning `fashion \naware' image representations directly, i.e., by training the image \nrepresentation (from the pixel level) and the recommender system jointly; this \ncontribution is related to recent work using Siamese CNNs, though we are able \nto show improvements over state-of-the-art recommendation techniques such as \nBPR and variants that make use of pre-trained visual features. Furthermore, we \nshow that our model can be used \\emph{generatively}, i.e., given a user and a \nproduct category, we can generate new images (i.e., clothing items) that are \nmost consistent with their personal taste. This represents a first step towards \nbuilding systems that go beyond recommending existing items from a product \ncorpus, but which can be used to suggest styles and aid the design of new \nproducts. \n</p>"}, "author": "Wang-Cheng Kang, Chen Fang, Zhaowen Wang, Julian McAuley", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164291", "id": "tag:google.com,2005:reader/item/000000032eb5f106", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?. (arXiv:1711.02301v1 [cs.AI])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02301"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02301", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep reinforcement learning has achieved many recent successes, but our \nunderstanding of its strengths and limitations is hampered by the lack of rich \nenvironments in which we can fully characterize optimal behavior, and \ncorrespondingly diagnose individual actions against such a characterization. \nHere we consider a family of combinatorial games, arising from work of Erdos, \nSelfridge, and Spencer, and we propose their use as environments for evaluating \nand comparing different approaches to reinforcement learning. These games have \na number of appealing features: they are challenging for current learning \napproaches, but they form (i) a low-dimensional, simply parametrized \nenvironment where (ii) there is a linear closed form solution for optimal \nbehavior from any state, and (iii) the difficulty of the game can be tuned by \nchanging environment parameters in an interpretable way. We use these \nErdos-Selfridge-Spencer games not only to compare different algorithms, but \nalso to compare approaches based on supervised and reinforcement learning, to \nanalyze the power of multi-agent approaches in improving performance, and to \nevaluate generalization to environments outside the training set. \n</p>"}, "author": "Maithra Raghu, Alex Irpan, Jacob Andreas, Robert Kleinberg, Quoc V. Le, Jon Kleinberg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164290", "id": "tag:google.com,2005:reader/item/000000032eb5f109", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning Overcomplete HMMs. (arXiv:1711.02309v1 [cs.LG])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02309"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02309", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of learning overcomplete HMMs---those that have many \nhidden states but a small output alphabet. Despite having significant practical \nimportance, such HMMs are poorly understood with no known positive or negative \nresults for efficient learning. In this paper, we present several new \nresults---both positive and negative---which help define the boundaries between \nthe tractable and intractable settings. Specifically, we show positive results \nfor a large subclass of HMMs whose transition matrices are sparse, \nwell-conditioned, and have small probability mass on short cycles. On the other \nhand, we show that learning is impossible given only a polynomial number of \nsamples for HMMs with a small output alphabet and whose transition matrices are \nrandom regular graphs with large degree. We also discuss these results in the \ncontext of learning HMMs which can capture long-term dependencies. \n</p>"}, "author": "Vatsal Sharan, Sham Kakade, Percy Liang, Gregory Valiant", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164289", "id": "tag:google.com,2005:reader/item/000000032eb5f10e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks. (arXiv:1711.02326v1 [cs.AI])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02326"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02326", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A major drawback of backpropagation through time (BPTT) is the difficulty of \nlearning long-term dependencies, coming from having to propagate credit \ninformation backwards through every single step of the forward computation. \nThis makes BPTT both computationally impractical and biologically implausible. \nFor this reason, full backpropagation through time is rarely used on long \nsequences, and truncated backpropagation through time is used as a heuristic. \nHowever, this usually leads to biased estimates of the gradient in which longer \nterm dependencies are ignored. Addressing this issue, we propose an alternative \nalgorithm, Sparse Attentive Backtracking, which might also be related to \nprinciples used by brains to learn long-term dependencies. Sparse Attentive \nBacktracking learns an attention mechanism over the hidden states of the past \nand selectively backpropagates through paths with high attention weights. This \nallows the model to learn long term dependencies while only backtracking for a \nsmall number of time steps, not just from the recent past but also from \nattended relevant past states. \n</p>"}, "author": "Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Laurent Charlin, Chris Pal, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164288", "id": "tag:google.com,2005:reader/item/000000032eb5f114", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Distributed Bayesian Piecewise Sparse Linear Models. (arXiv:1711.02368v1 [cs.AI])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02368"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02368", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3bfbc9cb\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3bfbc9cb&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The importance of interpretability of machine learning models has been \nincreasing due to emerging enterprise predictive analytics, threat of data \nprivacy, accountability of artificial intelligence in society, and so on. \nPiecewise linear models have been actively studied to achieve both accuracy and \ninterpretability. They often produce competitive accuracy against \nstate-of-the-art non-linear methods. In addition, their representations (i.e., \nrule-based segmentation plus sparse linear formula) are often preferred by \ndomain experts. A disadvantage of such models, however, is high computational \ncost for simultaneous determinations of the number of \"pieces\" and cardinality \nof each linear predictor, which has restricted their applicability to \nmiddle-scale data sets. This paper proposes a distributed factorized asymptotic \nBayesian (FAB) inference of learning piece-wise sparse linear models on \ndistributed memory architectures. The distributed FAB inference solves the \nsimultaneous model selection issue without communicating $O(N)$ data where N is \nthe number of training samples and achieves linear scale-out against the number \nof CPU cores. Experimental results demonstrate that the distributed FAB \ninference achieves high prediction accuracy and performance scalability with \nboth synthetic and benchmark data. \n</p>"}, "author": "Masato Asahara, Ryohei Fujimaki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164287", "id": "tag:google.com,2005:reader/item/000000032eb5f117", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Non-monotone Continuous DR-submodular Maximization: Structure and Algorithms. (arXiv:1711.02515v1 [cs.LG])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02515"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02515", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c03774a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c03774a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>DR-submodular continuous functions are important objectives with wide \nreal-world applications spanning MAP inference in determinantal point processes \n(DPPs), and mean-field inference for probabilistic submodular models, amongst \nothers. DR-submodularity captures a subclass of non-convex functions that \nenables both exact minimization and approximate maximization in polynomial \ntime. \n</p> \n<p>In this work we study the problem of maximizing non-monotone DR-submodular \ncontinuous functions under general down-closed convex constraints. We start by \ninvestigating geometric properties that underlie such objectives, e.g., a \nstrong relation between (approximately) stationary points and global optimum is \nproved. These properties are then used to devise two optimization algorithms \nwith provable guarantees. Concretely, we first devise a \"two-phase\" algorithm \nwith $1/4$ approximation guarantee. This algorithm allows the use of existing \nmethods for finding (approximately) stationary points as a subroutine, thus, \nharnessing recent progress in non-convex optimization. Then we present a \nnon-monotone Frank-Wolfe variant with $1/e$ approximation guarantee and \nsublinear convergence rate. Finally, we extend our approach to a broader class \nof generalized DR-submodular continuous functions, which captures a wider \nspectrum of applications. Our theoretical findings are validated on synthetic \nand real-world problem instances. \n</p>"}, "author": "An Bian, Kfir Y. Levy, Andreas Krause, Joachim M. Buhmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164285", "id": "tag:google.com,2005:reader/item/000000032eb5f120", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Off-policy evaluation for slate recommendation. (arXiv:1605.04812v3 [cs.LG] UPDATED)", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1605.04812"}], "alternate": [{"href": "http://arxiv.org/abs/1605.04812", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper studies the evaluation of policies that recommend an ordered set \nof items (e.g., a ranking) based on some context---a common scenario in web \nsearch, ads, and recommendation. We build on techniques from combinatorial \nbandits to introduce a new practical estimator that uses logged data to \nestimate a policy's performance. A thorough empirical evaluation on real-world \ndata reveals that our estimator is accurate in a variety of settings, including \nas a subroutine in a learning-to-rank task, where it achieves competitive \nperformance. We derive conditions under which our estimator is unbiased---these \nconditions are weaker than prior heuristics for slate evaluation---and \nexperimentally demonstrate a smaller bias than parametric approaches, even when \nthese conditions are violated. Finally, our theory and experiments also show \nexponential savings in the amount of required data compared with general \nunbiased estimators. \n</p>"}, "author": "Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miroslav Dud&#xed;k, John Langford, Damien Jose, Imed Zitouni", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164276", "id": "tag:google.com,2005:reader/item/000000032eb5f15e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "MMD GAN: Towards Deeper Understanding of Moment Matching Network. (arXiv:1705.08584v2 [cs.LG] CROSS LISTED)", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.08584"}], "alternate": [{"href": "http://arxiv.org/abs/1705.08584", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative moment matching network (GMMN) is a deep generative model that \ndiffers from Generative Adversarial Network (GAN) by replacing the \ndiscriminator in GAN with a two-sample test based on kernel maximum mean \ndiscrepancy (MMD). Although some theoretical guarantees of MMD have been \nstudied, the empirical performance of GMMN is still not as competitive as that \nof GAN on challenging and large benchmark datasets. The computational \nefficiency of GMMN is also less desirable in comparison with GAN, partially due \nto its requirement for a rather large batch size during the training. In this \npaper, we propose to improve both the model expressiveness of GMMN and its \ncomputational efficiency by introducing adversarial kernel learning techniques, \nas the replacement of a fixed Gaussian kernel in the original GMMN. The new \napproach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. \nThe new distance measure in MMD GAN is a meaningful loss that enjoys the \nadvantage of weak topology and can be optimized via gradient descent with \nrelatively small batch sizes. In our evaluation on multiple benchmark datasets, \nincluding MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN \nsignificantly outperforms GMMN, and is competitive with other representative \nGAN works. \n</p>"}, "author": "Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, Barnab&#xe1;s P&#xf3;czos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228481", "id": "tag:google.com,2005:reader/item/000000032e88cfdb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Bounding and Counting Linear Regions of Deep Neural Networks. (arXiv:1711.02114v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02114"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02114", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we study the representational power of deep neural networks \n(DNN) that belong to the family of piecewise-linear (PWL) functions, based on \nPWL activation units such as rectifier or maxout. We investigate the complexity \nof such networks by studying the number of linear regions of the PWL function. \nTypically, a PWL function from a DNN can be seen as a large family of linear \nfunctions acting on millions of such regions. We directly build upon the work \nof Montufar et al. (2014) and Raghu et al. (2017) by refining the upper and \nlower bounds on the number of linear regions for rectified and maxout networks. \nIn addition to achieving tighter bounds, we also develop a novel method to \nperform exact enumeration or counting of the number of linear regions with a \nmixed-integer linear formulation that maps the input space to output. We use \nthis new capability to visualize how the number of linear regions change while \ntraining DNNs. \n</p>"}, "author": "Thiago Serra, Christian Tjandraatmadja, Srikumar Ramalingam", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228480", "id": "tag:google.com,2005:reader/item/000000032e88cfeb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Adaptive Bayesian Sampling with Monte Carlo EM. (arXiv:1711.02159v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02159"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02159", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a novel technique for learning the mass matrices in samplers \nobtained from discretized dynamics that preserve some energy function. Existing \nadaptive samplers use Riemannian preconditioning techniques, where the mass \nmatrices are functions of the parameters being sampled. This leads to \nsignificant complexities in the energy reformulations and resultant dynamics, \noften leading to implicit systems of equations and requiring inversion of \nhigh-dimensional matrices in the leapfrog steps. Our approach provides a \nsimpler alternative, by using existing dynamics in the sampling step of a Monte \nCarlo EM framework, and learning the mass matrices in the M step with a novel \nonline technique. We also propose a way to adaptively set the number of samples \ngathered in the E step, using sampling error estimates from the leapfrog \ndynamics. Along with a novel stochastic sampler based on Nos\\'{e}-Poincar\\'{e} \ndynamics, we use this framework with standard Hamiltonian Monte Carlo (HMC) as \nwell as newer stochastic algorithms such as SGHMC and SGNHT, and show strong \nperformance on synthetic and real high-dimensional sampling scenarios; we \nachieve sampling accuracies comparable to Riemannian samplers while being \nsignificantly faster. \n</p>"}, "author": "Anirban Roychowdhury, Srinivasan Parthasarathy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228479", "id": "tag:google.com,2005:reader/item/000000032e88cff8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Alpha-expansion is Exact on Stable Instances. (arXiv:1711.02195v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02195"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02195", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Approximate algorithms for structured prediction problems---such as the \npopular alpha-expansion algorithm (Boykov et al. 2001) in computer \nvision---typically far exceed their theoretical performance guarantees on \nreal-world instances. These algorithms often find solutions that are very close \nto optimal. The goal of this paper is to partially explain the performance of \nalpha-expansion on MAP inference in Ferromagnetic Potts models (FPMs). Our main \nresults use the connection between energy minimization in FPMs and the Uniform \nMetric Labeling problem to give a stability condition under which the \nalpha-expansion algorithm provably recovers the optimal MAP solution. This \ntheoretical result complements the numerous empirical observations of \nalpha-expansion's performance. Additionally, we give a different stability \ncondition under which an LP-based algorithm recovers the optimal solution. \n</p>"}, "author": "Hunter Lang, David Sontag, Aravindan Vijayaraghavan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228478", "id": "tag:google.com,2005:reader/item/000000032e88d007", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Regret Bounds and Regimes of Optimality for User-User and Item-Item Collaborative Filtering. (arXiv:1711.02198v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02198"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02198", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider an online model for recommendation systems, with each user being \nrecommended an item at each time-step and providing 'like' or 'dislike' \nfeedback. A latent variable model specifies the user preferences: both users \nand items are clustered into types. All users of a given type have identical \npreferences for the items, and similarly, items of a given type are either all \nliked or all disliked by a given user. The model captures structure in both the \nitem and user spaces, and in this paper, we assume that the type preference \nmatrix is randomly generated. We describe two algorithms inspired by user-user \nand item-item collaborative filtering (CF), modified to explicitly make \nexploratory recommendations, and prove performance guarantees in terms of their \nexpected regret. For two regimes of model parameters, with structure only in \nitem space or only in user space, we prove information-theoretic lower bounds \non regret that match our upper bounds up to logarithmic factors. Our analysis \nelucidates system operating regimes in which existing CF algorithms are nearly \noptimal. \n</p>"}, "author": "Guy Bresler, Mina Karzand", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228477", "id": "tag:google.com,2005:reader/item/000000032e88d017", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Unsupervised Learning of Semantic Audio Representations. (arXiv:1711.02209v1 [cs.SD])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02209"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02209", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Even in the absence of any explicit semantic annotation, vast collections of \naudio recordings provide valuable information for learning the categorical \nstructure of sounds. We consider several class-agnostic semantic constraints \nthat apply to unlabeled nonspeech audio: (i) noise and translations in time do \nnot change the underlying sound category, (ii) a mixture of two sound events \ninherits the categories of the constituents, and (iii) the categories of events \nin close temporal proximity are likely to be the same or related. Without \nlabels to ground them, these constraints are incompatible with classification \nloss functions. However, they may still be leveraged to identify geometric \ninequalities needed for triplet loss-based training of convolutional neural \nnetworks. The result is low-dimensional embeddings of the input spectrograms \nthat recover 41% and 84% of the performance of their fully-supervised \ncounterparts when applied to downstream query-by-example sound retrieval and \nsound event classification tasks, respectively. Moreover, in \nlimited-supervision settings, our unsupervised embeddings double the \nstate-of-the-art classification performance. \n</p>"}, "author": "Aren Jansen, Manoj Plakal, Ratheet Pandya, Daniel P. W. Ellis, Shawn Hershey, Jiayang Liu, R. Channing Moore, Rif A. Saurous", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228476", "id": "tag:google.com,2005:reader/item/000000032e88d030", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks. (arXiv:1711.02213v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02213"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02213", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks are commonly developed and trained in 32-bit floating \npoint format. Significant gains in performance and energy efficiency could be \nrealized by training and inference in numerical formats optimized for deep \nlearning. Despite advances in limited precision inference in recent years, \ntraining of neural networks in low bit-width remains a challenging problem. \nHere we present the Flexpoint data format, aiming at a complete replacement of \n32-bit floating point format training and inference, designed to support modern \ndeep network topologies without modifications. Flexpoint tensors have a shared \nexponent that is dynamically adjusted to minimize overflows and maximize \navailable dynamic range. We validate Flexpoint by training AlexNet, a deep \nresidual network and a generative adversarial network, using a simulator \nimplemented with the neon deep learning framework. We demonstrate that 16-bit \nFlexpoint closely matches 32-bit floating point in training all three models, \nwithout any need for tuning of model hyperparameters. Our results suggest \nFlexpoint as a promising numerical format for future hardware for training and \ninference. \n</p>"}, "author": "Urs K&#xf6;ster, Tristan Webb, Xin Wang, Marcel Nassar, Arjun Bansal, William Constable, Oguz Elibol, Stewart Hall, Luke Hornof, Amir Khosrowshahi, Carey Kloss, Ruby Pai, Naveen Rao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228475", "id": "tag:google.com,2005:reader/item/000000032e88d037", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Unsupervised Transformation Learning via Convex Relaxations. (arXiv:1711.02226v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02226"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02226", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c0379b2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c0379b2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Our goal is to extract meaningful transformations from raw images, such as \nvarying the thickness of lines in handwriting or the lighting in a portrait. We \npropose an unsupervised approach to learn such transformations by attempting to \nreconstruct an image from a linear combination of transformations of its \nnearest neighbors. On handwritten digits and celebrity portraits, we show that \neven with linear transformations, our method generates visually high-quality \nmodified images. Moreover, since our method is semiparametric and does not \nmodel the data distribution, the learned transformations extrapolate off the \ntraining data and can be applied to new types of images. \n</p>"}, "author": "Tatsunori B. Hashimoto, John C. Duchi, Percy Liang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228474", "id": "tag:google.com,2005:reader/item/000000032e88d03d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net. (arXiv:1711.02282v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02282"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02282", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a novel method to directly learn a stochastic transition operator \nwhose repeated application provides generated samples. Traditional undirected \ngraphical models approach this problem indirectly by learning a Markov chain \nmodel whose stationary distribution obeys detailed balance with respect to a \nparameterized energy function. The energy function is then modified so the \nmodel and data distributions match, with no guarantee on the number of steps \nrequired for the Markov chain to converge. Moreover, the detailed balance \ncondition is highly restrictive: energy based models corresponding to neural \nnetworks must have symmetric weights, unlike biological neural circuits. In \ncontrast, we develop a method for directly learning arbitrarily parameterized \ntransition operators capable of expressing non-equilibrium stationary \ndistributions that violate detailed balance, thereby enabling us to learn more \nbiologically plausible asymmetric neural networks and more general non-energy \nbased dynamical systems. The proposed training objective, which we derive via \nprincipled variational methods, encourages the transition operator to \"walk \nback\" in multi-step trajectories that start at data-points, as quickly as \npossible back to the original data points. We present a series of experimental \nresults illustrating the soundness of the proposed approach, Variational \nWalkback (VW), on the MNIST, CIFAR-10, SVHN and CelebA datasets, demonstrating \nsuperior samples compared to earlier attempts to learn a transition operator. \nWe also show that although each rapid training trajectory is limited to a \nfinite but variable number of steps, our transition operator continues to \ngenerate good samples well past the length of such trajectories, thereby \ndemonstrating the match of its non-equilibrium stationary distribution to the \ndata distribution. Source Code: <a href=\"http://github.com/anirudh9119/walkback_nips17\">this http URL</a> \n</p>"}, "author": "Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228473", "id": "tag:google.com,2005:reader/item/000000032e88d044", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Large-Scale Optimal Transport and Mapping Estimation. (arXiv:1711.02283v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02283"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02283", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a novel two-step approach for the fundamental problem of \nlearning an optimal map from one distribution to another. First, we learn an \noptimal transport (OT) plan, which can be thought as a one-to-many map between \nthe two distributions. To that end, we propose a stochastic dual approach of \nregularized OT, and show empirically that it scales better than a recent \nrelated approach when the amount of samples is very large. Second, we estimate \na Monge map as a deep neural network learned by approximating the barycentric \nprojection of the previously-obtained OT plan. We prove two theoretical \nstability results of regularized OT which show that our estimations converge to \nthe OT plan and Monge map between the underlying continuous measures. We \nshowcase our proposed approach on two applications: domain adaptation and \ngenerative modeling. \n</p>"}, "author": "Vivien Seguy, Bharath Bhushan Damodaran, R&#xe9;mi Flamary, Nicolas Courty, Antoine Rolet, Mathieu Blondel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228472", "id": "tag:google.com,2005:reader/item/000000032e88d048", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?. (arXiv:1711.02301v1 [cs.AI])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02301"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02301", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep reinforcement learning has achieved many recent successes, but our \nunderstanding of its strengths and limitations is hampered by the lack of rich \nenvironments in which we can fully characterize optimal behavior, and \ncorrespondingly diagnose individual actions against such a characterization. \nHere we consider a family of combinatorial games, arising from work of Erdos, \nSelfridge, and Spencer, and we propose their use as environments for evaluating \nand comparing different approaches to reinforcement learning. These games have \na number of appealing features: they are challenging for current learning \napproaches, but they form (i) a low-dimensional, simply parametrized \nenvironment where (ii) there is a linear closed form solution for optimal \nbehavior from any state, and (iii) the difficulty of the game can be tuned by \nchanging environment parameters in an interpretable way. We use these \nErdos-Selfridge-Spencer games not only to compare different algorithms, but \nalso to compare approaches based on supervised and reinforcement learning, to \nanalyze the power of multi-agent approaches in improving performance, and to \nevaluate generalization to environments outside the training set. \n</p>"}, "author": "Maithra Raghu, Alex Irpan, Jacob Andreas, Robert Kleinberg, Quoc V. Le, Jon Kleinberg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228471", "id": "tag:google.com,2005:reader/item/000000032e88d04f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning Overcomplete HMMs. (arXiv:1711.02309v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02309"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02309", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of learning overcomplete HMMs---those that have many \nhidden states but a small output alphabet. Despite having significant practical \nimportance, such HMMs are poorly understood with no known positive or negative \nresults for efficient learning. In this paper, we present several new \nresults---both positive and negative---which help define the boundaries between \nthe tractable and intractable settings. Specifically, we show positive results \nfor a large subclass of HMMs whose transition matrices are sparse, \nwell-conditioned, and have small probability mass on short cycles. On the other \nhand, we show that learning is impossible given only a polynomial number of \nsamples for HMMs with a small output alphabet and whose transition matrices are \nrandom regular graphs with large degree. We also discuss these results in the \ncontext of learning HMMs which can capture long-term dependencies. \n</p>"}, "author": "Vatsal Sharan, Sham Kakade, Percy Liang, Gregory Valiant", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228470", "id": "tag:google.com,2005:reader/item/000000032e88d075", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Multi-Player Bandits Models Revisited. (arXiv:1711.02317v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02317"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02317", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Multi-player Multi-Armed Bandits (MAB) have been extensively studied in the \nliterature, motivated by applications to Cognitive Radio systems. Driven by \nsuch applications as well, we motivate the introduction of several levels of \nfeedback for multi-player MAB algorithms. Most existing work assume that \nsensing information is available to the algorithm. Under this assumption, we \nimprove the state-of-the-art lower bound for the regret of any decentralized \nalgorithms and introduce two algorithms, RandTopM and MCTopM, that are shown to \nempirically outperform existing algorithms. Moreover, we provide strong \ntheoretical guarantees for these algorithms, including a notion of asymptotic \noptimality in terms of the number of selections of bad arms. We then introduce \na promising heuristic, called Selfish, that can operate without sensing \ninformation, which is crucial for emerging applications to Internet of Things \nnetworks. We investigate the empirical performance of this algorithm and \nprovide some first theoretical elements for the understanding of its behavior. \n</p>"}, "author": "Lilian Besson (IETR), Emilie Kaufmann (SEQUEL)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228469", "id": "tag:google.com,2005:reader/item/000000032e88d096", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks. (arXiv:1711.02326v1 [cs.AI])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02326"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02326", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A major drawback of backpropagation through time (BPTT) is the difficulty of \nlearning long-term dependencies, coming from having to propagate credit \ninformation backwards through every single step of the forward computation. \nThis makes BPTT both computationally impractical and biologically implausible. \nFor this reason, full backpropagation through time is rarely used on long \nsequences, and truncated backpropagation through time is used as a heuristic. \nHowever, this usually leads to biased estimates of the gradient in which longer \nterm dependencies are ignored. Addressing this issue, we propose an alternative \nalgorithm, Sparse Attentive Backtracking, which might also be related to \nprinciples used by brains to learn long-term dependencies. Sparse Attentive \nBacktracking learns an attention mechanism over the hidden states of the past \nand selectively backpropagates through paths with high attention weights. This \nallows the model to learn long term dependencies while only backtracking for a \nsmall number of time steps, not just from the recent past but also from \nattended relevant past states. \n</p>"}, "author": "Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Laurent Charlin, Chris Pal, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228468", "id": "tag:google.com,2005:reader/item/000000032e88d0b2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Interpreting Convolutional Neural Networks Through Compression. (arXiv:1711.02329v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02329"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02329", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolutional neural networks (CNNs) achieve state-of-the-art performance in \na wide variety of tasks in computer vision. However, interpreting CNNs still \nremains a challenge. This is mainly due to the large number of parameters in \nthese networks. Here, we investigate the role of compression and particularly \npruning filters in the interpretation of CNNs. We exploit our recently-proposed \ngreedy structural compression scheme that prunes filters in a trained CNN. In \nour compression, the filter importance index is defined as the classification \naccuracy reduction (CAR) of the network after pruning that filter. The filters \nare then iteratively pruned based on the CAR index. We demonstrate the \ninterpretability of CAR-compressed CNNs by showing that our algorithm prunes \nfilters with visually redundant pattern selectivity. Specifically, we show the \nimportance of shape-selective filters for object recognition, as opposed to \ncolor-selective filters. Out of top 20 CAR-pruned filters in AlexNet, 17 of \nthem in the first layer and 14 of them in the second layer are color-selective \nfilters. Finally, we introduce a variant of our CAR importance index that \nquantifies the importance of each image class to each CNN filter. We show that \nthe most and the least important class labels present a meaningful \ninterpretation of each filter that is consistent with the visualized pattern \nselectivity of that filter. \n</p>"}, "author": "Reza Abbasi-Asl, Bin Yu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228467", "id": "tag:google.com,2005:reader/item/000000032e88d0dc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "FADO: A Deterministic Detection/Learning Algorithm. (arXiv:1711.02361v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02361"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02361", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes and studies a detection technique for adversarial \nscenarios (dubbed deterministic detection). This technique provides an \nalternative detection methodology in case the usual stochastic methods are not \napplicable: this can be because the studied phenomenon does not follow a \nstochastic sampling scheme, samples are high-dimensional and subsequent \nmultiple-testing corrections render results overly conservative, sample sizes \nare too low for asymptotic results (as e.g. the central limit theorem) to kick \nin, or one cannot allow for the small probability of failure inherent to \nstochastic approaches. This paper instead designs a method based on insights \nfrom machine learning and online learning theory: this detection algorithm - \nnamed Online FAult Detection (FADO) - comes with theoretical guarantees of its \ndetection capabilities. A version of the margin is found to regulate the \ndetection performance of FADO. A precise expression is derived for bounding the \nperformance, and experimental results are presented assessing the influence of \ninvolved quantities. A case study of scene detection is used to illustrate the \napproach. The technology is closely related to the linear perceptron rule, \ninherits its computational attractiveness and flexibility towards various \nextensions. \n</p>"}, "author": "Kristiaan Pelckmans", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228466", "id": "tag:google.com,2005:reader/item/000000032e88d0ec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Distributed Bayesian Piecewise Sparse Linear Models. (arXiv:1711.02368v1 [cs.AI])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02368"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02368", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The importance of interpretability of machine learning models has been \nincreasing due to emerging enterprise predictive analytics, threat of data \nprivacy, accountability of artificial intelligence in society, and so on. \nPiecewise linear models have been actively studied to achieve both accuracy and \ninterpretability. They often produce competitive accuracy against \nstate-of-the-art non-linear methods. In addition, their representations (i.e., \nrule-based segmentation plus sparse linear formula) are often preferred by \ndomain experts. A disadvantage of such models, however, is high computational \ncost for simultaneous determinations of the number of \"pieces\" and cardinality \nof each linear predictor, which has restricted their applicability to \nmiddle-scale data sets. This paper proposes a distributed factorized asymptotic \nBayesian (FAB) inference of learning piece-wise sparse linear models on \ndistributed memory architectures. The distributed FAB inference solves the \nsimultaneous model selection issue without communicating $O(N)$ data where N is \nthe number of training samples and achieves linear scale-out against the number \nof CPU cores. Experimental results demonstrate that the distributed FAB \ninference achieves high prediction accuracy and performance scalability with \nboth synthetic and benchmark data. \n</p>"}, "author": "Masato Asahara, Ryohei Fujimaki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228465", "id": "tag:google.com,2005:reader/item/000000032e88d0fc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "A Tutorial on Canonical Correlation Methods. (arXiv:1711.02391v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02391"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02391", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c037bee\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c037bee&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Canonical correlation analysis is a family of multivariate statistical \nmethods for the analysis of paired sets of variables. Since its proposition, \ncanonical correlation analysis has for instance been extended to extract \nrelations between two sets of variables when the sample size is insufficient in \nrelation to the data dimensionality, when the relations have been considered to \nbe non-linear, and when the dimensionality is too large for human \ninterpretation. This tutorial explains the theory of canonical correlation \nanalysis including its regularised, kernel, and sparse variants. Additionally, \nthe deep and Bayesian CCA extensions are briefly reviewed. Together with the \nnumerical examples, this overview provides a coherent compendium on the \napplicability of the variants of canonical correlation analysis. By bringing \ntogether techniques for solving the optimisation problems, evaluating the \nstatistical significance and generalisability of the canonical correlation \nmodel, and interpreting the relations, we hope that this article can serve as a \nhands-on tool for applying canonical correlation methods in data analysis. \n</p>"}, "author": "Viivi Uurtio, Jo&#xe3;o M. Monteiro, Jaz Kandola, John Shawe-Taylor, Delmiro Fernandez-Reyes, Juho Rousu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228464", "id": "tag:google.com,2005:reader/item/000000032e88d104", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Gaussian Lower Bound for the Information Bottleneck Limit. (arXiv:1711.02421v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02421"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02421", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c0b34d9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c0b34d9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The Information Bottleneck (IB) is a conceptual method for extracting the \nmost compact, yet informative, representation of a set of variables, with \nrespect to the target. It generalizes the notion of minimal sufficient \nstatistics from classical parametric statistics to a broader \ninformation-theoretic sense. The IB curve defines the optimal trade-off between \nrepresentation complexity and its predictive power. Specifically, it is \nachieved by minimizing the level of mutual information (MI) between the \nrepresentation and the original variables, subject to a minimal level of MI \nbetween the representation and the target. This problem is shown to be in \ngeneral NP hard. One important exception is the multivariate Gaussian case, for \nwhich the Gaussian IB (GIB) is known to obtain an analytical closed form \nsolution, similar to Canonical Correlation Analysis (CCA). In this work we \nintroduce a Gaussian lower bound to the IB curve; we find an embedding of the \ndata which maximizes its \"Gaussian part\", on which we apply the GIB. This \nembedding provides an efficient (and practical) representation of any arbitrary \ndata-set (in the IB sense), which in addition holds the favorable properties of \na Gaussian distribution. Importantly, we show that the optimal Gaussian \nembedding is bounded from above by non-linear CCA. This allows a fundamental \nlimit for our ability to Gaussianize arbitrary data-sets and solve complex \nproblems by linear methods. \n</p>"}, "author": "Amichai Painsky, Naftali Tishby", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228463", "id": "tag:google.com,2005:reader/item/000000032e88d11d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Cortical microcircuits as gated-recurrent neural networks. (arXiv:1711.02448v1 [q-bio.NC])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02448"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02448", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Cortical circuits exhibit intricate recurrent architectures that are \nremarkably similar across different brain areas. Such stereotyped structure \nsuggests the existence of common computational principles. However, such \nprinciples have remained largely elusive. Inspired by gated-memory networks, \nnamely long short-term memory networks (LSTMs), we introduce a recurrent neural \nnetwork in which information is gated through inhibitory cells that are \nsubtractive (subLSTM). We propose a natural mapping of subLSTMs onto known \ncanonical excitatory-inhibitory cortical microcircuits. Our empirical \nevaluation across sequential image classification and language modelling tasks \nshows that subLSTM units can achieve similar performance to LSTM units. These \nresults suggest that cortical circuits can be optimised to solve complex \ncontextual problems and proposes a novel view on their computational function. \nOverall our work provides a step towards unifying recurrent networks as used in \nmachine learning with their biological counterparts. \n</p>"}, "author": "Rui Ponte Costa, Yannis M. Assael, Brendan Shillingford, Nando de Freitas, Tim P. Vogels", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228462", "id": "tag:google.com,2005:reader/item/000000032e88d126", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Bayesian model and dimension reduction for uncertainty propagation: applications in random media. (arXiv:1711.02475v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02475"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02475", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Well-established methods for the solution of stochastic partial differential \nequations (SPDEs) typically struggle in problems with high-dimensional \ninputs/outputs. Such difficulties are only amplified in large-scale \napplications where even a few tens of full-order model runs are impractical. \nWhile dimensionality reduction can alleviate some of these issues, it is not \nknown which and how many features of the (high-dimensional) input are actually \npredictive of the (high-dimensional) output. In this paper, we advocate a \nBayesian formulation that is capable of performing simultaneous dimension and \nmodel-order reduction. It consists of a component that encodes the \nhigh-dimensional input into a low-dimensional set of feature functions by \nemploying sparsity-enforcing priors and a decoding component that makes use of \nthe solution of a coarse-grained model in order to reconstruct that of the \nfull-order model. Both components are represented with latent variables in a \nprobabilistic graphical model and are simultaneously trained using Stochastic \nVariational Inference methods. The model is capable of quantifying the \npredictive uncertainty due to the information loss that unavoidably takes place \nin any model-order/dimension reduction as well as the uncertainty arising from \nfinite-sized training datasets. We demonstrate its capabilities in the context \nof random media where fine-scale fluctuations can give rise to random inputs \nwith tens of thousands of variables. With a few tens of full-order model \nsimulations, the proposed model is capable of identifying salient physical \nfeatures and produce sharp predictions under different boundary conditions of \nthe full output which itself consists of thousands of components. \n</p>"}, "author": "Constantin Grigo, Phaedon-Stelios Koutsourelakis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228461", "id": "tag:google.com,2005:reader/item/000000032e88d131", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Grafting for Combinatorial Boolean Model using Frequent Itemset Mining. (arXiv:1711.02478v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02478"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02478", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper introduces the combinatorial Boolean model (CBM), which is defined \nas the class of linear combinations of conjunctions of Boolean attributes. This \npaper addresses the issue of learning CBM from labeled data. CBM is of high \nknowledge interpretability but na\\\"{i}ve learning of it requires exponentially \nlarge computation time with respect to data dimension and sample size. To \novercome this computational difficulty, we propose an algorithm GRAB (GRAfting \nfor Boolean datasets), which efficiently learns CBM within the \n$L_1$-regularized loss minimization framework. The key idea of GRAB is to \nreduce the loss minimization problem to the weighted frequent itemset mining, \nin which frequent patterns are efficiently computable. We employ benchmark \ndatasets to empirically demonstrate that GRAB is effective in terms of \ncomputational efficiency, prediction accuracy and knowledge discovery. \n</p>"}, "author": "Taito Lee, Shin Matsushima, Kenji Yamanishi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228460", "id": "tag:google.com,2005:reader/item/000000032e88d136", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Non-monotone Continuous DR-submodular Maximization: Structure and Algorithms. (arXiv:1711.02515v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02515"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02515", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>DR-submodular continuous functions are important objectives with wide \nreal-world applications spanning MAP inference in determinantal point processes \n(DPPs), and mean-field inference for probabilistic submodular models, amongst \nothers. DR-submodularity captures a subclass of non-convex functions that \nenables both exact minimization and approximate maximization in polynomial \ntime. \n</p> \n<p>In this work we study the problem of maximizing non-monotone DR-submodular \ncontinuous functions under general down-closed convex constraints. We start by \ninvestigating geometric properties that underlie such objectives, e.g., a \nstrong relation between (approximately) stationary points and global optimum is \nproved. These properties are then used to devise two optimization algorithms \nwith provable guarantees. Concretely, we first devise a \"two-phase\" algorithm \nwith $1/4$ approximation guarantee. This algorithm allows the use of existing \nmethods for finding (approximately) stationary points as a subroutine, thus, \nharnessing recent progress in non-convex optimization. Then we present a \nnon-monotone Frank-Wolfe variant with $1/e$ approximation guarantee and \nsublinear convergence rate. Finally, we extend our approach to a broader class \nof generalized DR-submodular continuous functions, which captures a wider \nspectrum of applications. Our theoretical findings are validated on synthetic \nand real-world problem instances. \n</p>"}, "author": "An Bian, Kfir Y. Levy, Andreas Krause, Joachim M. Buhmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228459", "id": "tag:google.com,2005:reader/item/000000032e88d140", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Online Learning for Changing Environments using Coin Betting. (arXiv:1711.02545v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02545"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02545", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A key challenge in online learning is that classical algorithms can be slow \nto adapt to changing environments. Recent studies have proposed \"meta\" \nalgorithms that convert any online learning algorithm to one that is adaptive \nto changing environments, where the adaptivity is analyzed in a quantity called \nthe strongly-adaptive regret. This paper describes a new meta algorithm that \nhas a strongly-adaptive regret bound that is a factor of $\\sqrt{\\log(T)}$ \nbetter than other algorithms with the same time complexity, where $T$ is the \ntime horizon. We also extend our algorithm to achieve a first-order (i.e., \ndependent on the observed losses) strongly-adaptive regret bound for the first \ntime, to our knowledge. At its heart is a new parameter-free algorithm for the \nlearning with expert advice (LEA) problem in which experts sometimes do not \noutput advice for consecutive time steps (i.e., \\emph{sleeping} experts). This \nalgorithm is derived by a reduction from optimal algorithms for the so-called \ncoin betting problem. Empirical results show that our algorithm outperforms \nstate-of-the-art methods in both learning with expert advice and metric \nlearning scenarios. \n</p>"}, "author": "Kwang-Sung Jun, Francesco Orabona, Stephen Wright, Rebecca Willett", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228458", "id": "tag:google.com,2005:reader/item/000000032e88d14c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Streaming Robust Submodular Maximization: A Partitioned Thresholding Approach. (arXiv:1711.02598v1 [cs.DS])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02598"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02598", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the classical problem of maximizing a monotone submodular function \nsubject to a cardinality constraint k, with two additional twists: (i) elements \narrive in a streaming fashion, and (ii) m items from the algorithm's memory are \nremoved after the stream is finished. We develop a robust submodular algorithm \nSTAR-T. It is based on a novel partitioning structure and an exponentially \ndecreasing thresholding rule. STAR-T makes one pass over the data and retains a \nshort but robust summary. We show that after the removal of any m elements from \nthe obtained summary, a simple greedy algorithm STAR-T-GREEDY that runs on the \nremaining elements achieves a constant-factor approximation guarantee. In two \ndifferent data summarization tasks, we demonstrate that it matches or \noutperforms existing greedy and streaming methods, even if they are allowed the \nbenefit of knowing the removed subset in advance. \n</p>"}, "author": "Slobodan Mitrovi&#x107;, Ilija Bogunovic, Ashkan Norouzi-Fard, Jakub Tarnawski, Volkan Cevher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228457", "id": "tag:google.com,2005:reader/item/000000032e88d157", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Moonshine: Distilling with Cheap Convolutions. (arXiv:1711.02613v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02613"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02613", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Model distillation compresses a trained machine learning model, such as a \nneural network, into a smaller alternative such that it could be easily \ndeployed in a resource limited setting. Unfortunately, this requires \nengineering two architectures: a student architecture smaller than the first \nteacher architecture but trained to emulate it. In this paper, we present a \ndistillation strategy that produces a student architecture that is a simple \ntransformation of the teacher architecture. Recent model distillation methods \nallow us to preserve most of the performance of the trained model after \nreplacing convolutional blocks with a cheap alternative. In addition, \ndistillation by attention transfer provides student network performance that is \nbetter than training that student architecture directly on data. \n</p>"}, "author": "Elliot J. Crowley, Gavin Gray, Amos Storkey", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228456", "id": "tag:google.com,2005:reader/item/000000032e88d168", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Theoretical limitations of Encoder-Decoder GAN architectures. (arXiv:1711.02651v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02651"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02651", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Encoder-decoder GANs architectures (e.g., BiGAN and ALI) seek to add an \ninference mechanism to the GANs setup, consisting of a small encoder deep net \nthat maps data-points to their succinct encodings. The intuition is that being \nforced to train an encoder alongside the usual generator forces the system to \nlearn meaningful mappings from the code to the data-point and vice-versa, which \nshould improve the learning of the target distribution and ameliorate \nmode-collapse. It should also yield meaningful codes that are useful as \nfeatures for downstream tasks. The current paper shows rigorously that even on \nreal-life distributions of images, the encode-decoder GAN training objectives \n(a) cannot prevent mode collapse; i.e. the objective can be near-optimal even \nwhen the generated distribution has low and finite support (b) cannot prevent \nlearning meaningless codes for data -- essentially white noise. Thus if \nencoder-decoder GANs do indeed work then it must be due to reasons as yet not \nunderstood, since the training objective can be low even for meaningless \nsolutions. \n</p>"}, "author": "Sanjeev Arora, Andrej Risteski, Yi Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228455", "id": "tag:google.com,2005:reader/item/000000032e88d16e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Neural system identification for large populations separating \"what\" and \"where\". (arXiv:1711.02653v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02653"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02653", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c0b372d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c0b372d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Neuroscientists classify neurons into different types that perform similar \ncomputations at different locations in the visual field. Traditional methods \nfor neural system identification do not capitalize on this separation of 'what' \nand 'where'. Learning deep convolutional feature spaces that are shared among \nmany neurons provides an exciting path forward, but the architectural design \nneeds to account for data limitations: While new experimental techniques enable \nrecordings from thousands of neurons, experimental time is limited so that one \ncan sample only a small fraction of each neuron's response space. Here, we show \nthat a major bottleneck for fitting convolutional neural networks (CNNs) to \nneural data is the estimation of the individual receptive field locations, a \nproblem that has been scratched only at the surface thus far. We propose a CNN \narchitecture with a sparse readout layer factorizing the spatial (where) and \nfeature (what) dimensions. Our network scales well to thousands of neurons and \nshort recordings and can be trained end-to-end. We evaluate this architecture \non ground-truth data to explore the challenges and limitations of CNN-based \nsystem identification. Moreover, we show that our network model outperforms \ncurrent state-of-the art system identification models of mouse primary visual \ncortex. \n</p>"}, "author": "David A. Klindt, Alexander S. Ecker, Thomas Euler, Matthias Bethge", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228454", "id": "tag:google.com,2005:reader/item/000000032e88d176", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Off-policy evaluation for slate recommendation. (arXiv:1605.04812v3 [cs.LG] UPDATED)", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1605.04812"}], "alternate": [{"href": "http://arxiv.org/abs/1605.04812", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper studies the evaluation of policies that recommend an ordered set \nof items (e.g., a ranking) based on some context---a common scenario in web \nsearch, ads, and recommendation. We build on techniques from combinatorial \nbandits to introduce a new practical estimator that uses logged data to \nestimate a policy's performance. A thorough empirical evaluation on real-world \ndata reveals that our estimator is accurate in a variety of settings, including \nas a subroutine in a learning-to-rank task, where it achieves competitive \nperformance. We derive conditions under which our estimator is unbiased---these \nconditions are weaker than prior heuristics for slate evaluation---and \nexperimentally demonstrate a smaller bias than parametric approaches, even when \nthese conditions are violated. Finally, our theory and experiments also show \nexponential savings in the amount of required data compared with general \nunbiased estimators. \n</p>"}, "author": "Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miroslav Dud&#xed;k, John Langford, Damien Jose, Imed Zitouni", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228433", "id": "tag:google.com,2005:reader/item/000000032e88d308", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "MMD GAN: Towards Deeper Understanding of Moment Matching Network. (arXiv:1705.08584v2 [cs.LG] CROSS LISTED)", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.08584"}], "alternate": [{"href": "http://arxiv.org/abs/1705.08584", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative moment matching network (GMMN) is a deep generative model that \ndiffers from Generative Adversarial Network (GAN) by replacing the \ndiscriminator in GAN with a two-sample test based on kernel maximum mean \ndiscrepancy (MMD). Although some theoretical guarantees of MMD have been \nstudied, the empirical performance of GMMN is still not as competitive as that \nof GAN on challenging and large benchmark datasets. The computational \nefficiency of GMMN is also less desirable in comparison with GAN, partially due \nto its requirement for a rather large batch size during the training. In this \npaper, we propose to improve both the model expressiveness of GMMN and its \ncomputational efficiency by introducing adversarial kernel learning techniques, \nas the replacement of a fixed Gaussian kernel in the original GMMN. The new \napproach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. \nThe new distance measure in MMD GAN is a meaningful loss that enjoys the \nadvantage of weak topology and can be optimized via gradient descent with \nrelatively small batch sizes. In our evaluation on multiple benchmark datasets, \nincluding MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN \nsignificantly outperforms GMMN, and is competitive with other representative \nGAN works. \n</p>"}, "author": "Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, Barnab&#xe1;s P&#xf3;czos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510106836167", "timestampUsec": "1510106836166582", "id": "tag:google.com,2005:reader/item/000000032e4743f3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Bounding and Counting Linear Regions of Deep Neural Networks. (arXiv:1711.02114v1 [cs.LG])", "published": 1510106836, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02114"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02114", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we study the representational power of deep neural networks \n(DNN) that belong to the family of piecewise-linear (PWL) functions, based on \nPWL activation units such as rectifier or maxout. We investigate the complexity \nof such networks by studying the number of linear regions of the PWL function. \nTypically, a PWL function from a DNN can be seen as a large family of linear \nfunctions acting on millions of such regions. We directly build upon the work \nof Montufar et al. (2014) and Raghu et al. (2017) by refining the upper and \nlower bounds on the number of linear regions for rectified and maxout networks. \nIn addition to achieving tighter bounds, we also develop a novel method to \nperform exact enumeration or counting of the number of linear regions with a \nmixed-integer linear formulation that maps the input space to output. We use \nthis new capability to visualize how the number of linear regions change while \ntraining DNNs. \n</p>"}, "author": "Thiago Serra, Christian Tjandraatmadja, Srikumar Ramalingam", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510106836167", "timestampUsec": "1510106836166581", "id": "tag:google.com,2005:reader/item/000000032e4743f6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net. (arXiv:1711.02282v1 [stat.ML])", "published": 1510106836, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02282"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02282", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a novel method to directly learn a stochastic transition operator \nwhose repeated application provides generated samples. Traditional undirected \ngraphical models approach this problem indirectly by learning a Markov chain \nmodel whose stationary distribution obeys detailed balance with respect to a \nparameterized energy function. The energy function is then modified so the \nmodel and data distributions match, with no guarantee on the number of steps \nrequired for the Markov chain to converge. Moreover, the detailed balance \ncondition is highly restrictive: energy based models corresponding to neural \nnetworks must have symmetric weights, unlike biological neural circuits. In \ncontrast, we develop a method for directly learning arbitrarily parameterized \ntransition operators capable of expressing non-equilibrium stationary \ndistributions that violate detailed balance, thereby enabling us to learn more \nbiologically plausible asymmetric neural networks and more general non-energy \nbased dynamical systems. The proposed training objective, which we derive via \nprincipled variational methods, encourages the transition operator to \"walk \nback\" in multi-step trajectories that start at data-points, as quickly as \npossible back to the original data points. We present a series of experimental \nresults illustrating the soundness of the proposed approach, Variational \nWalkback (VW), on the MNIST, CIFAR-10, SVHN and CelebA datasets, demonstrating \nsuperior samples compared to earlier attempts to learn a transition operator. \nWe also show that although each rapid training trajectory is limited to a \nfinite but variable number of steps, our transition operator continues to \ngenerate good samples well past the length of such trajectories, thereby \ndemonstrating the match of its non-equilibrium stationary distribution to the \ndata distribution. Source Code: <a href=\"http://github.com/anirudh9119/walkback_nips17\">this http URL</a> \n</p>"}, "author": "Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510106836167", "timestampUsec": "1510106836166580", "id": "tag:google.com,2005:reader/item/000000032e4743fd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?. (arXiv:1711.02301v1 [cs.AI])", "published": 1510106836, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02301"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02301", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep reinforcement learning has achieved many recent successes, but our \nunderstanding of its strengths and limitations is hampered by the lack of rich \nenvironments in which we can fully characterize optimal behavior, and \ncorrespondingly diagnose individual actions against such a characterization. \nHere we consider a family of combinatorial games, arising from work of Erdos, \nSelfridge, and Spencer, and we propose their use as environments for evaluating \nand comparing different approaches to reinforcement learning. These games have \na number of appealing features: they are challenging for current learning \napproaches, but they form (i) a low-dimensional, simply parametrized \nenvironment where (ii) there is a linear closed form solution for optimal \nbehavior from any state, and (iii) the difficulty of the game can be tuned by \nchanging environment parameters in an interpretable way. We use these \nErdos-Selfridge-Spencer games not only to compare different algorithms, but \nalso to compare approaches based on supervised and reinforcement learning, to \nanalyze the power of multi-agent approaches in improving performance, and to \nevaluate generalization to environments outside the training set. \n</p>"}, "author": "Maithra Raghu, Alex Irpan, Jacob Andreas, Robert Kleinberg, Quoc V. Le, Jon Kleinberg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510106836167", "timestampUsec": "1510106836166579", "id": "tag:google.com,2005:reader/item/000000032e474400", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks. (arXiv:1711.02326v1 [cs.AI])", "published": 1510106836, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02326"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02326", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A major drawback of backpropagation through time (BPTT) is the difficulty of \nlearning long-term dependencies, coming from having to propagate credit \ninformation backwards through every single step of the forward computation. \nThis makes BPTT both computationally impractical and biologically implausible. \nFor this reason, full backpropagation through time is rarely used on long \nsequences, and truncated backpropagation through time is used as a heuristic. \nHowever, this usually leads to biased estimates of the gradient in which longer \nterm dependencies are ignored. Addressing this issue, we propose an alternative \nalgorithm, Sparse Attentive Backtracking, which might also be related to \nprinciples used by brains to learn long-term dependencies. Sparse Attentive \nBacktracking learns an attention mechanism over the hidden states of the past \nand selectively backpropagates through paths with high attention weights. This \nallows the model to learn long term dependencies while only backtracking for a \nsmall number of time steps, not just from the recent past but also from \nattended relevant past states. \n</p>"}, "author": "Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Laurent Charlin, Chris Pal, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510106836167", "timestampUsec": "1510106836166578", "id": "tag:google.com,2005:reader/item/000000032e474405", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Beetle Antennae Search without Parameter Tuning (BAS-WPT) for Multi-objective Optimization. (arXiv:1711.02395v1 [cs.NE])", "published": 1510106836, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02395"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02395", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Beetle antennae search (BAS) is an efficient meta-heuristic algorithm \ninspired by foraging behaviors of beetles. This algorithm includes several \nparameters for tuning and the existing results are limited to solve single \nobjective optimization. This work pushes forward the research on BAS by \nproviding one variant that releases the tuning parameters and is able to handle \nmulti-objective optimization. This new approach applies normalization to \nsimplify the original algorithm and uses a penalty function to exploit \ninfeasible solutions with low constraint violation to solve the constraint \noptimization problem. Extensive experimental studies are carried out and the \nresults reveal efficacy of the proposed approach to constraint handling. \n</p>"}, "author": "Xiangyuan Jiang, Shuai Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510106836167", "timestampUsec": "1510106836166577", "id": "tag:google.com,2005:reader/item/000000032e47440b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Cortical microcircuits as gated-recurrent neural networks. (arXiv:1711.02448v1 [q-bio.NC])", "published": 1510106836, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02448"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02448", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Cortical circuits exhibit intricate recurrent architectures that are \nremarkably similar across different brain areas. Such stereotyped structure \nsuggests the existence of common computational principles. However, such \nprinciples have remained largely elusive. Inspired by gated-memory networks, \nnamely long short-term memory networks (LSTMs), we introduce a recurrent neural \nnetwork in which information is gated through inhibitory cells that are \nsubtractive (subLSTM). We propose a natural mapping of subLSTMs onto known \ncanonical excitatory-inhibitory cortical microcircuits. Our empirical \nevaluation across sequential image classification and language modelling tasks \nshows that subLSTM units can achieve similar performance to LSTM units. These \nresults suggest that cortical circuits can be optimised to solve complex \ncontextual problems and proposes a novel view on their computational function. \nOverall our work provides a step towards unifying recurrent networks as used in \nmachine learning with their biological counterparts. \n</p>"}, "author": "Rui Ponte Costa, Yannis M. Assael, Brendan Shillingford, Nando de Freitas, Tim P. Vogels", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188538", "id": "tag:google.com,2005:reader/item/000000032df4f63c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Mandolin: A Knowledge Discovery Framework for the Web of Data. (arXiv:1711.01283v1 [cs.DB])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01283"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01283", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Markov Logic Networks join probabilistic modeling with first-order logic and \nhave been shown to integrate well with the Semantic Web foundations. While \nseveral approaches have been devised to tackle the subproblems of rule mining, \ngrounding, and inference, no comprehensive workflow has been proposed so far. \nIn this paper, we fill this gap by introducing a framework called Mandolin, \nwhich implements a workflow for knowledge discovery specifically on RDF \ndatasets. Our framework imports knowledge from referenced graphs, creates \nsimilarity relationships among similar literals, and relies on state-of-the-art \ntechniques for rule mining, grounding, and inference computation. We show that \nour best configuration scales well and achieves at least comparable results \nwith respect to other statistical-relational-learning algorithms on link \nprediction. \n</p>"}, "author": "Tommaso Soru, Diego Esteves, Edgard Marx, Axel-Cyrille Ngonga Ngomo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188537", "id": "tag:google.com,2005:reader/item/000000032df4f645", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Discovering More Precise Process Models from Event Logs by Filtering Out Chaotic Activities. (arXiv:1711.01287v1 [cs.DB])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01287"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01287", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c0b397f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c0b397f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Process Discovery is concerned with the automatic generation of a process \nmodel that describes a business process from execution data of that business \nprocess. Real life event logs can contain chaotic activities. These activities \nare independent of the state of the process and can, therefore, happen at \nrather arbitrary points in time. We show that the presence of such chaotic \nactivities in an event log heavily impacts the quality of the process models \nthat can be discovered with process discovery techniques. The current modus \noperandi for filtering activities from event logs is to simply filter out \ninfrequent activities. We show that frequency-based filtering of activities \ndoes not solve the problems that are caused by chaotic activities. Moreover, we \npropose a novel technique to filter out chaotic activities from event logs. We \nevaluate this technique on a collection of seventeen real-life event logs that \noriginate from both the business process management domain and the smart home \nenvironment domain. As demonstrated, the developed activity filtering methods \nenable the discovery of process models that are more behaviorally specific \ncompared to process models that are discovered using standard frequency-based \nfiltering. \n</p>"}, "author": "Niek Tax, Natalia Sidorova, Wil M. P. van der Aalst", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188536", "id": "tag:google.com,2005:reader/item/000000032df4f64d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Decentralised firewall for malware detection. (arXiv:1711.01353v1 [cs.CR])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01353"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01353", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c128466\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c128466&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper describes the design and development of a decentralized firewall \nsystem powered by a novel malware detection engine. The firewall is built using \nblockchain technology. The detection engine aims to classify Portable \nExecutable (PE) files as malicious or benign. File classification is carried \nout using a deep belief neural network (DBN) as the detection engine. Our \napproach is to model the files as grayscale images and use the DBN to classify \nthose images into the aforementioned two classes. An extensive data set of \n10,000 files is used to train the DBN. Validation is carried out using 4,000 \nfiles previously unexposed to the network. The final result of whether to allow \nor block a file is obtained by arriving at a proof of work based consensus in \nthe blockchain network. \n</p>"}, "author": "Saurabh Raje, Shyamal Vaderia, Neil Wilson, Rudrakh Panigrahi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188535", "id": "tag:google.com,2005:reader/item/000000032df4f655", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Guiding the search in continuous state-action spaces by learning an action sampling distribution from off-target samples. (arXiv:1711.01391v1 [cs.AI])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01391"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01391", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In robotics, it is essential to be able to plan efficiently in \nhigh-dimensional continuous state-action spaces for long horizons. For such \ncomplex planning problems, unguided uniform sampling of actions until a path to \na goal is found is hopelessly inefficient, and gradient-based approaches often \nfall short when the optimization manifold of a given problem is not smooth. In \nthis paper we present an approach that guides the search of a state-space \nplanner, such as A*, by learning an action-sampling distribution that can \ngeneralize across different instances of a planning problem. The motivation is \nthat, unlike typical learning approaches for planning for continuous action \nspace that estimate a policy, an estimated action sampler is more robust to \nerror since it has a planner to fall back on. We use a Generative Adversarial \nNetwork (GAN), and address an important issue: search experience consists of a \nrelatively large number of actions that are not on a solution path and a \nrelatively small number of actions that actually are on a solution path. We \nintroduce a new technique, based on an importance-ratio estimation method, for \nusing samples from a non-target distribution to make GAN learning more \ndata-efficient. We provide theoretical guarantees and empirical evaluation in \nthree challenging continuous robot planning problems to illustrate the \neffectiveness of our algorithm. \n</p>"}, "author": "Beomjoon Kim, Leslie Pack Kaelbling, Tomas Lozano-Perez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188534", "id": "tag:google.com,2005:reader/item/000000032df4f663", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "The Case for Meta-Cognitive Machine Learning: On Model Entropy and Concept Formation in Deep Learning. (arXiv:1711.01431v1 [cs.AI])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01431"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01431", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine learning is usually defined in behaviourist terms, where external \nvalidation is the primary mechanism of learning. In this paper, I argue for a \nmore holistic interpretation in which finding more probable, efficient and \nabstract representations is as central to learning as performance. In other \nwords, machine learning should be extended with strategies to reason over its \nown learning process, leading to so-called meta-cognitive machine learning. As \nsuch, the de facto definition of machine learning should be reformulated in \nthese intrinsically multi-objective terms, taking into account not only the \ntask performance but also internal learning objectives. To this end, we suggest \na \"model entropy function\" to be defined that quantifies the efficiency of the \ninternal learning processes. It is conjured that the minimization of this model \nentropy leads to concept formation. Besides philosophical aspects, some initial \nillustrations are included to support the claims. \n</p>"}, "author": "Johan Loeckx", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188533", "id": "tag:google.com,2005:reader/item/000000032df4f672", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Searching for Biophysically Realistic Parameters for Dynamic Neuron Models by Genetic Algorithms from Calcium Imaging Recording. (arXiv:1711.01436v1 [q-bio.QM])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01436"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01436", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Individual Neurons in the nervous systems exploit various dynamics. To \ncapture these dynamics for single neurons, we tune the parameters of an \nelectrophysiological model of nerve cells, to fit experimental data obtained by \ncalcium imaging. A search for the biophysical parameters of this model is \nperformed by means of a genetic algorithm, where the model neuron is exposed to \na predefined input current representing overall inputs from other parts of the \nnervous system. The algorithm is then constrained for keeping the ion-channel \ncurrents within reasonable ranges, while producing the best fit to a calcium \nimaging time series of the AVA interneuron, from the brain of the soil-worm, C. \nelegans. Our settings enable us to project a set of biophysical parameters to \nthe the neuron kinetics observed in neuronal imaging. \n</p>"}, "author": "Magdalena Fuchs, Manuel Zimmer, Radu Grosu, Ramin M. Hasani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188532", "id": "tag:google.com,2005:reader/item/000000032df4f677", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Ensembles of Multiple Models and Architectures for Robust Brain Tumour Segmentation. (arXiv:1711.01468v1 [cs.CV])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01468"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01468", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning approaches such as convolutional neural nets have consistently \noutperformed previous methods on challenging tasks such as dense, semantic \nsegmentation. However, the various proposed networks perform differently, with \nbehaviour largely influenced by architectural choices and training settings. \nThis paper explores Ensembles of Multiple Models and Architectures (EMMA) for \nrobust performance through aggregation of predictions from a wide range of \nmethods. The approach reduces the influence of the meta-parameters of \nindividual models and the risk of overfitting the configuration to a particular \ndatabase. EMMA can be seen as an unbiased, generic deep learning model which is \nshown to yield excellent performance, winning the first position in the BRATS \n2017 competition among 50+ participating teams. \n</p>"}, "author": "Konstantinos Kamnitsas, Wenjia Bai, Enzo Ferrante, Steven McDonagh, Matthew Sinclair, Nick Pawlowski, Martin Rajchl, Matthew Lee, Bernhard Kainz, Daniel Rueckert, Ben Glocker", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188531", "id": "tag:google.com,2005:reader/item/000000032df4f681", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Composing Meta-Policies for Autonomous Driving Using Hierarchical Deep Reinforcement Learning. (arXiv:1711.01503v1 [cs.AI])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01503"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01503", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Rather than learning new control policies for each new task, it is possible, \nwhen tasks share some structure, to compose a \"meta-policy\" from previously \nlearned policies. This paper reports results from experiments using Deep \nReinforcement Learning on a continuous-state, discrete-action autonomous \ndriving simulator. We explore how Deep Neural Networks can represent \nmeta-policies that switch among a set of previously learned policies, \nspecifically in settings where the dynamics of a new scenario are composed of a \nmixture of previously learned dynamics and where the state observation is \npossibly corrupted by sensing noise. We also report the results of experiments \nvarying dynamics mixes, distractor policies, magnitudes/distributions of \nsensing noise, and obstacles. In a fully observed experiment, the meta-policy \nlearning algorithm achieves 2.6x the reward achieved by the next best policy \ncomposition technique with 80% less exploration. In a partially observed \nexperiment, the meta-policy learning algorithm converges after 50 iterations \nwhile a direct application of RL fails to converge even after 200 iterations. \n</p>"}, "author": "Richard Liaw, Sanjay Krishnan, Animesh Garg, Daniel Crankshaw, Joseph E. Gonzalez, Ken Goldberg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188530", "id": "tag:google.com,2005:reader/item/000000032df4f68f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Semantic Web Today: From Oil Rigs to Panama Papers. (arXiv:1711.01518v1 [cs.AI])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01518"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01518", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The next leap on the internet has already started as Semantic Web. At its \ncore, Semantic Web transforms the document oriented web to a data oriented web \nenriched with semantics embedded as metadata. This change in perspective \ntowards the web offers numerous benefits for vast amount of data intensive \nindustries that are bound to the web and its related applications. The \nindustries are diverse as they range from Oil &amp; Gas exploration to the \ninvestigative journalism, and everything in between. This paper discusses eight \ndifferent industries which currently reap the benefits of Semantic Web. The \npaper also offers a future outlook into Semantic Web applications and discusses \nthe areas in which Semantic Web would play a key role in the future. \n</p>"}, "author": "Rivindu Perera, Parma Nand, Boris Bacic, Wen-Hsin Yang, Kazuhiro Seki, Radek Burget", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188529", "id": "tag:google.com,2005:reader/item/000000032df4f6a3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "HPX Smart Executors. (arXiv:1711.01519v1 [cs.DC])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01519"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01519", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The performance of many parallel applications depends on loop-level \nparallelism. However, manually parallelizing all loops may result in degrading \nparallel performance, as some of them cannot scale desirably to a large number \nof threads. In addition, the overheads of manually tuning loop parameters might \nprevent an application from reaching its maximum parallel performance. We \nillustrate how machine learning techniques can be applied to address these \nchallenges. In this research, we develop a framework that is able to \nautomatically capture the static and dynamic information of a loop. Moreover, \nwe advocate a novel method by introducing HPX smart executors for determining \nthe execution policy, chunk size, and prefetching distance of an HPX loop to \nachieve higher possible performance by feeding static information captured \nduring compilation and runtime-based dynamic information to our learning model. \nOur evaluated execution results show that using these smart executors can speed \nup the HPX execution process by around 12%-35% for the Matrix Multiplication, \nStream and $2D$ Stencil benchmarks compared to setting their HPX loop's \nexecution policy/parameters manually or using HPX auto-parallelization \ntechniques. \n</p>"}, "author": "Zahra Khatami, Lukas Troska, Hartmut Kaiser, J. Ramanujam, Adrian Serio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188528", "id": "tag:google.com,2005:reader/item/000000032df4f6ae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Fisher-Rao Metric, Geometry, and Complexity of Neural Networks. (arXiv:1711.01530v1 [cs.LG])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01530"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01530", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the relationship between geometry and capacity measures for deep \nneural networks from an invariance viewpoint. We introduce a new notion of \ncapacity --- the Fisher-Rao norm --- that possesses desirable invariance \nproperties and is motivated by Information Geometry. We discover an analytical \ncharacterization of the new capacity measure, through which we establish \nnorm-comparison inequalities and further show that the new measure serves as an \numbrella for several existing norm-based complexity measures. We discuss upper \nbounds on the generalization error induced by the proposed measure. Extensive \nnumerical experiments on CIFAR-10 support our theoretical findings. Our \ntheoretical analysis rests on a key structural lemma about partial derivatives \nof multi-layer rectifier networks. \n</p>"}, "author": "Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, James Stokes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188527", "id": "tag:google.com,2005:reader/item/000000032df4f6b5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Double Q($\\sigma$) and Q($\\sigma, \\lambda$): Unifying Reinforcement Learning Control Algorithms. (arXiv:1711.01569v1 [cs.AI])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01569"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01569", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c128734\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c128734&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Temporal-difference (TD) learning is an important field in reinforcement \nlearning. Sarsa and Q-Learning are among the most used TD algorithms. The \nQ($\\sigma$) algorithm (Sutton and Barto (2017)) unifies both. This paper \nextends the Q($\\sigma$) algorithm to an online multi-step algorithm Q($\\sigma, \n\\lambda$) using eligibility traces and introduces Double Q($\\sigma$) as the \nextension of Q($\\sigma$) to double learning. Experiments suggest that the new \nQ($\\sigma, \\lambda$) algorithm can outperform the classical TD control methods \nSarsa($\\lambda$), Q($\\lambda$) and Q($\\sigma$). \n</p>"}, "author": "Markus Dumke", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188526", "id": "tag:google.com,2005:reader/item/000000032df4f6c1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning. (arXiv:1711.01577v1 [stat.ML])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01577"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01577", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Long Short-Term Memory (LSTM) is a popular approach to boosting the ability \nof Recurrent Neural Networks to store longer term temporal information. The \ncapacity of an LSTM network can be increased by widening and adding layers. \nHowever, usually the former introduces additional parameters, while the latter \nincreases the runtime. As an alternative we propose the Tensorized LSTM in \nwhich the hidden states are represented by tensors and updated via a \ncross-layer convolution. By increasing the tensor size, the network can be \nwidened efficiently without additional parameters since the parameters are \nshared across different locations in the tensor; by delaying the output, the \nnetwork can be deepened implicitly with little additional runtime since deep \ncomputations for each timestep are merged into temporal computations of the \nsequence. Experiments conducted on five challenging sequence learning tasks \nshow the potential of the proposed model. \n</p>"}, "author": "Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, David Barber", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188525", "id": "tag:google.com,2005:reader/item/000000032df4f6c8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Strategies for Conceptual Change in Convolutional Neural Networks. (arXiv:1711.01634v1 [cs.LG])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01634"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01634", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A remarkable feature of human beings is their capacity for creative \nbehaviour, referring to their ability to react to problems in ways that are \nnovel, surprising, and useful. Transformational creativity is a form of \ncreativity where the creative behaviour is induced by a transformation of the \nactor's conceptual space, that is, the representational system with which the \nactor interprets its environment. In this report, we focus on ways of adapting \nsystems of learned representations as they switch from performing one task to \nperforming another. We describe an experimental comparison of multiple \nstrategies for adaptation of learned features, and evaluate how effectively \neach of these strategies realizes the adaptation, in terms of the amount of \ntraining, and in terms of their ability to cope with restricted availability of \ntraining data. We show, among other things, that across handwritten digits, \nnatural images, and classical music, adaptive strategies are systematically \nmore effective than a baseline method that starts learning from scratch. \n</p>"}, "author": "Maarten Grachten, Carlos Eduardo Cancino Chac&#xf3;n", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188524", "id": "tag:google.com,2005:reader/item/000000032df4f6d4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Multilingual Speech Recognition With A Single End-To-End Model. (arXiv:1711.01694v1 [eess.AS])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01694"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01694", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Training a conventional automatic speech recognition (ASR) system to support \nmultiple languages is challenging because the sub-word unit, lexicon and word \ninventories are typically language specific. In contrast, sequence-to-sequence \nmodels are well suited for multilingual ASR because they encapsulate an \nacoustic, pronunciation and language model jointly in a single network. In this \nwork we present a single sequence-to-sequence ASR model trained on 9 different \nIndian languages, which have very little overlap in their scripts. \nSpecifically, we take a union of language-specific grapheme sets and train a \ngrapheme-based sequence-to-sequence model jointly on data from all languages. \nWe find that this model, which is not explicitly given any information about \nlanguage identity, improves recognition performance by 21% relative compared to \nanalogous sequence-to-sequence models trained on each language individually. By \nmodifying the model to accept a language identifier as an additional input \nfeature, we further improve performance by an additional 7% relative and \neliminate confusion between different languages. \n</p>"}, "author": "Shubham Toshniwal, Tara N. Sainath, Ron J. Weiss, Bo Li, Pedro Moreno, Eugene Weinstein, Kanishka Rao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188523", "id": "tag:google.com,2005:reader/item/000000032df4f6de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "RoboCupSimData: A RoboCup soccer research dataset. (arXiv:1711.01703v1 [cs.AI])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01703"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01703", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>RoboCup is an international scientific robot competition in which teams of \nmultiple robots compete against each other. Its different leagues provide many \nsources of robotics data, that can be used for further analysis and application \nof machine learning. This paper describes a large dataset from games of some of \nthe top teams (from 2016 and 2017) in RoboCup Soccer Simulation League (2D), \nwhere teams of 11 robots (agents) compete against each other. Overall, we used \n10 different teams to play each other, resulting in 45 unique pairings. For \neach pairing, we ran 25 matches (of 10mins), leading to 1125 matches or more \nthan 180 hours of game play. The generated CSV files are 17GB of data (zipped), \nor 229GB (unzipped). The dataset is unique in the sense that it contains both \nthe ground truth data (global, complete, noise-free information of all objects \non the field), as well as the noisy, local and incomplete percepts of each \nrobot. These data are made available as CSV files, as well as in the original \nsoccer simulator formats. \n</p>"}, "author": "Olivia Michael, Oliver Obst, Falk Schmidsberger, Frieder Stolzenburg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188522", "id": "tag:google.com,2005:reader/item/000000032df4f6e7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Coding-theorem Like Behaviour and Emergence of the Universal Distribution from Resource-bounded Algorithmic Probability. (arXiv:1711.01711v1 [cs.IT])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01711"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01711", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Introduced by Solomonoff and Levin, the seminal concept of Algorithmic \nProbability (AP) and the Universal Distribution (UD) predicts the way in which \nstrings distribute as the result of running 'random' computer programs. \nPreviously referred to as `miraculous' because of its surprisingly powerful \nproperties and applications as the optimal theoretical solution to the \nchallenge of induction and inference, approximations to AP and the UD are of \nthe greatest importance in computer science and science in general. Here we are \ninterested in the emergence, rates of convergence, and the Coding-theorem like \nbehaviour as a marker of acting AP emerging in subuniversal models of \ncomputation. To this end, we investigate empirical distributions of computer \nprograms of weaker computational power according to the Chomsky hierarchy. We \nintroduce measures of algorithmic probability and algorithmic complexity based \nupon resource-bounded computation compared to previously thoroughly \ninvestigated distributions produced from the output distribution of Turing \nmachines. The approach allows for numerical approximations to algorithmic \n(Kolmogorov-Chaitin) complexity-based estimations at each of the levels of a \ncomputational hierarchy. We demonstrate that all these estimations are \ncorrelated in rank and they converge both in rank as a function of \ncomputational power despite the fundamental differences of each computational \nmodel. \n</p>"}, "author": "Hector Zenil, Liliana Badillo, Santiago Hern&#xe1;ndez-Orozco, Francisco Hern&#xe1;ndez-Quiroz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188521", "id": "tag:google.com,2005:reader/item/000000032df4f6f1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "KGAN: How to Break The Minimax Game in GAN. (arXiv:1711.01744v1 [cs.LG])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01744"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01744", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative Adversarial Networks (GANs) were intuitively and attractively \nexplained under the perspective of game theory, wherein two involving parties \nare a discriminator and a generator. In this game, the task of the \ndiscriminator is to discriminate the real and generated (i.e., fake) data, \nwhilst the task of the generator is to generate the fake data that maximally \nconfuses the discriminator. In this paper, we propose a new viewpoint for GANs, \nwhich is termed as the minimizing general loss viewpoint. This viewpoint shows \na connection between the general loss of a classification problem regarding a \nconvex loss function and a f-divergence between the true and fake data \ndistributions. Mathematically, we proposed a setting for the classification \nproblem of the true and fake data, wherein we can prove that the general loss \nof this classification problem is exactly the negative f-divergence for a \ncertain convex function f. This allows us to interpret the problem of learning \nthe generator for dismissing the f-divergence between the true and fake data \ndistributions as that of maximizing the general loss which is equivalent to the \nmin-max problem in GAN if the Logistic loss is used in the classification \nproblem. However, this viewpoint strengthens GANs in two ways. First, it allows \nus to employ any convex loss function for the discriminator. Second, it \nsuggests that rather than limiting ourselves in NN-based discriminators, we can \nalternatively utilize other powerful families. Bearing this viewpoint, we then \npropose using the kernel-based family for discriminators. This family has two \nappealing features: i) a powerful capacity in classifying non-linear nature \ndata and ii) being convex in the feature space. Using the convexity of this \nfamily, we can further develop Fenchel duality to equivalently transform the \nmax-min problem to the max-max dual problem. \n</p>"}, "author": "Trung Le, Tu Dinh Nguyen, Dinh Phung", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188520", "id": "tag:google.com,2005:reader/item/000000032df4f6f5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning Solving Procedure for Artificial Neural Network. (arXiv:1711.01754v1 [cs.AI])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01754"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01754", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is expected that progress toward true artificial intelligence will be \nachieved through the emergence of a system that integrates representation \nlearning and complex reasoning (LeCun et al. 2015). In response to this \nprediction, research has been conducted on implementing the symbolic reasoning \nof a von Neumann computer in an artificial neural network (Graves et al. 2016; \nGraves et al. 2014; Reed et al. 2015). However, these studies have many \nlimitations in realizing neural-symbolic integration (Jaeger. 2016). Here, we \npresent a new learning paradigm: a learning solving procedure (LSP) that learns \nthe procedure for solving complex problems. This is not accomplished merely by \nlearning input-output data, but by learning algorithms through a solving \nprocedure that obtains the output as a sequence of tasks for a given input \nproblem. The LSP neural network system not only learns simple problems of \naddition and multiplication, but also the algorithms of complicated problems, \nsuch as complex arithmetic expression, sorting, and Hanoi Tower. To realize \nthis, the LSP neural network structure consists of a deep neural network and \nlong short-term memory, which are recursively combined. Through \nexperimentation, we demonstrate the efficiency and scalability of LSP and its \nvalidity as a mechanism of complex reasoning. \n</p>"}, "author": "Ju-Hong Lee, Moon-Ju Kang, Bumghi Choi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188519", "id": "tag:google.com,2005:reader/item/000000032df4f6fd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "A Foundry of Human Activities and Infrastructures. (arXiv:1711.01927v1 [cs.AI])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01927"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01927", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Direct representation knowledgebases can enhance and even provide an \nalternative to document-centered digital libraries. Here we consider realist \nsemantic modeling of everyday activities and infrastructures in such \nknowledgebases. Because we want to integrate a wide variety of topics, a \ncollection of ontologies (a foundry) and a range of other knowledge resources \nare needed. We first consider modeling the routine procedures that support \nhuman activities and technologies. Next, we examine the interactions of \ntechnologies with aspects of social organization. Then, we consider approaches \nand issues for developing and validating explanations of the relationships \namong various entities. \n</p>"}, "author": "Robert B. Allen, Eunsang Yang, Tatsawan Timakum", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188518", "id": "tag:google.com,2005:reader/item/000000032df4f701", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Hi, how can I help you?: Automating enterprise IT support help desks. (arXiv:1711.02012v1 [cs.CL])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02012"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02012", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Question answering is one of the primary challenges of natural language \nunderstanding. In realizing such a system, providing complex long answers to \nquestions is a challenging task as opposed to factoid answering as the former \nneeds context disambiguation. The different methods explored in the literature \ncan be broadly classified into three categories namely: 1) classification \nbased, 2) knowledge graph based and 3) retrieval based. Individually, none of \nthem address the need of an enterprise wide assistance system for an IT support \nand maintenance domain. In this domain the variance of answers is large ranging \nfrom factoid to structured operating procedures; the knowledge is present \nacross heterogeneous data sources like application specific documentation, \nticket management systems and any single technique for a general purpose \nassistance is unable to scale for such a landscape. To address this, we have \nbuilt a cognitive platform with capabilities adopted for this domain. Further, \nwe have built a general purpose question answering system leveraging the \nplatform that can be instantiated for multiple products, technologies in the \nsupport domain. The system uses a novel hybrid answering model that \norchestrates across a deep learning classifier, a knowledge graph based context \ndisambiguation module and a sophisticated bag-of-words search system. This \norchestration performs context switching for a provided question and also does \na smooth hand-off of the question to a human expert if none of the automated \ntechniques can provide a confident answer. This system has been deployed across \n675 internal enterprise IT support and maintenance projects. \n</p>"}, "author": "Senthil Mani, Neelamadhav Gantayat, Rahul Aralikatte, Monika Gupta, Sampath Dechu, Anush Sankaran, Shreya Khare, Barry Mitchell, Hemamalini Subramanian, Hema Venkatarangan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188517", "id": "tag:google.com,2005:reader/item/000000032df4f717", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon. (arXiv:1711.02013v1 [cs.CL])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02013"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02013", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c128a78\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c128a78&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a neural language model capable of unsupervised syntactic \nstructure induction. The model leverages the structure information to form \nbetter semantic representations and better language modeling. Standard \nrecurrent neural networks are limited by their structure and fail to \nefficiently use syntactic information. On the other hand, tree-structured \nrecursive networks usually require additional structural supervision at the \ncost of human expert annotation. In this paper, We propose a novel neural \nlanguage model, called the Parsing-Reading-Predict Networks (PRPN), that can \nsimultaneously induce the syntactic structure from unannotated sentences and \nleverage the inferred structure to learn a better language model. In our model, \nthe gradient can be directly back-propagated from the language model loss into \nthe neural parsing network. Experiments show that the proposed model can \ndiscover the underlying syntactic structure and achieve state-of-the-art \nperformance on word/character-level language model tasks. \n</p>"}, "author": "Yikang Shen, Zhouhan Lin, Chin-Wei Huang, Aaron Courville", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188516", "id": "tag:google.com,2005:reader/item/000000032df4f721", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm. (arXiv:1711.02017v1 [cs.NE])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02017"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02017", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c19332a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c19332a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Neural networks (NNs) have begun to have a pervasive impact on various \napplications of machine learning. However, the problem of finding an optimal NN \narchitecture for large applications has remained open for several decades. \nConventional approaches search for the optimal NN architecture through \nextensive trial-and-error. Such a procedure is quite inefficient. In addition, \nthe generated NN architectures incur substantial redundancy. To address these \nproblems, we propose an NN synthesis tool (NeST) that automatically generates \nvery compact architectures for a given dataset. NeST starts with a seed NN \narchitecture. It iteratively tunes the architecture with gradient-based growth \nand magnitude-based pruning of neurons and connections. Our experimental \nresults show that NeST yields accurate yet very compact NNs with a wide range \nof seed architecture selection. For example, for the LeNet-300-100 (LeNet-5) NN \narchitecture derived from the MNIST dataset, we reduce network parameters by \n34.1x (74.3x) and floating-point operations (FLOPs) by 35.8x (43.7x). For the \nAlexNet NN architecture derived from the ImageNet dataset, we reduce network \nparameters by 15.7x and FLOPs by 4.6x. All these results are the current \nstate-of-the-art for these architectures. \n</p>"}, "author": "Xiaoliang Dai, Hongxu Yin, Niraj K. Jha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510063178493", "timestampUsec": "1510063178493371", "id": "tag:google.com,2005:reader/item/000000032dea80ce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Accelerating Training of Deep Neural Networks via Sparse Edge Processing. (arXiv:1711.01343v1 [cs.NE])", "published": 1510063179, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01343"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01343", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a reconfigurable hardware architecture for deep neural networks \n(DNNs) capable of online training and inference, which uses algorithmically \npre-determined, structured sparsity to significantly lower memory and \ncomputational requirements. This novel architecture introduces the notion of \nedge-processing to provide flexibility and combines junction pipelining and \noperational parallelization to speed up training. The overall effect is to \nreduce network complexity by factors up to 30x and training time by up to 35x \nrelative to GPUs, while maintaining high fidelity of inference results. This \nhas the potential to enable extensive parameter searches and development of the \nlargely unexplored theoretical foundation of DNNs. The architecture \nautomatically adapts itself to different network sizes given available hardware \nresources. As proof of concept, we show results obtained for different bit \nwidths. \n</p>"}, "author": "Sourya Dey, Yinan Shao, Keith M. Chugg, Peter A. Beerel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510063178493", "timestampUsec": "1510063178493370", "id": "tag:google.com,2005:reader/item/000000032dea80d5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Language as a matrix product state. (arXiv:1711.01416v1 [cs.CL])", "published": 1510063179, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01416"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01416", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a statistical model for natural language that begins by \nconsidering language as a monoid, then representing it in complex matrices with \na compatible translation invariant probability measure. We interpret the \nprobability measure as arising via the Born rule from a translation invariant \nmatrix product state. \n</p>"}, "author": "Vasily Pestun, John Terilla, Yiannis Vlassopoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510063178493", "timestampUsec": "1510063178493369", "id": "tag:google.com,2005:reader/item/000000032dea80dd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Searching for Biophysically Realistic Parameters for Dynamic Neuron Models by Genetic Algorithms from Calcium Imaging Recording. (arXiv:1711.01436v1 [q-bio.QM])", "published": 1510063179, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01436"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01436", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Individual Neurons in the nervous systems exploit various dynamics. To \ncapture these dynamics for single neurons, we tune the parameters of an \nelectrophysiological model of nerve cells, to fit experimental data obtained by \ncalcium imaging. A search for the biophysical parameters of this model is \nperformed by means of a genetic algorithm, where the model neuron is exposed to \na predefined input current representing overall inputs from other parts of the \nnervous system. The algorithm is then constrained for keeping the ion-channel \ncurrents within reasonable ranges, while producing the best fit to a calcium \nimaging time series of the AVA interneuron, from the brain of the soil-worm, C. \nelegans. Our settings enable us to project a set of biophysical parameters to \nthe the neuron kinetics observed in neuronal imaging. \n</p>"}, "author": "Magdalena Fuchs, Manuel Zimmer, Radu Grosu, Ramin M. Hasani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510063178493", "timestampUsec": "1510063178493368", "id": "tag:google.com,2005:reader/item/000000032dea80e7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning. (arXiv:1711.01577v1 [stat.ML])", "published": 1510063179, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01577"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01577", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Long Short-Term Memory (LSTM) is a popular approach to boosting the ability \nof Recurrent Neural Networks to store longer term temporal information. The \ncapacity of an LSTM network can be increased by widening and adding layers. \nHowever, usually the former introduces additional parameters, while the latter \nincreases the runtime. As an alternative we propose the Tensorized LSTM in \nwhich the hidden states are represented by tensors and updated via a \ncross-layer convolution. By increasing the tensor size, the network can be \nwidened efficiently without additional parameters since the parameters are \nshared across different locations in the tensor; by delaying the output, the \nnetwork can be deepened implicitly with little additional runtime since deep \ncomputations for each timestep are merged into temporal computations of the \nsequence. Experiments conducted on five challenging sequence learning tasks \nshow the potential of the proposed model. \n</p>"}, "author": "Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, David Barber", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510063178493", "timestampUsec": "1510063178493367", "id": "tag:google.com,2005:reader/item/000000032dea80f2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm. (arXiv:1711.02017v1 [cs.NE])", "published": 1510063179, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02017"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02017", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks (NNs) have begun to have a pervasive impact on various \napplications of machine learning. However, the problem of finding an optimal NN \narchitecture for large applications has remained open for several decades. \nConventional approaches search for the optimal NN architecture through \nextensive trial-and-error. Such a procedure is quite inefficient. In addition, \nthe generated NN architectures incur substantial redundancy. To address these \nproblems, we propose an NN synthesis tool (NeST) that automatically generates \nvery compact architectures for a given dataset. NeST starts with a seed NN \narchitecture. It iteratively tunes the architecture with gradient-based growth \nand magnitude-based pruning of neurons and connections. Our experimental \nresults show that NeST yields accurate yet very compact NNs with a wide range \nof seed architecture selection. For example, for the LeNet-300-100 (LeNet-5) NN \narchitecture derived from the MNIST dataset, we reduce network parameters by \n34.1x (74.3x) and floating-point operations (FLOPs) by 35.8x (43.7x). For the \nAlexNet NN architecture derived from the ImageNet dataset, we reduce network \nparameters by 15.7x and FLOPs by 4.6x. All these results are the current \nstate-of-the-art for these architectures. \n</p>"}, "author": "Xiaoliang Dai, Hongxu Yin, Niraj K. Jha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663570", "id": "tag:google.com,2005:reader/item/000000032dc77689", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Implicit Weight Uncertainty in Neural Networks. (arXiv:1711.01297v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01297"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01297", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We interpret HyperNetworks within the framework of variational inference \nwithin implicit distributions. Our method, Bayes by Hypernet, is able to model \na richer variational distribution than previous methods. Experiments show that \nit achieves comparable predictive performance on the MNIST classification task \nwhile providing higher predictive uncertainties compared to MC-Dropout and \nregular maximum likelihood training. \n</p>"}, "author": "Nick Pawlowski, Martin Rajchl, Ben Glocker", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663569", "id": "tag:google.com,2005:reader/item/000000032dc776ed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "NeuralFDR: Learning Discovery Thresholds from Hypothesis Features. (arXiv:1711.01312v3 [stat.ME] UPDATED)", "published": 1510229029, "updated": 1510229038, "canonical": [{"href": "http://arxiv.org/abs/1711.01312"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01312", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>As datasets grow richer, an important challenge is to leverage the full \nfeatures in the data to maximize the number of useful discoveries while \ncontrolling for false positives. We address this problem in the context of \nmultiple hypotheses testing, where for each hypothesis, we observe a p-value \nalong with a set of features specific to that hypothesis. For example, in \ngenetic association studies, each hypothesis tests the correlation between a \nvariant and the trait. We have a rich set of features for each variant (e.g. \nits location, conservation, epigenetics etc.) which could inform how likely the \nvariant is to have a true association. However popular testing approaches, such \nas Benjamini-Hochberg's procedure (BH) and independent hypothesis weighting \n(IHW), either ignore these features or assume that the features are categorical \nor uni-variate. We propose a new algorithm, NeuralFDR, which automatically \nlearns a discovery threshold as a function of all the hypothesis features. We \nparametrize the discovery threshold as a neural network, which enables flexible \nhandling of multi-dimensional discrete and continuous features as well as \nefficient end-to-end optimization. We prove that NeuralFDR has strong false \ndiscovery rate (FDR) guarantees, and show that it makes substantially more \ndiscoveries in synthetic and real datasets. Moreover, we demonstrate that the \nlearned discovery threshold is directly interpretable. \n</p>"}, "author": "Fei Xia, Martin J. Zhang, James Zou, David Tse", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663568", "id": "tag:google.com,2005:reader/item/000000032dc7777c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Generalized Linear Model Regression under Distance-to-set Penalties. (arXiv:1711.01341v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01341"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01341", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Estimation in generalized linear models (GLM) is complicated by the presence \nof constraints. One can handle constraints by maximizing a penalized \nlog-likelihood. Penalties such as the lasso are effective in high dimensions, \nbut often lead to unwanted shrinkage. This paper explores instead penalizing \nthe squared distance to constraint sets. Distance penalties are more flexible \nthan algebraic and regularization penalties, and avoid the drawback of \nshrinkage. To optimize distance penalized objectives, we make use of the \nmajorization-minimization principle. Resulting algorithms constructed within \nthis framework are amenable to acceleration and come with global convergence \nguarantees. Applications to shape constraints, sparse regression, and \nrank-restricted matrix regression on synthetic and real data showcase strong \nempirical performance, even under non-convex constraints. \n</p>"}, "author": "Jason Xu, Eric C. Chi, Kenneth Lange", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663567", "id": "tag:google.com,2005:reader/item/000000032dc77819", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Automatic Differentiation for Tensor Algebras. (arXiv:1711.01348v1 [cs.SC])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01348"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01348", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c193661\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c193661&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Kjolstad et. al. proposed a tensor algebra compiler. It takes expressions \nthat define a tensor element-wise, such as $f_{ij}(a,b,c,d) = \n\\exp\\left[-\\sum_{k=0}^4 \\left((a_{ik}+b_{jk})^2\\, c_{ii} + d_{i+k}^3 \\right) \n\\right]$, and generates the corresponding compute kernel code. \n</p> \n<p>For machine learning, especially deep learning, it is often necessary to \ncompute the gradient of a loss function $l(a,b,c,d)=l(f(a,b,c,d))$ with respect \nto parameters $a,b,c,d$. If tensor compilers are to be applied in this field, \nit is necessary to derive expressions for the derivatives of element-wise \ndefined tensors, i.e. expressions for $(da)_{ik}=\\partial l/\\partial a_{ik}$. \n</p> \n<p>When the mapping between function indices and argument indices is not 1:1, \nspecial attention is required. For the function $f_{ij} (x) = x_i^2$, the \nderivative of the loss is $(dx)_i=\\partial l/\\partial x_i=\\sum_j \n(df)_{ij}2x_i$; the sum is necessary because index $j$ does not appear in the \nindices of $f$. Another example is $f_{i}(x)=x_{ii}^2$, where $x$ is a matrix; \nhere we have $(dx)_{ij}=\\delta_{ij}(df)_i2x_{ii}$; the Kronecker delta is \nnecessary because the derivative is zero for off-diagonal elements. Another \nindexing scheme is used by $f_{ij}(x)=\\exp x_{i+j}$; here the correct \nderivative is $(dx)_{k}=\\sum_i (df)_{i,k-i} \\exp x_{k}$, where the range of the \nsum must be chosen appropriately. \n</p> \n<p>In this publication we present an algorithm that can handle any case in which \nthe indices of an argument are an arbitrary linear combination of the indices \nof the function, thus all the above examples can be handled. Sums (and their \nranges) and Kronecker deltas are automatically inserted into the derivatives as \nnecessary. Additionally, the indices are transformed, if required (as in the \nlast example). The algorithm outputs a symbolic expression that can be \nsubsequently fed into a tensor algebra compiler. \n</p> \n<p>Source code is provided. \n</p>"}, "author": "Sebastian Urban, Patrick van der Smagt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663566", "id": "tag:google.com,2005:reader/item/000000032dc778e7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Language as a matrix product state. (arXiv:1711.01416v1 [cs.CL])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01416"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01416", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a statistical model for natural language that begins by \nconsidering language as a monoid, then representing it in complex matrices with \na compatible translation invariant probability measure. We interpret the \nprobability measure as arising via the Born rule from a translation invariant \nmatrix product state. \n</p>"}, "author": "Vasily Pestun, John Terilla, Yiannis Vlassopoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663565", "id": "tag:google.com,2005:reader/item/000000032dc77924", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "The Case for Meta-Cognitive Machine Learning: On Model Entropy and Concept Formation in Deep Learning. (arXiv:1711.01431v1 [cs.AI])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01431"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01431", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine learning is usually defined in behaviourist terms, where external \nvalidation is the primary mechanism of learning. In this paper, I argue for a \nmore holistic interpretation in which finding more probable, efficient and \nabstract representations is as central to learning as performance. In other \nwords, machine learning should be extended with strategies to reason over its \nown learning process, leading to so-called meta-cognitive machine learning. As \nsuch, the de facto definition of machine learning should be reformulated in \nthese intrinsically multi-objective terms, taking into account not only the \ntask performance but also internal learning objectives. To this end, we suggest \na \"model entropy function\" to be defined that quantifies the efficiency of the \ninternal learning processes. It is conjured that the minimization of this model \nentropy leads to concept formation. Besides philosophical aspects, some initial \nillustrations are included to support the claims. \n</p>"}, "author": "Johan Loeckx", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663564", "id": "tag:google.com,2005:reader/item/000000032dc77966", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Distribution-Preserving k-Anonymity. (arXiv:1711.01514v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01514"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01514", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Preserving the privacy of individuals by protecting their sensitive \nattributes is an important consideration during microdata release. However, it \nis equally important to preserve the quality or utility of the data for at \nleast some targeted workloads. We propose a novel framework for privacy \npreservation based on the k-anonymity model that is ideally suited for \nworkloads that require preserving the probability distribution of the \nquasi-identifier variables in the data. Our framework combines the principles \nof distribution-preserving quantization and k-member clustering, and we \nspecialize it to two variants that respectively use intra-cluster and Gaussian \ndithering of cluster centers to achieve distribution preservation. We perform \ntheoretical analysis of the proposed schemes in terms of distribution \npreservation, and describe their utility in workloads such as covariate shift \nand transfer learning where such a property is necessary. Using extensive \nexperiments on real-world Medical Expenditure Panel Survey data, we demonstrate \nthe merits of our algorithms over standard k-anonymization for a hallmark \nhealth care application where an insurance company wishes to understand the \nrisk in entering a new market. Furthermore, by empirically quantifying the \nreidentification risk, we also show that the proposed approaches indeed \nmaintain k-anonymity. \n</p>"}, "author": "Dennis Wei, Karthikeyan Natesan Ramamurthy, Kush R. Varshney", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663563", "id": "tag:google.com,2005:reader/item/000000032dc779ce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Fisher-Rao Metric, Geometry, and Complexity of Neural Networks. (arXiv:1711.01530v1 [cs.LG])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01530"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01530", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the relationship between geometry and capacity measures for deep \nneural networks from an invariance viewpoint. We introduce a new notion of \ncapacity --- the Fisher-Rao norm --- that possesses desirable invariance \nproperties and is motivated by Information Geometry. We discover an analytical \ncharacterization of the new capacity measure, through which we establish \nnorm-comparison inequalities and further show that the new measure serves as an \numbrella for several existing norm-based complexity measures. We discuss upper \nbounds on the generalization error induced by the proposed measure. Extensive \nnumerical experiments on CIFAR-10 support our theoretical findings. Our \ntheoretical analysis rests on a key structural lemma about partial derivatives \nof multi-layer rectifier networks. \n</p>"}, "author": "Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, James Stokes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663562", "id": "tag:google.com,2005:reader/item/000000032dc77a35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Wasserstein Auto-Encoders. (arXiv:1711.01558v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01558"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01558", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building \na generative model of the data distribution. WAE minimizes a penalized form of \nthe Wasserstein distance between the model distribution and the target \ndistribution, which leads to a different regularizer than the one used by the \nVariational Auto-Encoder (VAE). This regularizer encourages the encoded \ntraining distribution to match the prior. We compare our algorithm with several \nother techniques and show that it is a generalization of adversarial \nauto-encoders (AAE). Our experiments show that WAE shares many of the \nproperties of VAEs (stable training, encoder-decoder architecture, nice latent \nmanifold structure) while generating samples of better quality, as measured by \nthe FID score. \n</p>"}, "author": "Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, Bernhard Schoelkopf", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663561", "id": "tag:google.com,2005:reader/item/000000032dc77a98", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Stochastic Submodular Maximization: The Case of Coverage Functions. (arXiv:1711.01566v1 [cs.LG])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01566"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01566", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Stochastic optimization of continuous objectives is at the heart of modern \nmachine learning. However, many important problems are of discrete nature and \noften involve submodular objectives. We seek to unleash the power of stochastic \ncontinuous optimization, namely stochastic gradient descent and its variants, \nto such discrete problems. We first introduce the problem of stochastic \nsubmodular optimization, where one needs to optimize a submodular objective \nwhich is given as an expectation. Our model captures situations where the \ndiscrete objective arises as an empirical risk (e.g., in the case of \nexemplar-based clustering), or is given as an explicit stochastic model (e.g., \nin the case of influence maximization in social networks). By exploiting that \ncommon extensions act linearly on the class of submodular functions, we employ \nprojected stochastic gradient ascent and its variants in the continuous domain, \nand perform rounding to obtain discrete solutions. We focus on the rich and \nwidely used family of weighted coverage functions. We show that our approach \nyields solutions that are guaranteed to match the optimal approximation \nguarantees, while reducing the computational cost by several orders of \nmagnitude, as we demonstrate empirically. \n</p>"}, "author": "Mohammad Reza Karimi, Mario Lucic, Hamed Hassani, Andreas Krause", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663560", "id": "tag:google.com,2005:reader/item/000000032dc77af0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Double Q($\\sigma$) and Q($\\sigma, \\lambda$): Unifying Reinforcement Learning Control Algorithms. (arXiv:1711.01569v1 [cs.AI])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01569"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01569", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Temporal-difference (TD) learning is an important field in reinforcement \nlearning. Sarsa and Q-Learning are among the most used TD algorithms. The \nQ($\\sigma$) algorithm (Sutton and Barto (2017)) unifies both. This paper \nextends the Q($\\sigma$) algorithm to an online multi-step algorithm Q($\\sigma, \n\\lambda$) using eligibility traces and introduces Double Q($\\sigma$) as the \nextension of Q($\\sigma$) to double learning. Experiments suggest that the new \nQ($\\sigma, \\lambda$) algorithm can outperform the classical TD control methods \nSarsa($\\lambda$), Q($\\lambda$) and Q($\\sigma$). \n</p>"}, "author": "Markus Dumke", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663559", "id": "tag:google.com,2005:reader/item/000000032dc77bac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning. (arXiv:1711.01577v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01577"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01577", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Long Short-Term Memory (LSTM) is a popular approach to boosting the ability \nof Recurrent Neural Networks to store longer term temporal information. The \ncapacity of an LSTM network can be increased by widening and adding layers. \nHowever, usually the former introduces additional parameters, while the latter \nincreases the runtime. As an alternative we propose the Tensorized LSTM in \nwhich the hidden states are represented by tensors and updated via a \ncross-layer convolution. By increasing the tensor size, the network can be \nwidened efficiently without additional parameters since the parameters are \nshared across different locations in the tensor; by delaying the output, the \nnetwork can be deepened implicitly with little additional runtime since deep \ncomputations for each timestep are merged into temporal computations of the \nsequence. Experiments conducted on five challenging sequence learning tasks \nshow the potential of the proposed model. \n</p>"}, "author": "Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, David Barber", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663558", "id": "tag:google.com,2005:reader/item/000000032dc77c29", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Multilayer tensor factorization with applications to recommender systems. (arXiv:1711.01598v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01598"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01598", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recommender systems have been widely adopted by electronic commerce and \nentertainment industries for individualized prediction and recommendation, \nwhich benefit consumers and improve business intelligence. In this article, we \npropose an innovative method, namely the recommendation engine of multilayers \n(REM), for tensor recommender systems. The proposed method utilizes the \nstructure of a tensor response to integrate information from multiple modes, \nand creates an additional layer of nested latent factors to accommodate \nbetween-subjects dependency. One major advantage is that the proposed method is \nable to address the \"cold-start\" issue in the absence of information from new \ncustomers, new products or new contexts. Specifically, it provides more \neffective recommendations through sub-group information. To achieve scalable \ncomputation, we develop a new algorithm for the proposed method, which \nincorporates a maximum block improvement strategy into the cyclic \nblockwise-coordinate-descent algorithm. In theory, we investigate both \nalgorithmic properties for global and local convergence, along with the \nasymptotic consistency of estimated parameters. Finally, the proposed method is \napplied in simulations and IRI marketing data with 116 million observations of \nproduct sales. Numerical studies demonstrate that the proposed method \noutperforms existing competitors in the literature. \n</p>"}, "author": "Xuan Bi, Annie Qu, Xiaotong Shen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663557", "id": "tag:google.com,2005:reader/item/000000032dc77c8d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Approximating Partition Functions in Constant Time. (arXiv:1711.01655v1 [cs.LG])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01655"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01655", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c1938c8\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c1938c8&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We study approximations of the partition function of dense graphical models. \nPartition functions of graphical models play a fundamental role is statistical \nphysics, in statistics and in machine learning. Two of the main methods for \napproximating the partition function are Markov Chain Monte Carlo and \nVariational Methods. An impressive body of work in mathematics, physics and \ntheoretical computer science provides conditions under which Markov Chain Monte \nCarlo methods converge in polynomial time. These methods often lead to \npolynomial time approximation algorithms for the partition function in cases \nwhere the underlying model exhibits correlation decay. There are very few \ntheoretical guarantees for the performance of variational methods. One \nexception is recent results by Risteski (2016) who considered dense graphical \nmodels and showed that using variational methods, it is possible to find an \n$O(\\epsilon n)$ additive approximation to the log partition function in time \n$n^{O(1/\\epsilon^2)}$ even in a regime where correlation decay does not hold. \n</p> \n<p>We show that under essentially the same conditions, an $O(\\epsilon n)$ \nadditive approximation of the log partition function can be found in constant \ntime, independent of $n$. In particular, our results cover dense Ising and \nPotts models as well as dense graphical models with $k$-wise interaction. They \nalso apply for low threshold rank models. \n</p> \n<p>To the best of our knowledge, our results are the first to give a constant \ntime approximation to log partition functions and the first to use the \nalgorithmic regularity lemma for estimating partition functions. As an \napplication of our results we derive a constant time algorithm for \napproximating the magnetization of Ising and Potts model on dense graphs. \n</p>"}, "author": "Vishesh Jain, Frederic Koehler, Elchanan Mossel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663556", "id": "tag:google.com,2005:reader/item/000000032dc77cd9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Provenance and Pseudo-Provenance for Seeded Learning-Based Automated Test Generation. (arXiv:1711.01661v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01661"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01661", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c208be4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c208be4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Many methods for automated software test generation, including some that \nexplicitly use machine learning (and some that use ML more broadly conceived) \nderive new tests from existing tests (often referred to as seeds). Often, the \nseed tests from which new tests are derived are manually constructed, or at \nleast simpler than the tests that are produced as the final outputs of such \ntest generators. We propose annotation of generated tests with a provenance \n(trail) showing how individual generated tests of interest (especially failing \ntests) derive from seed tests, and how the population of generated tests \nrelates to the original seed tests. In some cases, post-processing of generated \ntests can invalidate provenance information, in which case we also propose a \nmethod for attempting to construct \"pseudo-provenance\" describing how the tests \ncould have been (partly) generated from seeds. \n</p>"}, "author": "Alex Groce, Josie Holmes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663555", "id": "tag:google.com,2005:reader/item/000000032dc77d67", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Estimation of Low-Rank Matrices via Approximate Message Passing. (arXiv:1711.01682v1 [math.ST])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01682"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01682", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Consider the problem of estimating a low-rank symmetric matrix when its \nentries are perturbed by Gaussian noise, a setting that is also known as \n`spiked model' or `deformed Wigner matrix.' If the empirical distribution of \nthe entries of the spikes is known, optimal estimators that exploit this \nknowledge can substantially outperform simple spectral approaches. Recent work \ncharacterizes the asymptotic accuracy of Bayes-optimal estimators in the \nhigh-dimensional limit. In this paper we present a practical algorithm that can \nachieve Bayes-optimal accuracy above the spectral threshold. A bold conjecture \nfrom statistical physics posits that no polynomial-time algorithm achieves \noptimal error below the same threshold (unless the best estimator is trivial). \n</p> \n<p>Our approach uses Approximate Message Passing (AMP) in conjunction with a \nspectral initialization. AMP algorithms have proved successful in a variety of \nstatistical estimation tasks, and are amenable to exact asymptotic analysis via \nstate evolution. Unfortunately, state evolution is uninformative when the \nalgorithm is initialized near an unstable fixed point, as is often happens in \nlow-rank matrix estimation problems. We develop a a new analysis of AMP that \nallows for spectral initializations, and builds on a decoupling between the \noutlier eigenvectors and the bulk in the spiked random matrix model. \n</p> \n<p>Our main theorem is general and applies beyond matrix estimation. However, we \nuse it to derive detailed predictions for the problem of estimating a rank-one \nmatrix in noise. Special cases of these problem are closely related -- via \nuniversality arguments -- to the network community detection problem for two \nasymmetric communities. As a further illustration, we consider the example of a \nblock-constant low-rank matrix with symmetric blocks, which we refer to as \n`Gaussian Block Model'. \n</p>"}, "author": "Andrea Montanari, Ramji Venkataramanan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663554", "id": "tag:google.com,2005:reader/item/000000032dc77dbb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Memory-efficient Kernel PCA via Partial Matrix Sampling and Nonconvex Optimization: a Model-free Analysis of Local Minima. (arXiv:1711.01742v1 [math.OC])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01742"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01742", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Kernel PCA is a widely used nonlinear dimension reduction technique in \nmachine learning, but storing the kernel matrix is notoriously challenging when \nthe sample size is large. Inspired by [YPCC16], where the idea of partial \nmatrix sampling followed by nonconvex optimization is proposed for matrix \ncompletion and robust PCA, we apply a similar approach to memory-efficient \nKernel PCA. In theory, with no assumptions on the kernel matrix in terms of \neigenvalues or eigenvectors, we established a model-free theory for the \nlow-rank approximation based on any local minimum of the proposed objective \nfunction. As interesting byproducts, when the underlying positive semidefinite \nmatrix is assumed to be low-rank and highly structured, corollaries of our main \ntheorem improve the state-of-the-art results [GLM16, GJZ17] for nonconvex \nmatrix completion with no spurious local minima. Numerical experiments also \nshow that our approach is competitive in terms of approximation accuracy \ncompared to the well-known Nystr\\\"{o}m algorithm for Kernel PCA. \n</p>"}, "author": "Ji Chen, Xiaodong Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663553", "id": "tag:google.com,2005:reader/item/000000032dc77e03", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "KGAN: How to Break The Minimax Game in GAN. (arXiv:1711.01744v1 [cs.LG])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01744"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01744", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative Adversarial Networks (GANs) were intuitively and attractively \nexplained under the perspective of game theory, wherein two involving parties \nare a discriminator and a generator. In this game, the task of the \ndiscriminator is to discriminate the real and generated (i.e., fake) data, \nwhilst the task of the generator is to generate the fake data that maximally \nconfuses the discriminator. In this paper, we propose a new viewpoint for GANs, \nwhich is termed as the minimizing general loss viewpoint. This viewpoint shows \na connection between the general loss of a classification problem regarding a \nconvex loss function and a f-divergence between the true and fake data \ndistributions. Mathematically, we proposed a setting for the classification \nproblem of the true and fake data, wherein we can prove that the general loss \nof this classification problem is exactly the negative f-divergence for a \ncertain convex function f. This allows us to interpret the problem of learning \nthe generator for dismissing the f-divergence between the true and fake data \ndistributions as that of maximizing the general loss which is equivalent to the \nmin-max problem in GAN if the Logistic loss is used in the classification \nproblem. However, this viewpoint strengthens GANs in two ways. First, it allows \nus to employ any convex loss function for the discriminator. Second, it \nsuggests that rather than limiting ourselves in NN-based discriminators, we can \nalternatively utilize other powerful families. Bearing this viewpoint, we then \npropose using the kernel-based family for discriminators. This family has two \nappealing features: i) a powerful capacity in classifying non-linear nature \ndata and ii) being convex in the feature space. Using the convexity of this \nfamily, we can further develop Fenchel duality to equivalently transform the \nmax-min problem to the max-max dual problem. \n</p>"}, "author": "Trung Le, Tu Dinh Nguyen, Dinh Phung", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663552", "id": "tag:google.com,2005:reader/item/000000032dc77e76", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "AdaBatch: Efficient Gradient Aggregation Rules for Sequential and Parallel Stochastic Gradient Methods. (arXiv:1711.01761v1 [cs.LG])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01761"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01761", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study a new aggregation operator for gradients coming from a mini-batch \nfor stochastic gradient (SG) methods that allows a significant speed-up in the \ncase of sparse optimization problems. We call this method AdaBatch and it only \nrequires a few lines of code change compared to regular mini-batch SGD \nalgorithms. We provide a theoretical insight to understand how this new class \nof algorithms is performing and show that it is equivalent to an implicit \nper-coordinate rescaling of the gradients, similarly to what Adagrad methods \ncan do. In theory and in practice, this new aggregation allows to keep the same \nsample efficiency of SG methods while increasing the batch size. \nExperimentally, we also show that in the case of smooth convex optimization, \nour procedure can even obtain a better loss when increasing the batch size for \na fixed number of samples. We then apply this new algorithm to obtain a \nparallelizable stochastic gradient method that is synchronous but allows \nspeed-up on par with Hogwild! methods as convergence does not deteriorate with \nthe increase of the batch size. The same approach can be used to make \nmini-batch provably efficient for variance-reduced SG methods such as SVRG. \n</p>"}, "author": "Alexandre D&#xe9;fossez (FAIR), Francis Bach (SIERRA)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663551", "id": "tag:google.com,2005:reader/item/000000032dc77f32", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Whitening Black-Box Neural Networks. (arXiv:1711.01768v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01768"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01768", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many deployed learned models are black boxes: given input, returns output. \nInternal information about the model, such as the architecture, optimisation \nprocedure, or training data, is not disclosed explicitly as it might contain \nproprietary information or make the system more vulnerable. This work shows \nthat such attributes of neural networks can be exposed from a sequence of \nqueries. This has multiple implications. On the one hand, our work exposes the \nvulnerability of black-box neural networks to different types of attacks -- we \nshow that the revealed internal information helps generate more effective \nadversarial examples against the black box model. On the other hand, this \ntechnique can be used for better protection of private content from automatic \nrecognition models using adversarial examples. Our paper suggests that it is \nactually hard to draw a line between white box and black box models. \n</p>"}, "author": "Seong Joon Oh, Max Augustin, Bernt Schiele, Mario Fritz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663550", "id": "tag:google.com,2005:reader/item/000000032dc77fd9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Simultaneous Block-Sparse Signal Recovery Using Pattern-Coupled Sparse Bayesian Learning. (arXiv:1711.01790v1 [cs.LG])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01790"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01790", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we consider the block-sparse signals recovery problem in the \ncontext of multiple measurement vectors (MMV) with common row sparsity \npatterns. We develop a new method for recovery of common row sparsity MMV \nsignals, where a pattern-coupled hierarchical Gaussian prior model is \nintroduced to characterize both the block-sparsity of the coefficients and the \nstatistical dependency between neighboring coefficients of the common row \nsparsity MMV signals. Unlike many other methods, the proposed method is able to \nautomatically capture the block sparse structure of the unknown signal. Our \nmethod is developed using an expectation-maximization (EM) framework. \nSimulation results show that our proposed method offers competitive performance \nin recovering block-sparse common row sparsity pattern MMV signals. \n</p>"}, "author": "Hang Xiao, Zhengli Xing, Linxiao Yang, Jun Fang, Yanlun Wu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663549", "id": "tag:google.com,2005:reader/item/000000032dc78055", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Independently Interpretable Lasso: A New Regularizer for Sparse Regression with Uncorrelated Variables. (arXiv:1711.01796v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01796"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01796", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Sparse regularization such as $\\ell_1$ regularization is a quite powerful and \nwidely used strategy for high dimensional learning problems. The effectiveness \nof sparse regularization have been supported practically and theoretically by \nseveral studies. However, one of the biggest issues in sparse regularization is \nthat its performance is quite sensitive to correlations between features. \nOrdinary $\\ell_1$ regularization often selects variables correlated with each \nother, which results in deterioration of not only its generalization error but \nalso interpretability. In this paper, we propose a new regularization method, \n\"Independently Interpretable Lasso\" (IILasso for short). Our proposed \nregularizer suppresses selecting correlated variables, and thus each active \nvariables independently affect the objective variable in the model. Hence, we \ncan interpret regression coefficients intuitively and also improve the \nperformance by avoiding overfitting. We analyze theoretical property of IILasso \nand show that the proposed method is much advantageous for its sign recovery \nand achieves almost minimax optimal convergence rate. Synthetic and real data \nanalyses also indicate the effectiveness of IILasso. \n</p>"}, "author": "Masaaki Takada, Taiji Suzuki, Hironori Fujisawa", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663548", "id": "tag:google.com,2005:reader/item/000000032dc780e8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Fast amortized inference of neural activity from calcium imaging data with variational autoencoders. (arXiv:1711.01846v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01846"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01846", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Calcium imaging permits optical measurement of neural activity. Since \nintracellular calcium concentration is an indirect measurement of neural \nactivity, computational tools are necessary to infer the true underlying \nspiking activity from fluorescence measurements. Bayesian model inversion can \nbe used to solve this problem, but typically requires either computationally \nexpensive MCMC sampling, or faster but approximate maximum-a-posteriori \noptimization. Here, we introduce a flexible algorithmic framework for fast, \nefficient and accurate extraction of neural spikes from imaging data. Using the \nframework of variational autoencoders, we propose to amortize inference by \ntraining a deep neural network to perform model inversion efficiently. The \nrecognition network is trained to produce samples from the posterior \ndistribution over spike trains. Once trained, performing inference amounts to a \nfast single forward pass through the network, without the need for iterative \noptimization or sampling. We show that amortization can be applied flexibly to \na wide range of nonlinear generative models and significantly improves upon the \nstate of the art in computation time, while achieving competitive accuracy. Our \nframework is also able to represent posterior distributions over spike-trains. \nWe demonstrate the generality of our method by proposing the first \nprobabilistic approach for separating backpropagating action potentials from \nputative synaptic inputs in calcium imaging of dendritic spines. \n</p>"}, "author": "Artur Speiser, Jinyao Yan, Evan Archer, Lars Buesing, Srinivas C. Turaga, Jakob H. Macke", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663547", "id": "tag:google.com,2005:reader/item/000000032dc7817b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Extracting low-dimensional dynamics from multiple large-scale neural population recordings by learning to predict correlations. (arXiv:1711.01847v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01847"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01847", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c2092ae\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c2092ae&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A powerful approach for understanding neural population dynamics is to \nextract low-dimensional trajectories from population recordings using \ndimensionality reduction methods. Current approaches for dimensionality \nreduction on neural data are limited to single population recordings, and can \nnot identify dynamics embedded across multiple measurements. We propose an \napproach for extracting low-dimensional dynamics from multiple, sequential \nrecordings. Our algorithm scales to data comprising millions of observed \ndimensions, making it possible to access dynamics distributed across large \npopulations or multiple brain areas. Building on subspace-identification \napproaches for dynamical systems, we perform parameter estimation by minimizing \na moment-matching objective using a scalable stochastic gradient descent \nalgorithm: The model is optimized to predict temporal covariations across \nneurons and across time. We show how this approach naturally handles missing \ndata and multiple partial recordings, and can identify dynamics and predict \ncorrelations even in the presence of severe subsampling and small overlap \nbetween recordings. We demonstrate the effectiveness of the approach both on \nsimulated data and a whole-brain larval zebrafish imaging dataset. \n</p>"}, "author": "Marcel Nonnenmacher, Srinivas C. Turaga, Jakob H. Macke", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663546", "id": "tag:google.com,2005:reader/item/000000032dc7820c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Flexible statistical inference for mechanistic models of neural dynamics. (arXiv:1711.01861v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01861"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01861", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Mechanistic models of single-neuron dynamics have been extensively studied in \ncomputational neuroscience. However, identifying which models can \nquantitatively reproduce empirically measured data has been challenging. We \npropose to overcome this limitation by using likelihood-free inference \napproaches (also known as Approximate Bayesian Computation, ABC) to perform \nfull Bayesian inference on single-neuron models. Our approach builds on recent \nadvances in ABC by learning a neural network which maps features of the \nobserved data to the posterior distribution over parameters. We learn a \nBayesian mixture-density network approximating the posterior over multiple \nrounds of adaptively chosen simulations. Furthermore, we propose an efficient \napproach for handling missing features and parameter settings for which the \nsimulator fails, as well as a strategy for automatically learning relevant \nfeatures using recurrent neural networks. On synthetic data, our approach \nefficiently estimates posterior distributions and recovers ground-truth \nparameters. On in-vitro recordings of membrane voltages, we recover \nmultivariate posteriors over biophysical parameters, which yield \nmodel-predicted voltage traces that accurately match empirical data. Our \napproach will enable neuroscientists to perform Bayesian inference on complex \nneuron models without having to design model-specific algorithms, closing the \ngap between mechanistic and statistical approaches to single-neuron modelling. \n</p>"}, "author": "Jan-Matthis Lueckmann, Pedro J. Goncalves, Giacomo Bassetto, Kaan &#xd6;cal, Marcel Nonnenmacher, Jakob H. Macke", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663545", "id": "tag:google.com,2005:reader/item/000000032dc782d0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Interpretable Feature Recommendation for Signal Analytics. (arXiv:1711.01870v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01870"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01870", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents an automated approach for interpretable feature \nrecommendation for solving signal data analytics problems. The method has been \ntested by performing experiments on datasets in the domain of prognostics where \ninterpretation of features is considered very important. The proposed approach \nis based on Wide Learning architecture and provides means for interpretation of \nthe recommended features. It is to be noted that such an interpretation is not \navailable with feature learning approaches like Deep Learning (such as \nConvolutional Neural Network) or feature transformation approaches like \nPrincipal Component Analysis. Results show that the feature recommendation and \ninterpretation techniques are quite effective for the problems at hand in terms \nof performance and drastic reduction in time to develop a solution. It is \nfurther shown by an example, how this human-in-loop interpretation system can \nbe used as a prescriptive system. \n</p>"}, "author": "Snehasis Banerjee, Tanushyam Chattopadhyay, Ayan Mukherjee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663544", "id": "tag:google.com,2005:reader/item/000000032dc78331", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "$A^{4}NT$: Author Attribute Anonymity by Adversarial Training of Neural Machine Translation. (arXiv:1711.01921v2 [cs.CR] UPDATED)", "published": 1510139306, "updated": 1510139311, "canonical": [{"href": "http://arxiv.org/abs/1711.01921"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01921", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Text-based analysis methods allow to reveal privacy relevant author \nattributes such as gender, age and identify of the text's author. Such methods \ncan compromise the privacy of an anonymous author even when the author tries to \nremove privacy sensitive content. In this paper, we propose an automatic \nmethod, called Adversarial Author Attribute Anonymity Neural Translation \n($A^4NT$), to combat such text-based adversaries. We combine \nsequence-to-sequence language models used in machine translation and generative \nadversarial networks to obfuscate author attributes. Unlike machine translation \ntechniques which need paired data, our method can be trained on unpaired \ncorpora of text containing different authors. Importantly, we propose and \nevaluate techniques to impose constraints on our $A^4NT$ to preserve the \nsemantics of the input text. $A^4NT$ learns to make minimal changes to the \ninput text to successfully fool author attribute classifiers, while aiming to \nmaintain the meaning of the input. We show through experiments on two different \ndatasets and three settings that our proposed method is effective in fooling \nthe author attribute classifiers and thereby improving the anonymity of \nauthors. \n</p>"}, "author": "Rakshith Shetty, Bernt Schiele, Mario Fritz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663543", "id": "tag:google.com,2005:reader/item/000000032dc7842b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time. (arXiv:1711.01944v2 [math.OC] UPDATED)", "published": 1510229029, "updated": 1510229038, "canonical": [{"href": "http://arxiv.org/abs/1711.01944"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01944", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Two classes of methods have been proposed for escaping from saddle points \nwith one using the second-order information carried by the Hessian and the \nother adding the noise into the first-order information. The existing analysis \nfor algorithms using noise in the first-order information is quite involved and \nhides the essence of added noise, which hinder further improvements of these \nalgorithms. In this paper, we present a novel perspective of noise-adding \ntechnique, i.e., adding the noise into the first-order information can help \nextract the negative curvature from the Hessian matrix, and provide a formal \nreasoning of this perspective by analyzing a simple first-order procedure. More \nimportantly, the proposed procedure enables one to design purely first-order \nstochastic algorithms for escaping from non-degenerate saddle points with a \nmuch better time complexity (almost linear time in terms of the problem's \ndimensionality). In particular, we develop a {\\bf first-order stochastic \nalgorithm} based on our new technique and an existing algorithm that only \nconverges to a first-order stationary point to enjoy a time complexity of \n{$\\widetilde O(d/\\epsilon^{3.5})$ for finding a nearly second-order stationary \npoint $\\bf{x}$ such that $\\|\\nabla F(bf{x})\\|\\leq \\epsilon$ and $\\nabla^2 \nF(bf{x})\\geq -\\sqrt{\\epsilon}I$ (in high probability), where $F(\\cdot)$ denotes \nthe objective function and $d$ is the dimensionality of the problem. To the \nbest of our knowledge, this is the best theoretical result of first-order \nalgorithms for stochastic non-convex optimization, which is even competitive \nwith if not better than existing stochastic algorithms hinging on the \nsecond-order information. \n</p>"}, "author": "Yi Xu, Tianbao Yang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663542", "id": "tag:google.com,2005:reader/item/000000032dc784e2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Deformable Deep Convolutional Generative Adversarial Network in Microwave Based Hand Gesture Recognition System. (arXiv:1711.01968v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01968"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01968", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Traditional vision-based hand gesture recognition systems is limited under \ndark circumstances. In this paper, we build a hand gesture recognition system \nbased on microwave transceiver and deep learning algorithm. A Doppler radar \nsensor with dual receiving channels at 5.8GHz is used to acquire a big database \nof hand gestures signals. The received hand gesture signals are then processed \nwith time-frequency analysis. Based on these big databases of hand gesture, we \npropose a new machine learning architecture called deformable deep \nconvolutional generative adversarial network. Experimental results show the new \narchitecture can upgrade the recognition rate by 10% and the deformable kernel \ncan reduce the testing time cost by 30%. \n</p>"}, "author": "Jiajun Zhang, Zhiguo Shi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663541", "id": "tag:google.com,2005:reader/item/000000032dc785fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Optimal transport maps for distribution preserving operations on latent spaces of Generative Models. (arXiv:1711.01970v1 [cs.LG])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01970"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01970", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative models such as Variational Auto Encoders (VAEs) and Generative \nAdversarial Networks (GANs) are typically trained for a fixed prior \ndistribution in the latent space, such as uniform or Gaussian. After a trained \nmodel is obtained, one can sample the Generator in various forms for \nexploration and understanding, such as interpolating between two samples, \nsampling in the vicinity of a sample or exploring differences between a pair of \nsamples applied to a third sample. In this paper, we show that the latent space \noperations used in the literature so far induce a distribution mismatch between \nthe resulting outputs and the prior distribution the model was trained on. To \naddress this, we propose to use distribution matching transport maps to ensure \nthat such latent space operations preserve the prior distribution, while \nminimally modifying the original operation. Our experimental results validate \nthat the proposed operations give higher quality samples compared to the \noriginal operations. \n</p>"}, "author": "Eirikur Agustsson, Alexander Sage, Radu Timofte, Luc Van Gool", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663540", "id": "tag:google.com,2005:reader/item/000000032dc78698", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Estimating Cosmological Parameters from the Dark Matter Distribution. (arXiv:1711.02033v1 [astro-ph.CO])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02033"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02033", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A grand challenge of the 21st century cosmology is to accurately estimate the \ncosmological parameters of our Universe. A major approach to estimating the \ncosmological parameters is to use the large-scale matter distribution of the \nUniverse. Galaxy surveys provide the means to map out cosmic large-scale \nstructure in three dimensions. Information about galaxy locations is typically \nsummarized in a \"single\" function of scale, such as the galaxy correlation \nfunction or power-spectrum. We show that it is possible to estimate these \ncosmological parameters directly from the distribution of matter. This paper \npresents the application of deep 3D convolutional networks to volumetric \nrepresentation of dark-matter simulations as well as the results obtained using \na recently proposed distribution regression framework, showing that machine \nlearning techniques are comparable to, and can sometimes outperform, \nmaximum-likelihood point estimates using \"cosmological models\". This opens the \nway to estimating the parameters of our Universe with higher accuracy. \n</p>"}, "author": "Siamak Ravanbakhsh, Junier Oliva, Sebastien Fromenteau, Layne C. Price, Shirley Ho, Jeff Schneider, Barnabas Poczos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663539", "id": "tag:google.com,2005:reader/item/000000032dc78712", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Computing Maximum Entropy Distributions Everywhere. (arXiv:1711.02036v1 [cs.DS])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02036"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02036", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of computing the maximum entropy distribution with a \nspecified expectation over a large discrete domain. Maximum entropy \ndistributions arise and have found numerous applications in economics, machine \nlearning and various sub-disciplines of mathematics and computer science. The \nkey computational questions related to maximum entropy distributions are \nwhether they have succinct descriptions and whether they can be efficiently \ncomputed. Here we provide positive answers to both of these questions for very \ngeneral domains and, importantly, with no restriction on the expectation. This \ncompletes the picture left open by the prior work on this problem which \nrequires that the expectation vector is polynomially far in the interior of the \nconvex hull of the domain. As a consequence we obtain a general algorithmic \ntool and show how it can be applied to derive several old and new results in a \nunified manner. In particular, our results imply that certain recent continuous \noptimization formulations, for instance, for discrete counting and optimization \nproblems, the matrix scaling problem, and the worst case Brascamp-Lieb \nconstants in the rank-1 regime, are efficiently computable. Attaining these \nimplications requires reformulating the underlying problem as a version of \nmaximum entropy computation where optimization also involves the expectation \nvector and, hence, cannot be assumed to be sufficiently deep in the interior. \nThe key new technical ingredient in our work is a polynomial bound on the bit \ncomplexity of near-optimal dual solutions to the maximum entropy convex \nprogram. This result is obtained by a geometrical reasoning that involves \nconvex analysis and polyhedral geometry, avoiding combinatorial arguments based \non the specific structure of the domain. We also provide a lower bound on the \nbit complexity of near-optimal solutions showing the tightness of our results. \n</p>"}, "author": "Damian Straszak, Nisheeth K. Vishnoi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663538", "id": "tag:google.com,2005:reader/item/000000032dc78785", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Randomized Nonnegative Matrix Factorization. (arXiv:1711.02037v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02037"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02037", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Nonnegative matrix factorization (NMF) is a powerful tool for data mining. \nHowever, the emergence of `big data' has severely challenged our ability to \ncompute this fundamental decomposition using deterministic algorithms. This \npaper presents a randomized hierarchical alternating least squares (HALS) \nalgorithm to compute the NMF. By deriving a smaller matrix from the nonnegative \ninput data, a more efficient nonnegative decomposition can be computed. Our \nalgorithm scales to big data applications while attaining a near-optimal \nfactorization, i.e., the algorithm scales with the target rank of the data \nrather than the ambient dimension of measurement space. The proposed algorithm \nis evaluated using synthetic and real world data and shows substantial speedups \ncompared to deterministic HALS. \n</p>"}, "author": "N. Benjamin Erichson, Ariana Mendible, Sophie Wihlborn, J. Nathan Kutz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663537", "id": "tag:google.com,2005:reader/item/000000032dc787f0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "An efficient quantum algorithm for generative machine learning. (arXiv:1711.02038v1 [quant-ph])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02038"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02038", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c209a62\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c209a62&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A central task in the field of quantum computing is to find applications \nwhere quantum computer could provide exponential speedup over any classical \ncomputer. Machine learning represents an important field with broad \napplications where quantum computer may offer significant speedup. Several \nquantum algorithms for discriminative machine learning have been found based on \nefficient solving of linear algebraic problems, with potential exponential \nspeedup in runtime under the assumption of effective input from a quantum \nrandom access memory. In machine learning, generative models represent another \nlarge class which is widely used for both supervised and unsupervised learning. \nHere, we propose an efficient quantum algorithm for machine learning based on a \nquantum generative model. We prove that our proposed model is exponentially \nmore powerful to represent probability distributions compared with classical \ngenerative models and has exponential speedup in training and inference at \nleast for some instances under a reasonable assumption in computational \ncomplexity theory. Our result opens a new direction for quantum machine \nlearning and offers a remarkable example in which a quantum algorithm shows \nexponential improvement over any classical algorithm in an important \napplication field. \n</p>"}, "author": "Xun Gao, Zhengyu Zhang, Luming Duan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663536", "id": "tag:google.com,2005:reader/item/000000032dc7883b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "End-to-End Abnormality Detection in Medical Imaging. (arXiv:1711.02074v1 [cs.CV])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02074"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02074", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c273e16\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c273e16&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Nearly all of the deep learning based image analysis methods work on \nreconstructed images, which are obtained from original acquisitions via solving \ninverse problems. The reconstruction algorithms are designed for human \nobservers, but not necessarily optimized for DNNs. It is desirable to train the \nDNNs directly from the original data which lie in a different domain with the \nimages. In this work, we proposed an end-to-end DNN for abnormality detection \nin medical imaging. A DNN was built as the unrolled version of iterative \nreconstruction algorithms to map the acquisitions to images, and followed by a \n3D convolutional neural network (CNN) to detect the abnormality in the \nreconstructed images. The two networks were trained jointly in order to \noptimize the entire DNN for the detection task from the original acquisitions. \nThe DNN was implemented for lung nodule detection in low-dose chest CT. The \nproposed end-to-end DNN demonstrated better sensitivity and accuracy for the \ntask compared to a two-step approach, in which the reconstruction and detection \nDNNs were trained separately. A significant reduction of false positive rate on \nsuspicious lesions were observed, which is crucial for the known over-diagnosis \nin low-dose lung CT imaging. The images reconstructed by the proposed \nend-to-end network also presented enhanced details in the region of interest. \n</p>"}, "author": "Dufan Wu, Kyungsang Kim, Bin Dong, Quanzheng Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663517", "id": "tag:google.com,2005:reader/item/000000032dc789ce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Factoring Exogenous State for Model-Free Monte Carlo. (arXiv:1703.09390v2 [cs.LG] UPDATED)", "published": 1510049224, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.09390"}], "alternate": [{"href": "http://arxiv.org/abs/1703.09390", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Policy analysts wish to visualize a range of policies for large \nsimulator-defined Markov Decision Processes (MDPs). One visualization approach \nis to invoke the simulator to generate on-policy trajectories and then \nvisualize those trajectories. When the simulator is expensive, this is not \npractical, and some method is required for generating trajectories for new \npolicies without invoking the simulator. The method of Model-Free Monte Carlo \n(MFMC) can do this by stitching together state transitions for a new policy \nbased on previously-sampled trajectories from other policies. This \"off-policy \nMonte Carlo simulation\" method works well when the state space has low \ndimension but fails as the dimension grows. This paper describes a method for \nfactoring out some of the state and action variables so that MFMC can work in \nhigh-dimensional MDPs. The new method, MFMCi, is evaluated on a very \nchallenging wildfire management MDP. \n</p>"}, "author": "Sean McGregor, Rachel Houtman, Claire Montgomery, Ronald Metoyer, Thomas G. Dietterich", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224663", "timestampUsec": "1510049224663488", "id": "tag:google.com,2005:reader/item/000000032dc7980e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "A Supervised STDP-based Training Algorithm for Living Neural Networks. (arXiv:1710.10944v2 [cs.NE] UPDATED)", "published": 1510049224, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10944"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10944", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks have shown great potential in many applications like speech \nrecognition, drug discovery, image classification, and object detection. Neural \nnetwork models are inspired by biological neural networks, but they are \noptimized to perform machine learning tasks on digital computers. The proposed \nwork explores the possibilities of using living neural networks in vitro as \nbasic computational elements for machine learning applications. A new \nsupervised STDP-based learning algorithm is proposed in this work, which \nconsiders neuron engineering constrains. A 74.7% accuracy is achieved on the \nMNIST benchmark for handwritten digit recognition. \n</p>"}, "author": "Yuan Zeng, Kevin Devincentis, Yao Xiao, Zubayer Ibne Ferdous, Xiaochen Guo, Zhiyuan Yan, Yevgeny Berdichevsky", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509977069829", "timestampUsec": "1509977069829003", "id": "tag:google.com,2005:reader/item/000000032d34447f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Weight-Based Variable Ordering in the Context of High-Level Consistencies. (arXiv:1711.00909v1 [cs.AI])", "published": 1509977070, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00909"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00909", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Dom/wdeg is one of the best performing heuristics for dynamic variable \nordering in backtrack search [Boussemart et al., 2004]. As originally defined, \nthis heuristic increments the weight of the constraint that causes a domain \nwipeout (i.e., a dead-end) when enforcing arc consistency during search. \"The \nprocess of weighting constraints with dom/wdeg is not defined when more than \none constraint lead to a domain wipeout [Vion et al., 2011].\" In this paper, we \ninvestigate how weights should be updated in the context of two high-level \nconsistencies, namely, singleton (POAC) and relational consistencies (RNIC). We \npropose, analyze, and empirically evaluate several strategies for updating the \nweights. We statistically compare the proposed strategies and conclude with our \nrecommendations. \n</p>"}, "author": "Robert J. Woodward, Berthe Y. Choueiry", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509977069829", "timestampUsec": "1509977069829002", "id": "tag:google.com,2005:reader/item/000000032d344490", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Running Time Analysis of the (1+1)-EA for OneMax and LeadingOnes under Bit-wise Noise. (arXiv:1711.00956v1 [cs.NE])", "published": 1509977070, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00956"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00956", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In many real-world optimization problems, the objective function evaluation \nis subject to noise, and we cannot obtain the exact objective value. \nEvolutionary algorithms (EAs), a type of general-purpose randomized \noptimization algorithm, have shown able to solve noisy optimization problems \nwell. However, previous theoretical analyses of EAs mainly focused on \nnoise-free optimization, which makes the theoretical understanding largely \ninsufficient. Meanwhile, the few existing theoretical studies under noise often \nconsidered the one-bit noise model, which flips a randomly chosen bit of a \nsolution before evaluation; while in many realistic applications, several bits \nof a solution can be changed simultaneously. In this paper, we study a natural \nextension of one-bit noise, the bit-wise noise model, which independently flips \neach bit of a solution with some probability. We analyze the running time of \nthe (1+1)-EA solving OneMax and LeadingOnes under bit-wise noise for the first \ntime, and derive the ranges of the noise level for polynomial and \nsuper-polynomial running time bounds. The analysis on LeadingOnes under \nbit-wise noise can be easily transferred to one-bit noise, and improves the \npreviously known results. Since our analysis discloses that the (1+1)-EA can be \nefficient only under low noise levels, we also study whether the sampling \nstrategy can bring robustness to noise. We prove that using sampling can \nsignificantly increase the largest noise level allowing a polynomial running \ntime, that is, sampling is robust to noise. \n</p>"}, "author": "Chao Qian, Chao Bian, Wu Jiang, Ke Tang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509977069829", "timestampUsec": "1509977069829001", "id": "tag:google.com,2005:reader/item/000000032d3444a0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "SPARK: Static Program Analysis Reasoning and Retrieving Knowledge. (arXiv:1711.01024v1 [cs.PL])", "published": 1509977070, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01024"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01024", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Program analysis is a technique to reason about programs without executing \nthem, and it has various applications in compilers, integrated development \nenvironments, and security. In this work, we present a machine learning \npipeline that induces a security analyzer for programs by example. The security \nanalyzer determines whether a program is either secure or insecure based on \nsymbolic rules that were deduced by our machine learning pipeline. The machine \npipeline is two-staged consisting of a Recurrent Neural Networks (RNN) and an \nExtractor that converts an RNN to symbolic rules. \n</p> \n<p>To evaluate the quality of the learned symbolic rules, we propose a \nsampling-based similarity measurement between two infinite regular languages. \nWe conduct a case study using real-world data. In this work, we discuss the \nlimitations of existing techniques and possible improvements in the future. The \nresults show that with sufficient training data and a fair distribution of \nprogram paths it is feasible to deducing symbolic security rules for the \nOpenJDK library with millions lines of code. \n</p>"}, "author": "Wasuwee Sodsong, Bernhard Scholz, Sanjay Chawla", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509977069829", "timestampUsec": "1509977069829000", "id": "tag:google.com,2005:reader/item/000000032d3444a8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Spintronics based Stochastic Computing for Efficient Bayesian Inference System. (arXiv:1711.01125v1 [cs.ET])", "published": 1509977070, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01125"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01125", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Bayesian inference is an effective approach for solving statistical learning \nproblems especially with uncertainty and incompleteness. However, inference \nefficiencies are physically limited by the bottlenecks of conventional \ncomputing platforms. In this paper, an emerging Bayesian inference system is \nproposed by exploiting spintronics based stochastic computing. A stochastic \nbitstream generator is realized as the kernel components by leveraging the \ninherent randomness of spintronics devices. The proposed system is evaluated by \ntypical applications of data fusion and Bayesian belief networks. Simulation \nresults indicate that the proposed approach could achieve significant \nimprovement on inference efficiencies in terms of power consumption and \ninference speed. \n</p>"}, "author": "Xiaotao Jia, Jianlei Yang, Zhaohao Wang, Yiran Chen, Hai (Helen)Li, Weisheng Zhao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509977069829", "timestampUsec": "1509977069828999", "id": "tag:google.com,2005:reader/item/000000032d3444b2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Accountability of AI Under the Law: The Role of Explanation. (arXiv:1711.01134v1 [cs.AI])", "published": 1509977070, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01134"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01134", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The ubiquity of systems using artificial intelligence or \"AI\" has brought \nincreasing attention to how those systems should be regulated. The choice of \nhow to regulate AI systems will require care. AI systems have the potential to \nsynthesize large amounts of data, allowing for greater levels of \npersonalization and precision than ever before---applications range from \nclinical decision support to autonomous driving and predictive policing. That \nsaid, there exist legitimate concerns about the intentional and unintentional \nnegative consequences of AI systems. There are many ways to hold AI systems \naccountable. In this work, we focus on one: explanation. Questions about a \nlegal right to explanation from AI systems was recently debated in the EU \nGeneral Data Protection Regulation, and thus thinking carefully about when and \nhow explanation from AI systems might improve accountability is timely. In this \nwork, we review contexts in which explanation is currently required under the \nlaw, and then list the technical considerations that must be considered if we \ndesired AI systems that could provide kinds of explanations that are currently \nrequired of humans. \n</p>"}, "author": "Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman, David O&#x27;Brien, Stuart Schieber, James Waldo, David Weinberger, Alexandra Wood", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509977069829", "timestampUsec": "1509977069828998", "id": "tag:google.com,2005:reader/item/000000032d3444c1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Lifelong Learning by Adjusting Priors. (arXiv:1711.01244v1 [stat.ML])", "published": 1509977070, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01244"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01244", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In representational lifelong learning an agent aims to continually learn to \nsolve novel tasks while updating its representation in light of previous tasks. \nUnder the assumption that future tasks are 'related' to previous tasks, \nrepresentations should be learned in such a way that they capture the common \nstructure across learned tasks, while allowing the learner sufficient \nflexibility to adapt to novel aspects of a new task. We develop a framework for \nlifelong learning in deep neural networks that is based on generalization \nbounds, developed within the PAC-Bayes framework. Learning takes place through \nthe construction of a distribution over networks based on the tasks seen so \nfar, and its utilization for learning a new task. Thus, prior knowledge is \nincorporated through setting a history-dependent prior for novel tasks. We \ndevelop a gradient-based algorithm implementing these ideas, based on \nminimizing an objective function motivated by generalization bounds, and \ndemonstrate its effectiveness through numerical examples. In addition to \nestablishing the improved performance available through lifelong learning, we \ndemonstrate the intuitive way by which prior information is manifested at \ndifferent levels of the network. \n</p>"}, "author": "Ron Amit, Ron Meir", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228294", "id": "tag:google.com,2005:reader/item/000000032d09f359", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "The (Un)reliability of saliency methods. (arXiv:1711.00867v1 [stat.ML])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00867"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00867", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c2741db\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c2741db&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Saliency methods aim to explain the predictions of deep neural networks. \nThese methods lack reliability when the explanation is sensitive to factors \nthat do not contribute to the model prediction. We use a simple and common \npre-processing step ---adding a constant shift to the input data--- to show \nthat a transformation with no effect on the model can cause numerous methods to \nincorrectly attribute. In order to guarantee reliability, we posit that methods \nshould fulfill input invariance, the requirement that a saliency method mirror \nthe sensitivity of the model with respect to transformations of the input. We \nshow, through several examples, that saliency methods that do not satisfy input \ninvariance result in misleading attribution. \n</p>"}, "author": "Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Sch&#xfc;tt, Sven D&#xe4;hne, Dumitru Erhan, Been Kim", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228293", "id": "tag:google.com,2005:reader/item/000000032d09f369", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Correcting Nuisance Variation using Wasserstein Distance. (arXiv:1711.00882v1 [stat.ML])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00882"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00882", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Profiling cellular phenotypes from microscopic imaging can provide meaningful \nbiological information resulting from various factors affecting the cells. One \nmotivating application is drug development: morphological cell features can be \ncaptured from images, from which similarities between different drugs applied \nat different dosages can be quantified. The general approach is to find a \nfunction mapping the images to an embedding space of manageable dimensionality \nwhose geometry captures relevant features of the input images. An important \nknown issue for such methods is separating relevant biological signal from \nnuisance variation. For example, the embedding vectors tend to be more \ncorrelated for cells that were cultured and imaged during the same week than \nfor cells from a different week, despite having identical drug compounds \napplied in both cases. In this case, the particular batch a set of experiments \nwere conducted in constitutes the domain of the data; an ideal set of image \nembeddings should contain only the relevant biological information (e.g. drug \neffects). We develop a method for adjusting the image embeddings in order to \n`forget' domain-specific information while preserving relevant biological \ninformation. To do this, we minimize a loss function based on the Wasserstein \ndistance. We find for our transformed embeddings (1) the underlying geometric \nstructure is preserved and (2) less domain-specific information is present. \n</p>"}, "author": "Gil Tabak, Minjie Fan, Samuel J. Yang, Stephan Hoyer, Geoff Davis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228292", "id": "tag:google.com,2005:reader/item/000000032d09f378", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Sparse-View X-Ray CT Reconstruction Using $\\ell_1$ Prior with Learned Transform. (arXiv:1711.00905v1 [stat.ML])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00905"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00905", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A major challenge in X-ray computed tomography (CT) is reducing radiation \ndose while maintaining high quality of reconstructed images. To reduce the \nradiation dose, one can reduce the number of projection views (sparse-view CT); \nhowever, it becomes difficult to achieve high quality image reconstruction as \nthe number of projection views decreases. Researchers have applied the concept \nof learning sparse representations from (high-quality) CT image dataset to the \nsparse-view CT reconstruction. We propose a new statistical CT reconstruction \nmodel that combines penalized weighted-least squares (PWLS) and $\\ell_1$ \nregularization with learned sparsifying transform (PWLS-ST-$\\ell_1$), and an \nalgorithm for PWLS-ST-$\\ell_1$. Numerical experiments for sparse-view 2D \nfan-beam CT and 3D axial cone-beam CT show that the $\\ell_1$ regularizer \nsignificantly improves the sharpness of edges of reconstructed images compared \nto the CT reconstruction methods using edge-preserving regularizer and $\\ell_2$ \nregularization with learned ST. \n</p>"}, "author": "Xuehang Zheng, Il Yong Chun, Zhipeng Li, Yong Long, Jeffrey A. Fessler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228291", "id": "tag:google.com,2005:reader/item/000000032d09f384", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Binary Bouncy Particle Sampler. (arXiv:1711.00922v1 [stat.CO])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00922"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00922", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Bouncy Particle Sampler is a novel rejection-free non-reversible sampler \nfor differentiable probability distributions over continuous variables. We \ngeneralize the algorithm to piecewise differentiable distributions and apply it \nto generic binary distributions using a piecewise differentiable augmentation. \nWe illustrate the new algorithm in a binary Markov Random Field example, and \ncompare it to binary Hamiltonian Monte Carlo. Our results suggest that binary \nBPS samplers are better for easy to mix distributions. \n</p>"}, "author": "Ari Pakman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228290", "id": "tag:google.com,2005:reader/item/000000032d09f38c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning Linear Dynamical Systems via Spectral Filtering. (arXiv:1711.00946v1 [cs.LG])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00946"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00946", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present an efficient and practical algorithm for the online prediction of \ndiscrete-time linear dynamical systems with a symmetric transition matrix. We \ncircumvent the non-convex optimization problem using improper learning: \ncarefully overparameterize the class of LDSs by a polylogarithmic factor, in \nexchange for convexity of the loss functions. From this arises a \npolynomial-time algorithm with a near-optimal regret guarantee, with an \nanalogous sample complexity bound for agnostic learning. Our algorithm is based \non a novel filtering technique, which may be of independent interest: we \nconvolve the time series with the eigenvectors of a certain Hankel matrix. \n</p>"}, "author": "Elad Hazan, Karan Singh, Cyril Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228289", "id": "tag:google.com,2005:reader/item/000000032d09f393", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Beyond normality: Learning sparse probabilistic graphical models in the non-Gaussian setting. (arXiv:1711.00950v1 [cs.LG])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00950"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00950", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present an algorithm to identify sparse dependence structure in continuous \nand non-Gaussian probability distributions, given a corresponding set of data. \nThe conditional independence structure of an arbitrary distribution can be \nrepresented as an undirected graph (or Markov random field), but most \nalgorithms for learning this structure are restricted to the discrete or \nGaussian cases. Our new approach allows for more realistic and accurate \ndescriptions of the distribution in question, and in turn better estimates of \nits sparse Markov structure. Sparsity in the graph is of interest as it can \naccelerate inference, improve sampling methods, and reveal important \ndependencies between variables. The algorithm relies on exploiting the \nconnection between the sparsity of the graph and the sparsity of transport \nmaps, which deterministically couple one probability measure to another. \n</p>"}, "author": "Rebecca E. Morrison, Ricardo Baptista, Youssef Marzouk", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228288", "id": "tag:google.com,2005:reader/item/000000032d09f39b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "A Classification-Based Perspective on GAN Distributions. (arXiv:1711.00970v1 [cs.LG])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00970"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00970", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A fundamental, and still largely unanswered, question in the context of \nGenerative Adversarial Networks (GANs) is whether GANs are actually able to \ncapture the key characteristics of the datasets they are trained on. The \ncurrent approaches to examining this issue require significant human \nsupervision, such as visual inspection of sampled images, and often offer only \nfairly limited scalability. In this paper, we propose new techniques that \nemploy a classification-based perspective to evaluate synthetic GAN \ndistributions and their capability to accurately reflect the essential \nproperties of the training data. These techniques require only minimal human \nsupervision and can easily be scaled and adapted to evaluate a variety of \nstate-of-the-art GANs on large, popular datasets. Our analysis indicates that \nGANs have significant problems in reproducing the more distributional \nproperties of the training dataset. In particular, the diversity of such \nsynthetic data is orders of magnitude smaller than that of the true data. \n</p>"}, "author": "Shibani Santurkar, Ludwig Schmidt, Aleksander Madry", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228287", "id": "tag:google.com,2005:reader/item/000000032d09f3a4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "From which world is your graph?. (arXiv:1711.00982v1 [cs.LG])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00982"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00982", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Discovering statistical structure from links is a fundamental problem in the \nanalysis of social networks. Choosing a misspecified model, or equivalently, an \nincorrect inference algorithm will result in an invalid analysis or even \nfalsely uncover patterns that are in fact artifacts of the model. This work \nfocuses on unifying two of the most widely used link-formation models: the \nstochastic blockmodel (SBM) and the small world (or latent space) model (SWM). \nIntegrating techniques from kernel learning, spectral graph theory, and \nnonlinear dimensionality reduction, we develop the first statistically sound \npolynomial-time algorithm to discover latent patterns in sparse graphs for both \nmodels. When the network comes from an SBM, the algorithm outputs a block \nstructure. When it is from an SWM, the algorithm outputs estimates of each \nnode's latent position. \n</p>"}, "author": "Cheng Li, Felix Wong, Zhenming Liu, Varun Kanade", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228286", "id": "tag:google.com,2005:reader/item/000000032d09f3a6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Analysis of Approximate Stochastic Gradient Using Quadratic Constraints and Sequential Semidefinite Programs. (arXiv:1711.00987v1 [math.OC])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00987"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00987", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present convergence rate analysis for the approximate stochastic gradient \nmethod, where individual gradient updates are corrupted by computation errors. \nWe develop stochastic quadratic constraints to formulate a small linear matrix \ninequality (LMI) whose feasible set characterizes convergence properties of the \napproximate stochastic gradient. Based on this LMI condition, we develop a \nsequential minimization approach to analyze the intricate trade-offs that \ncouple stepsize selection, convergence rate, optimization accuracy, and \nrobustness to gradient inaccuracy. We also analytically solve this LMI \ncondition and obtain theoretical formulas that quantify the convergence \nproperties of the approximate stochastic gradient under various assumptions on \nthe loss functions. \n</p>"}, "author": "Bin Hu, Peter Seiler, Laurent Lessard", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228285", "id": "tag:google.com,2005:reader/item/000000032d09f3b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Partial correlation graphs and the neighborhood lattice. (arXiv:1711.00991v1 [math.ST])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00991"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00991", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We define and study partial correlation graphs (PCGs) with variables in a \ngeneral Hilbert space and their connections to generalized neighborhood \nregression, without making any distributional assumptions. Using \noperator-theoretic arguments, and especially the properties of projection \noperators on Hilbert spaces, we show that these neighborhood regressions have \nthe algebraic structure of a lattice, which we call a neighborhood lattice. \nThis lattice property significantly reduces the number of conditions one has to \ncheck when testing all partial correlation relations among a collection of \nvariables. In addition, we generalize the notion of perfectness in graphical \nmodels for a general PCG to this Hilbert space setting, and establish that \nalmost all Gram matrices are perfect. Under this perfectness assumption, we \nshow how these neighborhood lattices may be \"graphically\" computed using \nseparation properties of PCGs. We also discuss extensions of these ideas to \ndirected models, which present unique challenges compared to their undirected \ncounterparts. Our results have implications for multivariate statistical \nlearning in general, including structural equation models, subspace clustering, \nand dimension reduction. For example, we discuss how to compute neighborhood \nlattices efficiently and furthermore how they can be used to reduce the sample \ncomplexity of learning directed acyclic graphs. Our work demonstrates that this \nabstract viewpoint via projection operators significantly simplifies existing \nideas and arguments from the graphical modeling literature, and furthermore can \nbe used to extend these ideas to more general nonparametric settings. \n</p>"}, "author": "Arash A. Amini, Bryon Aragam, Qing Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228284", "id": "tag:google.com,2005:reader/item/000000032d09f3c0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Genetic Policy Optimization. (arXiv:1711.01012v1 [stat.ML])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01012"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01012", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c274681\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c274681&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Genetic algorithms have been widely used in many practical optimization \nproblems. Inspired by natural selection, operators, including mutation, \ncrossover and selection, provide effective heuristics for search and black-box \noptimization. However, they have not been shown useful for deep reinforcement \nlearning, possibly due to the catastrophic consequence of parameter crossovers \nof neural networks. Here, we present Genetic Policy Optimization (GPO), a new \ngenetic algorithm for sample-efficient deep policy optimization. GPO uses \nimitation learning for policy crossover in the state space and applies policy \ngradient methods for mutation. Our experiments on Mujoco tasks show that GPO as \na genetic algorithm is able to provide superior performance over the \nstate-of-the-art policy gradient methods and achieves comparable or higher \nsample efficiency. \n</p>"}, "author": "Tanmay Gangwani, Jian Peng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228283", "id": "tag:google.com,2005:reader/item/000000032d09f3ca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Structured Variational Inference for Coupled Gaussian Processes. (arXiv:1711.01131v1 [stat.ML])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01131"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01131", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c2df514\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c2df514&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Sparse variational approximations allow for principled and scalable inference \nin Gaussian Process (GP) models. In settings where several GPs are part of the \ngenerative model, theses GPs are a posteriori coupled. For many applications \nsuch as regression where predictive accuracy is the quantity of interest, this \ncoupling is not crucial. Howewer if one is interested in posterior uncertainty, \nit cannot be ignored. A key element of variational inference schemes is the \nchoice of the approximate posterior parameterization. When the number of latent \nvariable is large, mean field (MF) methods provide fast and accurate posterior \nmeans while more structured posterior lead to inference algorithm of greater \ncomputational complexity. Here, we extend previous sparse GP approximations and \npropose a novel parameterization of variational posteriors in the multi-GP \nsetting allowing for fast and scalable inference capturing posterior \ndependencies. \n</p>"}, "author": "Vincent Adam", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228282", "id": "tag:google.com,2005:reader/item/000000032d09f3e1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Accountability of AI Under the Law: The Role of Explanation. (arXiv:1711.01134v1 [cs.AI])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01134"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01134", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The ubiquity of systems using artificial intelligence or \"AI\" has brought \nincreasing attention to how those systems should be regulated. The choice of \nhow to regulate AI systems will require care. AI systems have the potential to \nsynthesize large amounts of data, allowing for greater levels of \npersonalization and precision than ever before---applications range from \nclinical decision support to autonomous driving and predictive policing. That \nsaid, there exist legitimate concerns about the intentional and unintentional \nnegative consequences of AI systems. There are many ways to hold AI systems \naccountable. In this work, we focus on one: explanation. Questions about a \nlegal right to explanation from AI systems was recently debated in the EU \nGeneral Data Protection Regulation, and thus thinking carefully about when and \nhow explanation from AI systems might improve accountability is timely. In this \nwork, we review contexts in which explanation is currently required under the \nlaw, and then list the technical considerations that must be considered if we \ndesired AI systems that could provide kinds of explanations that are currently \nrequired of humans. \n</p>"}, "author": "Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman, David O&#x27;Brien, Stuart Schieber, James Waldo, David Weinberger, Alexandra Wood", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228281", "id": "tag:google.com,2005:reader/item/000000032d09f3fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "A mathematical framework for graph signal processing of time-varying signals. (arXiv:1711.01191v1 [eess.SP])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01191"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01191", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a general framework from which to understand the design of filters \nfor time-series signals supported on graphs. We organize linear, time-invariant \nfilters into three increasingly restrictive classes of operators: linear \ntime-invariant filters, linear time-invariant filters which commute with a \ngraph operator, and linear time-invariant filters which are functions of a \ngraph operator. Using spectral theory, we show that these yield \n$\\mathcal{O}(n^2)$, $\\mathcal{O}(n)$, and $\\mathcal{O}(1)$ design parameters \nrespectively. We consider arbitrary graph operators as to accommodate \nnon-self-adjoint weight operators and all classes of graph Laplacian-based \noperators. We provide an example application of each class of filter. \n</p>"}, "author": "Addison Bohannon, Radu Balan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228280", "id": "tag:google.com,2005:reader/item/000000032d09f403", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Metrics for Deep Generative Models. (arXiv:1711.01204v1 [stat.ML])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01204"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01204", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural samplers such as variational autoencoders (VAEs) or generative \nadversarial networks (GANs) approximate distributions by transforming samples \nfrom a simple random source---the latent space---to samples from a more complex \ndistribution represented by a dataset. While the manifold hypothesis implies \nthat the density induced by a dataset contains large regions of low density, \nthe training criterions of VAEs and GANs will make the latent space densely \ncovered. Consequently points that are separated by low-density regions in \nobservation space will be pushed together in latent space, making stationary \ndistances poor proxies for similarity. We transfer ideas from Riemannian \ngeometry to this setting, letting the distance between two points be the \nshortest path on a Riemannian manifold induced by the transformation. The \nmethod yields a principled distance measure, provides a tool for visual \ninspection of deep generative models, and an alternative to linear \ninterpolation in latent space. In addition, it can be applied for robot \nmovement generalization using previously learned skills. The method is \nevaluated on a synthetic dataset with known ground truth; on a simulated robot \narm dataset; on human motion capture data; and on a generative model of \nhandwritten digits. \n</p>"}, "author": "Nutan Chen, Alexej Klushyn, Richard Kurle, Xueyan Jiang, Justin Bayer, Patrick van der Smagt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228279", "id": "tag:google.com,2005:reader/item/000000032d09f40e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Lifelong Learning by Adjusting Priors. (arXiv:1711.01244v1 [stat.ML])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01244"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01244", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In representational lifelong learning an agent aims to continually learn to \nsolve novel tasks while updating its representation in light of previous tasks. \nUnder the assumption that future tasks are 'related' to previous tasks, \nrepresentations should be learned in such a way that they capture the common \nstructure across learned tasks, while allowing the learner sufficient \nflexibility to adapt to novel aspects of a new task. We develop a framework for \nlifelong learning in deep neural networks that is based on generalization \nbounds, developed within the PAC-Bayes framework. Learning takes place through \nthe construction of a distribution over networks based on the tasks seen so \nfar, and its utilization for learning a new task. Thus, prior knowledge is \nincorporated through setting a history-dependent prior for novel tasks. We \ndevelop a gradient-based algorithm implementing these ideas, based on \nminimizing an objective function motivated by generalization bounds, and \ndemonstrate its effectiveness through numerical examples. In addition to \nestablishing the improved performance available through lifelong learning, we \ndemonstrate the intuitive way by which prior information is manifested at \ndifferent levels of the network. \n</p>"}, "author": "Ron Amit, Ron Meir", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509932583470", "timestampUsec": "1509932583469668", "id": "tag:google.com,2005:reader/item/000000032cdb10b0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Does Phase Matter For Monaural Source Separation?. (arXiv:1711.00913v1 [cs.SD])", "published": 1509932584, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00913"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00913", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The \"cocktail party\" problem of fully separating multiple sources from a \nsingle channel audio waveform remains unsolved. Current biological \nunderstanding of neural encoding suggests that phase information is preserved \nand utilized at every stage of the auditory pathway. However, current \ncomputational approaches primarily discard phase information in order to mask \namplitude spectrograms of sound. In this paper, we seek to address whether \npreserving phase information in spectral representations of sound provides \nbetter results in monaural separation of vocals from a musical track by using a \nneurally plausible sparse generative model. Our results demonstrate that \npreserving phase information reduces artifacts in the separated tracks, as \nquantified by the signal to artifact ratio (GSAR). Furthermore, our proposed \nmethod achieves state-of-the-art performance for source separation, as \nquantified by a mean signal to interference ratio (GSIR) of 19.46. \n</p>"}, "author": "Mohit Dubey, Garrett Kenyon, Nils Carlson, Austin Thresher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509932583470", "timestampUsec": "1509932583469667", "id": "tag:google.com,2005:reader/item/000000032cdb10b8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Running Time Analysis of the (1+1)-EA for OneMax and LeadingOnes under Bit-wise Noise. (arXiv:1711.00956v1 [cs.NE])", "published": 1509932584, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00956"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00956", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In many real-world optimization problems, the objective function evaluation \nis subject to noise, and we cannot obtain the exact objective value. \nEvolutionary algorithms (EAs), a type of general-purpose randomized \noptimization algorithm, have shown able to solve noisy optimization problems \nwell. However, previous theoretical analyses of EAs mainly focused on \nnoise-free optimization, which makes the theoretical understanding largely \ninsufficient. Meanwhile, the few existing theoretical studies under noise often \nconsidered the one-bit noise model, which flips a randomly chosen bit of a \nsolution before evaluation; while in many realistic applications, several bits \nof a solution can be changed simultaneously. In this paper, we study a natural \nextension of one-bit noise, the bit-wise noise model, which independently flips \neach bit of a solution with some probability. We analyze the running time of \nthe (1+1)-EA solving OneMax and LeadingOnes under bit-wise noise for the first \ntime, and derive the ranges of the noise level for polynomial and \nsuper-polynomial running time bounds. The analysis on LeadingOnes under \nbit-wise noise can be easily transferred to one-bit noise, and improves the \npreviously known results. Since our analysis discloses that the (1+1)-EA can be \nefficient only under low noise levels, we also study whether the sampling \nstrategy can bring robustness to noise. We prove that using sampling can \nsignificantly increase the largest noise level allowing a polynomial running \ntime, that is, sampling is robust to noise. \n</p>"}, "author": "Chao Qian, Chao Bian, Wu Jiang, Ke Tang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509932583470", "timestampUsec": "1509932583469666", "id": "tag:google.com,2005:reader/item/000000032cdb10bb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "A Classification-Based Perspective on GAN Distributions. (arXiv:1711.00970v1 [cs.LG])", "published": 1509932584, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00970"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00970", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A fundamental, and still largely unanswered, question in the context of \nGenerative Adversarial Networks (GANs) is whether GANs are actually able to \ncapture the key characteristics of the datasets they are trained on. The \ncurrent approaches to examining this issue require significant human \nsupervision, such as visual inspection of sampled images, and often offer only \nfairly limited scalability. In this paper, we propose new techniques that \nemploy a classification-based perspective to evaluate synthetic GAN \ndistributions and their capability to accurately reflect the essential \nproperties of the training data. These techniques require only minimal human \nsupervision and can easily be scaled and adapted to evaluate a variety of \nstate-of-the-art GANs on large, popular datasets. Our analysis indicates that \nGANs have significant problems in reproducing the more distributional \nproperties of the training dataset. In particular, the diversity of such \nsynthetic data is orders of magnitude smaller than that of the true data. \n</p>"}, "author": "Shibani Santurkar, Ludwig Schmidt, Aleksander Madry", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509932583470", "timestampUsec": "1509932583469665", "id": "tag:google.com,2005:reader/item/000000032cdb10bd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Convolutional Drift Networks for Video Classification. (arXiv:1711.01201v1 [cs.CV])", "published": 1509932584, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01201"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01201", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Analyzing spatio-temporal data like video is a challenging task that requires \nprocessing visual and temporal information effectively. Convolutional Neural \nNetworks have shown promise as baseline fixed feature extractors through \ntransfer learning, a technique that helps minimize the training cost on visual \ninformation. Temporal information is often handled using hand-crafted features \nor Recurrent Neural Networks, but this can be overly specific or prohibitively \ncomplex. Building a fully trainable system that can efficiently analyze \nspatio-temporal data without hand-crafted features or complex training is an \nopen challenge. We present a new neural network architecture to address this \nchallenge, the Convolutional Drift Network (CDN). Our CDN architecture combines \nthe visual feature extraction power of deep Convolutional Neural Networks with \nthe intrinsically efficient temporal processing provided by Reservoir \nComputing. In this introductory paper on the CDN, we provide a very simple \nbaseline implementation tested on two egocentric (first-person) video activity \ndatasets.We achieve video-level activity classification results on-par with \nstate-of-the art methods. Notably, performance on this complex spatio-temporal \ntask was produced by only training a single feed-forward layer in the CDN. \n</p>"}, "author": "Dillon Graham, Seyed Hamed Fatemi Langroudi, Christopher Kanan, Dhireesha Kudithipudi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509932583470", "timestampUsec": "1509932583469664", "id": "tag:google.com,2005:reader/item/000000032cdb10c3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning. (arXiv:1711.01239v1 [cs.LG])", "published": 1509932584, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01239"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01239", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c2df78d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c2df78d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Multi-task learning (MTL) with neural networks leverages commonalities in \ntasks to improve performance, but often suffers from task interference which \nreduces transfer. To address this issue we introduce the routing network \nparadigm, a novel neural network unit and training algorithm. A routing network \nis a kind of self-organizing neural network consisting of two components: a \nrouter and a set of one or more function blocks. A function block may be any \nneural network - for example a fully-connected or a convolutional layer. Given \nan input the router makes a routing decision, choosing a function block to \napply and passing the output back to the router recursively, terminating when \nthe router decides to stop or a fixed recursion depth is reached. In this way \nthe routing network dynamically composes different function blocks for each \ninput. We employ a collaborative multi-agent reinforcement learning (MARL) \napproach to jointly train the router and function blocks. We evaluate our model \non multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our \nexperiments demonstrate significant improvement in accuracy with sharper \nconvergence over challenging joint training baselines for these tasks. \n</p>"}, "author": "Clemens Rosenbaum, Tim Klinger, Matthew Riemer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509932583470", "timestampUsec": "1509932583469663", "id": "tag:google.com,2005:reader/item/000000032cdb10c8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "ResBinNet: Residual Binary Neural Network. (arXiv:1711.01243v1 [cs.LG])", "published": 1509932584, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01243"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01243", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent efforts on training light-weight binary neural networks offer \npromising execution/memory efficiency. This paper introduces ResBinNet, which \nis a composition of two interlinked methodologies aiming to address the slow \nconvergence speed and limited accuracy of binary convolutional neural networks. \nThe first method, called residual binarization, learns a multi-level binary \nrepresentation for the features within a certain neural network layer. The \nsecond method, called temperature adjustment, gradually binarizes the weights \nof a particular layer. The two methods jointly learn a set of soft-binarized \nparameters that improve the convergence rate and accuracy of binary neural \nnetworks. We corroborate the applicability and scalability of ResBinNet by \nimplementing a prototype hardware accelerator. The accelerator is \nreconfigurable in terms of the numerical precision of the binarized features, \noffering a trade-off between runtime and inference accuracy. \n</p>"}, "author": "Mohammad Ghasemzadeh, Mohammad Samragh, Farinaz Koushanfar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594374", "id": "tag:google.com,2005:reader/item/000000032b1d5d95", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Solving the school bus routing problem by maximizing trip compatibility. (arXiv:1711.00530v1 [math.OC])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00530"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00530", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>School bus planning is usually divided into routing and scheduling due to the \ncomplexity of solving them concurrently. However, the separation between these \ntwo steps may lead to worst solutions with higher overall costs than that from \nsolving them together. When finding the minimal number of trips in the routing \nproblem, neglecting the importance of trip compatibility may increase the \nnumber of buses actually needed in the scheduling problem. This paper proposes \na new formulation for the multi-school homogeneous fleet routing problem that \nmaximizes trip compatibility while minimizing total travel time. This \nincorporates the trip compatibility for the scheduling problem in the routing \nproblem. Since the problem is inherently just a routing problem, finding a good \nsolution is not cumbersome. To compare the performance of the model with \ntraditional routing problems, we generate eight mid-size data sets. Through \nimporting the generated trips of the routing problems into the bus scheduling \n(blocking) problem, it is shown that the proposed model uses up to 13% fewer \nbuses than the common traditional routing models. \n</p>"}, "author": "Ali Shafahi, Zhongxiang Wang, Ali Haghani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594373", "id": "tag:google.com,2005:reader/item/000000032b1d5d9c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "An iterative school decomposition algorithm for solving the multi-school bus routing and scheduling problem. (arXiv:1711.00532v1 [math.OC])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00532"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00532", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Servicing the school transportation demand safely with a minimum number of \nbuses is one of the highest financial goals for school transportation \ndirectors. To achieve that objective, a good and efficient way to solve the \nrouting and scheduling problem is required. Due to the growth of the computing \npower, the spotlight has been shed on solving the combined problem of the \nschool bus routing and scheduling. Some recent attempts have tried to model the \nrouting problem in a way that maximizes the trip compatibilities with the hope \nof requiring fewer buses. However, an over-counting problem associated with \ntrip compatibility could diminish the performance of these models. An extended \nmodel is proposed in this paper to resolve this over-counting problem along \nwith an iterative solution algorithm. The result shows better solutions for 8 \ntest problems can be found with a fewer number of buses (up to 25%) and shorter \ntravel time (up to 7% per trip). \n</p>"}, "author": "Zhongxiang Wang, Ali Shafahi, Ali Haghani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594372", "id": "tag:google.com,2005:reader/item/000000032b1d5da3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Beautiful and damned. Combined effect of content quality and social ties on user engagement. (arXiv:1711.00536v1 [cs.SI])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00536"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00536", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>User participation in online communities is driven by the intertwinement of \nthe social network structure with the crowd-generated content that flows along \nits links. These aspects are rarely explored jointly and at scale. By looking \nat how users generate and access pictures of varying beauty on Flickr, we \ninvestigate how the production of quality impacts the dynamics of online social \nsystems. We develop a deep learning computer vision model to score images \naccording to their aesthetic value and we validate its output through \ncrowdsourcing. By applying it to over 15B Flickr photos, we study for the first \ntime how image beauty is distributed over a large-scale social system. \nBeautiful images are evenly distributed in the network, although only a small \ncore of people get social recognition for them. To study the impact of exposure \nto quality on user engagement, we set up matching experiments aimed at \ndetecting causality from observational data. Exposure to beauty is \ndouble-edged: following people who produce high-quality content increases one's \nprobability of uploading better photos; however, an excessive imbalance between \nthe quality generated by a user and the user's neighbors leads to a decline in \nengagement. Our analysis has practical implications for improving link \nrecommender systems. \n</p>"}, "author": "Luca M. Aiello, Rossano Schifanella, Miriam Redi, Stacey Svetlichnaya, Frank Liu, Simon Osindero", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594371", "id": "tag:google.com,2005:reader/item/000000032b1d5daa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "TasNet: time-domain audio separation network for real-time, single-channel speech separation. (arXiv:1711.00541v1 [cs.SD])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00541"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00541", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Robust speech processing in multi-talker environments requires effective \nspeech separation. Recent deep learning systems have made significant progress \ntoward solving this problem, yet it remains challenging particularly in \nreal-time, short latency applications. Most methods attempt to construct a mask \nfor each source in time-frequency representation of the mixture signal which is \nnot necessarily an optimal representation for speech separation. In addition, \ntime-frequency decomposition results in inherent problems such as \nphase/magnitude decoupling and long time window which is required to achieve \nsufficient frequency resolution. We propose Time-domain Audio Separation \nNetwork (TasNet) to overcome these limitations. We directly model the signal in \nthe time-domain using encoder-decoder framework and perform the source \nseparation on nonnegative encoder outputs. This method removes the frequency \ndecomposition step and reduces the separation problem to estimation of source \nmasks on encoder outputs which is then synthesized by the decoder. Our system \noutperforms the current state-of-the-art causal speech separation algorithms, \nreduces the computational cost of speech separation, and significantly reduces \nthe minimum required latency of the output. This makes TasNet suitable for \napplications where low-power, real-time implementation is desirable such as in \nhearable and telecommunication devices. \n</p>"}, "author": "Yi Luo, Nima Mesgarani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594370", "id": "tag:google.com,2005:reader/item/000000032b1d5db8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Just ASK: Building an Architecture for Extensible Self-Service Spoken Language Understanding. (arXiv:1711.00549v1 [cs.CL])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00549"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00549", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents the design of the machine learning architecture that \nunderlies the Alexa Skills Kit (ASK), which was the first Spoken Language \nUnderstanding (SLU) Software Development Kit (SDK) for a virtual digital \nassistant, as far as we are aware. At Amazon, the infrastructure powers more \nthan 20,000 skills built through the ASK, as well as AWS's Amazon Lex SLU \nService. The ASK emphasizes flexibility, predictability and a rapid iteration \ncycle for third party developers. It imposes inductive biases that allow it to \nlearn robust SLU models from extremely small and sparse datasets and, in doing \nso, removes significant barriers to entry for software developers and dialog \nsystems researchers. \n</p>"}, "author": "Anjishnu Kumar, Arpit Gupta, Julian Chan, Sam Tucker, Bjorn Hoffmeister, Markus Dreyer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594369", "id": "tag:google.com,2005:reader/item/000000032b1d5dc2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Interpretable and Pedagogical Examples. (arXiv:1711.00694v1 [cs.AI])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00694"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00694", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Teachers intentionally pick the most informative examples to show their \nstudents. However, if the teacher and student are neural networks, the examples \nthat the teacher network learns to give, although effective at teaching the \nstudent, are typically uninterpretable. We show that training the student and \nteacher iteratively, rather than jointly, can produce interpretable teaching \nstrategies. We evaluate interpretability by (1) measuring the similarity of the \nteacher's emergent strategies to intuitive strategies in each domain and (2) \nconducting human experiments to evaluate how effective the teacher's strategies \nare at teaching humans. We show that the teacher network learns to select or \ngenerate interpretable, pedagogical examples to teach rule-based, \nprobabilistic, boolean, and hierarchical concepts. \n</p>"}, "author": "Smitha Milli, Pieter Abbeel, Igor Mordatch", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594368", "id": "tag:google.com,2005:reader/item/000000032b1d5dc8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Adaptive coordination of working-memory and reinforcement learning in non-human primates performing a trial-and-error problem solving task. (arXiv:1711.00698v1 [cs.AI])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00698"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00698", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Accumulating evidence suggest that human behavior in trial-and-error learning \ntasks based on decisions between discrete actions may involve a combination of \nreinforcement learning (RL) and working-memory (WM). While the understanding of \nbrain activity at stake in this type of tasks often involve the comparison with \nnon-human primate neurophysiological results, it is not clear whether monkeys \nuse similar combined RL and WM processes to solve these tasks. Here we analyzed \nthe behavior of five monkeys with computational models combining RL and WM. Our \nmodel-based analysis approach enables to not only fit trial-by-trial choices \nbut also transient slowdowns in reaction times, indicative of WM use. We found \nthat the behavior of the five monkeys was better explained in terms of a \ncombination of RL and WM despite inter-individual differences. The same \ncoordination dynamics we used in a previous study in humans best explained the \nbehavior of some monkeys while the behavior of others showed the opposite \npattern, revealing a possible different dynamics of WM process. We further \nanalyzed different variants of the tested models to open a discussion on how \nthe long pretraining in these tasks may have favored particular coordination \ndynamics between RL and WM. This points towards either inter-species \ndifferences or protocol differences which could be further tested in humans. \n</p>"}, "author": "Guillaume Viejo (ISIR), Beno&#xee;t Girard (ISIR), Emmanuel Procyk, Mehdi Khamassi (ISIR)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594367", "id": "tag:google.com,2005:reader/item/000000032b1d5dd2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning to Represent Programs with Graphs. (arXiv:1711.00740v1 [cs.LG])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00740"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00740", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning tasks on source code (i.e., formal languages) have been considered \nrecently, but most work has tried to transfer natural language methods and does \nnot capitalize on the unique opportunities offered by code's known syntax. For \nexample, long-range dependencies induced by using the same variable or function \nin distant locations are often not considered. We propose to use graphs to \nrepresent both the syntactic and semantic structure of code and use graph-based \ndeep learning methods to learn to reason over program structures. \n</p> \n<p>In this work, we present how to construct graphs from source code and how to \nscale Gated Graph Neural Networks training to such large graphs. We evaluate \nour method on two tasks: VarNaming, in which a network attempts to predict the \nname of a variable given its usage, and VarMisuse, in which the network learns \nto reason about selecting the correct variable that should be used at a given \nprogram location. Our comparison to methods that use less structured program \nrepresentations shows the advantages of modeling known structure, and suggests \nthat our models learn to infer meaningful names and to solve the VarMisuse task \nin many cases. Additionally, our testing showed that VarMisuse identifies a \nnumber of bugs in mature open-source projects. \n</p>"}, "author": "Miltiadis Allamanis, Marc Brockschmidt, Mahmoud Khademi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594366", "id": "tag:google.com,2005:reader/item/000000032b1d5dda", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Framework for evaluation of sound event detection in web videos. (arXiv:1711.00804v1 [cs.SD])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00804"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00804", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c2dfbe0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c2dfbe0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The largest source of sound events is web videos. Most videos lack sound \nevent labels at segment level, however, a significant number of them do respond \nto text queries, from a match found to their metadata by the search engine. In \nthis paper we explore the extent to which a search query could be used as the \ntrue label for the presence of sound events in the videos. For this, we \ndeveloped a framework for large-scale sound event recognition on web videos. \nThe framework crawls videos using search queries corresponding to 78 sound \nevent labels drawn from three datasets. The datasets are used to train three \nclassifiers, which were then run on 3.7 million video segments. We evaluated \nperformance using the search query as the true label and compare it (on a \nsubset) with human labeling. Both types exhibited close performance, to within \n10%, and similar performance trends as the number of evaluated segments \nincreased. Hence, our experiments show potential for using search query as a \npreliminary true label for sound events in web videos. \n</p>"}, "author": "Rohan Badlani, Ankit Shah, Benjamin Elizalde, Anurag Kumar, Bhiksha Raj", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594365", "id": "tag:google.com,2005:reader/item/000000032b1d5de4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning. (arXiv:1711.00832v1 [cs.AI])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00832"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00832", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c356bbc\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c356bbc&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>To achieve general intelligence, agents must learn how to interact with \nothers in a shared environment: this is the challenge of multiagent \nreinforcement learning (MARL). The simplest form is independent reinforcement \nlearning (InRL), where each agent treats its experience as part of its \n(non-stationary) environment. In this paper, we first observe that policies \nlearned using InRL can overfit to the other agents' policies during training, \nfailing to sufficiently generalize during execution. We introduce a new metric, \njoint-policy correlation, to quantify this effect. We describe an algorithm for \ngeneral MARL, based on approximate best responses to mixtures of policies \ngenerated using deep reinforcement learning, and empirical game-theoretic \nanalysis to compute meta-strategies for policy selection. The algorithm \ngeneralizes previous ones such as InRL, iterated best response, double oracle, \nand fictitious play. Then, we present a scalable implementation which reduces \nthe memory requirement using decoupled meta-solvers. Finally, we demonstrate \nthe generality of the resulting policies in two partially observable settings: \ngridworld coordination games and poker. \n</p>"}, "author": "Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, Thore Graepel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594364", "id": "tag:google.com,2005:reader/item/000000032b1d5ded", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Variational Inference of Disentangled Latent Concepts from Unlabeled Observations. (arXiv:1711.00848v1 [cs.LG])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00848"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00848", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Disentangled representations, where the higher level data generative factors \nare reflected in disjoint latent dimensions, offer several benefits such as \nease of deriving invariant representations, transferability to other tasks, \ninterpretability, etc. We consider the problem of unsupervised learning of \ndisentangled representations from large pool of unlabeled observations, and \npropose a variational inference based approach to infer disentangled latent \nfactors. We introduce a regularizer on the expectation of the approximate \nposterior over observed data that encourages the disentanglement. We evaluate \nthe proposed approach using several quantitative metrics and empirically \nobserve significant gains over existing methods in terms of both \ndisentanglement and data likelihood (reconstruction quality). \n</p>"}, "author": "Abhishek Kumar, Prasanna Sattigeri, Avinash Balakrishnan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594363", "id": "tag:google.com,2005:reader/item/000000032b1d5df8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Provable defenses against adversarial examples via the convex outer adversarial polytope. (arXiv:1711.00851v1 [cs.LG])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00851"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00851", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a method to learn deep ReLU-based classifiers that are provably \nrobust against norm-bounded adversarial perturbations (on the training data; \nfor previously unseen examples, the approach will be guaranteed to detect all \nadversarial examples, though it may flag some non-adversarial examples as \nwell). The basic idea of the approach is to consider a convex outer \napproximation of the set of activations reachable through a norm-bounded \nperturbation, and we develop a robust optimization procedure that minimizes the \nworst case loss over this outer region (via a linear program). Crucially, we \nshow that the dual problem to this linear program can be represented itself as \na deep network similar to the backpropagation network, leading to very \nefficient optimization approaches that produce guaranteed bounds on the robust \nloss. The end result is that by executing a few more forward and backward \npasses through a slightly modified version of the original network (though \npossibly with much larger batch sizes), we can learn a classifier that is \nprovably robust to any norm-bounded adversarial attack. We illustrate the \napproach on a toy 2D robust classification task, and on a simple convolutional \narchitecture applied to MNIST, where we produce a classifier that provably has \nless than 8.4% test error for any adversarial attack with bounded $\\ell_\\infty$ \nnorm less than $\\epsilon = 0.1$. This represents the largest verified network \nthat we are aware of, and we discuss future challenges in scaling the approach \nto much larger domains. \n</p>"}, "author": "J. Zico Kolter, Eric Wong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594358", "id": "tag:google.com,2005:reader/item/000000032b1d5e3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Separation of Water and Fat Magnetic Resonance Imaging Signals Using Deep Learning with Convolutional Neural Networks. (arXiv:1711.00107v1 [cs.CV] CROSS LISTED)", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00107"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00107", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Purpose: A new method for magnetic resonance (MR) imaging water-fat \nseparation using a convolutional neural network (ConvNet) and deep learning \n(DL) is presented. Feasibility of the method with complex and magnitude images \nis demonstrated with a series of patient studies and accuracy of predicted \nquantitative values is analyzed. \n</p> \n<p>Methods: Water-fat separation of 1200 gradient-echo acquisitions from 90 \nimaging sessions (normal, acute and chronic myocardial infarction) was \nperformed using a conventional model based method with modeling of R2* and \noff-resonance and a multi-peak fat spectrum. A U-Net convolutional neural \nnetwork for calculation of water-only, fat-only, R2* and off-resonance images \nwas trained with 900 gradient-echo Multiple and single-echo complex and \nmagnitude input data algorithms were studied and compared to conventional \nextended echo modeling. \n</p> \n<p>Results: The U-Net ConvNet was easily trained and provided water-fat \nseparation results visually comparable to conventional methods. Myocardial fat \ndeposition in chronic myocardial infarction and intramyocardial hemorrhage in \nacute myocardial infarction were well visualized in the DL results. Predicted \nvalues for R2*, off-resonance, water and fat signal intensities were well \ncorrelated with conventional model based water fat separation (R2&gt;=0.97, \np&lt;0.001). DL images had a 14% higher signal-to-noise ratio (p&lt;0.001) when \ncompared to the conventional method. \n</p> \n<p>Conclusion: Deep learning utilizing ConvNets is a feasible method for MR \nwater-fat separationimaging with complex, magnitude and single echo image data. \nA trained U-Net can be efficiently used for MR water-fat separation, providing \nresults comparable to conventional model based methods. \n</p>"}, "author": "James W Goldfarb", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669484341", "timestampUsec": "1509669484340794", "id": "tag:google.com,2005:reader/item/000000032b1d2085", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning with Latent Language. (arXiv:1711.00482v1 [cs.CL])", "published": 1509669485, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00482"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00482", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The named concepts and compositional operators present in natural language \nprovide a rich source of information about the kinds of abstractions humans use \nto navigate the world. Can this linguistic background knowledge improve the \ngenerality and efficiency of learned classifiers and control policies? This \npaper aims to show that using the space of natural language strings as a \nparameter space is an effective way to capture natural task structure. In a \npretraining phase, we learn a language interpretation model that transforms \ninputs (e.g. images) into outputs (e.g. labels) given natural language \ndescriptions. To learn a new concept (e.g. a classifier), we search directly in \nthe space of descriptions to minimize the interpreter's loss on training \nexamples. Crucially, our models do not require language data to learn these \nconcepts: language is used only in pretraining to impose structure on \nsubsequent learning. Results on image classification, text editing, and \nreinforcement learning show that, in all settings, models with a linguistic \nparameterization outperform those without. \n</p>"}, "author": "Jacob Andreas, Dan Klein, Sergey Levine", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669484341", "timestampUsec": "1509669484340793", "id": "tag:google.com,2005:reader/item/000000032b1d208c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "TasNet: time-domain audio separation network for real-time, single-channel speech separation. (arXiv:1711.00541v1 [cs.SD])", "published": 1509669485, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00541"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00541", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Robust speech processing in multi-talker environments requires effective \nspeech separation. Recent deep learning systems have made significant progress \ntoward solving this problem, yet it remains challenging particularly in \nreal-time, short latency applications. Most methods attempt to construct a mask \nfor each source in time-frequency representation of the mixture signal which is \nnot necessarily an optimal representation for speech separation. In addition, \ntime-frequency decomposition results in inherent problems such as \nphase/magnitude decoupling and long time window which is required to achieve \nsufficient frequency resolution. We propose Time-domain Audio Separation \nNetwork (TasNet) to overcome these limitations. We directly model the signal in \nthe time-domain using encoder-decoder framework and perform the source \nseparation on nonnegative encoder outputs. This method removes the frequency \ndecomposition step and reduces the separation problem to estimation of source \nmasks on encoder outputs which is then synthesized by the decoder. Our system \noutperforms the current state-of-the-art causal speech separation algorithms, \nreduces the computational cost of speech separation, and significantly reduces \nthe minimum required latency of the output. This makes TasNet suitable for \napplications where low-power, real-time implementation is desirable such as in \nhearable and telecommunication devices. \n</p>"}, "author": "Yi Luo, Nima Mesgarani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669484341", "timestampUsec": "1509669484340792", "id": "tag:google.com,2005:reader/item/000000032b1d2093", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Just ASK: Building an Architecture for Extensible Self-Service Spoken Language Understanding. (arXiv:1711.00549v1 [cs.CL])", "published": 1509669485, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00549"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00549", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents the design of the machine learning architecture that \nunderlies the Alexa Skills Kit (ASK), which was the first Spoken Language \nUnderstanding (SLU) Software Development Kit (SDK) for a virtual digital \nassistant, as far as we are aware. At Amazon, the infrastructure powers more \nthan 20,000 skills built through the ASK, as well as AWS's Amazon Lex SLU \nService. The ASK emphasizes flexibility, predictability and a rapid iteration \ncycle for third party developers. It imposes inductive biases that allow it to \nlearn robust SLU models from extremely small and sparse datasets and, in doing \nso, removes significant barriers to entry for software developers and dialog \nsystems researchers. \n</p>"}, "author": "Anjishnu Kumar, Arpit Gupta, Julian Chan, Sam Tucker, Bjorn Hoffmeister, Markus Dreyer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669484341", "timestampUsec": "1509669484340791", "id": "tag:google.com,2005:reader/item/000000032b1d209b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Performance Evaluation of Channel Decoding With Deep Neural Networks. (arXiv:1711.00727v1 [eess.SP])", "published": 1509669485, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00727"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00727", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With the demand of high data rate and low latency in fifth generation (5G), \ndeep neural network decoder (NND) has become a promising candidate due to its \ncapability of one-shot decoding and parallel computing. In this paper, three \ntypes of NND, i.e., multi-layer perceptron (MLP), convolution neural network \n(CNN) and recurrent neural network (RNN), are proposed with the same parameter \nmagnitude. The performance of these deep neural networks are evaluated through \nextensive simulation. Numerical results show that RNN has the best decoding \nperformance, yet at the price of the highest computational overhead. Moreover, \nwe find there exists a saturation length for each type of neural network, which \nis caused by their restricted learning abilities. \n</p>"}, "author": "Wei Lyu, Zhaoyang Zhang, Chunxu Jiao, Kangjian Qin, Huazi Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895487", "id": "tag:google.com,2005:reader/item/000000032b1cf407", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Tensor Valued Common and Individual Feature Extraction: Multi-dimensional Perspective. (arXiv:1711.00487v1 [eess.SP])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00487"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00487", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A novel method for common and individual feature analysis from exceedingly \nlarge-scale data is proposed, in order to ensure the tractability of both the \ncomputation and storage and thus mitigate the curse of dimensionality, a major \nbottleneck in modern data science. This is achieved by making use of the \ninherent redundancy in so-called multi-block data structures, which represent \nmultiple observations of the same phenomenon taken at different times, angles \nor recording conditions. Upon providing an intrinsic link between the \nproperties of the outer vector product and extracted features in tensor \ndecompositions (TDs), the proposed common and individual information extraction \nfrom multi-block data is performed through imposing physical meaning to \notherwise unconstrained factorisation approaches. This is shown to dramatically \nreduce the dimensionality of search spaces for subsequent classification \nprocedures and to yield greatly enhanced accuracy. Simulations on a multi-class \nclassification task of large-scale extraction of individual features from a \ncollection of partially related real-world images demonstrate the advantages of \nthe \"blessing of dimensionality\" associated with TDs. \n</p>"}, "author": "Ilia Kisil, Giuseppe G. Calvi, Danilo P. Mandic", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895486", "id": "tag:google.com,2005:reader/item/000000032b1cf44d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Don't Decay the Learning Rate, Increase the Batch Size. (arXiv:1711.00489v1 [cs.LG])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00489"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00489", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c356f82\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c356f82&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>It is common practice to decay the learning rate. Here we show one can \nusually obtain the same learning curve on both training and test sets by \ninstead increasing the batch size during training. This procedure is successful \nfor stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, \nand Adam. It reaches equivalent test accuracies after the same number of \ntraining epochs, but with fewer parameter updates, leading to greater \nparallelism and shorter training times. We can further reduce the number of \nparameter updates by increasing the learning rate $\\epsilon$ and scaling the \nbatch size $B \\propto \\epsilon$. Finally, one can increase the momentum \ncoefficient $m$ and scale $B \\propto 1/(1-m)$, although this tends to slightly \nreduce the test accuracy. Crucially, our techniques allow us to repurpose \nexisting training schedules for large batch training with no hyper-parameter \ntuning. We train Inception-ResNet-V2 on ImageNet to $77\\%$ validation accuracy \nin under 2500 parameter updates, efficiently utilizing training batches of \n65536 images. \n</p>"}, "author": "Samuel L. Smith, Pieter-Jan Kindermans, Quoc V. Le", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895485", "id": "tag:google.com,2005:reader/item/000000032b1cf4b5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning One-hidden-layer Neural Networks with Landscape Design. (arXiv:1711.00501v2 [cs.LG] UPDATED)", "published": 1509959154, "updated": 1509959156, "canonical": [{"href": "http://arxiv.org/abs/1711.00501"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00501", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of learning a one-hidden-layer neural network: we \nassume the input $x\\in \\mathbb{R}^d$ is from Gaussian distribution and the \nlabel $y = a^\\top \\sigma(Bx) + \\xi$, where $a$ is a nonnegative vector in \n$\\mathbb{R}^m$ with $m\\le d$, $B\\in \\mathbb{R}^{m\\times d}$ is a full-rank \nweight matrix, and $\\xi$ is a noise vector. We first give an analytic formula \nfor the population risk of the standard squared loss and demonstrate that it \nimplicitly attempts to decompose a sequence of low-rank tensors simultaneously. \n</p> \n<p>Inspired by the formula, we design a non-convex objective function $G(\\cdot)$ \nwhose landscape is guaranteed to have the following properties: 1. All local \nminima of $G$ are also global minima. \n</p> \n<p>2. All global minima of $G$ correspond to the ground truth parameters. \n</p> \n<p>3. The value and gradient of $G$ can be estimated using samples. \n</p> \n<p>With these properties, stochastic gradient descent on $G$ provably converges \nto the global minimum and learn the ground-truth parameters. We also prove \nfinite sample complexity result and validate the results by simulations. \n</p>"}, "author": "Rong Ge, Jason D. Lee, Tengyu Ma", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895484", "id": "tag:google.com,2005:reader/item/000000032b1cf500", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Sleep Stage Classification Based on Multi-level Feature Learning and Recurrent Neural Networks via Wearable Device. (arXiv:1711.00629v1 [stat.ML])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00629"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00629", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes a practical approach for automatic sleep stage \nclassification based on a multi-level feature learning framework and Recurrent \nNeural Network (RNN) classifier using heart rate and wrist actigraphy derived \nfrom a wearable device. The feature learning framework is designed to extract \nlow- and mid-level features. Low-level features capture temporal and frequency \ndomain properties and mid-level features learn compositions and structural \ninformation of signals. Since sleep staging is a sequential problem with \nlong-term dependencies, we take advantage of RNNs with Bidirectional Long \nShort-Term Memory (BLSTM) architectures for sequence data learning. To simulate \nthe actual situation of daily sleep, experiments are conducted with a resting \ngroup in which sleep is recorded in resting state, and a comprehensive group in \nwhich both resting sleep and non-resting sleep are included.We evaluate the \nalgorithm based on an eight-fold cross validation to classify five sleep stages \n(W, N1, N2, N3, and REM). The proposed algorithm achieves weighted precision, \nrecall and F1 score of 58.0%, 60.3%, and 58.2% in the resting group and 58.5%, \n61.1%, and 58.5% in the comprehensive group, respectively. Various comparison \nexperiments demonstrate the effectiveness of feature learning and BLSTM. We \nfurther explore the influence of depth and width of RNNs on performance. Our \nmethod is specially proposed for wearable devices and is expected to be \napplicable for long-term sleep monitoring at home. Without using too much prior \ndomain knowledge, our method has the potential to generalize sleep disorder \ndetection. \n</p>"}, "author": "Xin Zhang, Weixuan Kou, Eric I-Chao Chang, He Gao, Yubo Fan, Yan Xu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895483", "id": "tag:google.com,2005:reader/item/000000032b1cf537", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Bayesian Recurrent Neural Network Models for Forecasting and Quantifying Uncertainty in Spatial-Temporal Data. (arXiv:1711.00636v1 [stat.ME])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00636"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00636", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent neural networks (RNNs) are nonlinear dynamical models commonly used \nin the machine learning and dynamical systems literature to represent complex \ndynamical or sequential relationships between variables. More recently, as deep \nlearning models have become more common, RNNs have been used to forecast \nincreasingly complicated systems. Dynamical spatio-temporal processes represent \na class of complex systems that can potentially benefit from these types of \nmodels. Although the RNN literature is expansive and thoroughly developed, \nuncertainty quantification is often ignored. Even when considered, the \nuncertainty is generally quantified without the use of a rigorous framework, \nsuch as a fully Bayesian setting. Here we attempt to quantify uncertainty in a \nmore formal framework while maintaining the forecast ac- curacy that makes \nthese models appealing, by presenting a Bayesian RNN model for nonlinear \nspatio-temporal forecasting. Additionally, we make simple modifications to the \nbasic RNN to help accommodate the unique nature of nonlinear spatio-temporal \ndata. The proposed model is applied to a multiscale Lorenz system and to \nlong-lead forecasting of tropical Pacific sea surface temperature. \n</p>"}, "author": "Patrick L. McDermott, Christopher K. Wikle", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895482", "id": "tag:google.com,2005:reader/item/000000032b1cf55f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Candidates v.s. Noises Estimation for Large Multi-Class Classification Problem. (arXiv:1711.00658v1 [stat.ML])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00658"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00658", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes a method for multi-class classification problems, where \nthe number of classes $K$ is large. The method, referred to as {\\em Candidates \nv.s. Noises Estimation} (CANE), selects a small subset of candidate classes and \nsamples the remaining classes. We show that CANE is always consistent and \ncomputationally efficient. Moreover, the resulting estimator has low \nstatistical variance approaching that of the maximum likelihood estimator, when \nthe observed label belongs to the selected candidates with high probability. In \npractice, we use a tree structure with leaves as classes to promote fast beam \nsearch for candidate selection. We also apply the CANE method to estimate word \nprobabilities in neural language models. Experiments show that CANE achieves \nbetter prediction accuracy over the Noise-Contrastive Estimation (NCE), its \nvariants and a number of the state-of-the-art tree classifiers, while it gains \nsignificant speedup compared to the standard $\\mathcal{O}(K)$ methods. \n</p>"}, "author": "Lei Han, Tong Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895481", "id": "tag:google.com,2005:reader/item/000000032b1cf57f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Concave losses for robust dictionary learning. (arXiv:1711.00659v1 [cs.LG])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00659"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00659", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Traditional dictionary learning methods are based on quadratic convex loss \nfunction and thus are sensitive to outliers. In this paper, we propose a \ngeneric framework for robust dictionary learning based on concave losses. We \nprovide results on composition of concave functions, notably regarding \nsuper-gradient computations, that are key for developing generic dictionary \nlearning algorithms applicable to smooth and non-smooth losses. In order to \nimprove identification of outliers, we introduce an initialization heuristic \nbased on undercomplete dictionary learning. Experimental results using \nsynthetic and real data demonstrate that our method is able to better detect \noutliers, is capable of generating better dictionaries, outperforming \nstate-of-the-art methods such as K-SVD and LC-KSVD. \n</p>"}, "author": "Rafael Will M de Araujo (USP), Roberto Hirata (USP), Alain Rakotomamonjy (LITIS)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895480", "id": "tag:google.com,2005:reader/item/000000032b1cf6a1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Fast Information-theoretic Bayesian Optimisation. (arXiv:1711.00673v1 [stat.ML])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00673"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00673", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Information-theoretic Bayesian optimisation techniques have demonstrated \nstate-of-the-art performance in tackling important global optimisation \nproblems. However, current information-theoretic approaches: require many \napproximations in implementation; limit the choice of kernels available to \nmodel the objective; and introduce often-prohibitive computational overhead. We \ndevelop a fast information-theoretic Bayesian Optimisation method, FITBO, that \ncircumvents the need for sampling the global minimiser, thus significantly \nreducing computational overhead. Moreover, in comparison with existing \napproaches, our method faces fewer constraints on kernel choice and enjoys the \nmerits of dealing with the output space. We demonstrate empirically that FITBO \ninherits the performance associated with information-theoretic Bayesian \noptimisation, while being even faster than simpler Bayesian optimisation \napproaches, such as Expected Improvement. \n</p>"}, "author": "Binxin Ru, Michael Osborne, Mark McLeod", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895479", "id": "tag:google.com,2005:reader/item/000000032b1cf73a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "A Universal Marginalizer for Amortized Inference in Generative Models. (arXiv:1711.00695v1 [cs.LG])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00695"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00695", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of inference in a causal generative model where the \nset of available observations differs between data instances. We show how \ncombining samples drawn from the graphical model with an appropriate masking \nfunction makes it possible to train a single neural network to approximate all \nthe corresponding conditional marginal distributions and thus amortize the cost \nof inference. We further demonstrate that the efficiency of importance sampling \nmay be improved by basing proposals on the output of the neural network. We \nalso outline how the same network can be used to generate samples from an \napproximate joint posterior via a chain decomposition of the graph. \n</p>"}, "author": "Laura Douglas, Iliyan Zarov, Konstantinos Gourgoulias, Chris Lucas, Chris Hart, Adam Baker, Maneesh Sahani, Yura Perov, Saurabh Johri", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895478", "id": "tag:google.com,2005:reader/item/000000032b1cf7c2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Estimating Historical Hourly Traffic Volumes via Machine Learning and Vehicle Probe Data: A Maryland Case Study. (arXiv:1711.00721v1 [stat.ML])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00721"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00721", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper focuses on the problem of estimating historical traffic volumes \nbetween sparsely-located traffic sensors, which transportation agencies need to \naccurately compute statewide performance measures. To this end, the paper \nexamines applications of vehicle probe data, automatic traffic recorder counts, \nand neural network models to estimate hourly volumes in the Maryland highway \nnetwork, and proposes a novel approach that combines neural networks with an \nexisting profiling method. On average, the proposed approach yields 26% more \naccurate estimates than volume profiles, which are currently used by \ntransportation agencies across the US to compute statewide performance \nmeasures. The paper also quantifies the value of using vehicle probe data in \nestimating hourly traffic volumes, which provides important managerial insights \nto transportation agencies interested in acquiring this type of data. For \nexample, results show that volumes can be estimated with a mean absolute \npercent error of about 20% at locations where average number of observed probes \nis between 30 and 47 vehicles/hr, which provides a useful guideline for \nassessing the value of probe vehicle data from different vendors. \n</p>"}, "author": "Przemys&#x142;aw Seku&#x142;a, Nikola Markovi&#x107;, Zachary Vander Laan, Kaveh Farokhi Sadabadi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895477", "id": "tag:google.com,2005:reader/item/000000032b1cf838", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Network-size independent covering number bounds for deep networks. (arXiv:1711.00753v2 [cs.LG] UPDATED)", "published": 1510319034, "updated": 1510319036, "canonical": [{"href": "http://arxiv.org/abs/1711.00753"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00753", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We give a covering number bound for deep learning networks that is \nindependent of the size of the network. The key for the simple analysis is that \nfor linear classifiers, rotating the data doesn't affect the covering number. \nThus, we can ignore the rotation part of each layer's linear transformation, \nand get the covering number bound by concentrating on the scaling part. \n</p>"}, "author": "Mayank Kabra, Kristin Branson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895476", "id": "tag:google.com,2005:reader/item/000000032b1cf920", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Approximation of Functions over Manifolds: A Moving Least-Squares Approach. (arXiv:1711.00765v1 [stat.ML])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00765"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00765", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c357313\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c357313&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present an algorithm for approximating a function defined over a \n$d$-dimensional manifold utilizing only noisy function values at locations \nsampled from the manifold with noise. To produce the approximation we do not \nrequire any knowledge regarding the manifold other than its dimension $d$. The \napproximation scheme is based upon the Manifold Moving Least-Squares (MMLS). \nThe proposed algorithm is resistant to noise in both the domain and function \nvalues. Furthermore, the approximant is shown to be smooth and of approximation \norder of $O(h^{m+1})$ for non-noisy data, where $h$ is the mesh size with \nrespect to the manifold domain, and $m$ is the degree of a local polynomial \napproximation utilized in our algorithm. In addition, the proposed algorithm is \nlinear in time with respect to the ambient-space's dimension. Thus, in case of \nextremely large ambient space dimension, we are able to avoid the curse of \ndimensionality without having to perform non-linear dimension reduction, which \ninevitably introduces distortions to the manifold data. We compare, using \nnumerical experiments, the presented algorithm to state-of-the-art algorithms \nfor regression over manifolds and show its potential. \n</p>"}, "author": "Barak Sober, Yariv Aizenbud, David Levin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895475", "id": "tag:google.com,2005:reader/item/000000032b1cf98e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Partition mixture of 1D wavelets for multi-dimensional data. (arXiv:1711.00789v1 [stat.ME])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00789"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00789", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c3db84f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c3db84f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Traditional statistical wavelet analysis that carries out modeling and \ninference based on wavelet coefficients under a given, predetermined wavelet \ntransform can quickly lose efficiency in multivariate problems, because such \nwavelet transforms, which are typically symmetric with respect to the \ndimensions, cannot adaptively exploit the energy distribution in a \nproblem-specific manner. We introduce a principled probabilistic framework for \nincorporating such adaptivity---by (i) representing multivariate functions \nusing one-dimensional (1D) wavelet transforms applied to a permuted version of \nthe original function, and (ii) placing a prior on the corresponding \npermutation, thereby forming a mixture of permuted 1D wavelet transforms. Such \na representation can achieve substantially better energy concentration in the \nwavelet coefficients. In particular, when combined with the Haar basis, we show \nthat exact Bayesian inference under the model can be achieved analytically \nthrough a recursive message passing algorithm with a computational complexity \nthat scales linearly with sample size. In addition, we propose a sequential \nMonte Carlo (SMC) inference algorithm for other wavelet bases using the exact \nHaar solution as the proposal. We demonstrate that with this framework even \nsimple 1D Haar wavelets can achieve excellent performance in both 2D and 3D \nimage reconstruction via numerical experiments, outperforming state-of-the-art \nmultidimensional wavelet-based methods especially in low signal-to-noise ratio \nsettings, at a fraction of the computational cost. \n</p>"}, "author": "Meng Li, Li Ma", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895474", "id": "tag:google.com,2005:reader/item/000000032b1cfa43", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation. (arXiv:1711.00799v1 [stat.ML])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00799"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00799", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Modeling sequential data has become more and more important in practice. Some \napplications are autonomous driving, virtual sensors and weather forecasting. \nTo model such systems so called recurrent models are used. In this article we \nintroduce two new Deep Recurrent Gaussian Process (DRGP) models based on the \nSparse Spectrum Gaussian Process (SSGP) and the improved variational version \ncalled Variational Sparse Spectrum Gaussian Process (VSSGP). We follow the \nrecurrent structure given by an existing DRGP based on a specific sparse \nNystr\\\"om approximation. Therefore, we also variationally integrate out the \ninput-space and hence can propagate uncertainty through the layers. We can show \nthat for the resulting lower bound an optimal variational distribution exists. \nTraining is realized through optimizing the variational lower bound. Using \nDistributed Variational Inference (DVI), we can reduce the computational \ncomplexity. We improve over current state of the art methods in prediction \naccuracy for experimental data-sets used for their evaluation and introduce a \nnew data-set for engine control, named Emission. Furthermore, our method can \neasily be adapted for unsupervised learning, e.g. the latent variable model and \nits deep version. \n</p>"}, "author": "Roman F&#xf6;ll, Bernard Haasdonk, Markus Hanselmann, Holger Ulmer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895473", "id": "tag:google.com,2005:reader/item/000000032b1cfad2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Medoids in almost linear time via multi-armed bandits. (arXiv:1711.00817v3 [stat.ML] UPDATED)", "published": 1510139306, "updated": 1510139311, "canonical": [{"href": "http://arxiv.org/abs/1711.00817"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00817", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Computing the medoid of a large number of points in high-dimensional space is \nan increasingly common operation in many data science problems. We present an \nalgorithm Med-dit which uses O(n log n) distance evaluations to compute the \nmedoid with high probability. Med-dit is based on a connection with the \nmulti-armed bandit problem. We evaluate the performance of Med-dit empirically \non the Netflix-prize and the single-cell RNA-Seq datasets, containing hundreds \nof thousands of points living in tens of thousands of dimensions, and observe a \n5-10x improvement in performance over the current state of the art. Med-dit is \navailable at https://github.com/bagavi/Meddit \n</p>"}, "author": "Vivek Bagaria, Govinda M. Kamath, Vasilis Ntranos, Martin J. Zhang, David Tse", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895472", "id": "tag:google.com,2005:reader/item/000000032b1cfb41", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Oversampling for Imbalanced Learning Based on K-Means and SMOTE. (arXiv:1711.00837v1 [cs.LG])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00837"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00837", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning from class-imbalanced data continues to be a common and challenging \nproblem in supervised learning as standard classification algorithms are \ndesigned to handle balanced class distributions. While different strategies \nexist to tackle this problem, methods which generate artificial data to achieve \na balanced class distribution are more versatile than modifications to the \nclassification algorithm. Such techniques, called oversamplers, modify the \ntraining data, allowing any classifier to be used with class-imbalanced \ndatasets. Many algorithms have been proposed for this task, but most are \ncomplex and tend to generate unnecessary noise. This work presents a simple and \neffective oversampling method based on k-means clustering and SMOTE \noversampling, which avoids the generation of noise and effectively overcomes \nimbalances between and within classes. Empirical results of extensive \nexperiments with 71 datasets show that training data oversampled with the \nproposed method improves classification results. Moreover, k-means SMOTE \nconsistently outperforms other popular oversampling methods. An implementation \nis made available in the python programming language. \n</p>"}, "author": "Felix Last, Georgios Douzas, Fernando Bacao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895471", "id": "tag:google.com,2005:reader/item/000000032b1cfbb2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Generalized Probabilistic Bisection for Stochastic Root-Finding. (arXiv:1711.00843v1 [stat.ML])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00843"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00843", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider numerical schemes for root finding of noisy responses through \ngeneralizing the Probabilistic Bisection Algorithm (PBA) to the more practical \ncontext where the sampling distribution is unknown and location-dependent. As \nin standard PBA, we rely on a knowledge state for the approximate posterior of \nthe root location. To implement the corresponding Bayesian updating, we also \ncarry out inference of oracle accuracy, namely learning the probability of \ncorrect response. To this end we utilize batched querying in combination with a \nvariety of frequentist and Bayesian estimators based on majority vote, as well \nas the underlying functional responses, if available. For guiding sampling \nselection we investigate both Information Directed sampling, as well as \nQuantile sampling. Our numerical experiments show that these strategies perform \nquite differently; in particular we demonstrate the efficiency of randomized \nquantile sampling which is reminiscent of Thompson sampling. Our work is \nmotivated by the root-finding sub-routine in pricing of Bermudan financial \nderivatives, illustrated in the last section of the paper. \n</p>"}, "author": "Sergio Rodriguez, Michael Ludkovski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895470", "id": "tag:google.com,2005:reader/item/000000032b1cfc64", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Variational Inference of Disentangled Latent Concepts from Unlabeled Observations. (arXiv:1711.00848v1 [cs.LG])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00848"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00848", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Disentangled representations, where the higher level data generative factors \nare reflected in disjoint latent dimensions, offer several benefits such as \nease of deriving invariant representations, transferability to other tasks, \ninterpretability, etc. We consider the problem of unsupervised learning of \ndisentangled representations from large pool of unlabeled observations, and \npropose a variational inference based approach to infer disentangled latent \nfactors. We introduce a regularizer on the expectation of the approximate \nposterior over observed data that encourages the disentanglement. We evaluate \nthe proposed approach using several quantitative metrics and empirically \nobserve significant gains over existing methods in terms of both \ndisentanglement and data likelihood (reconstruction quality). \n</p>"}, "author": "Abhishek Kumar, Prasanna Sattigeri, Avinash Balakrishnan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895460", "id": "tag:google.com,2005:reader/item/000000032b1cffff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Topology adaptive graph convolutional networks. (arXiv:1710.10370v2 [cs.LG] UPDATED)", "published": 1509669387, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10370"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10370", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolution acts as a local feature extractor in convolutional neural \nnetworks (CNNs). However, the convolution operation is not applicable when the \ninput data is supported on an irregular graph such as with social networks, \ncitation networks, or knowledge graphs. This paper proposes the topology \nadaptive graph convolutional network (TAGCN), a novel graph convolutional \nnetwork that generalizes CNN architectures to graph-structured data and \nprovides a systematic way to design a set of fixed-size learnable filters to \nperform convolutions on graphs. The topologies of these filters are adaptive to \nthe topology of the graph when they scan the graph to perform convolution, \nreplacing the square filter for the grid-structured data in traditional CNNs. \nThe outputs are the weighted sum of these filters' outputs, extraction of both \nvertex features and strength of correlation between vertices. It can be used \nwith both directed and undirected graphs. The proposed TAGCN not only inherits \nthe properties of convolutions in CNN for grid-structured data, but it is also \nconsistent with convolution in traditional signal processing. We apply TAGCN to \nsemi-supervised learning problems for graph vertex classification; experiments \non a number of data sets demonstrate that our method outperforms the existing \ngraph convolutional neural networks and achieves state-of-the-art performance \nfor each data set tested. \n</p>"}, "author": "Jian Du, Shanghang Zhang, Guanhang Wu, Jos&#xe9; M. F. Moura, Soummya Kar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895457", "id": "tag:google.com,2005:reader/item/000000032b1d003f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Orthogonal Machine Learning: Power and Limitations. (arXiv:1711.00342v1 [cs.LG] CROSS LISTED)", "published": 1509669387, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00342"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00342", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Double machine learning provides $\\sqrt{n}$-consistent estimates of \nparameters of interest even when high-dimensional or nonparametric nuisance \nparameters are estimated at an $n^{-1/4}$ rate. The key is to employ \n\\emph{Neyman-orthogonal} moment equations which are first-order insensitive to \nperturbations in the nuisance parameters. We show that the $n^{-1/4}$ \nrequirement can be improved to $n^{-1/(2k+2)}$ by employing a $k$-th order \nnotion of orthogonality that grants robustness to more complex or \nhigher-dimensional nuisance parameters. In the partially linear model setting \npopular in causal inference, we use Stein's lemma to show that we can construct \nsecond-order orthogonal moments if and only if the treatment residual is not \nnormally distributed. We conclude by demonstrating the robustness benefits of \nan explicit doubly-orthogonal estimation procedure for treatment effect. \n</p>"}, "author": "Lester Mackey, Vasilis Syrgkanis, Ilias Zadik", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230801", "id": "tag:google.com,2005:reader/item/000000032b1415bf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Gene Ontology (GO) Prediction using Machine Learning Methods. (arXiv:1711.00001v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00001"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00001", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We applied machine learning to predict whether a gene is involved in axon \nregeneration. We extracted 31 features from different databases and trained \nfive machine learning models. Our optimal model, a Random Forest Classifier \nwith 50 submodels, yielded a test score of 85.71%, which is 4.1% higher than \nthe baseline score. We concluded that our models have some predictive \ncapability. Similar methodology and features could be applied to predict other \nGene Ontology (GO) terms. \n</p>"}, "author": "Haoze Wu, Yangyu Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230800", "id": "tag:google.com,2005:reader/item/000000032b1415de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Calibration for Stratified Classification Models. (arXiv:1711.00064v1 [stat.ME])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00064"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00064", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c3dbbc3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c3dbbc3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In classification problems, sampling bias between training data and testing \ndata is critical to the ranking performance of classification scores. Such bias \ncan be both unintentionally introduced by data collection and intentionally \nintroduced by the algorithm, such as under-sampling or weighting techniques \napplied to imbalanced data. When such sampling bias exists, using the raw \nclassification score to rank observations in the testing data can lead to \nsuboptimal results. In this paper, I investigate the optimal calibration \nstrategy in general settings, and develop a practical solution for one specific \nsampling bias case, where the sampling bias is introduced by stratified \nsampling. The optimal solution is developed by analytically solving the problem \nof optimizing the ROC curve. For practical data, I propose a ranking algorithm \nfor general classification models with stratified data. Numerical experiments \ndemonstrate that the proposed algorithm effectively addresses the stratified \nsampling bias issue. Interestingly, the proposed method shows its potential \napplicability in two other machine learning areas: unsupervised learning and \nmodel ensembling, which can be future research topics. \n</p>"}, "author": "Chandler Zuo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230799", "id": "tag:google.com,2005:reader/item/000000032b141614", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Fraternal Dropout. (arXiv:1711.00066v1 [stat.ML])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00066"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00066", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent neural networks (RNNs) are important class of architectures among \nneural networks useful for language modeling and sequential prediction. \nHowever, optimizing RNNs is known to be harder compared to feed-forward neural \nnetworks. A number of techniques have been proposed in literature to address \nthis problem. In this paper we propose a simple technique called fraternal \ndropout that takes advantage of dropout to achieve this goal. Specifically, we \npropose to train two identical copies of an RNN (that share parameters) with \ndifferent dropout masks while minimizing the difference between their \n(pre-softmax) predictions. In this way our regularization encourages the \nrepresentations of RNNs to be invariant to dropout mask, thus being robust. We \nshow that our regularization term is upper bounded by the expectation-linear \ndropout objective which has been shown to address the gap due to the difference \nbetween the train and inference phases of dropout. We evaluate our model and \nachieve state-of-the-art results in sequence modeling tasks on two benchmark \ndatasets - Penn Treebank and Wikitext-2. We also show that our approach leads \nto performance improvement by a significant margin in image captioning \n(Microsoft COCO) and semi-supervised (CIFAR-10) tasks. \n</p>"}, "author": "Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230798", "id": "tag:google.com,2005:reader/item/000000032b141660", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Ranking Median Regression: Learning to Order through Local Consensus. (arXiv:1711.00070v1 [math.ST])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00070"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00070", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This article is devoted to the problem of predicting the value taken by a \nrandom permutation $\\Sigma$, describing the preferences of an individual over a \nset of numbered items $\\{1,\\; \\ldots,\\; n\\}$ say, based on the observation of \nan input/explanatory r.v. $X$ e.g. characteristics of the individual), when \nerror is measured by the Kendall $\\tau$ distance. In the probabilistic \nformulation of the 'Learning to Order' problem we propose, which extends the \nframework for statistical Kemeny ranking aggregation developped in \n\\citet{CKS17}, this boils down to recovering conditional Kemeny medians of \n$\\Sigma$ given $X$ from i.i.d. training examples $(X_1, \\Sigma_1),\\; \\ldots,\\; \n(X_N, \\Sigma_N)$. For this reason, this statistical learning problem is \nreferred to as \\textit{ranking median regression} here. Our contribution is \ntwofold. We first propose a probabilistic theory of ranking median regression: \nthe set of optimal elements is characterized, the performance of empirical risk \nminimizers is investigated in this context and situations where fast learning \nrates can be achieved are also exhibited. Next we introduce the concept of \nlocal consensus/median, in order to derive efficient methods for ranking median \nregression. The major advantage of this local learning approach lies in its \nclose connection with the widely studied Kemeny aggregation problem. From an \nalgorithmic perspective, this permits to build predictive rules for ranking \nmedian regression by implementing efficient techniques for (approximate) Kemeny \nmedian computations at a local level in a tractable manner. In particular, \nversions of $k$-nearest neighbor and tree-based methods, tailored to ranking \nmedian regression, are investigated. Accuracy of piecewise constant ranking \nmedian regression rules is studied under a specific smoothness assumption for \n$\\Sigma$'s conditional distribution given $X$. \n</p>"}, "author": "Stephan Cl&#xe9;men&#xe7;on, Anna Korba, Eric Sibony", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230797", "id": "tag:google.com,2005:reader/item/000000032b141695", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Synth-Validation: Selecting the Best Causal Inference Method for a Given Dataset. (arXiv:1711.00083v1 [stat.ML])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00083"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00083", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many decisions in healthcare, business, and other policy domains are made \nwithout the support of rigorous evidence due to the cost and complexity of \nperforming randomized experiments. Using observational data to answer causal \nquestions is risky: subjects who receive different treatments also differ in \nother ways that affect outcomes. Many causal inference methods have been \ndeveloped to mitigate these biases. However, there is no way to know which \nmethod might produce the best estimate of a treatment effect in a given study. \nIn analogy to cross-validation, which estimates the prediction error of \npredictive models applied to a given dataset, we propose synth-validation, a \nprocedure that estimates the estimation error of causal inference methods \napplied to a given dataset. In synth-validation, we use the observed data to \nestimate generative distributions with known treatment effects. We apply each \ncausal inference method to datasets sampled from these distributions and \ncompare the effect estimates with the known effects to estimate error. Using \nsimulations, we show that using synth-validation to select a causal inference \nmethod for each study lowers the expected estimation error relative to \nconsistently using any single method. \n</p>"}, "author": "Alejandro Schuler, Ken Jung, Robert Tibshirani, Trevor Hastie, Nigam Shah", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230796", "id": "tag:google.com,2005:reader/item/000000032b1416de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering. (arXiv:1711.00108v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00108"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00108", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Existing deep multitask learning (MTL) approaches align layers shared between \ntasks in a parallel ordering. Such an organization significantly constricts the \ntypes of shared structure that can be learned. The necessity of parallel \nordering for deep MTL is first tested by comparing it with permuted ordering of \nshared layers. The results indicate that a flexible ordering can enable more \neffective sharing, thus motivating the development of a soft ordering approach, \nwhich learns how shared layers are applied in different ways for different \ntasks. Deep MTL with soft ordering outperforms parallel ordering methods across \na series of domains. These results suggest that the power of deep MTL comes \nfrom learning highly general building blocks that can be assembled to meet the \ndemands of each task. \n</p>"}, "author": "Elliot Meyerson, Risto Miikkulainen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230795", "id": "tag:google.com,2005:reader/item/000000032b14171a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Accelerated Sparse Subspace Clustering. (arXiv:1711.00126v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00126"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00126", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>State-of-the-art algorithms for sparse subspace clustering perform spectral \nclustering on a similarity matrix typically obtained by representing each data \npoint as a sparse combination of other points using either basis pursuit (BP) \nor orthogonal matching pursuit (OMP). BP-based methods are often prohibitive in \npractice while the performance of OMP-based schemes are unsatisfactory, \nespecially in settings where data points are highly similar. In this paper, we \npropose a novel algorithm that exploits an accelerated variant of orthogonal \nleast-squares to efficiently find the underlying subspaces. We show that under \ncertain conditions the proposed algorithm returns a subspace-preserving \nsolution. Simulation results illustrate that the proposed method compares \nfavorably with BP-based method in terms of running time while being \nsignificantly more accurate than OMP-based schemes. \n</p>"}, "author": "Abolfazl Hashemi, Haris Vikalo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230794", "id": "tag:google.com,2005:reader/item/000000032b141764", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Pomegranate: fast and flexible probabilistic modeling in python. (arXiv:1711.00137v1 [cs.AI])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00137"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00137", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present pomegranate, an open source machine learning package for \nprobabilistic modeling in Python. Probabilistic modeling encompasses a wide \nrange of methods that explicitly describe uncertainty using probability \ndistributions. Three widely used probabilistic models implemented in \npomegranate are general mixture models, hidden Markov models, and Bayesian \nnetworks. A primary focus of pomegranate is to abstract away the complexities \nof training models from their definition. This allows users to focus on \nspecifying the correct model for their application instead of being limited by \ntheir understanding of the underlying algorithms. An aspect of this focus \ninvolves the collection of additive sufficient statistics from data sets as a \nstrategy for training models. This approach trivially enables many useful \nlearning strategies, such as out-of-core learning, minibatch learning, and \nsemi-supervised learning, without requiring the user to consider how to \npartition data or modify the algorithms to handle these tasks themselves. \npomegranate is written in Cython to speed up calculations and releases the \nglobal interpreter lock to allow for built-in multithreaded parallelism, making \nit competitive with---or outperform---other implementations of similar \nalgorithms. This paper presents an overview of the design choices in \npomegranate, and how they have enabled complex features to be supported by \nsimple code. \n</p>"}, "author": "Jacob Schreiber", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230793", "id": "tag:google.com,2005:reader/item/000000032b1417ae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Training GANs with Optimism. (arXiv:1711.00141v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00141"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00141", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We address the issue of limit cycling behavior in training Generative \nAdversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for \ntraining Wasserstein GANs. Recent theoretical results have shown that \noptimistic mirror decent (OMD) can enjoy faster regret rates in the context of \nzero-sum games. WGANs is exactly a context of solving a zero-sum game with \nsimultaneous no-regret dynamics. Moreover, we show that optimistic mirror \ndecent addresses the limit cycling problem in training WGANs. We formally show \nthat in the case of bi-linear zero-sum games the last iterate of OMD dynamics \nconverges to an equilibrium, in contrast to GD dynamics which are bound to \ncycle. We also portray the huge qualitative difference between GD and OMD \ndynamics with toy examples, even when GD is modified with many adaptations \nproposed in the recent literature, such as gradient penalty or momentum. We \napply OMD WGAN training to a bioinformatics problem of generating DNA \nsequences. We observe that models trained with OMD achieve consistently smaller \nKL divergence with respect to the true underlying distribution, than models \ntrained with GD variants. Finally, we introduce a new algorithm, Optimistic \nAdam, which is an optimistic variant of Adam. We apply it to WGAN training on \nCIFAR10 and observe improved performance in terms of inception score as \ncompared to Adam. \n</p>"}, "author": "Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, Haoyang Zeng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230792", "id": "tag:google.com,2005:reader/item/000000032b1417f2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Sampling and Reconstruction of Graph Signals via Weak Submodularity and Semidefinite Relaxation. (arXiv:1711.00142v1 [stat.ML])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00142"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00142", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of sampling a bandlimited graph signal in the presence \nof noise, where the objective is to select a node subset of prescribed \ncardinality that minimizes the signal reconstruction mean squared error (MSE). \nTo that end, we formulate the task at hand as the minimization of MSE subject \nto binary constraints, and approximate the resulting NP-hard problem via \nsemidefinite programming (SDP) relaxation. Moreover, we provide an alternative \nformulation based on maximizing a monotone weak submodular function and propose \na randomized-greedy algorithm to find a sub-optimal subset. We then derive a \nworst-case performance guarantee on the MSE returned by the randomized greedy \nalgorithm for general non-stationary graph signals. The efficacy of the \nproposed methods is illustrated through numerical simulations on synthetic and \nreal-world graphs. Notably, the randomized greedy algorithm yields an \norder-of-magnitude speedup over state-of-the-art greedy sampling schemes, while \nincurring only a marginal MSE performance loss. \n</p>"}, "author": "Abolfazl Hashemi, Rasoul Shafipour, Haris Vikalo, Gonzalo Mateos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230791", "id": "tag:google.com,2005:reader/item/000000032b141845", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Deep Neural Networks as Gaussian Processes. (arXiv:1711.00165v1 [stat.ML])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00165"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00165", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A deep fully-connected neural network with an i.i.d. prior over its \nparameters is equivalent to a Gaussian process (GP) in the limit of infinite \nnetwork width. This correspondence enables exact Bayesian inference for neural \nnetworks on regression tasks by means of straightforward matrix computations. \nFor single hidden-layer networks, the covariance function of this GP has long \nbeen known. Recently, kernel functions for multi-layer random neural networks \nhave been developed, but only outside of a Bayesian framework. As such, \nprevious work has not identified the correspondence between using these kernels \nas the covariance function for a GP and performing fully Bayesian prediction \nwith a deep neural network. In this work, we derive this correspondence and \ndevelop a computationally efficient pipeline to compute the covariance \nfunctions. We then use the resulting GP to perform Bayesian inference for deep \nneural networks on MNIST and CIFAR-10. We find that the GP-based predictions \nare competitive and can outperform neural networks trained with stochastic \ngradient descent. We observe that the trained neural network accuracy \napproaches that of the corresponding GP-based computation with increasing layer \nwidth, and that the GP uncertainty is strongly correlated with prediction \nerror. We connect our observations to the recent development of signal \npropagation in random neural networks. \n</p>"}, "author": "Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230790", "id": "tag:google.com,2005:reader/item/000000032b141885", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Stochastic Variational Inference for Fully Bayesian Sparse Gaussian Process Regression Models. (arXiv:1711.00221v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00221"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00221", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c3dbf55\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c3dbf55&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper presents a novel variational inference framework for deriving a \nfamily of Bayesian sparse Gaussian process regression (SGPR) models whose \napproximations are variationally optimal with respect to the full-rank GPR \nmodel enriched with various corresponding correlation structures of the \nobservation noises. \n</p> \n<p>Our variational Bayesian SGPR (VBSGPR) models jointly treat both the \ndistributions of the inducing variables and hyperparameters as variational \nparameters, which enables the decomposability of the variational lower bound \nthat in turn can be exploited for stochastic optimization. \n</p> \n<p>Such a stochastic optimization involves iteratively following the stochastic \ngradient of the variational lower bound to improve its estimates of the optimal \nvariational distributions of the inducing variables and hyperparameters (and \nhence the predictive distribution) of our VBSGPR models and is guaranteed to \nachieve asymptotic convergence to them. \n</p> \n<p>We show that the stochastic gradient is an unbiased estimator of the exact \ngradient and can be computed in constant time per iteration, hence achieving \nscalability to big data. \n</p> \n<p>We empirically evaluate the performance of our proposed framework on two \nreal-world, massive datasets. \n</p>"}, "author": "Haibin Yu, Trong Nghia Hoang, Kian Hsiang Low, Patrick Jaillet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230789", "id": "tag:google.com,2005:reader/item/000000032b1418c7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Smooth Neighbors on Teacher Graphs for Semi-supervised Learning. (arXiv:1711.00258v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00258"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00258", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c457b07\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c457b07&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The paper proposes an inductive semi-supervised learning method, called \nSmooth Neighbors on Teacher Graphs (SNTG). At each iteration during training, a \ngraph is dynamically constructed based on predictions of the teacher model, \ni.e., the implicit self-ensemble of models. Then the graph serves as a \nsimilarity measure with respect to which the representations of \"similar\" \nneighboring points are learned to be smooth on the low dimensional manifold. We \nachieve state-of-the-art results on semi-supervised learning benchmarks. The \nerror rates are 9.89%, 3.99% for CIFAR-10 with 4000 labels, SVHN with 500 \nlabels, respectively. In particular, the improvements are significant when the \nlabels are scarce. For non-augmented MNIST with only 20 labels, the error rate \nis reduced from previous 4.81% to 1.36%. Our method is also effective under \nnoisy supervision and shows robustness to incorrect labels. \n</p>"}, "author": "Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, Bo Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230788", "id": "tag:google.com,2005:reader/item/000000032b141913", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Avoiding Your Teacher's Mistakes: Training Neural Networks with Controlled Weak Supervision. (arXiv:1711.00313v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00313"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00313", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Training deep neural networks requires massive amounts of training data, but \nfor many tasks only limited labeled data is available. This makes weak \nsupervision attractive, using weak or noisy signals like the output of \nheuristic methods or user click-through data for training. In a semi-supervised \nsetting, we can use a large set of data with weak labels to pretrain a neural \nnetwork and then fine-tune the parameters with a small amount of data with true \nlabels. This feels intuitively sub-optimal as these two independent stages \nleave the model unaware about the varying label quality. What if we could \nsomehow inform the model about the label quality? In this paper, we propose a \nsemi-supervised learning method where we train two neural networks in a \nmulti-task fashion: a \"target network\" and a \"confidence network\". The target \nnetwork is optimized to perform a given task and is trained using a large set \nof unlabeled data that are weakly annotated. We propose to weight the gradient \nupdates to the target network using the scores provided by the second \nconfidence network, which is trained on a small amount of supervised data. Thus \nwe avoid that the weight updates computed from noisy labels harm the quality of \nthe target network model. We evaluate our learning strategy on two different \ntasks: document ranking and sentiment classification. The results demonstrate \nthat our approach not only enhances the performance compared to the baselines \nbut also speeds up the learning process from weak labels. \n</p>"}, "author": "Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, Jaap Kamps", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230787", "id": "tag:google.com,2005:reader/item/000000032b141981", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "A Large Dimensional Analysis of Regularized Discriminant Analysis Classifiers. (arXiv:1711.00382v1 [stat.ML])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00382"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00382", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This article carries out a large dimensional analysis of standard regularized \ndiscriminant analysis classifiers designed on the assumption that data arise \nfrom a Gaussian mixture model with different means and covariances. The \nanalysis relies on fundamental results from random matrix theory (RMT) when \nboth the number of features and the cardinality of the training data within \neach class grow large at the same pace. Under mild assumptions, we show that \nthe asymptotic classification error approaches a deterministic quantity that \ndepends only on the means and covariances associated with each class as well as \nthe problem dimensions. Such a result permits a better understanding of the \nperformance of regularized discriminant analsysis, in practical large but \nfinite dimensions, and can be used to determine and pre-estimate the optimal \nregularization parameter that minimizes the misclassification error \nprobability. Despite being theoretically valid only for Gaussian data, our \nfindings are shown to yield a high accuracy in predicting the performances \nachieved with real data sets drawn from the popular USPS data base, thereby \nmaking an interesting connection between theory and practice. \n</p>"}, "author": "Khalil Elkhalil, Abla Kammoun, Romain Couillet, Tareq Y. Al-Naffouri, Mohamed-Slim Alouini", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230786", "id": "tag:google.com,2005:reader/item/000000032b1419d9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Active Tolerant Testing. (arXiv:1711.00388v1 [stat.ML])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00388"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00388", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work, we give the first algorithms for tolerant testing of nontrivial \nclasses in the active model: estimating the distance of a target function to a \nhypothesis class C with respect to some arbitrary distribution D, using only a \nsmall number of label queries to a polynomial-sized pool of unlabeled examples \ndrawn from D. Specifically, we show that for the class D of unions of d \nintervals on the line, we can estimate the error rate of the best hypothesis in \nthe class to an additive error epsilon from only $O(\\frac{1}{\\epsilon^6}\\log \n\\frac{1}{\\epsilon})$ label queries to an unlabeled pool of size \n$O(\\frac{d}{\\epsilon^2}\\log \\frac{1}{\\epsilon})$. The key point here is the \nnumber of labels needed is independent of the VC-dimension of the class. This \nextends the work of Balcan et al. [2012] who solved the non-tolerant testing \nproblem for this class (distinguishing the zero-error case from the case that \nthe best hypothesis in the class has error greater than epsilon). \n</p> \n<p>We also consider the related problem of estimating the performance of a given \nlearning algorithm A in this setting. That is, given a large pool of unlabeled \nexamples drawn from distribution D, can we, from only a few label queries, \nestimate how well A would perform if the entire dataset were labeled? We focus \non k-Nearest Neighbor style algorithms, and also show how our results can be \napplied to the problem of hyperparameter tuning (selecting the best value of k \nfor the given learning problem). \n</p>"}, "author": "Avrim Blum, Lunjia Hu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230785", "id": "tag:google.com,2005:reader/item/000000032b141a3e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Minimal Exploration in Structured Stochastic Bandits. (arXiv:1711.00400v1 [stat.ML])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00400"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00400", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper introduces and addresses a wide class of stochastic bandit \nproblems where the function mapping the arm to the corresponding reward \nexhibits some known structural properties. Most existing structures (e.g. \nlinear, Lipschitz, unimodal, combinatorial, dueling, ...) are covered by our \nframework. We derive an asymptotic instance-specific regret lower bound for \nthese problems, and develop OSSB, an algorithm whose regret matches this \nfundamental limit. OSSB is not based on the classical principle of \"optimism in \nthe face of uncertainty\" or on Thompson sampling, and rather aims at matching \nthe minimal exploration rates of sub-optimal arms as characterized in the \nderivation of the regret lower bound. We illustrate the efficiency of OSSB \nusing numerical experiments in the case of the linear bandit problem and show \nthat OSSB outperforms existing algorithms, including Thompson sampling. \n</p>"}, "author": "Richard Combes, Stefan Magureanu, Alexandre Proutiere", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230784", "id": "tag:google.com,2005:reader/item/000000032b141a7b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Hierarchical Representations for Efficient Architecture Search. (arXiv:1711.00436v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00436"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00436", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We explore efficient neural architecture search methods and present a simple \nyet powerful evolutionary algorithm that can discover new architectures \nachieving state of the art results. Our approach combines a novel hierarchical \ngenetic representation scheme that imitates the modularized design pattern \ncommonly adopted by human experts, and an expressive search space that supports \ncomplex topologies. Our algorithm efficiently discovers architectures that \noutperform a large number of manually designed models for image classification, \nobtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to \nImageNet, which is competitive with the best existing neural architecture \nsearch approaches and represents the new state of the art for evolutionary \nstrategies on this task. We also present results using random search, achieving \n0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing \nthe architecture search time from 36 hours down to 1 hour. \n</p>"}, "author": "Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray Kavukcuoglu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230783", "id": "tag:google.com,2005:reader/item/000000032b141ad9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Attacking Binarized Neural Networks. (arXiv:1711.00449v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00449"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00449", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks with low-precision weights and activations offer compelling \nefficiency advantages over their full-precision equivalents. The two most \nfrequently discussed benefits of quantization are reduced memory consumption, \nand a faster forward pass when implemented with efficient bitwise operations. \nWe propose a third benefit of very low-precision neural networks: improved \nrobustness against some adversarial attacks, and in the worst case, performance \nthat is on par with full-precision models. We focus on the very low-precision \ncase where weights and activations are both quantized to $\\pm$1, and note that \nstochastically quantizing weights in just one layer can sharply reduce the \nimpact of iterative attacks. We observe that non-scaled binary neural networks \nexhibit a similar effect to the original defensive distillation procedure that \nled to gradient masking, and a false notion of security. We address this by \nconducting both black-box and white-box experiments with binary models that do \nnot artificially mask gradients. \n</p>"}, "author": "Angus Galloway, Graham W. Taylor, Medhat Moussa", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230782", "id": "tag:google.com,2005:reader/item/000000032b141b1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "An Information-Theoretic Analysis of Deep Latent-Variable Models. (arXiv:1711.00464v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00464"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00464", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present an information-theoretic framework for understanding trade-offs in \nunsupervised learning of deep latent-variables models using variational \ninference. This framework emphasizes the need to consider latent-variable \nmodels along two dimensions: the ability to reconstruct inputs (distortion) and \nthe communication cost (rate). We derive the optimal frontier of generative \nmodels in the two-dimensional rate-distortion plane, and show how the standard \nevidence lower bound objective is insufficient to select between points along \nthis frontier. However, by performing targeted optimization to learn generative \nmodels with different rates, we are able to learn many models that can achieve \nsimilar generative performance but make vastly different trade-offs in terms of \nthe usage of the latent variable. Through experiments on MNIST and Omniglot \nwith a variety of architectures, we show how our framework sheds light on many \nrecent proposed extensions to the variational autoencoder family. \n</p>"}, "author": "Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, Kevin Murphy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927579", "id": "tag:google.com,2005:reader/item/000000032aaaced3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Unsupervised Machine Translation Using Monolingual Corpora Only. (arXiv:1711.00043v1 [cs.CL])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00043"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00043", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine translation has recently achieved impressive performance thanks to \nrecent advances in deep learning and the availability of large-scale parallel \ncorpora. There have been numerous attempts to extend these successes to \nlow-resource language pairs, yet requiring tens of thousands of parallel \nsentences. In this work, we take this research direction to the extreme and \ninvestigate whether it is possible to learn to translate even without any \nparallel data. We propose a model that takes sentences from monolingual corpora \nin two different languages and maps them into the same latent space. By \nlearning to reconstruct in both languages from this shared feature space, the \nmodel effectively learns to translate without using any labeled data. We \ndemonstrate our model on two widely used datasets and two language pairs, \nreporting BLEU scores up to 32.8, without using even a single parallel sentence \nat training time. \n</p>"}, "author": "Guillaume Lample, Ludovic Denoyer, Marc&#x27;Aurelio Ranzato", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927578", "id": "tag:google.com,2005:reader/item/000000032aaacedc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Abnormal Spatial-Temporal Pattern Analysis for Niagara Frontier Border Wait Times. (arXiv:1711.00054v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00054"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00054", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c457e18\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c457e18&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Border crossing delays cause problems like huge economics loss and heavy \nenvironmental pollutions. To understand more about the nature of border \ncrossing delay, this study applies a dictionary-based compression algorithm to \nprocess the historical Niagara Frontier border wait times data. It can identify \nthe abnormal spatial-temporal patterns for both passenger vehicles and trucks \nat three bridges connecting US and Canada. Furthermore, it provides a \nquantitate anomaly score to rank the wait times patterns across the three \nbridges for each vehicle type and each direction. By analyzing the top three \nmost abnormal patterns, we find that there are at least two factors \ncontributing the anomaly of the patterns. The weekends and holidays may cause \nunusual heave congestions at the three bridges at the same time, and the \nfreight transportation demand may be uneven from Canada to the USA at Peace \nBridge and Lewiston-Queenston Bridge, which may lead to a high anomaly score. \nBy calculating the frequency of the top 5% abnormal patterns by hour of the \nday, the results show that for cars from the USA to Canada, the frequency of \nabnormal waiting time patterns is the highest during noon while for trucks in \nthe same direction, it is the highest during the afternoon peak hours. For \nCanada to US direction, the frequency of abnormal border wait time patterns for \nboth cars and trucks reaches to the peak during the afternoon. The analysis of \nabnormal spatial-temporal wait times patterns is promising to improve the \nborder crossing management \n</p>"}, "author": "Zhenhua Zhang, Lei Lin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927577", "id": "tag:google.com,2005:reader/item/000000032aaacef6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Fraternal Dropout. (arXiv:1711.00066v1 [stat.ML])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00066"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00066", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent neural networks (RNNs) are important class of architectures among \nneural networks useful for language modeling and sequential prediction. \nHowever, optimizing RNNs is known to be harder compared to feed-forward neural \nnetworks. A number of techniques have been proposed in literature to address \nthis problem. In this paper we propose a simple technique called fraternal \ndropout that takes advantage of dropout to achieve this goal. Specifically, we \npropose to train two identical copies of an RNN (that share parameters) with \ndifferent dropout masks while minimizing the difference between their \n(pre-softmax) predictions. In this way our regularization encourages the \nrepresentations of RNNs to be invariant to dropout mask, thus being robust. We \nshow that our regularization term is upper bounded by the expectation-linear \ndropout objective which has been shown to address the gap due to the difference \nbetween the train and inference phases of dropout. We evaluate our model and \nachieve state-of-the-art results in sequence modeling tasks on two benchmark \ndatasets - Penn Treebank and Wikitext-2. We also show that our approach leads \nto performance improvement by a significant margin in image captioning \n(Microsoft COCO) and semi-supervised (CIFAR-10) tasks. \n</p>"}, "author": "Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927576", "id": "tag:google.com,2005:reader/item/000000032aaacf14", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "DCN+: Mixed Objective and Deep Residual Coattention for Question Answering. (arXiv:1711.00106v1 [cs.CL])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00106"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00106", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Traditional models for question answering optimize using cross entropy loss, \nwhich encourages exact answers at the cost of penalizing nearby or overlapping \nanswers that are sometimes equally accurate. We propose a mixed objective that \ncombines cross entropy loss with self-critical policy learning. The objective \nuses rewards derived from word overlap to solve the misalignment between \nevaluation metric and optimization objective. In addition to the mixed \nobjective, we improve dynamic coattention networks (DCN) with a deep residual \ncoattention encoder that is inspired by recent work in deep self-attention and \nresidual networks. Our proposals improve model performance across question \ntypes and input lengths, especially for long questions that requires the \nability to capture long-term dependencies. On the Stanford Question Answering \nDataset, our model achieves state-of-the-art results with 75.1% exact match \naccuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy \nand 86.0% F1. \n</p>"}, "author": "Caiming Xiong, Victor Zhong, Richard Socher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927575", "id": "tag:google.com,2005:reader/item/000000032aaacf20", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering. (arXiv:1711.00108v1 [cs.LG])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00108"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00108", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Existing deep multitask learning (MTL) approaches align layers shared between \ntasks in a parallel ordering. Such an organization significantly constricts the \ntypes of shared structure that can be learned. The necessity of parallel \nordering for deep MTL is first tested by comparing it with permuted ordering of \nshared layers. The results indicate that a flexible ordering can enable more \neffective sharing, thus motivating the development of a soft ordering approach, \nwhich learns how shared layers are applied in different ways for different \ntasks. Deep MTL with soft ordering outperforms parallel ordering methods across \na series of domains. These results suggest that the power of deep MTL comes \nfrom learning highly general building blocks that can be assembled to meet the \ndemands of each task. \n</p>"}, "author": "Elliot Meyerson, Risto Miikkulainen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927574", "id": "tag:google.com,2005:reader/item/000000032aaacf36", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Automata Guided Hierarchical Reinforcement Learning for Zero-shot Skill Composition. (arXiv:1711.00129v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00129"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00129", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>An obstacle that prevents the wide adoption of (deep) reinforcement learning \n(RL) in control systems is its need for a large amount of interactions with the \nenviron- ment in order to master a skill. The learned skill usually generalizes \npoorly across domains and re-training is often necessary when presented with a \nnew task. We present a framework that combines methods in formal methods with \nhierarchi- cal reinforcement learning (HRL). The set of techniques we provide \nallows for convenient specification of tasks with complex logic, learn \nhierarchical policies (meta-controller and low-level controllers) with \nwell-defined intrinsic rewards us- ing any RL methods and is able to construct \nnew skills from existing ones without additional learning. We evaluate the \nproposed methods in a simple grid world simulation as well as simulation on a \nBaxter robot. \n</p>"}, "author": "Xiao Li, Yao Ma, Calin Belta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927573", "id": "tag:google.com,2005:reader/item/000000032aaacf5d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Pomegranate: fast and flexible probabilistic modeling in python. (arXiv:1711.00137v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00137"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00137", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present pomegranate, an open source machine learning package for \nprobabilistic modeling in Python. Probabilistic modeling encompasses a wide \nrange of methods that explicitly describe uncertainty using probability \ndistributions. Three widely used probabilistic models implemented in \npomegranate are general mixture models, hidden Markov models, and Bayesian \nnetworks. A primary focus of pomegranate is to abstract away the complexities \nof training models from their definition. This allows users to focus on \nspecifying the correct model for their application instead of being limited by \ntheir understanding of the underlying algorithms. An aspect of this focus \ninvolves the collection of additive sufficient statistics from data sets as a \nstrategy for training models. This approach trivially enables many useful \nlearning strategies, such as out-of-core learning, minibatch learning, and \nsemi-supervised learning, without requiring the user to consider how to \npartition data or modify the algorithms to handle these tasks themselves. \npomegranate is written in Cython to speed up calculations and releases the \nglobal interpreter lock to allow for built-in multithreaded parallelism, making \nit competitive with---or outperform---other implementations of similar \nalgorithms. This paper presents an overview of the design choices in \npomegranate, and how they have enabled complex features to be supported by \nsimple code. \n</p>"}, "author": "Jacob Schreiber", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927572", "id": "tag:google.com,2005:reader/item/000000032aaacf8a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Visualizing and Understanding Atari Agents. (arXiv:1711.00138v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00138"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00138", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep reinforcement learning (deep RL) agents have achieved remarkable success \nin a broad range of game-playing and continuous control tasks. While these \nagents are effective at maximizing rewards, it is often unclear what strategies \nthey use to do so. In this paper, we take a step toward explaining deep RL \nagents through a case study in three Atari 2600 environments. In particular, we \nfocus on understanding agents in terms of their visual attentional patterns \nduring decision making. To this end, we introduce a method for generating rich \nsaliency maps and use it to explain 1) what strong agents attend to 2) whether \nagents are making decisions for the right or wrong reasons, and 3) how agents \nevolve during the learning phase. We also test our method on non-expert human \nsubjects and find that it improves their ability to reason about these agents. \nOur techniques are general and, though we focus on Atari, our long-term \nobjective is to produce tools that explain any deep RL policy. \n</p>"}, "author": "Sam Greydanus, Anurag Koul, Jonathan Dodge, Alan Fern", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927571", "id": "tag:google.com,2005:reader/item/000000032aaacf9b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Link prediction in drug-target interactions network using similarity indices. (arXiv:1711.00150v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00150"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00150", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Background: In silico drug-target interaction (DTI) prediction plays an \nintegral role in drug repositioning: the discovery of new uses for existing \ndrugs. One popular method of drug repositioning is network-based DTI \nprediction, which uses complex network theory to predict DTIs from a \ndrug-target network. Currently, most network-based DTI prediction is based on \nmachine learning methods such as Restricted Boltzmann Machines (RBM) or Support \nVector Machines (SVM). These methods require additional information about the \ncharacteristics of drugs, targets and DTIs, such as chemical structure, genome \nsequence, binding types, causes of interactions, etc., and do not perform \nsatisfactorily when such information is unavailable. We propose a new, \nalternative method for DTI prediction that makes use of only network topology \ninformation attempting to solve this problem. \n</p> \n<p>Results: We compare our method for DTI prediction against the well-known RBM \napproach. We show that when applied to the MATADOR database, our approach based \non node neighborhoods yield higher precision for high-ranking predictions than \nRBM when no information regarding DTI types is available. \n</p> \n<p>Conclusion: This demonstrates that approaches purely based on network \ntopology provide a more suitable approach to DTI prediction in the many \nreal-life situations where little or no prior knowledge is available about the \ncharacteristics of drugs, targets, or their interactions. \n</p>"}, "author": "Yiding Lu, Yufan Guo, Anna Korhonen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927570", "id": "tag:google.com,2005:reader/item/000000032aaacfb4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Acquiring Target Stacking Skills by Goal-Parameterized Deep Reinforcement Learning. (arXiv:1711.00267v1 [cs.RO])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00267"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00267", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Understanding physical phenomena is a key component of human intelligence and \nenables physical interaction with previously unseen environments. In this \npaper, we study how an artificial agent can autonomously acquire this intuition \nthrough interaction with the environment. We created a synthetic block stacking \nenvironment with physics simulation in which the agent can learn a policy \nend-to-end through trial and error. Thereby, we bypass to explicitly model \nphysical knowledge within the policy. We are specifically interested in tasks \nthat require the agent to reach a given goal state that may be different for \nevery new trial. To this end, we propose a deep reinforcement learning \nframework that learns policies which are parametrized by a goal. We validated \nthe model on a toy example navigating in a grid world with different target \npositions and in a block stacking task with different target structures of the \nfinal tower. In contrast to prior work, our policies show better generalization \nacross different goals. \n</p>"}, "author": "Wenbin Li, Jeannette Bohg, Mario Fritz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927569", "id": "tag:google.com,2005:reader/item/000000032aaacfc1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks. (arXiv:1711.00350v1 [cs.CL])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00350"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00350", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Humans can understand and produce new utterances effortlessly, thanks to \ntheir systematic compositional skills. Once a person learns the meaning of a \nnew verb \"dax,\" he or she can immediately understand the meaning of \"dax twice\" \nor \"sing and dax.\" In this paper, we introduce the SCAN domain, consisting of a \nset of simple compositional navigation commands paired with the corresponding \naction sequences. We then test the zero-shot generalization capabilities of a \nvariety of recurrent neural networks (RNNs) trained on SCAN with \nsequence-to-sequence methods. We find that RNNs can generalize well when the \ndifferences between training and test commands are small, so that they can \napply \"mix-and-match\" strategies to solve the task. However, when \ngeneralization requires systematic compositional skills (as in the \"dax\" \nexample above), RNNs fail spectacularly. We conclude with a proof-of-concept \nexperiment in neural machine translation, supporting the conjecture that lack \nof systematicity is an important factor explaining why neural networks need \nvery large training sets. \n</p>"}, "author": "Brenden M. Lake, Marco Baroni", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927568", "id": "tag:google.com,2005:reader/item/000000032aaacfd7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Servant of Many Masters: Shifting priorities in Pareto-optimal sequential decision-making. (arXiv:1711.00363v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00363"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00363", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c458159\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c458159&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>It is often argued that an agent making decisions on behalf of two or more \nprincipals who have different utility functions should adopt a {\\em \nPareto-optimal} policy, i.e., a policy that cannot be improved upon for one \nagent without making sacrifices for another. A famous theorem of Harsanyi shows \nthat, when the principals have a common prior on the outcome distributions of \nall policies, a Pareto-optimal policy for the agent is one that maximizes a \nfixed, weighted linear combination of the principals' utilities. \n</p> \n<p>In this paper, we show that Harsanyi's theorem does not hold for principals \nwith different priors, and derive a more precise generalization which does \nhold, which constitutes our main result. In this more general case, the \nrelative weight given to each principal's utility should evolve over time \naccording to how well the agent's observations conform with that principal's \nprior. The result has implications for the design of contracts, treaties, joint \nventures, and robots. \n</p>"}, "author": "Andrew Critch, Stuart Russell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927567", "id": "tag:google.com,2005:reader/item/000000032aaacfee", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR. (arXiv:1711.00399v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00399"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00399", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c4c36a1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c4c36a1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>There has been much discussion of the right to explanation in the EU General \nData Protection Regulation, and its existence, merits, and disadvantages. \nImplementing a right to explanation that opens the black box of algorithmic \ndecision-making faces major legal and technical barriers. Explaining the \nfunctionality of complex algorithmic decision-making systems and their \nrationale in specific cases is a technically challenging problem. Some \nexplanations may offer little meaningful information to data subjects, raising \nquestions around their value. Explanations of automated decisions need not \nhinge on the general public understanding how algorithmic systems function. \nEven though such interpretability is of great importance and should be pursued, \nexplanations can, in principle, be offered without opening the black box. \nLooking at explanations as a means to help a data subject act rather than \nmerely understand, one could gauge the scope and content of explanations \naccording to the specific goal or action they are intended to support. From the \nperspective of individuals affected by automated decision-making, we propose \nthree aims for explanations: (1) to inform and help the individual understand \nwhy a particular decision was reached, (2) to provide grounds to contest the \ndecision if the outcome is undesired, and (3) to understand what would need to \nchange in order to receive a desired result in the future, based on the current \ndecision-making model. We assess how each of these goals finds support in the \nGDPR. We suggest data controllers should offer a particular type of \nexplanation, unconditional counterfactual explanations, to support these three \naims. These counterfactual explanations describe the smallest change to the \nworld that can be made to obtain a desirable outcome, or to arrive at the \nclosest possible world, without needing to explain the internal logic of the \nsystem. \n</p>"}, "author": "Sandra Wachter, Brent Mittelstadt, Chris Russell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927566", "id": "tag:google.com,2005:reader/item/000000032aaacffe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Minimal Exploration in Structured Stochastic Bandits. (arXiv:1711.00400v1 [stat.ML])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00400"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00400", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper introduces and addresses a wide class of stochastic bandit \nproblems where the function mapping the arm to the corresponding reward \nexhibits some known structural properties. Most existing structures (e.g. \nlinear, Lipschitz, unimodal, combinatorial, dueling, ...) are covered by our \nframework. We derive an asymptotic instance-specific regret lower bound for \nthese problems, and develop OSSB, an algorithm whose regret matches this \nfundamental limit. OSSB is not based on the classical principle of \"optimism in \nthe face of uncertainty\" or on Thompson sampling, and rather aims at matching \nthe minimal exploration rates of sub-optimal arms as characterized in the \nderivation of the regret lower bound. We illustrate the efficiency of OSSB \nusing numerical experiments in the case of the linear bandit problem and show \nthat OSSB outperforms existing algorithms, including Thompson sampling. \n</p>"}, "author": "Richard Combes, Stefan Magureanu, Alexandre Proutiere", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927565", "id": "tag:google.com,2005:reader/item/000000032aaad007", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Building Data-driven Models with Microstructural Images: Generalization and Interpretability. (arXiv:1711.00404v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00404"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00404", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>As data-driven methods rise in popularity in materials science applications, \na key question is how these machine learning models can be used to understand \nmicrostructure. Given the importance of process-structure-property relations \nthroughout materials science, it seems logical that models that can leverage \nmicrostructural data would be more capable of predicting property information. \nWhile there have been some recent attempts to use convolutional neural networks \nto understand microstructural images, these early studies have focused only on \nwhich featurizations yield the highest machine learning model accuracy for a \nsingle data set. This paper explores the use of convolutional neural networks \nfor classifying microstructure with a more holistic set of objectives in mind: \ngeneralization between data sets, number of features required, and \ninterpretability. \n</p>"}, "author": "Julia Ling, Maxwell Hutchinson, Erin Antono, Brian DeCost, Elizabeth A. Holm, Bryce Meredig", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927564", "id": "tag:google.com,2005:reader/item/000000032aaad00b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Piecewise Linear Neural Network verification: A comparative study. (arXiv:1711.00455v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00455"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00455", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The success of Deep Learning and its potential use in many important safety- \ncritical applications has motivated research on formal verification of Neural \nNetwork (NN) models. Despite the reputation of learned NN models to behave as \nblack boxes and the theoretical hardness of proving their properties, \nresearchers have been successful in verifying some classes of models by \nexploiting their piecewise linear structure. Unfortunately, most of these \napproaches test their algorithms without comparison with other approaches. As a \nresult, the pros and cons of the different algorithms are not well understood. \nMotivated by the need to accelerate progress in this very important area, we \ninvestigate the trade-offs of a number of different approaches based on Mixed \nInteger Programming, Satisfiability Modulo Theory, as well as a novel method \nbased on the Branch-and-Bound framework. We also propose a new data set of \nbenchmarks, in addition to a collection of pre- viously released testcases that \ncan be used to compare existing methods. Our analysis not only allows a \ncomparison to be made between different strategies, the comparison of results \nfrom different solvers also revealed implementation bugs in published methods. \nWe expect that the availability of our benchmark and the analysis of the \ndifferent approaches will allow researchers to develop and evaluate promising \napproaches for making progress on this important topic. \n</p>"}, "author": "Rudy Bunel, Ilker Turkaslan, Philip H.S. Torr, Pushmeet Kohli, M. Pawan Kumar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927563", "id": "tag:google.com,2005:reader/item/000000032aaad01d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Early prediction of the duration of protests using probabilistic Latent Dirichlet Allocation and Decision Trees. (arXiv:1711.00462v1 [cs.SI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00462"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00462", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Protests and agitations are an integral part of every democratic civil \nsociety. In recent years, South Africa has seen a large increase in its \nprotests. The objective of this paper is to provide an early prediction of the \nduration of protests from its free flowing English text description. Free \nflowing descriptions of the protests help us in capturing its various nuances \nsuch as multiple causes, courses of actions etc. Next we use a combination of \nunsupervised learning (topic modeling) and supervised learning (decision trees) \nto predict the duration of the protests. Our results show a high degree (close \nto 90%) of accuracy in early prediction of the duration of protests.We expect \nthe work to help police and other security services in planning and managing \ntheir resources in better handling protests in future. \n</p>"}, "author": "Satyakama Paul, Madhur Hasija, Tshilidzi Marwala", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509585685835", "timestampUsec": "1509585685834528", "id": "tag:google.com,2005:reader/item/000000032a72c6b8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Minimum Energy Quantized Neural Networks. (arXiv:1711.00215v1 [cs.NE])", "published": 1509585686, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00215"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00215", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This work targets the automated minimum-energy optimization of Quantized \nNeural Networks (QNNs) - networks using low precision weights and activations. \nThese networks are trained from scratch at an arbitrary fixed point precision. \nAt iso-accuracy, QNNs using fewer bits require deeper and wider network \narchitectures than networks using higher precision operators, while they \nrequire less complex arithmetic and less bits per weights. This fundamental \ntrade-off is analyzed and quantified to find the minimum energy QNN for any \nbenchmark and hence optimize energy-efficiency. To this end, the energy \nconsumption of inference is modeled for a generic hardware platform. This \nallows drawing several conclusions across different benchmarks. First, energy \nconsumption varies orders of magnitude at iso-accuracy depending on the number \nof bits used in the QNN. Second, in a typical system, BinaryNets or int4 \nimplementations lead to the minimum energy solution, outperforming int8 \nnetworks up to 2-10x at iso-accuracy. All code used for QNN training is \navailable from https://github.com/BertMoons. \n</p>"}, "author": "Bert Moons, Koen Goetschalckx, Nick Van Berckelaer, Marian Verhelst", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509585685835", "timestampUsec": "1509585685834527", "id": "tag:google.com,2005:reader/item/000000032a72c6c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Smooth Neighbors on Teacher Graphs for Semi-supervised Learning. (arXiv:1711.00258v1 [cs.LG])", "published": 1509585686, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00258"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00258", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The paper proposes an inductive semi-supervised learning method, called \nSmooth Neighbors on Teacher Graphs (SNTG). At each iteration during training, a \ngraph is dynamically constructed based on predictions of the teacher model, \ni.e., the implicit self-ensemble of models. Then the graph serves as a \nsimilarity measure with respect to which the representations of \"similar\" \nneighboring points are learned to be smooth on the low dimensional manifold. We \nachieve state-of-the-art results on semi-supervised learning benchmarks. The \nerror rates are 9.89%, 3.99% for CIFAR-10 with 4000 labels, SVHN with 500 \nlabels, respectively. In particular, the improvements are significant when the \nlabels are scarce. For non-augmented MNIST with only 20 labels, the error rate \nis reduced from previous 4.81% to 1.36%. Our method is also effective under \nnoisy supervision and shows robustness to incorrect labels. \n</p>"}, "author": "Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, Bo Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509585685835", "timestampUsec": "1509585685834526", "id": "tag:google.com,2005:reader/item/000000032a72c6cd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Avoiding Your Teacher's Mistakes: Training Neural Networks with Controlled Weak Supervision. (arXiv:1711.00313v1 [cs.LG])", "published": 1509585686, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00313"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00313", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Training deep neural networks requires massive amounts of training data, but \nfor many tasks only limited labeled data is available. This makes weak \nsupervision attractive, using weak or noisy signals like the output of \nheuristic methods or user click-through data for training. In a semi-supervised \nsetting, we can use a large set of data with weak labels to pretrain a neural \nnetwork and then fine-tune the parameters with a small amount of data with true \nlabels. This feels intuitively sub-optimal as these two independent stages \nleave the model unaware about the varying label quality. What if we could \nsomehow inform the model about the label quality? In this paper, we propose a \nsemi-supervised learning method where we train two neural networks in a \nmulti-task fashion: a \"target network\" and a \"confidence network\". The target \nnetwork is optimized to perform a given task and is trained using a large set \nof unlabeled data that are weakly annotated. We propose to weight the gradient \nupdates to the target network using the scores provided by the second \nconfidence network, which is trained on a small amount of supervised data. Thus \nwe avoid that the weight updates computed from noisy labels harm the quality of \nthe target network model. We evaluate our learning strategy on two different \ntasks: document ranking and sentiment classification. The results demonstrate \nthat our approach not only enhances the performance compared to the baselines \nbut also speeds up the learning process from weak labels. \n</p>"}, "author": "Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, Jaap Kamps", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509585685835", "timestampUsec": "1509585685834525", "id": "tag:google.com,2005:reader/item/000000032a72c6d8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Hierarchical Representations for Efficient Architecture Search. (arXiv:1711.00436v1 [cs.LG])", "published": 1509585686, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00436"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00436", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We explore efficient neural architecture search methods and present a simple \nyet powerful evolutionary algorithm that can discover new architectures \nachieving state of the art results. Our approach combines a novel hierarchical \ngenetic representation scheme that imitates the modularized design pattern \ncommonly adopted by human experts, and an expressive search space that supports \ncomplex topologies. Our algorithm efficiently discovers architectures that \noutperform a large number of manually designed models for image classification, \nobtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to \nImageNet, which is competitive with the best existing neural architecture \nsearch approaches and represents the new state of the art for evolutionary \nstrategies on this task. We also present results using random search, achieving \n0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing \nthe architecture search time from 36 hours down to 1 hour. \n</p>"}, "author": "Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray Kavukcuoglu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509585685835", "timestampUsec": "1509585685834522", "id": "tag:google.com,2005:reader/item/000000032a72c6e3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning. (arXiv:1710.11417v1 [cs.AI] CROSS LISTED)", "published": 1509585686, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11417"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11417", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c4c3a69\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c4c3a69&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Combining deep model-free reinforcement learning with on-line planning is a \npromising approach to building on the successes of deep RL. On-line planning \nwith look-ahead trees has proven successful in environments where transition \nmodels are known a priori. However, in complex environments where transition \nmodels need to be learned from data, the deficiencies of learned models have \nlimited their utility for planning. To address these challenges, we propose \nTreeQN, a differentiable, recursive, tree-structured model that serves as a \ndrop-in replacement for any value function network in deep RL with discrete \nactions. TreeQN dynamically constructs a tree by recursively applying a \ntransition model in a learned abstract state space and then aggregating \npredicted rewards and state-values using a tree backup to estimate Q-values. We \nalso propose ATreeC, an actor-critic variant that augments TreeQN with a \nsoftmax layer to form a stochastic policy network. Both approaches are trained \nend-to-end, such that the learned model is optimised for its actual use in the \nplanner. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a \nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et \nal., 2017) on multiple Atari games, with deeper trees often outperforming \nshallower ones. We also present a qualitative analysis that sheds light on the \ntrees learned by TreeQN. \n</p>"}, "author": "Gregory Farquhar, Tim Rockt&#xe4;schel, Maximilian Igl, Shimon Whiteson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509497758963", "timestampUsec": "1509497758962576", "id": "tag:google.com,2005:reader/item/0000000329c3ad91", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Super-polynomial separations for quantum-enhanced reinforcement learning. (arXiv:1710.11160v1 [quant-ph])", "published": 1509497759, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11160"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11160", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent work on quantum machine learning has demonstrated that quantum \ncomputers can offer dramatic improvements over classical devices for data \nmining, prediction and classification. However, less is known about the \nadvantages using quantum computers may bring in the more general setting of \nreinforcement learning, where learning is achieved via interaction with a task \nenvironment that provides occasional rewards. Reinforcement learning can \nincorporate data-analysis-oriented learning settings as special cases, but also \nincludes more complex situations where, e.g., reinforcing feedback is delayed. \nIn a few recent works, Grover-type amplification has been utilized to construct \nquantum agents that achieve up-to-quadratic improvements in learning \nefficiency. These encouraging results have left open the key question of \nwhether super-polynomial improvements in learning times are possible for \ngenuine reinforcement learning problems, that is problems that go beyond the \nother more restricted learning paradigms. In this work, we provide a family of \nsuch genuine reinforcement learning tasks, and we construct quantum-enhanced \nlearners which learn super-polynomially faster than any classical reinforcement \nlearning model. \n</p>"}, "author": "Vedran Dunjko, Yi-Kai Liu, Xingyao Wu, Jacob M. Taylor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509497758963", "timestampUsec": "1509497758962575", "id": "tag:google.com,2005:reader/item/0000000329c3ad9b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "ChainerMN: Scalable Distributed Deep Learning Framework. (arXiv:1710.11351v1 [cs.DC])", "published": 1509497759, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11351"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11351", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>One of the keys for deep learning to have made a breakthrough in various \nfields was to utilize high computing powers centering around GPUs. Enabling the \nuse of further computing abilities by distributed processing is essential not \nonly to make the deep learning bigger and faster but also to tackle unsolved \nchallenges. We present the design, implementation, and evaluation of ChainerMN, \nthe distributed deep learning framework we have developed. We demonstrate that \nChainerMN can scale the learning process of the ResNet-50 model to the ImageNet \ndataset up to 128 GPUs with the parallel efficiency of 90%. \n</p>"}, "author": "Takuya Akiba, Keisuke Fukuda, Shuji Suzuki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509497758963", "timestampUsec": "1509497758962574", "id": "tag:google.com,2005:reader/item/0000000329c3ada4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Parametrizing filters of a CNN with a GAN. (arXiv:1710.11386v1 [cs.LG])", "published": 1509497759, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11386"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11386", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is commonly agreed that the use of relevant invariances as a good \nstatistical bias is important in machine-learning. However, most approaches \nthat explicitly incorporate invariances into a model architecture only make use \nof very simple transformations, such as translations and rotations. Hence, \nthere is a need for methods to model and extract richer transformations that \ncapture much higher-level invariances. To that end, we introduce a tool \nallowing to parametrize the set of filters of a trained convolutional neural \nnetwork with the latent space of a generative adversarial network. We then show \nthat the method can capture highly non-linear invariances of the data by \nvisualizing their effect in the data space. \n</p>"}, "author": "Yannic Kilcher, Gary Becigneul, Thomas Hofmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509497758963", "timestampUsec": "1509497758962573", "id": "tag:google.com,2005:reader/item/0000000329c3adac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Deep Learning as a Mixed Convex-Combinatorial Optimization Problem. (arXiv:1710.11573v1 [cs.LG])", "published": 1509497759, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11573"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11573", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>As neural networks grow deeper and wider, learning networks with \nhard-threshold activations is becoming increasingly important, both for network \nquantization, which can drastically reduce time and energy requirements, and \nfor creating large integrated systems of deep networks, which may have \nnon-differentiable components and must avoid vanishing and exploding gradients \nfor effective learning. However, since gradient descent is not applicable to \nhard-threshold functions, it is not clear how to learn them in a principled \nway. We address this problem by observing that setting targets for \nhard-threshold hidden units in order to minimize loss is a discrete \noptimization problem, and can be solved as such. The discrete optimization goal \nis to find a set of targets such that each unit, including the output, has a \nlinearly separable problem to solve. Given these targets, the network \ndecomposes into individual perceptrons, which can then be learned with standard \nconvex approaches. Based on this, we develop a recursive mini-batch algorithm \nfor learning deep hard-threshold networks that includes the popular but poorly \njustified straight-through estimator as a special case. Empirically, we show \nthat our algorithm improves classification accuracy in a number of settings, \nincluding for AlexNet and ResNet-18 on ImageNet, when compared to the \nstraight-through estimator. \n</p>"}, "author": "Abram L. Friesen, Pedro Domingos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509497758963", "timestampUsec": "1509497758962572", "id": "tag:google.com,2005:reader/item/0000000329c3adb1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm. (arXiv:1710.11622v1 [cs.LG])", "published": 1509497759, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11622"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11622", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning to learn is a powerful paradigm for enabling models to learn from \ndata more effectively and efficiently. A popular approach to meta-learning is \nto train a recurrent model to read in a training dataset as input and output \nthe parameters of a learned model, or output predictions for new test inputs. \nAlternatively, a more recent approach to meta-learning aims to acquire deep \nrepresentations that can be effectively fine-tuned, via standard gradient \ndescent, to new tasks. In this paper, we consider the meta-learning problem \nfrom the perspective of universality, formalizing the notion of learning \nalgorithm approximation and comparing the expressive power of the \naforementioned recurrent models to the more recent approaches that embed \ngradient descent into the meta-learner. In particular, we seek to answer the \nfollowing question: does deep representation combined with standard gradient \ndescent have sufficient capacity to approximate any learning algorithm? We find \nthat this is indeed true, and further find, in our experiments, that \ngradient-based meta-learning consistently leads to learning strategies that \ngeneralize more widely compared to those represented by recurrent models. \n</p>"}, "author": "Chelsea Finn, Sergey Levine", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185103", "id": "tag:google.com,2005:reader/item/0000000329c22bb1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Onsets and Frames: Dual-Objective Piano Transcription. (arXiv:1710.11153v1 [cs.SD])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11153"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11153", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of transcribing polyphonic piano music with an \nemphasis on generalizing to unseen instruments. We use deep neural networks and \npropose a novel approach that predicts onsets and frames using both CNNs and \nLSTMs. This model predicts pitch onset events and then uses those predictions \nto condition framewise pitch predictions. During inference, we restrict the \npredictions from the framewise detector by not allowing a new note to start \nunless the onset detector also agrees that an onset for that pitch is present \nin the frame. We focus on improving onsets and offsets together instead of \neither in isolation as we believe it correlates better with human musical \nperception. This technique results in over a 100% relative improvement in note \nwith offset score on the MAPS dataset. \n</p>"}, "author": "Curtis Hawthorne, Erich Elsen, Jialin Song, Adam Roberts, Ian Simon, Colin Raffel, Jesse Engel, Sageev Oore, Douglas Eck", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185102", "id": "tag:google.com,2005:reader/item/0000000329c22bb8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "CrescendoNet: A Simple Deep Convolutional Neural Network with Ensemble Behavior. (arXiv:1710.11176v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11176"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11176", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce a new deep convolutional neural network, CrescendoNet, by \nstacking simple building blocks without residual connections. Each Crescendo \nblock contains independent convolution paths with increased depths. The numbers \nof convolution layers and parameters are only increased linearly in Crescendo \nblocks. In experiments, CrescendoNet with only 15 layers outperforms almost all \nnetworks without residual connections on benchmark datasets, CIFAR10, CIFAR100, \nand SVHN. Given sufficient amount of data as in SVHN dataset, CrescendoNet with \n15 layers and 4.1M parameters can match the performance of DenseNet-BC with 250 \nlayers and 15.3M parameters. CrescendoNet provides a new way to construct high \nperformance deep convolutional neural networks without residual connections. \nMoreover, through investigating the behavior and performance of subnetworks in \nCrescendoNet, we note that the high performance of CrescendoNet may come from \nits implicit ensemble behavior, which differs from the FractalNet that is also \na deep convolutional neural network without residual connections. Furthermore, \nthe independence between paths in CrescendoNet allows us to introduce a new \npath-wise training procedure, which can reduce the memory needed for training. \n</p>"}, "author": "Xiang Zhang, Nishant Vishwamitra, Hongxin Hu, Feng Luo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185101", "id": "tag:google.com,2005:reader/item/0000000329c22bbb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Sample-efficient Policy Optimization with Stein Control Variate. (arXiv:1710.11198v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11198"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11198", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Policy gradient methods have achieved remarkable successes in solving \nchallenging reinforcement learning problems. However, it still often suffers \nfrom the large variance issue on policy gradient estimation, which leads to \npoor sample efficiency during training. In this work, we propose a control \nvariate method to effectively reduce variance for policy gradient methods. \nMotivated by the Stein's identity, our method extends the previous control \nvariate methods used in REINFORCE and advantage actor-critic by introducing \nmore general action-dependent baseline functions. Empirical studies show that \nour method significantly improves the sample efficiency of the state-of-the-art \npolicy gradient approaches. \n</p>"}, "author": "Hao Liu, Yihao Feng, Yi Mao, Dengyong Zhou, Jian Peng, Qiang Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185100", "id": "tag:google.com,2005:reader/item/0000000329c22bbd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Critical Points of Neural Networks: Analytical Forms and Landscape Properties. (arXiv:1710.11205v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11205"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11205", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Due to the success of deep learning to solving a variety of challenging \nmachine learning tasks, there is a rising interest in understanding loss \nfunctions for training neural networks from a theoretical aspect. Particularly, \nthe properties of critical points and the landscape around them are of \nimportance to determine the convergence performance of optimization algorithms. \nIn this paper, we provide full (necessary and sufficient) characterization of \nthe analytical forms for the critical points (as well as global minimizers) of \nthe square loss functions for various neural networks. We show that the \nanalytical forms of the critical points characterize the values of the \ncorresponding loss functions as well as the necessary and sufficient conditions \nto achieve global minimum. Furthermore, we exploit the analytical forms of the \ncritical points to characterize the landscape properties for the loss functions \nof these neural networks. One particular conclusion is that: The loss function \nof linear networks has no spurious local minimum, while the loss function of \none-hidden-layer nonlinear networks with ReLU activation function does have \nlocal minimum that is not global minimum. \n</p>"}, "author": "Yi Zhou, Yingbin Liang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185099", "id": "tag:google.com,2005:reader/item/0000000329c22bc5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "How Algorithmic Confounding in Recommendation Systems Increases Homogeneity and Decreases Utility. (arXiv:1710.11214v1 [cs.CY])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11214"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11214", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c4c3e2e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c4c3e2e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Recommendation systems occupy an expanding role in everyday decision making, \nfrom choice of movies and household goods to consequential medical and legal \ndecisions. The data used to train and test these systems is algorithmically \nconfounded in that it is the result of a feedback loop between human choices \nand an existing algorithmic recommendation system. Using simulations, we \ndemonstrate that algorithmic confounding can disadvantage algorithms in \ntraining, bias held-out evaluation, and amplify homogenization of user behavior \nwithout gains in utility. \n</p>"}, "author": "Allison J.B. Chaney, Brandon M. Stewart, Barbara E. Engelhardt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185098", "id": "tag:google.com,2005:reader/item/0000000329c22bca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Fast and Scalable Learning of Sparse Changes in High-Dimensional Gaussian Graphical Model Structure. (arXiv:1710.11223v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11223"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11223", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c53c57d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c53c57d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We focus on the problem of estimating the change in the dependency structures \nof two $p$-dimensional Gaussian Graphical models (GGMs). Previous studies for \nsparse change estimation in GGMs involve expensive and difficult non-smooth \noptimization. We propose a novel method, DIFFEE for estimating DIFFerential \nnetworks via an Elementary Estimator under a high-dimensional situation. DIFFEE \nis solved through a faster and closed form solution that enables it to work in \nlarge-scale settings. We conduct a rigorous statistical analysis showing that \nsurprisingly DIFFEE achieves the same asymptotic convergence rates as the \nstate-of-the-art estimators that are much more difficult to compute. Our \nexperimental results on multiple synthetic datasets and one real-world data \nabout brain connectivity show strong performance improvements over baselines, \nas well as significant computational benefits. \n</p>"}, "author": "Beilun Wang, Arshdeep Sekhon, Yanjun Qi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185097", "id": "tag:google.com,2005:reader/item/0000000329c22bce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Prototype Matching Networks for Large-Scale Multi-label Genomic Sequence Classification. (arXiv:1710.11238v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11238"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11238", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>One of the fundamental tasks in understanding genomics is the problem of \npredicting Transcription Factor Binding Sites (TFBSs). With more than hundreds \nof Transcription Factors (TFs) as labels, genomic-sequence based TFBS \nprediction is a challenging multi-label classification task. There are two \nmajor biological mechanisms for TF binding: (1) sequence-specific binding \npatterns on genomes known as \"motifs\" and (2) interactions among TFs known as \nco-binding effects. In this paper, we propose a novel deep architecture, the \nPrototype Matching Network (PMN) to mimic the TF binding mechanisms. Our PMN \nmodel automatically extracts prototypes (\"motif\"-like features) for each TF \nthrough a novel prototype-matching loss. Borrowing ideas from few-shot matching \nmodels, we use the notion of support set of prototypes and an LSTM to learn how \nTFs interact and bind to genomic sequences. On a reference TFBS dataset with \n$2.1$ $million$ genomic sequences, PMN significantly outperforms baselines and \nvalidates our design choices empirically. To our knowledge, this is the first \ndeep learning architecture that introduces prototype learning and considers \nTF-TF interactions for large-scale TFBS prediction. Not only is the proposed \narchitecture accurate, but it also models the underlying biology. \n</p>"}, "author": "Jack Lanchantin, Arshdeep Sekhon, Ritambhara Singh, Yanjun Qi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185096", "id": "tag:google.com,2005:reader/item/0000000329c22bd3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Time-lagged autoencoders: Deep learning of slow collective variables for molecular kinetics. (arXiv:1710.11239v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11239"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11239", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Inspired by the success of deep learning techniques in the physical and \nchemical sciences, we apply a modification of an autoencoder type deep neural \nnetwork to the task of dimension reduction of molecular dynamics data. We can \nshow that our time-lagged autoencoder reliably finds low-dimensional embeddings \nfor high-dimensional feature spaces which capture the slow dynamics of the \nunderlying stochastic processes - beyond the capabilities of linear dimension \nreduction techniques. \n</p>"}, "author": "Christoph Wehmeyer, Frank No&#xe9;", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185095", "id": "tag:google.com,2005:reader/item/0000000329c22bd8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Approximation Algorithms for $\\ell_0$-Low Rank Approximation. (arXiv:1710.11253v1 [cs.DS])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11253"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11253", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the $\\ell_0$-Low Rank Approximation Problem, where the goal is, \ngiven an $m \\times n$ matrix $A$, to output a rank-$k$ matrix $A'$ for which \n$\\|A'-A\\|_0$ is minimized. Here, for a matrix $B$, $\\|B\\|_0$ denotes the number \nof its non-zero entries. This NP-hard variant of low rank approximation is \nnatural for problems with no underlying metric, and its goal is to minimize the \nnumber of disagreeing data positions. We provide approximation algorithms which \nsignificantly improve the running time and approximation factor of previous \nwork. For $k &gt; 1$, we show how to find, in poly$(mn)$ time for every $k$, a \nrank $O(k \\log(n/k))$ matrix $A'$ for which $\\|A'-A\\|_0 \\leq O(k^2 \\log(n/k)) \n\\mathrm{OPT}$. To the best of our knowledge, this is the first algorithm with \nprovable guarantees for the $\\ell_0$-Low Rank Approximation Problem for $k &gt; \n1$, even for bicriteria algorithms. For the well-studied case when $k = 1$, we \ngive a $(2+\\epsilon)$-approximation in {\\it sublinear time}, which is \nimpossible for other variants of low rank approximation such as for the \nFrobenius norm. We strengthen this for the well-studied case of binary matrices \nto obtain a $(1+O(\\psi))$-approximation in sublinear time, where $\\psi = \n\\mathrm{OPT}/\\lVert A\\rVert_0$. For small $\\psi$, our approximation factor is \n$1+o(1)$. \n</p>"}, "author": "Karl Bringmann, Pavel Kolev, David P. Woodruff", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185094", "id": "tag:google.com,2005:reader/item/0000000329c22bda", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Adaptive Sampling Strategies for Stochastic Optimization. (arXiv:1710.11258v1 [math.OC])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11258"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11258", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a stochastic optimization method that adaptively \ncontrols the sample size used in the computation of gradient approximations. \nUnlike other variance reduction techniques that either require additional \nstorage or the regular computation of full gradients, the proposed method \nreduces variance by increasing the sample size as needed. The decision to \nincrease the sample size is governed by an inner product test that ensures that \nsearch directions are descent directions with high probability. We show that \nthe inner product test improves upon the well known norm test, and can be used \nas a basis for an algorithm that is globally convergent on nonconvex functions \nand enjoys a global linear rate of convergence on strongly convex functions. \nNumerical experiments on logistic regression problems illustrate the \nperformance of the algorithm. \n</p>"}, "author": "Raghu Bollapragada, Richard Byrd, Jorge Nocedal", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185093", "id": "tag:google.com,2005:reader/item/0000000329c22bdd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Implicit Manifold Learning on Generative Adversarial Networks. (arXiv:1710.11260v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11260"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11260", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper raises an implicit manifold learning perspective in Generative \nAdversarial Networks (GANs), by studying how the support of the learned \ndistribution, modelled as a submanifold $\\mathcal{M}_{\\theta}$, perfectly match \nwith $\\mathcal{M}_{r}$, the support of the real data distribution. We show that \noptimizing Jensen-Shannon divergence forces $\\mathcal{M}_{\\theta}$ to perfectly \nmatch with $\\mathcal{M}_{r}$, while optimizing Wasserstein distance does not. \nOn the other hand, by comparing the gradients of the Jensen-Shannon divergence \nand the Wasserstein distances ($W_1$ and $W_2^2$) in their primal forms, we \nconjecture that Wasserstein $W_2^2$ may enjoy desirable properties such as \nreduced mode collapse. It is therefore interesting to design new distances that \ninherit the best from both distances. \n</p>"}, "author": "Kry Yik Chau Lui, Yanshuai Cao, Maxime Gazeau, Kelvin Shuangjian Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185092", "id": "tag:google.com,2005:reader/item/0000000329c22be0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Theoretical and Computational Guarantees of Mean Field Variational Inference for Community Detection. (arXiv:1710.11268v1 [math.ST])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11268"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11268", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The mean filed variational Bayes method is becoming increasingly popular in \nstatistics and machine learning. Its iterative Coordinate Ascent Variational \nInference algorithm has been widely applied to large scale Bayesian inference. \nSee Blei et al. (2017) for a recent comprehensive review. Despite the \npopularity of the mean field method there exist remarkably little fundamental \ntheoretical justifications. To the best of our knowledge, the iterative \nalgorithm has never been investigated for any high dimensional and complex \nmodel. In this paper, we study the mean field method for community detection \nunder the Stochastic Block Model. For an iterative Batch Coordinate Ascent \nVariational Inference algorithm, we show that it has a linear convergence rate \nand converges to the minimax rate within $\\log n$ iterations. This complements \nthe results of Bickel et al. (2013) which studied the global minimum of the \nmean field variational Bayes and obtained asymptotic normal estimation of \nglobal model parameters. In addition, we obtained similar optimality results \nfor Gibbs sampling and an iterative procedure to calculate maximum likelihood \nestimation, which can be of independent interest. \n</p>"}, "author": "Anderson Y. Zhang, Harrison H. Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185091", "id": "tag:google.com,2005:reader/item/0000000329c22be2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Empirical analysis of non-linear activation functions for Deep Neural Networks in classification tasks. (arXiv:1710.11272v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11272"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11272", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We provide an overview of several non-linear activation functions in a neural \nnetwork architecture that have proven successful in many machine learning \napplications. We conduct an empirical analysis on the effectiveness of using \nthese function on the MNIST classification task, with the aim of clarifying \nwhich functions produce the best results overall. Based on this first set of \nresults, we examine the effects of building deeper architectures with an \nincreasing number of hidden layers. We also survey the impact of using, on the \nsame task, different initialisation schemes for the weights of our neural \nnetwork. Using these sets of experiments as a base, we conclude by providing a \noptimal neural network architecture that yields impressive results in accuracy \non the MNIST classification task. \n</p>"}, "author": "Giovanni Alcantara", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185090", "id": "tag:google.com,2005:reader/item/0000000329c22be7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Approximating Continuous Functions by ReLU Nets of Minimal Width. (arXiv:1710.11278v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11278"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11278", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This article concerns the expressive power of depth in deep feed-forward \nneural nets with ReLU activations. Specifically, we answer the following \nquestion: for a fixed $d\\geq 1,$ what is the minimal width $w$ so that neural \nnets with ReLU activations, input dimension $d$, hidden layer widths at most \n$w,$ and arbitrary depth can approximate any continuous function of $d$ \nvariables arbitrarily well. It turns out that this minimal width is exactly \nequal to $d+1.$ That is, if all the hidden layer widths are bounded by $d$, \nthen even in the infinite depth limit, ReLU nets can only express a very \nlimited class of functions. On the other hand, we show that any continuous \nfunction on the $d$-dimensional unit cube can be approximated to arbitrary \nprecision by ReLU nets in which all hidden layers have width exactly $d+1.$ Our \nconstruction gives quantitative depth estimates for such an approximation. \n</p>"}, "author": "Boris Hanin, Mark Sellke", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185089", "id": "tag:google.com,2005:reader/item/0000000329c22bea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Tensor Sketching: Sparsification and Rank-One Projection. (arXiv:1710.11298v1 [stat.ME])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11298"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11298", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c53c918\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c53c918&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we investigate effective sketching schemes for high \ndimensional multilinear arrays or tensors. More specifically, we propose a \nnovel tensor sparsification algorithm that retains a subset of the entries of a \ntensor in a judicious way, and prove that it can attain a given level of \napproximation accuracy in terms of tensor spectral norm with a much smaller \nsample complexity when compared with existing approaches. In particular, we \nshow that for a $k$th order $d\\times\\cdots\\times d$ cubic tensor of stable rank \n$r_s$, the sample size requirement for achieving a relative error $\\varepsilon$ \nis, up to a logarithmic factor, of the order $r_s^{1/2} d^{k/2} /\\varepsilon$ \nwhen $\\varepsilon$ is relatively large, and $r_s d /\\varepsilon^2$ and \nessentially optimal when $\\varepsilon$ is sufficiently small. It is especially \nnoteworthy that the sample size requirement for achieving a high accuracy is of \nan order independent of $k$. In addition, we show that for larger \n$\\varepsilon$, the space complexity can be improved for higher order tensors \n($k\\ge 5$) by another sketching method via rank-one projection. To further \ndemonstrate the utility of our techniques, we also study how higher order \nsingular value decomposition (HOSVD) of large tensors can be efficiently \napproximated based on both sketching schemes. \n</p>"}, "author": "Dong Xia, Ming Yuan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185088", "id": "tag:google.com,2005:reader/item/0000000329c22bf0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Algorithmic learning of probability distributions from random data in the limit. (arXiv:1710.11303v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11303"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11303", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of identifying a probability distribution for some given \nrandomly sampled data in the limit, in the context of algorithmic learning \ntheory as proposed recently by Vinanyi and Chater. We show that there exists a \ncomputable partial learner for the computable probability measures, while by \nBienvenu, Monin and Shen it is known that there is no computable learner for \nthe computable probability measures. Our main result is the characterization of \nthe oracles that compute explanatory learners for the computable (continuous) \nprobability measures as the high oracles. This provides an analogue of a \nwell-known result of Adleman and Blum in the context of learning computable \nprobability distributions. We also discuss related learning notions such as \nbehaviorally correct learning and orther variations of explanatory learning, in \nthe context of learning probability distributions from data. \n</p>"}, "author": "George Barmpalias, Frank Stephan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185087", "id": "tag:google.com,2005:reader/item/0000000329c22bf9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Characterizing the structural diversity of complex networks across domains. (arXiv:1710.11304v1 [cs.SI])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11304"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11304", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The structure of complex networks has been of interest in many scientific and \nengineering disciplines over the decades. A number of studies in the field have \nbeen focused on finding the common properties among different kinds of networks \nsuch as heavy-tail degree distribution, small-worldness and modular structure \nand they have tried to establish a theory of structural universality in complex \nnetworks. However, there is no comprehensive study of network structure across \na diverse set of domains in order to explain the structural diversity we \nobserve in the real-world networks. In this paper, we study 986 real-world \nnetworks of diverse domains ranging from ecological food webs to online social \nnetworks along with 575 networks generated from four popular network models. \nOur study utilizes a number of machine learning techniques such as random \nforest and confusion matrix in order to show the relationships among network \ndomains in terms of network structure. Our results indicate that there are some \npartitions of network categories in which networks are hard to distinguish \nbased purely on network structure. We have found that these partitions of \nnetwork categories tend to have similar underlying functions, constraints \nand/or generative mechanisms of networks even though networks in the same \npartition have different origins, e.g., biological processes, results of \nengineering by human being, etc. This suggests that the origin of a network, \nwhether it's biological, technological or social, may not necessarily be a \ndecisive factor of the formation of similar network structure. Our findings \nshed light on the possible direction along which we could uncover the hidden \nprinciples for the structural diversity of complex networks. \n</p>"}, "author": "Kansuke Ikehara, Aaron Clauset", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185086", "id": "tag:google.com,2005:reader/item/0000000329c22c04", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "The Exact Solution to Rank-1 L1-norm TUCKER2 Decomposition. (arXiv:1710.11306v1 [cs.DS])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11306"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11306", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study rank-1 {L1-norm-based TUCKER2} (L1-TUCKER2) decomposition of 3-way \ntensors, treated as a collection of $N$ $D \\times M$ matrices that are to be \njointly decomposed. Our contributions are as follows. i) We prove that the \nproblem is equivalent to combinatorial optimization over $N$ antipodal-binary \nvariables. ii) We derive the first two algorithms in the literature for its \nexact solution. The first algorithm has cost exponential in $N$; the second one \nhas cost polynomial in $N$ (under a mild assumption). Our algorithms are \naccompanied by formal complexity analysis. iii) We conduct numerical studies to \ncompare the performance of exact L1-TUCKER2 (proposed) with standard HOSVD, \nHOOI, GLRAM, PCA, L1-PCA, and TPCA-L1. Our studies show that L1-TUCKER2 \noutperforms (in tensor approximation) all the above counterparts when the \nprocessed data are outlier corrupted. \n</p>"}, "author": "Panos P. Markopoulos, Dimitris G. Chachlakis, Evangelos E. Papalexakis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185085", "id": "tag:google.com,2005:reader/item/0000000329c22c0b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Rate-optimal Meta Learning of Classification Error. (arXiv:1710.11315v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11315"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11315", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Meta learning of optimal classifier error rates allows an experimenter to \nempirically estimate the intrinsic ability of any estimator to discriminate \nbetween two populations, circumventing the difficult problem of estimating the \noptimal Bayes classifier. To this end we propose a weighted nearest neighbor \n(WNN) graph estimator for a tight bound on the Bayes classification error; the \nHenze-Penrose (HP) divergence. Similar to recently proposed HP estimators \n[berisha2016], the proposed estimator is non-parametric and does not require \ndensity estimation. However, unlike previous approaches the proposed estimator \nis rate-optimal, i.e., its mean squared estimation error (MSEE) decays to zero \nat the fastest possible rate of $O(1/M+1/N)$ where $M,N$ are the sample sizes \nof the respective populations. We illustrate the proposed WNN meta estimator \nfor several simulated and real data sets. \n</p>"}, "author": "Morteza Noshad Iranzad, Alfred O. Hero III", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185084", "id": "tag:google.com,2005:reader/item/0000000329c22c10", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Tensor Regression Meets Gaussian Processes. (arXiv:1710.11345v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11345"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11345", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Low-rank tensor regression, a new model class that learns high-order \ncorrelation from data, has recently received considerable attention. At the \nsame time, Gaussian processes (GP) are well-studied machine learning models for \nstructure learning. In this paper, we demonstrate interesting connections \nbetween the two, especially for multi-way data analysis. We show that low-rank \ntensor regression is essentially learning a multi-linear kernel in Gaussian \nprocesses, and the low-rank assumption translates to the constrained Bayesian \ninference problem. We prove the oracle inequality and derive the average case \nlearning curve for the equivalent GP model. Our finding implies that low-rank \ntensor regression, though empirically successful, is highly dependent on the \neigenvalues of covariance functions as well as variable correlations. \n</p>"}, "author": "Rose Yu, Guangyu Li, Yan Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185083", "id": "tag:google.com,2005:reader/item/0000000329c22c22", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Latent Space Oddity: on the Curvature of Deep Generative Models. (arXiv:1710.11379v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11379"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11379", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep generative models provide a systematic way to learn nonlinear data \ndistributions, through a set of latent variables and a nonlinear \"generator\" \nfunction that maps latent points into the input space. The nonlinearity of the \ngenerator imply that the latent space gives a distorted view of the input \nspace. Under mild conditions, we show that this distortion can be characterized \nby a stochastic Riemannian metric, and demonstrate that distances and \ninterpolants are significantly improved under this metric. This in turn \nimproves probability distributions, sampling algorithms and clustering in the \nlatent space. Our geometric analysis further reveals that current generators \nprovide poor variance estimates and we propose a new generator architecture \nwith vastly improved variance estimates. Results are demonstrated on \nconvolutional and fully connected variational autoencoders, but the formalism \neasily generalize to other deep generative models. \n</p>"}, "author": "Georgios Arvanitidis, Lars Kai Hansen, S&#xf8;ren Hauberg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185082", "id": "tag:google.com,2005:reader/item/0000000329c22c35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Semantic Interpolation in Implicit Models. (arXiv:1710.11381v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11381"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11381", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In implicit models, one often interpolates between sampled points in latent \nspace. As we show in this paper, care needs to be taken to match-up the \ndistributional assumptions on code vectors with the geometry of the \ninterpolating paths. Otherwise, typical assumptions about the quality and \nsemantics of in-between points may not be justified. Based on our analysis we \npropose to modify the prior code distribution to put significantly more \nprobability mass closer to the origin. As a result, linear interpolation paths \nare not only shortest paths, but they are also guaranteed to pass through \nhigh-density regions, irrespective of the dimensionality of the latent space. \nExperiments on standard benchmark image datasets demonstrate clear visual \nimprovements in the quality of the generated samples and exhibit more \nmeaningful interpolation paths. \n</p>"}, "author": "Yannic Kilcher, Aurelien Lucchi, Thomas Hofmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185081", "id": "tag:google.com,2005:reader/item/0000000329c22c42", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Flexible Prior Distributions for Deep Generative Models. (arXiv:1710.11383v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11383"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11383", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of training generative models with deep neural \nnetworks as generators, i.e. to map latent codes to data points. Whereas the \ndominant paradigm combines simple priors over codes with complex deterministic \nmodels, we argue that it might be advantageous to use more flexible code \ndistributions. We demonstrate how these distributions can be induced directly \nfrom the data. The benefits include: more powerful generative models, better \nmodeling of latent structure and explicit control of the degree of \ngeneralization. \n</p>"}, "author": "Yannic Kilcher, Aurelien Lucchi, Thomas Hofmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185080", "id": "tag:google.com,2005:reader/item/0000000329c22c48", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Parametrizing filters of a CNN with a GAN. (arXiv:1710.11386v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11386"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11386", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is commonly agreed that the use of relevant invariances as a good \nstatistical bias is important in machine-learning. However, most approaches \nthat explicitly incorporate invariances into a model architecture only make use \nof very simple transformations, such as translations and rotations. Hence, \nthere is a need for methods to model and extract richer transformations that \ncapture much higher-level invariances. To that end, we introduce a tool \nallowing to parametrize the set of filters of a trained convolutional neural \nnetwork with the latent space of a generative adversarial network. We then show \nthat the method can capture highly non-linear invariances of the data by \nvisualizing their effect in the data space. \n</p>"}, "author": "Yannic Kilcher, Gary Becigneul, Thomas Hofmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185079", "id": "tag:google.com,2005:reader/item/0000000329c22c4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Updating the VESICLE-CNN Synapse Detector. (arXiv:1710.11397v1 [cs.CV])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11397"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11397", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c53cc9b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c53cc9b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present an updated version of the VESICLE-CNN algorithm presented by \nRoncal et al. (2014). The original implementation makes use of a patch-based \napproach. This methodology is known to be slow due to repeated computations. We \nupdate this implementation to be fully convolutional through the use of dilated \nconvolutions, recovering the expanded field of view achieved through the use of \nstrided maxpools, but without a degradation of spatial resolution. This updated \nimplementation performs as well as the original implementation, but with a \n$600\\times$ speedup at test time. We release source code and data into the \npublic domain. \n</p>"}, "author": "Andrew Warrington, Frank Wood", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185078", "id": "tag:google.com,2005:reader/item/0000000329c22c55", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning. (arXiv:1710.11417v1 [cs.AI])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11417"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11417", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c5ab062\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c5ab062&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Combining deep model-free reinforcement learning with on-line planning is a \npromising approach to building on the successes of deep RL. On-line planning \nwith look-ahead trees has proven successful in environments where transition \nmodels are known a priori. However, in complex environments where transition \nmodels need to be learned from data, the deficiencies of learned models have \nlimited their utility for planning. To address these challenges, we propose \nTreeQN, a differentiable, recursive, tree-structured model that serves as a \ndrop-in replacement for any value function network in deep RL with discrete \nactions. TreeQN dynamically constructs a tree by recursively applying a \ntransition model in a learned abstract state space and then aggregating \npredicted rewards and state-values using a tree backup to estimate Q-values. We \nalso propose ATreeC, an actor-critic variant that augments TreeQN with a \nsoftmax layer to form a stochastic policy network. Both approaches are trained \nend-to-end, such that the learned model is optimised for its actual use in the \nplanner. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a \nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et \nal., 2017) on multiple Atari games, with deeper trees often outperforming \nshallower ones. We also present a qualitative analysis that sheds light on the \ntrees learned by TreeQN. \n</p>"}, "author": "Gregory Farquhar, Tim Rockt&#xe4;schel, Maximilian Igl, Shimon Whiteson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185077", "id": "tag:google.com,2005:reader/item/0000000329c22c5d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Physics-guided Neural Networks (PGNN): An Application in Lake Temperature Modeling. (arXiv:1710.11431v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11431"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11431", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper introduces a novel framework for learning data science models by \nusing the scientific knowledge encoded in physics-based models. This framework, \ntermed as physics-guided neural network (PGNN), leverages the output of \nphysics-based model simulations along with observational features to generate \npredictions using a neural network architecture. Further, we present a novel \nclass of learning objective for training neural networks, which ensures that \nthe model predictions not only show lower errors on the training data but are \nalso \\emph{consistent} with the known physics. We illustrate the effectiveness \nof PGNN for the problem of lake temperature modeling, where physical \nrelationships between the temperature, density, and depth of water are used in \nthe learning of neural network model parameters. By using scientific knowledge \nto guide the construction and learning of neural networks, we are able to show \nthat the proposed framework ensures better generalizability as well as physical \nconsistency of results. \n</p>"}, "author": "Anuj Karpatne, William Watkins, Jordan Read, Vipin Kumar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185076", "id": "tag:google.com,2005:reader/item/0000000329c22c69", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning Neural Representations of Human Cognition across Many fMRI Studies. (arXiv:1710.11438v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11438"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11438", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Cognitive neuroscience is enjoying rapid increase in extensive public \nbrain-imaging datasets. It opens the door to large-scale statistical models. \nFinding a unified perspective for all available data calls for scalable and \nautomated solutions to an old challenge: how to aggregate heterogeneous \ninformation on brain function into a universal cognitive system that relates \nmental operations/cognitive processes/psychological tasks to brain networks? We \ncast this challenge in a machine-learning approach to predict conditions from \nstatistical brain maps across different studies. For this, we leverage \nmulti-task learning and multi-scale dimension reduction to learn \nlow-dimensional representations of brain images that carry cognitive \ninformation and can be robustly associated with psychological stimuli. Our \nmulti-dataset classification model achieves the best prediction performance on \nseveral large reference datasets, compared to models without cognitive-aware \nlow-dimension representations; it brings a substantial performance boost to the \nanalysis of small datasets, and can be introspected to identify universal \ntemplate cognitive concepts. \n</p>"}, "author": "Arthur Mensch (PARIETAL, NEUROSPIN), Julien Mairal (Thoth, LJK), Danilo Bzdok, Bertrand Thirion (PARIETAL, NEUROSPIN), Ga&#xeb;l Varoquaux (PARIETAL, NEUROSPIN)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185075", "id": "tag:google.com,2005:reader/item/0000000329c22c72", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Statistical Speech Enhancement Based on Probabilistic Integration of Variational Autoencoder and Non-Negative Matrix Factorization. (arXiv:1710.11439v1 [cs.SD])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11439"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11439", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a statistical method of single-channel speech enhancement \nthat uses a variational autoencoder (VAE) as a prior distribution on clean \nspeech. A standard approach to speech enhancement is to train a deep neural \nnetwork (DNN) to take noisy speech as input and output clean speech. Although \nthis supervised approach requires a very large amount of pair data for \ntraining, it is not robust against unknown environments. Another approach is to \nuse non-negative matrix factorization (NMF) based on basis spectra trained on \nclean speech in advance and those adapted to noise on the fly. This \nsemi-supervised approach, however, causes considerable signal distortion in \nenhanced speech due to the unrealistic assumption that speech spectrograms are \nlinear combinations of the basis spectra. Replacing the poor linear generative \nmodel of clean speech in NMF with a VAE---a powerful nonlinear deep generative \nmodel---trained on clean speech, we formulate a unified probabilistic \ngenerative model of noisy speech. Given noisy speech as observed data, we can \nsample clean speech from its posterior distribution. The proposed method \noutperformed the conventional DNN-based method in unseen noisy environments. \n</p>"}, "author": "Yoshiaki Bando, Masato Mimura, Katsutoshi Itoyama, Kazuyoshi Yoshii, Tatsuya Kawahara", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185074", "id": "tag:google.com,2005:reader/item/0000000329c22c88", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Guarding Against Adversarial Domain Shifts with Counterfactual Regularization. (arXiv:1710.11469v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11469"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11469", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>When training a deep network for image classification, one can broadly \ndistinguish between two types of latent features of images that will drive the \nclassification: (i) \"immutable\" or \"core\" features that are inherent to the \nobject in question and do not change substantially from one instance of the \nobject to another and (ii) \"mutable\" or \"style\" features such as position, \nrotation or image quality but also more complex ones like hair color or posture \nfor images of persons. The distribution of the style features can change in the \nfuture. While transfer learning would try to adapt to a shift in the \ndistribution(s), we here want to protect against future adversarial domain \nshifts, arising through changing style features, by ideally not using the \nmutable style features altogether. \n</p> \n<p>There are two broad scenarios and we show how exploiting grouping information \nin the data helps in both. (a) If the style features are known explicitly (e.g. \nrotation) one usually proceeds by using data augmentation. By exploiting the \ngrouping information about which original image an augmented sample belongs to, \nwe can reduce the sample size required to achieve invariance to the style \nfeature in question. (b) Sometimes the style features are not known explicitly \nbut we still have information about samples that belong to the same underlying \nobject (such as different pictures of the same person). By constraining the \nclassification to give the same forecast for all instances that belong to the \nsame object, we show how using this grouping information leads to invariance to \nsuch implicit style features and helps to protect against adversarial domain \nshifts. \n</p> \n<p>We provide a causal framework for the problem and treat groups of instances \nof the same object as counterfactuals under different interventions on the \nmutable style features. We show links to questions of fairness, transfer \nlearning and adversarial examples. \n</p>"}, "author": "Christina Heinze-Deml, Nicolai Meinshausen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185073", "id": "tag:google.com,2005:reader/item/0000000329c22c92", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Compact Multi-Class Boosted Trees. (arXiv:1710.11547v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11547"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11547", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Gradient boosted decision trees are a popular machine learning technique, in \npart because of their ability to give good accuracy with small models. We \ndescribe two extensions to the standard tree boosting algorithm designed to \nincrease this advantage. The first improvement extends the boosting formalism \nfrom scalar-valued trees to vector-valued trees. This allows individual trees \nto be used as multiclass classifiers, rather than requiring one tree per class, \nand drastically reduces the model size required for multiclass problems. We \nalso show that some other popular vector-valued gradient boosted trees \nmodifications fit into this formulation and can be easily obtained in our \nimplementation. The second extension, layer-by-layer boosting, takes smaller \nsteps in function space, which is empirically shown to lead to a faster \nconvergence and to a more compact ensemble. We have added both improvements to \nthe open-source TensorFlow Boosted trees (TFBT) package, and we demonstrate \ntheir efficacy on a variety of multiclass datasets. We expect these extensions \nwill be of particular interest to boosted tree applications that require small \nmodels, such as embedded devices, applications requiring fast inference, or \napplications desiring more interpretable models. \n</p>"}, "author": "Natalia Ponomareva, Thomas Colthurst, Gilbert Hendry, Salem Haykal, Soroush Radpour", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185072", "id": "tag:google.com,2005:reader/item/0000000329c22c9a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "TF Boosted Trees: A scalable TensorFlow based framework for gradient boosting. (arXiv:1710.11555v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11555"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11555", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>TF Boosted Trees (TFBT) is a new open-sourced frame-work for the distributed \ntraining of gradient boosted trees. It is based on TensorFlow, and its \ndistinguishing features include a novel architecture, automatic loss \ndifferentiation, layer-by-layer boosting that results in smaller ensembles and \nfaster prediction, principled multi-class handling, and a number of \nregularization techniques to prevent overfitting. \n</p>"}, "author": "Natalia Ponomareva, Soroush Radpour, Gilbert Hendry, Salem Haykal, Thomas Colthurst, Petr Mitrichev, Alexander Grushetsky", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185071", "id": "tag:google.com,2005:reader/item/0000000329c22ca1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning Graph Convolution Filters from Data Manifold. (arXiv:1710.11577v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11577"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11577", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolution Neural Network (CNN) has gained tremendous success in computer \nvision tasks with its outstanding ability to capture the local latent features. \nRecently, there has been an increasing interest in extending CNNs to the \ngeneral spatial domain. Although various types of graph and geometric \nconvolution methods have been proposed, their connections to traditional \n2D-convolution are not well-understood. In this paper, we show that depthwise \nseparable convolution is the key to close the gap, based on which we derive a \nnovel Depthwise Separable Graph Convolution that subsumes existing graph \nconvolution methods as special cases of our formulation. Experiments show that \nthe proposed approach consistently outperforms other graph and geometric \nconvolution baselines on benchmark datasets in multiple domains. \n</p>"}, "author": "Guokun Lai, Hanxiao Liu, Yiming Yang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185070", "id": "tag:google.com,2005:reader/item/0000000329c22ca9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Partial Least Squares Random Forest Ensemble Regression as a Soft Sensor. (arXiv:1710.11595v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11595"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11595", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Six simple, dynamic soft sensor methodologies with two update conditions were \ncompared on two experimentally-obtained datasets and one simulated dataset. The \nsoft sensors investigated were: moving window partial least squares regression \n(and a recursive variant), moving window random forest regression, feedforward \nneural networks, mean moving window, and a novel random forest partial least \nsquares regression ensemble (RF-PLS). We found that, on two of the datasets \nstudied, very small window sizes (4 samples) led to the lowest prediction \nerrors. The RF-PLS method offered the lowest one-step-ahead prediction errors \ncompared to those of the other methods, and demonstrated greater stability at \nlarger time lags than moving window PLS alone. We found that this method most \nadequately modeled the datasets that did not feature purely monotonic increases \nin property values. In general, we observed that linear models deteriorated \nmost rapidly at more delayed model update conditions while nonlinear methods \ntended to provide predictions that approached those from a simple mean moving \nwindow. Other data dependent findings are presented and discussed. \n</p>"}, "author": "Casey Kneale, Steven Brown", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185059", "id": "tag:google.com,2005:reader/item/0000000329c22ceb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Inhomogeneous Hypergraph Clustering with Applications. (arXiv:1709.01249v3 [cs.LG] UPDATED)", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.01249"}], "alternate": [{"href": "http://arxiv.org/abs/1709.01249", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c5ab2e3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c5ab2e3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Hypergraph partitioning is an important problem in machine learning, computer \nvision and network analytics. A widely used method for hypergraph partitioning \nrelies on minimizing a normalized sum of the costs of partitioning hyperedges \nacross clusters. Algorithmic solutions based on this approach assume that \ndifferent partitions of a hyperedge incur the same cost. However, this \nassumption fails to leverage the fact that different subsets of vertices within \nthe same hyperedge may have different structural importance. We hence propose a \nnew hypergraph clustering technique, termed inhomogeneous hypergraph \npartitioning, which assigns different costs to different hyperedge cuts. We \nprove that inhomogeneous partitioning produces a quadratic approximation to the \noptimal solution if the inhomogeneous costs satisfy submodularity constraints. \nMoreover, we demonstrate that inhomogenous partitioning offers significant \nperformance improvements in applications such as structure learning of \nrankings, subspace segmentation and motif clustering. \n</p>"}, "author": "Pan Li, Olgica Milenkovic", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185053", "id": "tag:google.com,2005:reader/item/0000000329c22cf6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Tracking Tetrahymena Pyriformis Cells using Decision Trees. (arXiv:1207.3127v1 [cs.CV] CROSS LISTED)", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1207.3127"}], "alternate": [{"href": "http://arxiv.org/abs/1207.3127", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Matching cells over time has long been the most difficult step in cell \ntracking. In this paper, we approach this problem by recasting it as a \nclassification problem. We construct a feature set for each cell, and compute a \nfeature difference vector between a cell in the current frame and a cell in a \nprevious frame. Then we determine whether the two cells represent the same cell \nover time by training decision trees as our binary classifiers. With the output \nof decision trees, we are able to formulate an assignment problem for our cell \nassociation task and solve it using a modified version of the Hungarian \nalgorithm. \n</p>"}, "author": "Quan Wang, Yan Ou, A. Agung Julius, Kim L. Boyer, Min Jun Kim", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185052", "id": "tag:google.com,2005:reader/item/0000000329c22cf9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Semantic Context Forests for Learning-Based Knee Cartilage Segmentation in 3D MR Images. (arXiv:1307.2965v2 [cs.CV] CROSS LISTED)", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1307.2965"}], "alternate": [{"href": "http://arxiv.org/abs/1307.2965", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The automatic segmentation of human knee cartilage from 3D MR images is a \nuseful yet challenging task due to the thin sheet structure of the cartilage \nwith diffuse boundaries and inhomogeneous intensities. In this paper, we \npresent an iterative multi-class learning method to segment the femoral, tibial \nand patellar cartilage simultaneously, which effectively exploits the spatial \ncontextual constraints between bone and cartilage, and also between different \ncartilages. First, based on the fact that the cartilage grows in only certain \narea of the corresponding bone surface, we extract the distance features of not \nonly to the surface of the bone, but more informatively, to the densely \nregistered anatomical landmarks on the bone surface. Second, we introduce a set \nof iterative discriminative classifiers that at each iteration, probability \ncomparison features are constructed from the class confidence maps derived by \npreviously learned classifiers. These features automatically embed the semantic \ncontext information between different cartilages of interest. Validated on a \ntotal of 176 volumes from the Osteoarthritis Initiative (OAI) dataset, the \nproposed approach demonstrates high robustness and accuracy of segmentation in \ncomparison with existing state-of-the-art MR cartilage segmentation methods. \n</p>"}, "author": "Quan Wang, Dijia Wu, Le Lu, Meizhu Liu, Kim L. Boyer, Shaohua Kevin Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185051", "id": "tag:google.com,2005:reader/item/0000000329c22cfe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "DGM: A deep learning algorithm for solving partial differential equations. (arXiv:1708.07469v1 [q-fin.MF] CROSS LISTED)", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.07469"}], "alternate": [{"href": "http://arxiv.org/abs/1708.07469", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>High-dimensional PDEs have been a longstanding computational challenge. We \npropose a deep learning algorithm similar in spirit to Galerkin methods, using \na deep neural network instead of linear combinations of basis functions. The \nPDE is approximated with a deep neural network, which is trained on random \nbatches of spatial points to satisfy the differential operator and boundary \nconditions. The algorithm is mesh-less, which is key since meshes become \ninfeasible in higher dimensions. Instead of forming a mesh, sequences of \nspatial points are randomly sampled. We implement the approach for American \noptions (a type of free-boundary PDE which is widely used in finance) in up to \n100 dimensions. We call the algorithm a \"Deep Galerkin Method (DGM)\". \n</p>"}, "author": "Justin Sirignano, Konstantinos Spiliopoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408105", "id": "tag:google.com,2005:reader/item/0000000329c20aec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Super-polynomial separations for quantum-enhanced reinforcement learning. (arXiv:1710.11160v1 [quant-ph])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11160"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11160", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent work on quantum machine learning has demonstrated that quantum \ncomputers can offer dramatic improvements over classical devices for data \nmining, prediction and classification. However, less is known about the \nadvantages using quantum computers may bring in the more general setting of \nreinforcement learning, where learning is achieved via interaction with a task \nenvironment that provides occasional rewards. Reinforcement learning can \nincorporate data-analysis-oriented learning settings as special cases, but also \nincludes more complex situations where, e.g., reinforcing feedback is delayed. \nIn a few recent works, Grover-type amplification has been utilized to construct \nquantum agents that achieve up-to-quadratic improvements in learning \nefficiency. These encouraging results have left open the key question of \nwhether super-polynomial improvements in learning times are possible for \ngenuine reinforcement learning problems, that is problems that go beyond the \nother more restricted learning paradigms. In this work, we provide a family of \nsuch genuine reinforcement learning tasks, and we construct quantum-enhanced \nlearners which learn super-polynomially faster than any classical reinforcement \nlearning model. \n</p>"}, "author": "Vedran Dunjko, Yi-Kai Liu, Xingyao Wu, Jacob M. Taylor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408104", "id": "tag:google.com,2005:reader/item/0000000329c20af6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Indirect Supervision for Relation Extraction using Question-Answer Pairs. (arXiv:1710.11169v1 [cs.CL])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11169"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11169", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Automatic relation extraction (RE) for types of interest is of great \nimportance for interpreting massive text corpora in an efficient manner. \nTraditional RE models have heavily relied on human-annotated corpus for \ntraining, which can be costly in generating labeled data and become obstacles \nwhen dealing with more relation types. Thus, more RE extraction systems have \nshifted to be built upon training data automatically acquired by linking to \nknowledge bases (distant supervision). However, due to the incompleteness of \nknowledge bases and the context-agnostic labeling, the training data collected \nvia distant supervision (DS) can be very noisy. In recent years, as increasing \nattention has been brought to tackling question-answering (QA) tasks, user \nfeedback or datasets of such tasks become more accessible. In this paper, we \npropose a novel framework, ReQuest, to leverage question-answer pairs as an \nindirect source of supervision for relation extraction, and study how to use \nsuch supervision to reduce noise induced from DS. Our model jointly embeds \nrelation mentions, types, QA entity mention pairs and text features in two \nlow-dimensional spaces (RE and QA), where objects with same relation types or \nsemantically similar question-answer pairs have similar representations. Shared \nfeatures connect these two spaces, carrying clearer semantic knowledge from \nboth sources. ReQuest, then use these learned embeddings to estimate the types \nof test relation mentions. We formulate a global objective function and adopt a \nnovel margin-based QA loss to reduce noise in DS by exploiting semantic \nevidence from the QA dataset. Our experimental results achieve an average of \n11% improvement in F1 score on two public RE datasets combined with TREC QA \ndataset. \n</p>"}, "author": "Zeqiu Wu, Xiang Ren, Frank F. Xu, Ji Li, Jiawei Han", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408103", "id": "tag:google.com,2005:reader/item/0000000329c20afc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Improve SAT-solving with Machine Learning. (arXiv:1710.11204v1 [cs.AI])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11204"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11204", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this project, we aimed to improve the runtime of Minisat, a \nConflict-Driven Clause Learning (CDCL) solver that solves the Propositional \nBoolean Satisfiability (SAT) problem. We first used a logistic regression model \nto predict the satisfiability of propositional boolean formulae after fixing \nthe values of a certain fraction of the variables in each formula. We then \napplied the logistic model and added a preprocessing period to Minisat to \ndetermine the preferable initial value (either true or false) of each boolean \nvariable using a Monte-Carlo approach. Concretely, for each Monte-Carlo trial, \nwe fixed the values of a certain ratio of randomly selected variables, and \ncalculated the confidence that the resulting sub-formula is satisfiable with \nour logistic regression model. The initial value of each variable was set based \non the mean confidence scores of the trials that started from the literals of \nthat variable. We were particularly interested in setting the initial values of \nthe backbone variables correctly, which are variables that have the same value \nin all solutions of a SAT formula. Our Monte-Carlo method was able to set 78% \nof the backbones correctly. Excluding the preprocessing time, compared with the \ndefault setting of Minisat, the runtime of Minisat for satisfiable formulae \ndecreased by 23%. However, our method did not outperform vanilla Minisat in \nruntime, as the decrease in the conflicts was outweighed by the long runtime of \nthe preprocessing period. \n</p>"}, "author": "Haoze Wu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408102", "id": "tag:google.com,2005:reader/item/0000000329c20afe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Fast and Scalable Learning of Sparse Changes in High-Dimensional Gaussian Graphical Model Structure. (arXiv:1710.11223v1 [cs.LG])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11223"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11223", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We focus on the problem of estimating the change in the dependency structures \nof two $p$-dimensional Gaussian Graphical models (GGMs). Previous studies for \nsparse change estimation in GGMs involve expensive and difficult non-smooth \noptimization. We propose a novel method, DIFFEE for estimating DIFFerential \nnetworks via an Elementary Estimator under a high-dimensional situation. DIFFEE \nis solved through a faster and closed form solution that enables it to work in \nlarge-scale settings. We conduct a rigorous statistical analysis showing that \nsurprisingly DIFFEE achieves the same asymptotic convergence rates as the \nstate-of-the-art estimators that are much more difficult to compute. Our \nexperimental results on multiple synthetic datasets and one real-world data \nabout brain connectivity show strong performance improvements over baselines, \nas well as significant computational benefits. \n</p>"}, "author": "Beilun Wang, Arshdeep Sekhon, Yanjun Qi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408101", "id": "tag:google.com,2005:reader/item/0000000329c20b07", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Prototype Matching Networks for Large-Scale Multi-label Genomic Sequence Classification. (arXiv:1710.11238v1 [cs.LG])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11238"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11238", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>One of the fundamental tasks in understanding genomics is the problem of \npredicting Transcription Factor Binding Sites (TFBSs). With more than hundreds \nof Transcription Factors (TFs) as labels, genomic-sequence based TFBS \nprediction is a challenging multi-label classification task. There are two \nmajor biological mechanisms for TF binding: (1) sequence-specific binding \npatterns on genomes known as \"motifs\" and (2) interactions among TFs known as \nco-binding effects. In this paper, we propose a novel deep architecture, the \nPrototype Matching Network (PMN) to mimic the TF binding mechanisms. Our PMN \nmodel automatically extracts prototypes (\"motif\"-like features) for each TF \nthrough a novel prototype-matching loss. Borrowing ideas from few-shot matching \nmodels, we use the notion of support set of prototypes and an LSTM to learn how \nTFs interact and bind to genomic sequences. On a reference TFBS dataset with \n$2.1$ $million$ genomic sequences, PMN significantly outperforms baselines and \nvalidates our design choices empirically. To our knowledge, this is the first \ndeep learning architecture that introduces prototype learning and considers \nTF-TF interactions for large-scale TFBS prediction. Not only is the proposed \narchitecture accurate, but it also models the underlying biology. \n</p>"}, "author": "Jack Lanchantin, Arshdeep Sekhon, Ritambhara Singh, Yanjun Qi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408100", "id": "tag:google.com,2005:reader/item/0000000329c20b0e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Adversarial Advantage Actor-Critic Model for Task-Completion Dialogue Policy Learning. (arXiv:1710.11277v1 [cs.CL])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11277"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11277", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a new method --- adversarial advantage actor-critic \n(Adversarial A2C), which significantly improves the efficiency of dialogue \npolicy learning in task-completion dialogue systems. Inspired by generative \nadversarial networks (GAN), we train a discriminator to differentiate \nresponses/actions generated by dialogue agents from responses/actions by \nexperts. Then, we incorporate the discriminator as another critic into the \nadvantage actor-critic (A2C) framework, to encourage the dialogue agent to \nexplore state-action within the regions where the agent takes actions similar \nto those of the experts. Experimental results in a movie-ticket booking domain \nshow that the proposed Adversarial A2C can accelerate policy exploration \nefficiently. \n</p>"}, "author": "Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, Yun-Nung Chen, Kam-Fai Wong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408099", "id": "tag:google.com,2005:reader/item/0000000329c20b14", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Deep Forward and Inverse Perceptual Models for Tracking and Prediction. (arXiv:1710.11311v1 [cs.RO])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11311"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11311", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c5ab571\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c5ab571&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We consider the problems of learning forward models that map state to \nhigh-dimensional images and inverse models that map high-dimensional images to \nstate in robotics. Specifically, we present a perceptual model for generating \nvideo frames from state with deep networks, and provide a framework for its use \nin tracking and prediction tasks. We show that our proposed model greatly \noutperforms standard deconvolutional methods and GANs for image generation, \nproducing clear, photo-realistic images. We also develop a convolutional neural \nnetwork model for state estimation and compare the result to an Extended Kalman \nFilter to estimate robot trajectories. We validate all models on a real robotic \nsystem. \n</p>"}, "author": "Alexander Lambert, Amirreza Shaban, Amit Raj, Zhen Liu, Byron Boots", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408098", "id": "tag:google.com,2005:reader/item/0000000329c20b17", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Generating Natural Adversarial Examples. (arXiv:1710.11342v1 [cs.LG])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11342"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11342", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c637d83\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c637d83&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Due to their complex nature, it is hard to characterize the ways in which \nmachine learning models can misbehave or be exploited when deployed. Recent \nwork on adversarial examples, i.e. inputs with minor perturbations that result \nin substantially different model predictions, is helpful in evaluating the \nrobustness of these models by exposing the adversarial scenarios where they \nfail. However, these malicious perturbations are often unnatural, not \nsemantically meaningful, and not applicable to complicated domains such as \nlanguage. In this paper, we propose a framework to generate natural and legible \nadversarial examples by searching in semantic space of dense and continuous \ndata representation, utilizing the recent advances in generative adversarial \nnetworks. We present generated adversaries to demonstrate the potential of the \nproposed approach for black-box classifiers in a wide range of applications \nsuch as image classification, textual entailment, and machine translation. We \ninclude experiments to show that the generated adversaries are natural, legible \nto humans, and useful in evaluating and analyzing black-box classifiers. \n</p>"}, "author": "Zhengli Zhao, Dheeru Dua, Sameer Singh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408097", "id": "tag:google.com,2005:reader/item/0000000329c20b1e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Parametrizing filters of a CNN with a GAN. (arXiv:1710.11386v1 [cs.LG])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11386"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11386", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is commonly agreed that the use of relevant invariances as a good \nstatistical bias is important in machine-learning. However, most approaches \nthat explicitly incorporate invariances into a model architecture only make use \nof very simple transformations, such as translations and rotations. Hence, \nthere is a need for methods to model and extract richer transformations that \ncapture much higher-level invariances. To that end, we introduce a tool \nallowing to parametrize the set of filters of a trained convolutional neural \nnetwork with the latent space of a generative adversarial network. We then show \nthat the method can capture highly non-linear invariances of the data by \nvisualizing their effect in the data space. \n</p>"}, "author": "Yannic Kilcher, Gary Becigneul, Thomas Hofmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408096", "id": "tag:google.com,2005:reader/item/0000000329c20b27", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning. (arXiv:1710.11417v1 [cs.AI])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11417"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11417", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Combining deep model-free reinforcement learning with on-line planning is a \npromising approach to building on the successes of deep RL. On-line planning \nwith look-ahead trees has proven successful in environments where transition \nmodels are known a priori. However, in complex environments where transition \nmodels need to be learned from data, the deficiencies of learned models have \nlimited their utility for planning. To address these challenges, we propose \nTreeQN, a differentiable, recursive, tree-structured model that serves as a \ndrop-in replacement for any value function network in deep RL with discrete \nactions. TreeQN dynamically constructs a tree by recursively applying a \ntransition model in a learned abstract state space and then aggregating \npredicted rewards and state-values using a tree backup to estimate Q-values. We \nalso propose ATreeC, an actor-critic variant that augments TreeQN with a \nsoftmax layer to form a stochastic policy network. Both approaches are trained \nend-to-end, such that the learned model is optimised for its actual use in the \nplanner. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a \nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et \nal., 2017) on multiple Atari games, with deeper trees often outperforming \nshallower ones. We also present a qualitative analysis that sheds light on the \ntrees learned by TreeQN. \n</p>"}, "author": "Gregory Farquhar, Tim Rockt&#xe4;schel, Maximilian Igl, Shimon Whiteson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408095", "id": "tag:google.com,2005:reader/item/0000000329c20b2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Regret Minimization for Partially Observable Deep Reinforcement Learning. (arXiv:1710.11424v1 [cs.LG])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11424"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11424", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep reinforcement learning algorithms that estimate state and state-action \nvalue functions have been shown to be effective in a variety of challenging \ndomains, including learning control strategies from raw image pixels. However, \nalgorithms that estimate state and state-action value functions typically \nassume a fully observed state and must compensate for partial or non-Markovian \nobservations by using finite-length frame-history observations or recurrent \nnetworks. In this work, we propose a new deep reinforcement learning algorithm \nbased on counterfactual regret minimization that iteratively updates an \napproximation to a cumulative clipped advantage function and is robust to \npartially observed state. We demonstrate that on several partially observed \nreinforcement learning tasks, this new class of algorithms can substantially \noutperform strong baseline methods: on Pong with single-frame observations, and \non the challenging Doom (ViZDoom) and Minecraft (Malm\\\"o) first-person \nnavigation benchmarks. \n</p>"}, "author": "Peter H. Jin, Sergey Levine, Kurt Keutzer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408094", "id": "tag:google.com,2005:reader/item/0000000329c20b34", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "SemTK: An Ontology-first, Open Source Semantic Toolkit for Managing and Querying Knowledge Graphs. (arXiv:1710.11531v1 [cs.AI])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11531"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11531", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The relatively recent adoption of Knowledge Graphs as an enabling technology \nin multiple high-profile artificial intelligence and cognitive applications has \nled to growing interest in the Semantic Web technology stack. Many \nsemantics-related tools, however, are focused on serving experts with a deep \nunderstanding of semantic technologies. For example, triplification of \nrelational data is available but there is no open source tool that allows a \nuser unfamiliar with OWL/RDF to import data into a semantic triple store in an \nintuitive manner. Further, many tools require users to have a working \nunderstanding of SPARQL to query data. Casual users interested in benefiting \nfrom the power of Knowledge Graphs have few tools available for exploring, \nquerying, and managing semantic data. We present SemTK, the Semantics Toolkit, \na user-friendly suite of tools that allow both expert and non-expert semantics \nusers convenient ingestion of relational data, simplified query generation, and \nmore. The exploration of ontologies and instance data is performed through \nSPARQLgraph, an intuitive web-based user interface in SemTK understandable and \nnavigable by a lay user. The open source version of SemTK is available at \n<a href=\"http://semtk.research.ge.com.\">this http URL</a> \n</p>"}, "author": "Paul Cuddihy, Justin McHugh, Jenny Weisenberg Williams, Varish Mulwad, Kareem S. Aggour", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408093", "id": "tag:google.com,2005:reader/item/0000000329c20b3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning Graph Convolution Filters from Data Manifold. (arXiv:1710.11577v1 [cs.LG])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11577"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11577", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolution Neural Network (CNN) has gained tremendous success in computer \nvision tasks with its outstanding ability to capture the local latent features. \nRecently, there has been an increasing interest in extending CNNs to the \ngeneral spatial domain. Although various types of graph and geometric \nconvolution methods have been proposed, their connections to traditional \n2D-convolution are not well-understood. In this paper, we show that depthwise \nseparable convolution is the key to close the gap, based on which we derive a \nnovel Depthwise Separable Graph Convolution that subsumes existing graph \nconvolution methods as special cases of our formulation. Experiments show that \nthe proposed approach consistently outperforms other graph and geometric \nconvolution baselines on benchmark datasets in multiple domains. \n</p>"}, "author": "Guokun Lai, Hanxiao Liu, Yiming Yang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408092", "id": "tag:google.com,2005:reader/item/0000000329c20b43", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Whodunnit? Crime Drama as a Case for Natural Language Understanding. (arXiv:1710.11601v1 [cs.CL])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11601"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11601", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we argue that crime drama exemplified in television programs \nsuch as CSI:Crime Scene Investigation is an ideal testbed for approximating \nreal-world natural language understanding and the complex inferences associated \nwith it. We propose to treat crime drama as a new inference task, capitalizing \non the fact that each episode poses the same basic question (i.e., who \ncommitted the crime) and naturally provides the answer when the perpetrator is \nrevealed. We develop a new dataset based on CSI episodes, formalize perpetrator \nidentification as a sequence labeling problem, and develop an LSTM-based model \nwhich learns from multi-modal data. Experimental results show that an \nincremental inference strategy is key to making accurate guesses as well as \nlearning from representations fusing textual, visual, and acoustic input. \n</p>"}, "author": "Lea Frermann, Shay B. Cohen, Mirella Lapata", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408091", "id": "tag:google.com,2005:reader/item/0000000329c20b4f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm. (arXiv:1710.11622v1 [cs.LG])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11622"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11622", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning to learn is a powerful paradigm for enabling models to learn from \ndata more effectively and efficiently. A popular approach to meta-learning is \nto train a recurrent model to read in a training dataset as input and output \nthe parameters of a learned model, or output predictions for new test inputs. \nAlternatively, a more recent approach to meta-learning aims to acquire deep \nrepresentations that can be effectively fine-tuned, via standard gradient \ndescent, to new tasks. In this paper, we consider the meta-learning problem \nfrom the perspective of universality, formalizing the notion of learning \nalgorithm approximation and comparing the expressive power of the \naforementioned recurrent models to the more recent approaches that embed \ngradient descent into the meta-learner. In particular, we seek to answer the \nfollowing question: does deep representation combined with standard gradient \ndescent have sufficient capacity to approximate any learning algorithm? We find \nthat this is indeed true, and further find, in our experiments, that \ngradient-based meta-learning consistently leads to learning strategies that \ngeneralize more widely compared to those represented by recurrent models. \n</p>"}, "author": "Chelsea Finn, Sergey Levine", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582667", "id": "tag:google.com,2005:reader/item/0000000329abb9d2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "One-shot and few-shot learning of word embeddings. (arXiv:1710.10280v1 [cs.CL])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10280"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10280", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Standard deep learning systems require thousands or millions of examples to \nlearn a concept, and cannot integrate new concepts easily. By contrast, humans \nhave an incredible ability to do one-shot or few-shot learning. For instance, \nfrom just hearing a word used in a sentence, humans can infer a great deal \nabout it, by leveraging what the syntax and semantics of the surrounding words \ntells us. Here, we draw inspiration from this to highlight a simple technique \nby which deep recurrent networks can similarly exploit their prior knowledge to \nlearn a useful representation for a new word from little data. This could make \nnatural language processing systems much more flexible, by allowing them to \nlearn continually from the new words they encounter. \n</p>"}, "author": "Andrew K. Lampinen, James L. McClelland", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582666", "id": "tag:google.com,2005:reader/item/0000000329abb9de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Probability Series Expansion Classifier that is Interpretable by Design. (arXiv:1710.10301v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10301"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10301", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c638249\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c638249&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This work presents a new classifier that is specifically designed to be fully \ninterpretable. This technique determines the probability of a class outcome, \nbased directly on probability assignments measured from the training data. The \naccuracy of the predicted probability can be improved by measuring more \nprobability estimates from the training data to create a series expansion that \nrefines the predicted probability. We use this work to classify four standard \ndatasets and achieve accuracies comparable to that of Random Forests. Because \nthis technique is interpretable by design, it is capable of determining the \ncombinations of features that contribute to a particular classification \nprobability for individual cases as well as the weightings of each of \ncombination of features. \n</p>"}, "author": "Sapan Agarwal, Corey M. Hudson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582665", "id": "tag:google.com,2005:reader/item/0000000329abb9e4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "A Self-Training Method for Semi-Supervised GANs. (arXiv:1710.10313v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10313"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10313", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Since the creation of Generative Adversarial Networks (GANs), much work has \nbeen done to improve their training stability, their generated image quality, \ntheir range of application but nearly none of them explored their self-training \npotential. Self-training has been used before the advent of deep learning in \norder to allow training on limited labelled training data and has shown \nimpressive results in semi-supervised learning. In this work, we combine these \ntwo ideas and make GANs self-trainable for semi-supervised learning tasks by \nexploiting their infinite data generation potential. Results show that using \neven the simplest form of self-training yields an improvement. We also show \nresults for a more complex self-training scheme that performs at least as well \nas the basic self-training scheme but with significantly less data \naugmentation. \n</p>"}, "author": "Alan Do-Omri, Dalei Wu, Xiaohua Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582664", "id": "tag:google.com,2005:reader/item/0000000329abb9ed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Spectral Graph Wavelets for Structural Role Similarity in Networks. (arXiv:1710.10321v1 [cs.SI])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10321"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10321", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Nodes residing in different parts of a graph can have similar structural \nroles within their local network topology. The identification of such roles \nprovides key insight into the organization of networks and can also be used to \ninform machine learning on graphs. However, learning structural representations \nof nodes is a challenging unsupervised-learning task, which typically involves \nmanually specifying and tailoring topological features for each node. Here we \ndevelop GraphWave, a method that represents each node's local network \nneighborhood via a low-dimensional embedding by leveraging spectral graph \nwavelet diffusion patterns. We prove that nodes with similar local network \nneighborhoods will have similar GraphWave embeddings even though these nodes \nmay reside in very different parts of the network. Our method scales linearly \nwith the number of edges and does not require any hand-tailoring of topological \nfeatures. We evaluate performance on both synthetic and real-world datasets, \nobtaining improvements of up to 71% over state-of-the-art baselines. \n</p>"}, "author": "Claire Donnat, Marinka Zitnik, David Hallac, Jure Leskovec", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582663", "id": "tag:google.com,2005:reader/item/0000000329abb9f6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Lower Bounds for Higher-Order Convex Optimization. (arXiv:1710.10329v1 [math.OC])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10329"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10329", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>State-of-the-art methods in convex and non-convex optimization employ \nhigher-order derivative information, either implicitly or explicitly. We \nexplore the limitations of higher-order optimization and prove that even for \nconvex optimization, a polynomial dependence on the approximation guarantee and \nhigher-order smoothness parameters is necessary. As a special case, we show \nNesterov's accelerated cubic regularization method to be nearly tight. \n</p>"}, "author": "Naman Agarwal, Elad Hazan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582662", "id": "tag:google.com,2005:reader/item/0000000329abb9fc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Similarity-based Multi-label Learning. (arXiv:1710.10335v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10335"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10335", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Multi-label classification is an important learning problem with many \napplications. In this work, we propose a principled similarity-based approach \nfor multi-label learning called SML. We also introduce a similarity-based \napproach for predicting the label set size. The experimental results \ndemonstrate the effectiveness of SML for multi-label classification where it is \nshown to compare favorably with a wide variety of existing algorithms across a \nrange of evaluation criterion. \n</p>"}, "author": "Ryan A. Rossi, Nesreen K. Ahmed, Hoda Eldardiry, Rong Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582661", "id": "tag:google.com,2005:reader/item/0000000329abba01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "The Implicit Bias of Gradient Descent on Separable Data. (arXiv:1710.10345v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10345"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10345", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We show that gradient descent on an unregularized logistic regression problem \nwith separable data converges to the max-margin solution. The result \ngeneralizes also to other monotone decreasing loss functions with an infimum at \ninfinity, and we also discuss a multi-class generalizations to the cross \nentropy loss. Furthermore, we show this convergence is very slow, and only \nlogarithmic in the convergence of the loss itself. This can help explain the \nbenefit of continuing to optimize the logistic or cross-entropy loss even after \nthe training error is zero and the training loss is extremely small, and, as we \nshow, even if the validation loss increases. Our methodology can also aid in \nunderstanding implicit regularization in more complex models and with other \noptimization methods. \n</p>"}, "author": "Daniel Soudry, Elad Hoffer, Nathan Srebro", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582660", "id": "tag:google.com,2005:reader/item/0000000329abba05", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Multi-level Residual Networks from Dynamical Systems View. (arXiv:1710.10348v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10348"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10348", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep residual networks (ResNets) and their variants are widely used in many \ncomputer vision applications and natural language processing tasks. However, \nthe theoretical principles for designing and training ResNets are still not \nfully understood. Recently, several points of view have emerged to try to \ninterpret ResNet theoretically, such as unraveled view, unrolled iterative \nestimation and dynamical systems view. In this paper, we adopt the dynamical \nsystems point of view, and analyze the lesioning properties of ResNet both \ntheoretically and experimentally. Based on these analyses, we additionally \npropose a novel method for accelerating ResNet training. We apply the proposed \nmethod to train ResNets and Wide ResNets for three image classification \nbenchmarks, reducing training time by more than 40\\% with superior or on-par \naccuracy. \n</p>"}, "author": "Bo Chang, Lili Meng, Eldad Haber, Frederick Tung, David Begert", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582659", "id": "tag:google.com,2005:reader/item/0000000329abba08", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Automated Design using Neural Networks and Gradient Descent. (arXiv:1710.10352v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10352"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10352", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a novel method that makes use of deep neural networks and gradient \ndecent to perform automated design on complex real world engineering tasks. Our \napproach works by training a neural network to mimic the fitness function of a \ndesign optimization task and then, using the differential nature of the neural \nnetwork, perform gradient decent to maximize the fitness. We demonstrate this \nmethods effectiveness by designing an optimized heat sink and both 2D and 3D \nairfoils that maximize the lift drag ratio under steady state flow conditions. \nWe highlight that our method has two distinct benefits over other automated \ndesign approaches. First, evaluating the neural networks prediction of fitness \ncan be orders of magnitude faster then simulating the system of interest. \nSecond, using gradient decent allows the design space to be searched much more \nefficiently then other gradient free methods. These two strengths work together \nto overcome some of the current shortcomings of automated design. \n</p>"}, "author": "Oliver Hennigh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582658", "id": "tag:google.com,2005:reader/item/0000000329abba0d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Diff-DAC: Distributed Actor-Critic for Multitask Deep Reinforcement Learning. (arXiv:1710.10363v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10363"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10363", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a multiagent distributed actor-critic algorithm for multitask \nreinforcement learning (MRL), named \\textit{Diff-DAC}. The agents are \nconnected, forming a (possibly sparse) network. Each agent is assigned a task \nand has access to data from this local task only. During the learning process, \nthe agents are able to communicate some parameters to their neighbors. Since \nthe agents incorporate their neighbors' parameters into their own learning \nrules, the information is diffused across the network, and they can learn a \ncommon policy that generalizes well across all tasks. Diff-DAC is scalable \nsince the computational complexity and communication overhead per agent grow \nwith the number of neighbors, rather than with the total number of agents. \nMoreover, the algorithm is fully distributed in the sense that agents \nself-organize, with no need for coordinator node. Diff-DAC follows an \nactor-critic scheme where the value function and the policy are approximated \nwith deep neural networks, being able to learn expressive policies from raw \ndata. As a by-product of Diff-DAC's derivation from duality theory, we provide \nnovel insights into the standard actor-critic framework, showing that it is \nactually an instance of the dual ascent method to approximate the solution of a \nlinear program. Experiments illustrate the performance of the algorithm in the \ncart-pole, inverted pendulum, and swing-up cart-pole environments. \n</p>"}, "author": "Sergio Valcarcel Macua, Aleksi Tukiainen, Daniel Garc&#xed;a-Oca&#xf1;a Hern&#xe1;ndez, David Baldazo, Enrique Munoz de Cote, Santiago Zazo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582657", "id": "tag:google.com,2005:reader/item/0000000329abba12", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Minimax Rates and Efficient Algorithms for Noisy Sorting. (arXiv:1710.10388v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10388"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10388", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>There has been a recent surge of interest in studying permutation-based \nmodels for ranking from pairwise comparison data. Despite being structurally \nricher and more robust than parametric ranking models, permutation-based models \nare less well understood statistically and generally lack efficient learning \nalgorithms. In this work, we study a prototype of permutation-based ranking \nmodels, namely, the noisy sorting model. We establish the optimal rates of \nlearning the model under two sampling procedures. Furthermore, we provide a \nfast algorithm to achieve near-optimal rates if the observations are sampled \nindependently. Along the way, we discover properties of the symmetric group \nwhich are of theoretical interest. \n</p>"}, "author": "Cheng Mao, Jonathan Weed, Philippe Rigollet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582656", "id": "tag:google.com,2005:reader/item/0000000329abba1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Trainable back-propagated functional transfer matrices. (arXiv:1710.10403v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10403"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10403", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c63869a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c63869a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Connections between nodes of fully connected neural networks are usually \nrepresented by weight matrices. In this article, functional transfer matrices \nare introduced as alternatives to the weight matrices: Instead of using real \nweights, a functional transfer matrix uses real functions with trainable \nparameters to represent connections between nodes. Multiple functional transfer \nmatrices are then stacked together with bias vectors and activations to form \ndeep functional transfer neural networks. These neural networks can be trained \nwithin the framework of back-propagation, based on a revision of the delta \nrules and the error transmission rule for functional connections. In \nexperiments, it is demonstrated that the revised rules can be used to train a \nrange of functional connections: 20 different functions are applied to neural \nnetworks with up to 10 hidden layers, and most of them gain high test \naccuracies on the MNIST database. It is also demonstrated that a functional \ntransfer matrix with a memory function can roughly memorise a non-cyclical \nsequence of 400 digits. \n</p>"}, "author": "Cheng-Hao Cai, Yanyan Xu, Dengfeng Ke, Kaile Su, Jing Sun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582655", "id": "tag:google.com,2005:reader/item/0000000329abba27", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Efficient Localized Inference for Large Graphical Models. (arXiv:1710.10404v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10404"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10404", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c6a1112\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c6a1112&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a new localized inference algorithm for answering marginalization \nqueries in large graphical models with the correlation decay property. Given a \nquery variable and a large graphical model, we define a much smaller model in a \nlocal region around the query variable in the target model so that the marginal \ndistribution of the query variable can be accurately approximated. We introduce \ntwo approximation error bounds based on the Dobrushin's comparison theorem and \napply our bounds to derive a greedy expansion algorithm that efficiently guides \nthe selection of neighbor nodes for localized inference. We verify our \ntheoretical bounds on various datasets and demonstrate that our localized \ninference algorithm can provide fast and accurate approximation for large \ngraphical models. \n</p>"}, "author": "Jinglin Chen, Jian Peng, Qiang Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582654", "id": "tag:google.com,2005:reader/item/0000000329abba2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Generalized End-to-End Loss for Speaker Verification. (arXiv:1710.10467v1 [eess.AS])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10467"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10467", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a new loss function called generalized end-to-end \n(GE2E) loss, which makes the training of speaker verification models more \nefficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike \nTE2E, the GE2E loss function updates the network in a way that emphasizes \nexamples that are difficult to verify at each step of the training process. \nAdditionally, the GE2E loss does not require an initial stage of example \nselection. With these properties, the model with new loss function learns a \nbetter model, by decreasing EER by more than 10%, in shorter period of time, by \nreducing the training time by &gt;60%. We also introduce the MultiReader \ntechnique, which allow us do domain adaptation - training more accurate model \nthat supports multiple keywords (i.e. \"OK Google\" and \"Hey Google\") as well as \nmultiple dialects. \n</p>"}, "author": "Li Wan, Quan Wang, Alan Papir, Ignacio Lopez Moreno", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582653", "id": "tag:google.com,2005:reader/item/0000000329abba36", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Speaker Diarization with LSTM. (arXiv:1710.10468v3 [eess.AS] UPDATED)", "published": 1509959154, "updated": 1509959156, "canonical": [{"href": "http://arxiv.org/abs/1710.10468"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10468", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>For many years, i-vector based speaker embedding techniques were the dominant \napproach for speaker verification and speaker diarization applications. \nHowever, mirroring the rise of deep learning in various domains, neural network \nbased speaker embeddings, also known as d-vectors, have consistently \ndemonstrated superior speaker verification performance. In this paper, we build \non the success of d-vector based speaker verification systems to develop a new \nd-vector based approach to speaker diarization. Specifically, we combine \nLSTM-based d-vector audio embeddings with recent work in non-parametric \nclustering to obtain a state-of-the-art speaker diarization system. Our \nexperiments on CALLHOME American English and 2003 NIST Rich Transcription \nEnglish conversational telephone speech (CTS) corpus suggest that d-vector \nbased diarization systems offer significant advantages over traditional \ni-vector based systems. \n</p>"}, "author": "Quan Wang, Carlton Downey, Li Wan, Philip Andrew Mansfield, Ignacio Lopez Moreno", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582652", "id": "tag:google.com,2005:reader/item/0000000329abba3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Attention-Based Models for Text-Dependent Speaker Verification. (arXiv:1710.10470v1 [eess.AS])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10470"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10470", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Attention-based models have recently shown great performance on a range of \ntasks, such as speech recognition, machine translation, and image captioning \ndue to their ability to summarize relevant information that expands through the \nentire length of an input sequence. In this paper, we analyze the usage of \nattention mechanisms to the problem of sequence summarization in our end-to-end \ntext-dependent speaker recognition system. We explore different topologies and \ntheir variants of the attention layer, and compare different pooling methods on \nthe attention weights. Ultimately, we show that attention-based models can \nimproves the Equal Error Rate (EER) of our speaker verification system by \nrelatively 14% compared to our non-attention LSTM baseline model. \n</p>"}, "author": "F A Rezaur Rahman Chowdhury, Quan Wang, Ignacio Lopez Moreno, Li Wan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582651", "id": "tag:google.com,2005:reader/item/0000000329abba4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Crime incidents embedding using restricted Boltzmann machines. (arXiv:1710.10513v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10513"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10513", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a new approach for detecting related crime series, by unsupervised \nlearning of the latent feature embeddings from narratives of crime record via \nthe Gaussian-Bernoulli Restricted Boltzmann Machines (RBM). This is a \ndrastically different approach from prior work on crime analysis, which \ntypically considers only time and location and at most category information. \nAfter the embedding, related cases are closer to each other in the Euclidean \nfeature space, and the unrelated cases are far apart, which is a good property \ncan enable subsequent analysis such as detection and clustering of related \ncases. Experiments over several series of related crime incidents hand labeled \nby the Atlanta Police Department reveal the promise of our embedding methods. \n</p>"}, "author": "Shixiang Zhu, Yao Xie", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582650", "id": "tag:google.com,2005:reader/item/0000000329abba63", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Interpretation of Neural Networks is Fragile. (arXiv:1710.10547v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10547"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10547", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In order for machine learning to be deployed and trusted in many \napplications, it is crucial to be able to reliably explain why the machine \nlearning algorithm makes certain predictions. For example, if an algorithm \nclassifies a given pathology image to be a malignant tumor, then the doctor may \nneed to know which parts of the image led the algorithm to this classification. \nHow to interpret black-box predictors is thus an important and active area of \nresearch. A fundamental question is: how much can we trust the interpretation \nitself? In this paper, we show that interpretation of deep learning predictions \nis extremely fragile in the following sense: two perceptively indistinguishable \ninputs with the same predicted label can be assigned very different \ninterpretations. We systematically characterize the fragility of several \nwidely-used feature-importance interpretation methods (saliency maps, relevance \npropagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that \neven small random perturbation can change the feature importance and new \nsystematic perturbations can lead to dramatically different interpretations \nwithout changing the label. We extend these results to show that \ninterpretations based on exemplars (e.g. influence functions) are similarly \nfragile. Our analysis of the geometry of the Hessian matrix gives insight on \nwhy fragility could be a fundamental challenge to the current interpretation \napproaches. \n</p>"}, "author": "Amirata Ghorbani, Abubakar Abid, James Zou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582649", "id": "tag:google.com,2005:reader/item/0000000329abba77", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Stochastic Zeroth-order Optimization in High Dimensions. (arXiv:1710.10551v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10551"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10551", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of optimizing a high-dimensional convex function \nusing stochastic zeroth-order query oracles. Such problems arise naturally in a \nvariety of practical applications, including optimizing experimental or \nsimulation parameters with many variables. Under sparsity assumptions on the \ngradients or function values, we present a successive component/feature \nselection algorithm and a noisy mirror descent algorithm with Lasso gradient \nestimates and show that both algorithms have convergence rates depending only \nlogarithmically on the ambient problem dimension. Empirical results verify our \ntheoretical findings and suggest that our designed algorithms outperform \nclassical zeroth-order optimization methods in the high-dimensional setting. \n</p>"}, "author": "Yining Wang, Simon Du, Sivaraman Balakrishnan, Aarti Singh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582648", "id": "tag:google.com,2005:reader/item/0000000329abba8a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Stochastic Training of Graph Convolutional Networks. (arXiv:1710.10568v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10568"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10568", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Graph convolutional networks (GCNs) are powerful deep neural networks for \ngraph-structured data. However, GCN computes nodes' representation recursively \nfrom their neighbors, making the receptive field size grow exponentially with \nthe number of layers. Previous attempts on reducing the receptive field size by \nsubsampling neighbors do not have any convergence guarantee, and their \nreceptive field size per node is still in the order of hundreds. In this paper, \nwe develop a preprocessing strategy and two control variate based algorithms to \nfurther reduce the receptive field size. Our algorithms are guaranteed to \nconverge to GCN's local optimum regardless of the neighbor sampling size. \nEmpirical results show that our algorithms have a similar convergence speed per \nepoch with the exact algorithm even using only two neighbors per node. The time \nconsumption of our algorithm on the Reddit dataset is only one fifth of \nprevious neighbor sampling algorithms. \n</p>"}, "author": "Jianfei Chen, Jun Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582647", "id": "tag:google.com,2005:reader/item/0000000329abba94", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Weight Initialization of Deep Neural Networks(DNNs) using Data Statistics. (arXiv:1710.10570v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10570"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10570", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks (DNNs) form the backbone of almost every \nstate-of-the-art technique in the fields such as computer vision, speech \nprocessing, and text analysis. The recent advances in computational technology \nhave made the use of DNNs more practical. Despite the overwhelming performances \nby DNN and the advances in computational technology, it is seen that very few \nresearchers try to train their models from the scratch. Training of DNNs still \nremains a difficult and tedious job. The main challenges that researchers face \nduring training of DNNs are the vanishing/exploding gradient problem and the \nhighly non-convex nature of the objective function which has up to million \nvariables. The approaches suggested in He and Xavier solve the vanishing \ngradient problem by providing a sophisticated initialization technique. These \napproaches have been quite effective and have achieved good results on standard \ndatasets, but these same approaches do not work very well on more practical \ndatasets. We think the reason for this is not making use of data statistics for \ninitializing the network weights. Optimizing such a high dimensional loss \nfunction requires careful initialization of network weights. In this work, we \npropose a data dependent initialization and analyze its performance against the \nstandard initialization techniques such as He and Xavier. We performed our \nexperiments on some practical datasets and the results show our algorithm's \nsuperior classification accuracy. \n</p>"}, "author": "Saiprasad Koturwar, Shabbir Merchant", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582646", "id": "tag:google.com,2005:reader/item/0000000329abba9d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Certifiable Distributional Robustness with Principled Adversarial Training. (arXiv:1710.10571v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10571"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10571", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c6a14d6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c6a14d6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Neural networks are vulnerable to adversarial examples and researchers have \nproposed many heuristic attack and defense mechanisms. We take the principled \nview of distributionally robust optimization, which guarantees performance \nunder adversarial input perturbations. By considering a Lagrangian penalty \nformulation of perturbation of the underlying data distribution in a \nWasserstein ball, we provide a training procedure that augments model parameter \nupdates with worst-case perturbations of training data. For smooth losses, our \nprocedure provably achieves moderate levels of robustness with little \ncomputational or statistical cost relative to empirical risk minimization. \nFurthermore, our statistical guarantees allow us to efficiently certify \nrobustness for the population loss. We match or outperform heuristic approaches \non supervised and reinforcement learning tasks. \n</p>"}, "author": "Aman Sinha, Hongseok Namkoong, John Duchi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582645", "id": "tag:google.com,2005:reader/item/0000000329abbaa7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Regularization approaches for support vector machines with applications to biomedical data. (arXiv:1710.10600v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10600"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10600", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The support vector machine (SVM) is a widely used machine learning tool for \nclassification based on statistical learning theory. Given a set of training \ndata, the SVM finds a hyperplane that separates two different classes of data \npoints by the largest distance. While the standard form of SVM uses L2-norm \nregularization, other regularization approaches are particularly attractive for \nbiomedical datasets where, for example, sparsity and interpretability of the \nclassifier's coefficient values are highly desired features. Therefore, in this \npaper we consider different types of regularization approaches for SVMs, and \nexplore them in both synthetic and real biomedical datasets. \n</p>"}, "author": "Daniel Lopez-Martinez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582644", "id": "tag:google.com,2005:reader/item/0000000329abbaad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Variational Continual Learning. (arXiv:1710.10628v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10628"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10628", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper develops variational continual learning (VCL), a simple but \ngeneral framework for continual learning that fuses online variational \ninference (VI) and recent advances in Monte Carlo VI for neural networks. The \nframework can successfully train both deep discriminative models and deep \ngenerative models in complex continual learning settings where existing tasks \nevolve over time and entirely new tasks emerge. Experimental results show that \nvariational continual learning outperforms state-of-the-art continual learning \nmethods on a variety of tasks, avoiding catastrophic forgetting in a fully \nautomatic way. \n</p>"}, "author": "Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, Richard E. Turner", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582643", "id": "tag:google.com,2005:reader/item/0000000329abbab4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Dimensionality reduction methods for molecular simulations. (arXiv:1710.10629v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10629"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10629", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Molecular simulations produce very high-dimensional data-sets with millions \nof data points. As analysis methods are often unable to cope with so many \ndimensions, it is common to use dimensionality reduction and clustering methods \nto reach a reduced representation of the data. Yet these methods often fail to \ncapture the most important features necessary for the construction of a Markov \nmodel. Here we demonstrate the results of various dimensionality reduction \nmethods on two simulation data-sets, one of protein folding and another of \nprotein-ligand binding. The methods tested include a k-means clustering \nvariant, a non-linear auto encoder, principal component analysis and tICA. The \ndimension-reduced data is then used to estimate the implied timescales of the \nslowest process by a Markov state model analysis to assess the quality of the \nprojection. The projected dimensions learned from the data are visualized to \ndemonstrate which conformations the various methods choose to represent the \nmolecular process. \n</p>"}, "author": "Stefan Doerr, Igor Ariz, Matthew J. Harvey, Gianni De Fabritiis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582642", "id": "tag:google.com,2005:reader/item/0000000329abbabc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "On the Consistency of Quick Shift. (arXiv:1710.10646v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10646"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10646", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Quick Shift is a popular mode-seeking and clustering algorithm. We present \nfinite sample statistical consistency guarantees for Quick Shift on mode and \ncluster recovery under mild distributional assumptions. We then apply our \nresults to construct a consistent modal regression algorithm. \n</p>"}, "author": "Heinrich Jiang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582641", "id": "tag:google.com,2005:reader/item/0000000329abbac2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "If it ain't broke, don't fix it: Sparse metric repair. (arXiv:1710.10655v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10655"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10655", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many modern data-intensive computational problems either require, or benefit \nfrom distance or similarity data that adhere to a metric. The algorithms run \nfaster or have better performance guarantees. Unfortunately, in real \napplications, the data are messy and values are noisy. The distances between \nthe data points are far from satisfying a metric. Indeed, there are a number of \ndifferent algorithms for finding the closest set of distances to the given ones \nthat also satisfy a metric (sometimes with the extra condition of being \nEuclidean). These algorithms can have unintended consequences, they can change \na large number of the original data points, and alter many other features of \nthe data. \n</p> \n<p>The goal of sparse metric repair is to make as few changes as possible to the \noriginal data set or underlying distances so as to ensure the resulting \ndistances satisfy the properties of a metric. In other words, we seek to \nminimize the sparsity (or the $\\ell_0$ \"norm\") of the changes we make to the \ndistances subject to the new distances satisfying a metric. We give three \ndifferent combinatorial algorithms to repair a metric sparsely. In one setting \nthe algorithm is guaranteed to return the sparsest solution and in the other \nsettings, the algorithms repair the metric. Without prior information, the \nalgorithms run in time proportional to the cube of the number of input data \npoints and, with prior information we can reduce the running time considerably. \n</p>"}, "author": "Anna C. Gilbert, Lalit Jain", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582640", "id": "tag:google.com,2005:reader/item/0000000329abbacb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Regularization for Deep Learning: A Taxonomy. (arXiv:1710.10686v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10686"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10686", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Regularization is one of the crucial ingredients of deep learning, yet the \nterm regularization has various definitions, and regularization methods are \noften studied separately from each other. In our work we present a systematic, \nunifying taxonomy to categorize existing methods. We distinguish methods that \naffect data, network architectures, error terms, regularization terms, and \noptimization procedures. We do not provide all details about the listed \nmethods; instead, we present an overview of how the methods can be sorted into \nmeaningful categories and sub-categories. This helps revealing links and \nfundamental similarities between them. Finally, we include practical \nrecommendations both for users and for developers of new regularization \nmethods. \n</p>"}, "author": "Jan Kuka&#x10d;ka, Vladimir Golkov, Daniel Cremers", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582639", "id": "tag:google.com,2005:reader/item/0000000329abbad3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Training Probabilistic Spiking Neural Networks with First-to-spike Decoding. (arXiv:1710.10704v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10704"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10704", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Third-generation neural networks, or Spiking Neural Networks (SNNs), aim at \nharnessing the energy efficiency of spike-domain processing by building on \ncomputing elements that operate on, and exchange, spikes. In this paper, the \nproblem of training a two-layer SNNs is studied for the purpose of \nclassification, under a Generalized Linear Model (GLM) probabilistic neural \nmodel that was previously considered within the computational neuroscience \nliterature. Conventional classification rules for SNNs operate offline based on \nthe number of output spikes at each output neuron. In contrast, a novel \ntraining method is proposed here for a first-to-spike decoding rule, whereby \nthe SNN can perform an early classification decision once spike firing is \ndetected at an output neuron. Numerical results bring insights into the optimal \nparameter selection for the GLM neuron and on the accuracy-complexity trade-off \nperformance of conventional and first-to-spike decoding. \n</p>"}, "author": "Alireza Bagheri, Osvaldo Simeone, Bipin Rajendran", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582638", "id": "tag:google.com,2005:reader/item/0000000329abbae6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Globally Optimal Symbolic Regression. (arXiv:1710.10720v2 [stat.ML] UPDATED)", "published": 1510139306, "updated": 1510139311, "canonical": [{"href": "http://arxiv.org/abs/1710.10720"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10720", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this study we introduce a new technique for symbolic regression that \nguarantees global optimality. This is achieved by formulating a mixed integer \nnon-linear program (MINLP) whose solution is a symbolic mathematical expression \nof minimum complexity that explains the observations. We demonstrate our \napproach by rediscovering Kepler's law on planetary motion using exoplanet data \nand Galileo's pendulum periodicity equation using experimental data. \n</p>"}, "author": "Vernon Austel, Sanjeeb Dash, Oktay Gunluk, Lior Horesh, Leo Liberti, Giacomo Nannicini, Baruch Schieber", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582637", "id": "tag:google.com,2005:reader/item/0000000329abbaeb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Contextual Regression: An Accurate and Conveniently Interpretable Nonlinear Model for Mining Discovery from Scientific Data. (arXiv:1710.10728v1 [q-bio.QM])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10728"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10728", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine learning algorithms such as linear regression, SVM and neural network \nhave played an increasingly important role in the process of scientific \ndiscovery. However, none of them is both interpretable and accurate on \nnonlinear datasets. Here we present contextual regression, a method that joins \nthese two desirable properties together using a hybrid architecture of neural \nnetwork embedding and dot product layer. We demonstrate its high prediction \naccuracy and sensitivity through the task of predictive feature selection on a \nsimulated dataset and the application of predicting open chromatin sites in the \nhuman genome. On the simulated data, our method achieved high fidelity recovery \nof feature contributions under random noise levels up to 200%. On the open \nchromatin dataset, the application of our method not only outperformed the \nstate of the art method in terms of accuracy, but also unveiled two previously \nunfound open chromatin related histone marks. Our method can fill the blank of \naccurate and interpretable nonlinear modeling in scientific data mining tasks. \n</p>"}, "author": "Chengyu Liu, Wei Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582636", "id": "tag:google.com,2005:reader/item/0000000329abbaf8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Attacking the Madry Defense Model with $L_1$-based Adversarial Examples. (arXiv:1710.10733v2 [stat.ML] UPDATED)", "published": 1509496837, "updated": 1509496839, "canonical": [{"href": "http://arxiv.org/abs/1710.10733"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10733", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c6a1c13\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c6a1c13&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The Madry Lab recently hosted a competition designed to test the robustness \nof their adversarially trained MNIST model. Attacks were constrained to perturb \neach pixel of the input image by a scaled maximal $L_\\infty$ distortion \n$\\epsilon$ = 0.3. This discourages the use of attacks which are not optimized \non the $L_\\infty$ distortion metric. Our experimental results demonstrate that \nby relaxing the $L_\\infty$ constraint of the competition, the elastic-net \nattack to deep neural networks (EAD) can generate transferable adversarial \nexamples which, despite their high average $L_\\infty$ distortion, have minimal \nvisual distortion. These results call into question the use of $L_\\infty$ as a \nsole measure for visual distortion, and further demonstrate the power of EAD at \ngenerating robust adversarial examples. \n</p>"}, "author": "Yash Sharma, Pin-Yu Chen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582635", "id": "tag:google.com,2005:reader/item/0000000329abbafe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Linearly convergent stochastic heavy ball method for minimizing generalization error. (arXiv:1710.10737v1 [math.OC])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10737"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10737", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c71582f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c71582f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this work we establish the first linear convergence result for the \nstochastic heavy ball method. The method performs SGD steps with a fixed \nstepsize, amended by a heavy ball momentum term. In the analysis, we focus on \nminimizing the expected loss and not on finite-sum minimization, which is \ntypically a much harder problem. While in the analysis we constrain ourselves \nto quadratic loss, the overall objective is not necessarily strongly convex. \n</p>"}, "author": "Nicolas Loizou, Peter Richt&#xe1;rik", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582634", "id": "tag:google.com,2005:reader/item/0000000329abbb03", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning neural trans-dimensional random field language models with noise-contrastive estimation. (arXiv:1710.10739v1 [cs.CL])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10739"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10739", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Trans-dimensional random field language models (TRF LMs) where sentences are \nmodeled as a collection of random fields, have shown close performance with \nLSTM LMs in speech recognition and are computationally more efficient in \ninference. However, the training efficiency of neural TRF LMs is not \nsatisfactory, which limits the scalability of TRF LMs on large training corpus. \nIn this paper, several techniques on both model formulation and parameter \nestimation are proposed to improve the training efficiency and the performance \nof neural TRF LMs. First, TRFs are reformulated in the form of exponential \ntilting of a reference distribution. Second, noise-contrastive estimation (NCE) \nis introduced to jointly estimate the model parameters and normalization \nconstants. Third, we extend the neural TRF LMs by marrying the deep \nconvolutional neural network (CNN) and the bidirectional LSTM into the \npotential function to extract the deep hierarchical features and \nbidirectionally sequential features. Utilizing all the above techniques enables \nthe successful and efficient training of neural TRF LMs on a 40x larger \ntraining set with only 1/3 training time and further reduces the WER with \nrelative reduction of 4.7% on top of a strong LSTM LM baseline. \n</p>"}, "author": "Bin Wang, Zhijian Ou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582633", "id": "tag:google.com,2005:reader/item/0000000329abbb1c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Implicit Causal Models for Genome-wide Association Studies. (arXiv:1710.10742v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10742"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10742", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Progress in probabilistic generative models has accelerated, developing \nricher models with neural architectures, implicit densities, and with scalable \nalgorithms for their Bayesian inference. However, there has been limited \nprogress in models that capture causal relationships, for example, how \nindividual genetic factors cause major human diseases. In this work, we focus \non two challenges in particular: How do we build richer causal models, which \ncan capture highly nonlinear relationships and interactions between multiple \ncauses? How do we adjust for latent confounders, which are variables \ninfluencing both cause and effect and which prevent learning of causal \nrelationships? To address these challenges, we synthesize ideas from causality \nand modern probabilistic modeling. For the first, we describe implicit causal \nmodels, a class of causal models that leverages neural architectures with an \nimplicit density. For the second, we describe an implicit causal model that \nadjusts for confounders by sharing strength across examples. In experiments, we \nscale Bayesian inference on up to a billion genetic measurements. We achieve \nstate of the art accuracy for identifying causal factors: we significantly \noutperform existing genetics methods by an absolute difference of 15-45.3%. \n</p>"}, "author": "Dustin Tran, David M. Blei", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582632", "id": "tag:google.com,2005:reader/item/0000000329abbb27", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Distance-based classifier by data transformation for high-dimension, strongly spiked eigenvalue models. (arXiv:1710.10768v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10768"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10768", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider classifiers for high-dimensional data under the strongly spiked \neigenvalue (SSE) model. We first show that high-dimensional data often have the \nSSE model. We consider a distance-based classifier using eigenstructures for \nthe SSE model. We apply the noise reduction methodology to estimation of the \neigenvalues and eigenvectors in the SSE model. We create a new distance-based \nclassifier by transforming data from the SSE model to the non-SSE model. We \ngive simulation studies and discuss the performance of the new classifier. \nFinally, we demonstrate the new classifier by using microarray data sets. \n</p>"}, "author": "Makoto Aoshima, Kazuyoshi Yata", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582631", "id": "tag:google.com,2005:reader/item/0000000329abbb2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Communication-Avoiding Optimization Methods for Massive-Scale Graphical Model Structure Learning. (arXiv:1710.10769v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10769"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10769", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Undirected graphical models compactly represent the structure of large, \nhigh-dimensional data sets, which are especially important in interpreting \ncomplex scientific data. Some data sets may run to multiple terabytes, and \ncurrent methods are intractable in both memory size and running time. We \nintroduce HP-CONCORD, a highly scalable optimization algorithm to estimate a \nsparse inverse covariance matrix based on a regularized pseudolikelihood \nframework. Our parallel proximal gradient method runs across a multi-node \ncluster and achieves parallel scalability using a novel communication-avoiding \nlinear algebra algorithm. We demonstrate scalability on problems with 1.28 \nmillion dimensions (over 800 billion parameters) and show that it can \noutperform a previous method on a single node and scales to 1K nodes (24K \ncores). We use HP-CONCORD to estimate the underlying conditional dependency \nstructure of the brain from fMRI data and use the result to automatically \nidentify functional regions. The results show good agreement with a \nstate-of-the-art clustering from the neuroscience literature. \n</p>"}, "author": "Penporn Koanantakool, Alnur Ali, Ariful Azad, Aydin Buluc, Dmitriy Morozov, Sang-Yun Oh, Leonid Oliker, Katherine Yelick", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582630", "id": "tag:google.com,2005:reader/item/0000000329abbb42", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Tensorizing Generative Adversarial Nets. (arXiv:1710.10772v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10772"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10772", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative Adversarial Network (GAN) and its variants demonstrate \nstate-of-the-art performance in the class of generative models. To capture \nhigher dimensional distributions, the common learning procedure requires high \ncomputational complexity and large number of parameters. In this paper, we \npresent a new generative adversarial framework by representing each layer as a \ntensor structure connected by multilinear operations, aiming to reduce the \nnumber of model parameters by a large factor while preserving the quality of \ngeneralized performance. To learn the model, we develop an efficient algorithm \nby alternating optimization of the mode connections. Experimental results \ndemonstrate that our model can achieve high compression rate for model \nparameters up to 40 times as compared to the existing GAN. \n</p>"}, "author": "Xingwei Cao, Qibin Zhao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582629", "id": "tag:google.com,2005:reader/item/0000000329abbb4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Transfer Learning to Learn with Multitask Neural Model Search. (arXiv:1710.10776v1 [cs.AI])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10776"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10776", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning models require extensive architecture design exploration and \nhyperparameter optimization to perform well on a given task. The exploration of \nthe model design space is often made by a human expert, and optimized using a \ncombination of grid search and search heuristics over a large space of possible \nchoices. Neural Architecture Search (NAS) is a Reinforcement Learning approach \nthat has been proposed to automate architecture design. NAS has been \nsuccessfully applied to generate Neural Networks that rival the best \nhuman-designed architectures. However, NAS requires sampling, constructing, and \ntraining hundreds to thousands of models to achieve well-performing \narchitectures. This procedure needs to be executed from scratch for each new \ntask. The application of NAS to a wide set of tasks currently lacks a way to \ntransfer generalizable knowledge across tasks. In this paper, we present the \nMultitask Neural Model Search (MNMS) controller. Our goal is to learn a \ngeneralizable framework that can condition model construction on successful \nmodel searches for previously seen tasks, thus significantly speeding up the \nsearch for new tasks. We demonstrate that MNMS can conduct an automated \narchitecture search for multiple tasks simultaneously while still learning \nwell-performing, specialized models for each task. We then show that \npre-trained MNMS controllers can transfer learning to new tasks. By leveraging \nknowledge from previous searches, we find that pre-trained MNMS models start \nfrom a better location in the search space and reduce search time on unseen \ntasks, while still discovering models that outperform published human-designed \nmodels. \n</p>"}, "author": "Catherine Wong, Andrea Gesmundo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582628", "id": "tag:google.com,2005:reader/item/0000000329abbb52", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Generative Adversarial Source Separation. (arXiv:1710.10779v1 [cs.SD])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10779"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10779", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative source separation methods such as non-negative matrix \nfactorization (NMF) or auto-encoders, rely on the assumption of an output \nprobability density. Generative Adversarial Networks (GANs) can learn data \ndistributions without needing a parametric assumption on the output density. We \nshow on a speech source separation experiment that, a multi-layer perceptron \ntrained with a Wasserstein-GAN formulation outperforms NMF, auto-encoders \ntrained with maximum likelihood, and variational auto-encoders in terms of \nsource to distortion ratio. \n</p>"}, "author": "Cem Subakan, Paris Smaragdis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582627", "id": "tag:google.com,2005:reader/item/0000000329abbb58", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Stochastic variance reduced multiplicative update for nonnegative matrix factorization. (arXiv:1710.10781v1 [cs.NA])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10781"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10781", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Nonnegative matrix factorization (NMF), a dimensionality reduction and factor \nanalysis method, is a special case in which factor matrices have low-rank \nnonnegative constraints. Considering the stochastic learning in NMF, we \nspecifically address the multiplicative update (MU) rule, which is the most \npopular, but which has slow convergence property. This present paper introduces \non the stochastic MU rule a variance-reduced technique of stochastic gradient. \nNumerical comparisons suggest that our proposed algorithms robustly outperform \nstate-of-the-art algorithms across different synthetic and real-world datasets. \n</p>"}, "author": "Hiroyuki Kasai", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582626", "id": "tag:google.com,2005:reader/item/0000000329abbb5e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "How deep learning works --The geometry of deep learning. (arXiv:1710.10784v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10784"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10784", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c715c2f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c715c2f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Why and how that deep learning works well on different tasks remains a \nmystery from a theoretical perspective. In this paper we draw a geometric \npicture of the deep learning system by finding its analogies with two existing \ngeometric structures, the geometry of quantum computations and the geometry of \nthe diffeomorphic template matching. In this framework, we give the geometric \nstructures of different deep learning systems including convolutional neural \nnetworks, residual networks, recursive neural networks, recurrent neural \nnetworks and the equilibrium prapagation framework. We can also analysis the \nrelationship between the geometrical structures and their performance of \ndifferent networks in an algorithmic level so that the geometric framework may \nguide the design of the structures and algorithms of deep learning systems. \n</p>"}, "author": "Xiao Dong, Jiasong Wu, Ling Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582625", "id": "tag:google.com,2005:reader/item/0000000329abbb62", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Understanding GANs: the LQG Setting. (arXiv:1710.10793v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10793"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10793", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative Adversarial Networks (GANs) have become a popular method to learn \na probability model from data. Many GAN architectures with different \noptimization metrics have been introduced recently. Instead of proposing yet \nanother architecture, this paper aims to provide an understanding of some of \nthe basic issues surrounding GANs. First, we propose a natural way of \nspecifying the loss function for GANs by drawing a connection with supervised \nlearning. Second, we shed light on the generalization peformance of GANs \nthrough the analysis of a simple LQG setting: the generator is Linear, the loss \nfunction is Quadratic and the data is drawn from a Gaussian distribution. We \nshow that in this setting: 1) the optimal GAN solution converges to population \nPrincipal Component Analysis (PCA) as the number of training samples increases; \n2) the number of samples required scales exponentially with the dimension of \nthe data; 3) the number of samples scales almost linearly if the discriminator \nis constrained to be quadratic. Thus, linear generators and quadratic \ndiscriminators provide a good balance for fast learning. \n</p>"}, "author": "Soheil Feizi, Changho Suh, Fei Xia, David Tse", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582624", "id": "tag:google.com,2005:reader/item/0000000329abbb68", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Hit Song Prediction for Pop Music by Siamese CNN with Ranking Loss. (arXiv:1710.10814v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10814"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10814", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A model for hit song prediction can be used in the pop music industry to \nidentify emerging trends and potential artists or songs before they are \nmarketed to the public. While most previous work formulates hit song prediction \nas a regression or classification problem, we present in this paper a \nconvolutional neural network (CNN) model that treats it as a ranking problem. \nSpecifically, we use a commercial dataset with daily play-counts to train a \nmulti-objective Siamese CNN model with Euclidean loss and pairwise ranking loss \nto learn from audio the relative ranking relations among songs. Besides, we \ndevise a number of pair sampling methods according to some empirical \nobservation of the data. Our experiment shows that the proposed model with a \nsampling method called A/B sampling leads to much higher accuracy in hit song \nprediction than the baseline regression model. Moreover, we can further improve \nthe accuracy by using a neural attention mechanism to extract the highlights of \nsongs and by using a separate CNN model to offer high-level features of songs. \n</p>"}, "author": "Lang-Chi Yu, Yi-Hsuan Yang, Yun-Ning Hung, Yi-An Chen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582623", "id": "tag:google.com,2005:reader/item/0000000329abbb6c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Rough extreme learning machine: a new classification method based on uncertainty measure. (arXiv:1710.10824v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10824"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10824", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Extreme learning machine (ELM) is a new single hidden layer feedback neural \nnetwork. The weights of the input layer and the biases of neurons in hidden \nlayer are randomly generated, the weights of the output layer can be \nanalytically determined. ELM has been achieved good results for a large number \nof classification tasks. In this paper, a new extreme learning machine called \nrough extreme learning machine (RELM) was proposed. RELM uses rough set to \ndivide data into upper approximation set and lower approximation set, and the \ntwo approximation sets are utilized to train upper approximation neurons and \nlower approximation neurons. In addition, an attribute reduction is executed in \nthis algorithm to remove redundant attributes. The experimental results showed, \ncomparing with the comparison algorithms, RELM can get a better accuracy and \nrepeatability in most cases, RELM can not only maintain the advantages of fast \nspeed, but also effectively cope with the classification task for \nhigh-dimensional data. \n</p>"}, "author": "Shuliang Xu, Lin Feng, Feilong Wang, Shenglan Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582622", "id": "tag:google.com,2005:reader/item/0000000329abbb75", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Unifying Value Iteration, Advantage Learning, and Dynamic Policy Programming. (arXiv:1710.10866v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10866"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10866", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Approximate dynamic programming algorithms, such as approximate value \niteration, have been successfully applied to many complex reinforcement \nlearning tasks, and a better approximate dynamic programming algorithm is \nexpected to further extend the applicability of reinforcement learning to \nvarious tasks. In this paper we propose a new, robust dynamic programming \nalgorithm that unifies value iteration, advantage learning, and dynamic policy \nprogramming. We call it generalized value iteration (GVI) and its approximated \nversion, approximate GVI (AGVI). We show AGVI's performance guarantee, which \nincludes performance guarantees for existing algorithms, as special cases. We \ndiscuss theoretical weaknesses of existing algorithms, and explain the \nadvantages of AGVI. Numerical experiments in a simple environment support \ntheoretical arguments, and suggest that AGVI is a promising alternative to \nprevious algorithms. \n</p>"}, "author": "Tadashi Kozuno, Eiji Uchibe, Kenji Doya", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582621", "id": "tag:google.com,2005:reader/item/0000000329abbb81", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Fast Linear Model for Knowledge Graph Embeddings. (arXiv:1710.10881v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10881"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10881", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper shows that a simple baseline based on a Bag-of-Words (BoW) \nrepresentation learns surprisingly good knowledge graph embeddings. By casting \nknowledge base completion and question answering as supervised classification \nproblems, we observe that modeling co-occurences of entities and relations \nleads to state-of-the-art performance with a training time of a few minutes \nusing the open sourced library fastText. \n</p>"}, "author": "Armand Joulin, Edouard Grave, Piotr Bojanowski, Maximilian Nickel, Tomas Mikolov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582620", "id": "tag:google.com,2005:reader/item/0000000329abbb9f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Graph Attention Networks. (arXiv:1710.10903v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10903"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10903", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present graph attention networks (GATs), novel neural network \narchitectures that operate on graph-structured data, leveraging masked \nself-attentional layers to address the shortcomings of prior methods based on \ngraph convolutions or their approximations. By stacking layers in which nodes \nare able to attend over their neighborhoods' features, we enable (implicitly) \nspecifying different weights to different nodes in a neighborhood, without \nrequiring any kind of costly matrix operation (such as inversion) or depending \non knowing the graph structure upfront. In this way, we address several key \nchallenges of spectral-based graph neural networks simultaneously, and make our \nmodel readily applicable to inductive as well as transductive problems. Our GAT \nmodels have achieved state-of-the-art results across three established \ntransductive and inductive graph benchmarks: the Cora and Citeseer citation \nnetwork datasets, as well as a protein-protein interaction dataset (wherein \ntest graphs are entirely unseen during training). \n</p>"}, "author": "Petar Veli&#x10d;kovi&#x107;, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li&#xf2;, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582619", "id": "tag:google.com,2005:reader/item/0000000329abbbb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks. (arXiv:1710.10916v1 [cs.CV])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10916"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10916", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Although Generative Adversarial Networks (GANs) have shown remarkable success \nin various tasks, they still face challenges in generating high quality images. \nIn this paper, we propose Stacked Generative Adversarial Networks (StackGAN) \naimed at generating high-resolution photorealistic images. First, we propose a \ntwo-stage generative adversarial network architecture, StackGAN-v1, for \ntext-to-image synthesis. The Stage-I GAN sketches primitive shape and colors of \nthe object based on given text description, yielding low-resolution images. The \nStage-II GAN takes Stage-I results and text descriptions as inputs, and \ngenerates high-resolution images with photo-realistic details. Second, an \nadvanced multi-stage generative adversarial network architecture, StackGAN-v2, \nis proposed for both conditional and unconditional generative tasks. Our \nStackGAN-v2 consists of multiple generators and discriminators in a tree-like \nstructure; images at multiple scales corresponding to the same scene are \ngenerated from different branches of the tree. StackGAN-v2 shows more stable \ntraining behavior than StackGAN-v1 by jointly approximating multiple \ndistributions. Extensive experiments demonstrate that the proposed stacked \ngenerative adversarial networks significantly outperform other state-of-the-art \nmethods in generating photo-realistic images. \n</p>"}, "author": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582618", "id": "tag:google.com,2005:reader/item/0000000329abbbca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimal Kernel-Based Dynamic Mode Decomposition. (arXiv:1710.10919v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10919"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10919", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The state-of-the-art algorithm known as kernel-based dynamic mode \ndecomposition (K-DMD) provides a sub-optimal solution to the problem of reduced \nmodeling of a dynamical system based on a finite approximation of the Koopman \noperator. It relies on crude approximations and on restrictive assumptions. The \npurpose of this work is to propose a kernel-based algorithm solving exactly \nthis low-rank approximation problem in a general setting. \n</p>"}, "author": "Patrick H&#xe9;as, C&#xe9;dric Herzet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582617", "id": "tag:google.com,2005:reader/item/0000000329abbbda", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The loss surface and expressivity of deep convolutional neural networks. (arXiv:1710.10928v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10928"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10928", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We analyze the expressiveness and loss surface of practical deep \nconvolutional neural networks (CNNs) with shared weights and max pooling \nlayers. We show that such CNNs produce linearly independent features at a \n\"wide\" layer which has more neurons than the number of training samples. This \ncondition holds e.g. for the VGG network. Furthermore, we provide for such wide \nCNNs necessary and sufficient conditions for global minima with zero training \nerror. For the case where the wide layer is followed by a fully connected \nlayer, we show that almost every critical point of the empirical loss is a \nglobal minimum with zero training error. Our analysis suggests that both depth \nand width are very important in deep learning. While depth brings more \nrepresentational power and allows the network to learn high level features, \nwidth smoothes the optimization landscape of the loss function in the sense \nthat a sufficiently wide network has a well-behaved loss surface with \npotentially no bad local minima. \n</p>"}, "author": "Quynh Nguyen, Matthias Hein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582616", "id": "tag:google.com,2005:reader/item/0000000329abbbf2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SGDLibrary: A MATLAB library for stochastic gradient descent algorithms. (arXiv:1710.10951v1 [cs.MS])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10951"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10951", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c715fe7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c715fe7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We consider the problem of finding the minimizer of a function $f: \n\\mathbb{R}^d \\rightarrow \\mathbb{R}$ of the form $\\min f(w) = \n\\frac{1}{n}\\sum_{i}f_i({w})$. This problem has been studied intensively in \nrecent years in machine learning research field. One typical but promising \napproach for large-scale data is stochastic optimization algorithm. SGDLibrary \nis a flexible, extensible and efficient pure-Matlab library of a collection of \nstochastic optimization algorithms. The purpose of the library is to provide \nresearchers and implementers a comprehensive evaluation environment of those \nalgorithms on various machine learning problems. \n</p>"}, "author": "Hiroyuki Kasai", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582615", "id": "tag:google.com,2005:reader/item/0000000329abbc01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Denoising random forests. (arXiv:1710.11004v1 [cs.CV])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11004"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11004", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c783403\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c783403&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper proposes a novel type of random forests called a denoising random \nforests that are robust against noises contained in test samples. Such \nnoise-corrupted samples cause serious damage to the estimation performances of \nrandom forests, since unexpected child nodes are often selected and the leaf \nnodes that the input sample reaches are sometimes far from those for a clean \nsample. Our main idea for tackling this problem originates from a binary \nindicator vector that encodes a traversal path of a sample in the forest. Our \nproposed method effectively employs this vector by introducing denoising \nautoencoders into random forests. A denoising autoencoder can be trained with \nindicator vectors produced from clean and noisy input samples, and non-leaf \nnodes where incorrect decisions are made can be identified by comparing the \ninput and output of the trained denoising autoencoder. Multiple traversal paths \nwith respect to the nodes with incorrect decisions caused by the noises can \nthen be considered for the estimation. \n</p>"}, "author": "Masaya Hibino, Akisato Kimura, Takayoshi Yamashita, Yuji Yamauchi, Hironobu Fujiyoshi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582614", "id": "tag:google.com,2005:reader/item/0000000329abbc0f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. (arXiv:1710.11029v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11029"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11029", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Stochastic gradient descent (SGD) is widely believed to perform implicit \nregularization when used to train deep neural networks, but the precise manner \nin which this occurs has thus far been elusive. We prove that SGD minimizes an \naverage potential over the posterior distribution of weights along with an \nentropic regularization term. This potential is however not the original loss \nfunction in general. So SGD does perform variational inference, but for a \ndifferent loss than the one used to compute the gradients. Even more \nsurprisingly, SGD does not even converge in the classical sense: we show that \nthe most likely trajectories of SGD for deep networks do not behave like \nBrownian motion around critical points. Instead, they resemble closed loops \nwith deterministic components. We prove that such \"out-of-equilibrium\" behavior \nis a consequence of the fact that the gradient noise in SGD is highly \nnon-isotropic; the covariance matrix of mini-batch gradients has a rank as \nsmall as 1% of its dimension. We provide extensive empirical validation of \nthese claims, proven in the appendix. \n</p>"}, "author": "Pratik Chaudhari, Stefano Soatto", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582613", "id": "tag:google.com,2005:reader/item/0000000329abbc1e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Connection between Feed-Forward Neural Networks and Probabilistic Graphical Models. (arXiv:1710.11052v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11052"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11052", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Two of the most popular modelling paradigms in computer vision are \nfeed-forward neural networks (FFNs) and probabilistic graphical models (GMs). \nVarious connections between the two have been studied in recent works, such as \ne.g. expressing mean-field based inference in a GM as an FFN. This paper \nestablishes a new connection between FFNs and GMs. Our key observation is that \nany FFN implements a certain approximation of a corresponding Bayesian network \n(BN). We characterize various benefits of having this connection. In \nparticular, it results in a new learning algorithm for BNs. We validate the \nproposed methods for a classification problem on CIFAR-10 dataset and for \nbinary image segmentation on Weizmann Horse dataset. We show that statistically \nlearned BNs improve performance, having at the same time essentially better \ngeneralization capability, than their FFN counterparts. \n</p>"}, "author": "Dmitrij Schlesinger", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582612", "id": "tag:google.com,2005:reader/item/0000000329abbc2a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convergence Rates of Latent Topic Models Under Relaxed Identifiability Conditions. (arXiv:1710.11070v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11070"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11070", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we study the frequentist convergence rate for the Latent \nDirichlet Allocation (Blei et al., 2003) topic models. We show that the maximum \nlikelihood estimator converges to one of the finitely many equivalent \nparameters in Wasserstein's distance metric at a rate of $n^{-1/4}$ without \nassuming separability or non-degeneracy of the underlying topics and/or the \nexistence of more than three words per document, thus generalizing the previous \nworks of Anandkumar et al. (2012, 2014) from an information-theoretical \nperspective. We also show that the $n^{-1/4}$ convergence rate is optimal in \nthe worst case. \n</p>"}, "author": "Yining Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852421", "id": "tag:google.com,2005:reader/item/00000003293f86e6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Similarity-based Multi-label Learning. (arXiv:1710.10335v1 [stat.ML])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10335"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10335", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Multi-label classification is an important learning problem with many \napplications. In this work, we propose a principled similarity-based approach \nfor multi-label learning called SML. We also introduce a similarity-based \napproach for predicting the label set size. The experimental results \ndemonstrate the effectiveness of SML for multi-label classification where it is \nshown to compare favorably with a wide variety of existing algorithms across a \nrange of evaluation criterion. \n</p>"}, "author": "Ryan A. Rossi, Nesreen K. Ahmed, Hoda Eldardiry, Rong Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852420", "id": "tag:google.com,2005:reader/item/00000003293f86ef", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Partitioning Relational Matrices of Similarities or Dissimilarities using the Value of Information. (arXiv:1710.10381v1 [cs.AI])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10381"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10381", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we provide an approach to clustering relational matrices whose \nentries correspond to either similarities or dissimilarities between objects. \nOur approach is based on the value of information, a parameterized, \ninformation-theoretic criterion that measures the change in costs associated \nwith changes in information. Optimizing the value of information yields a \ndeterministic annealing style of clustering with many benefits. For instance, \ninvestigators avoid needing to a priori specify the number of clusters, as the \npartitions naturally undergo phase changes, during the annealing process, \nwhereby the number of clusters changes in a data-driven fashion. The \nglobal-best partition can also often be identified. \n</p>"}, "author": "Isaac J. Sledge, Jose C. Principe", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852419", "id": "tag:google.com,2005:reader/item/00000003293f86f6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Left-Right Skip-DenseNets for Coarse-to-Fine Object Categorization. (arXiv:1710.10386v1 [cs.CV])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10386"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10386", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Inspired by the recent neuroscience studies on the left-right asymmetry of \nthe brains in the low and high spatial frequency processing, we introduce a \nnovel type of network -- the left-right skip-densenets for coarse-to-fine \nobject categorization. This network can enable both coarse and fine-grained \nclassification in a single framework. We also for the first time propose the \nlayer-skipping mechanism which learns a gating network to predict whether skip \nsome layers in the testing stage. This layer-skipping mechanism assigns more \nflexibility and capability to our network for the categorization tasks. Our \nnetwork is evaluated on three widely used datasets; the results show that our \nnetwork is more promising in solving the coarse-to-fine object categorization \nthan the competitors. \n</p>"}, "author": "Changmao Cheng, Yanwei Fu, Wenlian Lu, Yu-Gang Jiang, Jianfeng Feng, Xiangyang Xue", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852418", "id": "tag:google.com,2005:reader/item/00000003293f86fb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Ontology to support automated negotiation. (arXiv:1710.10433v1 [cs.AI])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10433"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10433", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work we propose an ontology to support automated negotiation in \nmultiagent systems. The ontology can be connected with some domain-specific \nontologies to facilitate the negotiation in different domains, such as \nIntelligent Transportation Systems (ITS), e-commerce, etc. The specific \nnegotiation rules for each type of negotiation strategy can also be defined as \npart of the ontology, reducing the amount of knowledge hardcoded in the agents \nand ensuring the interoperability. The expressiveness of the ontology was \nproved in a multiagent architecture for the automatic traffic light setting \napplication on ITS. \n</p>"}, "author": "Susel Fernandez, Takayuki Ito", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852417", "id": "tag:google.com,2005:reader/item/00000003293f8707", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Long-Distance Loop Closure Using General Object Landmarks. (arXiv:1710.10466v1 [cs.RO])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10466"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10466", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Visual localization under large changes in scale is an important capability \nin many robotic mapping applications, such as localizing at low altitudes in \nmaps built at high altitudes, or performing loop closure over long distances. \nExisting approaches, however, are robust only up to a 3x difference in scale \nbetween map and query images. We propose a novel combination of \ndeep-learning-based object features and hand-engineered point-features that \nyields improved robustness to scale change, perspective change, and image \nnoise. We conduct experiments in simulation and in real-world outdoor scenes \nexhibiting up to a 7x change in scale, and compare our approach against \nlocalization using state-of-the-art SIFT features. This technique is \ntraining-free and class-agnostic, and in principle can be deployed in any \nenvironment out-of-the-box. \n</p>"}, "author": "Andrew Holliday, Gregory Dudek", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852416", "id": "tag:google.com,2005:reader/item/00000003293f870d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Exploiting Points and Lines in Regression Forests for RGB-D Camera Relocalization. (arXiv:1710.10519v1 [cs.CV])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10519"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10519", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c783802\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c783802&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Camera relocalization plays a vital role in many robotics and computer vision \ntasks, such as global localization, recovery from tracking failure and loop \nclosure detection. Recent random forests based methods exploit randomly sampled \npixel comparison features to predict 3D world locations for 2D image locations \nto guide the camera pose optimization. However, these image features are only \nsampled randomly in the images, without considering the spatial structures or \ngeometric information, leading to large errors or failure cases with the \nexistence of poorly textured areas or in motion blur. Line segment features are \nmore robust in these environments. In this work, we propose to jointly exploit \npoints and lines within the framework of uncertainty driven regression forests. \nThe proposed approach is thoroughly evaluated on three publicly available \ndatasets against several strong state-of-the-art baselines in terms of several \ndifferent error metrics. Experimental results prove the efficacy of our method, \nshowing superior or on-par state-of-the-art performance. \n</p>"}, "author": "Lili Meng, Frederick Tung, James J. Little, Julien Valentin, Clarence W. de Silva", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852415", "id": "tag:google.com,2005:reader/item/00000003293f8716", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Interpretable Apprenticship Learning with Temporal Logic Specifications. (arXiv:1710.10532v1 [cs.SY])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10532"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10532", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent work has addressed using formulas in linear temporal logic (LTL) as \nspecifications for agents planning in Markov Decision Processes (MDPs). We \nconsider the inverse problem: inferring an LTL specification from demonstrated \nbehavior trajectories in MDPs. We formulate this as a multiobjective \noptimization problem, and describe state-based (\"what actually happened\") and \naction-based (\"what the agent expected to happen\") objective functions based on \na notion of \"violation cost\". We demonstrate the efficacy of the approach by \nemploying genetic programming to solve this problem in two simple domains. \n</p>"}, "author": "Daniel Kasenberg, Matthias Scheutz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852414", "id": "tag:google.com,2005:reader/item/00000003293f8726", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Partial Knowledge In Embeddings. (arXiv:1710.10538v1 [cs.AI])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10538"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10538", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Representing domain knowledge is crucial for any task. There has been a wide \nrange of techniques developed to represent this knowledge, from older logic \nbased approaches to the more recent deep learning based techniques (i.e. \nembeddings). In this paper, we discuss some of these methods, focusing on the \nrepresentational expressiveness tradeoffs that are often made. In particular, \nwe focus on the the ability of various techniques to encode `partial knowledge' \n- a key component of successful knowledge systems. We introduce and describe \nthe concepts of `ensembles of embeddings' and `aggregate embeddings' and \ndemonstrate how they allow for partial knowledge. \n</p>"}, "author": "Ramanathan V. Guha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852413", "id": "tag:google.com,2005:reader/item/00000003293f8730", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Regularization approaches for support vector machines with applications to biomedical data. (arXiv:1710.10600v1 [cs.LG])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10600"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10600", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The support vector machine (SVM) is a widely used machine learning tool for \nclassification based on statistical learning theory. Given a set of training \ndata, the SVM finds a hyperplane that separates two different classes of data \npoints by the largest distance. While the standard form of SVM uses L2-norm \nregularization, other regularization approaches are particularly attractive for \nbiomedical datasets where, for example, sparsity and interpretability of the \nclassifier's coefficient values are highly desired features. Therefore, in this \npaper we consider different types of regularization approaches for SVMs, and \nexplore them in both synthetic and real biomedical datasets. \n</p>"}, "author": "Daniel Lopez-Martinez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852412", "id": "tag:google.com,2005:reader/item/00000003293f8735", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Discovery Radiomics with CLEAR-DR: Interpretable Computer Aided Diagnosis of Diabetic Retinopathy. (arXiv:1710.10675v1 [cs.AI])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10675"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10675", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Objective: Radiomics-driven Computer Aided Diagnosis (CAD) has shown \nconsiderable promise in recent years as a potential tool for improving clinical \ndecision support in medical oncology, particularly those based around the \nconcept of Discovery Radiomics, where radiomic sequencers are discovered \nthrough the analysis of medical imaging data. One of the main limitations with \ncurrent CAD approaches is that it is very difficult to gain insight or \nrationale as to how decisions are made, thus limiting their utility to \nclinicians. Methods: In this study, we propose CLEAR-DR, a novel interpretable \nCAD system based on the notion of CLass-Enhanced Attentive Response Discovery \nRadiomics for the purpose of clinical decision support for diabetic \nretinopathy. Results: In addition to disease grading via the discovered deep \nradiomic sequencer, the CLEAR-DR system also produces a visual interpretation \nof the decision-making process to provide better insight and understanding into \nthe decision-making process of the system. Conclusion: We demonstrate the \neffectiveness and utility of the proposed CLEAR-DR system of enhancing the \ninterpretability of diagnostic grading results for the application of diabetic \nretinopathy grading. Significance: CLEAR-DR can act as a potential powerful \ntool to address the uninterpretability issue of current CAD systems, thus \nimproving their utility to clinicians. \n</p>"}, "author": "Devinder Kumar, Graham W. Taylor, Alexander Wong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852411", "id": "tag:google.com,2005:reader/item/00000003293f8745", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Regularization for Deep Learning: A Taxonomy. (arXiv:1710.10686v1 [cs.LG])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10686"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10686", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Regularization is one of the crucial ingredients of deep learning, yet the \nterm regularization has various definitions, and regularization methods are \noften studied separately from each other. In our work we present a systematic, \nunifying taxonomy to categorize existing methods. We distinguish methods that \naffect data, network architectures, error terms, regularization terms, and \noptimization procedures. We do not provide all details about the listed \nmethods; instead, we present an overview of how the methods can be sorted into \nmeaningful categories and sub-categories. This helps revealing links and \nfundamental similarities between them. Finally, we include practical \nrecommendations both for users and for developers of new regularization \nmethods. \n</p>"}, "author": "Jan Kuka&#x10d;ka, Vladimir Golkov, Daniel Cremers", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852410", "id": "tag:google.com,2005:reader/item/00000003293f874d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Training Probabilistic Spiking Neural Networks with First-to-spike Decoding. (arXiv:1710.10704v1 [stat.ML])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10704"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10704", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Third-generation neural networks, or Spiking Neural Networks (SNNs), aim at \nharnessing the energy efficiency of spike-domain processing by building on \ncomputing elements that operate on, and exchange, spikes. In this paper, the \nproblem of training a two-layer SNNs is studied for the purpose of \nclassification, under a Generalized Linear Model (GLM) probabilistic neural \nmodel that was previously considered within the computational neuroscience \nliterature. Conventional classification rules for SNNs operate offline based on \nthe number of output spikes at each output neuron. In contrast, a novel \ntraining method is proposed here for a first-to-spike decoding rule, whereby \nthe SNN can perform an early classification decision once spike firing is \ndetected at an output neuron. Numerical results bring insights into the optimal \nparameter selection for the GLM neuron and on the accuracy-complexity trade-off \nperformance of conventional and first-to-spike decoding. \n</p>"}, "author": "Alireza Bagheri, Osvaldo Simeone, Bipin Rajendran", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852409", "id": "tag:google.com,2005:reader/item/00000003293f8755", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Computational Social Choice and Computational Complexity: BFFs?. (arXiv:1710.10753v1 [cs.MA])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10753"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10753", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We discuss the connection between computational social choice (comsoc) and \ncomputational complexity. We stress the work so far on, and urge continued \nfocus on, two less-recognized aspects of this connection. Firstly, this is very \nmuch a two-way street: Everyone knows complexity classification is used in \ncomsoc, but we also highlight benefits to complexity that have arisen from its \nuse in comsoc. Secondly, more subtle, less-known complexity tools often can be \nvery productively used in comsoc. \n</p>"}, "author": "Lane A. Hemaspaandra", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852408", "id": "tag:google.com,2005:reader/item/00000003293f8759", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Tensorizing Generative Adversarial Nets. (arXiv:1710.10772v1 [cs.AI])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10772"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10772", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative Adversarial Network (GAN) and its variants demonstrate \nstate-of-the-art performance in the class of generative models. To capture \nhigher dimensional distributions, the common learning procedure requires high \ncomputational complexity and large number of parameters. In this paper, we \npresent a new generative adversarial framework by representing each layer as a \ntensor structure connected by multilinear operations, aiming to reduce the \nnumber of model parameters by a large factor while preserving the quality of \ngeneralized performance. To learn the model, we develop an efficient algorithm \nby alternating optimization of the mode connections. Experimental results \ndemonstrate that our model can achieve high compression rate for model \nparameters up to 40 times as compared to the existing GAN. \n</p>"}, "author": "Xingwei Cao, Qibin Zhao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852407", "id": "tag:google.com,2005:reader/item/00000003293f875c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Transfer Learning to Learn with Multitask Neural Model Search. (arXiv:1710.10776v1 [cs.AI])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10776"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10776", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning models require extensive architecture design exploration and \nhyperparameter optimization to perform well on a given task. The exploration of \nthe model design space is often made by a human expert, and optimized using a \ncombination of grid search and search heuristics over a large space of possible \nchoices. Neural Architecture Search (NAS) is a Reinforcement Learning approach \nthat has been proposed to automate architecture design. NAS has been \nsuccessfully applied to generate Neural Networks that rival the best \nhuman-designed architectures. However, NAS requires sampling, constructing, and \ntraining hundreds to thousands of models to achieve well-performing \narchitectures. This procedure needs to be executed from scratch for each new \ntask. The application of NAS to a wide set of tasks currently lacks a way to \ntransfer generalizable knowledge across tasks. In this paper, we present the \nMultitask Neural Model Search (MNMS) controller. Our goal is to learn a \ngeneralizable framework that can condition model construction on successful \nmodel searches for previously seen tasks, thus significantly speeding up the \nsearch for new tasks. We demonstrate that MNMS can conduct an automated \narchitecture search for multiple tasks simultaneously while still learning \nwell-performing, specialized models for each task. We then show that \npre-trained MNMS controllers can transfer learning to new tasks. By leveraging \nknowledge from previous searches, we find that pre-trained MNMS models start \nfrom a better location in the search space and reduce search time on unseen \ntasks, while still discovering models that outperform published human-designed \nmodels. \n</p>"}, "author": "Catherine Wong, Andrea Gesmundo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852406", "id": "tag:google.com,2005:reader/item/00000003293f875f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Understanding Hidden Memories of Recurrent Neural Networks. (arXiv:1710.10777v1 [cs.CL])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10777"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10777", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c783c05\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c783c05&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Recurrent neural networks (RNNs) have been successfully applied to various \nnatural language processing (NLP) tasks and achieved better results than \nconventional methods. However, the lack of understanding of the mechanisms \nbehind their effectiveness limits further improvements on their architectures. \nIn this paper, we present a visual analytics method for understanding and \ncomparing RNN models for NLP tasks. We propose a technique to explain the \nfunction of individual hidden state units based on their expected response to \ninput texts. We then co-cluster hidden state units and words based on the \nexpected response and visualize co-clustering results as memory chips and word \nclouds to provide more structured knowledge on RNNs' hidden states. We also \npropose a glyph-based sequence visualization based on aggregate information to \nanalyze the behavior of an RNN's hidden state at the sentence-level. The \nusability and effectiveness of our method are demonstrated through case studies \nand reviews from domain experts. \n</p>"}, "author": "Yao Ming, Shaozu Cao, Ruixiang Zhang, Zhen Li, Yuanzhe Chen, Yangqiu Song, Huamin Qu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852405", "id": "tag:google.com,2005:reader/item/00000003293f8768", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Rough extreme learning machine: a new classification method based on uncertainty measure. (arXiv:1710.10824v1 [cs.LG])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10824"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10824", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c7eb670\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c7eb670&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Extreme learning machine (ELM) is a new single hidden layer feedback neural \nnetwork. The weights of the input layer and the biases of neurons in hidden \nlayer are randomly generated, the weights of the output layer can be \nanalytically determined. ELM has been achieved good results for a large number \nof classification tasks. In this paper, a new extreme learning machine called \nrough extreme learning machine (RELM) was proposed. RELM uses rough set to \ndivide data into upper approximation set and lower approximation set, and the \ntwo approximation sets are utilized to train upper approximation neurons and \nlower approximation neurons. In addition, an attribute reduction is executed in \nthis algorithm to remove redundant attributes. The experimental results showed, \ncomparing with the comparison algorithms, RELM can get a better accuracy and \nrepeatability in most cases, RELM can not only maintain the advantages of fast \nspeed, but also effectively cope with the classification task for \nhigh-dimensional data. \n</p>"}, "author": "Shuliang Xu, Lin Feng, Feilong Wang, Shenglan Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852404", "id": "tag:google.com,2005:reader/item/00000003293f8771", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Graph Attention Networks. (arXiv:1710.10903v1 [stat.ML])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10903"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10903", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present graph attention networks (GATs), novel neural network \narchitectures that operate on graph-structured data, leveraging masked \nself-attentional layers to address the shortcomings of prior methods based on \ngraph convolutions or their approximations. By stacking layers in which nodes \nare able to attend over their neighborhoods' features, we enable (implicitly) \nspecifying different weights to different nodes in a neighborhood, without \nrequiring any kind of costly matrix operation (such as inversion) or depending \non knowing the graph structure upfront. In this way, we address several key \nchallenges of spectral-based graph neural networks simultaneously, and make our \nmodel readily applicable to inductive as well as transductive problems. Our GAT \nmodels have achieved state-of-the-art results across three established \ntransductive and inductive graph benchmarks: the Cora and Citeseer citation \nnetwork datasets, as well as a protein-protein interaction dataset (wherein \ntest graphs are entirely unseen during training). \n</p>"}, "author": "Petar Veli&#x10d;kovi&#x107;, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li&#xf2;, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852403", "id": "tag:google.com,2005:reader/item/00000003293f8779", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks. (arXiv:1710.10916v1 [cs.CV])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10916"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10916", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Although Generative Adversarial Networks (GANs) have shown remarkable success \nin various tasks, they still face challenges in generating high quality images. \nIn this paper, we propose Stacked Generative Adversarial Networks (StackGAN) \naimed at generating high-resolution photorealistic images. First, we propose a \ntwo-stage generative adversarial network architecture, StackGAN-v1, for \ntext-to-image synthesis. The Stage-I GAN sketches primitive shape and colors of \nthe object based on given text description, yielding low-resolution images. The \nStage-II GAN takes Stage-I results and text descriptions as inputs, and \ngenerates high-resolution images with photo-realistic details. Second, an \nadvanced multi-stage generative adversarial network architecture, StackGAN-v2, \nis proposed for both conditional and unconditional generative tasks. Our \nStackGAN-v2 consists of multiple generators and discriminators in a tree-like \nstructure; images at multiple scales corresponding to the same scene are \ngenerated from different branches of the tree. StackGAN-v2 shows more stable \ntraining behavior than StackGAN-v1 by jointly approximating multiple \ndistributions. Extensive experiments demonstrate that the proposed stacked \ngenerative adversarial networks significantly outperform other state-of-the-art \nmethods in generating photo-realistic images. \n</p>"}, "author": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852402", "id": "tag:google.com,2005:reader/item/00000003293f878a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The loss surface and expressivity of deep convolutional neural networks. (arXiv:1710.10928v1 [cs.LG])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10928"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10928", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We analyze the expressiveness and loss surface of practical deep \nconvolutional neural networks (CNNs) with shared weights and max pooling \nlayers. We show that such CNNs produce linearly independent features at a \n\"wide\" layer which has more neurons than the number of training samples. This \ncondition holds e.g. for the VGG network. Furthermore, we provide for such wide \nCNNs necessary and sufficient conditions for global minima with zero training \nerror. For the case where the wide layer is followed by a fully connected \nlayer, we show that almost every critical point of the empirical loss is a \nglobal minimum with zero training error. Our analysis suggests that both depth \nand width are very important in deep learning. While depth brings more \nrepresentational power and allows the network to learn high level features, \nwidth smoothes the optimization landscape of the loss function in the sense \nthat a sufficiently wide network has a well-behaved loss surface with \npotentially no bad local minima. \n</p>"}, "author": "Quynh Nguyen, Matthias Hein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852401", "id": "tag:google.com,2005:reader/item/00000003293f87a4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "A Supervised STDP-based Training Algorithm for Living Neural Networks. (arXiv:1710.10944v1 [cs.NE])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10944"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10944", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks have shown great potential in many applications like speech \nrecognition, drug discovery, image classification, and object detection. Neural \nnetwork models are inspired by biological neural networks, but they are \noptimized to perform machine learning tasks on digital computers. The proposed \nwork explores the possibilities of using living neural networks in vitro as the \nbasic computational elements for machine learning applications. A supervised \nSTDP-based learning algorithm is proposed in this work, which considers neuron \nengineering constrains. A 75% accuracy is achieved on the MNIST benchmark for \nhandwritten digit recognition. \n</p>"}, "author": "Yuan Zeng, Kevin Devincentis, Yao Xiao, Zubayer Ibne Ferdous, Xiaochen Guo, Zhiyuan Yan, Yevgeny Berdichevsky", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852400", "id": "tag:google.com,2005:reader/item/00000003293f87b3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "How Should a Robot Assess Risk? Towards an Axiomatic Theory of Risk in Robotics. (arXiv:1710.11040v1 [cs.RO])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11040"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11040", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Endowing robots with the capability of assessing risk and making risk-aware \ndecisions is widely considered a key step toward ensuring safety for robots \noperating under uncertainty. But, how should a robot quantify risk? A natural \nand common approach is to consider the framework whereby costs are assigned to \nstochastic outcomes - an assignment captured by a cost random variable. \nQuantifying risk then corresponds to evaluating a risk metric, i.e., a mapping \nfrom the cost random variable to a real number. Yet, the question of what \nconstitutes a \"good\" risk metric has received little attention within the \nrobotics community. The goal of this paper is to explore and partially address \nthis question by advocating axioms that risk metrics in robotics applications \nshould satisfy in order to be employed as rational assessments of risk. We \ndiscuss general representation theorems that precisely characterize the class \nof metrics that satisfy these axioms (referred to as distortion risk metrics), \nand provide instantiations that can be used in applications. We further discuss \npitfalls of commonly used risk metrics in robotics, and discuss additional \nproperties that one must consider in sequential decision making tasks. Our hope \nis that the ideas presented here will lead to a foundational framework for \nquantifying risk (and hence safety) in robotics applications. \n</p>"}, "author": "Anirudha Majumdar, Marco Pavone", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852399", "id": "tag:google.com,2005:reader/item/00000003293f87c8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Unsupervised Neural Machine Translation. (arXiv:1710.11041v1 [cs.CL])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11041"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11041", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In spite of the recent success of neural machine translation (NMT) in \nstandard benchmarks, the lack of large parallel corpora poses a major practical \nproblem for many language pairs. There have been several proposals to alleviate \nthis issue with, for instance, triangulation and semi-supervised learning \ntechniques, but they still require a strong cross-lingual signal. In this work, \nwe completely remove the need of parallel data and propose a novel method to \ntrain an NMT system in a completely unsupervised manner, relying on nothing but \nmonolingual corpora. Our model builds upon the recent work on unsupervised \nembedding mappings, and consists of a slightly modified attentional \nencoder-decoder model that can be trained on monolingual corpora alone using a \ncombination of denoising and backtranslation. Despite the simplicity of the \napproach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014 \nFrench-to-English and German-to-English translation. The model can also profit \nfrom small parallel corpora, and attains 21.81 and 15.24 points when combined \nwith 100,000 parallel sentences, respectively. Our approach is a breakthrough \nin unsupervised NMT, and opens exciting opportunities for future research. \n</p>"}, "author": "Mikel Artetxe, Gorka Labaka, Eneko Agirre, Kyunghyun Cho", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852398", "id": "tag:google.com,2005:reader/item/00000003293f87f7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Semantic Code Repair using Neuro-Symbolic Transformation Networks. (arXiv:1710.11054v1 [cs.AI])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11054"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11054", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of semantic code repair, which can be broadly defined as \nautomatically fixing non-syntactic bugs in source code. The majority of past \nwork in semantic code repair assumed access to unit tests against which \ncandidate repairs could be validated. In contrast, the goal here is to develop \na strong statistical model to accurately predict both bug locations and exact \nfixes without access to information about the intended correct behavior of the \nprogram. Achieving such a goal requires a robust contextual repair model, which \nwe train on a large corpus of real-world source code that has been augmented \nwith synthetically injected bugs. Our framework adopts a two-stage approach \nwhere first a large set of repair candidates are generated by rule-based \nprocessors, and then these candidates are scored by a statistical model using a \nnovel neural network architecture which we refer to as Share, Specialize, and \nCompete. Specifically, the architecture (1) generates a shared encoding of the \nsource code using an RNN over the abstract syntax tree, (2) scores each \ncandidate repair using specialized network modules, and (3) then normalizes \nthese scores together so they can compete against one another in comparable \nprobability space. We evaluate our model on a real-world test set gathered from \nGitHub containing four common categories of bugs. Our model is able to predict \nthe exact correct repair 41\\% of the time with a single guess, compared to 13\\% \naccuracy for an attentional sequence-to-sequence model. \n</p>"}, "author": "Jacob Devlin, Jonathan Uesato, Rishabh Singh, Pushmeet Kohli", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852397", "id": "tag:google.com,2005:reader/item/00000003293f880e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Eigenoption Discovery through the Deep Successor Representation. (arXiv:1710.11089v1 [cs.LG])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11089"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11089", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Options in reinforcement learning allow agents to hierarchically decompose a \ntask into subtasks, having the potential to speed up learning and planning. \nHowever, autonomously learning effective sets of options is still a major \nchallenge in the field. In this paper we focus on the recently introduced idea \nof using representation learning methods to guide the option discovery process. \nSpecifically, we look at eigenoptions, options obtained from representations \nthat encode diffusive information flow in the environment. We extend the \nexisting algorithms for eigenoption discovery to settings with stochastic \ntransitions and in which handcrafted features are not available. We propose an \nalgorithm that discovers eigenoptions while learning non-linear state \nrepresentations from raw pixels. It exploits recent successes in the deep \nreinforcement learning literature and the equivalence between proto-value \nfunctions and the successor representation. We use traditional tabular domains \nto provide intuition about our approach and Atari 2600 games to demonstrate its \npotential. \n</p>"}, "author": "Marlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, Murray Campbell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852381", "id": "tag:google.com,2005:reader/item/00000003293f88b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Communicating Semantics: Reference by Description. (arXiv:1511.06341v4 [cs.CL] CROSS LISTED)", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1511.06341"}], "alternate": [{"href": "http://arxiv.org/abs/1511.06341", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c7ebb50\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c7ebb50&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Messages often refer to entities such as people, places and events. Correct \nidentification of the intended reference is an essential part of communication. \nLack of shared unique names often complicates entity reference. Shared \nknowledge can be used to construct uniquely identifying descriptive references \nfor entities with ambiguous names. We introduce a mathematical model for \n`Reference by Description', derive results on the conditions under which, with \nhigh probability, programs can construct unambiguous references to most \nentities in the domain of discourse and provide empirical validation of these \nresults. \n</p>"}, "author": "Ramanathan V Guha, Vineet Gupta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852380", "id": "tag:google.com,2005:reader/item/00000003293f88c1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Understanding Grounded Language Learning Agents. (arXiv:1710.09867v1 [cs.CL] CROSS LISTED)", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09867"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09867", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural network-based systems can now learn to locate the referents of words \nand phrases in images, answer questions about visual scenes, and even execute \nsymbolic instructions as first-person actors in partially-observable worlds. To \nachieve this so-called grounded language learning, models must overcome certain \nwell-studied learning challenges that are also fundamental to infants learning \ntheir first words. While it is notable that models with no meaningful prior \nknowledge overcome these learning obstacles, AI researchers and practitioners \ncurrently lack a clear understanding of exactly how they do so. Here we address \nthis question as a way of achieving a clearer general understanding of grounded \nlanguage learning, both to inform future research and to improve confidence in \nmodel predictions. For maximum control and generality, we focus on a simple \nneural network-based language learning agent trained via policy-gradient \nmethods to interpret synthetic linguistic instructions in a simulated 3D world. \nWe apply experimental paradigms from developmental psychology to this agent, \nexploring the conditions under which established human biases and learning \neffects emerge. We further propose a novel way to visualise and analyse \nsemantic representation in grounded language learning agents that yields a \nplausible computational account of the observed effects. \n</p>"}, "author": "Felix Hill, Karl Moritz Hermann, Phil Blunsom, Stephen Clark", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200350", "id": "tag:google.com,2005:reader/item/000000032916de43", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The implementation of a Deep Recurrent Neural Network Language Model on a Xilinx FPGA. (arXiv:1710.10296v1 [cs.NE])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10296"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10296", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recently, FPGA has been increasingly applied to problems such as speech \nrecognition, machine learning, and cloud computation such as the Bing search \nengine used by Microsoft. This is due to FPGAs great parallel computation \ncapacity as well as low power consumption compared to general purpose \nprocessors. However, these applications mainly focus on large scale FPGA \nclusters which have an extreme processing power for executing massive matrix or \nconvolution operations but are unsuitable for portable or mobile applications. \nThis paper describes research on single-FPGA platform to explore the \napplications of FPGAs in these fields. In this project, we design a Deep \nRecurrent Neural Network (DRNN) Language Model (LM) and implement a hardware \naccelerator with AXI Stream interface on a PYNQ board which is equipped with a \nXILINX ZYNQ SOC XC7Z020 1CLG400C. The PYNQ has not only abundant programmable \nlogic resources but also a flexible embedded operation system, which makes it \nsuitable to be applied in the natural language processing field. We design the \nDRNN language model with Python and Theano, train the model on a CPU platform, \nand deploy the model on a PYNQ board to validate the model with Jupyter \nnotebook. Meanwhile, we design the hardware accelerator with Overlay, which is \na kind of hardware library on PYNQ, and verify the acceleration effect on the \nPYNQ board. Finally, we have found that the DRNN language model can be deployed \non the embedded system smoothly and the Overlay accelerator with AXI Stream \ninterface performs at 20 GOPS processing throughput, which constitutes a 70.5X \nand 2.75X speed up compared to the work in Ref.30 and Ref.31 respectively. \n</p>"}, "author": "Yufeng Hao, Steven Quigley", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200349", "id": "tag:google.com,2005:reader/item/000000032916de4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions. (arXiv:1710.10304v1 [cs.NE])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10304"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10304", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep autoregressive models have shown state-of-the-art performance in density \nestimation for natural images on large-scale datasets such as ImageNet. \nHowever, such models require many thousands of gradient-based weight updates \nand unique image examples for training. Ideally, the models would rapidly learn \nvisual concepts from only a handful of examples, similar to the manner in which \nhumans learns across many vision tasks. In this paper, we show how 1) neural \nattention and 2) meta learning techniques can be used in combination with \nautoregressive models to enable effective few-shot density estimation. Our \nproposed modifications to PixelCNN result in state-of-the art few-shot density \nestimation on the Omniglot dataset. Furthermore, we visualize the learned \nattention policy and find that it learns intuitive algorithms for simple tasks \nsuch as image mirroring on ImageNet and handwriting on Omniglot without \nsupervision. Finally, we extend the model to natural images and demonstrate \nfew-shot image generation on the Stanford Online Products dataset. \n</p>"}, "author": "Scott Reed, Yutian Chen, Thomas Paine, A&#xe4;ron van den Oord, S. M. Ali Eslami, Danilo Rezende, Oriol Vinyals, Nando de Freitas", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200348", "id": "tag:google.com,2005:reader/item/000000032916de52", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convolutional Neural Networks Via Node-Varying Graph Filters. (arXiv:1710.10355v1 [cs.LG])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10355"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10355", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolutional neural networks (CNNs) are being applied to an increasing \nnumber of problems and fields due to their superior performance in \nclassification and regression tasks. Since two of the key operations that CNNs \nimplement are convolution and pooling, this type of networks is implicitly \ndesigned to act on data described by regular structures such as images. \nMotivated by the recent interest in processing signals defined in irregular \ndomains, we advocate a CNN architecture that operates on signals supported on \ngraphs. The proposed design replaces the classical convolution not with a \nnode-invariant graph filter (GF), which is the natural generalization of \nconvolution to graph domains, but with a node-varying GF. This filter extracts \ndifferent local features without increasing the output dimension of each layer \nand, as a result, bypasses the need for a pooling stage while involving only \nlocal operations. A second contribution is to replace the node-varying GF with \na hybrid node-varying GF, which is a new type of GF introduced in this paper. \nWhile the alternative architecture can still be run locally without requiring a \npooling stage, the number of trainable parameters is smaller and can be \nrendered independent of the data dimension. Tests are run on a synthetic source \nlocalization problem and on the 20NEWS dataset. \n</p>"}, "author": "Fernando Gama, Geert Leus, Antonio Garcia Marques, Alejandro Ribeiro", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200347", "id": "tag:google.com,2005:reader/item/000000032916de60", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning. (arXiv:1710.10380v1 [cs.NE])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10380"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10380", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Context information plays an important role in human language understanding, \nand it is also useful for machines to learn vector representations of language. \nIn this paper, we explore an asymmetric encoder-decoder structure for \nunsupervised context-based sentence representation learning. As a result, we \nbuild an encoder-decoder architecture with an RNN encoder and a CNN decoder. We \nfurther combine a suite of effective designs to significantly improve model \nefficiency while also achieving better performance. Our model is trained on two \ndifferent large unlabeled corpora, and in both cases transferability is \nevaluated on a set of downstream language understanding tasks. We empirically \nshow that our model is simple and fast while producing rich sentence \nrepresentations that excel in downstream tasks. \n</p>"}, "author": "Shuai Tang, Hailin Jin, Chen Fang, Zhaowen Wang, Virginia R. de Sa", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200346", "id": "tag:google.com,2005:reader/item/000000032916de62", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Efficient Licence Plate Detection By Unique Edge Detection Algorithm and Smarter Interpretation Through IoT. (arXiv:1710.10418v1 [cs.NE])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10418"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10418", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Vehicles play a vital role in modern day transportation systems. Number plate \nprovides a standard means of identification for any vehicle. To serve this \npurpose, automatic licence plate recognition system was developed. This \nconsisted of four major steps: Pre-processing of the obtained image, extraction \nof licence plate region, segmentation and character recognition. In earlier \nresearch, direct application of Sobel edge detection algorithm or applying \nthreshold were used as key steps to extract the licence plate region, which \ndoes not produce effective results when the captured image is subjected to the \nhigh intensity of light. The use of morphological operations causes deformity \nin the characters during segmentation. We propose a novel algorithm to tackle \nthe mentioned issues through a unique edge detection algorithm. It is also a \ntedious task to create and update the database of required vehicles frequently. \nThis problem is solved by the use of Internet of things(IOT) where an online \ndatabase can be created and updated from any module instantly. Also, through \nIoT, we connect all the cameras in a geographical area to one server to create \na universal eye which drastically increases the probability of tracing a \nvehicle over having manual database attached to each camera for identification \npurpose. \n</p>"}, "author": "Tejas K, Ashok Reddy K, Pradeep Reddy D, Rajesh Kumar M", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200345", "id": "tag:google.com,2005:reader/item/000000032916de6e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sample-level CNN Architectures for Music Auto-tagging Using Raw Waveforms. (arXiv:1710.10451v1 [cs.SD])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10451"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10451", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent work has shown that the end-to-end approach using convolutional neural \nnetwork (CNN) is effective in various types of machine learning tasks. For \naudio signals, the approach takes raw waveforms as input using an 1-D \nconvolution layer. In this paper, we improve the 1-D CNN architecture for music \nauto-tagging by adopting building blocks from state-of-the-art image \nclassification models, ResNets and SENets, and adding multi-level feature \naggregation to it. We compare different combinations of the modules in building \nCNN architectures. The results show that they achieve significant improvements \nover previous state-of-the-art models on the MagnaTagATune dataset and \ncomparable results on Million Song Dataset. Furthermore, we analyze and \nvisualize our model to show how the 1-D CNN operates. \n</p>"}, "author": "Taejun Kim, Jongpil Lee, Juhan Nam", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200344", "id": "tag:google.com,2005:reader/item/000000032916de78", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Toward predictive machine learning for active vision. (arXiv:1710.10460v1 [cs.NE])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10460"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10460", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We develop a comprehensive description of the active inference framework, as \nproposed by Friston (2010), under a machine-learning compliant perspective. \nStemming from a biological inspiration and the auto-encoding principles, a \nsketch of a cognitive architecture is proposed that should provide ways to \nimplement estimation-oriented control policies under a POMDP perspective. \nComputer simulations illustrate the effectiveness of the approach through a \nfoveated inspection the input data. The pros and cons of the control policy are \nreviewed in details, showing interesting promises in term of processing \ncompression, but also putative risks of a confirmation bias that may degrade \nthe recognition performance if the model is too optimistic about its own \npredictions. The presented formalism is fully compliant with the auto-encoding \nframework and would deserve further developments under variational encoding \narchitectures. \n</p>"}, "author": "Emmanuel Dauc&#xe9;", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200343", "id": "tag:google.com,2005:reader/item/000000032916de7d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Discovery Radiomics with CLEAR-DR: Interpretable Computer Aided Diagnosis of Diabetic Retinopathy. (arXiv:1710.10675v1 [cs.AI])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10675"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10675", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Objective: Radiomics-driven Computer Aided Diagnosis (CAD) has shown \nconsiderable promise in recent years as a potential tool for improving clinical \ndecision support in medical oncology, particularly those based around the \nconcept of Discovery Radiomics, where radiomic sequencers are discovered \nthrough the analysis of medical imaging data. One of the main limitations with \ncurrent CAD approaches is that it is very difficult to gain insight or \nrationale as to how decisions are made, thus limiting their utility to \nclinicians. Methods: In this study, we propose CLEAR-DR, a novel interpretable \nCAD system based on the notion of CLass-Enhanced Attentive Response Discovery \nRadiomics for the purpose of clinical decision support for diabetic \nretinopathy. Results: In addition to disease grading via the discovered deep \nradiomic sequencer, the CLEAR-DR system also produces a visual interpretation \nof the decision-making process to provide better insight and understanding into \nthe decision-making process of the system. Conclusion: We demonstrate the \neffectiveness and utility of the proposed CLEAR-DR system of enhancing the \ninterpretability of diagnostic grading results for the application of diabetic \nretinopathy grading. Significance: CLEAR-DR can act as a potential powerful \ntool to address the uninterpretability issue of current CAD systems, thus \nimproving their utility to clinicians. \n</p>"}, "author": "Devinder Kumar, Graham W. Taylor, Alexander Wong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200342", "id": "tag:google.com,2005:reader/item/000000032916de83", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Regularization for Deep Learning: A Taxonomy. (arXiv:1710.10686v1 [cs.LG])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10686"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10686", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c7ebff3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c7ebff3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Regularization is one of the crucial ingredients of deep learning, yet the \nterm regularization has various definitions, and regularization methods are \noften studied separately from each other. In our work we present a systematic, \nunifying taxonomy to categorize existing methods. We distinguish methods that \naffect data, network architectures, error terms, regularization terms, and \noptimization procedures. We do not provide all details about the listed \nmethods; instead, we present an overview of how the methods can be sorted into \nmeaningful categories and sub-categories. This helps revealing links and \nfundamental similarities between them. Finally, we include practical \nrecommendations both for users and for developers of new regularization \nmethods. \n</p>"}, "author": "Jan Kuka&#x10d;ka, Vladimir Golkov, Daniel Cremers", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200341", "id": "tag:google.com,2005:reader/item/000000032916de87", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Training Probabilistic Spiking Neural Networks with First-to-spike Decoding. (arXiv:1710.10704v1 [stat.ML])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10704"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10704", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c873358\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c873358&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Third-generation neural networks, or Spiking Neural Networks (SNNs), aim at \nharnessing the energy efficiency of spike-domain processing by building on \ncomputing elements that operate on, and exchange, spikes. In this paper, the \nproblem of training a two-layer SNNs is studied for the purpose of \nclassification, under a Generalized Linear Model (GLM) probabilistic neural \nmodel that was previously considered within the computational neuroscience \nliterature. Conventional classification rules for SNNs operate offline based on \nthe number of output spikes at each output neuron. In contrast, a novel \ntraining method is proposed here for a first-to-spike decoding rule, whereby \nthe SNN can perform an early classification decision once spike firing is \ndetected at an output neuron. Numerical results bring insights into the optimal \nparameter selection for the GLM neuron and on the accuracy-complexity trade-off \nperformance of conventional and first-to-spike decoding. \n</p>"}, "author": "Alireza Bagheri, Osvaldo Simeone, Bipin Rajendran", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200340", "id": "tag:google.com,2005:reader/item/000000032916de8f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "BAS: Beetle Antennae Search Algorithm for Optimization Problems. (arXiv:1710.10724v1 [cs.NE])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10724"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10724", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Meta-heuristic algorithms have become very popular because of powerful \nperformance on the optimization problem. A new algorithm called beetle antennae \nsearch algorithm (BAS) is proposed in the paper inspired by the searching \nbehavior of longhorn beetles. The BAS algorithm imitates the function of \nantennae and the random walking mechanism of beetles in nature, and then two \nmain steps of detecting and searching are implemented. Finally, the algorithm \nis benchmarked on 2 well-known test functions, in which the numerical results \nvalidate the efficacy of the proposed BAS algorithm. \n</p>"}, "author": "Xiangyuan Jiang, Shuai Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200339", "id": "tag:google.com,2005:reader/item/000000032916de96", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Evolving Deep Convolutional Neural Networks for Image Classification. (arXiv:1710.10741v1 [cs.NE])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10741"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10741", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Evolutionary computation methods have been successfully applied to neural \nnetworks since two decades ago, while those methods cannot scale well to the \nmodern deep neural networks due to the complicated architectures and large \nquantities of connection weights. In this paper, we propose a new method using \ngenetic algorithms for evolving the architectures and connection weight \ninitialization values of a deep convolutional neural network to address image \nclassification problems. In the proposed algorithm, an efficient \nvariable-length gene encoding strategy is designed to represent the different \nbuilding blocks and the unpredictable optimal depth in convolutional neural \nnetworks. In addition, a new representation scheme is developed for effectively \ninitializing connection weights of deep convolutional neural networks, which is \nexpected to avoid networks getting stuck into local minima which is typically a \nmajor issue in the backward gradient-based optimization. Furthermore, a novel \nfitness evaluation method is proposed to speed up the heuristic search with \nsubstantially less computational resource. The proposed algorithm is examined \nand compared with 22 existing algorithms on nine widely used image \nclassification tasks, including the state-of-the-art methods. The experimental \nresults demonstrate the remarkable superiority of the proposed algorithm over \nthe state-of-the-art algorithms in terms of classification error rate and the \nnumber of parameters (weights). \n</p>"}, "author": "Yanan Sun, Bing Xue, Mengjie Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200338", "id": "tag:google.com,2005:reader/item/000000032916de9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Crack Is Controllable, a controllable crack propagation method by using artificial neural network assisted particle swarm optimization. (arXiv:1710.10748v1 [cs.NE])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10748"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10748", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This study proposed a controllable crack propagation (CCP) method by \nintegrating back propagation neural network (BPNN) assisted particle swarm \noptimization (PSO), reanalysis and extended finite element methods (X-FEM). It \nis well known that the internal crack propagation always leads the failure of \nengineering structure because the crack crossed the critical domain of the \nstructure. Therefore, the controllable crack propagation method is proposed to \ncontrol the crack propagation path and make the crack propagate along the \nspecified path, so that the critical domain should not be crossed by the crack \nand the failure will be avoided. Considering the optimization iteration is a \ntime consuming process, an efficient reanalysis based X-FEM is used to \ncalculate the crack propagation path, in which a reanalysis solver is used to \nsolve the equilibrium equations efficiently. Then the BPNN assisted PSO method \nshould be used to obtain the optimal design variables to make the real crack \npath match with the specified path. Moreover, an adaptive subdomain partition \nstrategy is suggested to improve the fitting accuracy between real crack path \nand specified path. To verify the performance of the proposed method, several \ntypical numerical examples have been analyzed and the results demonstrate that \nthis method can really control the crack propagation path with high efficiency. \n</p>"}, "author": "Zhenxing Cheng, Hu Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200337", "id": "tag:google.com,2005:reader/item/000000032916dea5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generative Adversarial Source Separation. (arXiv:1710.10779v1 [cs.SD])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10779"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10779", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative source separation methods such as non-negative matrix \nfactorization (NMF) or auto-encoders, rely on the assumption of an output \nprobability density. Generative Adversarial Networks (GANs) can learn data \ndistributions without needing a parametric assumption on the output density. We \nshow on a speech source separation experiment that, a multi-layer perceptron \ntrained with a Wasserstein-GAN formulation outperforms NMF, auto-encoders \ntrained with maximum likelihood, and variational auto-encoders in terms of \nsource to distortion ratio. \n</p>"}, "author": "Cem Subakan, Paris Smaragdis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200336", "id": "tag:google.com,2005:reader/item/000000032916deb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Supervised STDP-based Training Algorithm for Living Neural Networks. (arXiv:1710.10944v2 [cs.NE] UPDATED)", "published": 1510063179, "updated": 1510063179, "canonical": [{"href": "http://arxiv.org/abs/1710.10944"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10944", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks have shown great potential in many applications like speech \nrecognition, drug discovery, image classification, and object detection. Neural \nnetwork models are inspired by biological neural networks, but they are \noptimized to perform machine learning tasks on digital computers. The proposed \nwork explores the possibilities of using living neural networks in vitro as \nbasic computational elements for machine learning applications. A new \nsupervised STDP-based learning algorithm is proposed in this work, which \nconsiders neuron engineering constrains. A 74.7% accuracy is achieved on the \nMNIST benchmark for handwritten digit recognition. \n</p>"}, "author": "Yuan Zeng, Kevin Devincentis, Yao Xiao, Zubayer Ibne Ferdous, Xiaochen Guo, Zhiyuan Yan, Yevgeny Berdichevsky", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200329", "id": "tag:google.com,2005:reader/item/000000032916ded7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Understanding Grounded Language Learning Agents. (arXiv:1710.09867v1 [cs.CL] CROSS LISTED)", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09867"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09867", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural network-based systems can now learn to locate the referents of words \nand phrases in images, answer questions about visual scenes, and even execute \nsymbolic instructions as first-person actors in partially-observable worlds. To \nachieve this so-called grounded language learning, models must overcome certain \nwell-studied learning challenges that are also fundamental to infants learning \ntheir first words. While it is notable that models with no meaningful prior \nknowledge overcome these learning obstacles, AI researchers and practitioners \ncurrently lack a clear understanding of exactly how they do so. Here we address \nthis question as a way of achieving a clearer general understanding of grounded \nlanguage learning, both to inform future research and to improve confidence in \nmodel predictions. For maximum control and generality, we focus on a simple \nneural network-based language learning agent trained via policy-gradient \nmethods to interpret synthetic linguistic instructions in a simulated 3D world. \nWe apply experimental paradigms from developmental psychology to this agent, \nexploring the conditions under which established human biases and learning \neffects emerge. We further propose a novel way to visualise and analyse \nsemantic representation in grounded language learning agents that yields a \nplausible computational account of the observed effects. \n</p>"}, "author": "Felix Hill, Karl Moritz Hermann, Phil Blunsom, Stephen Clark", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509325337660", "timestampUsec": "1509325337659589", "id": "tag:google.com,2005:reader/item/000000032869321e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Phase Transitions in Image Denoising via Sparsely Coding Convolutional Neural Networks. (arXiv:1710.09875v1 [cs.NE])", "published": 1509325338, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09875"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09875", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks are analogous in many ways to spin glasses, systems which are \nknown for their rich set of dynamics and equally complex phase diagrams. We \napply well-known techniques in the study of spin glasses to a convolutional \nsparsely encoding neural network and observe power law finite-size scaling \nbehavior in the sparsity and reconstruction error as the network denoises \n32$\\times$32 RGB CIFAR-10 images. This finite-size scaling indicates the \npresence of a continuous phase transition at a critical value of this sparsity. \nBy using the power law scaling relations inherent to finite-size scaling, we \ncan determine the optimal value of sparsity for any network size by tuning the \nsystem to the critical point and operate the system at the minimum denoising \nerror. \n</p>"}, "author": "Jacob Carroll, Nils Carlson, Garrett T. Kenyon", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509325337660", "timestampUsec": "1509325337659588", "id": "tag:google.com,2005:reader/item/000000032869322d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Data-driven Feature Sampling for Deep Hyperspectral Classification and Segmentation. (arXiv:1710.09934v1 [cs.NE])", "published": 1509325338, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09934"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09934", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The high dimensionality of hyperspectral imaging forces unique challenges in \nscope, size and processing requirements. Motivated by the potential for an \nin-the-field cell sorting detector, we examine a $\\textit{Synechocystis sp.}$ \nPCC 6803 dataset wherein cells are grown alternatively in nitrogen rich or \ndeplete cultures. We use deep learning techniques to both successfully classify \ncells and generate a mask segmenting the cells/condition from the background. \nFurther, we use the classification accuracy to guide a data-driven, iterative \nfeature selection method, allowing the design neural networks requiring 90% \nfewer input features with little accuracy degradation. \n</p>"}, "author": "William M. Severa, Jerilyn A. Timlin, Suraj Kholwadwala, Conrad D. James, James B. Aimone", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509325337660", "timestampUsec": "1509325337659587", "id": "tag:google.com,2005:reader/item/000000032869323a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation. (arXiv:1710.10196v1 [cs.NE])", "published": 1509325338, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10196"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10196", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c8735a2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c8735a2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We describe a new training methodology for generative adversarial networks. \nThe key idea is to grow both the generator and discriminator progressively: \nstarting from a low resolution, we add new layers that model increasingly fine \ndetails as training progresses. This both speeds the training up and greatly \nstabilizes it, allowing us to produce images of unprecedented quality, e.g., \nCelebA images at 1024^2. We also propose a simple way to increase the variation \nin generated images, and achieve a record inception score of 8.80 in \nunsupervised CIFAR10. Additionally, we describe several implementation details \nthat are important for discouraging unhealthy competition between the generator \nand discriminator. Finally, we suggest a new metric for evaluating GAN results, \nboth in terms of image quality and variation. As an additional contribution, we \nconstruct a higher-quality version of the CelebA dataset. \n</p>"}, "author": "Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509325337660", "timestampUsec": "1509325337659586", "id": "tag:google.com,2005:reader/item/0000000328693245", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Tensor network language model. (arXiv:1710.10248v2 [cs.CL] UPDATED)", "published": 1509413371, "updated": 1509413373, "canonical": [{"href": "http://arxiv.org/abs/1710.10248"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10248", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a new statistical model suitable for machine learning of systems \nwith long distance correlations such as natural languages. The model is based \non directed acyclic graph decorated by multi-linear tensor maps in the vertices \nand vector spaces in the edges, called tensor network. Such tensor networks \nhave been previously employed for effective numerical computation of the \nrenormalization group flow on the space of effective quantum field theories and \nlattice models of statistical mechanics. We provide explicit algebro-geometric \nanalysis of the parameter moduli space for tree graphs, discuss model \nproperties and applications such as statistical translation. \n</p>"}, "author": "Vasily Pestun, Yiannis Vlassopoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505092", "id": "tag:google.com,2005:reader/item/000000032866e7af", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Gradient Sparsification for Communication-Efficient Distributed Optimization. (arXiv:1710.09854v1 [cs.LG])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09854"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09854", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Modern large scale machine learning applications require stochastic \noptimization algorithms to be implemented on distributed computational \narchitectures. A key bottleneck is the communication overhead for exchanging \ninformation such as stochastic gradients among different workers. In this \npaper, to reduce the communication cost we propose a convex optimization \nformulation to minimize the coding length of stochastic gradients. To solve the \noptimal sparsification efficiently, several simple and fast algorithms are \nproposed for approximate solution, with theoretical guaranteed for sparseness. \nExperiments on $\\ell_2$ regularized logistic regression, support vector \nmachines, and convolutional neural networks validate our sparsification \napproaches. \n</p>"}, "author": "Jianqiao Wangni, Jialei Wang, Ji Liu, Tong Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505091", "id": "tag:google.com,2005:reader/item/000000032866e841", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Energy Clustering. (arXiv:1710.09859v1 [stat.ML])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09859"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09859", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Energy statistics was proposed by Sz\\'{e}kely in the 80's inspired by the \nNewtonian gravitational potential from classical mechanics, and it provides a \nhypothesis test for equality of distributions. It was further generalized from \nEuclidean spaces to metric spaces of strong negative type, and more recently, a \nconnection with reproducing kernel Hilbert spaces (RKHS) was established. Here \nwe consider the clustering problem from an energy statistics theory \nperspective, providing a precise mathematical formulation yielding a \nquadratically constrained quadratic program (QCQP) in the associated RKHS, thus \nestablishing the connection with kernel methods. We show that this QCQP is \nequivalent to kernel $k$-means optimization problem once the kernel is fixed. \nThese results imply a first principles derivation of kernel $k$-means from \nenergy statistics. However, energy statistics fixes a family of standard \nkernels. Furthermore, we also consider a weighted version of energy statistics, \nmaking connection to graph partitioning problems. To find local optimizers of \nsuch QCQP we propose an iterative algorithm based on Hartigan's method, which \nin this case has the same computational cost as kernel $k$-means algorithm, \nbased on Lloyd's heuristic, but usually with better clustering quality. We \nprovide carefully designed numerical experiments showing the superiority of the \nproposed method compared to kernel $k$-means, spectral clustering, standard \n$k$-means, and Gaussian mixture models in a variety of settings. \n</p>"}, "author": "Guilherme Fran&#xe7;a, Joshua T. Vogelstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505090", "id": "tag:google.com,2005:reader/item/000000032866e8b6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Error Probability of Random Fourier Features is Dimensionality Independent. (arXiv:1710.09953v1 [cs.LG])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09953"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09953", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We show that the error probability of reconstructing kernel matrices from \nRandom Fourier Features for any shift-invariant kernel function is at most \n$\\mathcal{O}(\\exp(-D))$, where $D$ is the number of random features. We also \nprovide a matching information-theoretic method-independent lower bound of \n$\\Omega(\\exp(-D))$ for standard Gaussian distributions. Compared to prior work, \nwe are the first to show that the error probability for random Fourier features \nis independent of the dimensionality of data points as well as the size of \ntheir domain. As applications of our theory, we obtain dimension-independent \nbounds for kernel ridge regression and support vector machines. \n</p>"}, "author": "Jean Honorio, Yu-Jun Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505089", "id": "tag:google.com,2005:reader/item/000000032866e90a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Conjugate Gradient Algorithm with Variance Reduction. (arXiv:1710.09979v1 [cs.LG])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09979"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09979", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Conjugate gradient methods are a class of important methods for solving \nlinear equations and nonlinear optimization. In our work, we propose a new \nstochastic conjugate gradient algorithm with variance reduction (CGVR) and \nprove its linear convergence with the Fletcher and Revves method for strongly \nconvex and smooth functions. We experimentally demonstrate that the CGVR \nalgorithm converges faster than its counterparts for six large-scale \noptimization problems that may be convex, non-convex or non-smooth, and its AUC \n(Area Under Curve) performance with $L2$-regularized $L2$-loss is comparable to \nthat of LIBLINEAR but with significant improvement in computational efficiency. \n</p>"}, "author": "Xiao-Bo Jin, Xu-Yao Zhang, Kaizhu Huang, Guang-Gang Geng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505088", "id": "tag:google.com,2005:reader/item/000000032866e95a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Zeroth Order Nonconvex Multi-Agent Optimization over Networks. (arXiv:1710.09997v1 [math.OC])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09997"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09997", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we consider distributed optimization problems over a \nmulti-agent network, where each agent can only partially evaluate the objective \nfunction, and it is allowed to exchange messages with its immediate neighbors. \nDifferently from all existing works on distributed optimization, our focus is \ngiven to optimizing a class of difficult non-convex problems, and under the \nchallenging setting where each agent can only access the zeroth-order \ninformation (i.e., the functional values) of its local functions. For different \ntypes of network topologies such as undirected connected networks or star \nnetworks, we develop efficient distributed algorithms and rigorously analyze \ntheir convergence and rate of convergence (to the set of stationary solutions). \nNumerical results are provided to demonstrate the efficiency of the proposed \nalgorithms. \n</p>"}, "author": "Davood Hajinezhad, Mingyi Hong, Alfredo Garcia", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505087", "id": "tag:google.com,2005:reader/item/000000032866e9b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning for Accelerated Ultrasound Imaging. (arXiv:1710.10006v1 [cs.CV])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10006"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10006", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In portable, 3-D, or ultra-fast ultrasound (US) imaging systems, there is an \nincreasing demand to reconstruct high quality images from limited number of \ndata. However, the existing solutions require either hardware changes or \ncomputationally expansive algorithms. To overcome these limitations, here we \npropose a novel deep learning approach that interpolates the missing RF data by \nutilizing the sparsity of the RF data in the Fourier domain. Extensive \nexperimental results from sub-sampled RF data from a real US system confirmed \nthat the proposed method can effectively reduce the data rate without \nsacrificing the image quality. \n</p>"}, "author": "Yeo Hun Yoon, Jong Chul Ye", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505086", "id": "tag:google.com,2005:reader/item/000000032866e9fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Regularization via Mass Transportation. (arXiv:1710.10016v1 [math.OC])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10016"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10016", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The goal of regression and classification methods in supervised learning is \nto minimize the empirical risk, that is, the expectation of some loss function \nquantifying the prediction error under the empirical distribution. When facing \nscarce training data, overfitting is typically mitigated by adding \nregularization terms to the objective that penalize hypothesis complexity. In \nthis paper we introduce new regularization techniques using ideas from \ndistributionally robust optimization, and we give new probabilistic \ninterpretations to existing techniques. Specifically, we propose to minimize \nthe worst-case expected loss, where the worst case is taken over the ball of \nall (continuous or discrete) distributions that have a bounded transportation \ndistance from the (discrete) empirical distribution. By choosing the radius of \nthis ball judiciously, we can guarantee that the worst-case expected loss \nprovides an upper confidence bound on the loss on test data, thus offering new \ngeneralization bounds. We prove that the resulting regularized learning \nproblems are tractable and can be tractably kernelized for many popular loss \nfunctions. We validate our theoretical out-of-sample guarantees through \nsimulated and empirical experiments. \n</p>"}, "author": "Soroosh Shafieezadeh-Abadeh, Daniel Kuhn, Peyman Mohajerin Esfahani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505085", "id": "tag:google.com,2005:reader/item/000000032866ea59", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Online Learning of Power Transmission Dynamics. (arXiv:1710.10021v1 [cs.SY])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10021"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10021", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of reconstructing the dynamic state matrix of \ntransmission power grids from time-stamped PMU measurements in the regime of \nambient fluctuations. Using a maximum likelihood based approach, we construct a \nfamily of convex estimators that adapt to the structure of the problem \ndepending on the available prior information. The proposed method is fully \ndata-driven and does not assume any knowledge of system parameters. It can be \nimplemented in near real-time and requires a small amount of data. Our learning \nalgorithms can be used for model validation and calibration, and can also be \napplied to related problems of system stability, detection of forced \noscillations, generation re-dispatch, as well as to the estimation of the \nsystem state. \n</p>"}, "author": "Andrey Y. Lokhov, Marc Vuffray, Dmitry Shemetov, Deepjyoti Deka, Michael Chertkov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505084", "id": "tag:google.com,2005:reader/item/000000032866eaed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generalization Tower Network: A Novel Deep Neural Network Architecture for Multi-Task Learning. (arXiv:1710.10036v1 [cs.AI])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10036"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10036", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c8737bf\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c8737bf&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by \nincorporating deep neural networks in learning representations from the input \nto RL. However, the conventional deep neural network architecture is limited in \nlearning representations for multi-task RL (MT-RL), as multiple tasks can refer \nto different kinds of representations. In this paper, we thus propose a novel \ndeep neural network architecture, namely generalization tower network (GTN), \nwhich can achieve MT-RL within a single learned model. Specifically, the \narchitecture of GTN is composed of both horizontal and vertical streams. In our \nGTN architecture, horizontal streams are used to learn representation shared in \nsimilar tasks. In contrast, the vertical streams are introduced to be more \nsuitable for handling diverse tasks, which encodes hierarchical shared \nknowledge of these tasks. The effectiveness of the introduced vertical stream \nis validated by experimental results. Experimental results further verify that \nour GTN architecture is able to advance the state-of-the-art MT-RL, via being \ntested on 51 Atari games. \n</p>"}, "author": "Yuhang Song, Main Xu, Songyang Zhang, Liangyu Huo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505083", "id": "tag:google.com,2005:reader/item/000000032866eb3e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Distributional Reinforcement Learning with Quantile Regression. (arXiv:1710.10044v1 [cs.AI])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10044"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10044", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c8dd9cc\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c8dd9cc&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In reinforcement learning an agent interacts with the environment by taking \nactions and observing the next state and reward. When sampled \nprobabilistically, these state transitions, rewards, and actions can all induce \nrandomness in the observed long-term return. Traditionally, reinforcement \nlearning algorithms average over this randomness to estimate the value \nfunction. In this paper, we build on recent work advocating a distributional \napproach to reinforcement learning in which the distribution over returns is \nmodeled explicitly instead of only estimating the mean. That is, we examine \nmethods of learning the value distribution instead of the value function. We \ngive results that close a number of gaps between the theoretical and \nalgorithmic results given by Bellemare, Dabney, and Munos (2017). First, we \nextend existing results to the approximate distribution setting. Second, we \npresent a novel distributional reinforcement learning algorithm consistent with \nour theoretical formulation. Finally, we evaluate this new algorithm on the \nAtari 2600 games, observing that it significantly outperforms many of the \nrecent improvements on DQN, including the related distributional algorithm C51. \n</p>"}, "author": "Will Dabney, Mark Rowland, Marc G. Bellemare, R&#xe9;mi Munos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505082", "id": "tag:google.com,2005:reader/item/000000032866eb88", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations. (arXiv:1710.10121v1 [cs.CV])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10121"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10121", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In our work, we bridge deep neural network design with numerical differential \nequations. We show that many effective networks, such as ResNet, PolyNet, \nFractalNet and RevNet, can be interpreted as different numerical \ndiscretizations of differential equations. This finding brings us a brand new \nperspective on the design of effective deep architectures. We can take \nadvantage of the rich knowledge in numerical analysis to guide us in designing \nnew and potentially more effective deep networks. As an example, we propose a \nlinear multi-step architecture (LM-architecture) which is inspired by the \nlinear multi-step method solving ordinary differential equations. The \nLM-architecture is an effective structure that can be used on any ResNet-like \nnetworks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the \nnetworks obtained by applying the LM-architecture on ResNet and ResNeXt \nrespectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on \nboth CIFAR and ImageNet with comparable numbers of trainable parameters. In \nparticular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly \ncompress ($&gt;50$\\%) the original networks while maintaining a similar \nperformance. This can be explained mathematically using the concept of modified \nequation from numerical analysis. Last but not least, we also establish a \nconnection between stochastic control and noise injection in the training \nprocess which helps to improve generalization of the networks. Furthermore, by \nrelating stochastic training strategy with stochastic dynamic system, we can \neasily apply stochastic training to the networks with the LM-architecture. As \nan example, we introduced stochastic depth to LM-ResNet and achieve significant \nimprovement over the original LM-ResNet on CIFAR10. \n</p>"}, "author": "Yiping Lu, Aoxiao Zhong, Quanzheng Li, Bin Dong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505081", "id": "tag:google.com,2005:reader/item/000000032866ec0a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Development and analysis of a Bayesian water balance model for large lake systems. (arXiv:1710.10161v1 [stat.AP])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10161"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10161", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Water balance models are often employed to improve understanding of drivers \nof change in regional hydrologic cycles. Most of these models, however, are \nphysically-based, and few employ state-of-the-art statistical methods to \nreconcile measurement uncertainty and bias. Here, we introduce a framework for \ndeveloping, analyzing, and selecting among alternative formulations of a \nstatistical water balance model for large lake systems that addresses this \nresearch gap. We demonstrate our new analytical framework using a model \ncustomized for Lakes Superior and Michigan-Huron, the two largest lakes on \nEarth by surface area. The selected model (from among 26 alternatives) closed \nthe water balance across both lakes and had a computation time of roughly 95 \nminutes - an order of magnitude less than prototype versions of the same model. \nWe expect our new framework will be used to improve computational efficiency \nand skill of water balance models for other lakes around the world. \n</p>"}, "author": "Joeseph P. Smith, Andrew D. Gronewold", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505080", "id": "tag:google.com,2005:reader/item/000000032866ec71", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation. (arXiv:1710.10196v1 [cs.NE])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10196"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10196", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We describe a new training methodology for generative adversarial networks. \nThe key idea is to grow both the generator and discriminator progressively: \nstarting from a low resolution, we add new layers that model increasingly fine \ndetails as training progresses. This both speeds the training up and greatly \nstabilizes it, allowing us to produce images of unprecedented quality, e.g., \nCelebA images at 1024^2. We also propose a simple way to increase the variation \nin generated images, and achieve a record inception score of 8.80 in \nunsupervised CIFAR10. Additionally, we describe several implementation details \nthat are important for discouraging unhealthy competition between the generator \nand discriminator. Finally, we suggest a new metric for evaluating GAN results, \nboth in terms of image quality and variation. As an additional contribution, we \nconstruct a higher-quality version of the CelebA dataset. \n</p>"}, "author": "Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505079", "id": "tag:google.com,2005:reader/item/000000032866ecc2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On denoising modulo 1 samples of a function. (arXiv:1710.10210v2 [stat.ML] UPDATED)", "published": 1510049224, "updated": 1510049278, "canonical": [{"href": "http://arxiv.org/abs/1710.10210"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10210", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Consider an unknown smooth function $f: [0,1] \\rightarrow \\mathbb{R}$, and \nsay we are given $n$ noisy mod 1 samples of $f$, i.e., $y_i = (f(x_i) + \n\\eta_i)\\mod 1$ for $x_i \\in [0,1]$, where $\\eta_i$ denotes noise. Given the \nsamples $(x_i,y_i)_{i=1}^{n}$, our goal is to recover smooth, robust estimates \nof the clean samples $f(x_i) \\bmod 1$. We formulate a natural approach for \nsolving this problem which works with angular embeddings of the noisy mod 1 \nsamples over the unit complex circle, inspired by the angular synchronization \nframework. Our approach amounts to solving a quadratically constrained \nquadratic program (QCQP) which is NP-hard in its basic form, and therefore we \nconsider its relaxation which is a trust region sub-problem and hence solvable \nefficiently. We demonstrate its robustness to noise via extensive numerical \nsimulations on several synthetic examples, along with a detailed theoretical \nanalysis. To the best of our knowledge, we provide the first algorithm for \ndenoising mod 1 samples of a smooth function, which comes with robustness \nguarantees. \n</p>"}, "author": "Mihai Cucuringu, Hemant Tyagi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505078", "id": "tag:google.com,2005:reader/item/000000032866ed2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Not-So-Random Features. (arXiv:1710.10230v1 [cs.LG])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10230"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10230", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a principled method for kernel learning, which relies on a \nFourier-analytic characterization of translation-invariant or \nrotation-invariant kernels. Our method produces a sequence of feature maps, \niteratively refining the SVM margin. We provide rigorous guarantees for \noptimality and generalization, interpreting our algorithm as online \nequilibrium-finding dynamics in a certain two-player min-max game. Evaluations \non synthetic and real-world datasets demonstrate scalability and consistent \nimprovements over related random features-based methods. \n</p>"}, "author": "Brian Bullins, Cyril Zhang, Yi Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505077", "id": "tag:google.com,2005:reader/item/000000032866ed54", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Tensor network language model. (arXiv:1710.10248v2 [cs.CL] UPDATED)", "published": 1509484038, "updated": 1509484044, "canonical": [{"href": "http://arxiv.org/abs/1710.10248"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10248", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a new statistical model suitable for machine learning of systems \nwith long distance correlations such as natural languages. The model is based \non directed acyclic graph decorated by multi-linear tensor maps in the vertices \nand vector spaces in the edges, called tensor network. Such tensor networks \nhave been previously employed for effective numerical computation of the \nrenormalization group flow on the space of effective quantum field theories and \nlattice models of statistical mechanics. We provide explicit algebro-geometric \nanalysis of the parameter moduli space for tree graphs, discuss model \nproperties and applications such as statistical translation. \n</p>"}, "author": "Vasily Pestun, Yiannis Vlassopoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097875", "id": "tag:google.com,2005:reader/item/000000032866b89b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Gradient Sparsification for Communication-Efficient Distributed Optimization. (arXiv:1710.09854v1 [cs.LG])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09854"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09854", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Modern large scale machine learning applications require stochastic \noptimization algorithms to be implemented on distributed computational \narchitectures. A key bottleneck is the communication overhead for exchanging \ninformation such as stochastic gradients among different workers. In this \npaper, to reduce the communication cost we propose a convex optimization \nformulation to minimize the coding length of stochastic gradients. To solve the \noptimal sparsification efficiently, several simple and fast algorithms are \nproposed for approximate solution, with theoretical guaranteed for sparseness. \nExperiments on $\\ell_2$ regularized logistic regression, support vector \nmachines, and convolutional neural networks validate our sparsification \napproaches. \n</p>"}, "author": "Jianqiao Wangni, Jialei Wang, Ji Liu, Tong Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097874", "id": "tag:google.com,2005:reader/item/000000032866b89f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Enhancements of linked data expressiveness for ontologies. (arXiv:1710.09952v1 [cs.AI])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09952"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09952", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The semantic web has received many contributions of researchers as ontologies \nwhich, in this context, i.e. within RDF linked data, are formalized \nconceptualizations that might use different protocols, such as RDFS, OWL DL and \nOWL FULL. In this article, we describe new expressive techniques which were \nfound necessary after elaborating dozens of OWL ontologies for the scientific \nacademy, the State and the civil society. They consist in: 1) stating possible \nuses a property might have without incurring into axioms or restrictions; 2) \nassigning a level of priority for an element (class, property, triple); 3) \ncorrect depiction in diagrams of relations between classes, between individuals \nwhich are imperative, and between individuals which are optional; 4) a \nconvenient association between OWL classes and SKOS concepts. We propose \nspecific rules to accomplish these enhancements and exemplify both its use and \nthe difficulties that arise because these techniques are currently not \nestablished as standards to the ontology designer. \n</p>"}, "author": "Renato Fabbri", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097873", "id": "tag:google.com,2005:reader/item/000000032866b8a5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Audiovisual Analytics Vocabulary and Ontology (AAVO): initial core and example expansion. (arXiv:1710.09954v1 [cs.CY])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09954"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09954", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c8ddc09\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c8ddc09&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Visual Analytics might be defined as data mining assisted by interactive \nvisual interfaces. The field has been receiving prominent consideration by \nresearchers, developers and the industry. The literature, however, is complex \nbecause it involves multiple fields of knowledge and is considerably recent. In \nthis article we describe an initial tentative organization of the knowledge in \nthe field as an OWL ontology and a SKOS vocabulary. This effort might be useful \nin many ways that include conceptual considerations and software \nimplementations. Within the results and discussions, we expose a core and an \nexample expansion of the conceptualization, and incorporate design issues that \nenhance the expressive power of the abstraction. \n</p>"}, "author": "Renato Fabbri, Maria Cristina Ferreira de Oliveira", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097872", "id": "tag:google.com,2005:reader/item/000000032866b8aa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning for Accelerated Ultrasound Imaging. (arXiv:1710.10006v1 [cs.CV])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10006"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10006", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In portable, 3-D, or ultra-fast ultrasound (US) imaging systems, there is an \nincreasing demand to reconstruct high quality images from limited number of \ndata. However, the existing solutions require either hardware changes or \ncomputationally expansive algorithms. To overcome these limitations, here we \npropose a novel deep learning approach that interpolates the missing RF data by \nutilizing the sparsity of the RF data in the Fourier domain. Extensive \nexperimental results from sub-sampled RF data from a real US system confirmed \nthat the proposed method can effectively reduce the data rate without \nsacrificing the image quality. \n</p>"}, "author": "Yeo Hun Yoon, Jong Chul Ye", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097871", "id": "tag:google.com,2005:reader/item/000000032866b8ad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Online Learning of Power Transmission Dynamics. (arXiv:1710.10021v1 [cs.SY])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10021"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10021", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of reconstructing the dynamic state matrix of \ntransmission power grids from time-stamped PMU measurements in the regime of \nambient fluctuations. Using a maximum likelihood based approach, we construct a \nfamily of convex estimators that adapt to the structure of the problem \ndepending on the available prior information. The proposed method is fully \ndata-driven and does not assume any knowledge of system parameters. It can be \nimplemented in near real-time and requires a small amount of data. Our learning \nalgorithms can be used for model validation and calibration, and can also be \napplied to related problems of system stability, detection of forced \noscillations, generation re-dispatch, as well as to the estimation of the \nsystem state. \n</p>"}, "author": "Andrey Y. Lokhov, Marc Vuffray, Dmitry Shemetov, Deepjyoti Deka, Michael Chertkov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097870", "id": "tag:google.com,2005:reader/item/000000032866b8b5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convolutional neural networks on irregular domains through approximate translations on inferred graphs. (arXiv:1710.10035v1 [cs.DM])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10035"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10035", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a generalization of convolutional neural networks (CNNs) to \nirregular domains, through the use of an inferred graph structure. In more \ndetails, we introduce a three-step methodology to create convolutional layers \nthat are adapted to the signals to process: 1) From a training set of signals, \ninfer a graph representing the topology on which they evolve; 2) Identify \ntranslation operators in the vertex domain; 3) Emulate a convolution operator \nby translating a localized kernel on the graph. Using these layers, a \nconvolutional neural network is built, and is trained on the initial signals to \nperform a classification task. Contributions are twofold. First, we adapt a \ndefinition of translations on graphs to make them more robust to \nirregularities, and to take into account locality of the kernel. Second, we \nintroduce a procedure to build CNNs from data. We apply our methodology on a \nscrambled version of the CIFAR-10 and Haxby datasets. Without using any \nknowledge on the signals, we significantly outperform existing methods. \nMoreover, our approach extends classical CNNs on images in the sense that such \nnetworks are a particular case of our approach when the inferred graph is a \ngrid. \n</p>"}, "author": "Bastien Pasdeloup, Vincent Gripon, Jean-Charles Vialatte, Dominique Pastor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097869", "id": "tag:google.com,2005:reader/item/000000032866b8b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Generalization Tower Network: A Novel Deep Neural Network Architecture for Multi-Task Learning. (arXiv:1710.10036v1 [cs.AI])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10036"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10036", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by \nincorporating deep neural networks in learning representations from the input \nto RL. However, the conventional deep neural network architecture is limited in \nlearning representations for multi-task RL (MT-RL), as multiple tasks can refer \nto different kinds of representations. In this paper, we thus propose a novel \ndeep neural network architecture, namely generalization tower network (GTN), \nwhich can achieve MT-RL within a single learned model. Specifically, the \narchitecture of GTN is composed of both horizontal and vertical streams. In our \nGTN architecture, horizontal streams are used to learn representation shared in \nsimilar tasks. In contrast, the vertical streams are introduced to be more \nsuitable for handling diverse tasks, which encodes hierarchical shared \nknowledge of these tasks. The effectiveness of the introduced vertical stream \nis validated by experimental results. Experimental results further verify that \nour GTN architecture is able to advance the state-of-the-art MT-RL, via being \ntested on 51 Atari games. \n</p>"}, "author": "Yuhang Song, Main Xu, Songyang Zhang, Liangyu Huo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097868", "id": "tag:google.com,2005:reader/item/000000032866b8bd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Distributional Reinforcement Learning with Quantile Regression. (arXiv:1710.10044v1 [cs.AI])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10044"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10044", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In reinforcement learning an agent interacts with the environment by taking \nactions and observing the next state and reward. When sampled \nprobabilistically, these state transitions, rewards, and actions can all induce \nrandomness in the observed long-term return. Traditionally, reinforcement \nlearning algorithms average over this randomness to estimate the value \nfunction. In this paper, we build on recent work advocating a distributional \napproach to reinforcement learning in which the distribution over returns is \nmodeled explicitly instead of only estimating the mean. That is, we examine \nmethods of learning the value distribution instead of the value function. We \ngive results that close a number of gaps between the theoretical and \nalgorithmic results given by Bellemare, Dabney, and Munos (2017). First, we \nextend existing results to the approximate distribution setting. Second, we \npresent a novel distributional reinforcement learning algorithm consistent with \nour theoretical formulation. Finally, we evaluate this new algorithm on the \nAtari 2600 games, observing that it significantly outperforms many of the \nrecent improvements on DQN, including the related distributional algorithm C51. \n</p>"}, "author": "Will Dabney, Mark Rowland, Marc G. Bellemare, R&#xe9;mi Munos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097867", "id": "tag:google.com,2005:reader/item/000000032866b8c2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Group Fairness in Multiwinner Voting. (arXiv:1710.10057v1 [cs.CY])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10057"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10057", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study multiwinner voting problems when there is an additional requirement \nthat the selected committee should be fair with respect to attributes such as \ngender, ethnicity, or political parties. Every setting of an attribute gives \nrise to a group, and the goal is to ensure that each group is neither over nor \nunder represented in the selected committee. Prior work has largely focused on \ndesigning specialized score functions that lead to a precise level of \nrepresentation with respect to disjoint attributes (e.g., only political \naffiliation). Here we propose a general algorithmic framework that allows the \nuse of any score function and can guarantee flexible notions of fairness with \nrespect to multiple, non-disjoint attributes (e.g., political affiliation and \ngender). Technically, we study the complexity of this constrained multiwinner \nvoting problem subject to group-fairness constraints for monotone submodular \nscore functions. We present approximation algorithms and hardness of \napproximation results for various attribute set structures and score functions. \n</p>"}, "author": "L. Elisa Celis, Lingxiao Huang, Nisheeth K. Vishnoi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097866", "id": "tag:google.com,2005:reader/item/000000032866b8ca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Direction of arrival estimation for multiple sound sources using convolutional recurrent neural network. (arXiv:1710.10059v1 [cs.SD])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10059"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10059", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes a deep neural network for estimating the directions of \narrival (DOA) of multiple sound sources. The proposed stacked convolutional and \nrecurrent neural network (DOAnet) generates a spatial pseudo-spectrum along \nwith the DOA estimates in both azimuth and elevation. We avoid any explicit \nfeature extraction step by using the magnitude and phase of the spectrogram as \ninput to the network. The proposed DOAnet is evaluated by estimating the DOAs \nof multiple concurrently present sources in anechoic, matched and unmatched \nreverberant conditions. The results show that the proposed DOAnet is capable of \nestimating the number of sources and their respective DOAs with good precision \nand generate SPS with high signal-to-noise ratio. \n</p>"}, "author": "Sharath Adavanne, Archontis Politis, Tuomas Virtanen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097865", "id": "tag:google.com,2005:reader/item/000000032866b8d2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On modeling vagueness and uncertainty in data-to-text systems through fuzzy sets. (arXiv:1710.10093v1 [cs.AI])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10093"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10093", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Vagueness and uncertainty management is counted among one of the challenges \nthat remain unresolved in systems that generate texts from non-linguistic data, \nknown as data-to-text systems. In the last decade, work in fuzzy linguistic \nsummarization and description of data has raised the interest of using fuzzy \nsets to model and manage the imprecision of human language in data-to-text \nsystems. However, despite some research in this direction, there has not been \nan actual clear discussion and justification on how fuzzy sets can contribute \nto data-to-text for modeling vagueness and uncertainty in words and \nexpressions. This paper intends to bridge this gap by answering the following \nquestions: What does vagueness mean in fuzzy sets theory? What does vagueness \nmean in data-to-text contexts? In what ways can fuzzy sets theory contribute to \nimprove data-to-text systems? What are the challenges that researchers from \nboth disciplines need to address for a successful integration of fuzzy sets \ninto data-to-text systems? In what cases should the use of fuzzy sets be \navoided in D2T? For this, we review and discuss the state of the art of \nvagueness modeling in natural language generation and data-to-text, describe \npotential and actual usages of fuzzy sets in data-to-text contexts, and provide \nsome additional insights about the engineering of data-to-text systems that \nmake use of fuzzy set-based techniques. \n</p>"}, "author": "A. Ramos-Soto, M. Pereira-Fari&#xf1;a", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097864", "id": "tag:google.com,2005:reader/item/000000032866b8db", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An efficient SAT formulation for learning multiple criteria non-compensatory sorting rules from examples. (arXiv:1710.10098v1 [cs.AI])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10098"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10098", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The literature on Multiple Criteria Decision Analysis (MCDA) proposes several \nmethods in order to sort alternatives evaluated on several attributes into \nordered classes. Non Compensatory Sorting models (NCS) assign alternatives to \nclasses based on the way they compare to multicriteria profiles separating the \nconsecutive classes. Previous works have proposed approaches to learn the \nparameters of a NCS model based on a learning set. Exact approaches based on \nmixed integer linear programming ensures that the learning set is best \nrestored, but can only handle datasets of limited size. Heuristic approaches \ncan handle large learning sets, but do not provide any guarantee about the \ninferred model. In this paper, we propose an alternative formulation to learn a \nNCS model. This formulation, based on a SAT problem, guarantees to find a model \nfully consistent with the learning set (whenever it exists), and is \ncomputationally much more efficient than existing exact MIP approaches. \n</p>"}, "author": "K. Belahc&#xe8;ne, C. Labreuche, N. Maudet, V. Mousseau, W. Ouerdane", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097863", "id": "tag:google.com,2005:reader/item/000000032866b8ec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Inverse Reinforcement Learning Under Noisy Observations. (arXiv:1710.10116v1 [cs.RO])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10116"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10116", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c8dde32\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c8dde32&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We consider the problem of performing inverse reinforcement learning when the \ntrajectory of the expert is not perfectly observed by the learner. Instead, a \nnoisy continuous-time observation of the trajectory is provided to the learner. \nThis problem exhibits wide-ranging applications and the specific application we \nconsider here is the scenario in which the learner seeks to penetrate a \nperimeter patrolled by a robot. The learner's field of view is limited due to \nwhich it cannot observe the patroller's complete trajectory. Instead, we allow \nthe learner to listen to the expert's movement sound, which it can also use to \nestimate the expert's state and action using an observation model. We treat the \nexpert's state and action as hidden data and present an algorithm based on \nexpectation maximization and maximum entropy principle to solve the non-linear, \nnon-convex problem. Related work considers discrete-time observations and an \nobservation model that does not include actions. In contrast, our technique \ntakes expectations over both state and action of the expert, enabling learning \neven in the presence of extreme noise and broader applications. \n</p>"}, "author": "Shervin Shahryari, Prashant Doshi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097862", "id": "tag:google.com,2005:reader/item/000000032866b8f9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards a new paradigm for assistive technology at home: research challenges, design issues and performance assessment. (arXiv:1710.10164v1 [cs.AI])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10164"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10164", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c954d78\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c954d78&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Providing elderly and people with special needs, including those suffering \nfrom physical disabilities and chronic diseases, with the possibility of \nretaining their independence at best is one of the most important challenges \nour society is expected to face. Assistance models based on the home care \nparadigm are being adopted rapidly in almost all industrialized and emerging \ncountries. Such paradigms hypothesize that it is necessary to ensure that the \nso-called Activities of Daily Living are correctly and regularly performed by \nthe assisted person to increase the perception of an improved quality of life. \nThis chapter describes the computational inference engine at the core of \nArianna, a system able to understand whether an assisted person performs a \ngiven set of ADL and to motivate him/her in performing them through a \nspeech-mediated motivational dialogue, using a set of nearables to be installed \nin an apartment, plus a wearable to be worn or fit in garments. \n</p>"}, "author": "Luca Buoncompagni, Barbara Bruno, Antonella Giuni, Fulvio Mastrogiovanni, Renato Zaccaria", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097861", "id": "tag:google.com,2005:reader/item/000000032866b904", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Advanced LSTM: A Study about Better Time Dependency Modeling in Emotion Recognition. (arXiv:1710.10197v1 [cs.AI])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10197"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10197", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Long short-term memory (LSTM) is normally used in recurrent neural network \n(RNN) as basic recurrent unit. However,conventional LSTM assumes that the state \nat current time step depends on previous time step. This assumption constraints \nthe time dependency modeling capability. In this study, we propose a new \nvariation of LSTM, advanced LSTM (A-LSTM), for better temporal context \nmodeling. We employ A-LSTM in weighted pooling RNN for emotion recognition. The \nA-LSTM outperforms the conventional LSTM by 5.5% relatively. The A-LSTM based \nweighted pooling RNN can also complement the state-of-the-art emotion \nclassification framework. This shows the advantage of A-LSTM. \n</p>"}, "author": "Fei Tao, Gang Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097860", "id": "tag:google.com,2005:reader/item/000000032866b90b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "BridgeNets: Student-Teacher Transfer Learning Based on Recursive Neural Networks and its Application to Distant Speech Recognition. (arXiv:1710.10224v1 [cs.AI])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10224"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10224", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Despite the remarkable progress achieved on automatic speech recognition, \nrecognizing far-field speeches mixed with various noise sources is still a \nchallenging task. In this paper, we introduce novel student-teacher transfer \nlearning, BridgeNet which can provide a solution to improve distant speech \nrecognition. There are two key features in BridgeNet. First, BridgeNet extends \ntraditional student-teacher frameworks by providing multiple hints from a \nteacher network. Hints are not limited to the soft labels from a teacher \nnetwork. Teacher's intermediate feature representations can better guide a \nstudent network to learn how to denoise or dereverberate noisy input. Second, \nthe proposed recursive architecture in the BridgeNet can iteratively improve \ndenoising and recognition performance. The experimental results of BridgeNet \nshowed significant improvements in tackling the distant speech recognition \nproblem, where it achieved up to 13.24% relative WER reductions on AMI corpus \ncompared to a baseline neural network without teacher's hints. \n</p>"}, "author": "Jaeyoung Kim, Mostafa El-Khamy, Jungwon Lee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509066299539", "timestampUsec": "1509066299538883", "id": "tag:google.com,2005:reader/item/0000000326bd5bdd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Benefits of Depth for Long-Term Memory of Recurrent Networks. (arXiv:1710.09431v1 [cs.LG])", "published": 1509066300, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09431"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09431", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The key attribute that drives the unprecedented success of modern Recurrent \nNeural Networks (RNNs) on learning tasks which involve sequential data, is \ntheir ever-improving ability to model intricate long-term temporal \ndependencies. However, an adequate measure of RNNs long-term memory capacity is \nlacking, and thus formal understanding of their ability to correlate data \nthroughout time is limited. Though depth efficiency in convolutional networks \nis well established, it does not suffice in order to account for the success of \ndeep RNNs on data of varying lengths, and the need to address their \n`time-series expressive power' arises. In this paper, we analyze the effect of \ndepth on the ability of recurrent networks to express correlations ranging over \nlong time-scales. To meet the above need, we introduce a measure of the \ninformation flow across time supported by the network, referred to as the \nStart-End separation rank. This measure essentially reflects the distance of \nthe function realized by the recurrent network from a function that models no \ninteraction whatsoever between the beginning and end of the input sequence. We \nprove that deep recurrent networks support Start-End separation ranks which are \nexponentially higher than those supported by their shallow counterparts. Thus, \nwe establish that depth brings forth an overwhelming advantage in the ability \nof recurrent networks to model long-term dependencies. Such analyses may be \nreadily extended to other RNN architectures of interest, e.g. variants of LSTM \nnetworks. We obtain our results by considering a class of recurrent networks \nreferred to as Recurrent Arithmetic Circuits (RACs), which merge the hidden \nstate with the input via the Multiplicative Integration operation. Finally, we \nmake use of the tool of quantum Tensor Networks to gain additional graphic \ninsight regarding the complexity brought forth by depth in recurrent networks. \n</p>"}, "author": "Yoav Levine, Or Sharir, Amnon Shashua", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509066299539", "timestampUsec": "1509066299538882", "id": "tag:google.com,2005:reader/item/0000000326bd5bf1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Rotational Unit of Memory. (arXiv:1710.09537v1 [cs.LG])", "published": 1509066300, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09537"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09537", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The concepts of unitary evolution matrices and associative memory have \nboosted the field of Recurrent Neural Networks (RNN) to state-of-the-art \nperformance in a variety of sequential tasks. However, RNN still have a limited \ncapacity to manipulate long-term memory. To bypass this weakness the most \nsuccessful applications of RNN use external techniques such as attention \nmechanisms. In this paper we propose a novel RNN model that unifies the \nstate-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM \nis its rotational operation, which is, naturally, a unitary matrix, providing \narchitectures with the power to learn long-term dependencies by overcoming the \nvanishing and exploding gradients problem. Moreover, the rotational unit also \nserves as associative memory. We evaluate our model on synthetic memorization, \nquestion answering and language modeling tasks. RUM learns the Copying Memory \ntask completely and improves the state-of-the-art result in the Recall task. \nRUM's performance in the bAbI Question Answering task is comparable to that of \nmodels with attention mechanism. We also improve the state-of-the-art result to \n1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB) \ntask, which is to signify the applications of RUM to real-world sequential \ndata. The universality of our construction, at the core of RNN, establishes RUM \nas a promising approach to language modeling, speech recognition and machine \ntranslation. \n</p>"}, "author": "Rumen Dangovski, Li Jing, Marin Soljacic", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509066299539", "timestampUsec": "1509066299538881", "id": "tag:google.com,2005:reader/item/0000000326bd5bfb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Biologically Inspired Feedforward Supervised Learning for Deep Self-Organizing Map Networks. (arXiv:1710.09574v1 [stat.ML])", "published": 1509066300, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09574"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09574", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this study, we propose a novel deep neural network and its supervised \nlearning method that uses a feedforward supervisory signal. The method is \ninspired by the human visual system and performs human-like association-based \nlearning without any backward error propagation. The feedforward supervisory \nsignal that produces the correct result is preceded by the target signal and \nassociates its confirmed label with the classification result of the target \nsignal. It effectively uses a large amount of information from the feedforward \nsignal, and forms a continuous and rich learning representation. The method is \nvalidated using visual recognition tasks on the MNIST handwritten dataset. \n</p>"}, "author": "Takashi Shinozaki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509066299539", "timestampUsec": "1509066299538880", "id": "tag:google.com,2005:reader/item/0000000326bd5c04", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "PDE-Net: Learning PDEs from Data. (arXiv:1710.09668v1 [math.NA])", "published": 1509066300, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09668"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09668", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we present an initial attempt to learn evolution PDEs from \ndata. Inspired by the latest development of neural network designs in deep \nlearning, we propose a new feed-forward deep network, called PDE-Net, to \nfulfill two objectives at the same time: to accurately predict dynamics of \ncomplex systems and to uncover the underlying hidden PDE models. The basic idea \nof the proposed PDE-Net is to learn differential operators by learning \nconvolution kernels (filters), and apply neural networks or other machine \nlearning methods to approximate the unknown nonlinear responses. Comparing with \nexisting approaches, which either assume the form of the nonlinear response is \nknown or fix certain finite difference approximations of differential \noperators, our approach has the most flexibility by learning both differential \noperators and the nonlinear responses. A special feature of the proposed \nPDE-Net is that all filters are properly constrained, which enables us to \neasily identify the governing PDE models while still maintaining the expressive \nand predictive power of the network. These constrains are carefully designed by \nfully exploiting the relation between the orders of differential operators and \nthe orders of sum rules of filters (an important concept originated from \nwavelet theory). We also discuss relations of the PDE-Net with some existing \nnetworks in computer vision such as Network-In-Network (NIN) and Residual \nNeural Network (ResNet). Numerical experiments show that the PDE-Net has the \npotential to uncover the hidden PDE of the observed dynamics, and predict the \ndynamical behavior for a relatively long time, even in a noisy environment. \n</p>"}, "author": "Zichao Long, Yiping Lu, Xianzhong Ma, Bin Dong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509066299539", "timestampUsec": "1509066299538879", "id": "tag:google.com,2005:reader/item/0000000326bd5c19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the role of synaptic stochasticity in training low-precision neural networks. (arXiv:1710.09825v1 [cond-mat.dis-nn])", "published": 1509066300, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09825"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09825", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Stochasticity and limited precision of synaptic weights in neural network \nmodels is a key aspect of both biological and hardware modeling of learning \nprocesses. Here we show that a neural network model with stochastic binary \nweights naturally gives prominence to exponentially rare dense regions of \nsolutions with a number of desirable properties such as robustness and good \ngeneralization per- formance, while typical solutions are isolated and hard to \nfind. Binary solutions of the standard perceptron problem are obtained from a \nsimple gradient descent procedure on a set of real values parametrizing a \nprobability distribution over the binary synapses. Both analytical and \nnumerical results are presented. An algorithmic extension aimed at training \ndiscrete deep neural networks is also investigated. \n</p>"}, "author": "Carlo Baldassi, Federica Gerace, Hilbert J. Kappen, Carlo Lucibello, Luca Saglietti, Enzo Tartaglione, Riccardo Zecchina", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858293", "id": "tag:google.com,2005:reader/item/0000000326bb094a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "mixup: Beyond Empirical Risk Minimization. (arXiv:1710.09412v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09412"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09412", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Large deep neural networks are powerful, but exhibit undesirable behaviors \nsuch as memorization and sensitivity to adversarial examples. In this work, we \npropose mixup, a simple learning principle to alleviate these issues. In \nessence, mixup trains a neural network on convex combinations of pairs of \nexamples and their labels. By doing so, mixup regularizes the neural network to \nfavor simple linear behavior in-between training examples. Our experiments on \nthe ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show \nthat mixup improves the generalization of state-of-the-art neural network \narchitectures. We also find that mixup reduces the memorization of corrupt \nlabels, increases the robustness to adversarial examples, and stabilizes the \ntraining of generative adversarial networks. \n</p>"}, "author": "Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858292", "id": "tag:google.com,2005:reader/item/0000000326bb0974", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "DPCA: Dimensionality Reduction for Discriminative Analytics of Multiple Large-Scale Datasets. (arXiv:1710.09429v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09429"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09429", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c954fca\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c954fca&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Principal component analysis (PCA) has well-documented merits for data \nextraction and dimensionality reduction. PCA deals with a single dataset at a \ntime, and it is challenged when it comes to analyzing multiple datasets. Yet in \ncertain setups, one wishes to extract the most significant information of one \ndataset relative to other datasets. Specifically, the interest may be on \nidentifying, namely extracting features that are specific to a single target \ndataset but not the others. This paper develops a novel approach for such \nso-termed discriminative data analysis, and establishes its optimality in the \nleast-squares (LS) sense under suitable data modeling assumptions. The \ncriterion reveals linear combinations of variables by maximizing the ratio of \nthe variance of the target data to that of the remainders. The novel approach \nsolves a generalized eigenvalue problem by performing SVD just once. Numerical \ntests using synthetic and real datasets showcase the merits of the proposed \napproach relative to its competing alternatives. \n</p>"}, "author": "Gang Wang, Jia Chen, Georgios B. Giannakis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858291", "id": "tag:google.com,2005:reader/item/0000000326bb0981", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Markov Chain Theory Approach to Characterizing the Minimax Optimality of Stochastic Gradient Descent (for Least Squares). (arXiv:1710.09430v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09430"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09430", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This work provides a simplified proof of the statistical minimax optimality \nof (iterate averaged) stochastic gradient descent (SGD), for the special case \nof least squares. This result is obtained by analyzing SGD as a stochastic \nprocess and by sharply characterizing the stationary covariance matrix of this \nprocess. The finite rate optimality characterization captures the constant \nfactors and addresses model mis-specification. \n</p>"}, "author": "Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, Venkata Krishna Pillutla, Aaron Sidford", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858290", "id": "tag:google.com,2005:reader/item/0000000326bb098d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Malware Detection by Eating a Whole EXE. (arXiv:1710.09435v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09435"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09435", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work we introduce malware detection from raw byte sequences as a \nfruitful research area to the larger machine learning community. Building a \nneural network for such a problem presents a number of interesting challenges \nthat have not occurred in tasks such as image processing or NLP. In particular, \nwe note that detection from raw bytes presents a sequence problem with over two \nmillion time steps and a problem where batch normalization appear to hinder the \nlearning process. We present our initial work in building a solution to tackle \nthis problem, which has linear complexity dependence on the sequence length, \nand allows for interpretable sub-regions of the binary to be identified. In \ndoing so we will discuss the many challenges in building a neural network to \nprocess data at this scale, and the methods we used to work around them. \n</p>"}, "author": "Edward Raff, Jon Barker, Jared Sylvester, Robert Brandon, Bryan Catanzaro, Charles Nicholas", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858289", "id": "tag:google.com,2005:reader/item/0000000326bb0998", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "General Bayesian Inference over the Stiefel Manifold via the Givens Transform. (arXiv:1710.09443v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09443"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09443", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce the Givens Transform, a novel transform between the space of \northonormal matrices and $\\mathbb{R}^D$. The Givens Transform allows for the \napplication of any general Bayesian inference algorithm to probabilistic models \ncontaining constrained unit-vectors or orthonormal matrix parameters. This \nincludes a variety of matrix factorizations and dimensionality reduction models \nsuch as Probabilistic PCA (PPCA), Exponential Family PPCA (BXPCA), and \nCanonical Correlation Analysis (CCA). While previous Bayesian approaches to \nthese models relied on separate sampling update rules for constrained and \nunconstrained parameters, the Givens Transform enables the treatment of \nunit-vectors and orthonormal matrices agnostically as unconstrained parameters. \nThus any Bayesian inference algorithm can be used on these models without \nmodification. This opens the door to not just sampling algorithms, but \nVariational Inference (VI) as well. We illustrate with several examples and \nsupplied code, how the Givens Transform allows end-users to easily build \ncomplex models in their favorite Bayesian modeling framework such as Stan, \nEdward, or PyMC3, a task that was previously intractable due to technical \nconstraints. \n</p>"}, "author": "Arya A Pourzanjani, Richard M Jiang, Paul J Atzberger, Linda R Petzold", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858288", "id": "tag:google.com,2005:reader/item/0000000326bb09b1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Non-convex Optimization with Strong High Probability Second-order Convergence. (arXiv:1710.09447v1 [math.OC])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09447"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09447", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we study stochastic non-convex optimization with non-convex \nrandom functions. Recent studies on non-convex optimization revolve around \nestablishing second-order convergence, i.e., converging to a nearly \nsecond-order optimal stationary points. However, existing results on stochastic \nnon-convex optimization are limited, especially with a high probability \nsecond-order convergence. We propose a novel updating step (named NCG-S) by \nleveraging a stochastic gradient and a noisy negative curvature of a stochastic \nHessian, where the stochastic gradient and Hessian are based on a proper \nmini-batch of random functions. Building on this step, we develop two \nalgorithms and establish their high probability second-order convergence. To \nthe best of our knowledge, the proposed stochastic algorithms are the first \nwith a second-order convergence in {\\it high probability} and a time complexity \nthat is {\\it almost linear} in the problem's dimensionality. \n</p>"}, "author": "Mingrui Liu, Tianbao Yang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858287", "id": "tag:google.com,2005:reader/item/0000000326bb09b5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Representation Learning in Large Attributed Graphs. (arXiv:1710.09471v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09471"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09471", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Graphs (networks) are ubiquitous and allow us to model entities (nodes) and \nthe dependencies (edges) between them. Learning a useful feature representation \nfrom graph data lies at the heart and success of many machine learning tasks \nsuch as classification, anomaly detection, link prediction, among many others. \nMany existing techniques use random walks as a basis for learning features or \nestimating the parameters of a graph model for a downstream prediction task. \nExamples include recent node embedding methods such as DeepWalk, node2vec, as \nwell as graph-based deep learning algorithms. However, the simple random walk \nused by these methods is fundamentally tied to the identity of the node. This \nhas three main disadvantages. First, these approaches are inherently \ntransductive and do not generalize to unseen nodes and other graphs. Second, \nthey are not space-efficient as a feature vector is learned for each node which \nis impractical for large graphs. Third, most of these approaches lack support \nfor attributed graphs. \n</p> \n<p>To make these methods more generally applicable, we propose a framework based \non the notion of attributed random walk that is not tied to node identity and \nis instead based on learning a function $\\Phi : \\mathrm{\\rm \\bf x} \\rightarrow \nw$ that maps a node attribute vector $\\mathrm{\\rm \\bf x}$ to a type $w$. This \nframework serves as a basis for generalizing existing methods such as DeepWalk, \nnode2vec, and many other previous methods that leverage traditional random \nwalks. \n</p>"}, "author": "Nesreen K. Ahmed, Ryan A. Rossi, Rong Zhou, John Boaz Lee, Xiangnan Kong, Theodore L. Willke, Hoda Eldardiry", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858286", "id": "tag:google.com,2005:reader/item/0000000326bb09ba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Reparameterizing the Birkhoff Polytope for Variational Permutation Inference. (arXiv:1710.09508v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09508"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09508", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many matching, tracking, sorting, and ranking problems require probabilistic \nreasoning about possible permutations, a set that grows factorially with \ndimension. Combinatorial optimization algorithms may enable efficient point \nestimation, but fully Bayesian inference poses a severe challenge in this \nhigh-dimensional, discrete space. To surmount this challenge, we start with the \nusual step of relaxing a discrete set (here, of permutation matrices) to its \nconvex hull, which here is the Birkhoff polytope: the set of all \ndoubly-stochastic matrices. We then introduce two novel transformations: first, \nan invertible and differentiable stick-breaking procedure that maps \nunconstrained space to the Birkhoff polytope; second, a map that rounds points \ntoward the vertices of the polytope. Both transformations include a temperature \nparameter that, in the limit, concentrates the densities on permutation \nmatrices. We then exploit these transformations and reparameterization \ngradients to introduce variational inference over permutation matrices, and we \ndemonstrate its utility in a series of experiments. \n</p>"}, "author": "Scott W. Linderman, Gonzalo E. Mena, Hal Cooper, Liam Paninski, John P. Cunningham", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858285", "id": "tag:google.com,2005:reader/item/0000000326bb09c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "InterpNET: Neural Introspection for Interpretable Deep Learning. (arXiv:1710.09511v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09511"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09511", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Humans are able to explain their reasoning. On the contrary, deep neural \nnetworks are not. This paper attempts to bridge this gap by introducing a new \nway to design interpretable neural networks for classification, inspired by \nphysiological evidence of the human visual system's inner-workings. This paper \nproposes a neural network design paradigm, termed InterpNET, which can be \ncombined with any existing classification architecture to generate natural \nlanguage explanations of the classifications. The success of the module relies \non the assumption that the network's computation and reasoning is represented \nin its internal layer activations. While in principle InterpNET could be \napplied to any existing classification architecture, it is evaluated via an \nimage classification and explanation task. Experiments on a CUB bird \nclassification and explanation dataset show qualitatively and quantitatively \nthat the model is able to generate high-quality explanations. While the current \nstate-of-the-art METEOR score on this dataset is 29.2, InterpNET achieves a \nmuch higher METEOR score of 37.9. \n</p>"}, "author": "Shane Barratt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858284", "id": "tag:google.com,2005:reader/item/0000000326bb09cf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Maximum Principle Based Algorithms for Deep Learning. (arXiv:1710.09513v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09513"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09513", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The continuous dynamical system approach to deep learning is explored in \norder to devise alternative frameworks for training algorithms. Training is \nrecast as a control problem and this allows us to formulate necessary \noptimality conditions in continuous time using the Pontryagin's maximum \nprinciple (PMP). A modification of the method of successive approximations is \nthen used to solve the PMP, giving rise to an alternative training algorithm \nfor deep learning. This approach has the advantage that rigorous error \nestimates and convergence results can be established. We also show that it may \navoid some pitfalls of gradient-based methods, such as slow convergence on flat \nlandscapes near saddle points. Furthermore, we demonstrate that it obtains \nfavorable initial convergence rate per-iteration, provided Hamiltonian \nmaximization can be efficiently carried out - a step which is still in need of \nimprovement. Overall, the approach opens up new avenues to attack problems \nassociated with deep learning, such as trapping in slow manifolds and \ninapplicability of gradient-based methods for discrete trainable variables. \n</p>"}, "author": "Qianxiao Li, Long Chen, Cheng Tai, E Weinan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858283", "id": "tag:google.com,2005:reader/item/0000000326bb09d8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Laplacian Prior Variational Automatic Relevance Determination for Transmission Tomography. (arXiv:1710.09522v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09522"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09522", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In the classic sparsity-driven problems, the fundamental L-1 penalty method \nhas been shown to have good performance in reconstructing signals for a wide \nrange of problems. However this performance relies on a good choice of penalty \nweight which is often found from empirical experiments. We propose an algorithm \ncalled the Laplacian variational automatic relevance determination (Lap-VARD) \nthat takes this penalty weight as a parameter of a prior Laplace distribution. \nOptimization of this parameter using an automatic relevance determination \nframework results in a balance between the sparsity and accuracy of signal \nreconstruction. Our algorithm is implemented in a transmission tomography model \nwith sparsity constraint in wavelet domain. \n</p>"}, "author": "Jingwei Lu, David G. Politte, Joseph A. O&#x27;Sullivan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858282", "id": "tag:google.com,2005:reader/item/0000000326bb09e2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Rotational Unit of Memory. (arXiv:1710.09537v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09537"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09537", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c955279\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c955279&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The concepts of unitary evolution matrices and associative memory have \nboosted the field of Recurrent Neural Networks (RNN) to state-of-the-art \nperformance in a variety of sequential tasks. However, RNN still have a limited \ncapacity to manipulate long-term memory. To bypass this weakness the most \nsuccessful applications of RNN use external techniques such as attention \nmechanisms. In this paper we propose a novel RNN model that unifies the \nstate-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM \nis its rotational operation, which is, naturally, a unitary matrix, providing \narchitectures with the power to learn long-term dependencies by overcoming the \nvanishing and exploding gradients problem. Moreover, the rotational unit also \nserves as associative memory. We evaluate our model on synthetic memorization, \nquestion answering and language modeling tasks. RUM learns the Copying Memory \ntask completely and improves the state-of-the-art result in the Recall task. \nRUM's performance in the bAbI Question Answering task is comparable to that of \nmodels with attention mechanism. We also improve the state-of-the-art result to \n1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB) \ntask, which is to signify the applications of RUM to real-world sequential \ndata. The universality of our construction, at the core of RNN, establishes RUM \nas a promising approach to language modeling, speech recognition and machine \ntranslation. \n</p>"}, "author": "Rumen Dangovski, Li Jing, Marin Soljacic", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858281", "id": "tag:google.com,2005:reader/item/0000000326bb09e9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior. (arXiv:1710.09553v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09553"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09553", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c9c1c85\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c9c1c85&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We describe an approach to understand the peculiar and counterintuitive \ngeneralization properties of deep neural networks. The approach involves going \nbeyond worst-case theoretical capacity control frameworks that have been \npopular in machine learning in recent years to revisit old ideas in the \nstatistical mechanics of neural networks. Within this approach, we present a \nprototypical Very Simple Deep Learning (VSDL) model, whose behavior is \ncontrolled by two control parameters, one describing an effective amount of \ndata, or load, on the network (that decreases when noise is added to the \ninput), and one with an effective temperature interpretation (that increases \nwhen algorithms are early stopped). Using this model, we describe how a very \nsimple application of ideas from the statistical mechanics theory of \ngeneralization provides a strong qualitative description of recently-observed \nempirical results regarding the inability of deep neural networks not to \noverfit training data, discontinuous learning and sharp transitions in the \ngeneralization properties of learning algorithms, etc. \n</p>"}, "author": "Charles H. Martin, Michael W. Mahoney", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858280", "id": "tag:google.com,2005:reader/item/0000000326bb0a00", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Duality-free Methods for Stochastic Composition Optimization. (arXiv:1710.09554v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09554"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09554", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the composition optimization with two expected-value functions in \nthe form of $\\frac{1}{n}\\sum\\nolimits_{i = 1}^n F_i(\\frac{1}{m}\\sum\\nolimits_{j \n= 1}^m G_j(x))+R(x)$, { which formulates many important problems in statistical \nlearning and machine learning such as solving Bellman equations in \nreinforcement learning and nonlinear embedding}. Full Gradient or classical \nstochastic gradient descent based optimization algorithms are unsuitable or \ncomputationally expensive to solve this problem due to the inner expectation \n$\\frac{1}{m}\\sum\\nolimits_{j = 1}^m G_j(x)$. We propose a duality-free based \nstochastic composition method that combines variance reduction methods to \naddress the stochastic composition problem. We apply SVRG and SAGA based \nmethods to estimate the inner function, and duality-free method to estimate the \nouter function. We prove the linear convergence rate not only for the convex \ncomposition problem, but also for the case that the individual outer functions \nare non-convex while the objective function is strongly-convex. We also provide \nthe results of experiments that show the effectiveness of our proposed methods. \n</p>"}, "author": "Liu Liu, Ji Liu, Dacheng Tao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858279", "id": "tag:google.com,2005:reader/item/0000000326bb0a04", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Big Data Classification Using Augmented Decision Trees. (arXiv:1710.09567v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09567"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09567", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present an algorithm for classification tasks on big data. Experiments \nconducted as part of this study indicate that the algorithm can be as accurate \nas ensemble methods such as random forests or gradient boosted trees. Unlike \nensemble methods, the models produced by the algorithm can be easily \ninterpreted. The algorithm is based on a divide and conquer strategy and \nconsists of two steps. The first step consists of using a decision tree to \nsegment the large dataset. By construction, decision trees attempt to create \nhomogeneous class distributions in their leaf nodes. However, non-homogeneous \nleaf nodes are usually produced. The second step of the algorithm consists of \nusing a suitable classifier to determine the class labels for the \nnon-homogeneous leaf nodes. The decision tree segment provides a coarse segment \nprofile while the leaf level classifier can provide information about the \nattributes that affect the label within a segment. \n</p>"}, "author": "Rajiv Sambasivan, Sourish Das", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858278", "id": "tag:google.com,2005:reader/item/0000000326bb0a11", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Biologically Inspired Feedforward Supervised Learning for Deep Self-Organizing Map Networks. (arXiv:1710.09574v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09574"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09574", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this study, we propose a novel deep neural network and its supervised \nlearning method that uses a feedforward supervisory signal. The method is \ninspired by the human visual system and performs human-like association-based \nlearning without any backward error propagation. The feedforward supervisory \nsignal that produces the correct result is preceded by the target signal and \nassociates its confirmed label with the classification result of the target \nsignal. It effectively uses a large amount of information from the feedforward \nsignal, and forms a continuous and rich learning representation. The method is \nvalidated using visual recognition tasks on the MNIST handwritten dataset. \n</p>"}, "author": "Takashi Shinozaki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858277", "id": "tag:google.com,2005:reader/item/0000000326bb0a1a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Watch Your Step: Learning Graph Embeddings Through Attention. (arXiv:1710.09599v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09599"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09599", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Graph embedding methods represent nodes in a continuous vector space, \npreserving information from the graph (e.g. by sampling random walks). There \nare many hyper-parameters to these methods (such as random walk length) which \nhave to be manually tuned for every graph. In this paper, we replace random \nwalk hyper-parameters with trainable parameters that we automatically learn via \nbackpropagation. In particular, we learn a novel attention model on the power \nseries of the transition matrix, which guides the random walk to optimize an \nupstream objective. Unlike previous approaches to attention models, the method \nthat we propose utilizes attention parameters exclusively on the data (e.g. on \nthe random walk), and not used by the model for inference. We experiment on \nlink prediction tasks, as we aim to produce embeddings that best-preserve the \ngraph structure, generalizing to unseen information. We improve \nstate-of-the-art on a comprehensive suite of real world datasets including \nsocial, collaboration, and biological networks. Adding attention to random \nwalks can reduce the error by 20% to 45% on datasets we attempted. Further, our \nlearned attention parameters are different for every graph, and our \nautomatically-found values agree with the optimal choice of hyper-parameter if \nwe manually tune existing methods. \n</p>"}, "author": "Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, Alex Alemi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858276", "id": "tag:google.com,2005:reader/item/0000000326bb0a23", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Segment Parameter Labelling in MCMC Mean-Shift Change Detection. (arXiv:1710.09657v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09657"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09657", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This work addresses the problem of segmentation in time series data with \nrespect to a statistical parameter of interest in Bayesian models. It is common \nto assume that the parameters are distinct within each segment. As such, many \nBayesian change point detection models do not exploit the segment parameter \npatterns, which can improve performance. This work proposes a Bayesian \nmean-shift change point detection algorithm that makes use of repetition in \nsegment parameters, by introducing segment class labels that utilise a \nDirichlet process prior. The performance of the proposed approach was assessed \non both synthetic and real world data, highlighting the enhanced performance \nwhen using parameter labelling. \n</p>"}, "author": "Alireza Ahrabian, Shirin Enshaeifar, Clive Cheong-Took, Payam Barnaghi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858275", "id": "tag:google.com,2005:reader/item/0000000326bb0a28", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "PDE-Net: Learning PDEs from Data. (arXiv:1710.09668v1 [math.NA])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09668"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09668", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we present an initial attempt to learn evolution PDEs from \ndata. Inspired by the latest development of neural network designs in deep \nlearning, we propose a new feed-forward deep network, called PDE-Net, to \nfulfill two objectives at the same time: to accurately predict dynamics of \ncomplex systems and to uncover the underlying hidden PDE models. The basic idea \nof the proposed PDE-Net is to learn differential operators by learning \nconvolution kernels (filters), and apply neural networks or other machine \nlearning methods to approximate the unknown nonlinear responses. Comparing with \nexisting approaches, which either assume the form of the nonlinear response is \nknown or fix certain finite difference approximations of differential \noperators, our approach has the most flexibility by learning both differential \noperators and the nonlinear responses. A special feature of the proposed \nPDE-Net is that all filters are properly constrained, which enables us to \neasily identify the governing PDE models while still maintaining the expressive \nand predictive power of the network. These constrains are carefully designed by \nfully exploiting the relation between the orders of differential operators and \nthe orders of sum rules of filters (an important concept originated from \nwavelet theory). We also discuss relations of the PDE-Net with some existing \nnetworks in computer vision such as Network-In-Network (NIN) and Residual \nNeural Network (ResNet). Numerical experiments show that the PDE-Net has the \npotential to uncover the hidden PDE of the observed dynamics, and predict the \ndynamical behavior for a relatively long time, even in a noisy environment. \n</p>"}, "author": "Zichao Long, Yiping Lu, Xianzhong Ma, Bin Dong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858274", "id": "tag:google.com,2005:reader/item/0000000326bb0a2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Weighting Scheme for a Pairwise Multi-label Classifier Based on the Fuzzy Confusion Matrix. (arXiv:1710.09710v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09710"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09710", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work we addressed the issue of applying a stochastic classifier and a \nlocal, fuzzy confusion matrix under the framework of multi-label \nclassification. We proposed a novel solution to the problem of correcting label \npairwise ensembles. The main step of the correction procedure is to compute \nclassifier-specific competence and cross-competence measures, which estimates \nerror pattern of the underlying classifier. At the fusion phase we employed two \nweighting approaches based on information theory. The classifier weights \npromote base classifiers which are the most susceptible to the correction based \non the fuzzy confusion matrix. During the experimental study, the proposed \napproach was compared against two reference methods. The comparison was made in \nterms of six different quality criteria. The conducted experiments reveals that \nthe proposed approach eliminates one of main drawbacks of the original \nFCM-based approach i.e. the original approach is vulnerable to the imbalanced \nclass/label distribution. What is more, the obtained results shows that the \nintroduced method achieves satisfying classification quality under all \nconsidered quality criteria. Additionally, the impact of fluctuations of data \nset characteristics is reduced. \n</p>"}, "author": "Pawel Trajdos, Marek Kurzynski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858273", "id": "tag:google.com,2005:reader/item/0000000326bb0a34", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "From Distance Correlation to Multiscale Generalized Correlation. (arXiv:1710.09768v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09768"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09768", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Understanding and developing a correlation measure that can detect general \ndependencies is not only imperative to statistics and machine learning, but \nalso crucial to general scientific discovery in the big data age. We proposed \nthe Multiscale Generalized Correlation (MGC) in Shen et al. 2017 as a novel \ncorrelation measure, which worked well empirically and helped a number of real \ndata discoveries. But there is a wide gap with respect to the theoretical side, \ne.g., the population statistic, the convergence from sample to population, how \nwell does the algorithmic Sample MGC perform, etc. To better understand its \nunderlying mechanism, in this paper we formalize the population version of \nlocal distance correlations, MGC, and the optimal local scale between the \nunderlying random variables, by utilizing the characteristic functions and \nincorporating the nearest-neighbor machinery. The population version enables a \nseamless connection with, and significant improvement to, the algorithmic \nSample MGC, both theoretically and in practice, which further allows a number \nof desirable asymptotic and finite-sample properties to be proved and explored \nfor MGC. The advantages of MGC are further illustrated via a comprehensive set \nof simulations with linear, nonlinear, univariate, multivariate, and noisy \ndependencies, where it loses almost no power against monotone dependencies \nwhile achieving superior performance against general dependencies. \n</p>"}, "author": "Cencheng Shen, Carey E. Priebe, Joshua T. Vogelstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858272", "id": "tag:google.com,2005:reader/item/0000000326bb0a3a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimal Shrinkage of Singular Values Under Random Data Contamination. (arXiv:1710.09787v1 [cs.IT])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09787"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09787", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c9c1f50\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c9c1f50&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A low rank matrix X has been contaminated by uniformly distributed noise, \nmissing values, outliers and corrupt entries. Reconstruction of X from the \nsingular values and singular vectors of the contaminated matrix Y is a key \nproblem in machine learning, computer vision and data science. In this paper we \nshow that common contamination models (including arbitrary combinations of \nuniform noise,missing values, outliers and corrupt entries) can be described \nefficiently using a single framework. We develop an asymptotically optimal \nalgorithm that estimates X by manipulation of the singular values of Y , which \napplies to any of the contamination models considered. Finally, we find an \nexplicit signal-to-noise cutoff, below which estimation of X from the singular \nvalue decomposition of Y must fail, in a well-defined sense. \n</p>"}, "author": "Danny Barash, Matan Gavish", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858271", "id": "tag:google.com,2005:reader/item/0000000326bb0a43", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improving Negative Sampling for Word Representation using Self-embedded Features. (arXiv:1710.09805v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09805"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09805", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Although the word-popularity based negative sampler has shown superb \nperformance in the skip-gram model, the theoretical motivation behind \noversampling popular (non-observed) words as negative samples is still not well \nunderstood. In this paper, we start from an investigation of the gradient \nvanishing issue in the skip-gram model without a proper negative sampler. By \nperforming an insightful analysis from the stochastic gradient descent (SGD) \nlearning perspective, we demonstrate that, both theoretically and intuitively, \nnegative samples with larger inner product scores are more informative than \nthose with lower scores for the SGD learner in terms of both convergence rate \nand accuracy. Understanding this, we propose an alternative sampling algorithm \nthat dynamically selects informative negative samples during each SGD update. \nMore importantly, the proposed sampler accounts for multi-dimensional \nself-embedded features during the sampling process, which essentially makes it \nmore effective than the original popularity-based (one-dimensional) sampler. \nEmpirical experiments further verify our observations, and show that our \nfine-grained samplers gain significant improvement over the existing ones \nwithout increasing computational complexity. \n</p>"}, "author": "Long Chen, Fajie Yuan, Joemon M. Jose, Weinan Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858270", "id": "tag:google.com,2005:reader/item/0000000326bb0a47", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Joint Screening Tests for LASSO. (arXiv:1710.09809v2 [cs.LG] UPDATED)", "published": 1510319034, "updated": 1510319036, "canonical": [{"href": "http://arxiv.org/abs/1710.09809"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09809", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper focusses on \"safe\" screening techniques for the LASSO problem. \nMotivated by the need for low-complexity algorithms, we propose a new approach, \ndubbed \"joint\" screening test, allowing to screen a set of atoms by carrying \nout one single test. The approach is particularized to two different sets of \natoms, respectively expressed as sphere and dome regions. After presenting the \nmathematical derivations of the tests, we elaborate on their relative \neffectiveness and discuss the practical use of such procedures. \n</p>"}, "author": "C. Herzet, A. Dr&#xe9;meau", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858269", "id": "tag:google.com,2005:reader/item/0000000326bb0a4b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the role of synaptic stochasticity in training low-precision neural networks. (arXiv:1710.09825v1 [cond-mat.dis-nn])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09825"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09825", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Stochasticity and limited precision of synaptic weights in neural network \nmodels is a key aspect of both biological and hardware modeling of learning \nprocesses. Here we show that a neural network model with stochastic binary \nweights naturally gives prominence to exponentially rare dense regions of \nsolutions with a number of desirable properties such as robustness and good \ngeneralization per- formance, while typical solutions are isolated and hard to \nfind. Binary solutions of the standard perceptron problem are obtained from a \nsimple gradient descent procedure on a set of real values parametrizing a \nprobability distribution over the binary synapses. Both analytical and \nnumerical results are presented. An algorithmic extension aimed at training \ndiscrete deep neural networks is also investigated. \n</p>"}, "author": "Carlo Baldassi, Federica Gerace, Hilbert J. Kappen, Carlo Lucibello, Luca Saglietti, Enzo Tartaglione, Riccardo Zecchina", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064764149", "timestampUsec": "1509064764149170", "id": "tag:google.com,2005:reader/item/0000000326bafaf5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Representation Learning in Large Attributed Graphs. (arXiv:1710.09471v1 [stat.ML])", "published": 1509064764, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09471"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09471", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Graphs (networks) are ubiquitous and allow us to model entities (nodes) and \nthe dependencies (edges) between them. Learning a useful feature representation \nfrom graph data lies at the heart and success of many machine learning tasks \nsuch as classification, anomaly detection, link prediction, among many others. \nMany existing techniques use random walks as a basis for learning features or \nestimating the parameters of a graph model for a downstream prediction task. \nExamples include recent node embedding methods such as DeepWalk, node2vec, as \nwell as graph-based deep learning algorithms. However, the simple random walk \nused by these methods is fundamentally tied to the identity of the node. This \nhas three main disadvantages. First, these approaches are inherently \ntransductive and do not generalize to unseen nodes and other graphs. Second, \nthey are not space-efficient as a feature vector is learned for each node which \nis impractical for large graphs. Third, most of these approaches lack support \nfor attributed graphs. \n</p> \n<p>To make these methods more generally applicable, we propose a framework based \non the notion of attributed random walk that is not tied to node identity and \nis instead based on learning a function $\\Phi : \\mathrm{\\rm \\bf x} \\rightarrow \nw$ that maps a node attribute vector $\\mathrm{\\rm \\bf x}$ to a type $w$. This \nframework serves as a basis for generalizing existing methods such as DeepWalk, \nnode2vec, and many other previous methods that leverage traditional random \nwalks. \n</p>"}, "author": "Nesreen K. Ahmed, Ryan A. Rossi, Rong Zhou, John Boaz Lee, Xiangnan Kong, Theodore L. Willke, Hoda Eldardiry", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064764149", "timestampUsec": "1509064764149169", "id": "tag:google.com,2005:reader/item/0000000326bafafb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Context-Aware Generative Adversarial Privacy. (arXiv:1710.09549v2 [cs.LG] UPDATED)", "published": 1509437322, "updated": 1509437326, "canonical": [{"href": "http://arxiv.org/abs/1710.09549"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09549", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Preserving the utility of published datasets while simultaneously providing \nprovable privacy guarantees is a well-known challenge. On the one hand, \ncontext-free privacy solutions, such as differential privacy, provide strong \nprivacy guarantees, but often lead to a significant reduction in utility. On \nthe other hand, context-aware privacy solutions, such as information theoretic \nprivacy, achieve an improved privacy-utility tradeoff, but assume that the data \nholder has access to dataset statistics. We circumvent these limitations by \nintroducing a novel context-aware privacy framework called generative \nadversarial privacy (GAP). GAP leverages recent advancements in generative \nadversarial networks (GANs) to allow the data holder to learn privatization \nschemes from the dataset itself. Under GAP, learning the privacy mechanism is \nformulated as a constrained minimax game between two players: a privatizer that \nsanitizes the dataset in a way that limits the risk of inference attacks on the \nindividuals' private variables, and an adversary that tries to infer the \nprivate variables from the sanitized dataset. To evaluate GAP's performance, we \ninvestigate two simple (yet canonical) statistical dataset models: (a) the \nbinary data model, and (b) the binary Gaussian mixture model. For both models, \nwe derive game-theoretically optimal minimax privacy mechanisms, and show that \nthe privacy mechanisms learned from data (in a generative adversarial fashion) \nmatch the theoretically optimal ones. This demonstrates that our framework can \nbe easily applied in practice, even in the absence of dataset statistics. \n</p>"}, "author": "Chong Huang, Peter Kairouz, Xiao Chen, Lalitha Sankar, Ram Rajagopal", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064764149", "timestampUsec": "1509064764149168", "id": "tag:google.com,2005:reader/item/0000000326bafb04", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Duality-free Methods for Stochastic Composition Optimization. (arXiv:1710.09554v1 [stat.ML])", "published": 1509064764, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09554"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09554", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the composition optimization with two expected-value functions in \nthe form of $\\frac{1}{n}\\sum\\nolimits_{i = 1}^n F_i(\\frac{1}{m}\\sum\\nolimits_{j \n= 1}^m G_j(x))+R(x)$, { which formulates many important problems in statistical \nlearning and machine learning such as solving Bellman equations in \nreinforcement learning and nonlinear embedding}. Full Gradient or classical \nstochastic gradient descent based optimization algorithms are unsuitable or \ncomputationally expensive to solve this problem due to the inner expectation \n$\\frac{1}{m}\\sum\\nolimits_{j = 1}^m G_j(x)$. We propose a duality-free based \nstochastic composition method that combines variance reduction methods to \naddress the stochastic composition problem. We apply SVRG and SAGA based \nmethods to estimate the inner function, and duality-free method to estimate the \nouter function. We prove the linear convergence rate not only for the convex \ncomposition problem, but also for the case that the individual outer functions \nare non-convex while the objective function is strongly-convex. We also provide \nthe results of experiments that show the effectiveness of our proposed methods. \n</p>"}, "author": "Liu Liu, Ji Liu, Dacheng Tao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064764149", "timestampUsec": "1509064764149167", "id": "tag:google.com,2005:reader/item/0000000326bafb0b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SRE: Semantic Rules Engine For the Industrial Internet-Of-Things Gateways. (arXiv:1710.09627v1 [cs.AI])", "published": 1509064764, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09627"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09627", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Advent of the Internet-of-Things (IoT) paradigm has brought opportunities \nto solve many real-world problems. Energy management, for example, has \nattracted huge interest from academia, industries, governments and regulatory \nbodies. It involves collecting energy usage data, analyzing it, and optimizing \nthe energy consumption by applying control strategies. However, in industrial \nenvironments, performing such optimization is not trivial. The changes in \nbusiness rules, process control, and customer requirements make it much more \nchallenging. In this paper, a Semantic Rules Engine (SRE) for industrial \ngateways is presented that allows implementing dynamic and flexible rule-based \ncontrol strategies. It is simple, expressive, and allows managing rules \non-the-fly without causing any service interruption. Additionally, it can \nhandle semantic queries and provide results by inferring additional knowledge \nfrom previously defined concepts in ontologies. SRE has been validated and \ntested on different hardware platforms and in commercial products. Performance \nevaluations are also presented to validate its conformance to the customer \nrequirements. \n</p>"}, "author": "Charbel El Kaed, Imran Khan, Andre Van Den Berg, Hicham Hossayni, Christophe Saint-Marcel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064764149", "timestampUsec": "1509064764149166", "id": "tag:google.com,2005:reader/item/0000000326bafb10", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "FashionBrain Project: A Vision for Understanding Europe's Fashion Data Universe. (arXiv:1710.09788v1 [cs.AI])", "published": 1509064764, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09788"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09788", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A core business in the fashion industry is the understanding and prediction \nof customer needs and trends. Search engines and social networks are at the \nsame time a fundamental bridge and a costly middleman between the customer's \npurchase intention and the retailer. To better exploit Europe's distinctive \ncharacteristics e.g., multiple languages, fashion and cultural differences, it \nis pivotal to reduce retailers' dependence to search engines. This goal can be \nachieved by harnessing various data channels (manufacturers and distribution \nnetworks, online shops, large retailers, social media, market observers, call \ncenters, press/magazines etc.) that retailers can leverage in order to gain \nmore insight about potential buyers, and on the industry trends as a whole. \nThis can enable the creation of novel on-line shopping experiences, the \ndetection of influencers, and the prediction of upcoming fashion trends. \n</p> \n<p>In this paper, we provide an overview of the main research challenges and an \nanalysis of the most promising technological solutions that we are \ninvestigating in the FashionBrain project. \n</p>"}, "author": "Alessandro Checco, Gianluca Demartini, Alexander Loeser, Ines Arous, Mourad Khayati, Matthias Dantone, Richard Koopmanschap, Svetlin Stalinov, Martin Kersten, Ying Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064764149", "timestampUsec": "1509064764149165", "id": "tag:google.com,2005:reader/item/0000000326bafb15", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Klout Topics for Modeling Interests and Expertise of Users Across Social Networks. (arXiv:1710.09824v1 [cs.IR])", "published": 1509064764, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09824"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09824", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents Klout Topics, a lightweight ontology to describe social \nmedia users' topics of interest and expertise. Klout Topics is designed to: be \nhuman-readable and consumer-friendly; cover multiple domains of knowledge in \ndepth; and promote data extensibility via knowledge base entities. We discuss \nwhy this ontology is well-suited for text labeling and interest modeling \napplications, and how it compares to available alternatives. We show its \ncoverage against common social media interest sets, and examples of how it is \nused to model the interests of over 780M social media users on Klout.com. \nFinally, we open the ontology for external use. \n</p>"}, "author": "Sarah Ellinger, Prantik Bhattacharyya, Preeti Bhargava, Nemanja Spasojevic", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508980048582", "timestampUsec": "1508980048582302", "id": "tag:google.com,2005:reader/item/00000003260e5194", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast and Scalable Distributed Deep Convolutional Autoencoder for fMRI Big Data Analytics. (arXiv:1710.08961v1 [cs.DC])", "published": 1508980049, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08961"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08961", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3c9c22e8\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3c9c22e8&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In recent years, analyzing task-based fMRI (tfMRI) data has become an \nessential tool for understanding brain function and networks. However, due to \nthe sheer size of tfMRI data, its intrinsic complex structure, and lack of \nground truth of underlying neural activities, modeling tfMRI data is hard and \nchallenging. Previously proposed data-modeling methods including Independent \nComponent Analysis (ICA) and Sparse Dictionary Learning only provided a weakly \nestablished model based on blind source separation under the strong assumption \nthat original fMRI signals could be linearly decomposed into time series \ncomponents with corresponding spatial maps. Meanwhile, analyzing and learning a \nlarge amount of tfMRI data from a variety of subjects has been shown to be very \ndemanding but yet challenging even with technological advances in computational \nhardware. Given the Convolutional Neural Network (CNN), a robust method for \nlearning high-level abstractions from low-level data such as tfMRI time series, \nin this work we propose a fast and scalable novel framework for distributed \ndeep Convolutional Autoencoder model. This model aims to both learn the complex \nhierarchical structure of the tfMRI data and to leverage the processing power \nof multiple GPUs in a distributed fashion. To implement such a model, we have \ncreated an enhanced processing pipeline on the top of Apache Spark and \nTensorflow library, leveraging from a very large cluster of GPU machines. \nExperimental data from applying the model on the Human Connectome Project (HCP) \nshow that the proposed model is efficient and scalable toward tfMRI big data \nanalytics, thus enabling data-driven extraction of hierarchical neuroscientific \ninformation from massive fMRI big data in the future. \n</p>"}, "author": "Milad Makkie, Heng Huang, Yu Zhao, Athanasios V. Vasilakos, Tianming Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508980048582", "timestampUsec": "1508980048582301", "id": "tag:google.com,2005:reader/item/00000003260e5196", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Adversarial Deep Structured Nets for Mass Segmentation from Mammograms. (arXiv:1710.09288v1 [cs.CV])", "published": 1508980049, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09288"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09288", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3ca3bd05\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3ca3bd05&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Mass segmentation provides effective morphological features which are \nimportant for mass diagnosis. In this work, we propose a novel end-to-end \nnetwork for mammographic mass segmentation which employs a fully convolutional \nnetwork (FCN) to model a potential function, followed by a CRF to perform \nstructured learning. Because the mass distribution varies greatly with pixel \nposition, the FCN is combined with a position priori. Further, we employ \nadversarial training to eliminate over-fitting due to the small sizes of \nmammogram datasets. Multi-scale FCN is employed to improve the segmentation \nperformance. Experimental results on two public datasets, INbreast and \nDDSM-BCRP, demonstrate that our end-to-end network achieves better performance \nthan state-of-the-art approaches. \n\\footnote{https://github.com/wentaozhu/adversarial-deep-structural-networks.git} \n</p>"}, "author": "Wentao Zhu, Xiang Xiang, Trac D. Tran, Gregory D. Hager, Xiaohui Xie", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508980048582", "timestampUsec": "1508980048582300", "id": "tag:google.com,2005:reader/item/00000003260e5199", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Feature learning in feature-sample networks using multi-objective optimization. (arXiv:1710.09300v1 [cs.AI])", "published": 1508980049, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09300"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09300", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Data and knowledge representation are fundamental concepts in machine \nlearning. The quality of the representation impacts the performance of the \nlearning model directly. Feature learning transforms or enhances raw data to \nstructures that are effectively exploited by those models. In recent years, \nseveral works have been using complex networks for data representation and \nanalysis. However, no feature learning method has been proposed for such \ncategory of techniques. Here, we present an unsupervised feature learning \nmechanism that works on datasets with binary features. First, the dataset is \nmapped into a feature--sample network. Then, a multi-objective optimization \nprocess selects a set of new vertices to produce an enhanced version of the \nnetwork. The new features depend on a nonlinear function of a combination of \npreexisting features. Effectively, the process projects the input data into a \nhigher-dimensional space. To solve the optimization problem, we design two \nmetaheuristics based on the lexicographic genetic algorithm and the improved \nstrength Pareto evolutionary algorithm (SPEA2). We show that the enhanced \nnetwork contains more information and can be exploited to improve the \nperformance of machine learning methods. The advantages and disadvantages of \neach optimization strategy are discussed. \n</p>"}, "author": "Filipe Alves Neto Verri, Renato Tin&#xf3;s, Liang Zhao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978243979", "timestampUsec": "1508978243979485", "id": "tag:google.com,2005:reader/item/00000003260b690f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. (arXiv:1710.08969v1 [cs.SD])", "published": 1508978244, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08969"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08969", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper describes a novel text-to-speech (TTS) technique based on deep \nconvolutional neural networks (CNN), without any recurrent units. Recurrent \nneural network (RNN) has been a standard technique to model sequential data \nrecently, and this technique has been used in some cutting-edge neural TTS \ntechniques. However, training RNN component often requires a very powerful \ncomputer, or very long time typically several days or weeks. Recent other \nstudies, on the other hand, have shown that CNN-based sequence synthesis can be \nmuch faster than RNN-based techniques, because of high parallelizability. The \nobjective of this paper is to show an alternative neural TTS system, based only \non CNN, that can alleviate these economic costs of training. In our experiment, \nthe proposed Deep Convolutional TTS can be sufficiently trained only in a night \n(15 hours), using an ordinary gaming PC equipped with two GPUs, while the \nquality of the synthesized speech was almost acceptable. \n</p>"}, "author": "Hideyuki Tachibana, Katsuya Uenoyama, Shunsuke Aihara", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978243979", "timestampUsec": "1508978243979484", "id": "tag:google.com,2005:reader/item/00000003260b6933", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multi-Objective Approaches to Markov Decision Processes with Uncertain Transition Parameters. (arXiv:1710.08986v1 [cs.AI])", "published": 1508978244, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08986"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08986", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Markov decision processes (MDPs) are a popular model for performance analysis \nand optimization of stochastic systems. The parameters of stochastic behavior \nof MDPs are estimates from empirical observations of a system; their values are \nnot known precisely. Different types of MDPs with uncertain, imprecise or \nbounded transition rates or probabilities and rewards exist in the literature. \n</p> \n<p>Commonly, analysis of models with uncertainties amounts to searching for the \nmost robust policy which means that the goal is to generate a policy with the \ngreatest lower bound on performance (or, symmetrically, the lowest upper bound \non costs). However, hedging against an unlikely worst case may lead to losses \nin other situations. In general, one is interested in policies that behave well \nin all situations which results in a multi-objective view on decision making. \n</p> \n<p>In this paper, we consider policies for the expected discounted reward \nmeasure of MDPs with uncertain parameters. In particular, the approach is \ndefined for bounded-parameter MDPs (BMDPs) [8]. In this setting the worst, best \nand average case performances of a policy are analyzed simultaneously, which \nyields a multi-scenario multi-objective optimization problem. The paper \npresents and evaluates approaches to compute the pure Pareto optimal policies \nin the value vector space. \n</p>"}, "author": "Dimitri Scheftelowitsch, Peter Buchholz, Vahid Hashemi, Holger Hermanns", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978243979", "timestampUsec": "1508978243979483", "id": "tag:google.com,2005:reader/item/00000003260b694b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sufficient and necessary causation are dual. (arXiv:1710.09102v1 [cs.AI])", "published": 1508978244, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09102"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09102", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Causation has been the issue of philosophic debate since Hippocrates. Recent \nwork defines actual causation in terms of Pearl/Halpern's causality framework, \nformalizing necessary causes (IJCAI'15). This has inspired causality notions in \nthe security domain (CSF'15), which, perhaps surprisingly, formalize sufficient \ncauses instead. We provide an explicit relation between necessary and \nsufficient causes. \n</p>"}, "author": "Robert K&#xfc;nnemann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978243979", "timestampUsec": "1508978243979482", "id": "tag:google.com,2005:reader/item/00000003260b695d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Evidence of an exponential speed-up in the solution of hard optimization problems. (arXiv:1710.09278v1 [cs.AI])", "published": 1508978244, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09278"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09278", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Optimization problems pervade essentially every scientific discipline and \nindustry. Many such problems require finding a solution that maximizes the \nnumber of constraints satisfied. Often, these problems are particularly \ndifficult to solve because they belong to the NP-hard class, namely algorithms \nthat always find a solution in polynomial time are not known. Over the past \ndecades, research has focused on developing heuristic approaches that attempt \nto find an approximation to the solution. However, despite numerous research \nefforts, in many cases even approximations to the optimal solution are hard to \nfind, as the computational time for further refining a candidate solution grows \nexponentially with input size. Here, we show a non-combinatorial approach to \nhard optimization problems that achieves an exponential speed-up and finds \nbetter approximations than the current state-of-the-art. First, we map the \noptimization problem into a boolean circuit made of specially designed, \nself-organizing logic gates, which can be built with (non-quantum) electronic \ncomponents; the equilibrium points of the circuit represent the approximation \nto the problem at hand. Then, we solve its associated non-linear ordinary \ndifferential equations numerically, towards the equilibrium points. We \ndemonstrate this exponential gain by comparing a sequential MatLab \nimplementation of our solver with the winners of the 2016 Max-SAT competition \non a variety of hard optimization instances. We show empirical evidence that \nour solver scales linearly with the size of the problem, both in time and \nmemory, and argue that this property derives from the collective behavior of \nthe simulated physical circuit. Our approach can be applied to other types of \noptimization problems and the results presented here have far-reaching \nconsequences in many fields. \n</p>"}, "author": "Fabio L. Traversa, Pietro Cicotti, Forrest Sheldon, Massimiliano Di Ventra", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978243979", "timestampUsec": "1508978243979481", "id": "tag:google.com,2005:reader/item/00000003260b696f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Feature learning in feature-sample networks using multi-objective optimization. (arXiv:1710.09300v1 [cs.AI])", "published": 1508978244, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09300"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09300", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Data and knowledge representation are fundamental concepts in machine \nlearning. The quality of the representation impacts the performance of the \nlearning model directly. Feature learning transforms or enhances raw data to \nstructures that are effectively exploited by those models. In recent years, \nseveral works have been using complex networks for data representation and \nanalysis. However, no feature learning method has been proposed for such \ncategory of techniques. Here, we present an unsupervised feature learning \nmechanism that works on datasets with binary features. First, the dataset is \nmapped into a feature--sample network. Then, a multi-objective optimization \nprocess selects a set of new vertices to produce an enhanced version of the \nnetwork. The new features depend on a nonlinear function of a combination of \npreexisting features. Effectively, the process projects the input data into a \nhigher-dimensional space. To solve the optimization problem, we design two \nmetaheuristics based on the lexicographic genetic algorithm and the improved \nstrength Pareto evolutionary algorithm (SPEA2). We show that the enhanced \nnetwork contains more information and can be exploited to improve the \nperformance of machine learning methods. The advantages and disadvantages of \neach optimization strategy are discussed. \n</p>"}, "author": "Filipe Alves Neto Verri, Renato Tin&#xf3;s, Liang Zhao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539635", "id": "tag:google.com,2005:reader/item/00000003260b2a6e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Curvature-aided Incremental Aggregated Gradient Method. (arXiv:1710.08936v1 [stat.ML])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08936"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08936", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a new algorithm for finite sum optimization which we call the \ncurvature-aided incremental aggregated gradient (CIAG) method. Motivated by the \nproblem of training a classifier for a d-dimensional problem, where the number \nof training data is $m$ and $m \\gg d \\gg 1$, the CIAG method seeks to \naccelerate incremental aggregated gradient (IAG) methods using aids from the \ncurvature (or Hessian) information, while avoiding the evaluation of matrix \ninverses required by the incremental Newton (IN) method. Specifically, our idea \nis to exploit the incrementally aggregated Hessian matrix to trace the full \ngradient vector at every incremental step, therefore achieving an improved \nlinear convergence rate over the state-of-the-art IAG methods. For strongly \nconvex problems, the fast linear convergence rate requires the objective \nfunction to be close to quadratic, or the initial point to be close to optimal \nsolution. Importantly, we show that running one iteration of the CIAG method \nyields the same improvement to the optimality gap as running one iteration of \nthe full gradient method, while the complexity is $O(d^2)$ for CIAG and $O(md)$ \nfor the full gradient. Overall, the CIAG method strikes a balance between the \nhigh computation complexity incremental Newton-type methods and the slow IAG \nmethod. Our numerical results support the theoretical findings and show that \nthe CIAG method often converges with much fewer iterations than IAG, and \nrequires much shorter running time than IN when the problem dimension is high. \n</p>"}, "author": "Hoi-To Wai, Wei Shi, Angelia Nedic, Anna Scaglione", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539634", "id": "tag:google.com,2005:reader/item/00000003260b2a75", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Estimating the Operating Characteristics of Ensemble Methods. (arXiv:1710.08952v1 [stat.ML])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08952"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08952", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we present a technique for using the bootstrap to estimate the \noperating characteristics and their variability for certain types of ensemble \nmethods. Bootstrapping a model can require a huge amount of work if the \ntraining data set is large. Fortunately in many cases the technique lets us \ndetermine the effect of infinite resampling without actually refitting a single \nmodel. We apply the technique to the study of meta-parameter selection for \nrandom forests. We demonstrate that alternatives to bootstrap aggregation and \nto considering \\sqrt{d} features to split each node, where d is the number of \nfeatures, can produce improvements in predictive accuracy. \n</p>"}, "author": "Anthony Gamst, Jay-Calvin Reyes, Alden Walker", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539633", "id": "tag:google.com,2005:reader/item/00000003260b2a7a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast and Scalable Distributed Deep Convolutional Autoencoder for fMRI Big Data Analytics. (arXiv:1710.08961v1 [cs.DC])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08961"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08961", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3ca3c0bf\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3ca3c0bf&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In recent years, analyzing task-based fMRI (tfMRI) data has become an \nessential tool for understanding brain function and networks. However, due to \nthe sheer size of tfMRI data, its intrinsic complex structure, and lack of \nground truth of underlying neural activities, modeling tfMRI data is hard and \nchallenging. Previously proposed data-modeling methods including Independent \nComponent Analysis (ICA) and Sparse Dictionary Learning only provided a weakly \nestablished model based on blind source separation under the strong assumption \nthat original fMRI signals could be linearly decomposed into time series \ncomponents with corresponding spatial maps. Meanwhile, analyzing and learning a \nlarge amount of tfMRI data from a variety of subjects has been shown to be very \ndemanding but yet challenging even with technological advances in computational \nhardware. Given the Convolutional Neural Network (CNN), a robust method for \nlearning high-level abstractions from low-level data such as tfMRI time series, \nin this work we propose a fast and scalable novel framework for distributed \ndeep Convolutional Autoencoder model. This model aims to both learn the complex \nhierarchical structure of the tfMRI data and to leverage the processing power \nof multiple GPUs in a distributed fashion. To implement such a model, we have \ncreated an enhanced processing pipeline on the top of Apache Spark and \nTensorflow library, leveraging from a very large cluster of GPU machines. \nExperimental data from applying the model on the Human Connectome Project (HCP) \nshow that the proposed model is efficient and scalable toward tfMRI big data \nanalytics, thus enabling data-driven extraction of hierarchical neuroscientific \ninformation from massive fMRI big data in the future. \n</p>"}, "author": "Milad Makkie, Heng Huang, Yu Zhao, Athanasios V. Vasilakos, Tianming Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539632", "id": "tag:google.com,2005:reader/item/00000003260b2a98", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Scaling Text with the Class Affinity Model. (arXiv:1710.08963v1 [stat.ML])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08963"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08963", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Probabilistic methods for classifying text form a rich tradition in machine \nlearning and natural language processing. For many important problems, however, \nclass prediction is uninteresting because the class is known, and instead the \nfocus shifts to estimating latent quantities related to the text, such as \naffect or ideology. We focus on one such problem of interest, estimating the \nideological positions of 55 Irish legislators in the 1991 D\\'ail confidence \nvote. To solve the D\\'ail scaling problem and others like it, we develop a text \nmodeling framework that allows actors to take latent positions on a \"gray\" \nspectrum between \"black\" and \"white\" polar opposites. We are able to validate \nresults from this model by measuring the influences exhibited by individual \nwords, and we are able to quantify the uncertainty in the scaling estimates by \nusing a sentence-level block bootstrap. Applying our method to the D\\'ail \ndebate, we are able to scale the legislators between extreme pro-government and \npro-opposition in a way that reveals nuances in their speeches not captured by \ntheir votes or party affiliations. \n</p>"}, "author": "Patrick O. Perry, Kenneth Benoit", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539631", "id": "tag:google.com,2005:reader/item/00000003260b2aa9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Hidden Quantum Markov Models. (arXiv:1710.09016v1 [stat.ML])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09016"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09016", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Hidden Quantum Markov Models (HQMMs) can be thought of as quantum \nprobabilistic graphical models that can model sequential data. We extend \nprevious work on HQMMs with three contributions: (1) we show how classical \nhidden Markov models (HMMs) can be simulated on a quantum circuit, (2) we \nreformulate HQMMs by relaxing the constraints for modeling HMMs on quantum \ncircuits, and (3) we present a learning algorithm to estimate the parameters of \nan HQMM from data. While our algorithm requires further optimization to handle \nlarger datasets, we are able to evaluate our algorithm using several synthetic \ndatasets. We show that on HQMM generated data, our algorithm learns HQMMs with \nthe same number of hidden states and predictive accuracy as the true HQMMs, \nwhile HMMs learned with the Baum-Welch algorithm require more states to match \nthe predictive accuracy. \n</p>"}, "author": "Siddarth Srinivasan, Geoff Gordon, Byron Boots", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539630", "id": "tag:google.com,2005:reader/item/00000003260b2ac0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Trace norm regularization and faster inference for embedded speech recognition RNNs. (arXiv:1710.09026v1 [cs.LG])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09026"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09026", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose and evaluate new techniques for compressing and speeding up dense \nmatrix multiplications as found in the fully connected and recurrent layers of \nneural networks for embedded large vocabulary continuous speech recognition \n(LVCSR). For compression, we introduce and study a trace norm regularization \ntechnique for training low rank factored versions of matrix multiplications. \nCompared to standard low rank training, we show that our method more \nconsistently leads to good accuracy versus number of parameter trade-offs and \ncan be used to speed up training of large models. For speedup, we enable faster \ninference on ARM processors through new open sourced kernels optimized for \nsmall batch sizes, resulting in 3x to 7x speed ups over the widely used \ngemmlowp library. Beyond LVCSR, we expect our techniques and kernels to be more \ngenerally applicable to embedded neural networks with large fully connected or \nrecurrent layers. \n</p>"}, "author": "Markus Kliegl, Siddharth Goyal, Kexin Zhao, Kavya Srinet, Mohammad Shoeybi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539629", "id": "tag:google.com,2005:reader/item/00000003260b2ac8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Nonparametric estimation of the fragmentation kernel based on a PDE stationary distribution approximation. (arXiv:1710.09172v1 [stat.ML])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09172"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09172", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider a stochastic individual-based model in continuous time to \ndescribe a size-structured population for cell divisions. This model is \nmotivated by the detection of cellular aging in biology. We address here the \nproblem of nonparametric estimation of the kernel ruling the divisions based on \nthe eigenvalue problem related to the asymptotic behavior in large population. \nThis inverse problem involves a multiplicative deconvolution operator. Using \nFourier technics we derive a nonparametric estimator whose consistency is \nstudied. The main difficulty comes from the non-standard equations connecting \nthe Fourier transforms of the kernel and the parameters of the model. A \nnumerical study is carried out and we pay special attention to the derivation \nof bandwidths by using resampling. \n</p>"}, "author": "Van Ha Hoang (1), Thanh Mai Pham Ngoc (1), Vincent Rivoirard (2), Viet Chi Tran (3) ((1) LM-Orsay, (2) CEREMADE, (3) LPP)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539628", "id": "tag:google.com,2005:reader/item/00000003260b2ad6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Anatomical labeling of brain CT scan anomalies using multi-context nearest neighbor relation networks. (arXiv:1710.09180v1 [cs.CV])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09180"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09180", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This work is an endeavor to develop a deep learning methodology for automated \nanatomical labeling of a given region of interest (ROI) in brain computed \ntomography (CT) scans. We combine both local and global context to obtain a \nrepresentation of the ROI. We then use Relation Networks (RNs) to predict the \ncorresponding anatomy of the ROI based on its relationship score for each \nclass. Further, we propose a novel strategy employing nearest neighbors \napproach for training RNs. We train RNs to learn the relationship of the target \nROI with the joint representation of its nearest neighbors in each class \ninstead of all data-points in each class. The proposed strategy leads to better \ntraining of RNs along with increased performance as compared to training \nbaseline RN network. \n</p>"}, "author": "Srikrishna Varadarajan, Muktabh Mayank Srivastava, Monika Grewal, Pulkit Kumar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539627", "id": "tag:google.com,2005:reader/item/00000003260b2add", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Inversion using a new low-dimensional representation of complex binary geological media based on a deep neural network. (arXiv:1710.09196v1 [stat.ML])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09196"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09196", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Efficient and high-fidelity prior sampling and inversion for complex \ngeological media is still a largely unsolved challenge. Here, we use a deep \nneural network of the variational autoencoder type to construct a parametric \nlow-dimensional base model parameterization of complex binary geological media. \nFor inversion purposes, it has the attractive feature that random draws from an \nuncorrelated standard normal distribution yield model realizations with spatial \ncharacteristics that are in agreement with the training set. In comparison with \nthe most commonly used parametric representations in probabilistic inversion, \nwe find that our dimensionality reduction (DR) approach outperforms principle \ncomponent analysis (PCA), optimization-PCA (OPCA) and discrete cosine transform \n(DCT) DR techniques for unconditional geostatistical simulation of a \nchannelized prior model. For the considered examples, important compression \nratios (200 - 500) are achieved. Given that the construction of our \nparameterization requires a training set of several tens of thousands of prior \nmodel realizations, our DR approach is more suited for probabilistic (or \ndeterministic) inversion than for unconditional (or point-conditioned) \ngeostatistical simulation. Probabilistic inversions of 2D steady-state and 3D \ntransient hydraulic tomography data are used to demonstrate the DR-based \ninversion. For the 2D case study, the performance is superior compared to \ncurrent state-of-the-art multiple-point statistics inversion by sequential \ngeostatistical resampling (SGR). Inversion results for the 3D application are \nalso encouraging. \n</p>"}, "author": "Eric Laloy, Romain H&#xe9;rault, John Lee, Diederik Jacques, Niklas Linde", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539626", "id": "tag:google.com,2005:reader/item/00000003260b2ae1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Unsupervised and Semi-supervised Anomaly Detection with LSTM Neural Networks. (arXiv:1710.09207v1 [eess.SP])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09207"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09207", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We investigate anomaly detection in an unsupervised framework and introduce \nLong Short Term Memory (LSTM) neural network based algorithms. In particular, \ngiven variable length data sequences, we first pass these sequences through our \nLSTM based structure and obtain fixed length sequences. We then find a decision \nfunction for our anomaly detectors based on the One Class Support Vector \nMachines (OC-SVM) and Support Vector Data Description (SVDD) algorithms. As the \nfirst time in the literature, we jointly train and optimize the parameters of \nthe LSTM architecture and the OC-SVM (or SVDD) algorithm using highly effective \ngradient and quadratic programming based training methods. To apply the \ngradient based training method, we modify the original objective criteria of \nthe OC-SVM and SVDD algorithms, where we prove the convergence of the modified \nobjective criteria to the original criteria. We also provide extensions of our \nunsupervised formulation to the semi-supervised and fully supervised \nframeworks. Thus, we obtain anomaly detection algorithms that can process \nvariable length data sequences while providing high performance, especially for \ntime series data. Our approach is generic so that we also apply this approach \nto the Gated Recurrent Unit (GRU) architecture by directly replacing our LSTM \nbased structure with the GRU based structure. In our experiments, we illustrate \nsignificant performance gains achieved by our algorithms with respect to the \nconventional methods. \n</p>"}, "author": "Tolga Ergen, Ali Hassan Mirza, Suleyman Serdar Kozat", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539625", "id": "tag:google.com,2005:reader/item/00000003260b2ae5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "The Heterogeneous Ensembles of Standard Classification Algorithms (HESCA): the Whole is Greater than the Sum of its Parts. (arXiv:1710.09220v1 [cs.LG])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09220"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09220", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Building classification models is an intrinsically practical exercise that \nrequires many design decisions prior to deployment. We aim to provide some \nguidance in this decision making process. Specifically, given a classification \nproblem with real valued attributes, we consider which classifier or family of \nclassifiers should one use. Strong contenders are tree based homogeneous \nensembles, support vector machines or deep neural networks. All three families \nof model could claim to be state-of-the-art, and yet it is not clear when one \nis preferable to the others. Our extensive experiments with over 200 data sets \nfrom two distinct archives demonstrate that, rather than choose a single family \nand expend computing resources on optimising that model, it is significantly \nbetter to build simpler versions of classifiers from each family and ensemble. \nWe show that the Heterogeneous Ensembles of Standard Classification Algorithms \n(HESCA), which ensembles based on error estimates formed on the train data, is \nsignificantly better (in terms of error, balanced error, negative log \nlikelihood and area under the ROC curve) than its individual components, \npicking the component that is best on train data, and a support vector machine \ntuned over 1089 different parameter configurations. We demonstrate HESCA+, \nwhich contains a deep neural network, a support vector machine and two decision \ntree forests, is significantly better than its components, picking the best \ncomponent, and HESCA. We analyse the results further and find that HESCA and \nHESCA+ are of particular value when the train set size is relatively small and \nthe problem has multiple classes. HESCA is a fast approach that is, on average, \nas good as state-of-the-art classifiers, whereas HESCA+ is significantly better \nthan average and represents a strong benchmark for future research. \n</p>"}, "author": "James Large, Jason Lines, Anthony Bagnall", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539624", "id": "tag:google.com,2005:reader/item/00000003260b2af0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Supervised Classification: Quite a Brief Overview. (arXiv:1710.09230v1 [cs.LG])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09230"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09230", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The original problem of supervised classification considers the task of \nautomatically assigning objects to their respective classes on the basis of \nnumerical measurements derived from these objects. Classifiers are the tools \nthat implement the actual functional mapping from these measurements---also \ncalled features or inputs---to the so-called class label---or output. The \nfields of pattern recognition and machine learning study ways of constructing \nsuch classifiers. The main idea behind supervised methods is that of learning \nfrom examples: given a number of example input-output relations, to what extent \ncan the general mapping be learned that takes any new and unseen feature vector \nto its correct class? This chapter provides a basic introduction to the \nunderlying ideas of how to come to a supervised classification problem. In \naddition, it provides an overview of some specific classification techniques, \ndelves into the issues of object representation and classifier evaluation, and \n(very) briefly covers some variations on the basic supervised classification \ntask that may also be of interest to the practitioner. \n</p>"}, "author": "Marco Loog", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539623", "id": "tag:google.com,2005:reader/item/00000003260b2af7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Deep Neural Networks. (arXiv:1710.09302v3 [stat.ML] UPDATED)", "published": 1510139306, "updated": 1510139311, "canonical": [{"href": "http://arxiv.org/abs/1710.09302"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09302", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3ca3c4ad\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3ca3c4ad&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep Neural Networks (DNNs) are universal function approximators providing \nstate-of- the-art solutions on wide range of applications. Common perceptual \ntasks such as speech recognition, image classification, and object tracking are \nnow commonly tackled via DNNs. Some fundamental problems remain: (1) the lack \nof a mathematical framework providing an explicit and interpretable \ninput-output formula for any topology, (2) quantification of DNNs stability \nregarding adversarial examples (i.e. modified inputs fooling DNN predictions \nwhilst undetectable to humans), (3) absence of generalization guarantees and \ncontrollable behaviors for ambiguous patterns, (4) leverage unlabeled data to \napply DNNs to domains where expert labeling is scarce as in the medical field. \nAnswering those points would provide theoretical perspectives for further \ndevelopments based on a common ground. Furthermore, DNNs are now deployed in \ntremendous societal applications, pushing the need to fill this theoretical gap \nto ensure control, reliability, and interpretability. \n</p>"}, "author": "Randall Balestriero, Richard Baraniuk", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539622", "id": "tag:google.com,2005:reader/item/00000003260b2aff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks. (arXiv:1710.09363v1 [stat.ML])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09363"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09363", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3caae3d7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3caae3d7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The Fisher information metric is an important foundation of information \ngeometry, wherein it allows us to approximate the local geometry of a \nprobability distribution. Recurrent neural networks such as the \nSequence-to-Sequence (Seq2Seq) networks that have lately been used to yield \nstate-of-the-art performance on speech translation or image captioning have so \nfar ignored the geometry of the latent embedding, that they iteratively learn. \nWe propose the information geometric Seq2Seq network which abridges the gap \nbetween deep recurrent neural networks and information geometry. Specifically, \nthe latent embedding offered by a recurrent network is encoded as a Fisher \nkernel of a parametric Gaussian Mixture Model, a formalism common in computer \nvision. We utilise such a network to predict the shortest routes between two \nnodes of a graph by learning the adjacency matrix using the information \ngeometric Seq2Seq model; our results show that for such a problem the \nprobabilistic representation of the latent embedding supersedes the \nnon-probabilistic embedding by 10-15%. \n</p>"}, "author": "Alessandro Bay, Biswa Sengupta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539619", "id": "tag:google.com,2005:reader/item/00000003260b2b0a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Minimax Optimal Algorithm for Crowdsourcing. (arXiv:1606.00226v2 [stat.ML] UPDATED)", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1606.00226"}], "alternate": [{"href": "http://arxiv.org/abs/1606.00226", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of accurately estimating the reliability of workers \nbased on noisy labels they provide, which is a fundamental question in \ncrowdsourcing. We propose a novel lower bound on the minimax estimation error \nwhich applies to any estimation procedure. We further propose Triangular \nEstimation (TE), an algorithm for estimating the reliability of workers. TE has \nlow complexity, may be implemented in a streaming setting when labels are \nprovided by workers in real time, and does not rely on an iterative procedure. \nWe further prove that TE is minimax optimal and matches our lower bound. We \nconclude by assessing the performance of TE and other state-of-the-art \nalgorithms on both synthetic and real-world data sets. \n</p>"}, "author": "Thomas Bonald, Richard Combes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508893682617", "timestampUsec": "1508893682617052", "id": "tag:google.com,2005:reader/item/00000003255e1bc4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Pre-Processing-Free Gear Fault Diagnosis Using Small Datasets with Deep Convolutional Neural Network-Based Transfer Learning. (arXiv:1710.08904v1 [cs.NE])", "published": 1508893683, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08904"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08904", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Early fault diagnosis in complex mechanical systems such as gearbox has \nalways been a great challenge, even with the recent development in deep neural \nnetworks. The performance of a classic fault diagnosis system predominantly \ndepends on the features extracted and the classifier subsequently applied. \nAlthough a large number of attempts have been made regarding feature extraction \ntechniques, the methods require great human involvements are heavily depend on \ndomain expertise and may thus be non-representative and biased from application \nto application. On the other hand, while the deep neural networks based \napproaches feature adaptive feature extractions and inherent classifications, \nthey usually require a substantial set of training data and thus hinder their \nusage for engineering applications with limited training data such as gearbox \nfault diagnosis. This paper develops a deep convolutional neural network-based \ntransfer learning approach that not only entertains pre-processing free \nadaptive feature extractions, but also requires only a small set of training \ndata. The proposed approach performs gear fault diagnosis using pre-processing \nfree raw accelerometer data and experiments with various sizes of training data \nwere conducted. The superiority of the proposed approach is revealed by \ncomparing the performance with other methods such as locally trained \nconvolution neural network and angle-frequency analysis based support vector \nmachine. The achieved accuracy indicates that the proposed approach is not only \nviable and robust, but also has the potential to be readily applicable to other \nfault diagnosis practices. \n</p>"}, "author": "Pei Cao, Shengli Zhang, Jiong Tang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891849586", "timestampUsec": "1508891849586136", "id": "tag:google.com,2005:reader/item/00000003255b32a6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neural Stain-Style Transfer Learning using GAN for Histopathological Images. (arXiv:1710.08543v1 [cs.CV])", "published": 1508891850, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08543"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08543", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Performance of data-driven network for tumor classification varies with \nstain-style of histopathological images. This article proposes the stain-style \ntransfer (SST) model based on conditional generative adversarial networks \n(GANs) which is to learn not only the certain color distribution but also the \ncorresponding histopathological pattern. Our model considers feature-preserving \nloss in addition to well-known GAN loss. Consequently our model does not only \ntransfers initial stain-styles to the desired one but also prevent the \ndegradation of tumor classifier on transferred images. The model is examined \nusing the CAMELYON16 dataset. \n</p>"}, "author": "Hyungjoo Cho, Sungbin Lim, Gunho Choi, Hyunseok Min", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891849586", "timestampUsec": "1508891849586135", "id": "tag:google.com,2005:reader/item/00000003255b32b1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Max-Margin Invariant Features from Transformed Unlabeled Data. (arXiv:1710.08585v1 [cs.LG])", "published": 1508891850, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08585"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08585", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The study of representations invariant to common transformations of the data \nis important to learning. Most techniques have focused on local approximate \ninvariance implemented within expensive optimization frameworks lacking \nexplicit theoretical guarantees. In this paper, we study kernels that are \ninvariant to a unitary group while having theoretical guarantees in addressing \nthe important practical issue of unavailability of transformed versions of \nlabelled data. A problem we call the Unlabeled Transformation Problem which is \na special form of semi-supervised learning and one-shot learning. We present a \ntheoretically motivated alternate approach to the invariant kernel SVM based on \nwhich we propose Max-Margin Invariant Features (MMIF) to solve this problem. As \nan illustration, we design an framework for face recognition and demonstrate \nthe efficacy of our approach on a large scale semi-synthetic dataset with \n153,000 images and a new challenging protocol on Labelled Faces in the Wild \n(LFW) while out-performing strong baselines. \n</p>"}, "author": "Dipan K. Pal, Ashwin A. Kannan, Gautam Arakalgud, Marios Savvides", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891849586", "timestampUsec": "1508891849586134", "id": "tag:google.com,2005:reader/item/00000003255b32bd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Auto-Differentiating Linear Algebra. (arXiv:1710.08717v1 [cs.MS])", "published": 1508891850, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08717"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08717", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Development systems for deep learning, such as Theano, Torch, TensorFlow, or \nMXNet, are easy-to-use tools for creating complex neural network models. Since \ngradient computations are automatically baked in, and execution is mapped to \nhigh performance hardware, these models can be trained end-to-end on large \namounts of data. However, it is currently not easy to implement many basic \nmachine learning primitives in these systems (such as Gaussian processes, least \nsquares estimation, principal components analysis, Kalman smoothing), mainly \nbecause they lack efficient support of linear algebra primitives as \ndifferentiable operators. We detail how a number of matrix decompositions \n(Cholesky, LQ, symmetric eigen) can be implemented as differentiable operators. \nWe have implemented these primitives in MXNet, running on CPU and GPU in single \nand double precision. We sketch use cases of these new operators, learning \nGaussian process and Bayesian linear regression models. Our implementation is \nbased on BLAS/LAPACK APIs, for which highly tuned implementations are available \non all major CPUs and GPUs. \n</p>"}, "author": "Matthias Seeger, Asmus Hetzel, Zhenwen Dai, Neil D. Lawrence", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891849586", "timestampUsec": "1508891849586133", "id": "tag:google.com,2005:reader/item/00000003255b32c5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "One pixel attack for fooling deep neural networks. (arXiv:1710.08864v1 [cs.AI])", "published": 1508891850, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08864"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08864", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent research has revealed that the output of Deep neural networks(DNN) is \nnot continuous and very sensitive to tiny perturbation on the input vectors and \naccordingly several methods have been proposed for crafting effective \nperturbation against the networks. In this paper, we propose a novel method for \noptically calculating extremely small adversarial perturbation (few-pixels \nattack), based on differential evolution. It requires much less adversarial \ninformation and works with a broader classes of DNN models. The results show \nthat 73.8$\\%$ of the test images can be crafted to adversarial images with \nmodification just on one pixel with 98.7$\\%$ confidence on average. In \naddition, it is known that investigating the robustness problem of DNN can \nbring critical clues for understanding the geometrical features of the DNN \ndecision map in high dimensional input space. The results of conducting \nfew-pixels attack contribute quantitative measurements and analysis to the \ngeometrical understanding from a different perspective compared to previous \nworks. \n</p>"}, "author": "Jiawei Su, Danilo Vasconcellos Vargas, Sakurai Kouichi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891849586", "timestampUsec": "1508891849586132", "id": "tag:google.com,2005:reader/item/00000003255b32cc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Model Identification via Physics Engines for Improved Policy Search. (arXiv:1710.08893v1 [cs.RO])", "published": 1508891850, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08893"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08893", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a practical approach for identifying unknown mechanical \nparameters, such as mass and friction models of manipulated rigid objects or \nactuated robotic links, in a succinct manner that aims to improve the \nperformance of policy search algorithms. Key features of this approach are the \nuse of off-the-shelf physics engines and the adaptation of a black-box Bayesian \noptimization framework for this purpose. The physics engine is used to \nreproduce in simulation experiments that are performed on a real robot, and the \nmechanical parameters of the simulated system are automatically fine-tuned so \nthat the simulated trajectories match with the real ones. The optimized model \nis then used for learning a policy in simulation, before safely deploying it on \nthe real robot. Given the well-known limitations of physics engines in modeling \nreal-world objects, it is generally not possible to find a mechanical model \nthat reproduces in simulation the real trajectories exactly. Moreover, there \nare many scenarios where a near-optimal policy can be found without having a \nperfect knowledge of the system. Therefore, searching for a perfect model may \nnot be worth the computational effort in practice. The proposed approach aims \nthen to identify a model that is good enough to approximate the value of a \nlocally optimal policy with a certain confidence, instead of spending all the \ncomputational resources on searching for the most accurate model. Empirical \nevaluations, performed in simulation and on a real robotic manipulation task, \nshow that model identification via physics engines can significantly boost the \nperformance of policy search algorithms that are popular in robotics, such as \nTRPO, PoWER and PILCO, with no additional real-world data. \n</p>"}, "author": "Shaojun Zhu, Andrew Kimmel, Kostas E. Bekris, Abdeslam Boularias", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891849586", "timestampUsec": "1508891849586121", "id": "tag:google.com,2005:reader/item/00000003255b335d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep reinforcement learning from human preferences. (arXiv:1706.03741v3 [stat.ML] CROSS LISTED)", "published": 1508891850, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.03741"}], "alternate": [{"href": "http://arxiv.org/abs/1706.03741", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>For sophisticated reinforcement learning (RL) systems to interact usefully \nwith real-world environments, we need to communicate complex goals to these \nsystems. In this work, we explore goals defined in terms of (non-expert) human \npreferences between pairs of trajectory segments. We show that this approach \ncan effectively solve complex RL tasks without access to the reward function, \nincluding Atari games and simulated robot locomotion, while providing feedback \non less than one percent of our agent's interactions with the environment. This \nreduces the cost of human oversight far enough that it can be practically \napplied to state-of-the-art RL systems. To demonstrate the flexibility of our \napproach, we show that we can successfully train complex novel behaviors with \nabout an hour of human time. These behaviors and environments are considerably \nmore complex than any that have been previously learned from human feedback. \n</p>"}, "author": "Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882736", "id": "tag:google.com,2005:reader/item/00000003255ab3de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Many Paths to Equilibrium: GANs Do Not Need to Decrease aDivergence At Every Step. (arXiv:1710.08446v1 [stat.ML])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08446"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08446", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3caae892\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3caae892&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Generative adversarial networks (GANs) are a family of generative models that \ndo not minimize a single training criterion. Unlike other generative models, \nthe data distribution is learned via a game between a generator (the generative \nmodel) and a discriminator (a teacher providing training signal) that each \nminimize their own cost. GANs are designed to reach a Nash equilibrium at which \neach player cannot reduce their cost without changing the other players' \nparameters. One useful approach for the theory of GANs is to show that a \ndivergence between the training distribution and the model distribution obtains \nits minimum value at equilibrium. Several recent research directions have been \nmotivated by the idea that this divergence is the primary guide for the \nlearning process and that every step of learning should decrease the \ndivergence. We show that this view is overly restrictive. During GAN training, \nthe discriminator provides learning signal in situations where the gradients of \nthe divergences between distributions would not be useful. We provide empirical \ncounterexamples to the view of GAN training as divergence minimization. \nSpecifically, we demonstrate that GANs are able to learn distributions in \nsituations where the divergence minimization point of view predicts they would \nfail. We also show that gradient penalties motivated from the divergence \nminimization perspective are equally helpful when applied in other contexts in \nwhich the divergence minimization perspective does not predict they would be \nhelpful. This contributes to a growing body of evidence that GAN training may \nbe more usefully viewed as approaching Nash equilibria via trajectories that do \nnot necessarily minimize a specific divergence at each step. \n</p>"}, "author": "William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M. Dai, Shakir Mohamed, Ian Goodfellow", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882735", "id": "tag:google.com,2005:reader/item/00000003255ab401", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Interpretable Machine Learning for Privacy-Preserving IoT and Pervasive Systems. (arXiv:1710.08464v1 [stat.ML])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08464"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08464", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The presence of pervasive computing in our everyday lives and emergence of \nthe Internet of Things, such as the interaction of users with connected devices \nlike smartphones or home appliances generate increasing amounts of traces that \nreflect users' behavior. A plethora of machine learning techniques enable \nservice providers to process these traces to extract latent information about \nthe users. While most of the existing projects have focused on the accuracy of \nthese techniques, little work has been done on the interpretation of the \ninference and identification algorithms based on them. In this paper, we \npropose a machine learning interpretability framework for inference algorithms \nbased on data collected through IoT and pervasive systems and we outline the \nopen challenges in this research area. Our interpretability framework enable \nusers to understand how the traces they generate could expose their privacy, \nwhile allowing for usable and personalized services at the same time. \n</p>"}, "author": "Benjamin Baron, Mirco Musolesi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882734", "id": "tag:google.com,2005:reader/item/00000003255ab44c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Unified Framework for Long Range and Cold Start Forecasting of Seasonal Profiles in Time Series. (arXiv:1710.08473v1 [stat.ML])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08473"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08473", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Providing long-range forecasts is a fundamental challenge in time series \nmodeling, which is only compounded by the challenge of having to form such \nforecasts when a time series has never previously been observed. The latter \nchallenge is the time series version of the cold-start problem seen in \nrecommender systems which, to our knowledge, has not been directly addressed in \nprevious work. In addition, modern time series datasets are often plagued by \nmissing data. We focus on forecasting seasonal profiles---or baseline \ndemand---for periods on the order of a year long, even in the cold-start \nsetting or with otherwise missing data. Traditional time series approaches that \nperform iterated step-ahead methods struggle to provide accurate forecasts on \nsuch problems, let alone in the missing data regime. We present a \ncomputationally efficient framework which combines ideas from high-dimensional \nregression and matrix factorization on a carefully constructed data matrix. Key \nto our formulation and resulting performance is (1) leveraging repeated \npatterns over fixed periods of time and across series, and (2) metadata \nassociated with the individual series. We provide analyses of our framework on \nlarge messy real-world datasets. \n</p>"}, "author": "Christopher Xie, Alex Tank, Alec Greaves-Tunnell, Emily Fox", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882733", "id": "tag:google.com,2005:reader/item/00000003255ab49e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convolutional Neural Knowledge Graph Learning. (arXiv:1710.08502v1 [cs.LG])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08502"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08502", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Previous models for learning entity and relationship embeddings of knowledge \ngraphs such as TransE, TransH, and TransR aim to explore new links based on \nlearned representations. However, these models interpret relationships as \nsimple translations on entity embeddings. In this paper, we try to learn more \ncomplex connections between entities and relationships. In particular, we use a \nConvolutional Neural Network (CNN) to learn entity and relationship \nrepresentations in knowledge graphs. In our model, we treat entities and \nrelationships as one-dimensional numerical sequences with the same length. \nAfter that, we combine each triplet of head, relationship, and tail together as \na matrix with height 3. CNN is applied to the triplets to get confidence \nscores. Positive and manually corrupted negative triplets are used to train the \nembeddings and the CNN model simultaneously. Experimental results on public \nbenchmark datasets show that the proposed model outperforms state-of-the-art \nmodels on exploring unseen relationships, which proves that CNN is effective to \nlearn complex interactive patterns between entities and relationships. \n</p>"}, "author": "Feipeng Zhao, Martin Renqiang Min, Chen Shen, Amit Chakraborty", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882732", "id": "tag:google.com,2005:reader/item/00000003255ab4ed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Expectation Maximization Framework for Preferential Attachment Models. (arXiv:1710.08511v1 [stat.CO])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08511"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08511", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we develop an Expectation Maximization(EM) algorithm to \nestimate the parameter of a Yule-Simon distribution. The Yule-Simon \ndistribution exhibits the \"rich get richer\" effect whereby an 80-20 type of \nrule tends to dominate. These distributions are ubiquitous in industrial \nsettings. The EM algorithm presented provides both frequentist and Bayesian \nestimates of the $\\lambda$ parameter. By placing the estimation method within \nthe EM framework we are able to derive Standard errors of the resulting \nestimate. Additionally, we prove convergence of the Yule-Simon EM algorithm and \nstudy the rate of convergence. An explicit, closed form solution for the rate \nof convergence of the algorithm is given. \n</p>"}, "author": "Lucas Roberts, Denisa Roberts", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882731", "id": "tag:google.com,2005:reader/item/00000003255ab528", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stability Analysis of Optimal Adaptive Control using Value Iteration with Approximation Errors. (arXiv:1710.08530v1 [math.OC])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08530"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08530", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Adaptive optimal control using value iteration initiated from a stabilizing \ncontrol policy is theoretically analyzed in terms of stability of the system \nduring the learning stage without ignoring the effects of approximation errors. \nThis analysis includes the system operated using any single/constant resulting \ncontrol policy and also using an evolving/time-varying control policy. A \nfeature of the presented results is providing estimations of the \\textit{region \nof attraction} so that if the initial condition is within the region, the whole \ntrajectory will remain inside it and hence, the function approximation results \nremain valid. \n</p>"}, "author": "Ali Heydari", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882730", "id": "tag:google.com,2005:reader/item/00000003255ab585", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Benchmark of Deep Learning Models on Large Healthcare MIMIC Datasets. (arXiv:1710.08531v1 [cs.LG])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08531"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08531", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning models (aka Deep Neural Networks) have revolutionized many \nfields including computer vision, natural language processing, speech \nrecognition, and is being increasingly used in clinical healthcare \napplications. However, few works exist which have benchmarked the performance \nof the deep learning models with respect to the state-of-the-art machine \nlearning models and prognostic scoring systems on publicly available healthcare \ndatasets. In this paper, we present the benchmarking results for several \nclinical prediction tasks such as mortality prediction, length of stay \nprediction, and ICD-9 code group prediction using Deep Learning models, \nensemble of machine learning models (Super Learner algorithm), SAPS II and SOFA \nscores. We used the Medical Information Mart for Intensive Care III (MIMIC-III) \n(v1.4) publicly available dataset, which includes all patients admitted to an \nICU at the Beth Israel Deaconess Medical Center from 2001 to 2012, for the \nbenchmarking tasks. Our results show that deep learning models consistently \noutperform all the other approaches especially when the `raw' clinical time \nseries data is used as input features to the models. \n</p>"}, "author": "Sanjay Purushotham, Chuizheng Meng, Zhengping Che, Yan Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882729", "id": "tag:google.com,2005:reader/item/00000003255ab5c5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Display advertising: Estimating conversion probability efficiently. (arXiv:1710.08583v1 [stat.ML])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08583"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08583", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The goal of online display advertising is to entice users to \"convert\" (i.e., \ntake a pre-defined action such as making a purchase) after clicking on the ad. \nAn important measure of the value of an ad is the probability of conversion. \nThe focus of this paper is the development of a computationally efficient, \naccurate, and precise estimator of conversion probability. The challenges \nassociated with this estimation problem are the delays in observing conversions \nand the size of the data set (both number of observations and number of \npredictors). Two models have previously been considered as a basis for \nestimation: A logistic regression model and a joint model for observed \nconversion statuses and delay times. Fitting the former is simple, but ignoring \nthe delays in conversion leads to an under-estimate of conversion probability. \nOn the other hand, the latter is less biased but computationally expensive to \nfit. Our proposed estimator is a compromise between these two estimators. We \napply our results to a data set from Criteo, a commerce marketing company that \npersonalizes online display advertisements for users. \n</p>"}, "author": "Abdollah Safari, Rachel MacKay Altman, Thomas M. Loughin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882728", "id": "tag:google.com,2005:reader/item/00000003255ab5fb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Interpretable Deep Learning applied to Plant Stress Phenotyping. (arXiv:1710.08619v1 [stat.ML])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08619"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08619", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Availability of an explainable deep learning model that can be applied to \npractical real world scenarios and in turn, can consistently, rapidly and \naccurately identify specific and minute traits in applicable fields of \nbiological sciences, is scarce. Here we consider one such real world example \nviz., accurate identification, classification and quantification of biotic and \nabiotic stresses in crop research and production. Up until now, this has been \npredominantly done manually by visual inspection and require specialized \ntraining. However, such techniques are hindered by subjectivity resulting from \ninter- and intra-rater cognitive variability. Here, we demonstrate the ability \nof a machine learning framework to identify and classify a diverse set of \nfoliar stresses in the soybean plant with remarkable accuracy. We also present \nan explanation mechanism using gradient-weighted class activation mapping that \nisolates the visual symptoms used by the model to make predictions. This \nunsupervised identification of unique visual symptoms for each stress provides \na quantitative measure of stress severity, allowing for identification, \nclassification and quantification in one framework. The learnt model appears to \nbe agnostic to species and make good predictions for other (non-soybean) \nspecies, demonstrating an ability of transfer learning. \n</p>"}, "author": "Sambuddha Ghosal, David Blystone, Asheesh K. Singh, Baskar Ganapathysubramanian, Arti Singh, Soumik Sarkar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882727", "id": "tag:google.com,2005:reader/item/00000003255ab65c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improving Accuracy of Nonparametric Transfer Learning via Vector Segmentation. (arXiv:1710.08637v1 [cs.LG])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08637"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08637", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Transfer learning using deep neural networks as feature extractors has become \nincreasingly popular over the past few years. It allows to obtain \nstate-of-the-art accuracy on datasets too small to train a deep neural network \non its own, and it provides cutting edge descriptors that, combined with \nnonparametric learning methods, allow rapid and flexible deployment of \nperforming solutions in computationally restricted settings. In this paper, we \nare interested in showing that the features extracted using deep neural \nnetworks have specific properties which can be used to improve accuracy of \ndownstream nonparametric learning methods. Namely, we demonstrate that for some \ndistributions where information is embedded in a few coordinates, segmenting \nfeature vectors can lead to better accuracy. We show how this model can be \napplied to real datasets by performing experiments using three mainstream deep \nneural network feature extractors and four databases, in vision and audio. \n</p>"}, "author": "Vincent Gripon, Ghouthi B. Hacene, Matthias L&#xf6;we, Franck Vermet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882726", "id": "tag:google.com,2005:reader/item/00000003255ab691", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Auto-Differentiating Linear Algebra. (arXiv:1710.08717v1 [cs.MS])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08717"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08717", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3caaeea5\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3caaeea5&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Development systems for deep learning, such as Theano, Torch, TensorFlow, or \nMXNet, are easy-to-use tools for creating complex neural network models. Since \ngradient computations are automatically baked in, and execution is mapped to \nhigh performance hardware, these models can be trained end-to-end on large \namounts of data. However, it is currently not easy to implement many basic \nmachine learning primitives in these systems (such as Gaussian processes, least \nsquares estimation, principal components analysis, Kalman smoothing), mainly \nbecause they lack efficient support of linear algebra primitives as \ndifferentiable operators. We detail how a number of matrix decompositions \n(Cholesky, LQ, symmetric eigen) can be implemented as differentiable operators. \nWe have implemented these primitives in MXNet, running on CPU and GPU in single \nand double precision. We sketch use cases of these new operators, learning \nGaussian process and Bayesian linear regression models. Our implementation is \nbased on BLAS/LAPACK APIs, for which highly tuned implementations are available \non all major CPUs and GPUs. \n</p>"}, "author": "Matthias Seeger, Asmus Hetzel, Zhenwen Dai, Neil D. Lawrence", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882725", "id": "tag:google.com,2005:reader/item/00000003255ab6d2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Greater data science at baccalaureate institutions. (arXiv:1710.08728v1 [stat.OT])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08728"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08728", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cb23037\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cb23037&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Donoho's JCGS (in press) paper is a spirited call to action for \nstatisticians, who he points out are losing ground in the field of data science \nby refusing to accept that data science is its own domain. (Or, at least, a \ndomain that is becoming distinctly defined.) He calls on writings by John \nTukey, Bill Cleveland, and Leo Breiman, among others, to remind us that \nstatisticians have been dealing with data science for years, and encourages \nacceptance of the direction of the field while also ensuring that statistics is \ntightly integrated. \n</p> \n<p>As faculty at baccalaureate institutions (where the growth of undergraduate \nstatistics programs has been dramatic), we are keen to ensure statistics has a \nplace in data science and data science education. In his paper, Donoho is \nprimarily focused on graduate education. At our undergraduate institutions, we \nare considering many of the same questions. \n</p>"}, "author": "Amelia McNamara, Nicholas J. Horton, Benjamin S. Baumer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882724", "id": "tag:google.com,2005:reader/item/00000003255ab703", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Correction Method of a Binary Classifier Applied to Multi-label Pairwise Models. (arXiv:1710.08729v1 [cs.LG])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08729"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08729", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work, we addressed the issue of applying a stochastic classifier and \na local, fuzzy confusion matrix under the framework of multi-label \nclassification. We proposed a novel solution to the problem of correcting label \npairwise ensembles. The main step of the correction procedure is to compute \nclassifier- specific competence and cross-competence measures, which estimates \nerror pattern of the underlying classifier. We considered two improvements of \nthe method of obtaining confusion matrices. The first one is aimed to deal with \nimbalanced labels. The other utilizes double labelled instances which are \nusually removed during the pairwise transformation. The proposed methods were \nevaluated using 29 benchmark datasets. In order to assess the efficiency of the \nintroduced models, they were compared against 1 state-of-the-art approach and \nthe correction scheme based on the original method of confusion matrix \nestimation. The comparison was performed using four different multi-label \nevaluation measures: macro and micro-averaged F1 loss, zero-one loss and \nHamming loss. Additionally, we investigated relations between classification \nquality, which is expressed in terms of different quality criteria, and \ncharacteristics of multi-label datasets such as average imbalance ratio or \nlabel density. The experimental study reveals that the correction approaches \nsignificantly outperforms the reference method only in terms of zero-one loss. \n</p>"}, "author": "Pawel Trajdos, Marek Kurzynski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882723", "id": "tag:google.com,2005:reader/item/00000003255ab770", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Markov Properties for Graphical Models with Cycles and Latent Variables. (arXiv:1710.08775v1 [math.ST])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08775"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08775", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We investigate probabilistic graphical models that allow for both cycles and \nlatent variables. For this we introduce directed graphs with hyperedges \n(HEDGes), generalizing and combining both marginalized directed acyclic graphs \n(mDAGs) that can model latent (dependent) variables, and directed mixed graphs \n(DMGs) that can model cycles. We define and analyse several different Markov \nproperties that relate the graphical structure of a HEDG with a probability \ndistribution on a corresponding product space over the set of nodes, for \nexample factorization properties, structural equations properties, \nordered/local/global Markov properties, and marginal versions of these. The \nvarious Markov properties for HEDGes are in general not equivalent to each \nother when cycles or hyperedges are present, in contrast with the simpler case \nof directed acyclic graphical (DAG) models (also known as Bayesian networks). \nWe show how the Markov properties for HEDGes - and thus the corresponding \ngraphical Markov models - are logically related to each other. \n</p>"}, "author": "Patrick Forr&#xe9;, Joris M. Mooij", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882722", "id": "tag:google.com,2005:reader/item/00000003255ab79b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Algorithmic infeasibility of community detection in higher-order networks. (arXiv:1710.08816v1 [cs.SI])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08816"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08816", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In principle, higher-order networks that have multiple edge types are more \ninformative than their lower-order counterparts. In practice, however, \nexcessively rich information may be algorithmically infeasible to extract. It \nrequires an algorithm that assumes a high-dimensional model and such an \nalgorithm may perform poorly or be extremely sensitive to the initial estimate \nof the model parameters. Herein, we address this problem of community detection \nthrough a detectability analysis. We focus on the expectation-maximization (EM) \nalgorithm with belief propagation (BP), and analytically derive its algorithmic \ndetectability threshold, i.e., the limit of the modular structure strength \nbelow which the algorithm can no longer detect any modular structures. The \nresults indicate the existence of a phase in which the community detection of a \nlower-order network outperforms its higher-order counterpart. \n</p>"}, "author": "Tatsuro Kawamoto", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882721", "id": "tag:google.com,2005:reader/item/00000003255ab7c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Algorithmic detectability threshold of the stochastic blockmodel. (arXiv:1710.08841v1 [cs.SI])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08841"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08841", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The assumption that the values of model parameters are known or correctly \nlearned, i.e., the Nishimori condition, is one of the requirements for the \ndetectability analysis of the stochastic blockmodel in statistical inference. \nIn practice, however, there is no example demonstrating that we can know the \nmodel parameters beforehand, and there is no guarantee that the model \nparameters can be learned accurately. In this study, we consider the \nexpectation-maximization (EM) algorithm with belief propagation (BP) and derive \nits algorithmic detectability threshold. Our analysis is not restricted to the \ncommunity structure, but includes general modular structures. Because the \nalgorithm cannot always learn the planted model parameters correctly, the \nalgorithmic detectability threshold is qualitatively different from the one \nwith the Nishimori condition. \n</p>"}, "author": "Tatsuro Kawamoto", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882720", "id": "tag:google.com,2005:reader/item/00000003255ab856", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Bayesian Method for Joint Clustering of Vectorial Data and Network Data. (arXiv:1710.08846v1 [stat.ML])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08846"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08846", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a new model-based integrative method for clustering objects given \nboth vectorial data, which describes the feature of each object, and network \ndata, which indicates the similarity of connected objects. The proposed general \nmodel is able to cluster the two types of data simultaneously within one \nintegrative probabilistic model, while traditional methods can only handle one \ndata type or depend on transforming one data type to another. Bayesian \ninference of the clustering is conducted based on a Markov chain Monte Carlo \nalgorithm. A special case of the general model combining the Gaussian mixture \nmodel and the stochastic block model is extensively studied. We used both \nsynthetic data and real data to evaluate this new method and compare it with \nalternative methods. The results show that our simultaneous clustering method \nperforms much better. This improvement is due to the power of the model-based \nprobabilistic approach for efficiently integrating information. \n</p>"}, "author": "Yunchuan Kong, Xiaodan Fan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882719", "id": "tag:google.com,2005:reader/item/00000003255ab8c0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Robust Photometric Stereo via Dictionary Learning. (arXiv:1710.08873v1 [cs.CV])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08873"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08873", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Photometric stereo is a method that seeks to reconstruct the normal vectors \nof an object from a set of images of the object illuminated under different \nlight sources. While effective in some situations, classical photometric stereo \nrelies on a diffuses surface model that cannot handle objects with complex \nreflectance patterns, and it is sensitive to non-idealities in the images. In \nthis work, we propose a novel approach to photometric stereo that relies on \ndictionary learning to produce robust normal vector reconstructions. \nSpecifically, we develop three formulations for applying dictionary learning to \nphotometric stereo. We propose a preprocessing step that utilizes dictionary \nlearning to denoise the images. We also present a model that applies dictionary \nlearning to regularize and reconstruct the normal vectors from the images under \nthe classic Lambertian reflectance model. Finally, we generalize the latter \nmodel to explicitly model non-Lambertian objects. We investigate all three \napproaches through extensive experimentation on synthetic and real benchmark \ndatasets and observe state-of-the-art performance compared to existing robust \nphotometric stereo methods. \n</p>"}, "author": "Andrew J. Wagenmaker, Brian E. Moore, Raj Rao Nadakuditi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882718", "id": "tag:google.com,2005:reader/item/00000003255abafb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Classification on Large Networks: A Quantitative Bound via Motifs and Graphons. (arXiv:1710.08878v1 [cs.LG])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08878"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08878", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>When each data point is a large graph, graph statistics such as densities of \ncertain subgraphs (motifs) can be used as feature vectors for machine learning. \nWhile intuitive, motif counts are expensive to compute and difficult to work \nwith theoretically. Via graphon theory, we give an explicit quantitative bound \nfor the ability of motif homomorphisms to distinguish large networks under both \ngenerative and sampling noise. Furthermore, we give similar bounds for the \ngraph spectrum and connect it to homomorphism densities of cycles. This results \nin an easily computable classifier on graph data with theoretical performance \nguarantee. Our method yields competitive results on classification tasks for \nthe autoimmune disease Lupus Erythematosus. \n</p>"}, "author": "Andreas Haupt, Mohammad Khatami, Thomas Schultz, Ngoc Mai Tran", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882717", "id": "tag:google.com,2005:reader/item/00000003255abd19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Memristor-Based Optimization Framework for AI Applications. (arXiv:1710.08882v1 [cs.ET])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08882"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08882", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Memristors have recently received significant attention as ubiquitous \ndevice-level components for building a novel generation of computing systems. \nThese devices have many promising features, such as non-volatility, low power \nconsumption, high density, and excellent scalability. The ability to control \nand modify biasing voltages at the two terminals of memristors make them \npromising candidates to perform matrix-vector multiplications and solve systems \nof linear equations. In this article, we discuss how networks of memristors \narranged in crossbar arrays can be used for efficiently solving optimization \nand machine learning problems. We introduce a new memristor-based optimization \nframework that combines the computational merit of memristor crossbars with the \nadvantages of an operator splitting method, alternating direction method of \nmultipliers (ADMM). Here, ADMM helps in splitting a complex optimization \nproblem into subproblems that involve the solution of systems of linear \nequations. The capability of this framework is shown by applying it to linear \nprogramming, quadratic programming, and sparse optimization. In addition to \nADMM, implementation of a customized power iteration (PI) method for \neigenvalue/eigenvector computation using memristor crossbars is discussed. The \nmemristor-based PI method can further be applied to principal component \nanalysis (PCA). The use of memristor crossbars yields a significant speed-up in \ncomputation, and thus, we believe, has the potential to advance optimization \nand machine learning research in artificial intelligence (AI). \n</p>"}, "author": "Sijia Liu, Yanzhi Wang, Makan Fardad, Pramod K. Varshney", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882716", "id": "tag:google.com,2005:reader/item/00000003255abd59", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Conformal predictive distributions with kernels. (arXiv:1710.08894v1 [cs.LG])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08894"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08894", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cb23513\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cb23513&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper reviews the checkered history of predictive distributions in \nstatistics and discusses two developments, one from recent literature and the \nother new. The first development is bringing predictive distributions into \nmachine learning, whose early development was so deeply influenced by two \nremarkable groups at the Institute of Automation and Remote Control. The second \ndevelopment is combining predictive distributions with kernel methods, which \nwere originated by one of those groups, including Emmanuel Braverman. \n</p>"}, "author": "Vladimir Vovk, Ilia Nouretdinov, Valery Manokhin, Alex Gammerman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882715", "id": "tag:google.com,2005:reader/item/00000003255abd8e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Calibration of Machine Learning Classifiers for Probability of Default Modelling. (arXiv:1710.08901v1 [econ.EM])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08901"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08901", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Binary classification is highly used in credit scoring in the estimation of \nprobability of default. The validation of such predictive models is based both \non rank ability, and also on calibration (i.e. how accurately the probabilities \noutput by the model map to the observed probabilities). In this study we cover \nthe current best practices regarding calibration for binary classification, and \nexplore how different approaches yield different results on real world credit \nscoring data. The limitations of evaluating credit scoring models using only \nrank ability metrics are explored. A benchmark is run on 18 real world \ndatasets, and results compared. The calibration techniques used are Platt \nScaling and Isotonic Regression. Also, different machine learning models are \nused: Logistic Regression, Random Forest Classifiers, and Gradient Boosting \nClassifiers. Results show that when the dataset is treated as a time series, \nthe use of re-calibration with Isotonic Regression is able to improve the long \nterm calibration better than the alternative methods. Using re-calibration, the \nnon-parametric models are able to outperform the Logistic Regression on Brier \nScore Loss. \n</p>"}, "author": "Pedro G. Fonseca, Hugo D. Lopes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890291", "id": "tag:google.com,2005:reader/item/00000003253733dc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Voice 3: 2000-Speaker Neural Text-to-Speech. (arXiv:1710.07654v1 [cs.SD])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07654"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07654", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present Deep Voice 3, a fully-convolutional attention-based neural \ntext-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural \nspeech synthesis systems in naturalness while training ten times faster. We \nscale Deep Voice 3 to data set sizes unprecedented for TTS, training on more \nthan eight hundred hours of audio from over two thousand speakers. In addition, \nwe identify common error modes of attention-based speech synthesis networks, \ndemonstrate how to mitigate them, and compare several different waveform \nsynthesis methods. We also describe how to scale inference to ten million \nqueries per day on one single-GPU server. \n</p>"}, "author": "Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman, John Miller", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890290", "id": "tag:google.com,2005:reader/item/00000003253734f1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Point Neurons with Conductance-Based Synapses in the Neural Engineering Framework. (arXiv:1710.07659v1 [q-bio.NC])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07659"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07659", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The mathematical model underlying the Neural Engineering Framework (NEF) \nexpresses neuronal input as a linear combination of synaptic currents. However, \nin biology, synapses are not perfect current sources and are thus nonlinear. \nDetailed synapse models are based on channel conductances instead of currents, \nwhich require independent handling of excitatory and inhibitory synapses. This, \nin particular, significantly affects the influence of inhibitory signals on the \nneuronal dynamics. In this technical report we first summarize the relevant \nportions of the NEF and conductance-based synapse models. We then discuss a \nna\\\"ive translation between populations of LIF neurons with current- and \nconductance-based synapses based on an estimation of an average membrane \npotential. Experiments show that this simple approach works relatively well for \nfeed-forward communication channels, yet performance degrades for NEF networks \ndescribing more complex dynamics, such as integration. \n</p>"}, "author": "Andreas St&#xf6;ckel, Aaron R. Voelker, Chris Eliasmith", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890289", "id": "tag:google.com,2005:reader/item/000000032537359d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Low Precision RNNs: Quantizing RNNs Without Losing Accuracy. (arXiv:1710.07706v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07706"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07706", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Similar to convolution neural networks, recurrent neural networks (RNNs) \ntypically suffer from over-parameterization. Quantizing bit-widths of weights \nand activations results in runtime efficiency on hardware, yet it often comes \nat the cost of reduced accuracy. This paper proposes a quantization approach \nthat increases model size with bit-width reduction. This approach will allow \nnetworks to perform at their baseline accuracy while still maintaining the \nbenefits of reduced precision and overall model size reduction. \n</p>"}, "author": "Supriya Kapur, Asit Mishra, Debbie Marr", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890288", "id": "tag:google.com,2005:reader/item/0000000325373677", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Solving the \"false positives\" problem in fraud prediction. (arXiv:1710.07709v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07709"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07709", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we present an automated feature engineering based approach to \ndramatically reduce false positives in fraud prediction. False positives plague \nthe fraud prediction industry. It is estimated that only 1 in 5 declared as \nfraud are actually fraud and roughly 1 in every 6 customers have had a valid \ntransaction declined in the past year. To address this problem, we use the Deep \nFeature Synthesis algorithm to automatically derive behavioral features based \non the historical data of the card associated with a transaction. We generate \n237 features (&gt;100 behavioral patterns) for each transaction, and use a random \nforest to learn a classifier. We tested our machine learning model on data from \na large multinational bank and compared it to their existing solution. On an \nunseen data of 1.852 million transactions, we were able to reduce the false \npositives by 54% and provide a savings of 190K euros. We also assess how to \ndeploy this solution, and whether it necessitates streaming computation for \nreal time scoring. We found that our solution can maintain similar benefits \neven when historical features are computed once every 7 days. \n</p>"}, "author": "Roy Wedge, James Max Kanter, Santiago Moral Rubio, Sergio Iglesias Perez, Kalyan Veeramachaneni", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890287", "id": "tag:google.com,2005:reader/item/00000003253736dc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Adversarially Optimizing Intersection over Union for Object Localization Tasks. (arXiv:1710.07735v1 [cs.CV])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07735"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07735", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>An implicit uncertainty exists in the annotations of computer visions \ndatasets due to annotator disagreement and the high-dimensional space that \nannotations must be selected from.Rather than attempting to remove all \nannotation uncertainty,which we view as hopeless, or ignoring it, which can be \ndetrimental, we choose to embrace uncertainty in the design of our learning \napproach. Specifically, we address uncertainty adversarially by approximating \nprovided datasets annotations within a game-theoretic formulation of prediction \ntasks. The adversarial approximator is constrained to resemble the training \ndata annotations according to a set of specified features.This induces a \nlearned feature-based potential function that we then apply to new test cases. \nWe demonstrate the efficiency and predictive performance of our approach on the \nILSVRC2012 image dataset, showing significant improvements over existing \nmethods. \n</p>"}, "author": "Sima Behpour, Kris M. Kitani, Brian D. Ziebart", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890286", "id": "tag:google.com,2005:reader/item/0000000325373729", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Novel Stochastic Stratified Average Gradient Method: Convergence Rate and Its Complexity. (arXiv:1710.07783v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07783"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07783", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>SGD (Stochastic Gradient Descent) is a popular algorithm for large scale \noptimization problems due to its low iterative cost. However, SGD can not \nachieve linear convergence rate as FGD (Full Gradient Descent) because of the \ninherent gradient variance. To attack the problem, mini-batch SGD was proposed \nto get a trade-off in terms of convergence rate and iteration cost. In this \npaper, a general CVI (Convergence-Variance Inequality) equation is presented to \nstate formally the interaction of convergence rate and gradient variance. Then \na novel algorithm named SSAG (Stochastic Stratified Average Gradient) is \nintroduced to reduce gradient variance based on two techniques, stratified \nsampling and averaging over iterations that is a key idea in SAG (Stochastic \nAverage Gradient). Furthermore, SSAG can achieve linear convergence rate of \n$\\mathcal {O}((1-\\frac{\\mu}{8CL})^k)$ at smaller storage and iterative costs, \nwhere $C\\geq 2$ is the category number of training data. This convergence rate \ndepends mainly on the variance between classes, but not on the variance within \nthe classes. In the case of $C\\ll N$ ($N$ is the training data size), SSAG's \nconvergence rate is much better than SAG's convergence rate of $\\mathcal \n{O}((1-\\frac{\\mu}{8NL})^k)$. Our experimental results show SSAG outperforms SAG \nand many other algorithms. \n</p>"}, "author": "Aixiang Chen, Bingchuan Chen, Xiaolong Chai, Rui Bian, Hengguang Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890285", "id": "tag:google.com,2005:reader/item/00000003253737a0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Learning-to-Infer Method for Real-Time Power Grid Topology Identification. (arXiv:1710.07818v1 [cs.LG])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07818"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07818", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Identifying arbitrary topologies of power networks in real time is a \ncomputationally hard problem due to the number of hypotheses that grows \nexponentially with the network size. A new \"Learning-to-Infer\" variational \ninference method is developed for efficient inference of every line status in \nthe network. Optimizing the variational model is transformed to and solved as a \ndiscriminative learning problem based on Monte Carlo samples generated with \npower flow simulations. A major advantage of the developed Learning-to-Infer \nmethod is that the labeled data used for training can be generated in an \narbitrarily large amount fast and at very little cost. As a result, the power \nof offline training is fully exploited to learn very complex classifiers for \neffective real-time topology identification. The proposed methods are evaluated \nin the IEEE 30, 118 and 300 bus systems. Excellent performance in identifying \narbitrary power network topologies in real time is achieved even with \nrelatively simple variational models and a reasonably small amount of data. \n</p>"}, "author": "Yue Zhao, Jianshu Chen, H. Vincent Poor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890284", "id": "tag:google.com,2005:reader/item/000000032537380a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Incomplete Dot Products for Dynamic Computation Scaling in Neural Network Inference. (arXiv:1710.07830v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07830"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07830", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose the use of incomplete dot products (IDP) to dynamically adjust the \nnumber of input channels used in each layer of a convolutional neural network \nduring feedforward inference. IDP adds monotonically non-increasing \ncoefficients, referred to as a \"profile\", to the channels during training. The \nprofile orders the contribution of each channel in non-increasing order. At \ninference time, the number of channels used can be dynamically adjusted to \ntrade off accuracy for lowered power consumption and reduced latency by \nselecting only a beginning subset of channels. This approach allows for a \nsingle network to dynamically scale over a computation range, as opposed to \ntraining and deploying multiple networks to support different levels of \ncomputation scaling. Additionally, we extend the notion to multiple profiles, \neach optimized for some specific range of computation scaling. We present \nexperiments on the computation and accuracy trade-offs of IDP for popular image \nclassification models and datasets. We demonstrate that, for MNIST and \nCIFAR-10, IDP reduces computation significantly, e.g., by 75%, without \nsignificantly compromising accuracy. We argue that IDP provides a convenient \nand effective means for devices to lower computation costs dynamically to \nreflect the current computation budget of the system. For example, VGG-16 with \n50% IDP (using only the first 50% of channels) achieves 70% in accuracy on the \nCIFAR-10 dataset compared to the standard network which achieves only 35% \naccuracy when using the reduced channel set. \n</p>"}, "author": "Bradley McDanel, Surat Teerapittayanon, H.T. Kung", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890283", "id": "tag:google.com,2005:reader/item/0000000325373865", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Neural Network Approximation using Tensor Sketching. (arXiv:1710.07850v1 [stat.ML])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07850"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07850", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cb23960\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cb23960&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep neural networks are powerful learning models that achieve \nstate-of-the-art performance on many computer vision, speech, and language \nprocessing tasks. In this paper, we study a fundamental question that arises \nwhen designing deep network architectures: Given a target network architecture \ncan we design a smaller network architecture that approximates the operation of \nthe target network? The question is, in part, motivated by the challenge of \nparameter reduction (compression) in modern deep neural networks, as the ever \nincreasing storage and memory requirements of these networks pose a problem in \nresource constrained environments. \n</p> \n<p>In this work, we focus on deep convolutional neural network architectures, \nand propose a novel randomized tensor sketching technique that we utilize to \ndevelop a unified framework for approximating the operation of both the \nconvolutional and fully connected layers. By applying the sketching technique \nalong different tensor dimensions, we design changes to the convolutional and \nfully connected layers that substantially reduce the number of effective \nparameters in a network. We show that the resulting smaller network can be \ntrained directly, and has a classification accuracy that is comparable to the \noriginal network. \n</p>"}, "author": "Shiva Prasad Kasiviswanathan, Nina Narodytska, Hongxia Jin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890282", "id": "tag:google.com,2005:reader/item/00000003253738bf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Triphone Embedding Improves Phoneme Recognition. (arXiv:1710.07868v1 [cs.SD])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07868"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07868", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cb8e2ff\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cb8e2ff&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we present a novel Deep Triphone Embedding (DTE) \nrepresentation derived from Deep Neural Network (DNN) to encapsulate the \ndiscriminative information present in the adjoining speech frames. DTEs are \ngenerated using a four hidden layer DNN with 3000 nodes in each hidden layer at \nthe first-stage. This DNN is trained with the tied-triphone classification \naccuracy as an optimization criterion. Thereafter, we retain the activation \nvectors (3000) of the last hidden layer, for each speech MFCC frame, and \nperform dimension reduction to further obtain a 300 dimensional representation, \nwhich we termed as DTE. DTEs along with MFCC features are fed into a \nsecond-stage four hidden layer DNN, which is subsequently trained for the task \nof tied-triphone classification. Both DNNs are trained using tri-phone labels \ngenerated from a tied-state triphone HMM-GMM system, by performing a \nforced-alignment between the transcriptions and MFCC feature frames. We conduct \nthe experiments on publicly available TED-LIUM speech corpus. The results show \nthat the proposed DTE method provides an improvement of absolute 2.11% in \nphoneme recognition, when compared with a competitive hybrid tied-state \ntriphone HMM-DNN system. \n</p>"}, "author": "Mohit Yadav, Vivek Tyagi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890281", "id": "tag:google.com,2005:reader/item/0000000325373947", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Complexity of Graph-Based Reductions for Reachability in Markov Decision Processes. (arXiv:1710.07903v1 [cs.LO])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07903"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07903", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the never-worse relation (NWR) for Markov decision processes with an \ninfinite-horizon reachability objective. A state q is never worse than a state \np if the maximal probability of reaching the target set of states from p is at \nmost the same value from q, regardless of the probabilities labelling the \ntransitions. Extremal-probability states, end components, and essential states \nare all special cases of the equivalence relation induced by the NWR. Using the \nNWR, states in the same equivalence class can be collapsed. Then, actions \nleading to sub-optimal states can be removed. We show the natural decision \nproblem associated to computing the NWR is coNP-complete. Finally, we describe \nan incomplete polynomial-time iterative algorithm to under-approximate the NWR. \n</p>"}, "author": "Stephane Le Roux, Guillermo A. Perez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890280", "id": "tag:google.com,2005:reader/item/000000032537399e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Safety-Aware Apprenticeship Learning. (arXiv:1710.07983v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07983"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07983", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Apprenticeship learning (AL) is a class of \"learning from demonstrations\" \ntechniques where the reward function of a Markov Decision Process (MDP) is \nunknown to the learning agent and the agent has to derive a good policy by \nobserving an expert's demonstrations. In this paper, we study the problem of \nhow to make AL algorithms inherently safe while still meeting its learning \nobjective. We consider a setting where the unknown reward function is assumed \nto be a linear combination of a set of state features, and the safety property \nis specified in Probabilistic Computation Tree Logic (PCTL). By embedding \nprobabilistic model checking inside AL, we propose a novel \ncounterexample-guided approach that can ensure both safety and performance of \nthe learned policy. We demonstrate the effectiveness of our approach on several \nchallenging AL scenarios where safety is essential. \n</p>"}, "author": "Weichao Zhou, Wenchao Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890279", "id": "tag:google.com,2005:reader/item/00000003253739fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hierarchical State Abstractions for Decision-Making Problems with Computational Constraints. (arXiv:1710.07990v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07990"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07990", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this semi-tutorial paper, we first review the information-theoretic \napproach to account for the computational costs incurred during the search for \noptimal actions in a sequential decision-making problem. The traditional (MDP) \nframework ignores computational limitations while searching for optimal \npolicies, essentially assuming that the acting agent is perfectly rational and \naims for exact optimality. Using the free-energy, a variational principle is \nintroduced that accounts not only for the value of a policy alone, but also \nconsiders the cost of finding this optimal policy. The solution of the \nvariational equations arising from this formulation can be obtained using \nfamiliar Bellman-like value iterations from dynamic programming (DP) and the \nBlahut-Arimoto (BA) algorithm from rate distortion theory. Finally, we \ndemonstrate the utility of the approach for generating hierarchies of state \nabstractions that can be used to best exploit the available computational \nresources. A numerical example showcases these concepts for a path-planning \nproblem in a grid world environment. \n</p>"}, "author": "Daniel T. Larsson, Daniel Braun, Panagiotis Tsiotras", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890278", "id": "tag:google.com,2005:reader/item/0000000325373a56", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Probabilistic Pursuits on Graphs. (arXiv:1710.08107v1 [cs.DM])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08107"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08107", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider discrete dynamical systems of \"ant-like\" agents engaged in a \nsequence of pursuits on a graph environment. The agents emerge one by one at \nequal time intervals from a source vertex $s$ and pursue each other by greedily \nattempting to close the distance to their immediate predecessor, the agent that \nemerged just before them from $s$, until they arrive at the destination point \n$t$. Such pursuits have been investigated before in the continuous setting and \nin discrete time when the underlying environment is a regular grid. In both \nthese settings the agents' walks provably converge to a shortest path from $s$ \nto $t$. Furthermore, assuming a certain natural probability distribution over \nthe move choices of the agents on the grid (in case there are multiple shortest \npaths between an agent and its predecessor), the walks converge to the uniform \ndistribution over all shortest paths from $s$ to $t$. \n</p> \n<p>We study the evolution of agent walks over a general finite graph environment \n$G$. Our model is a natural generalization of the pursuit rule proposed for the \ncase of the grid. The main results are as follows. We show that \"convergence\" \nto the shortest paths in the sense of previous work extends to all \npseudo-modular graphs (i.e. graphs in which every three pairwise intersecting \ndisks have a nonempty intersection), and also to environments obtained by \ntaking graph products, generalizing previous results in two different ways. We \nshow that convergence to the shortest paths is also obtained by chordal graphs, \nand discuss some further positive and negative results for planar graphs. In \nthe most general case, convergence to the shortest paths is not guaranteed, and \nthe agents may get stuck on sets of recurrent, non-optimal walks from $s$ to \n$t$. However, we show that the limiting distributions of the agents' walks will \nalways be uniform distributions over some set of walks of equal length. \n</p>"}, "author": "Michael Amir, Alfred M. Bruckstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890277", "id": "tag:google.com,2005:reader/item/0000000325373aa9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Human-in-the-loop Artificial Intelligence. (arXiv:1710.08191v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08191"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08191", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Little by little, newspapers are revealing the bright future that Artificial \nIntelligence (AI) is building. Intelligent machines will help everywhere. \nHowever, this bright future has a dark side: a dramatic job market contraction \nbefore its unpredictable transformation. Hence, in a near future, large numbers \nof job seekers will need financial support while catching up with these novel \nunpredictable jobs. This possible job market crisis has an antidote inside. In \nfact, the rise of AI is sustained by the biggest knowledge theft of the recent \nyears. Learning AI machines are extracting knowledge from unaware skilled or \nunskilled workers by analyzing their interactions. By passionately doing their \njobs, these workers are digging their own graves. \n</p> \n<p>In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI) \nas a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward \naware and unaware knowledge producers with a different scheme: decisions of AI \nsystems generating revenues will repay the legitimate owners of the knowledge \nused for taking those decisions. As modern Robin Hoods, HIT-AI researchers \nshould fight for a fairer Artificial Intelligence that gives back what it \nsteals. \n</p>"}, "author": "Fabio Massimo Zanzotto", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890276", "id": "tag:google.com,2005:reader/item/0000000325373afc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Investigating the feature collection for semantic segmentation via single skip connection. (arXiv:1710.08192v1 [cs.CV])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08192"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08192", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Since the study of deep convolutional neural network became prevalent, one of \nthe important discoveries is that a feature map from a convolutional network \ncan be extracted before going into the fully connected layer and can be used as \na saliency map for object detection. Furthermore, the model can use features \nfrom each different layer for accurate object detection: the features from \ndifferent layers can have different properties. As the model goes deeper, it \nhas many latent skip connections and feature maps to elaborate object \ndetection. Although there are many intermediate layers that we can use for \nsemantic segmentation through skip connection, still the characteristics of \neach skip connection and the best skip connection for this task are uncertain. \nTherefore, in this study, we exhaustively research skip connections of \nstate-of-the-art deep convolutional networks and investigate the \ncharacteristics of the features from each intermediate layer. In addition, this \nstudy would suggest how to use a recent deep neural network model for semantic \nsegmentation and it would therefore become a cornerstone for later studies with \nthe state-of-the-art network models. \n</p>"}, "author": "Jonghwa Yim, Kyung-Ah Sohn", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890275", "id": "tag:google.com,2005:reader/item/0000000325373b37", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Autoencoder Feature Selector. (arXiv:1710.08310v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08310"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08310", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>High-dimensional data in many areas such as computer vision and machine \nlearning brings in computational and analytical difficulty. Feature selection \nwhich select a subset of features from original ones has been proven to be \neffective and efficient to deal with high-dimensional data. In this paper, we \npropose a novel AutoEncoder Feature Selector (AEFS) for unsupervised feature \nselection. AEFS is based on the autoencoder and the group lasso regularization. \nCompared to traditional feature selection methods, AEFS can select the most \nimportant features in spite of nonlinear and complex correlation among \nfeatures. It can be viewed as a nonlinear extension of the linear method \nregularized self-representation (RSR) for unsupervised feature selection. In \norder to deal with noise and corruption, we also propose robust AEFS. An \nefficient iterative algorithm is designed for model optimization and \nexperimental results verify the effectiveness and superiority of the proposed \nmethod. \n</p>"}, "author": "Kai Han, Chao Li, Xin Shi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890274", "id": "tag:google.com,2005:reader/item/0000000325373b6e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "BENCHIP: Benchmarking Intelligence Processors. (arXiv:1710.08315v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08315"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08315", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The increasing attention on deep learning has tremendously spurred the design \nof intelligence processing hardware. The variety of emerging intelligence \nprocessors requires standard benchmarks for fair comparison and system \noptimization (in both software and hardware). However, existing benchmarks are \nunsuitable for benchmarking intelligence processors due to their non-diversity \nand nonrepresentativeness. Also, the lack of a standard benchmarking \nmethodology further exacerbates this problem. In this paper, we propose \nBENCHIP, a benchmark suite and benchmarking methodology for intelligence \nprocessors. The benchmark suite in BENCHIP consists of two sets of benchmarks: \nmicrobenchmarks and macrobenchmarks. The microbenchmarks consist of \nsingle-layer networks. They are mainly designed for bottleneck analysis and \nsystem optimization. The macrobenchmarks contain state-of-the-art industrial \nnetworks, so as to offer a realistic comparison of different platforms. We also \npropose a standard benchmarking methodology built upon an industrial software \nstack and evaluation metrics that comprehensively reflect the various \ncharacteristics of the evaluated intelligence processors. BENCHIP is utilized \nfor evaluating various hardware platforms, including CPUs, GPUs, and \naccelerators. BENCHIP will be open-sourced soon. \n</p>"}, "author": "Jin-Hua Tao, Zi-Dong Du, Qi Guo, Hui-Ying Lan, Lei Zhang, Sheng-Yuan Zhou, Cong Liu, Hai-Feng Liu, Shan Tang, Allen Rush, Willian Chen, Shao-Li Liu, Yun-Ji Chen, Tian-Shi Chen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890273", "id": "tag:google.com,2005:reader/item/0000000325373bbd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Listening to the World Improves Speech Command Recognition. (arXiv:1710.08377v1 [cs.SD])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08377"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08377", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cb8e81d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cb8e81d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We study transfer learning in convolutional network architectures applied to \nthe task of recognizing audio, such as environmental sound events and speech \ncommands. Our key finding is that not only is it possible to transfer \nrepresentations from an unrelated task like environmental sound classification \nto a voice-focused task like speech command recognition, but also that doing so \nimproves accuracies significantly. We also investigate the effect of increased \nmodel capacity for transfer learning audio, by first validating known results \nfrom the field of Computer Vision of achieving better accuracies with \nincreasingly deeper networks on two audio datasets: UrbanSound8k and the newly \nreleased Google Speech Commands dataset. Then we propose a simple multiscale \ninput representation using dilated convolutions and show that it is able to \naggregate larger contexts and increase classification performance. Further, the \nmodels trained using a combination of transfer learning and multiscale input \nrepresentations need only 40% of the training data to achieve similar \naccuracies as a freshly trained model with 100% of the training data. Finally, \nwe demonstrate a positive interaction effect for the multiscale input and \ntransfer learning, making a case for the joint application of the two \ntechniques. \n</p>"}, "author": "Brian McMahan, Delip Rao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068177", "id": "tag:google.com,2005:reader/item/00000003250c2f1a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the Consistency of Graph-based Bayesian Learning and the Scalability of Sampling Algorithms. (arXiv:1710.07702v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07702"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07702", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A popular approach to semi-supervised learning proceeds by endowing the input \ndata with a graph structure in order to extract geometric information and \nincorporate it into a Bayesian framework. We introduce new theory that gives \nappropriate scalings of graph parameters that provably lead to a well-defined \nlimiting posterior as the size of the unlabeled data set grows. Furthermore, we \nshow that these consistency results have profound algorithmic implications. \nWhen consistency holds, carefully designed graph-based Markov chain Monte Carlo \nalgorithms are proved to have a uniform spectral gap, independent of the number \nof unlabeled inputs. Several numerical experiments corroborate both the \nstatistical consistency and the algorithmic scalability established by the \ntheory. \n</p>"}, "author": "Nicolas Garcia Trillos, Zachary Kaplan, Thabo Samakhoana, Daniel Sanz-Alonso", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068176", "id": "tag:google.com,2005:reader/item/00000003250c2fab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Tight Excess Risk Bound via a Unified PAC-Bayesian-Rademacher-Shtarkov-MDL Complexity. (arXiv:1710.07732v1 [cs.LG])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07732"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07732", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a novel notion of complexity that interpolates between and \ngeneralizes some classic existing complexity notions in learning theory: for \nestimators like empirical risk minimization (ERM) with arbitrary bounded \nlosses, it is upper bounded in terms of data-independent Rademacher complexity; \nfor generalized Bayesian estimators, it is upper bounded by the data-dependent \ninformation complexity (also known as stochastic or PAC-Bayesian, \n$\\mathrm{KL}(\\text{posterior} \\operatorname{\\|} \\text{prior})$ complexity. For \n(penalized) ERM, the new complexity reduces to (generalized) normalized maximum \nlikelihood (NML) complexity, i.e. a minimax log-loss individual-sequence \nregret. Our first main result bounds excess risk in terms of the new \ncomplexity. Our second main result links the new complexity via Rademacher \ncomplexity to $L_2(P)$ entropy, thereby generalizing earlier results of Opper, \nHaussler, Lugosi, and Cesa-Bianchi who did the log-loss case with $L_\\infty$. \nTogether, these results recover optimal bounds for VC- and large (polynomial \nentropy) classes, replacing localized Rademacher complexity by a simpler \nanalysis which almost completely separates the two aspects that determine the \nachievable rates: 'easiness' (Bernstein) conditions and model complexity. \n</p>"}, "author": "Peter D. Gr&#xfc;nwald, Nishant A. Mehta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068175", "id": "tag:google.com,2005:reader/item/00000003250c3017", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Discrete Weights Using the Local Reparameterization Trick. (arXiv:1710.07739v2 [cs.LG] UPDATED)", "published": 1509484038, "updated": 1509484044, "canonical": [{"href": "http://arxiv.org/abs/1710.07739"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07739", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent breakthroughs in computer vision make use of large deep neural \nnetworks, utilizing the substantial speedup offered by GPUs. For applications \nrunning on limited hardware, however, high precision real-time processing can \nstill be a challenge. One approach to solving this problem is training networks \nwith binary or ternary weights, thus removing the need to calculate \nmultiplications and significantly reducing memory size. In this work, we \nintroduce LR-nets (Local reparameterization networks), a new method for \ntraining neural networks with discrete weights using stochastic parameters. We \nshow how a simple modification to the local reparameterization trick, \npreviously used to train Gaussian distributed weights, enables the training of \ndiscrete weights. Using the proposed training we test both binary and ternary \nmodels on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art \nresults on most experiments. \n</p>"}, "author": "Oran Shayer, Dan Levi, Ethan Fetaya", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068174", "id": "tag:google.com,2005:reader/item/00000003250c30d2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards Black-box Iterative Machine Teaching. (arXiv:1710.07742v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07742"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07742", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we make an important step towards the black-box machine \nteaching by considering the cross-space teaching setting, where the teacher and \nthe learner use different feature representations and the teacher can not fully \nobserve the learner's model. In such scenario, we study how the teacher is \nstill able to teach the learner to achieve a faster convergence rate than the \ntraditional passive learning. We propose an active teacher model that can \nactively query the learner (i.e., make the learner take exams) for estimating \nthe learner's status, and provide the sample complexity for both teaching and \nquery, respectively. In the experiments, we compare the proposed active teacher \nwith the omniscient teacher and verify the effectiveness of the active teacher \nmodel. \n</p>"}, "author": "Weiyang Liu, Bo Dai, Xingguo Li, James M. Rehg, Le Song", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068173", "id": "tag:google.com,2005:reader/item/00000003250c3123", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Novel Stochastic Stratified Average Gradient Method: Convergence Rate and Its Complexity. (arXiv:1710.07783v1 [cs.AI])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07783"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07783", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>SGD (Stochastic Gradient Descent) is a popular algorithm for large scale \noptimization problems due to its low iterative cost. However, SGD can not \nachieve linear convergence rate as FGD (Full Gradient Descent) because of the \ninherent gradient variance. To attack the problem, mini-batch SGD was proposed \nto get a trade-off in terms of convergence rate and iteration cost. In this \npaper, a general CVI (Convergence-Variance Inequality) equation is presented to \nstate formally the interaction of convergence rate and gradient variance. Then \na novel algorithm named SSAG (Stochastic Stratified Average Gradient) is \nintroduced to reduce gradient variance based on two techniques, stratified \nsampling and averaging over iterations that is a key idea in SAG (Stochastic \nAverage Gradient). Furthermore, SSAG can achieve linear convergence rate of \n$\\mathcal {O}((1-\\frac{\\mu}{8CL})^k)$ at smaller storage and iterative costs, \nwhere $C\\geq 2$ is the category number of training data. This convergence rate \ndepends mainly on the variance between classes, but not on the variance within \nthe classes. In the case of $C\\ll N$ ($N$ is the training data size), SSAG's \nconvergence rate is much better than SAG's convergence rate of $\\mathcal \n{O}((1-\\frac{\\mu}{8NL})^k)$. Our experimental results show SSAG outperforms SAG \nand many other algorithms. \n</p>"}, "author": "Aixiang Chen, Bingchuan Chen, Xiaolong Chai, Rui Bian, Hengguang Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068172", "id": "tag:google.com,2005:reader/item/00000003250c3157", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimal Rates for Learning with Nystr\\\"om Stochastic Gradient Methods. (arXiv:1710.07797v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07797"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07797", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In the setting of nonparametric regression, we propose and study a \ncombination of stochastic gradient methods with Nystr\\\"om subsampling, allowing \nmultiple passes over the data and mini-batches. Generalization error bounds for \nthe studied algorithm are provided. Particularly, optimal learning rates are \nderived considering different possible choices of the step-size, the mini-batch \nsize, the number of iterations/passes, and the subsampling level. In comparison \nwith state-of-the-art algorithms such as the classic stochastic gradient \nmethods and kernel ridge regression with Nystr\\\"om, the studied algorithm has \nadvantages on the computational complexity, while achieving the same optimal \nlearning rates. Moreover, our results indicate that using mini-batches can \nreduce the total computational cost while achieving the same optimal \nstatistical results. \n</p>"}, "author": "Junhong Lin, Lorenzo Rosasco", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068171", "id": "tag:google.com,2005:reader/item/00000003250c31c2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Zeroth-Order Online Alternating Direction Method of Multipliers: Convergence Analysis and Applications. (arXiv:1710.07804v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07804"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07804", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we design and analyze a new zeroth-order online algorithm, \nnamely, the zeroth-order online alternating direction method of multipliers \n(ZOO-ADMM), which enjoys dual advantages of being gradient-free operation and \nemploying the ADMM to accommodate complex structured regularizers. Compared to \nthe first-order gradient-based online algorithm, we show that ZOO-ADMM requires \n$\\sqrt{m}$ times more iterations, leading to a convergence rate of \n$O(\\sqrt{m}/\\sqrt{T})$, where $m$ is the number of optimization variables, and \n$T$ is the number of iterations. To accelerate ZOO-ADMM, we propose two \nminibatch strategies: gradient sample averaging and observation averaging, \nresulting in an improved convergence rate of $O(\\sqrt{1+q^{-1}m}/\\sqrt{T})$, \nwhere $q$ is the minibatch size. In addition to convergence analysis, we also \ndemonstrate ZOO-ADMM to applications in signal processing, statistics, and \nmachine learning. \n</p>"}, "author": "Sijia Liu, Jie Chen, Pin-Yu Chen, Alfred O. Hero", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068170", "id": "tag:google.com,2005:reader/item/00000003250c323f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Learning-to-Infer Method for Real-Time Power Grid Topology Identification. (arXiv:1710.07818v1 [cs.LG])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07818"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07818", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Identifying arbitrary topologies of power networks in real time is a \ncomputationally hard problem due to the number of hypotheses that grows \nexponentially with the network size. A new \"Learning-to-Infer\" variational \ninference method is developed for efficient inference of every line status in \nthe network. Optimizing the variational model is transformed to and solved as a \ndiscriminative learning problem based on Monte Carlo samples generated with \npower flow simulations. A major advantage of the developed Learning-to-Infer \nmethod is that the labeled data used for training can be generated in an \narbitrarily large amount fast and at very little cost. As a result, the power \nof offline training is fully exploited to learn very complex classifiers for \neffective real-time topology identification. The proposed methods are evaluated \nin the IEEE 30, 118 and 300 bus systems. Excellent performance in identifying \narbitrary power network topologies in real time is achieved even with \nrelatively simple variational models and a reasonably small amount of data. \n</p>"}, "author": "Yue Zhao, Jianshu Chen, H. Vincent Poor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068169", "id": "tag:google.com,2005:reader/item/00000003250c3295", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Neural Network Approximation using Tensor Sketching. (arXiv:1710.07850v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07850"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07850", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks are powerful learning models that achieve \nstate-of-the-art performance on many computer vision, speech, and language \nprocessing tasks. In this paper, we study a fundamental question that arises \nwhen designing deep network architectures: Given a target network architecture \ncan we design a smaller network architecture that approximates the operation of \nthe target network? The question is, in part, motivated by the challenge of \nparameter reduction (compression) in modern deep neural networks, as the ever \nincreasing storage and memory requirements of these networks pose a problem in \nresource constrained environments. \n</p> \n<p>In this work, we focus on deep convolutional neural network architectures, \nand propose a novel randomized tensor sketching technique that we utilize to \ndevelop a unified framework for approximating the operation of both the \nconvolutional and fully connected layers. By applying the sketching technique \nalong different tensor dimensions, we design changes to the convolutional and \nfully connected layers that substantially reduce the number of effective \nparameters in a network. We show that the resulting smaller network can be \ntrained directly, and has a classification accuracy that is comparable to the \noriginal network. \n</p>"}, "author": "Shiva Prasad Kasiviswanathan, Nina Narodytska, Hongxia Jin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068168", "id": "tag:google.com,2005:reader/item/00000003250c32de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Insulin Regimen ML-based control for T2DM patients. (arXiv:1710.07855v1 [q-bio.QM])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07855"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07855", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cb8ed12\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cb8ed12&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>\\begin{abstract} We model individual T2DM patient blood glucose level (BGL) \nby stochastic process with discrete number of states mainly but not solely \ngoverned by medication regimen (e.g. insulin injections). BGL states change \notherwise according to various physiological triggers which render a \nstochastic, statistically unknown, yet assumed to be quasi-stationary, nature \nof the process. In order to express incentive for being in desired healthy BGL \nwe heuristically define a reward function which returns positive values for \ndesirable BG levels and negative values for undesirable BG levels. The state \nspace consists of sufficient number of states in order to allow for memoryless \nassumption. This, in turn, allows to formulate Markov Decision Process (MDP), \nwith an objective to maximize the total reward, summarized over a long run. The \nprobability law is found by model-based reinforcement learning (RL) and the \noptimal insulin treatment policy is retrieved from MDP solution. \n</p>"}, "author": "Mark Shifrin, Hava Siegelmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068167", "id": "tag:google.com,2005:reader/item/00000003250c3326", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Iteratively reweighted $\\ell_1$ algorithms with extrapolation. (arXiv:1710.07886v1 [math.OC])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07886"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07886", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cc14bfe\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cc14bfe&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Iteratively reweighted $\\ell_1$ algorithm is a popular algorithm for solving \na large class of optimization problems whose objective is the sum of a \nLipschitz differentiable loss function and a possibly nonconvex sparsity \ninducing regularizer. In this paper, motivated by the success of extrapolation \ntechniques in accelerating first-order methods, we study how widely used \nextrapolation techniques such as those in [4,5,22,28] can be incorporated to \npossibly accelerate the iteratively reweighted $\\ell_1$ algorithm. We consider \nthree versions of such algorithms. For each version, we exhibit an explicitly \ncheckable condition on the extrapolation parameters so that the sequence \ngenerated provably clusters at a stationary point of the optimization problem. \nWe also investigate global convergence under additional Kurdyka-{\\L}ojasiewicz \nassumptions on certain potential functions. Our numerical experiments show that \nour algorithms usually outperform the general iterative shrinkage and \nthresholding algorithm in [21] and an adaptation of the iteratively reweighted \n$\\ell_1$ algorithm in [23, Algorithm 7] with nonmonotone line-search for \nsolving random instances of log penalty regularized least squares problems in \nterms of both CPU time and solution quality. \n</p>"}, "author": "Peiran Yu, Ting Kei Pong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068166", "id": "tag:google.com,2005:reader/item/00000003250c337f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Elliptical modeling and pattern analysis for perturbation models and classfication. (arXiv:1710.07939v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07939"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07939", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The characteristics (or numerical patterns) of a feature vector in the \ntransform domain of a perturbation model differ significantly from those of its \ncorresponding feature vector in the input domain. These differences - caused by \nthe perturbation techniques used for the transformation of feature patterns - \ndegrade the performance of machine learning techniques in the transform domain. \nIn this paper, we proposed a nonlinear parametric perturbation model that \ntransforms the input feature patterns to a set of elliptical patterns, and \nstudied the performance degradation issues associated with random forest \nclassification technique using both the input and transform domain features. \nCompared with the linear transformation such as Principal Component Analysis \n(PCA), the proposed method requires less statistical assumptions and is highly \nsuitable for the applications such as data privacy and security due to the \ndifficulty of inverting the elliptical patterns from the transform domain to \nthe input domain. In addition, we adopted a flexible block-wise dimensionality \nreduction step in the proposed method to accommodate the possible \nhigh-dimensional data in modern applications. We evaluated the empirical \nperformance of the proposed method on a network intrusion data set and a \nbiological data set, and compared the results with PCA in terms of \nclassification performance and data privacy protection (measured by the blind \nsource separation attack and signal interference ratio). Both results confirmed \nthe superior performance of the proposed elliptical transformation. \n</p>"}, "author": "Shan Suthaharan, Weining Shen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068165", "id": "tag:google.com,2005:reader/item/00000003250c33c2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "WristAuthen: A Dynamic Time Wrapping Approach for User Authentication by Hand-Interaction through Wrist-Worn Devices. (arXiv:1710.07941v1 [cs.HC])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07941"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07941", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The growing trend of using wearable devices for context-aware computing and \npervasive sensing systems has raised its potentials for quick and reliable \nauthentication techniques. Since personal writing habitats differ from each \nother, it is possible to realize user authentication through writing. This is \nof great significance as sensible information is easily collected by these \ndevices. This paper presents a novel user authentication system through \nwrist-worn devices by analyzing the interaction behavior with users, which is \nboth accurate and efficient for future usage. The key feature of our approach \nlies in using much more effective Savitzky-Golay filter and Dynamic Time \nWrapping method to obtain fine-grained writing metrics for user authentication. \nThese new metrics are relatively unique from person to person and independent \nof the computing platform. Analyses are conducted on the wristband-interaction \ndata collected from 50 users with diversity in gender, age, and height. \nExtensive experimental results show that the proposed approach can identify \nusers in a timely and accurate manner, with a false-negative rate of 1.78\\%, \nfalse-positive rate of 6.7\\%, and Area Under ROC Curve of 0.983 . Additional \nexamination on robustness to various mimic attacks, tolerance to training data, \nand comparisons to further analyze the applicability. \n</p>"}, "author": "Qi Lyu, Zhifeng Kong, Chao Shen, Tianwei Yue", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068164", "id": "tag:google.com,2005:reader/item/00000003250c343f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Approach to One-Bit Compressed Sensing Based on Probably Approximately Correct Learning Theory. (arXiv:1710.07973v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07973"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07973", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, the problem of one-bit compressed sensing (OBCS) is formulated \nas a problem in probably approximately correct (PAC) learning. It is shown that \nthe Vapnik-Chervonenkis (VC-) dimension of the set of half-spaces in \n$\\mathbb{R}^n$ generated by $k$-sparse vectors is bounded below by $k \\lg \n(n/k)$ and above by $2k \\lg (n/k)$, plus some round-off terms. By coupling this \nestimate with well-established results in PAC learning theory, we show that a \nconsistent algorithm can recover a $k$-sparse vector with $O(k \\lg (n/k))$ \nmeasurements, given only the signs of the measurement vector. This result holds \nfor \\textit{all} probability measures on $\\mathbb{R}^n$. It is further shown \nthat random sign-flipping errors result only in an increase in the constant in \nthe $O(k \\lg (n/k))$ estimate. Because constructing a consistent algorithm is \nnot straight-forward, we present a heuristic based on the $\\ell_1$-norm support \nvector machine, and illustrate that its computational performance is superior \nto a currently popular method. \n</p>"}, "author": "Mehmet Eren Ahsen, Mathukumalli Vidyasagar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068163", "id": "tag:google.com,2005:reader/item/00000003250c34b5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hierarchical State Abstractions for Decision-Making Problems with Computational Constraints. (arXiv:1710.07990v1 [cs.AI])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07990"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07990", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this semi-tutorial paper, we first review the information-theoretic \napproach to account for the computational costs incurred during the search for \noptimal actions in a sequential decision-making problem. The traditional (MDP) \nframework ignores computational limitations while searching for optimal \npolicies, essentially assuming that the acting agent is perfectly rational and \naims for exact optimality. Using the free-energy, a variational principle is \nintroduced that accounts not only for the value of a policy alone, but also \nconsiders the cost of finding this optimal policy. The solution of the \nvariational equations arising from this formulation can be obtained using \nfamiliar Bellman-like value iterations from dynamic programming (DP) and the \nBlahut-Arimoto (BA) algorithm from rate distortion theory. Finally, we \ndemonstrate the utility of the approach for generating hierarchies of state \nabstractions that can be used to best exploit the available computational \nresources. A numerical example showcases these concepts for a path-planning \nproblem in a grid world environment. \n</p>"}, "author": "Daniel T. Larsson, Daniel Braun, Panagiotis Tsiotras", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068162", "id": "tag:google.com,2005:reader/item/00000003250c3517", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Rethinking Convolutional Semantic Segmentation Learning. (arXiv:1710.07991v1 [cs.LG])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07991"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07991", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep convolutional semantic segmentation (DCSS) learning doesn't converge to \nan optimal local minimum with random parameters initializations; a pre-trained \nmodel on the same domain becomes necessary to achieve convergence.In this work, \nwe propose a joint cooperative end-to-end learning method for DCSS. It \naddresses many drawbacks with existing deep semantic segmentation learning; the \nproposed approach simultaneously learn both segmentation and classification; \ntaking away the essential need of the pre-trained model for learning \nconvergence. We present an improved inception based architecture with partial \nattention gating (PAG) over encoder information. The PAG also adds to achieve \nfaster convergence and better accuracy for segmentation task. We will show the \neffectiveness of this learning on a diabetic retinopathy classification and \nsegmentation dataset. \n</p>"}, "author": "Mrinal Haloi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068161", "id": "tag:google.com,2005:reader/item/00000003250c3577", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Smart \"Predict, then Optimize\". (arXiv:1710.08005v1 [math.OC])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08005"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08005", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many real-world analytics problems involve two significant challenges: \nprediction and optimization. Due to the typically complex nature of each \nchallenge, the standard paradigm is to predict, then optimize. By and large, \nmachine learning tools are intended to minimize prediction error and do not \naccount for how the predictions will be used in a downstream optimization \nproblem. In contrast, we propose a new and very general framework, called Smart \n\"Predict, then Optimize\" (SPO), which directly leverages the optimization \nproblem structure, i.e., its objective and constraints, for designing \nsuccessful analytics tools. A key component of our framework is the SPO loss \nfunction, which measures the quality of a prediction by comparing the objective \nvalues of the solutions generated using the predicted and observed parameters, \nrespectively. Training a model with respect to the SPO loss is computationally \nchallenging, and therefore we also develop a surrogate loss function, called \nthe SPO+ loss, which upper bounds the SPO loss, has desirable convexity \nproperties, and is statistically consistent under mild conditions. We also \npropose a stochastic gradient descent algorithm which allows for situations in \nwhich the number of training samples is large, model regularization is desired, \nand/or the optimization problem of interest is nonlinear or integer. Finally, \nwe perform computational experiments to empirically verify the success of our \nSPO framework in comparison to the standard predict-then-optimize approach. \n</p>"}, "author": "Adam N. Elmachtoub, Paul Grigas", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068160", "id": "tag:google.com,2005:reader/item/00000003250c35ae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Exploiting generalization in the subspaces for faster model-based learning. (arXiv:1710.08012v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08012"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08012", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Due to the lack of enough generalization in the state-space, common methods \nin Reinforcement Learning (RL) suffer from slow learning speed especially in \nthe early learning trials. This paper introduces a model-based method in \ndiscrete state-spaces for increasing learning speed in terms of required \nexperience (but not required computational time) by exploiting generalization \nin the experiences of the subspaces. A subspace is formed by choosing a subset \nof features in the original state representation (full-space). Generalization \nand faster learning in a subspace are due to many-to-one mapping of experiences \nfrom the full-space to each state in the subspace. Nevertheless, due to \ninherent perceptual aliasing in the subspaces, the policy suggested by each \nsubspace does not generally converge to the optimal policy. Our approach, \ncalled Model Based Learning with Subspaces (MoBLeS), calculates confidence \nintervals of the estimated Q-values in the full-space and in the subspaces. \nThese confidence intervals are used in the decision making, such that the agent \nbenefits the most from the possible generalization while avoiding from \ndetriment of the perceptual aliasing in the subspaces. Convergence of MoBLeS to \nthe optimal policy is theoretically investigated. Additionally, we show through \nseveral experiments that MoBLeS improves the learning speed in the early \ntrials. \n</p>"}, "author": "Maryam Hashemzadeh, Reshad Hosseini, Majid Nili Ahmadabadi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068159", "id": "tag:google.com,2005:reader/item/00000003250c3609", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sequential Matrix Completion. (arXiv:1710.08045v1 [cs.IR])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08045"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08045", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a novel algorithm for sequential matrix completion in a \nrecommender system setting, where the $(i,j)$th entry of the matrix corresponds \nto a user $i$'s rating of product $j$. The objective of the algorithm is to \nprovide a sequential policy for user-product pair recommendation which will \nyield the highest possible ratings after a finite time horizon. The algorithm \nuses a Gamma process factor model with two posterior-focused bandit policies, \nThompson Sampling and Information-Directed Sampling. While Thompson Sampling \nshows competitive performance in simulations, state-of-the-art performance is \nobtained from Information-Directed Sampling, which makes its recommendations \nbased off a ratio between the expected reward and a measure of information \ngain. To our knowledge, this is the first implementation of Information \nDirected Sampling on large real datasets. \n</p> \n<p>This approach contributes to a recent line of research on bandit approaches \nto collaborative filtering including Kawale et al. (2015), Li et al. (2010), \nBresler et al. (2014), Li et al. (2016), Deshpande &amp; Montanari (2012), and Zhao \net al. (2013). The setting of this paper, as has been noted in Kawale et al. \n(2015) and Zhao et al. (2013), presents significant challenges to bounding \nregret after finite horizons. We discuss these challenges in relation to \nsimpler models for bandits with side information, such as linear or gaussian \nprocess bandits, and hope the experiments presented here motivate further \nresearch toward theoretical guarantees. \n</p>"}, "author": "Annie Marsden, Sergio Bacallado", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068158", "id": "tag:google.com,2005:reader/item/00000003250c36a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Online Boosting Algorithms for Multi-label Ranking. (arXiv:1710.08079v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08079"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08079", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cc14f22\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cc14f22&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We consider the multi-label ranking approach to multi-label learning. \nBoosting is a natural method for multi-label ranking as it aggregates weak \npredictions through majority votes, which can be directly used as scores to \nproduce a ranking of the labels. We design online boosting algorithms with \nprovable loss bounds for multi-label ranking. We show that our first algorithm \nis optimal in terms of the number of learners required to attain a desired \naccuracy, but it requires knowledge of the edge of the weak learners. We also \ndesign an adaptive algorithm that does not require this knowledge and is hence \nmore practical. Experimental results on real data sets demonstrate that our \nalgorithms are at least as good as existing batch boosting algorithms. \n</p>"}, "author": "Young Hun Jung, Ambuj Tewari", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068157", "id": "tag:google.com,2005:reader/item/00000003250c36fc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SMSSVD - SubMatrix Selection Singular Value Decomposition. (arXiv:1710.08144v1 [stat.AP])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08144"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08144", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>High throughput biomedical measurements normally capture multiple overlaid \nbiologically relevant signals and often also signals representing different \ntypes of technical artefacts like e.g. batch effects. Signal identification and \ndecomposition are accordingly main objectives in statistical biomedical \nmodeling and data analysis. Existing methods, aimed at signal reconstruction \nand deconvolution, in general, are either supervised, contain parameters that \nneed to be estimated or present other types of ad hoc features. We here \nintroduce SubMatrix Selection SingularValue Decomposition (SMSSVD), a \nparameter-free unsupervised signal decomposition and dimension reduction \nmethod, designed to reduce noise, adaptively for each low-rank-signal in a \ngiven data matrix, and represent the signals in the data in a way that enable \nunbiased exploratory analysis and reconstruction of multiple overlaid signals, \nincluding identifying groups of variables that drive different signals. \n</p> \n<p>The Submatrix Selection Singular Value Decomposition (SMSSVD) method produces \na denoised signal decomposition from a given data matrix. The SMSSVD method \nguarantees orthogonality between signal components in a straightforward manner \nand it is designed to make automation possible. We illustrate SMSSVD by \napplying it to several real and synthetic datasets and compare its performance \nto golden standard methods like PCA (Principal Component Analysis) and SPC \n(Sparse Principal Components, using Lasso constraints). The SMSSVD is \ncomputationally efficient and despite being a parameter-free method, in \ngeneral, outperforms existing statistical learning methods. \n</p> \n<p>A Julia implementation of SMSSVD is openly available on GitHub \n(https://github.com/rasmushenningsson/SMSSVD.jl). \n</p>"}, "author": "Rasmus Henningsson (1,2), Magnus Fontes (1,2,3,4) ((1) The Centre for Mathematical Sciences, Lund University, Sweden, (2) The International Group for Data Analysis, Institut Pasteur, Paris, France, (3) The Center for Genomic Medicine, Rigshospitalet, Copenhagen, Denmark, (4) Persimune, The Centre of Excellence for Personalized Medicine, Copenhagen, Denmark)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068156", "id": "tag:google.com,2005:reader/item/00000003250c37a0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast MCMC sampling algorithms on polytopes. (arXiv:1710.08165v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08165"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08165", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose and analyze two new MCMC sampling algorithms, the Vaidya walk and \nthe John walk, for generating samples from the uniform distribution over a \npolytope. Both random walks are sampling algorithms derived from interior point \nmethods. The former is based on volumetric-logarithmic barrier introduced by \nVaidya whereas the latter uses John's ellipsoids. We show that the Vaidya walk \nmixes in significantly fewer steps than the logarithmic-barrier based Dikin \nwalk studied in past work. For a polytope in $\\mathbb{R}^d$ defined by $n &gt;d$ \nlinear constraints, we show that the mixing time from a warm start is bounded \nas $\\mathcal{O}(n^{0.5}d^{1.5})$, compared to the $\\mathcal{O}(nd)$ mixing time \nbound for the Dikin walk. The cost of each step of the Vaidya walk is of the \nsame order as the Dikin walk, and at most twice as large in terms of constant \npre-factors. For the John walk, we prove an \n$\\mathcal{O}(d^{2.5}\\cdot\\log^4(n/d))$ bound on its mixing time and conjecture \nthat an improved variant of it could achieve a mixing time of \n$\\mathcal{O}(d^2\\cdot\\text{polylog}(n/d))$. Additionally, we propose variants \nof the Vaidya and John walks that mix in polynomial time from a deterministic \nstarting point. We illustrate the speed-up of the Vaidya walk over the Dikin \nwalk via several numerical examples. \n</p>"}, "author": "Yuansi Chen, Raaz Dwivedi, Martin J. Wainwright, Bin Yu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068155", "id": "tag:google.com,2005:reader/item/00000003250c3815", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Interactive Visual Data Exploration with Subjective Feedback: An Information-Theoretic Approach. (arXiv:1710.08167v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08167"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08167", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Visual exploration of high-dimensional real-valued datasets is a fundamental \ntask in exploratory data analysis (EDA). Existing methods use predefined \ncriteria to choose the representation of data. There is a lack of methods that \n(i) elicit from the user what she has learned from the data and (ii) show \npatterns that she does not know yet. We construct a theoretical model where \nidentified patterns can be input as knowledge to the system. The knowledge \nsyntax here is intuitive, such as \"this set of points forms a cluster\", and \nrequires no knowledge of maths. This background knowledge is used to find a \nMaximum Entropy distribution of the data, after which the system provides the \nuser data projections in which the data and the Maximum Entropy distribution \ndiffer the most, hence showing the user aspects of the data that are maximally \ninformative given the user's current knowledge. We provide an open source EDA \nsystem with tailored interactive visualizations to demonstrate these concepts. \nWe study the performance of the system and present use cases on both synthetic \nand real data. We find that the model and the prototype system allow the user \nto learn information efficiently from various data sources and the system works \nsufficiently fast in practice. We conclude that the information theoretic \napproach to exploratory data analysis where patterns observed by a user are \nformalized as constraints provides a principled, intuitive, and efficient basis \nfor constructing an EDA system. \n</p>"}, "author": "Kai Puolam&#xe4;ki, Emilia Oikarinen, Bo Kang, Jefrey Lijffijt, Tijl De Bie", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068154", "id": "tag:google.com,2005:reader/item/00000003250c3884", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Progressive Learning for Systematic Design of Large Neural Networks. (arXiv:1710.08177v1 [cs.NE])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08177"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08177", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We develop an algorithm for systematic design of a large artificial neural \nnetwork using a progression property. We find that some non-linear functions, \nsuch as the rectifier linear unit and its derivatives, hold the property. The \nsystematic design addresses the choice of network size and regularization of \nparameters. The number of nodes and layers in network increases in progression \nwith the objective of consistently reducing an appropriate cost. Each layer is \noptimized at a time, where appropriate parameters are learned using convex \noptimization. Regularization parameters for convex optimization do not need a \nsignificant manual effort for tuning. We also use random instances for some \nweight matrices, and that helps to reduce the number of parameters we learn. \nThe developed network is expected to show good generalization power due to \nappropriate regularization and use of random weights in the layers. This \nexpectation is verified by extensive experiments for classification and \nregression problems, using standard databases. \n</p>"}, "author": "Saikat Chatterjee, Alireza M. Javid, Mostafa Sadeghi, Partha P. Mitra, Mikael Skoglund", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068153", "id": "tag:google.com,2005:reader/item/00000003250c38ed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Autoencoder Feature Selector. (arXiv:1710.08310v1 [cs.AI])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08310"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08310", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>High-dimensional data in many areas such as computer vision and machine \nlearning brings in computational and analytical difficulty. Feature selection \nwhich select a subset of features from original ones has been proven to be \neffective and efficient to deal with high-dimensional data. In this paper, we \npropose a novel AutoEncoder Feature Selector (AEFS) for unsupervised feature \nselection. AEFS is based on the autoencoder and the group lasso regularization. \nCompared to traditional feature selection methods, AEFS can select the most \nimportant features in spite of nonlinear and complex correlation among \nfeatures. It can be viewed as a nonlinear extension of the linear method \nregularized self-representation (RSR) for unsupervised feature selection. In \norder to deal with noise and corruption, we also propose robust AEFS. An \nefficient iterative algorithm is designed for model optimization and \nexperimental results verify the effectiveness and superiority of the proposed \nmethod. \n</p>"}, "author": "Kai Han, Chao Li, Xin Shi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068152", "id": "tag:google.com,2005:reader/item/00000003250c3949", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stability and Generalization of Learning Algorithms that Converge to Global Optima. (arXiv:1710.08402v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08402"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08402", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We establish novel generalization bounds for learning algorithms that \nconverge to global minima. We do so by deriving black-box stability results \nthat only depend on the convergence of a learning algorithm and the geometry \naround the minimizers of the loss function. The results are shown for nonconvex \nloss functions satisfying the Polyak-{\\L}ojasiewicz (PL) and the quadratic \ngrowth (QG) conditions. We further show that these conditions arise for some \nneural networks with linear activations. We use our black-box results to \nestablish the stability of optimization algorithms such as stochastic gradient \ndescent (SGD), gradient descent (GD), randomized coordinate descent (RCD), and \nthe stochastic variance reduced gradient method (SVRG), in both the PL and the \nstrongly convex setting. Our results match or improve state-of-the-art \ngeneralization bounds and can easily be extended to similar optimization \nalgorithms. Finally, we show that although our results imply comparable \nstability for SGD and GD in the PL setting, there exist simple neural networks \nwith multiple local minima where SGD is stable but GD is not. \n</p>"}, "author": "Zachary Charles, Dimitris Papailiopoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068149", "id": "tag:google.com,2005:reader/item/00000003250c399c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimal Rates for Multi-pass Stochastic Gradient Methods. (arXiv:1605.08882v2 [cs.LG] UPDATED)", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1605.08882"}], "alternate": [{"href": "http://arxiv.org/abs/1605.08882", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We analyze the learning properties of the stochastic gradient method when \nmultiple passes over the data and mini-batches are allowed. We study how \nregularization properties are controlled by the step-size, the number of passes \nand the mini-batch size. In particular, we consider the square loss and show \nthat for a universal step-size choice, the number of passes acts as a \nregularization parameter, and optimal finite sample bounds can be achieved by \nearly-stopping. Moreover, we show that larger step-sizes are allowed when \nconsidering mini-batches. Our analysis is based on a unifying approach, \nencompassing both batch and stochastic gradient methods as special cases. As a \nbyproduct, we derive optimal convergence results for batch gradient methods \n(even in the non-attainable cases). \n</p>"}, "author": "Junhong Lin, Lorenzo Rosasco", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068131", "id": "tag:google.com,2005:reader/item/00000003250c3e87", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Feeling of Success: Does Touch Sensing Help Predict Grasp Outcomes?. (arXiv:1710.05512v1 [cs.RO] CROSS LISTED)", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05512"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05512", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A successful grasp requires careful balancing of the contact forces. Deducing \nwhether a particular grasp will be successful from indirect measurements, such \nas vision, is therefore quite challenging, and direct sensing of contacts \nthrough touch sensing provides an appealing avenue toward more successful and \nconsistent robotic grasping. However, in order to fully evaluate the value of \ntouch sensing for grasp outcome prediction, we must understand how touch \nsensing can influence outcome prediction accuracy when combined with other \nmodalities. Doing so using conventional model-based techniques is exceptionally \ndifficult. In this work, we investigate the question of whether touch sensing \naids in predicting grasp outcomes within a multimodal sensing framework that \ncombines vision and touch. To that end, we collected more than 9,000 grasping \ntrials using a two-finger gripper equipped with GelSight high-resolution \ntactile sensors on each finger, and evaluated visuo-tactile deep neural network \nmodels to directly predict grasp outcomes from either modality individually, \nand from both modalities together. Our experimental results indicate that \nincorporating tactile readings substantially improve grasping performance. \n</p>"}, "author": "Roberto Calandra, Andrew Owens, Manu Upadhyaya, Wenzhen Yuan, Justin Lin, Edward H. Adelson, Sergey Levine", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508807308329", "timestampUsec": "1508807308328718", "id": "tag:google.com,2005:reader/item/0000000324a850b1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Point Neurons with Conductance-Based Synapses in the Neural Engineering Framework. (arXiv:1710.07659v1 [q-bio.NC])", "published": 1508807308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07659"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07659", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The mathematical model underlying the Neural Engineering Framework (NEF) \nexpresses neuronal input as a linear combination of synaptic currents. However, \nin biology, synapses are not perfect current sources and are thus nonlinear. \nDetailed synapse models are based on channel conductances instead of currents, \nwhich require independent handling of excitatory and inhibitory synapses. This, \nin particular, significantly affects the influence of inhibitory signals on the \nneuronal dynamics. In this technical report we first summarize the relevant \nportions of the NEF and conductance-based synapse models. We then discuss a \nna\\\"ive translation between populations of LIF neurons with current- and \nconductance-based synapses based on an estimation of an average membrane \npotential. Experiments show that this simple approach works relatively well for \nfeed-forward communication channels, yet performance degrades for NEF networks \ndescribing more complex dynamics, such as integration. \n</p>"}, "author": "Andreas St&#xf6;ckel, Aaron R. Voelker, Chris Eliasmith", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508807308329", "timestampUsec": "1508807308328717", "id": "tag:google.com,2005:reader/item/0000000324a850c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Superposed Episodic and Semantic Memory via Sparse Distributed Representation. (arXiv:1710.07829v1 [cs.NE])", "published": 1508807308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07829"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07829", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cc15204\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cc15204&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The abilities to perceive, learn, and use generalities, similarities, \nclasses, i.e., semantic memory (SM), is central to cognition. Machine learning \n(ML), neural network, and AI research has been primarily driven by tasks \nrequiring such abilities. However, another central facet of cognition, \nsingle-trial formation of permanent memories of experiences, i.e., episodic \nmemory (EM), has had relatively little focus. Only recently has EM-like \nfunctionality been added to Deep Learning (DL) models, e.g., Neural Turing \nMachine, Memory Networks. However, in these cases: a) EM is implemented as a \nseparate module, which entails substantial data movement (and so, time and \npower) between the DL net itself and EM; and b) individual items are stored \nlocalistically within the EM, precluding realizing the exponential \nrepresentational efficiency of distributed over localist coding. We describe \nSparsey, an unsupervised, hierarchical, spatial/spatiotemporal associative \nmemory model differing fundamentally from mainstream ML models, most crucially, \nin its use of sparse distributed representations (SDRs), or, cell assemblies, \nwhich admits an extremely efficient, single-trial learning algorithm that maps \ninput similarity into code space similarity (measured as intersection). SDRs of \nindividual inputs are stored in superposition and because similarity is \npreserved, the patterns of intersections over the assigned codes reflect the \nsimilarity, i.e., statistical, structure, of all orders, not simply pairwise, \nover the inputs. Thus, SM, i.e., a generative model, is built as a \ncomputationally free side effect of the act of storing episodic memory traces \nof individual inputs, either spatial patterns or sequences. We report initial \nresults on MNIST and on the Weizmann video event recognition benchmarks. While \nwe have not yet attained SOTA class accuracy, learning takes only minutes on a \nsingle CPU. \n</p>"}, "author": "Rod Rinkus, Jasmin Leveille", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508807308329", "timestampUsec": "1508807308328716", "id": "tag:google.com,2005:reader/item/0000000324a850e3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Moderate Environmental Variation Promotes Adaptation in Artificial Evolution. (arXiv:1710.07913v1 [cs.NE])", "published": 1508807308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07913"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07913", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cc94bfe\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cc94bfe&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper we analyze the role of environmental variations in the \nevolution of artificial agents situated in an external environment and we \ndemonstrate how environmental variations promote the evolution of better \nagents. The beneficial effect is maximized at intermediate rates of variations, \ni.e. when the dynamics of the environment displays a sufficient level of \nstability and variability. The analysis of the obtained results indicate that \nthe adaptive advantage provided by environmental variations is due to the fact \nthat it increases the rate with which evolving agents change phylogenetically. \nThe performance of the adaptive agents and the rate with which agents change \nphylogenetically are maximized at moderate rate of variations of the \nenvironment which provide a good tradeoff between environmental variation and \nstability. \n</p>"}, "author": "Nicola Milano, J&#xf4;nata Tyska Carvalho, Stefano Nolfi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508807308329", "timestampUsec": "1508807308328715", "id": "tag:google.com,2005:reader/item/0000000324a850f8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Progressive Learning for Systematic Design of Large Neural Networks. (arXiv:1710.08177v1 [cs.NE])", "published": 1508807308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08177"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08177", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We develop an algorithm for systematic design of a large artificial neural \nnetwork using a progression property. We find that some non-linear functions, \nsuch as the rectifier linear unit and its derivatives, hold the property. The \nsystematic design addresses the choice of network size and regularization of \nparameters. The number of nodes and layers in network increases in progression \nwith the objective of consistently reducing an appropriate cost. Each layer is \noptimized at a time, where appropriate parameters are learned using convex \noptimization. Regularization parameters for convex optimization do not need a \nsignificant manual effort for tuning. We also use random instances for some \nweight matrices, and that helps to reduce the number of parameters we learn. \nThe developed network is expected to show good generalization power due to \nappropriate regularization and use of random weights in the layers. This \nexpectation is verified by extensive experiments for classification and \nregression problems, using standard databases. \n</p>"}, "author": "Saikat Chatterjee, Alireza M. Javid, Mostafa Sadeghi, Partha P. Mitra, Mikael Skoglund", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508807308329", "timestampUsec": "1508807308328714", "id": "tag:google.com,2005:reader/item/0000000324a85105", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generic 3D Representation via Pose Estimation and Matching. (arXiv:1710.08247v1 [cs.CV])", "published": 1508807308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08247"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08247", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Though a large body of computer vision research has investigated developing \ngeneric semantic representations, efforts towards developing a similar \nrepresentation for 3D has been limited. In this paper, we learn a generic 3D \nrepresentation through solving a set of foundational proxy 3D tasks: \nobject-centric camera pose estimation and wide baseline feature matching. Our \nmethod is based upon the premise that by providing supervision over a set of \ncarefully selected foundational tasks, generalization to novel tasks and \nabstraction capabilities can be achieved. We empirically show that the internal \nrepresentation of a multi-task ConvNet trained to solve the above core problems \ngeneralizes to novel 3D tasks (e.g., scene layout estimation, object pose \nestimation, surface normal estimation) without the need for fine-tuning and \nshows traits of abstraction abilities (e.g., cross-modality pose estimation). \nIn the context of the core supervised tasks, we demonstrate our representation \nachieves state-of-the-art wide baseline feature matching results without \nrequiring apriori rectification (unlike SIFT and the majority of learned \nfeatures). We also show 6DOF camera pose estimation given a pair local image \npatches. The accuracy of both supervised tasks come comparable to humans. \nFinally, we contribute a large-scale dataset composed of object-centric street \nview scenes along with point correspondences and camera pose information, and \nconclude with a discussion on the learned representation and open research \nquestions. \n</p>"}, "author": "Amir R. Zamir, Tilman Wekel, Pulkit Argrawal, Colin Weil, Jitendra Malik, Silvio Savarese", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508807308329", "timestampUsec": "1508807308328713", "id": "tag:google.com,2005:reader/item/0000000324a85124", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning applied to Road Traffic Speed forecasting. (arXiv:1710.08266v1 [stat.AP])", "published": 1508807308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08266"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08266", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose deep learning architectures (FNN, CNN and LSTM) to \nforecast a regression model for time dependent data. These algorithm's are \ndesigned to handle Floating Car Data (FCD) historic speeds to predict road \ntraffic data. For this we aggregate the speeds into the network inputs in an \ninnovative way. We compare the RMSE thus obtained with the results of a simpler \nphysical model, and show that the latter achieves better RMSE accuracy. We also \npropose a new indicator, which evaluates the algorithms improvement when \ncompared to a benchmark prediction. We conclude by questioning the interest of \nusing deep learning methods for this specific regression task. \n</p>"}, "author": "Thomas Epelbaum (IPHT), Fabrice Gamboa (IMT), Jean-Michel Loubes (IMT), Jessica Martin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508807308329", "timestampUsec": "1508807308328712", "id": "tag:google.com,2005:reader/item/0000000324a85137", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Adversarial Domain Adaptation for Identifying Phase Transitions. (arXiv:1710.08382v1 [cond-mat.stat-mech])", "published": 1508807308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08382"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08382", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The identification of phases of matter is a challenging task, especially in \nquantum mechanics, where the complexity of the ground state appears to grow \nexponentially with the size of the system. We address this problem with \nstate-of-the-art deep learning techniques: adversarial domain adaptation. We \nderive the phase diagram of the whole parameter space starting from a fixed and \nknown subspace using unsupervised learning. The input data set contains both \nlabeled and unlabeled data instances. The first kind is a system that admits an \naccurate analytical or numerical solution, and one can recover its phase \ndiagram. The second type is the physical system with an unknown phase diagram. \nAdversarial domain adaptation uses both types of data to create invariant \nfeature extracting layers in a deep learning architecture. Once these layers \nare trained, we can attach an unsupervised learner to the network to find phase \ntransitions. We show the success of this technique by applying it on several \nparadigmatic models: the Ising model with different temperatures, the \nBose-Hubbard model, and the SSH model with disorder. The input is the ground \nstate without any manual feature engineering, and the dimension of the \nparameter space is unrestricted. The method finds unknown transitions \nsuccessfully and predicts transition points in close agreement with standard \nmethods. This study opens the door to the classification of physical systems \nwhere the phases boundaries are complex such as the many-body localization \nproblem or the Bose glass phase. \n</p>"}, "author": "Patrick Huembeli, Alexandre Dauphin, Peter Wittek", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508720882022", "timestampUsec": "1508720882022227", "id": "tag:google.com,2005:reader/item/0000000323fd2419", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning to Recognize Actions from Limited Training Examples Using a Recurrent Spiking Neural Model. (arXiv:1710.07354v1 [cs.NE])", "published": 1508720882, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07354"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07354", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A fundamental challenge in machine learning today is to build a model that \ncan learn from few examples. Here, we describe a reservoir based spiking neural \nmodel for learning to recognize actions with a limited number of labeled \nvideos. First, we propose a novel encoding, inspired by how microsaccades \ninfluence visual perception, to extract spike information from raw video data \nwhile preserving the temporal correlation across different frames. Using this \nencoding, we show that the reservoir generalizes its rich dynamical activity \ntoward signature action/movements enabling it to learn from few training \nexamples. We evaluate our approach on the UCF-101 dataset. Our experiments \ndemonstrate that our proposed reservoir achieves 81.3%/87% Top-1/Top-5 \naccuracy, respectively, on the 101-class data while requiring just 8 video \nexamples per class for training. Our results establish a new benchmark for \naction recognition from limited video examples for spiking neural models while \nyielding competetive accuracy with respect to state-of-the-art non-spiking \nneural models. \n</p>"}, "author": "Priyadarshini Panda, Narayan Srinivasa", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508720882022", "timestampUsec": "1508720882022226", "id": "tag:google.com,2005:reader/item/0000000323fd241f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning compressed representations of blood samples time series with missing data. (arXiv:1710.07547v1 [cs.NE])", "published": 1508720882, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07547"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07547", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Clinical measurements collected over time are naturally represented as \nmultivariate time series (MTS), which often contain missing data. An \nautoencoder can learn low dimensional vectorial representations of MTS that \npreserve important data characteristics, but cannot deal explicitly with \nmissing data. In this work, we propose a new framework that combines an \nautoencoder with the Time series Cluster Kernel (TCK), a kernel that accounts \nfor missingness patterns in MTS. Via kernel alignment, we incorporate TCK in \nthe autoencoder to improve the learned representations in presence of missing \ndata. We consider a classification problem of MTS with missing values, \nrepresenting blood samples of patients with surgical site infection. With our \napproach, rather than with a standard autoencoder, we learn representations in \nlow dimensions that can be classified better. \n</p>"}, "author": "Filippo Maria Bianchi, Karl &#xd8;yvind Mikalsen, Robert Jenssen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508719250276", "timestampUsec": "1508719250275721", "id": "tag:google.com,2005:reader/item/0000000323fac489", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Go game formal revealing by Ising model. (arXiv:1710.07360v1 [cs.AI])", "published": 1508719250, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07360"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07360", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Go gaming is a struggle for territory control between rival, black and white, \nstones on a board. We model the Go dynamics in a game by means of the Ising \nmodel whose interaction coefficients reflect essential rules and tactics \nemployed in Go to build long-term strategies. At any step of the game, the \nenergy functional of the model provides the control degree (strength) of a \nplayer over the board. A close fit between predictions of the model with actual \ngames is obtained. \n</p>"}, "author": "Mat&#xed;as Alvarado, Arturo Yee, Carlos Villarreal", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508719250276", "timestampUsec": "1508719250275720", "id": "tag:google.com,2005:reader/item/0000000323fac48f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Spoken Language Biomarkers for Detecting Cognitive Impairment. (arXiv:1710.07551v1 [cs.AI])", "published": 1508719250, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07551"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07551", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this study we developed an automated system that evaluates speech and \nlanguage features from audio recordings of neuropsychological examinations of \n92 subjects in the Framingham Heart Study. A total of 265 features were used in \nan elastic-net regularized binomial logistic regression model to classify the \npresence of cognitive impairment, and to select the most predictive features. \nWe compared performance with a demographic model from 6,258 subjects in the \ngreater study cohort (0.79 AUC), and found that a system that incorporated both \naudio and text features performed the best (0.92 AUC), with a True Positive \nRate of 29% (at 0% False Positive Rate) and a good model fit (Hosmer-Lemeshow \ntest &gt; 0.05). We also found that decreasing pitch and jitter, shorter segments \nof speech, and responses phrased as questions were positively associated with \ncognitive impairment. \n</p>"}, "author": "Tuka Alhanai, Rhoda Au, James Glass", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508719250276", "timestampUsec": "1508719250275719", "id": "tag:google.com,2005:reader/item/0000000323fac494", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Classification Driven Dynamic Image Enhancement. (arXiv:1710.07558v1 [cs.CV])", "published": 1508719250, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07558"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07558", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cc95066\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cc95066&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Convolutional neural networks rely on image texture and structure to serve as \ndiscriminative features to classify the image content. Image enhancement \ntechniques can be used as preprocessing steps to help improve the overall image \nquality and in turn improve the overall effectiveness of a CNN. Existing image \nenhancement methods, however, are designed to improve the perceptual quality of \nan image for a human observer. In this paper, we are interested in learning \nCNNs that can emulate image enhancement and restoration, but with the overall \ngoal to improve image classification and not necessarily human perception. To \nthis end, we present a unified CNN architecture that uses a range of \nenhancement filters that can enhance image-specific details via end-to-end \ndynamic filter learning. We demonstrate the effectiveness of this strategy on \nfour challenging benchmark datasets for fine-grained, object, scene and texture \nclassification: CUB-200-2011, PASCAL-VOC2007, MIT-Indoor, and DTD. Experiments \nusing our proposed enhancement shows promising results on all the datasets. In \naddition, our approach is capable of improving the performance of all generic \nCNN architectures. \n</p>"}, "author": "Vivek Sharma, Ali Diba, Davy Neven, Michael S. Brown, Luc Van Gool, Rainer Stiefelhagen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288529", "id": "tag:google.com,2005:reader/item/0000000323fa7133", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Machine Learning as Statistical Data Assimilation. (arXiv:1710.07276v1 [cs.LG])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07276"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07276", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We identify a strong equivalence between neural network based machine \nlearning (ML) methods and the formulation of statistical data assimilation \n(DA), known to be a problem in statistical physics. DA, as used widely in \nphysical and biological sciences, systematically transfers information in \nobservations to a model of the processes producing the observations. The \ncorrespondence is that layer label in the ML setting is the analog of time in \nthe data assimilation setting. Utilizing aspects of this equivalence we discuss \nhow to establish the global minimum of the cost functions in the ML context, \nusing a variational annealing method from DA. This provides a design method for \noptimal networks for ML applications and may serve as the basis for \nunderstanding the success of \"deep learning\". Results from an ML example are \npresented. \n</p> \n<p>When the layer label is taken to be continuous, the Euler-Lagrange equation \nfor the ML optimization problem is an ordinary differential equation, and we \nsee that the problem being solved is a two point boundary value problem. The \nuse of continuous layers is denoted \"deepest learning\". The Hamiltonian version \nprovides a direct rationale for back propagation as a solution method for the \ncanonical momentum; however, it suggests other solution methods are to be \npreferred. \n</p>"}, "author": "H. D. I. Abarbanel, P. J. Rozdeba, S. Shirman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288528", "id": "tag:google.com,2005:reader/item/0000000323fa713f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Decomposition of Uncertainty for Active Learning and Reliable Reinforcement Learning in Stochastic Systems. (arXiv:1710.07283v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07283"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07283", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Bayesian neural networks (BNNs) with latent variables are probabilistic \nmodels which can automatically identify complex stochastic patterns in the \ndata. We study in these models a decomposition of predictive uncertainty into \nits epistemic and aleatoric components. We show how such a decomposition arises \nnaturally in a Bayesian active learning scenario and develop a new objective \nfor reliable reinforcement learning (RL) with an epistemic and aleatoric risk \nelement. Our experiments illustrate the usefulness of the resulting \ndecomposition in active learning and reliable RL. \n</p>"}, "author": "Stefan Depeweg, Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato, Finale Doshi-Velez, Steffen Udluft", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288527", "id": "tag:google.com,2005:reader/item/0000000323fa7144", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Power Plant Performance Modeling with Concept Drift. (arXiv:1710.07314v1 [cs.LG])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07314"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07314", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Power plant is a complex and nonstationary system for which the traditional \nmachine learning modeling approaches fall short of expectations. The \nensemble-based online learning methods provide an effective way to continuously \nlearn from the dynamic environment and autonomously update models to respond to \nenvironmental changes. This paper proposes such an online ensemble regression \napproach to model power plant performance, which is critically important for \noperation optimization. The experimental results on both simulated and real \ndata show that the proposed method can achieve performance with less than 1% \nmean average percentage error, which meets the general expectations in field \noperations. \n</p>"}, "author": "Rui Xu, Yunwen Xu, Weizhong Yan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288526", "id": "tag:google.com,2005:reader/item/0000000323fa714f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Visual Integration of Data and Model Space in Ensemble Learning. (arXiv:1710.07322v1 [cs.HC])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07322"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07322", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Ensembles of classifier models typically deliver superior performance and can \noutperform single classifier models given a dataset and classification task at \nhand. However, the gain in performance comes together with the lack in \ncomprehensibility, posing a challenge to understand how each model affects the \nclassification outputs and where the errors come from. We propose a tight \nvisual integration of the data and the model space for exploring and combining \nclassifier models. We introduce a workflow that builds upon the visual \nintegration and enables the effective exploration of classification outputs and \nmodels. We then present a use case in which we start with an ensemble \nautomatically selected by a standard ensemble selection algorithm, and show how \nwe can manipulate models and alternative combinations. \n</p>"}, "author": "Bruno Schneider, Dominik J&#xe4;ckle, Florian Stoffel, Alexandra Diehl, Johannes Fuchs, Daniel Keim", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288525", "id": "tag:google.com,2005:reader/item/0000000323fa7156", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition. (arXiv:1710.07324v1 [cs.LG])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07324"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07324", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a method (TT-GP) for approximate inference in Gaussian Process \n(GP) models. We build on previous scalable GP research including stochastic \nvariational inference based on inducing inputs, kernel interpolation, and \nstructure exploiting algebra. The key idea of our method is to use Tensor Train \ndecomposition for variational parameters, which allows us to train GPs with \nbillions of inducing inputs and achieve state-of-the-art results on several \nbenchmarks. Further, our approach allows for training kernels based on deep \nneural networks without any modifications to the underlying GP model. A neural \nnetwork learns a multidimensional embedding for the data, which is used by the \nGP to make the final prediction. We train GP and neural network parameters \nend-to-end without pretraining, through maximization of GP marginal likelihood. \nWe show the efficiency of the proposed approach on several regression and \nclassification benchmark datasets including MNIST, CIFAR-10, and Airline. \n</p>"}, "author": "Pavel Izmailov, Alexander Novikov, Dmitry Kropotov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288524", "id": "tag:google.com,2005:reader/item/0000000323fa7167", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Frequency Based Index Estimating the Subclusters' Connection Strength. (arXiv:1710.07340v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07340"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07340", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, a frequency coefficient based on the Sen-Shorrocks-Thon (SST) \npoverty index notion is proposed. The clustering SST index can be used as the \nmethod for determination of the connection between similar neighbor \nsub-clusters. Consequently, connections can reveal existence of natural \nhomogeneous. Through estimation of the connection strength, we can also verify \ninformation about the estimated number of natural clusters that is necessary \nassumption of efficient market segmentation and campaign management and \nfinancial decisions. The index can be used as the complementary tool for the \nU-matrix visualization. The index is tested on an artificial dataset with known \nparameters and compared with results obtained by the Unified-distance matrix \nmethod. \n</p>"}, "author": "Lukas Pastorek", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288523", "id": "tag:google.com,2005:reader/item/0000000323fa716c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Linear-Time Algorithm in Bayesian Image Denoising based on Gaussian Markov Random Field. (arXiv:1710.07393v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07393"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07393", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we consider Bayesian image denoising based on a Gaussian \nMarkov random field (GMRF) model, for which we propose an new algorithm. Our \nmethod can solve Bayesian image denoising problems, including hyperparameter \nestimation, in $O(n)$-time, where $n$ is the number of pixels in a given image. \nFrom the perspective of the order of the computational time, this is a \nstate-of-the-art algorithm for the present problem setting. Moreover, the \nresults of our numerical experiments we show our method is in fact effective in \npractice. \n</p>"}, "author": "Muneki Yasuda, Junpei Watanabe, Shun Kataoka, kazuyuki Tanaka", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288522", "id": "tag:google.com,2005:reader/item/0000000323fa7170", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Ligand Pose Optimization with Atomic Grid-Based Convolutional Neural Networks. (arXiv:1710.07400v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07400"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07400", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Docking is an important tool in computational drug discovery that aims to \npredict the binding pose of a ligand to a target protein through a combination \nof pose scoring and optimization. A scoring function that is differentiable \nwith respect to atom positions can be used for both scoring and gradient-based \noptimization of poses for docking. Using a differentiable grid-based atomic \nrepresentation as input, we demonstrate that a scoring function learned by \ntraining a convolutional neural network (CNN) to identify binding poses can \nalso be applied to pose optimization. We also show that an iteratively-trained \nCNN that includes poses optimized by the first CNN in its training set performs \neven better at optimizing randomly initialized poses than either the first CNN \nscoring function or AutoDock Vina. \n</p>"}, "author": "Matthew Ragoza, Lillian Turner, David Ryan Koes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288521", "id": "tag:google.com,2005:reader/item/0000000323fa7173", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "First-order Methods Almost Always Avoid Saddle Points. (arXiv:1710.07406v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07406"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07406", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We establish that first-order methods avoid saddle points for almost all \ninitializations. Our results apply to a wide variety of first-order methods, \nincluding gradient descent, block coordinate descent, mirror descent and \nvariants thereof. The connecting thread is that such algorithms can be studied \nfrom a dynamical systems perspective in which appropriate instantiations of the \nStable Manifold Theorem allow for a global stability analysis. Thus, neither \naccess to second-order derivative information nor randomness beyond \ninitialization is necessary to provably avoid saddle points. \n</p>"}, "author": "Jason D. Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I. Jordan, Benjamin Recht", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288520", "id": "tag:google.com,2005:reader/item/0000000323fa7177", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Differentially Private Empirical Risk Minimization with Input Perturbation. (arXiv:1710.07425v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07425"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07425", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cc954d8\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cc954d8&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a novel framework for the differentially private ERM, input \nperturbation. Existing differentially private ERM implicitly assumed that the \ndata contributors submit their private data to a database expecting that the \ndatabase invokes a differentially private mechanism for publication of the \nlearned model. In input perturbation, each data contributor independently \nrandomizes her/his data by itself and submits the perturbed data to the \ndatabase. We show that the input perturbation framework theoretically \nguarantees that the model learned with the randomized data eventually satisfies \ndifferential privacy with the prescribed privacy parameters. At the same time, \ninput perturbation guarantees that local differential privacy is guaranteed to \nthe server. We also show that the excess risk bound of the model learned with \ninput perturbation is $O(1/n)$ under a certain condition, where $n$ is the \nsample size. This is the same as the excess risk bound of the state-of-the-art. \n</p>"}, "author": "Kazuto Fukuchi, Quang Khai Tran, Jun Sakuma", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288519", "id": "tag:google.com,2005:reader/item/0000000323fa717a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Distributed Deep Transfer Learning by Basic Probability Assignment. (arXiv:1710.07437v1 [cs.LG])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07437"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07437", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cd0d818\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cd0d818&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Transfer learning is a popular practice in deep neural networks, but \nfine-tuning of large number of parameters is a hard task due to the complex \nwiring of neurons between splitting layers and imbalance distributions of data \nin pretrained and transferred domains. The reconstruction of the original \nwiring for the target domain is a heavy burden due to the size of \ninterconnections across neurons. We propose a distributed scheme that tunes the \nconvolutional filters individually while backpropagates them jointly by means \nof basic probability assignment. Some of the most recent advances in evidence \ntheory show that in a vast variety of the imbalanced regimes, optimizing of \nsome proper objective functions derived from contingency matrices prevents \nbiases towards high-prior class distributions. Therefore, the original filters \nget gradually transferred based on individual contributions to overall \nperformance of the target domain. This largely reduces the expected complexity \nof transfer learning whilst highly improves precision. Our experiments on \nstandard benchmarks and scenarios confirm the consistent improvement of our \ndistributed deep transfer learning strategy. \n</p>"}, "author": "Arash Shahriari", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288518", "id": "tag:google.com,2005:reader/item/0000000323fa717b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Unified Backpropagation for Multi-Objective Deep Learning. (arXiv:1710.07438v1 [cs.LG])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07438"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07438", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A common practice in most of deep convolutional neural architectures is to \nemploy fully-connected layers followed by Softmax activation to minimize \ncross-entropy loss for the sake of classification. Recent studies show that \nsubstitution or addition of the Softmax objective to the cost functions of \nsupport vector machines or linear discriminant analysis is highly beneficial to \nimprove the classification performance in hybrid neural networks. We propose a \nnovel paradigm to link the optimization of several hybrid objectives through \nunified backpropagation. This highly alleviates the burden of extensive \nboosting for independent objective functions or complex formulation of \nmultiobjective gradients. Hybrid loss functions are linked by basic probability \nassignment from evidence theory. We conduct our experiments for a variety of \nscenarios and standard datasets to evaluate the advantage of our proposed \nunification approach to deliver consistent improvements into the classification \nperformance of deep convolutional neural networks. \n</p>"}, "author": "Arash Shahriari", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288517", "id": "tag:google.com,2005:reader/item/0000000323fa717e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Finite-dimensional Gaussian approximation with linear inequality constraints. (arXiv:1710.07453v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07453"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07453", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Introducing inequality constraints in Gaussian process (GP) models can lead \nto more realistic uncertainties in learning a great variety of real-world \nproblems. We consider the finite-dimensional Gaussian approach from Maatouk and \nBay (2017) which can satisfy inequality conditions everywhere (either \nboundedness, monotonicity or convexity). Our contributions are threefold. \nFirst, we extend their approach in order to deal with general sets of linear \ninequalities. Second, we explore several Markov Chain Monte Carlo (MCMC) \ntechniques to approximate the posterior distribution. Third, we investigate \ntheoretical and numerical properties of the constrained likelihood for \ncovariance parameter estimation. According to experiments on both artificial \nand real data, our full framework together with a Hamiltonian Monte Carlo-based \nsampler provides efficient results on both data fitting and uncertainty \nquantification. \n</p>"}, "author": "Andr&#xe9;s F. L&#xf3;pez-Lopera, Fran&#xe7;ois Bachoc, Nicolas Durrande, Olivier Roustant", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288516", "id": "tag:google.com,2005:reader/item/0000000323fa7183", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Learning Wasserstein Embeddings. (arXiv:1710.07457v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07457"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07457", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Wasserstein distance received a lot of attention recently in the \ncommunity of machine learning, especially for its principled way of comparing \ndistributions. It has found numerous applications in several hard problems, \nsuch as domain adaptation, dimensionality reduction or generative models. \nHowever, its use is still limited by a heavy computational cost. Our goal is to \nalleviate this problem by providing an approximation mechanism that allows to \nbreak its inherent complexity. It relies on the search of an embedding where \nthe Euclidean distance mimics the Wasserstein distance. We show that such an \nembedding can be found with a siamese architecture associated with a decoder \nnetwork that allows to move from the embedding space back to the original input \nspace. Once this embedding has been found, computing optimization problems in \nthe Wasserstein space (e.g. barycenters, principal directions or even \narchetypes) can be conducted extremely fast. Numerical experiments supporting \nthis idea are conducted on image datasets, and show the wide potential benefits \nof our method. \n</p>"}, "author": "Nicolas Courty, R&#xe9;mi Flamary, M&#xe9;lanie Ducoffe", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288515", "id": "tag:google.com,2005:reader/item/0000000323fa718b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Tracking the gradients using the Hessian: A new look at variance reducing stochastic methods. (arXiv:1710.07462v1 [math.OC])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07462"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07462", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Our goal is to improve variance reducing stochastic methods through better \ncontrol variates. We first propose a modification of SVRG which uses the \nHessian to track gradients over time, rather than to recondition, increasing \nthe correlation of the control variates and leading to faster theoretical \nconvergence close to the optimum. We then propose accurate and computationally \nefficient approximations to the Hessian, both using a diagonal and a low-rank \nmatrix. Finally, we demonstrate the effectiveness of our method on a wide range \nof problems. \n</p>"}, "author": "Robert M. Gower, Nicolas Le Roux, Francis Bach", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288514", "id": "tag:google.com,2005:reader/item/0000000323fa7190", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Dynamic classifier chains for multi-label learning. (arXiv:1710.07491v1 [cs.LG])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07491"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07491", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we deal with the task of building a dynamic ensemble of chain \nclassifiers for multi-label classification. To do so, we proposed two concepts \nof classifier chains algorithms that are able to change label order of the \nchain without rebuilding the entire model. Such modes allows anticipating the \ninstance-specific chain order without a significant increase in computational \nburden. The proposed chain models are built using the Naive Bayes classifier \nand nearest neighbour approach as a base single-label classifiers. To take the \nbenefits of the proposed algorithms, we developed a simple heuristic that \nallows the system to find relatively good label order. The heuristic sort \nlabels according to the label-specific classification quality gained during the \nvalidation phase. The heuristic tries to minimise the phenomenon of error \npropagation in the chain. The experimental results showed that the proposed \nmodel based on Naive Bayes classifier the above-mentioned heuristic is an \nefficient tool for building dynamic chain classifiers. \n</p>"}, "author": "Pawel Trajdos, Marek Kurzynski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288513", "id": "tag:google.com,2005:reader/item/0000000323fa7197", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning compressed representations of blood samples time series with missing data. (arXiv:1710.07547v1 [cs.NE])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07547"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07547", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Clinical measurements collected over time are naturally represented as \nmultivariate time series (MTS), which often contain missing data. An \nautoencoder can learn low dimensional vectorial representations of MTS that \npreserve important data characteristics, but cannot deal explicitly with \nmissing data. In this work, we propose a new framework that combines an \nautoencoder with the Time series Cluster Kernel (TCK), a kernel that accounts \nfor missingness patterns in MTS. Via kernel alignment, we incorporate TCK in \nthe autoencoder to improve the learned representations in presence of missing \ndata. We consider a classification problem of MTS with missing values, \nrepresenting blood samples of patients with surgical site infection. With our \napproach, rather than with a standard autoencoder, we learn representations in \nlow dimensions that can be classified better. \n</p>"}, "author": "Filippo Maria Bianchi, Karl &#xd8;yvind Mikalsen, Robert Jenssen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288512", "id": "tag:google.com,2005:reader/item/0000000323fa719a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Belief Propagation Min-Sum Algorithm for Generalized Min-Cost Network Flow. (arXiv:1710.07600v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07600"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07600", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Belief Propagation algorithms are instruments used broadly to solve graphical \nmodel optimization and statistical inference problems. In the general case of a \nloopy Graphical Model, Belief Propagation is a heuristic which is quite \nsuccessful in practice, even though its empirical success, typically, lacks \ntheoretical guarantees. This paper extends the short list of special cases \nwhere correctness and/or convergence of a Belief Propagation algorithm is \nproven. We generalize formulation of Min-Sum Network Flow problem by relaxing \nthe flow conservation (balance) constraints and then proving that the Belief \nPropagation algorithm converges to the exact result. \n</p>"}, "author": "Andrii Riazanov, Yury Maximov, Michael Chertkov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508461685574", "timestampUsec": "1508461685573759", "id": "tag:google.com,2005:reader/item/00000003224e8cc3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Protein Folding Optimization using Differential Evolution Extended with Local Search and Component Reinitialization. (arXiv:1710.07031v1 [cs.AI])", "published": 1508461686, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07031"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07031", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a novel differential evolution algorithm for protein \nfolding optimization that is applied to a three-dimensional AB off-lattice \nmodel. The proposed algorithm includes two new mechanisms. A local search is \nused to improve convergence speed and to reduce the runtime complexity of the \nenergy calculation. For this purpose, a local movement is introduced within the \nlocal search. The designed evolutionary algorithm has fast convergence and, \ntherefore, when it is trapped into local optimum or a relatively good solution \nis located, it is hard to locate a better similar solution. The similar \nsolution is different from the good solution in only a few components. A \ncomponent reinitialization method is designed to mitigate this problem. Both \nthe new mechanisms and the proposed algorithm were analyzed on well-known \namino-acid sequences that are used frequently in the literature. Experimental \nresults show that the employed new mechanisms improve the efficiency of our \nalgorithm and the proposed algorithm is superior to other state-of-the-art \nalgorithms. It obtained a hit ratio of 100 % for sequences up to 18 monomers \nwithin a budget of $10^{11}$ solution evaluations. New best-known solutions \nwere obtained for most of the sequences. The existence of the symmetric \nbest-known solutions is also demonstrated in the paper. \n</p>"}, "author": "Borko Bo&#x161;kovi&#x107;, Janez Brest", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361826", "id": "tag:google.com,2005:reader/item/00000003224b3326", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Graph Embedding with Rich Information through Bipartite Heterogeneous Network. (arXiv:1710.06879v1 [cs.AI])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06879"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06879", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cd0dab9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cd0dab9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Graph embedding has attracted increasing attention due to its critical \napplication in social network analysis. Most existing algorithms for graph \nembedding only rely on the typology information and fail to use the copious \ninformation in nodes as well as edges. As a result, their performance for many \ntasks may not be satisfactory. In this paper, we proposed a novel and general \nframework of representation learning for graph with rich text information \nthrough constructing a bipartite heterogeneous network. Specially, we designed \na biased random walk to explore the constructed heterogeneous network with the \nnotion of flexible neighborhood. The efficacy of our method is demonstrated by \nextensive comparison experiments with several baselines on various datasets. It \nimproves the Micro-F1 and Macro-F1 of node classification by 10% and 7% on Cora \ndataset. \n</p>"}, "author": "Guolei Sun, Xiangliang Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361825", "id": "tag:google.com,2005:reader/item/00000003224b3328", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Emergent Translation in Multi-Agent Communication. (arXiv:1710.06922v1 [cs.CL])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06922"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06922", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>While most machine translation systems to date are trained on large parallel \ncorpora, humans learn language in a different way: by being grounded in an \nenvironment and interacting with other humans. In this work, we propose a \ncommunication game where two agents, native speakers of their own respective \nlanguages, jointly learn to solve a visual referential task. We find that the \nability to understand and translate a foreign language emerges as a means to \nachieve shared goals. The emergent translation is interactive and multimodal, \nand crucially does not require parallel corpora, but only monolingual, \nindependent text and corresponding images. Our proposed translation model \nachieves this by grounding the source and target languages into a shared visual \nmodality, and outperforms several baselines on both word-level and \nsentence-level translation tasks. Furthermore, we show that agents in a \nmultilingual community learn to translate better and faster than in a bilingual \ncommunication setting. \n</p>"}, "author": "Jason Lee, Kyunghyun Cho, Jason Weston, Douwe Kiela", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361824", "id": "tag:google.com,2005:reader/item/00000003224b332b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Adapting general-purpose speech recognition engine output for domain-specific natural language question answering. (arXiv:1710.06923v1 [cs.CL])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06923"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06923", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Speech-based natural language question-answering interfaces to enterprise \nsystems are gaining a lot of attention. General-purpose speech engines can be \nintegrated with NLP systems to provide such interfaces. Usually, \ngeneral-purpose speech engines are trained on large `general' corpus. However, \nwhen such engines are used for specific domains, they may not recognize \ndomain-specific words well, and may produce erroneous output. Further, the \naccent and the environmental conditions in which the speaker speaks a sentence \nmay induce the speech engine to inaccurately recognize certain words. The \nsubsequent natural language question-answering does not produce the requisite \nresults as the question does not accurately represent what the speaker \nintended. Thus, the speech engine's output may need to be adapted for a domain \nbefore further natural language processing is carried out. We present two \nmechanisms for such an adaptation, one based on evolutionary development and \nthe other based on machine learning, and show how we can repair the \nspeech-output to make the subsequent natural language question-answering \nbetter. \n</p>"}, "author": "C. Anantaram, Sunil Kumar Kopparapu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361823", "id": "tag:google.com,2005:reader/item/00000003224b332f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Consequentialist conditional cooperation in social dilemmas with imperfect information. (arXiv:1710.06975v1 [cs.AI])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06975"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06975", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Social dilemmas, where mutual cooperation can lead to high payoffs but \nparticipants face incentives to cheat, are ubiquitous in multi-agent \ninteraction. We wish to construct agents that cooperate with pure cooperators, \navoid exploitation by pure defectors, and incentivize cooperation from the \nrest. However, often the actions taken by a partner are (partially) unobserved \nor the consequences of individual actions are hard to predict. We show that in \na large class of games good strategies can be constructed by conditioning one's \nbehavior solely on outcomes (ie. one's past rewards). We call this \nconsequentialist conditional cooperation. We show how to construct such \nstrategies using deep reinforcement learning techniques and demonstrate, both \nanalytically and experimentally, that they are effective in social dilemmas \nbeyond simple matrix games. We also show the limitations of relying purely on \nconsequences and discuss the need for understanding both the consequences of \nand the intentions behind an action. \n</p>"}, "author": "Alexander Peysakhovich, Adam Lerer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361822", "id": "tag:google.com,2005:reader/item/00000003224b3333", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Protein Folding Optimization using Differential Evolution Extended with Local Search and Component Reinitialization. (arXiv:1710.07031v1 [cs.AI])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07031"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07031", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a novel differential evolution algorithm for protein \nfolding optimization that is applied to a three-dimensional AB off-lattice \nmodel. The proposed algorithm includes two new mechanisms. A local search is \nused to improve convergence speed and to reduce the runtime complexity of the \nenergy calculation. For this purpose, a local movement is introduced within the \nlocal search. The designed evolutionary algorithm has fast convergence and, \ntherefore, when it is trapped into local optimum or a relatively good solution \nis located, it is hard to locate a better similar solution. The similar \nsolution is different from the good solution in only a few components. A \ncomponent reinitialization method is designed to mitigate this problem. Both \nthe new mechanisms and the proposed algorithm were analyzed on well-known \namino-acid sequences that are used frequently in the literature. Experimental \nresults show that the employed new mechanisms improve the efficiency of our \nalgorithm and the proposed algorithm is superior to other state-of-the-art \nalgorithms. It obtained a hit ratio of 100 % for sequences up to 18 monomers \nwithin a budget of $10^{11}$ solution evaluations. New best-known solutions \nwere obtained for most of the sequences. The existence of the symmetric \nbest-known solutions is also demonstrated in the paper. \n</p>"}, "author": "Borko Bo&#x161;kovi&#x107;, Janez Brest", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361821", "id": "tag:google.com,2005:reader/item/00000003224b3336", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Decision Trees for Helpdesk Advisor Graphs. (arXiv:1710.07075v1 [cs.AI])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07075"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07075", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We use decision trees to build a helpdesk agent reference network to \nfacilitate the on-the-job advising of junior or less experienced staff on how \nto better address telecommunication customer fault reports. Such reports \ngenerate field measurements and remote measurements which, when coupled with \nlocation data and client attributes, and fused with organization-level \nstatistics, can produce models of how support should be provided. Beyond \ndecision support, these models can help identify staff who can act as advisors, \nbased on the quality, consistency and predictability of dealing with complex \ntroubleshooting reports. Advisor staff models are then used to guide less \nexperienced staff in their decision making; thus, we advocate the deployment of \na simple mechanism which exploits the availability of staff with a sound track \nrecord at the helpdesk to act as dormant tutors. \n</p>"}, "author": "Spyros Gkezerlis, Dimitris Kalles", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361820", "id": "tag:google.com,2005:reader/item/00000003224b333b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Swift Linked Data Miner: Mining OWL 2 EL class expressions directly from online RDF datasets. (arXiv:1710.07114v1 [cs.AI])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07114"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07114", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this study, we present Swift Linked Data Miner, an interruptible algorithm \nthat can directly mine an online Linked Data source (e.g., a SPARQL endpoint) \nfor OWL 2 EL class expressions to extend an ontology with new SubClassOf: \naxioms. The algorithm works by downloading only a small part of the Linked Data \nsource at a time, building a smart index in the memory and swiftly iterating \nover the index to mine axioms. We propose a transformation function from mined \naxioms to RDF Data Shapes. We show, by means of a crowdsourcing experiment, \nthat most of the axioms mined by Swift Linked Data Miner are correct and can be \nadded to an ontology. We provide a ready to use Prot\\'eg\\'e plugin implementing \nthe algorithm, to support ontology engineers in their daily modeling work. \n</p>"}, "author": "Jedrzej Potoniec, Piotr Jakubowski, Agnieszka &#x141;awrynowicz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361819", "id": "tag:google.com,2005:reader/item/00000003224b3340", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Two-Phase Safe Vehicle Routing and Scheduling Problem: Formulations and Solution Algorithms. (arXiv:1710.07147v1 [cs.AI])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07147"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07147", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a two phase time dependent vehicle routing and scheduling \noptimization model that identifies the safest routes, as a substitute for the \nclassical objectives given in the literature such as shortest distance or \ntravel time, through (1) avoiding recurring congestions, and (2) selecting \nroutes that have a lower probability of crash occurrences and non-recurring \ncongestion caused by those crashes. In the first phase, we solve a \nmixed-integer programming model which takes the dynamic speed variations into \naccount on a graph of roadway networks according to the time of day, and \nidentify the routing of a fleet and sequence of nodes on the safest feasible \npaths. Second phase considers each route as an independent transit path (fixed \nroute with fixed node sequences), and tries to avoid congestion by rescheduling \nthe departure times of each vehicle from each node, and by adjusting the \nsub-optimal speed on each arc. A modified simulated annealing (SA) algorithm is \nformulated to solve both complex models iteratively, which is found to be \ncapable of providing solutions in a considerably short amount of time. \n</p>"}, "author": "Aschkan Omidvar, Eren Erman Ozguven, O. Arda Vanli, R. Tavakkoli-Moghaddam", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361818", "id": "tag:google.com,2005:reader/item/00000003224b3342", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On Using Linear Diophantine Equations to Tune the extent of Look Ahead while Hiding Decision Tree Rules. (arXiv:1710.07214v1 [cs.AI])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07214"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07214", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper focuses on preserving the privacy of sensitive pat-terns when \ninducing decision trees. We adopt a record aug-mentation approach for hiding \nsensitive classification rules in binary datasets. Such a hiding methodology is \npreferred over other heuristic solutions like output perturbation or \ncrypto-graphic techniques - which restrict the usability of the data - since \nthe raw data itself is readily available for public use. In this paper, we \npropose a look ahead approach using linear Diophantine equations in order to \nadd the appropriate number of instances while minimally disturbing the initial \nentropy of the nodes. \n</p>"}, "author": "Georgios Feretzakis, Dimitris Kalles, Vassilios S. Verykios", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361815", "id": "tag:google.com,2005:reader/item/00000003224b3347", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Detecting Bias in Black-Box Models Using Transparent Model Distillation. (arXiv:1710.06169v1 [stat.ML] CROSS LISTED)", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06169"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06169", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Black-box risk scoring models permeate our lives, yet are typically \nproprietary and opaque. We propose a transparent model distillation approach to \nunderstand and detect bias in such models. Model distillation was originally \ndesigned to distill knowledge from a large, complex model (the teacher model) \nto a faster, simpler model (the student model) without significant loss in \nprediction accuracy. We add a third restriction - transparency - and show that \nit is possible to train transparent, yet still accurate student models to \nunderstand the predictions made by black-box teacher models. Central to our \napproach is the use of data sets that contain two labels to train on: the risk \nscore as well as the actual outcome the risk score was intended to predict. We \nfully characterize the asymptotic distribution of the difference between the \nrisk score and actual outcome models with variance estimates based on \nbootstrap-of-little-bags. This suggests a new method to detect bias in \nblack-box risk scores via assessing if contributions of protected features to \nthe risk score are statistically different from contributions to the actual \noutcome. \n</p>"}, "author": "Sarah Tan, Rich Caruana, Giles Hooker, Yin Lou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459465394", "timestampUsec": "1508459465393688", "id": "tag:google.com,2005:reader/item/00000003224b08ad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Bayesian Nonparametric Method for Clustering Imputation, and Forecasting in Multivariate Time Series. (arXiv:1710.06900v1 [stat.ME])", "published": 1508459465, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06900"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06900", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cd0dcf3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cd0dcf3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This article proposes a Bayesian nonparametric method for forecasting, \nimputation, and clustering in sparsely observed, multivariate time series. The \nmethod is appropriate for jointly modeling hundreds of time series with widely \nvarying, non-stationary dynamics. Given a collection of $N$ time series, the \nBayesian model first partitions them into independent clusters using a Chinese \nrestaurant process prior. Within a cluster, all time series are modeled jointly \nusing a novel \"temporally-coupled\" extension of the Chinese restaurant process \nmixture. Markov chain Monte Carlo techniques are used to obtain samples from \nthe posterior distribution, which are then used to form predictive inferences. \nWe apply the technique to challenging prediction and imputation tasks using \nseasonal flu data from the US Center for Disease Control and Prevention, \ndemonstrating competitive imputation performance and improved forecasting \naccuracy as compared to several state-of-the art baselines. We also show that \nthe model discovers interpretable clusters in datasets with hundreds of time \nseries using macroeconomic data from the Gapminder Foundation. \n</p>"}, "author": "Feras A. Saad, Vikash K. Mansinghka", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459465394", "timestampUsec": "1508459465393687", "id": "tag:google.com,2005:reader/item/00000003224b08b5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Characterization of Gradient Dominance and Regularity Conditions for Neural Networks. (arXiv:1710.06910v1 [stat.ML])", "published": 1508459465, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06910"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06910", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cd78b95\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cd78b95&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The past decade has witnessed a successful application of deep learning to \nsolving many challenging problems in machine learning and artificial \nintelligence. However, the loss functions of deep neural networks (especially \nnonlinear networks) are still far from being well understood from a theoretical \naspect. In this paper, we enrich the current understanding of the landscape of \nthe square loss functions for three types of neural networks. Specifically, \nwhen the parameter matrices are square, we provide an explicit characterization \nof the global minimizers for linear networks, linear residual networks, and \nnonlinear networks with one hidden layer. Then, we establish two quadratic \ntypes of landscape properties for the square loss of these neural networks, \ni.e., the gradient dominance condition within the neighborhood of their full \nrank global minimizers, and the regularity condition along certain directions \nand within the neighborhood of their global minimizers. These two landscape \nproperties are desirable for the optimization around the global minimizers of \nthe loss function for these neural networks. \n</p>"}, "author": "Yi Zhou, Yingbin Liang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459465394", "timestampUsec": "1508459465393686", "id": "tag:google.com,2005:reader/item/00000003224b08b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Concept Drift Learning with Alternating Learners. (arXiv:1710.06940v1 [cs.LG])", "published": 1508459465, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06940"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06940", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Data-driven predictive analytics are in use today across a number of \nindustrial applications, but further integration is hindered by the requirement \nof similarity among model training and test data distributions. This paper \naddresses the need of learning from possibly nonstationary data streams, or \nunder concept drift, a commonly seen phenomenon in practical applications. A \nsimple dual-learner ensemble strategy, alternating learners framework, is \nproposed. A long-memory model learns stable concepts from a long relevant time \nwindow, while a short-memory model learns transient concepts from a small \nrecent window. The difference in prediction performance of these two models is \nmonitored and induces an alternating policy to select, update and reset the two \nmodels. The method features an online updating mechanism to maintain the \nensemble accuracy, and a concept-dependent trigger to focus on relevant data. \nThrough empirical studies the method demonstrates effective tracking and \nprediction when the steaming data carry abrupt and/or gradual changes. \n</p>"}, "author": "Yunwen Xu, Rui Xu, Weizhong Yan, Paul Ardis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459465394", "timestampUsec": "1508459465393685", "id": "tag:google.com,2005:reader/item/00000003224b08bd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Asynchronous Decentralized Parallel Stochastic Gradient Descent. (arXiv:1710.06952v1 [math.OC])", "published": 1508459465, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06952"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06952", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent work shows that decentralized parallel stochastic gradient decent \n(D-PSGD) can outperform its centralized counterpart both theoretically and \npractically. While asynchronous parallelism is a powerful technology to improve \nthe efficiency of parallelism in distributed machine learning platforms and has \nbeen widely used in many popular machine learning softwares and solvers based \non centralized parallel protocols such as Tensorflow, it still remains unclear \nhow to apply the asynchronous parallelism to improve the efficiency of \ndecentralized parallel algorithms. This paper proposes an asynchronous \ndecentralize parallel stochastic gradient descent algorithm to apply the \nasynchronous parallelism technology to decentralized algorithms. Our \ntheoretical analysis provides the convergence rate or equivalently the \ncomputational complexity, which is consistent with many special cases and \nindicates we can achieve nice linear speedup when we increase the number of \nnodes or the batchsize. Extensive experiments in deep learning validate the \nproposed algorithm. \n</p>"}, "author": "Xiangru Lian, Wei Zhang, Ce Zhang, Ji Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459465394", "timestampUsec": "1508459465393684", "id": "tag:google.com,2005:reader/item/00000003224b08c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Minimax Estimation of Bandable Precision Matrices. (arXiv:1710.07006v1 [stat.ML])", "published": 1508459465, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07006"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07006", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The inverse covariance matrix provides considerable insight for understanding \nstatistical models in the multivariate setting. In particular, when the \ndistribution over variables is assumed to be multivariate normal, the sparsity \npattern in the inverse covariance matrix, commonly referred to as the precision \nmatrix, corresponds to the adjacency matrix representation of the Gauss-Markov \ngraph, which encodes conditional independence statements between variables. \nMinimax results under the spectral norm have previously been established for \ncovariance matrices, both sparse and banded, and for sparse precision matrices. \nWe establish minimax estimation bounds for estimating banded precision matrices \nunder the spectral norm. Our results greatly improve upon the existing bounds; \nin particular, we find that the minimax rate for estimating banded precision \nmatrices matches that of estimating banded covariance matrices. The key insight \nin our analysis is that we are able to obtain barely-noisy estimates of $k \n\\times k$ subblocks of the precision matrix by inverting slightly wider blocks \nof the empirical covariance matrix along the diagonal. Our theoretical results \nare complemented by experiments demonstrating the sharpness of our bounds. \n</p>"}, "author": "Addison Hu, Sahand Negahban", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459465394", "timestampUsec": "1508459465393683", "id": "tag:google.com,2005:reader/item/00000003224b08cd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Reti bayesiane per lo studio del fenomeno degli incidenti stradali tra i giovani in Toscana. (arXiv:1710.07066v1 [stat.AP])", "published": 1508459465, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07066"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07066", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper aims to analyse adolescents' road accidents in Tuscany. The \nanalysis is based on the Database Edit of Osservatorio di Epidemiologia della \nToscana. Complexity and heterogeneity of Edit's data represet an interesting \nscope to apply Machine Learning methods. In particular, in this paper is \nproposed an analysis based on a Bayesian probabilistic network, used to \ndiscover relationships between adolescents' characteristics and behaviours that \nare more often associated with an audacious driving style. The probabilistic \nnetwork developed by this study can be considered a useful starting point for \nfollow up reasearches, aiming to develop a causal network, a tool to limit this \nphenomenon. \n</p>"}, "author": "Filippo Elba, Lisa Gnaulati, Fabio Voeller", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459465394", "timestampUsec": "1508459465393682", "id": "tag:google.com,2005:reader/item/00000003224b08d5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Meta-Learning via Feature-Label Memory Network. (arXiv:1710.07110v1 [cs.LG])", "published": 1508459465, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07110"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07110", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning typically requires training a very capable architecture using \nlarge datasets. However, many important learning problems demand an ability to \ndraw valid inferences from small size datasets, and such problems pose a \nparticular challenge for deep learning. In this regard, various researches on \n\"meta-learning\" are being actively conducted. Recent work has suggested a \nMemory Augmented Neural Network (MANN) for meta-learning. MANN is an \nimplementation of a Neural Turing Machine (NTM) with the ability to rapidly \nassimilate new data in its memory, and use this data to make accurate \npredictions. In models such as MANN, the input data samples and their \nappropriate labels from previous step are bound together in the same memory \nlocations. This often leads to memory interference when performing a task as \nthese models have to retrieve a feature of an input from a certain memory \nlocation and read only the label information bound to that location. In this \npaper, we tried to address this issue by presenting a more robust MANN. We \nrevisited the idea of meta-learning and proposed a new memory augmented neural \nnetwork by explicitly splitting the external memory into feature and label \nmemories. The feature memory is used to store the features of input data \nsamples and the label memory stores their labels. Hence, when predicting the \nlabel of a given input, our model uses its feature memory unit as a reference \nto extract the stored feature of the input, and based on that feature, it \nretrieves the label information of the input from the label memory unit. In \norder for the network to function in this framework, a new memory-writingmodule \nto encode label information into the label memory in accordance with the \nmeta-learning task structure is designed. Here, we demonstrate that our model \noutperforms MANN by a large margin in supervised one-shot classification tasks \nusing Omniglot and MNIST datasets. \n</p>"}, "author": "Dawit Mureja, Hyunsin Park, Chang D. Yoo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459465394", "timestampUsec": "1508459465393681", "id": "tag:google.com,2005:reader/item/00000003224b08d9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Binary Classification from Positive-Confidence Data. (arXiv:1710.07138v1 [stat.ML])", "published": 1508459465, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07138"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07138", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Reducing labeling costs in supervised learning is a critical issue in many \npractical machine learning applications. In this paper, we consider \npositive-confidence (Pconf) classification, the problem of training a binary \nclassifier only from positive data equipped with confidence. Pconf \nclassification can be regarded as a discriminative extension of one-class \nclassification (which is aimed at \"describing\" the positive class), with \nability to tune hyper-parameters for \"classifying\" positive and negative \nsamples. Pconf classification is also related to positive-unlabeled (PU) \nclassification (which uses hard-labeled positive data and unlabeled data), \nallowing us to avoid estimating the class priors, which is a critical \nbottleneck in typical PU classification methods. For the Pconf classification \nproblem, we provide a simple empirical risk minimization framework and give a \nformulation for linear-in-parameter models that can be implemented easily and \ncomputationally efficiently. We also theoretically establish the consistency \nand generalization error bounds for Pconf classification, and demonstrate the \npractical usefulness of the proposed method through experiments. \n</p>"}, "author": "Takashi Ishida, Gang Niu, Masashi Sugiyama", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373889130", "timestampUsec": "1508373889130374", "id": "tag:google.com,2005:reader/item/0000000321a0cbea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Classification and Geometry of General Perceptual Manifolds. (arXiv:1710.06487v1 [cond-mat.dis-nn])", "published": 1508373890, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06487"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06487", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Perceptual manifolds arise when a neural population responds to an ensemble \nof sensory signals associated with different physical features (e.g., \norientation, pose, scale, location, and intensity) of the same perceptual \nobject. Object recognition and discrimination requires classifying the \nmanifolds in a manner that is insensitive to variability within a manifold. How \nneuronal systems give rise to invariant object classification and recognition \nis a fundamental problem in brain theory as well as in machine learning. Here \nwe study the ability of a readout network to classify objects from their \nperceptual manifold representations. We develop a statistical mechanical theory \nfor the linear classification of manifolds with arbitrary geometry revealing a \nremarkable relation to the mathematics of conic decomposition. Novel \ngeometrical measures of manifold radius and manifold dimension are introduced \nwhich can explain the classification capacity for manifolds of various \ngeometries. The general theory is demonstrated on a number of representative \nmanifolds, including L2 ellipsoids prototypical of strictly convex manifolds, \nL1 balls representing polytopes consisting of finite sample points, and \norientation manifolds which arise from neurons tuned to respond to a continuous \nangle variable, such as object orientation. The effects of label sparsity on \nthe classification capacity of manifolds are elucidated, revealing a scaling \nrelation between label sparsity and manifold radius. Theoretical predictions \nare corroborated by numerical simulations using recently developed algorithms \nto compute maximum margin solutions for manifold dichotomies. Our theory and \nits extensions provide a powerful and rich framework for applying statistical \nmechanics of linear classification to data arising from neuronal responses to \nobject stimuli, as well as to artificial deep networks trained for object \nrecognition tasks. \n</p>"}, "author": "SueYeon Chung, Daniel D. Lee, Haim Sompolinsky", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373889130", "timestampUsec": "1508373889130373", "id": "tag:google.com,2005:reader/item/0000000321a0cbf3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SQG-Differential Evolution for difficult optimization problems under a tight function evaluation budget. (arXiv:1710.06770v1 [cs.NE])", "published": 1508373890, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06770"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06770", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In the context of industrial engineering it is important to integrate \nefficient computational optimization methods in the product development \nprocess. Some of the most challenging simulation based engineering design \noptimization problems are characterized by: a large number of design variables, \nthe absence of analytical gradient information, highly non-linear objectives \nand a limited function evaluation budget. Although a huge variety of different \noptimization algorithms is available, the development and selection of \nefficient algorithms for problems with these industrial relevant \ncharacteristics, remains a challenge. In this communication a hybrid variant of \nDifferential Evolution (DE) is introduced which combines aspects of Stochastic \nQuasi-Gradient (SQG) methods within the framework of DE, in order to improve \noptimization efficiency on problems with the previously mentioned \ncharacteristics. The performance of the resulting method is compared with other \nstate-of-the-art DE variants on 25 commonly used test functions, under tight \nfunction evaluation budget constraints of 1000 evaluations. The experimental \nresults indicate that the proposed method performs particularly good on the \n\"difficult\" (high dimensional, multi-modal, inseparable) test functions. The \noperations used in the proposed mutation scheme, are computationally \ninexpensive, and can be easily implemented in existing differential evolution \nor other optimization algorithms by a few lines of program code as an \nnon-invasive optional setting. Besides the applicability of the presented \nalgorithm by itself, the described concepts can serve as a useful and \ninteresting addition to the algorithmic operators in the frameworks of \nheuristics and evolutionary optimization and computing. \n</p>"}, "author": "Ramses Sala, Niccolo Baldanzini, Marco Pierini", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373889130", "timestampUsec": "1508373889130372", "id": "tag:google.com,2005:reader/item/0000000321a0cbfb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Reverse Curriculum Generation for Reinforcement Learning. (arXiv:1707.05300v2 [cs.AI] CROSS LISTED)", "published": 1508373890, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.05300"}], "alternate": [{"href": "http://arxiv.org/abs/1707.05300", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cd78e33\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cd78e33&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Many relevant tasks require an agent to reach a certain state, or to \nmanipulate objects into a desired configuration. For example, we might want a \nrobot to align and assemble a gear onto an axle or insert and turn a key in a \nlock. These goal-oriented tasks present a considerable challenge for \nreinforcement learning, since their natural reward function is sparse and \nprohibitive amounts of exploration are required to reach the goal and receive \nsome learning signal. Past approaches tackle these problems by exploiting \nexpert demonstrations or by manually designing a task-specific reward shaping \nfunction to guide the learning agent. Instead, we propose a method to learn \nthese tasks without requiring any prior knowledge other than obtaining a single \nstate in which the task is achieved. The robot is trained in reverse, gradually \nlearning to reach the goal from a set of start states increasingly far from the \ngoal. Our method automatically generates a curriculum of start states that \nadapts to the agent's performance, leading to efficient training on \ngoal-oriented tasks. We demonstrate our approach on difficult simulated \nnavigation and fine-grained manipulation problems, not solvable by \nstate-of-the-art reinforcement learning methods. \n</p>"}, "author": "Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, Pieter Abbeel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373576856", "timestampUsec": "1508373576855689", "id": "tag:google.com,2005:reader/item/0000000321a054d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Bayesian Perspective on Generalization and Stochastic Gradient Descent. (arXiv:1710.06451v2 [cs.LG] UPDATED)", "published": 1509064764, "updated": 1509064765, "canonical": [{"href": "http://arxiv.org/abs/1710.06451"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06451", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper tackles two related questions at the heart of machine learning; \nhow can we predict if a minimum will generalize to the test set, and why does \nstochastic gradient descent find minima that generalize well? Our work is \ninspired by Zhang et al. (2017), who showed deep networks can easily memorize \nrandomly labeled training data, despite generalizing well when shown real \nlabels of the same inputs. We show here that the same phenomenon occurs in \nsmall linear models. These observations are explained by evaluating the \nBayesian evidence, which penalizes sharp minima but is invariant to model \nparameterization. We also explore the \"generalization gap\" between small and \nlarge batch training, identifying an optimum batch size which maximizes the \ntest set accuracy. Interpreting stochastic gradient descent as a stochastic \ndifferential equation, we identify a \"noise scale\" $g = \\epsilon (\\frac{N}{B} - \n1) \\approx \\epsilon N/B$, where $\\epsilon$ is the learning rate, $N$ training \nset size and $B$ batch size. Consequently the optimum batch size is \nproportional to the learning rate and the training set size, $B_{opt} \\propto \n\\epsilon N$. We verify these predictions empirically. \n</p>"}, "author": "Samuel L. Smith, Quoc V. Le", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373576856", "timestampUsec": "1508373576855688", "id": "tag:google.com,2005:reader/item/0000000321a054e3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Constructing Datasets for Multi-hop Reading Comprehension Across Documents. (arXiv:1710.06481v1 [cs.CL])", "published": 1508373577, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06481"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06481", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Most Reading Comprehension methods limit themselves to queries which can be \nanswered using a single sentence, paragraph, or document. Enabling models to \ncombine disjoint pieces of textual evidence would extend the scope of machine \ncomprehension methods, but currently there exist no resources to train and test \nthis capability. We propose a novel task to encourage the development of models \nfor text understanding across multiple documents and to investigate the limits \nof existing methods. In our task, a model learns to seek and combine evidence - \neffectively performing multi-hop (alias multi-step) inference. We devise a \nmethodology to produce datasets for this task, given a collection of \nquery-answer pairs and thematically linked documents. Two datasets from \ndifferent domains are induced, and we identify potential pitfalls and devise \ncircumvention strategies. We evaluate two previously proposed competitive \nmodels and find that one can integrate information across documents. However, \nboth models struggle to select relevant information, as providing documents \nguaranteed to be relevant greatly improves their performance. While the models \noutperform several strong baselines, their best accuracy reaches 42.9% compared \nto human performance at 74.0% - leaving ample room for improvement. \n</p>"}, "author": "Johannes Welbl, Pontus Stenetorp, Sebastian Riedel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373576856", "timestampUsec": "1508373576855687", "id": "tag:google.com,2005:reader/item/0000000321a054f0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Knowledge-guided Pose Grammar Machine for 3D Human Pose Estimation. (arXiv:1710.06513v1 [cs.CV])", "published": 1508373577, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06513"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06513", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a knowledge-guided pose grammar network to tackle \nthe problem of 3D human pose estimation. Our model directly takes 2D poses as \ninputs and learns the generalized 2D-3D mapping function, which renders high \napplicability. The proposed network consists of a base network which \nefficiently captures pose-aligned features and a hierarchy of Bidirectional \nRNNs on top of it to explicitly incorporate a set of knowledge (e.g., \nkinematics, symmetry, motor coordination) and thus enforce high-level \nconstraints over human poses. In learning, we develop a pose-guided sample \nsimulator to augment training samples in virtual camera views, which further \nimproves the generalization ability of our model. We validate our method on \npublic 3D human pose benchmarks and propose a new evaluation protocol working \non cross-view setting to verify the generalization ability of different \nmethods. We empirically observe that most state-of-the-arts face difficulty \nunder such setting while our method obtains superior performance. \n</p>"}, "author": "Haoshu Fang, Yuanlu Xu, Wenguan Wang, Xiaobai Liu, Song-Chun Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373576856", "timestampUsec": "1508373576855686", "id": "tag:google.com,2005:reader/item/0000000321a054f7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Near-Optimal Adversarial Policy Switching for Decentralized Asynchronous Multi-Agent Systems. (arXiv:1710.06525v1 [cs.AI])", "published": 1508373577, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06525"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06525", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A key challenge in multi-robot and multi-agent systems is generating \nsolutions that are robust to other self-interested or even adversarial parties \nwho actively try to prevent the agents from achieving their goals. The \npracticality of existing works addressing this challenge is limited to only \nsmall-scale synchronous decision-making scenarios or a single agent planning \nits best response against a single adversary with fixed, procedurally \ncharacterized strategies. In contrast this paper considers a more realistic \nclass of problems where a team of asynchronous agents with limited observation \nand communication capabilities need to compete against multiple strategic \nadversaries with changing strategies. This problem necessitates agents that can \ncoordinate to detect changes in adversary strategies and plan the best response \naccordingly. Our approach first optimizes a set of stratagems that represent \nthese best responses. These optimized stratagems are then integrated into a \nunified policy that can detect and respond when the adversaries change their \nstrategies. The near-optimality of the proposed framework is established \ntheoretically as well as demonstrated empirically in simulation and hardware. \n</p>"}, "author": "Trong Nghia Hoang, Yuchen Xiao, Kavinayan Sivakumar, Christopher Amato, Jonathan How", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373576856", "timestampUsec": "1508373576855685", "id": "tag:google.com,2005:reader/item/0000000321a054fd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Asymmetric Actor Critic for Image-Based Robot Learning. (arXiv:1710.06542v1 [cs.RO])", "published": 1508373577, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06542"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06542", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep reinforcement learning (RL) has proven a powerful technique in many \nsequential decision making domains. However, Robotics poses many challenges for \nRL, most notably training on a physical system can be expensive and dangerous, \nwhich has sparked significant interest in learning control policies using a \nphysics simulator. While several recent works have shown promising results in \ntransferring policies trained in simulation to the real world, they often do \nnot fully utilize the advantage of working with a simulator. In this work, we \nexploit the full state observability in the simulator to train better policies \nwhich take as input only partial observations (RGBD images). We do this by \nemploying an actor-critic training algorithm in which the critic is trained on \nfull states while the actor (or policy) gets rendered images as input. We show \nexperimentally on a range of simulated tasks that using these asymmetric inputs \nsignificantly improves performance. Finally, we combine this method with domain \nrandomization and show real robot experiments for several tasks like picking, \npushing, and moving a block. We achieve this simulation to real world transfer \nwithout training on any real world data. \n</p>"}, "author": "Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, Pieter Abbeel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373576856", "timestampUsec": "1508373576855684", "id": "tag:google.com,2005:reader/item/0000000321a05506", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Effects of Memory Replay in Reinforcement Learning. (arXiv:1710.06574v1 [cs.AI])", "published": 1508373577, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06574"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06574", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Experience replay is a key technique behind many recent advances in deep \nreinforcement learning. Allowing the agent to learn from earlier memories can \nspeed up learning and break undesirable temporal correlations. Despite its \nwide-spread application, very little is understood about the properties of \nexperience replay. How does the amount of memory kept affect learning dynamics? \nDoes it help to prioritize certain experiences? In this paper, we address these \nquestions by formulating a dynamical systems ODE model of Q-learning with \nexperience replay. We derive analytic solutions of the ODE for a simple \nsetting. We show that even in this very simple setting, the amount of memory \nkept can substantially affect the agent's performance. Too much or too little \nmemory both slow down learning. Moreover, we characterize regimes where \nprioritized replay harms the agent's learning. We show that our analytic \nsolutions have excellent agreement with experiments. Finally, we propose a \nsimple algorithm for adaptively changing the memory buffer size which achieves \nconsistently good empirical performance. \n</p>"}, "author": "Ruishan Liu, James Zou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373576856", "timestampUsec": "1508373576855683", "id": "tag:google.com,2005:reader/item/0000000321a0550a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deceased Organ Matching in Australia. (arXiv:1710.06636v1 [cs.GT])", "published": 1508373577, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06636"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06636", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Despite efforts to increase the supply of organs from living donors, most \nkidney transplants performed in Australia still come from deceased donors. The \nage of these donated organs has increased substantially in recent decades as \nthe rate of fatal accidents on roads has fallen. The Organ and Tissue Authority \nin Australia is therefore looking to design a new mechanism that better matches \nthe age of the organ to the age of the patient. I discuss the design, \naxiomatics and performance of several candidate mechanisms that respect the \nspecial online nature of this fair division problem. \n</p>"}, "author": "Toby Walsh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373576856", "timestampUsec": "1508373576855682", "id": "tag:google.com,2005:reader/item/0000000321a05510", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Photo-Guided Exploration of Volume Data Features. (arXiv:1710.06815v1 [cs.GR])", "published": 1508373577, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06815"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06815", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work, we pose the question of whether, by considering qualitative \ninformation such as a sample target image as input, one can produce a rendered \nimage of scientific data that is similar to the target. The algorithm resulting \nfrom our research allows one to ask the question of whether features like those \nin the target image exists in a given dataset. In that way, our method is one \nof imagery query or reverse engineering, as opposed to manual parameter \ntweaking of the full visualization pipeline. For target images, we can use \nreal-world photographs of physical phenomena. Our method leverages deep neural \nnetworks and evolutionary optimization. Using a trained similarity function \nthat measures the difference between renderings of a phenomenon and real-world \nphotographs, our method optimizes rendering parameters. We demonstrate the \nefficacy of our method using a superstorm simulation dataset and images found \nonline. We also discuss a parallel implementation of our method, which was run \non NCSA's Blue Waters. \n</p>"}, "author": "Mohammad Raji, Alok Hota, Robert Sisneros, Peter Messmer, Jian Huang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707203", "id": "tag:google.com,2005:reader/item/00000003219fce9b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Bayesian Perspective on Generalization and Stochastic Gradient Descent. (arXiv:1710.06451v2 [cs.LG] UPDATED)", "published": 1509064799, "updated": 1509064802, "canonical": [{"href": "http://arxiv.org/abs/1710.06451"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06451", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper tackles two related questions at the heart of machine learning; \nhow can we predict if a minimum will generalize to the test set, and why does \nstochastic gradient descent find minima that generalize well? Our work is \ninspired by Zhang et al. (2017), who showed deep networks can easily memorize \nrandomly labeled training data, despite generalizing well when shown real \nlabels of the same inputs. We show here that the same phenomenon occurs in \nsmall linear models. These observations are explained by evaluating the \nBayesian evidence, which penalizes sharp minima but is invariant to model \nparameterization. We also explore the \"generalization gap\" between small and \nlarge batch training, identifying an optimum batch size which maximizes the \ntest set accuracy. Interpreting stochastic gradient descent as a stochastic \ndifferential equation, we identify a \"noise scale\" $g = \\epsilon (\\frac{N}{B} - \n1) \\approx \\epsilon N/B$, where $\\epsilon$ is the learning rate, $N$ training \nset size and $B$ batch size. Consequently the optimum batch size is \nproportional to the learning rate and the training set size, $B_{opt} \\propto \n\\epsilon N$. We verify these predictions empirically. \n</p>"}, "author": "Samuel L. Smith, Quoc V. Le", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707202", "id": "tag:google.com,2005:reader/item/00000003219fcea2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "S-Isomap++: Multi Manifold Learning from Streaming Data. (arXiv:1710.06462v1 [stat.ML])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06462"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06462", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cd7908e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cd7908e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Manifold learning based methods have been widely used for non-linear \ndimensionality reduction (NLDR). However, in many practical settings, the need \nto process streaming data is a challenge for such methods, owing to the high \ncomputational complexity involved. Moreover, most methods operate under the \nassumption that the input data is sampled from a single manifold, embedded in a \nhigh dimensional space. We propose a method for streaming NLDR when the \nobserved data is either sampled from multiple manifolds or irregularly sampled \nfrom a single manifold. We show that existing NLDR methods, such as Isomap, \nfail in such situations, primarily because they rely on smoothness and \ncontinuity of the underlying manifold, which is violated in the scenarios \nexplored in this paper. However, the proposed algorithm is able to learn \neffectively in presence of multiple, and potentially intersecting, manifolds, \nwhile allowing for the input data to arrive as a massive stream. \n</p>"}, "author": "Suchismit Mahapatra, Varun Chandola", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707201", "id": "tag:google.com,2005:reader/item/00000003219fceaa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Classification and Geometry of General Perceptual Manifolds. (arXiv:1710.06487v1 [cond-mat.dis-nn])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06487"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06487", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3ce0578f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3ce0578f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Perceptual manifolds arise when a neural population responds to an ensemble \nof sensory signals associated with different physical features (e.g., \norientation, pose, scale, location, and intensity) of the same perceptual \nobject. Object recognition and discrimination requires classifying the \nmanifolds in a manner that is insensitive to variability within a manifold. How \nneuronal systems give rise to invariant object classification and recognition \nis a fundamental problem in brain theory as well as in machine learning. Here \nwe study the ability of a readout network to classify objects from their \nperceptual manifold representations. We develop a statistical mechanical theory \nfor the linear classification of manifolds with arbitrary geometry revealing a \nremarkable relation to the mathematics of conic decomposition. Novel \ngeometrical measures of manifold radius and manifold dimension are introduced \nwhich can explain the classification capacity for manifolds of various \ngeometries. The general theory is demonstrated on a number of representative \nmanifolds, including L2 ellipsoids prototypical of strictly convex manifolds, \nL1 balls representing polytopes consisting of finite sample points, and \norientation manifolds which arise from neurons tuned to respond to a continuous \nangle variable, such as object orientation. The effects of label sparsity on \nthe classification capacity of manifolds are elucidated, revealing a scaling \nrelation between label sparsity and manifold radius. Theoretical predictions \nare corroborated by numerical simulations using recently developed algorithms \nto compute maximum margin solutions for manifold dichotomies. Our theory and \nits extensions provide a powerful and rich framework for applying statistical \nmechanics of linear classification to data arising from neuronal responses to \nobject stimuli, as well as to artificial deep networks trained for object \nrecognition tasks. \n</p>"}, "author": "SueYeon Chung, Daniel D. Lee, Haim Sompolinsky", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707200", "id": "tag:google.com,2005:reader/item/00000003219fceb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On reducing sampling variance in covariate shift using control variates. (arXiv:1710.06514v1 [cs.LG])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06514"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06514", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Covariate shift classification problems can in principle be tackled by \nimportance-weighting training samples. However, the sampling variance of the \nrisk estimator is often scaled up dramatically by the weights. This means that \nduring cross-validation - when the importance-weighted risk is repeatedly \nevaluated - suboptimal hyperparameter estimates are produced. We study the \nsampling variances of the importance-weighted versus the oracle estimator as a \nfunction of the relative scale of the training data. We show that introducing a \ncontrol variate can reduce the variance of the importance-weighted risk \nestimator, which leads to superior regularization parameter estimates when the \ntraining data is much smaller in scale than the test data. \n</p>"}, "author": "Wouter Kouw, Marco Loog", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707199", "id": "tag:google.com,2005:reader/item/00000003219fcec3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Exploiting oddsmaker bias to improve the prediction of NFL outcomes. (arXiv:1710.06551v1 [stat.AP])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06551"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06551", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Accurately predicting the outcome of sporting events has been a goal for many \ngroups who seek to maximize profit. What makes this challenging is that the \noutcome of an event can be influenced by many factors that dynamically change \nacross time. Oddsmakers attempt to estimate these factors by using both \nalgorithmic and subjective methods to set the spread. However, it is well-known \nthat both human and algorithmic decision-making can be biased, so this paper \nexplores if oddsmaker biases can be used in an exploitative manner, in order to \nimprove the prediction of NFL game outcomes. Real-world gambling data was used \nto train and test different predictive models under varying assumptions. The \nresults show that methods that leverage oddsmaker biases in an exploitative \nmanner perform best under the conditions tested in this paper. These findings \nsuggest that leveraging human and algorithmic decision biases in an \nexploitative manner may be useful for predicting the outcomes of competitive \nevents, and could lead to increased profit for those who have financial \ninterest in the outcomes. \n</p>"}, "author": "Erik J. Schlicht", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707198", "id": "tag:google.com,2005:reader/item/00000003219fcecb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Revenue-based Attribution Modeling for Online Advertising. (arXiv:1710.06561v1 [econ.EM])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06561"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06561", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper examines and proposes several attribution modeling methods that \nquantify how revenue should be attributed to online advertising inputs. We \nadopt and further develop relative importance method, which is based on \nregression models that have been extensively studied and utilized to \ninvestigate the relationship between advertising efforts and market reaction \n(revenue). Relative importance method aims at decomposing and allocating \nmarginal contributions to the coefficient of determination (R^2) of regression \nmodels as attribution values. In particular, we adopt two alternative \nsubmethods to perform this decomposition: dominance analysis and relative \nweight analysis. Moreover, we demonstrate an extension of the decomposition \nmethods from standard linear model to additive model. We claim that our new \napproaches are more flexible and accurate in modeling the underlying \nrelationship and calculating the attribution values. We use simulation examples \nto demonstrate the superior performance of our new approaches over traditional \nmethods. We further illustrate the value of our proposed approaches using a \nreal advertising campaign dataset. \n</p>"}, "author": "Kaifeng Zhao, Seyed Hanif Mahboobi, Saeed Bagheri", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707197", "id": "tag:google.com,2005:reader/item/00000003219fced1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Replacement AutoEncoder: A Privacy-Preserving Algorithm for Sensory Data Analysis. (arXiv:1710.06564v1 [cs.LG])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06564"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06564", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>An increasing number of sensors on mobile, Internet of things (IoT), and \nwearable devices generate time-series measurements of physical activities. \nThough access to the sensory data is critical to the success of many beneficial \napplications such as health monitoring or activity recognition, a wide range of \npotentially sensitive information about the individuals can also be discovered \nthrough these datasets and this cannot easily be protected using traditional \nprivacy approaches. \n</p> \n<p>In this paper, we propose an integrated sensing framework for managing access \nto personal time-series data in order to provide utility while protecting \nindividuals' privacy. We introduce \\textit{Replacement AutoEncoder}, a novel \nfeature-learning algorithm which learns how to transform discriminative \nfeatures of multidimensional time-series that correspond to sensitive \ninferences, into some features that have been more observed in non-sensitive \ninferences, to protect users' privacy. The main advantage of Replacement \nAutoEncoder is its ability to keep important features of desired inferences \nunchanged to preserve the utility of the data. We evaluate the efficacy of the \nalgorithm with an activity recognition task in a multi-sensing environment \nusing extensive experiments on three benchmark datasets. We show that it can \nretain the recognition accuracy of state-of-the-art techniques while \nsimultaneously preserving the privacy of sensitive information. We use a \nGenerative Adversarial Network to attempt to detect the replacement of \nsensitive data with fake non-sensitive data. We show that this approach does \nnot detect the replacement unless the network can train using the users' \noriginal unmodified data. \n</p>"}, "author": "Mohammad Malekzadeh, Richard G. Clegg, Hamed Haddadi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707196", "id": "tag:google.com,2005:reader/item/00000003219fced7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Correspondence Between Random Neural Networks and Statistical Field Theory. (arXiv:1710.06570v1 [stat.ML])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06570"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06570", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A number of recent papers have provided evidence that practical design \nquestions about neural networks may be tackled theoretically by studying the \nbehavior of random networks. However, until now the tools available for \nanalyzing random neural networks have been relatively ad-hoc. In this work, we \nshow that the distribution of pre-activations in random neural networks can be \nexactly mapped onto lattice models in statistical physics. We argue that \nseveral previous investigations of stochastic networks actually studied a \nparticular factorial approximation to the full lattice model. For random linear \nnetworks and random rectified linear networks we show that the corresponding \nlattice models in the wide network limit may be systematically approximated by \na Gaussian distribution with covariance between the layers of the network. In \neach case, the approximate distribution can be diagonalized by Fourier \ntransformation. We show that this approximation accurately describes the \nresults of numerical simulations of wide random neural networks. Finally, we \ndemonstrate that in each case the large scale behavior of the random networks \ncan be approximated by an effective field theory. \n</p>"}, "author": "Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707195", "id": "tag:google.com,2005:reader/item/00000003219fcee1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Effects of Memory Replay in Reinforcement Learning. (arXiv:1710.06574v1 [cs.AI])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06574"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06574", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Experience replay is a key technique behind many recent advances in deep \nreinforcement learning. Allowing the agent to learn from earlier memories can \nspeed up learning and break undesirable temporal correlations. Despite its \nwide-spread application, very little is understood about the properties of \nexperience replay. How does the amount of memory kept affect learning dynamics? \nDoes it help to prioritize certain experiences? In this paper, we address these \nquestions by formulating a dynamical systems ODE model of Q-learning with \nexperience replay. We derive analytic solutions of the ODE for a simple \nsetting. We show that even in this very simple setting, the amount of memory \nkept can substantially affect the agent's performance. Too much or too little \nmemory both slow down learning. Moreover, we characterize regimes where \nprioritized replay harms the agent's learning. We show that our analytic \nsolutions have excellent agreement with experiments. Finally, we propose a \nsimple algorithm for adaptively changing the memory buffer size which achieves \nconsistently good empirical performance. \n</p>"}, "author": "Ruishan Liu, James Zou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707194", "id": "tag:google.com,2005:reader/item/00000003219fcee7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Social Image Embedding with Deep Multimodal Attention Networks. (arXiv:1710.06582v1 [cs.MM])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06582"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06582", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning social media data embedding by deep models has attracted extensive \nresearch interest as well as boomed a lot of applications, such as link \nprediction, classification, and cross-modal search. However, for social images \nwhich contain both link information and multimodal contents (e.g., text \ndescription, and visual content), simply employing the embedding learnt from \nnetwork structure or data content results in sub-optimal social image \nrepresentation. In this paper, we propose a novel social image embedding \napproach called Deep Multimodal Attention Networks (DMAN), which employs a deep \nmodel to jointly embed multimodal contents and link information. Specifically, \nto effectively capture the correlations between multimodal contents, we propose \na multimodal attention network to encode the fine-granularity relation between \nimage regions and textual words. To leverage the network structure for \nembedding learning, a novel Siamese-Triplet neural network is proposed to model \nthe links among images. With the joint deep model, the learnt embedding can \ncapture both the multimodal contents and the nonlinear network information. \nExtensive experiments are conducted to investigate the effectiveness of our \napproach in the applications of multi-label classification and cross-modal \nsearch. Compared to state-of-the-art image embeddings, our proposed DMAN \nachieves significant improvement in the tasks of multi-label classification and \ncross-modal search. \n</p>"}, "author": "Feiran Huang, Xiaoming Zhang, Zhoujun Li, Tao Mei, Yueying He, Zhonghua Zhao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707193", "id": "tag:google.com,2005:reader/item/00000003219fceef", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Variational Inference based on Robust Divergences. (arXiv:1710.06595v1 [stat.ML])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06595"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06595", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Robustness to outliers is a central issue in real-world machine learning \napplications. While replacing a model to a heavy-tailed one (e.g., from \nGaussian to Student-t) is a standard approach for robustification, it can only \nbe applied to simple models. In this paper, based on Zellner's optimization and \nvariational formulation of Bayesian inference, we propose an outlier-robust \npseudo-Bayesian variational method by replacing the Kullback-Leibler divergence \nused for data fitting to a robust divergence such as the beta and \ngamma-divergences. An advantage of our approach is that complex models such as \ndeep networks can be handled. We theoretically prove that, for deep networks \nwith ReLU activation functions, the influence function in our proposed method \nis bounded, while it is unbounded in the ordinary variational inference. This \nimplies that our proposed method is robust to both of input and output \noutliers, while the ordinary variational method is not. We experimentally \ndemonstrate that our robust variational method outperforms ordinary variational \ninference in regression and classification with deep networks. \n</p>"}, "author": "Futoshi Futami, Issei Sato, Masashi Sugiyama", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707192", "id": "tag:google.com,2005:reader/item/00000003219fcef5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Sinkhorn-Newton method for entropic optimal transport. (arXiv:1710.06635v1 [math.OC])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06635"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06635", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3ce05b64\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3ce05b64&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We consider the entropic regularization of discretized optimal transport and \npropose to solve its optimality conditions via a logarithmic Newton iteration. \nWe show a quadratic convergence rate and validate numerically that the method \ncompares favorably with the more commonly used Sinkhorn--Knopp algorithm for \nsmall regularization strength. We further investigate numerically the \nrobustness of the proposed method with respect to parameters such as the mesh \nsize of the discretization. \n</p>"}, "author": "Christoph Brauer, Christian Clason, Dirk Lorenz, Benedikt Wirth", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707191", "id": "tag:google.com,2005:reader/item/00000003219fcefc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Weighted Function Norm Regularization. (arXiv:1710.06703v1 [cs.LG])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06703"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06703", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks (DNNs) have become increasingly important due to their \nexcellent empirical performance on a wide range of problems. However, \nregularization is generally achieved by indirect means, largely due to the \ncomplex set of functions defined by a network and the difficulty in measuring \nfunction complexity. There exists no method in the literature for additive \nregularization based on a norm of the function, as is classically considered in \nstatistical learning theory. In this work, we propose sampling-based \napproximations to weighted function norms as regularizers for deep neural \nnetworks. We provide, to the best of our knowledge, the first proof in the \nliterature of the NP-hardness of computing function norms of DNNs, motivating \nthe necessity of a stochastic optimization strategy. Based on our proposed \nregularization scheme, stability-based bounds yield a \n$\\mathcal{O}(N^{-\\frac{1}{2}})$ generalization error for our proposed \nregularizer when applied to convex function sets. We demonstrate broad \nconditions for the convergence of stochastic gradient descent on our objective, \nincluding for non-convex function sets such as those defined by DNNs. Finally, \nwe empirically validate the improved performance of the proposed regularization \nstrategy for both convex function sets as well as DNNs on real-world \nclassification and segmentation tasks. \n</p>"}, "author": "Amal Rannen Triki, Maxim Berman, Matthew B. Blaschko", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707190", "id": "tag:google.com,2005:reader/item/00000003219fcf01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A complete characterization of optimal dictionaries for least squares representation. (arXiv:1710.06763v1 [math.OC])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06763"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06763", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Dictionaries are collections of vectors used for representations of elements \nin Euclidean spaces. While recent research on optimal dictionaries is focussed \non providing sparse (i.e., $\\ell_0$-optimal,) representations, here we consider \nthe problem of finding optimal dictionaries such that representations of \nsamples of a random vector are optimal in an $\\ell_2$-sense. For us, optimality \nof representation is equivalent to minimization of the average $\\ell_2$-norm of \nthe coefficients used to represent the random vector, with the lengths of the \ndictionary vectors being specified a priori. With the help of recent results on \nrank-$1$ decompositions of symmetric positive semidefinite matrices and the \ntheory of majorization, we provide a complete characterization of \n$\\ell_2$-optimal dictionaries. Our results are accompanied by polynomial time \nalgorithms that construct $\\ell_2$-optimal dictionaries from given data. \n</p>"}, "author": "Mohammed Rayyan Sheriff, Debasish Chatterjee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707189", "id": "tag:google.com,2005:reader/item/00000003219fcf05", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Phase Transitions in the Pooled Data Problem. (arXiv:1710.06766v1 [stat.ML])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06766"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06766", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we study the pooled data problem of identifying the labels \nassociated with a large collection of items, based on a sequence of pooled \ntests revealing the counts of each label within the pool. In the noiseless \nsetting, we identify an exact asymptotic threshold on the required number of \ntests with optimal decoding, and prove a phase transition between complete \nsuccess and complete failure. In addition, we present a novel noisy variation \nof the problem, and provide an information-theoretic framework for \ncharacterizing the required number of tests for general random noise models. \nOur results reveal that noise can make the problem considerably more difficult, \nwith strict increases in the scaling laws even at low noise levels. Finally, we \ndemonstrate similar behavior in an approximate recovery setting, where a given \nnumber of errors is allowed in the decoded labels. \n</p>"}, "author": "Jonathan Scarlett, Volkan Cevher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707188", "id": "tag:google.com,2005:reader/item/00000003219fcf08", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Weighted Tensor Decomposition for Learning Latent Variables with Partial Data. (arXiv:1710.06818v1 [stat.ML])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06818"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06818", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Tensor decomposition methods are popular tools for learning latent variables \ngiven only lower-order moments of the data. However, the standard assumption is \nthat we have sufficient data to estimate these moments to high accuracy. In \nthis work, we consider the case in which certain dimensions of the data are not \nalways observed---common in applied settings, where not all measurements may be \ntaken for all observations---resulting in moment estimates of varying quality. \nWe derive a weighted tensor decomposition approach that is computationally as \nefficient as the non-weighted approach, and demonstrate that it outperforms \nmethods that do not appropriately leverage these less-observed dimensions. \n</p>"}, "author": "Omer Gottesman, Weiwei Pan, Finale Doshi-Velez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707178", "id": "tag:google.com,2005:reader/item/00000003219fcf35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neural Networks Quantum States, String-Bond States and chiral topological states. (arXiv:1710.04045v2 [quant-ph] CROSS LISTED)", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04045"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04045", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural Networks Quantum States have been recently introduced as an Ansatz for \ndescribing the wave function of quantum many-body systems. We show that there \nare strong connections between Neural Networks Quantum States in the form of \nRestricted Boltzmann Machines and some classes of Tensor Network states in \narbitrary dimension. In particular we demonstrate that short-range Restricted \nBoltzmann Machines are Entangled Plaquette States, while fully connected \nRestricted Boltzmann Machines are String-Bond States with a non-local geometry \nand low bond dimension. These results shed light on the underlying architecture \nof Restricted Boltzmann Machines and their efficiency at representing many-body \nquantum states. String-Bond States also provide a generic way of enhancing the \npower of Neural Networks Quantum States and a natural generalization to systems \nwith larger local Hilbert space. We compare the advantages and drawbacks of \nthese different classes of states and present a method to combine them \ntogether. This allows us to benefit from both the entanglement structure of \nTensor Networks and the efficiency of Neural Network Quantum States into a \nsingle Ansatz capable of targeting the wave function of strongly correlated \nsystems. While it remains a challenge to describe states with chiral \ntopological order using traditional Tensor Networks, we show that Neural \nNetworks Quantum States and their String-Bond States extension can describe a \nlattice Fractional Quantum Hall state exactly. In addition, we provide \nnumerical evidence that Neural Networks Quantum States can approximate a chiral \nspin liquid with better accuracy than Entangled Plaquette States and local \nString-Bond States. Our results demonstrate the efficiency of neural networks \nto describe complex quantum wave functions and pave the way towards the use of \nString-Bond States as a tool in more traditional machine learning applications. \n</p>"}, "author": "Ivan Glasser, Nicola Pancotti, Moritz August, Ivan D. Rodriguez, J. Ignacio Cirac", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707177", "id": "tag:google.com,2005:reader/item/00000003219fcf39", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "User Modelling for Avoiding Overfitting in Interactive Knowledge Elicitation for Prediction. (arXiv:1710.04881v1 [cs.HC] CROSS LISTED)", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04881"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04881", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In human-in-the-loop machine learning, the user provides information beyond \nthat in the training data. Many algorithms and user interfaces have been \ndesigned to optimize and facilitate this human--machine interaction; however, \nfewer studies have addressed the potential defects the designs can cause. \nEffective interaction often requires exposing the user to the training data or \nits statistics. The design of the system is then critical, as this can lead to \ndouble use of data and overfitting, if the user reinforces noisy patterns in \nthe data. We propose a user modelling methodology, by assuming simple rational \nbehaviour, to correct the problem. We show, in a user study with 48 \nparticipants, that the method improves predictive performance in a sparse \nlinear regression sentiment analysis task, where graded user knowledge on \nfeature relevance is elicited. We believe that the key idea of inferring user \nknowledge with probabilistic user models has general applicability in guarding \nagainst overfitting and improving interactive machine learning. \n</p>"}, "author": "Pedram Daee, Tomi Peltola, Aki Vehtari, Samuel Kaski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976761", "id": "tag:google.com,2005:reader/item/00000003214c0854", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neuro Fuzzy Modelling for Prediction of Consumer Price Index. (arXiv:1710.05944v1 [cs.CY])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05944"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05944", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Economic indicators such as Consumer Price Index (CPI) have frequently used \nin predicting future economic wealth for financial policy makers of respective \ncountry. Most central banks, on guidelines of research studies, have recently \nadopted an inflation targeting monetary policy regime, which accounts for high \nrequirement for effective prediction model of consumer price index. However, \nprediction accuracy by numerous studies is still low, which raises a need for \nimprovement. This manuscript presents findings of study that use neuro fuzzy \ntechnique to design a machine-learning model that train and test data to \npredict a univariate time series CPI. The study establishes a matrix of monthly \nCPI data from secondary data source of Tanzania National Bureau of Statistics \nfrom January 2000 to December 2015 as case study and thereafter conducted \nsimulation experiments on MATLAB whereby ninety five percent (95%) of data used \nto train the model and five percent (5%) for testing. Furthermore, the study \nuse root mean square error (RMSE) and mean absolute percentage error (MAPE) as \nerror metrics for model evaluation. The results show that the neuro fuzzy model \nhave an architecture of 5:74:1 with Gaussian membership functions (2, 2, 2, 2, \n2), provides RMSE of 0.44886 and MAPE 0.23384, which is far better compared to \nexisting research studies. \n</p>"}, "author": "Godwin Ambukege, Godfrey Justo, Joseph Mushi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976760", "id": "tag:google.com,2005:reader/item/00000003214c0860", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Gradient-free Policy Architecture Search and Adaptation. (arXiv:1710.05958v1 [cs.LG])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05958"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05958", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We develop a method for policy architecture search and adaptation via \ngradient-free optimization which can learn to perform autonomous driving tasks. \nBy learning from both demonstration and environmental reward we develop a model \nthat can learn with relatively few early catastrophic failures. We first learn \nan architecture of appropriate complexity to perceive aspects of world state \nrelevant to the expert demonstration, and then mitigate the effect of \ndomain-shift during deployment by adapting a policy demonstrated in a source \ndomain to rewards obtained in a target environment. We show that our approach \nallows safer learning than baseline methods, offering a reduced cumulative \ncrash metric over the agent's lifetime as it learns to drive in a realistic \nsimulated environment. \n</p>"}, "author": "Sayna Ebrahimi, Anna Rohrbach, Trevor Darrell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976759", "id": "tag:google.com,2005:reader/item/00000003214c0879", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Safe Medicine Recommendation via Medical Knowledge Graph Embedding. (arXiv:1710.05980v1 [cs.IR])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05980"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05980", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Most of the existing medicine recommendation systems that are mainly based on \nelectronic medical records (EMRs) are significantly assisting doctors to make \nbetter clinical decisions benefiting both patients and caregivers. Even though \nthe growth of EMRs is at a lighting fast speed in the era of big data, content \nlimitations in EMRs restrain the existed recommendation systems to reflect \nrelevant medical facts, such as drug-drug interactions. Many medical knowledge \ngraphs that contain drug-related information, such as DrugBank, may give hope \nfor the recommendation systems. However, the direct use of these knowledge \ngraphs in the systems suffers from robustness caused by the incompleteness of \nthe graphs. To address these challenges, we stand on recent advances in graph \nembedding learning techniques and propose a novel framework, called Safe \nMedicine Recommendation (SMR), in this paper. Specifically, SMR first \nconstructs a high-quality heterogeneous graph by bridging EMRs (MIMIC-III) and \nmedical knowledge graphs (ICD-9 ontology and DrugBank). Then, SMR jointly \nembeds diseases, medicines, patients, and their corresponding relations into a \nshared lower dimensional space. Finally, SMR uses the embeddings to decompose \nthe medicine recommendation into a link prediction process while considering \nthe patient's diagnoses and adverse drug reactions. To our best knowledge, SMR \nis the first to learn embeddings of a patient-disease-medicine graph for \nmedicine recommendation in the world. Extensive experiments on real datasets \nare conducted to evaluate the effectiveness of proposed framework. \n</p>"}, "author": "Meng Wang, Mengyue Liu, Jun Liu, Sen Wang, Guodong Long, Buyue Qian", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976758", "id": "tag:google.com,2005:reader/item/00000003214c088b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Reply With: Proactive Recommendation of Email Attachments. (arXiv:1710.06061v1 [cs.IR])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06061"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06061", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3ce05f0c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3ce05f0c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Email responses often contain items-such as a file or a hyperlink to an \nexternal document-that are attached to or included inline in the body of the \nmessage. Analysis of an enterprise email corpus reveals that 35% of the time \nwhen users include these items as part of their response, the attachable item \nis already present in their inbox or sent folder. A modern email client can \nproactively retrieve relevant attachable items from the user's past emails \nbased on the context of the current conversation, and recommend them for \ninclusion, to reduce the time and effort involved in composing the response. In \nthis paper, we propose a weakly supervised learning framework for recommending \nattachable items to the user. As email search systems are commonly available, \nwe constrain the recommendation task to formulating effective search queries \nfrom the context of the conversations. The query is submitted to an existing IR \nsystem to retrieve relevant items for attachment. We also present a novel \nstrategy for generating labels from an email corpus---without the need for \nmanual annotations---that can be used to train and evaluate the query \nformulation model. In addition, we describe a deep convolutional neural network \nthat demonstrates satisfactory performance on this query formulation task when \nevaluated on the publicly available Avocado dataset and a proprietary dataset \nof internal emails obtained through an employee participation program. \n</p>"}, "author": "Christophe Van Gysel, Bhaskar Mitra, Matteo Venanzi, Roy Rosemarin, Grzegorz Kukla, Piotr Grudzien, Nicola Cancedda", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976757", "id": "tag:google.com,2005:reader/item/00000003214c089d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts. (arXiv:1710.06071v1 [cs.CL])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06071"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06071", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3ce70438\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3ce70438&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present PubMed 200k RCT, a new dataset based on PubMed for sequential \nsentence classification. The dataset consists of approximately 200,000 \nabstracts of randomized controlled trials, totaling 2.3 million sentences. Each \nsentence of each abstract is labeled with their role in the abstract using one \nof the following classes: background, objective, method, result, or conclusion. \nThe purpose of releasing this dataset is twofold. First, the majority of \ndatasets for sequential short-text classification (i.e., classification of \nshort texts that appear in sequences) are small: we hope that releasing a new \nlarge dataset will help develop more accurate algorithms for this task. Second, \nfrom an application perspective, researchers need better tools to efficiently \nskim through the literature. Automatically classifying each sentence in an \nabstract would help researchers read abstracts more efficiently, especially in \nfields where abstracts may be long, such as the medical field. \n</p>"}, "author": "Franck Dernoncourt, Ji Young Lee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976756", "id": "tag:google.com,2005:reader/item/00000003214c08ad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Spontaneous Symmetry Breaking in Neural Networks. (arXiv:1710.06096v1 [stat.CO])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06096"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06096", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a framework to understand the unprecedented performance and \nrobustness of deep neural networks using field theory. Correlations between the \nweights within the same layer can be described by symmetries in that layer, and \nnetworks generalize better if such symmetries are broken to reduce the \nredundancies of the weights. Using a two parameter field theory, we find that \nthe network can break such symmetries itself towards the end of training in a \nprocess commonly known in physics as spontaneous symmetry breaking. This \ncorresponds to a network generalizing itself without any user input layers to \nbreak the symmetry, but by communication with adjacent layers. In the layer \ndecoupling limit applicable to residual networks (He et al., 2015), we show \nthat the remnant symmetries that survive the non-linear layers are \nspontaneously broken. The Lagrangian for the non-linear and weight layers \ntogether has striking similarities with the one in quantum field theory of a \nscalar. Using results from quantum field theory we show that our framework is \nable to explain many experimentally observed phenomena,such as training on \nrandom labels with zero error (Zhang et al., 2017), the information bottleneck, \nthe phase transition out of it and gradient variance explosion (Shwartz-Ziv &amp; \nTishby, 2017), shattered gradients (Balduzzi et al., 2017), and many more. \n</p>"}, "author": "Ricky Fok, Aijun An, Xiaogang Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976755", "id": "tag:google.com,2005:reader/item/00000003214c08c1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Map-based Multi-Policy Reinforcement Learning: Enhancing Adaptability of Robots by Deep Reinforcement Learning. (arXiv:1710.06117v2 [cs.RO] UPDATED)", "published": 1508373577, "updated": 1508373578, "canonical": [{"href": "http://arxiv.org/abs/1710.06117"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06117", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In order for robots to perform mission-critical tasks, it is essential that \nthey are able to quickly adapt to changes in their environment as well as to \ninjuries and or other bodily changes. Deep reinforcement learning has been \nshown to be successful in training robot control policies for operation in \ncomplex environments. However, existing methods typically employ only a single \npolicy. This can limit the adaptability since a large environmental \nmodification might require a completely different behavior compared to the \nlearning environment. To solve this problem, we propose Map-based Multi-Policy \nReinforcement Learning (MMPRL), which aims to search and store multiple \npolicies that encode different behavioral features while maximizing the \nexpected reward in advance of the environment change. Thanks to these policies, \nwhich are stored into a multi-dimensional discrete map according to its \nbehavioral feature, adaptation can be performed within reasonable time without \nretraining the robot. An appropriate pre-trained policy from the map can be \nrecalled using Bayesian optimization. Our experiments show that MMPRL enables \nrobots to quickly adapt to large changes without requiring any prior knowledge \non the type of injuries that could occur. A highlight of the learned behaviors \ncan be found here: https://youtu.be/QwInbilXNOE . \n</p>"}, "author": "Ayaka Kume, Eiichi Matsumoto, Kuniyuki Takahashi, Wilson Ko, Jethro Tan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976754", "id": "tag:google.com,2005:reader/item/00000003214c08d9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Distributed algorithm for empty vehicles management in personal rapid transit (PRT) network. (arXiv:1710.06331v1 [cs.DC])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06331"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06331", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, an original heuristic algorithm of empty vehicles management \nin personal rapid transit network is presented. The algorithm is used for the \ndelivery of empty vehicles for waiting passengers, for balancing the \ndistribution of empty vehicles within the network, and for providing an empty \nspace for vehicles approaching a station. Each of these tasks involves a \ndecision on the trip that has to be done by a selected empty vehicle from its \nactual location to some determined destination. The decisions are based on a \nmulti-parameter function involving a set of factors and thresholds. An \nimportant feature of the algorithm is that it does not use any central database \nof passenger input (demand) and locations of free vehicles. Instead, it is \nbased on the local exchange of data between stations: on their states and on \nthe vehicles they expect. Therefore, it seems well-tailored for a distributed \nimplementation. The algorithm is uniform, meaning that the same basic procedure \nis used for multiple tasks using a task-specific set of parameters. \n</p>"}, "author": "Wiktor B. Daszczuk, Jerzy Mie&#x15b;cicki, Waldemar Grabski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976753", "id": "tag:google.com,2005:reader/item/00000003214c08f6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Laying Down the Yellow Brick Road: Development of a Wizard-of-Oz Interface for Collecting Human-Robot Dialogue. (arXiv:1710.06406v1 [cs.CL])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06406"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06406", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We describe the adaptation and refinement of a graphical user interface \ndesigned to facilitate a Wizard-of-Oz (WoZ) approach to collecting human-robot \ndialogue data. The data collected will be used to develop a dialogue system for \nrobot navigation. Building on an interface previously used in the development \nof dialogue systems for virtual agents and video playback, we add templates \nwith open parameters which allow the wizard to quickly produce a wide variety \nof utterances. Our research demonstrates that this approach to data collection \nis viable as an intermediate step in developing a dialogue system for physical \nrobots in remote locations from their users - a domain in which the human and \nrobot need to regularly verify and update a shared understanding of the \nphysical environment. We show that our WoZ interface and the fixed set of \nutterances and templates therein provide for a natural pace of dialogue with \ngood coverage of the navigation domain. \n</p>"}, "author": "Claire Bonial, Matthew Marge, Ron artstein, Ashley Foots, Felix Gervits, Cory J. Hayes, Cassidy Henry, Susan G. Hill, Anton Leuski, Stephanie M. Lukin, Pooja Moolchandani, Kimberly A. Pollard, David Traum, Clare R. Voss", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976752", "id": "tag:google.com,2005:reader/item/00000003214c0910", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multi-task Domain Adaptation for Deep Learning of Instance Grasping from Simulation. (arXiv:1710.06422v1 [cs.LG])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06422"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06422", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning-based approaches to robotic manipulation are limited by the \nscalability of data collection and accessibility of labels. In this paper, we \npresent a multi-task domain adaptation framework for instance grasping in \ncluttered scenes by utilizing simulated robot experiments. Our neural network \ntakes monocular RGB images and the instance segmentation mask of a specified \ntarget object as inputs, and predicts the probability of successfully grasping \nthe specified object for each candidate motor command. The proposed transfer \nlearning framework trains a model for instance grasping in simulation and uses \na domain-adversarial loss to transfer the trained model to real robots using \nindiscriminate grasping data, which is available both in simulation and the \nreal world. We evaluate our model in real-world robot experiments, comparing it \nwith alternative model architectures as well as an indiscriminate grasping \nbaseline. \n</p>"}, "author": "Kuan Fang, Yunfei Bai, Stefan Hinterstoisser, Mrinal Kalakrishnan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976742", "id": "tag:google.com,2005:reader/item/00000003214c0989", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Self-Supervised Visual Planning with Temporal Skip Connections. (arXiv:1710.05268v1 [cs.RO] CROSS LISTED)", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05268"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05268", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In order to autonomously learn wide repertoires of complex skills, robots \nmust be able to learn from their own autonomously collected data, without human \nsupervision. One learning signal that is always available for autonomously \ncollected data is prediction: if a robot can learn to predict the future, it \ncan use this predictive model to take actions to produce desired outcomes, such \nas moving an object to a particular location. However, in complex open-world \nscenarios, designing a representation for prediction is difficult. In this \nwork, we instead aim to enable self-supervised robotic learning through direct \nvideo prediction: instead of attempting to design a good representation, we \ndirectly predict what the robot will see next, and then use this model to \nachieve desired goals. A key challenge in video prediction for robotic \nmanipulation is handling complex spatial arrangements such as occlusions. To \nthat end, we introduce a video prediction model that can keep track of objects \nthrough occlusion by incorporating temporal skip-connections. Together with a \nnovel planning criterion and action space formulation, we demonstrate that this \nmodel substantially outperforms prior work on video prediction-based control. \nOur results show manipulation of objects not seen during training, handling \nmultiple objects, and pushing objects around obstructions. These results \nrepresent a significant advance in the range and complexity of skills that can \nbe performed entirely with self-supervised robotic learning. \n</p>"}, "author": "Frederik Ebert, Chelsea Finn, Alex X. Lee, Sergey Levine", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715974", "id": "tag:google.com,2005:reader/item/0000000321219be4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convolutional neural networks for structured omics: OmicsCNN and the OmicsConv layer. (arXiv:1710.05918v1 [q-bio.QM])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05918"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05918", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolutional Neural Networks (CNNs) are a popular deep learning architecture \nwidely applied in different domains, in particular in classifying over images, \nfor which the concept of convolution with a filter comes naturally. \nUnfortunately, the requirement of a distance (or, at least, of a neighbourhood \nfunction) in the input feature space has so far prevented its direct use on \ndata types such as omics data. However, a number of omics data are metrizable, \ni.e., they can be endowed with a metric structure, enabling to adopt a \nconvolutional based deep learning framework, e.g., for prediction. We propose a \ngeneralized solution for CNNs on omics data, implemented through a dedicated \nKeras layer. In particular, for metagenomics data, a metric can be derived from \nthe patristic distance on the phylogenetic tree. For transcriptomics data, we \ncombine Gene Ontology semantic similarity and gene co-expression to define a \ndistance; the function is defined through a multilayer network where 3 layers \nare defined by the GO mutual semantic similarity while the fourth one by gene \nco-expression. As a general tool, feature distance on omics data is enabled by \nOmicsConv, a novel Keras layer, obtaining OmicsCNN, a dedicated deep learning \nframework. Here we demonstrate OmicsCNN on gut microbiota sequencing data, for \nInflammatory Bowel Disease (IBD) 16S data, first on synthetic data and then a \nmetagenomics collection of gut microbiota of 222 IBD patients. \n</p>"}, "author": "Giuseppe Jurman, Valerio Maggio, Diego Fioravanti, Ylenia Giarratano, Isotta Landi, Margherita Francescatto, Claudio Agostinelli, Marco Chierici, Manlio De Domenico, Cesare Furlanello", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715973", "id": "tag:google.com,2005:reader/item/0000000321219bea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sparse Linear Isotonic Models. (arXiv:1710.05989v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05989"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05989", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In machine learning and data mining, linear models have been widely used to \nmodel the response as parametric linear functions of the predictors. To relax \nsuch stringent assumptions made by parametric linear models, additive models \nconsider the response to be a summation of unknown transformations applied on \nthe predictors; in particular, additive isotonic models (AIMs) assume the \nunknown transformations to be monotone. In this paper, we introduce sparse \nlinear isotonic models (SLIMs) for highdimensional problems by hybridizing \nideas in parametric sparse linear models and AIMs, which enjoy a few appealing \nadvantages over both. In the high-dimensional setting, a two-step algorithm is \nproposed for estimating the sparse parameters as well as the monotone functions \nover predictors. Under mild statistical assumptions, we show that the algorithm \ncan accurately estimate the parameters. Promising preliminary experiments are \npresented to support the theoretical results. \n</p>"}, "author": "Sheng Chen, Arindam Banerjee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715972", "id": "tag:google.com,2005:reader/item/0000000321219bf2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "VAMPnets: Deep learning of molecular kinetics. (arXiv:1710.06012v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06012"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06012", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3ce70aee\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3ce70aee&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Here we develop a deep learning framework for molecular kinetics from \nmolecular dynamics (MD) simulation data. There is an increasing demand for \ncomputing the relevant structures, equilibria and long-timescale kinetics of \ncomplex biomolecular processes, such as protein-drug binding, from \nhigh-throughput MD simulations. State-of-the art methods employ a handcrafted \ndata processing pipeline, involving (i) transformation of simulated coordinates \ninto a set of features characterizing the molecular structure, (ii) dimension \nreduction to collective variables, (iii) clustering the dimension-reduced data, \nand (iv) estimation of a Markov state model (MSM) or related model of the \ninterconversion rates between molecular structures. This approach demands a \nsubstantial amount of modeling expertise, as poor decisions at every step will \nlead to large modeling errors. Here we employ the recently developed \nvariational approach for Markov processes (VAMP) to develop a deep learning \nframework for molecular kinetics using neural networks, dubbed VAMPnets. A \nVAMPnet encodes the entire mapping from molecular coordinates to Markov states \nand learns optimal feature transformations, nonlinear dimension reduction, \ncluster discretization and MSM estimation within a single end-to-end learning \nframework. Our results, ranging from toy models to protein folding, are \ncompetitive or outperform state-of-the art Markov modeling methods and readily \nprovide easily interpretable few-state kinetic models. \n</p>"}, "author": "Andreas Mardt, Luca Pasquali, Hao Wu, Frank No&#xe9;", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715971", "id": "tag:google.com,2005:reader/item/0000000321219bf9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Linear Regression with Sparsely Permuted Data. (arXiv:1710.06030v1 [math.ST])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06030"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06030", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In regression analysis of multivariate data, it is tacitly assumed that \nresponse and predictor variables in each observed response-predictor pair \ncorrespond to the same entity or unit. In this paper, we consider the situation \nof \"permuted data\" in which this basic correspondence has been lost. Several \nrecent papers have considered this situation without further assumptions on the \nunderlying permutation. In applications, the latter is often to known to have \nadditional structure that can be leveraged. Specifically, we herein consider \nthe common scenario of \"sparsely permuted data\" in which only a small fraction \nof the data is affected by a mismatch between response and predictors. However, \nan adverse effect already observed for sparsely permuted data is that the least \nsquares estimator as well as other estimators not accounting for such partial \nmismatch are inconsistent. One approach studied in detail herein is to treat \npermuted data as outliers which motivates the use of robust regression \nformulations to estimate the regression parameter. The resulting estimate can \nsubsequently be used to recover the permutation. A notable benefit of the \nproposed approach is its computational simplicity given the general lack of \nprocedures for the above problem that are both statistically sound and \ncomputationally appealing. \n</p>"}, "author": "Martin Slawski, Emanuel Ben-David", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715970", "id": "tag:google.com,2005:reader/item/0000000321219bfd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Variance Reduction for Policy Gradient Estimation. (arXiv:1710.06034v1 [cs.LG])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06034"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06034", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent advances in policy gradient methods and deep learning have \ndemonstrated their applicability for complex reinforcement learning problems. \nHowever, the variance of the performance gradient estimates obtained from the \nsimulation is often excessive, leading to poor sample efficiency. In this \npaper, we apply the stochastic variance reduced gradient descent (SVRG) to \nmodel-free policy gradient to significantly improve the sample-efficiency. The \nSVRG estimation is incorporated into a trust-region Newton conjugate gradient \nframework for the policy optimization. On several Mujoco tasks, our method \nachieves significantly better performance compared to the state-of-the-art \nmodel-free policy gradient methods in robotic continuous control such as trust \nregion policy optimization (TRPO) \n</p>"}, "author": "Tianbing Xu, Qiang Liu, Jian Peng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715969", "id": "tag:google.com,2005:reader/item/0000000321219c12", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts. (arXiv:1710.06071v1 [cs.CL])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06071"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06071", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present PubMed 200k RCT, a new dataset based on PubMed for sequential \nsentence classification. The dataset consists of approximately 200,000 \nabstracts of randomized controlled trials, totaling 2.3 million sentences. Each \nsentence of each abstract is labeled with their role in the abstract using one \nof the following classes: background, objective, method, result, or conclusion. \nThe purpose of releasing this dataset is twofold. First, the majority of \ndatasets for sequential short-text classification (i.e., classification of \nshort texts that appear in sequences) are small: we hope that releasing a new \nlarge dataset will help develop more accurate algorithms for this task. Second, \nfrom an application perspective, researchers need better tools to efficiently \nskim through the literature. Automatically classifying each sentence in an \nabstract would help researchers read abstracts more efficiently, especially in \nfields where abstracts may be long, such as the medical field. \n</p>"}, "author": "Franck Dernoncourt, Ji Young Lee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715968", "id": "tag:google.com,2005:reader/item/0000000321219c14", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Estimate exponential memory decay in Hidden Markov Model and its applications. (arXiv:1710.06078v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06078"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06078", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Inference in hidden Markov model has been challenging in terms of scalability \ndue to dependencies in the observation data. In this paper, we utilize the \ninherent memory decay in hidden Markov models, such that the forward and \nbackward probabilities can be carried out with subsequences, enabling efficient \ninference over long sequences of observations. We formulate this forward \nfiltering process in the setting of the random dynamical system and there exist \nLyapunov exponents in the i.i.d random matrices production. And the rate of the \nmemory decay is known as $\\lambda_2-\\lambda_1$, the gap of the top two Lyapunov \nexponents almost surely. An efficient and accurate algorithm is proposed to \nnumerically estimate the gap after the soft-max parametrization. The length of \nsubsequences $B$ given the controlled error $\\epsilon$ is \n$B=\\log(\\epsilon)/(\\lambda_2-\\lambda_1)$. We theoretically prove the validity \nof the algorithm and demonstrate the effectiveness with numerical examples. The \nmethod developed here can be applied to widely used algorithms, such as \nmini-batch stochastic gradient method. Moreover, the continuity of Lyapunov \nspectrum ensures the estimated $B$ could be reused for the nearby parameter \nduring the inference. \n</p>"}, "author": "Felix X.-F. Ye, Yi-an Ma, Hong Qian", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715967", "id": "tag:google.com,2005:reader/item/0000000321219c18", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Discovering Adversarial Examples with Momentum. (arXiv:1710.06081v1 [cs.LG])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06081"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06081", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine learning models, especially Deep Neural Networks, are vulnerable to \nadversarial examples---malicious inputs crafted by adding small noises to real \nexamples, but fool the models. Adversarial examples transfer from one model to \nanother, enabling black-box attacks to real-world applications. In this paper, \nwe propose a strong attack algorithm named momentum iterative fast gradient \nsign method (MI-FGSM) to discover adversarial examples. MI-FGSM is an extension \nof iterative fast gradient sign method (I-FGSM) but improves the \ntransferability significantly. Besides, we study how to attack an ensemble of \nmodels efficiently. Experiments demonstrate the effectiveness of the proposed \nalgorithm. We hope that MI-FGSM can serve as a benchmark attack algorithm for \nevaluating the robustness of various models and defense methods. \n</p>"}, "author": "Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Xiaolin Hu, Jun Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715966", "id": "tag:google.com,2005:reader/item/0000000321219c22", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the challenges of learning with inference networks on sparse, high-dimensional data. (arXiv:1710.06085v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06085"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06085", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study parameter estimation in Nonlinear Factor Analysis (NFA) where the \ngenerative model is parameterized by a deep neural network. Recent work has \nfocused on learning such models using inference (or recognition) networks; we \nidentify a crucial problem when modeling large, sparse, high-dimensional \ndatasets -- underfitting. We study the extent of underfitting, highlighting \nthat its severity increases with the sparsity of the data. We propose methods \nto tackle it via iterative optimization inspired by stochastic variational \ninference \\citep{hoffman2013stochastic} and improvements in the sparse data \nrepresentation used for inference. The proposed techniques drastically improve \nthe ability of these powerful models to fit sparse data, achieving \nstate-of-the-art results on a benchmark text-count dataset and excellent \nresults on the task of top-N recommendation. \n</p>"}, "author": "Rahul G. Krishnan, Dawen Liang, Matthew Hoffman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715965", "id": "tag:google.com,2005:reader/item/0000000321219c35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Detecting Bias in Black-Box Models Using Transparent Model Distillation. (arXiv:1710.06169v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06169"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06169", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Black-box risk scoring models permeate our lives, yet are typically \nproprietary and opaque. We propose a transparent model distillation approach to \nunderstand and detect bias in such models. Model distillation was originally \ndesigned to distill knowledge from a large, complex model (the teacher model) \nto a faster, simpler model (the student model) without significant loss in \nprediction accuracy. We add a third restriction - transparency - and show that \nit is possible to train transparent, yet still accurate student models to \nunderstand the predictions made by black-box teacher models. Central to our \napproach is the use of data sets that contain two labels to train on: the risk \nscore as well as the actual outcome the risk score was intended to predict. We \nfully characterize the asymptotic distribution of the difference between the \nrisk score and actual outcome models with variance estimates based on \nbootstrap-of-little-bags. This suggests a new method to detect bias in \nblack-box risk scores via assessing if contributions of protected features to \nthe risk score are statistically different from contributions to the actual \noutcome. \n</p>"}, "author": "Sarah Tan, Rich Caruana, Giles Hooker, Yin Lou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715964", "id": "tag:google.com,2005:reader/item/0000000321219c52", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Gaussian Covariance Network. (arXiv:1710.06202v2 [cs.LG] UPDATED)", "published": 1509323746, "updated": 1509323757, "canonical": [{"href": "http://arxiv.org/abs/1710.06202"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06202", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The correlation length-scale next to the noise variance are the most used \nhyperparameters for the Gaussian processes. Typically, stationary covariance \nfunctions are used, which are only dependent on the distances between input \npoints and thus invariant to the translations in the input space. The \noptimization of the hyperparameters is commonly done by maximizing the log \nmarginal likelihood. This works quite well, if the distances are uniform \ndistributed. In the case of a locally adapted or even sparse input space, the \nprediction of a test point can be worse dependent of its position. A possible \nsolution to this, is the usage of a non-stationary covariance function, where \nthe hyperparameters are calculated by a deep neural network. So that the \ncorrelation length scales and possibly the noise variance are dependent on the \ntest point. Furthermore, different types of covariance functions are trained \nsimultaneously, so that the Gaussian process prediction is an additive overlay \nof different covariance matrices. The right covariance functions combination \nand its hyperparameters are learned by the deep neural network. Additional, the \nGaussian process will be able to be trained by batches or online and so it can \nhandle arbitrarily large data sets. We call this framework Deep Gaussian \nCovariance Network (DGCP). There are also further extensions to this framework \npossible, for example sequentially dependent problems like time series or the \nlocal mixture of experts. The basic framework and some extension possibilities \nwill be presented in this work. Moreover, a comparison to some recent state of \nthe art surrogate model methods will be performed, also for a time dependent \nproblem. \n</p>"}, "author": "Kevin Cremanns, Dirk Roos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715963", "id": "tag:google.com,2005:reader/item/0000000321219c62", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning to Transfer Initializations for Bayesian Hyperparameter Optimization. (arXiv:1710.06219v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06219"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06219", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Hyperparameter optimization undergoes extensive evaluations of validation \nerrors in order to find the best configuration of hyperparameters. Bayesian \noptimization is now popular for hyperparameter optimization, since it reduces \nthe number of validation error evaluations required. Suppose that we are given \na collection of datasets on which hyperparameters are already tuned by either \nhumans with domain expertise or extensive trials of cross-validation. When a \nmodel is applied to a new dataset, it is desirable to let Bayesian \nhyperparameter optimzation start from configurations that were successful on \nsimilar datasets. To this end, we construct a Siamese network with \nconvolutional layers followed by bi-directional LSTM layers, to learn {\\em \nmeta-features} over datasets. Learned meta-features are used to select a few \ndatasets that are similar to the new dataset, so that a set of configurations \nin similar datasets is adopted as initializations for Bayesian hyperparameter \noptimization. Experiments on image datasets demonstrate that our learned \nmeta-features are useful in optimizing several hyperparameters in deep residual \nnetworks for image classification. \n</p>"}, "author": "Jungtaek Kim, Saehoon Kim, Seungjin Choi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715962", "id": "tag:google.com,2005:reader/item/0000000321219c6b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Nonlinear Interference Mitigation via Deep Neural Networks. (arXiv:1710.06234v1 [cs.IT])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06234"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06234", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3ce71016\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3ce71016&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A neural-network-based approach is presented to efficiently implement digital \nbackpropagation (DBP). For a 32x100 km fiber-optic link, the resulting \n\"learned\" DBP significantly reduces the complexity compared to conventional DBP \nimplementations. \n</p>"}, "author": "Christian H&#xe4;ger, Henry D. Pfister", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715961", "id": "tag:google.com,2005:reader/item/0000000321219c7b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convergence Rate of Riemannian Hamiltonian Monte Carlo and Faster Polytope Volume Computation. (arXiv:1710.06261v1 [cs.DS])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06261"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06261", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3ceecb08\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3ceecb08&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We give the first rigorous proof of the convergence of Riemannian Hamiltonian \nMonte Carlo, a general (and practical) method for sampling Gibbs distributions. \nOur analysis shows that the rate of convergence is bounded in terms of natural \nsmoothness parameters of an associated Riemannian manifold. We then apply the \nmethod with the manifold defined by the log barrier function to the problems of \n(1) uniformly sampling a polytope and (2) computing its volume, the latter by \nextending Gaussian cooling to the manifold setting. In both cases, the total \nnumber of steps needed is O^{*}(mn^{\\frac{2}{3}}), improving the state of the \nart. A key ingredient of our analysis is a proof of an analog of the KLS \nconjecture for Gibbs distributions over manifolds. \n</p>"}, "author": "Yin Tat Lee, Santosh S. Vempala", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715960", "id": "tag:google.com,2005:reader/item/0000000321219c88", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Combinatorial Penalties: Which structures are preserved by convex relaxations?. (arXiv:1710.06273v1 [cs.LG])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06273"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06273", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the homogeneous and the non-homogeneous convex relaxations for \ncombinatorial penalty functions defined on support sets. Our study identifies \nkey differences in the tightness of the resulting relaxations through the \nnotion of the lower combinatorial envelope of a set-function along with new \nnecessary conditions for support identification. We then propose a general \nadaptive estimator for convex monotone regularizers, and derive new sufficient \nconditions for support recovery in the asymptotic setting. \n</p>"}, "author": "Marwa El Halabi, Francis Bach, Volkan Cevher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715959", "id": "tag:google.com,2005:reader/item/0000000321219c91", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Smooth and Sparse Optimal Transport. (arXiv:1710.06276v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06276"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06276", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Entropic regularization is quickly emerging as a new standard in optimal \ntransport (OT). It enables to cast the OT computation as a differentiable and \nunconstrained convex optimization problem, which can be efficiently solved \nusing the Sinkhorn algorithm. However, the entropy term keeps the \ntransportation plan strictly positive and therefore completely dense, unlike \nunregularized OT. This lack of sparsity can be problematic for applications \nwhere the transportation plan itself is of interest. In this paper, we explore \nregularizing both the primal and dual original formulations with an arbitrary \nstrongly convex term. We show that this corresponds to relaxing dual and primal \nconstraints with approximate smooth constraints. We show how to incorporate \nsquared 2-norm and group lasso regularizations within that framework, leading \nto sparse and group-sparse transportation plans. On the theoretical side, we \nare able to bound the approximation error introduced by smoothing the original \nprimal and dual formulations. Our results suggest that, for the smoothed dual, \nthe approximation error can often be smaller with squared 2-norm regularization \nthan with entropic regularization. We showcase our proposed framework on the \ntask of color transfer. \n</p>"}, "author": "Mathieu Blondel, Vivien Seguy, Antoine Rolet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715958", "id": "tag:google.com,2005:reader/item/0000000321219ca8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Good Arm Identification via Bandit Feedback. (arXiv:1710.06360v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06360"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06360", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we consider and discuss a new stochastic multi-armed bandit \nproblem called {\\em good arm identification} (GAI), where a good arm is an arm \nwith expected reward greater than or equal to a given threshold. GAI is a \npure-exploration problem that an agent repeats a process of outputting an arm \nas soon as it is identified as a good one before confirming the other arms are \nactually not good. The objective of GAI is to minimize the number of samples \nfor each process. We find that GAI faces a new kind of dilemma, the {\\em \nexploration-exploitation dilemma of confidence}, while best arm identification \ndoes not. Therefore, GAI is not just an extension of the best arm \nidentification. Actually, an efficient design of algorithms for GAI is quite \ndifferent from that for best arm identification. We derive a lower bound on the \nsample complexity for GAI and develop an algorithm whose sample complexity \nalmost matches the lower bound. We also confirm experimentally that the \nproposed algorithm outperforms a naive algorithm and a thresholding-bandit-like \nalgorithm in synthetic settings and in settings based on medical data. \n</p>"}, "author": "Hideaki Kano, Junya Honda, Kentaro Sakamaki, Kentaro Matsuura, Atsuyoshi Nakamura, Masashi Sugiyama", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715957", "id": "tag:google.com,2005:reader/item/0000000321219cc6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convergence diagnostics for stochastic gradient descent with constant step size. (arXiv:1710.06382v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06382"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06382", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Iterative procedures in stochastic optimization are typically comprised of a \ntransient phase and a stationary phase. During the transient phase the \nprocedure converges towards a region of interest, and during the stationary \nphase the procedure oscillates in a convergence region, commonly around a \nsingle point. In this paper, we develop a statistical diagnostic test to detect \nsuch phase transition in the context of stochastic gradient descent with \nconstant step size. We present theoretical and experimental results suggesting \nthat the diagnostic behaves as intended, and the region where the diagnostic is \nactivated coincides with the convergence region. For a class of loss functions, \nwe derive a closed-form solution describing such region, and support this \ntheoretical result with simulated experiments. Finally, we suggest an \napplication to speed up convergence of stochastic gradient descent by halving \nthe learning rate each time convergence is detected. This leads to remarkable \nspeed gains that are empirically comparable to state-of-art procedures. \n</p>"}, "author": "Jerry Chee, Panos Toulis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508289709375", "timestampUsec": "1508289709374727", "id": "tag:google.com,2005:reader/item/0000000320f681e9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Searching for Activation Functions. (arXiv:1710.05941v2 [cs.NE] UPDATED)", "published": 1509325338, "updated": 1509325338, "canonical": [{"href": "http://arxiv.org/abs/1710.05941"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05941", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The choice of activation functions in deep networks has a significant effect \non the training dynamics and task performance. Currently, the most successful \nand widely-used activation function is the Rectified Linear Unit (ReLU). \nAlthough various hand-designed alternatives to ReLU have been proposed, none \nhave managed to replace it due to inconsistent gains. In this work, we propose \nto leverage automatic search techniques to discover new activation functions. \nUsing a combination of exhaustive and reinforcement learning-based search, we \ndiscover multiple novel activation functions. We verify the effectiveness of \nthe searches by conducting an empirical evaluation with the best discovered \nactivation function. Our experiments show that the best discovered activation \nfunction, $f(x) = x \\cdot \\text{sigmoid}(\\beta x)$, which we name Swish, tends \nto work better than ReLU on deeper models across a number of challenging \ndatasets. For example, simply replacing ReLUs with Swish units improves top-1 \nclassification accuracy on ImageNet by 0.9\\% for Mobile NASNet-A and 0.6\\% for \nInception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it \neasy for practitioners to replace ReLUs with Swish units in any neural network. \n</p>"}, "author": "Prajit Ramachandran, Barret Zoph, Quoc V. Le", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508289709375", "timestampUsec": "1508289709374726", "id": "tag:google.com,2005:reader/item/0000000320f681ec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Evolution in Virtual Worlds. (arXiv:1710.06055v1 [cs.NE])", "published": 1508289710, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06055"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06055", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This chapter discusses the possibility of instilling a virtual world with \nmechanisms for evolution and natural selection in order to generate rich \necosystems of complex organisms in a process akin to biological evolution. Some \nprevious work in the area is described, and successes and failures are \ndiscussed. The components of a more comprehensive framework for designing such \nworlds are mapped out, including the design of the individual organisms, the \nproperties and dynamics of the environmental medium in which they are evolving, \nand the representational relationship between organism and environment. Some of \nthe key issues discussed include how to allow organisms to evolve new \nstructures and functions with few restrictions, and how to create an \ninterconnectedness between organisms in order to generate drives for continuing \nevolutionary activity. \n</p>"}, "author": "Tim Taylor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893785", "id": "tag:google.com,2005:reader/item/00000003208f6155", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Functional Decision Theory: A New Theory of Instrumental Rationality. (arXiv:1710.05060v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05060"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05060", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper describes and motivates a new decision theory known as functional \ndecision theory (FDT), as distinct from causal decision theory and evidential \ndecision theory. Functional decision theorists hold that the normative \nprinciple for action is to treat one's decision as the output of a fixed \nmathematical function that answers the question, \"Which output of this very \nfunction would yield the best outcome?\" Adhering to this principle delivers a \nnumber of benefits, including the ability to maximize wealth in an array of \ntraditional decision-theoretic and game-theoretic problems where CDT and EDT \nperform poorly. Using one simple and coherent decision rule, functional \ndecision theorists (for example) achieve more utility than CDT on Newcomb's \nproblem, more utility than EDT on the smoking lesion problem, and more utility \nthan both in Parfit's hitchhiker problem. In this paper, we define FDT, explore \nits prescriptions in a number of different decision problems, compare it to CDT \nand EDT, and give philosophical justifications for FDT as a normative theory of \ndecision-making. \n</p>"}, "author": "Eliezer Yudkowsky, Nate Soares", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893784", "id": "tag:google.com,2005:reader/item/00000003208f615e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the Ontological Modeling of Trees. (arXiv:1710.05096v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05096"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05096", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Trees -- i.e., the type of data structure known under this name -- are \ncentral to many aspects of knowledge organization. We investigate some central \ndesign choices concerning the ontological modeling of such trees. In \nparticular, we consider the limits of what is expressible in the Web Ontology \nLanguage, and provide a reusable ontology design pattern for trees. \n</p>"}, "author": "David Carral, Pascal Hitzler, Hilmar Lapp, Sebastian Rudolph", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893783", "id": "tag:google.com,2005:reader/item/00000003208f6168", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Community Aware Random Walk for Network Embedding. (arXiv:1710.05199v1 [cs.SI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05199"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05199", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3ceecd65\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3ceecd65&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Social network analysis provides meaningful information about behavior of \nnetwork members that can be used in diverse applications such as \nclassification, link prediction, etc. however, network analysis is \ncomputationally expensive because of feature learning for different \napplications. In recent years, many researches have focused on feature learning \nmethods in social networks. Network embedding represents the network in a lower \ndimensional representation space with the same properties which presents a \ncompressed representation of the input network. In this paper, we introduce a \nnovel algorithm named \"CARE\" for network embedding that can be used for \ndifferent types of networks including weighted, directed and complex. While \ncurrent methods try to preserve local neighborhood information of nodes, we \nutilize local neighborhood and community information of network nodes to cover \nboth local and global structure of social networks. CARE builds customized \npaths, which are consisted of local and global structure of network nodes, as a \nbasis for network embedding and uses skip-gram model to learn representation \nvector of nodes. Then, stochastic gradient descent is used to optimize our \nobjective function and learn the final representation of nodes. Our method can \nbe scalable when new nodes are appended to network without information loss. \nParallelize generation of customized random walks is also used for speeding up \nCARE. We evaluate the performance of CARE on multi label classification and \nlink prediction tasks. Experimental results on different networks indicate that \nthe proposed method outperforms others in both Micro-f1 and Macro-f1 measures \nfor different size of training data. \n</p>"}, "author": "Mohammad Mehdi Keikha, Maseud Rahgozar, Masoud Asadpour", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893782", "id": "tag:google.com,2005:reader/item/00000003208f6178", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Network Model Selection Using Task-Focused Minimum Description Length. (arXiv:1710.05207v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05207"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05207", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Networks are fundamental models for data used in practically every \napplication domain. In most instances, several implicit or explicit choices \nabout the network definition impact the translation of underlying data to a \nnetwork representation, and the subsequent question(s) about the underlying \nsystem being represented. Users of downstream network data may not even be \naware of these choices or their impacts. We propose a task-focused network \nmodel selection methodology which addresses several key challenges. Our \napproach constructs network models from underlying data and uses minimum \ndescription length (MDL) criteria for selection. Our methodology measures \nefficiency, a general and comparable measure of the network's performance of a \nlocal (i.e. node-level) predictive task of interest. Selection on efficiency \nfavors parsimonious (e.g. sparse) models to avoid overfitting and can be \napplied across arbitrary tasks and representations (including networks and \nnon-network models). We show stability, sensitivity, and significance testing \nin our methodology. \n</p>"}, "author": "Ivan Brugere, Tanya Y. Berger-Wolf", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893781", "id": "tag:google.com,2005:reader/item/00000003208f6181", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Mental Sampling in Multimodal Representations. (arXiv:1710.05219v1 [cs.LG])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05219"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05219", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Both resources in the natural environment and concepts in a semantic space \nare distributed \"patchily\", with large gaps in between the patches. To describe \npeople's internal and external foraging behavior, various random walk models \nhave been proposed. In particular, internal foraging has been modeled as \nsampling: in order to gather relevant information for making a decision, people \ndraw samples from a mental representation using random-walk algorithms such as \nMarkov chain Monte Carlo (MCMC). However, two common empirical observations \nargue against simple sampling algorithms such as MCMC. First, the spatial \nstructure is often best described by a L\\'evy flight distribution: the \nprobability of the distance between two successive locations follows a \npower-law on the distances. Second, the temporal structure of the sampling that \nhumans and other animals produce have long-range, slowly decaying serial \ncorrelations characterized as $1/f$-like fluctuations. We propose that mental \nsampling is not done by simple MCMC, but is instead adapted to multimodal \nrepresentations and is implemented by Metropolis-coupled Markov chain Monte \nCarlo (MC$^3$), one of the first algorithms developed for sampling from \nmultimodal distributions. MC$^3$ involves running multiple Markov chains in \nparallel but with target distributions of different temperatures, and it swaps \nthe states of the chains whenever a better location is found. Heated chains \nmore readily traverse valleys in the probability landscape to propose moves to \nfar-away peaks, while the colder chains make the local steps that explore the \ncurrent peak or patch. We show that MC$^3$ generates distances between \nsuccessive samples that follow a L\\'evy flight distribution and $1/f$-like \nserial correlations, providing a single mechanistic account of these two \npuzzling empirical phenomena. \n</p>"}, "author": "Jian-Qiao Zhu, Adam N. Sanborn, Nick Chater", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893780", "id": "tag:google.com,2005:reader/item/00000003208f619a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learners that Leak Little Information. (arXiv:1710.05233v1 [cs.LG])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05233"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05233", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study learning algorithms that are restricted to revealing little \ninformation about their input sample. Various manifestations of this notion \nhave been recently studied. A central theme in these works, and in ours, is \nthat such algorithms generalize. We study a category of learning algorithms, \nwhich we term d-bit information learners}. These are algorithms whose output \nconveys at most d bits of information on their input. \n</p> \n<p>We focus on the learning capacity of such algorithms: we prove generalization \nbounds with tight dependencies on the confidence and error parameters. We \nobserve connections with well studied notions such as PAC-Bayes and \ndifferential privacy. For example, it is known that pure differentially private \nalgorithms leak little information. We complement this fact with a separation \nbetween bounded information and pure differential privacy in the setting of \nproper learning, showing that differential privacy is strictly more \nrestrictive. \n</p> \n<p>We also demonstrate limitations by exhibiting simple concept classes for \nwhich every (possibly randomized) empirical risk minimizer must leak a lot of \ninformation. On the other hand, we show that in the distribution-dependent \nsetting every VC class has empirical risk minimizers that do not leak a lot of \ninformation. \n</p>"}, "author": "Raef Bassily, Shay Moran, Ido Nachum, Jonathan Shafer, Amir Yehudayoff", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893779", "id": "tag:google.com,2005:reader/item/00000003208f61a5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On Hashing-Based Approaches to Approximate DNF-Counting. (arXiv:1710.05247v1 [cs.LO])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05247"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05247", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Propositional model counting is a fundamental problem in artificial \nintelligence with a wide variety of applications, such as probabilistic \ninference, decision making under uncertainty, and probabilistic databases. \nConsequently, the problem is of theoretical as well as practical interest. When \nthe constraints are expressed as DNF formulas, Monte Carlo-based techniques \nhave been shown to provide a fully polynomial randomized approximation scheme \n(FPRAS). For CNF constraints, hashing-based approximation techniques have been \ndemonstrated to be highly successful. Furthermore, it was shown that \nhashing-based techniques also yield an FPRAS for DNF counting without usage of \nMonte Carlo sampling. Our analysis, however, shows that the proposed \nhashing-based approach to DNF counting provides poor time complexity compared \nto the Monte Carlo-based DNF counting techniques. Given the success of \nhashing-based techniques for CNF constraints, it is natural to ask: Can \nhashing-based techniques provide an efficient FPRAS for DNF counting? In this \npaper, we provide a positive answer to this question. To this end, we introduce \ntwo novel algorithmic techniques: \\emph{Symbolic Hashing} and \\emph{Stochastic \nCell Counting}, along with a new hash family of \\emph{Row-Echelon hash \nfunctions}. These innovations allow us to design a hashing-based FPRAS for DNF \ncounting of similar complexity (up to polylog factors) as that of prior works. \nFurthermore, we expect these techniques to have potential applications beyond \nDNF counting. \n</p>"}, "author": "Kuldeep S. Meel (1), Aditya A. Shrotri (2), Moshe Y. Vardi (2) ((1) National University of Singapore, (2) Rice University)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893778", "id": "tag:google.com,2005:reader/item/00000003208f61bb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multi-Value Rule Sets. (arXiv:1710.05257v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05257"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05257", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present the Multi-vAlue Rule Set (MARS) model for interpretable \nclassification with feature efficient presentations. MARS introduces a more \ngeneralized form of association rules that allows multiple values in a \ncondition. Rules of this form are more concise than traditional single-valued \nrules in capturing and describing patterns in data. MARS mitigates the problem \nof dealing with continuous features and high-cardinality categorical features \nfaced by rule-based models. Our formulation also pursues a higher efficiency of \nfeature utilization, which reduces the cognitive load to understand the \ndecision process. We propose an efficient inference method for learning a \nmaximum a posteriori model, incorporating theoretically grounded bounds to \niteratively reduce the search space to improve search efficiency. Experiments \nwith synthetic and real-world data demonstrate that MARS models have \nsignificantly smaller complexity and fewer features, providing better \ninterpretability while being competitive in predictive accuracy. We conducted a \nusability study with human subjects and results show that MARS is the easiest \nto use compared with other competing rule-based models, in terms of the correct \nrate and response time. Overall, MARS introduces a new approach to rule-based \nmodels that balance accuracy and interpretability with feature-efficient \nrepresentations. \n</p>"}, "author": "Tong Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893777", "id": "tag:google.com,2005:reader/item/00000003208f61c5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Infinite RBMs with Frank-Wolfe. (arXiv:1710.05270v1 [cs.LG])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05270"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05270", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work, we propose an infinite restricted Boltzmann machine~(RBM), \nwhose maximum likelihood estimation~(MLE) corresponds to a constrained convex \noptimization. We consider the Frank-Wolfe algorithm to solve the program, which \nprovides a sparse solution that can be interpreted as inserting a hidden unit \nat each iteration, so that the optimization process takes the form of a \nsequence of finite models of increasing complexity. As a side benefit, this can \nbe used to easily and efficiently identify an appropriate number of hidden \nunits during the optimization. The resulting model can also be used as an \ninitialization for typical state-of-the-art RBM training algorithms such as \ncontrastive divergence, leading to models with consistently higher test \nlikelihood than random initialization. \n</p>"}, "author": "Wei Ping, Qiang Liu, Alexander Ihler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893776", "id": "tag:google.com,2005:reader/item/00000003208f61e9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Complete Extensions do not form a Complete Semilattice. (arXiv:1710.05341v2 [cs.AI] UPDATED)", "published": 1509064764, "updated": 1509064765, "canonical": [{"href": "http://arxiv.org/abs/1710.05341"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05341", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In his seminal paper that inaugurated abstract argumentation, Dung proved \nthat the set of complete extensions forms a complete semilattice with respect \nto set inclusion. In this note we demonstrate that this proof is incorrect with \ncounterexamples. We then trace the error in the proof and explain why it arose. \nWe then examine the implications for the grounded extension. \n</p> \n<p>[Reason for withdrawal continued] Page 4, Example 2 is not a counterexample \nto Dung 1995 Theorem 25(3). It was believed to be a counter-example because the \nauthor misunderstood ``glb'' to be set-theoretic intersection. But in this \ncase, ``glb'' is defined to be other than set-theoretic intersection such that \nTheorem 25(3) is true. \n</p> \n<p>The author was motivated to fully understand the lattice-theoretic claims of \nDung 1995 in writing this note and was not aware that this issue is probably \nfolklore; the author bears full responsibility for this error. \n</p>"}, "author": "Anthony P. Young", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893775", "id": "tag:google.com,2005:reader/item/00000003208f61f7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A systematic study of the class imbalance problem in convolutional neural networks. (arXiv:1710.05381v1 [cs.CV])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05381"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05381", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this study, we systematically investigate the impact of class imbalance on \nclassification performance of convolutional neural networks (CNNs) and compare \nfrequently used methods to address the issue. Class imbalance is a common \nproblem that has been comprehensively studied in classical machine learning, \nyet very limited systematic research is available in the context of deep \nlearning. In our study, we use three benchmark datasets of increasing \ncomplexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of \nimbalance on classification and perform an extensive comparison of several \nmethods to address the issue: oversampling, undersampling, two-phase training, \nand thresholding that compensates for prior class probabilities. Our main \nevaluation metric is area under the receiver operating characteristic curve \n(ROC AUC) adjusted to multi-class tasks since overall accuracy metric is \nassociated with notable difficulties in the context of imbalanced data. Based \non results from our experiments we conclude that (i) the effect of class \nimbalance on classification performance is detrimental; (ii) the method of \naddressing class imbalance that emerged as dominant in almost all analyzed \nscenarios was oversampling; (iii) oversampling should be applied to the level \nthat totally eliminates the imbalance, whereas undersampling can perform better \nwhen the imbalance is only removed to some extent; (iv) as opposed to some \nclassical machine learning models, oversampling does not necessarily cause \noverfitting of CNNs; (v) thresholding should be applied to compensate for prior \nclass probabilities when overall number of properly classified cases is of \ninterest. \n</p>"}, "author": "Mateusz Buda, Atsuto Maki, Maciej A. Mazurowski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893774", "id": "tag:google.com,2005:reader/item/00000003208f620c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Manifold Regularization for Kernelized LSTD. (arXiv:1710.05387v1 [cs.LG])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05387"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05387", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Policy evaluation or value function or Q-function approximation is a key \nprocedure in reinforcement learning (RL). It is a necessary component of policy \niteration and can be used for variance reduction in policy gradient methods. \nTherefore its quality has a significant impact on most RL algorithms. Motivated \nby manifold regularized learning, we propose a novel kernelized policy \nevaluation method that takes advantage of the intrinsic geometry of the state \nspace learned from data, in order to achieve better sample efficiency and \nhigher accuracy in Q-function approximation. Applying the proposed method in \nthe Least-Squares Policy Iteration (LSPI) framework, we observe superior \nperformance compared to widely used parametric basis functions on two standard \nbenchmarks in terms of policy quality. \n</p>"}, "author": "Xinyan Yan, Krzysztof Choromanski, Byron Boots, Vikas Sindhwani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893773", "id": "tag:google.com,2005:reader/item/00000003208f6224", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Causal Rule Sets for Identifying Subgroups with Enhanced Treatment Effect. (arXiv:1710.05426v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05426"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05426", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3ceecf96\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3ceecf96&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We introduce a novel generative model for interpretable subgroup analysis for \ncausal inference applications, Causal Rule Sets (CRS). A CRS model uses a small \nset of short rules to capture a subgroup where the average treatment effect is \nelevated compared to the entire population. We present a Bayesian framework for \nlearning a causal rule set. The Bayesian framework consists of a prior that \nfavors simpler models and a Bayesian logistic regression that characterizes the \nrelation between outcomes, attributes and subgroup membership. We find maximum \na posteriori models using discrete Monte Carlo steps in the joint solution \nspace of rules sets and parameters. We provide theoretically grounded \nheuristics and bounding strategies to improve search efficiency. Experiments \nshow that the search algorithm can efficiently recover a true underlying \nsubgroup and CRS shows consistently competitive performance compared to other \nstate-of-the-art baseline methods. \n</p>"}, "author": "Tong Wang, Cynthia Rudin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893772", "id": "tag:google.com,2005:reader/item/00000003208f6237", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Flow: Architecture and Benchmarking for Reinforcement Learning in Traffic Control. (arXiv:1710.05465v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05465"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05465", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cf64795\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cf64795&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Flow is a new computational framework, built to support a key need triggered \nby the rapid growth of autonomy in ground traffic: controllers for autonomous \nvehicles in the presence of complex nonlinear dynamics in traffic. Leveraging \nrecent advances in deep Reinforcement Learning (RL), Flow enables the use of RL \nmethods such as policy gradient for traffic control and enables benchmarking \nthe performance of classical (including hand-designed) controllers with learned \npolicies (control laws). Flow integrates traffic microsimulator SUMO with deep \nreinforcement learning library rllab and enables the easy design of traffic \ntasks, including different networks configurations and vehicle dynamics. We use \nFlow to develop reliable controllers for complex problems, such as controlling \nmixed-autonomy traffic (involving both autonomous and human-driven vehicles) in \na ring road. For this, we first show that state-of-the-art hand-designed \ncontrollers excel when in-distribution, but fail to generalize; then, we show \nthat even simple neural network policies can solve the stabilization task \nacross density settings and generalize to out-of-distribution settings. \n</p>"}, "author": "Cathy Wu, Aboudy Kreidieh, Kanaad Parvate, Eugene Vinitsky, Alexandre M Bayen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893771", "id": "tag:google.com,2005:reader/item/00000003208f623e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generalization in Deep Learning. (arXiv:1710.05468v1 [stat.ML])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05468"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05468", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper explains why deep learning can generalize well, despite large \ncapacity and possible algorithmic instability, nonrobustness, and sharp minima, \neffectively addressing an open problem in the literature. Based on our \ntheoretical insight, this paper also proposes a family of new regularization \nmethods. Its simplest member was empirically shown to improve base models and \nachieve state-of-the-art performance on MNIST and CIFAR-10 benchmarks. \nMoreover, this paper presents both data-dependent and data-independent \ngeneralization guarantees with improved convergence rates. Our results suggest \nseveral new open areas of research. \n</p>"}, "author": "Kenji Kawaguchi, Leslie Pack Kaelbling, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893770", "id": "tag:google.com,2005:reader/item/00000003208f6244", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Toward Crowd-Sensitive Path Planning. (arXiv:1710.05503v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05503"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05503", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>If a robot can predict crowds in parts of its environment that are \ninaccessible to its sensors, then it can plan to avoid them. This paper \nproposes a fast, online algorithm that learns average crowd densities in \ndifferent areas. It also describes how these densities can be incorporated into \nexisting navigation architectures. In simulation across multiple challenging \ncrowd scenarios, the robot reaches its target faster, travels less, and risks \nfewer collisions than if it were to plan with the traditional A* algorithm. \n</p>"}, "author": "Anoop Aroor, Susan L. Epstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893769", "id": "tag:google.com,2005:reader/item/00000003208f624d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Intention-Net: Integrating Planning and Deep Learning for Goal-Directed Autonomous Navigation. (arXiv:1710.05627v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05627"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05627", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>How can a delivery robot navigate reliably to a destination in a new office \nbuilding, with minimal prior information? To tackle this challenge, this paper \nintroduces a two-level hierarchical approach, which integrates model-free deep \nlearning and model-based path planning. At the low level, a neural-network \nmotion controller, called the intention-net, is trained end-to-end to provide \nrobust local navigation. The intention-net maps images from a single monocular \ncamera and \"intentions\" directly to robot controls. At the high level, a path \nplanner uses a crude map, e.g., a 2-D floor plan, to compute a path from the \nrobot's current location to the goal. The planned path provides intentions to \nthe intention-net. Preliminary experiments suggest that the learned motion \ncontroller is robust against perceptual uncertainty and by integrating with a \npath planner, it generalizes effectively to new environments and goals. \n</p>"}, "author": "Gao Wei, David Hus, Wee Sun Lee, Shengmei Shen, Karthikk Subramanian", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893768", "id": "tag:google.com,2005:reader/item/00000003208f625e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Mining Frequent Patterns in Process Models. (arXiv:1710.05693v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05693"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05693", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Process mining has emerged as a way to analyze the behavior of an \norganization by extracting knowledge from event logs and by offering techniques \nto discover, monitor and enhance real processes. In the discovery of process \nmodels, retrieving a complex one, i.e., a hardly readable process model, can \nhinder the extraction of information. Even in well-structured process models, \nthere is information that cannot be obtained with the current techniques. In \nthis paper, we present WoMine, an algorithm to retrieve frequent behavioural \npatterns from the model. Our approach searches in process models extracting \nstructures with sequences, selections, parallels and loops, which are \nfrequently executed in the logs. This proposal has been validated with a set of \nprocess models, including some from BPI Challenges, and compared with the state \nof the art techniques. Experiments have validated that WoMine can find all \ntypes of patterns, extracting information that cannot be mined with the state \nof the art techniques. \n</p>"}, "author": "David Chapela-Campa, Manuel Mucientes, Manuel Lama", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893767", "id": "tag:google.com,2005:reader/item/00000003208f626e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Survey on Optical Character Recognition System. (arXiv:1710.05703v1 [cs.CV])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05703"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05703", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Optical Character Recognition (OCR) has been a topic of interest for many \nyears. It is defined as the process of digitizing a document image into its \nconstituent characters. Despite decades of intense research, developing OCR \nwith capabilities comparable to that of human still remains an open challenge. \nDue to this challenging nature, researchers from industry and academic circles \nhave directed their attentions towards Optical Character Recognition. Over the \nlast few years, the number of academic laboratories and companies involved in \nresearch on Character Recognition has increased dramatically. This research \naims at summarizing the research so far done in the field of OCR. It provides \nan overview of different aspects of OCR and discusses corresponding proposals \naimed at resolving issues of OCR. \n</p>"}, "author": "Noman Islam, Zeeshan Islam, Nazia Noor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893766", "id": "tag:google.com,2005:reader/item/00000003208f627f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "ACCBench: A Framework for Comparing Causality Algorithms. (arXiv:1710.05720v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05720"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05720", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Modern socio-technical systems are increasingly complex. A fundamental \nproblem is that the borders of such systems are often not well-defined \na-priori, which among other problems can lead to unwanted behavior during \nruntime. Ideally, unwanted behavior should be prevented. If this is not \npossible the system shall at least be able to help determine potential cause(s) \na-posterori, identify responsible parties and make them accountable for their \nbehavior. Recently, several algorithms addressing these concepts have been \nproposed. However, the applicability of the corresponding approaches, \nspecifically their effectiveness and performance, is mostly unknown. Therefore, \nin this paper, we propose ACCBench, a benchmark tool that allows to compare and \nevaluate causality algorithms under a consistent setting. Furthermore, we \ncontribute an implementation of the two causality algorithms by G\\\"o{\\ss}ler \nand Metayer and G\\\"o{\\ss}ler and Astefanoaei as well as of a policy compliance \napproach based on some concepts of Main et al. Lastly, we conduct a case study \nof an Intelligent Door Control System, which exposes concrete strengths and \nweaknesses of all algorithms under different aspects. In the course of this, we \nshow that the effectiveness of the algorithms in terms of cause detection as \nwell as their performance differ to some extent. In addition, our analysis \nreports on some qualitative aspects that should be considered when evaluating \neach algorithm. For example, the human effort needed to configure the algorithm \nand model the use case is analyzed. \n</p>"}, "author": "Simon Rehwald, Amjad Ibrahim, Kristian Beckers, Alexander Pretschner", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893765", "id": "tag:google.com,2005:reader/item/00000003208f62a9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Characterizing Driving Context from Driver Behavior. (arXiv:1710.05733v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05733"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05733", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Because of the increasing availability of spatiotemporal data, a variety of \ndata-analytic applications have become possible. Characterizing driving \ncontext, where context may be thought of as a combination of location and time, \nis a new challenging application. An example of such a characterization is \nfinding the correlation between driving behavior and traffic conditions. This \ncontextual information enables analysts to validate observation-based \nhypotheses about the driving of an individual. In this paper, we present \nDriveContext, a novel framework to find the characteristics of a context, by \nextracting significant driving patterns (e.g., a slow-down), and then \nidentifying the set of potential causes behind patterns (e.g., traffic \ncongestion). Our experimental results confirm the feasibility of the framework \nin identifying meaningful driving patterns, with improvements in comparison \nwith the state-of-the-art. We also demonstrate how the framework derives \ninteresting characteristics for different contexts, through real-world \nexamples. \n</p>"}, "author": "Sobhan Moosavi, Behrooz Omidvar-Tehrani, R. Bruce Craig, Arnab Nandi, Rajiv Ramnath", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893764", "id": "tag:google.com,2005:reader/item/00000003208f62b7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "A retrieval-based dialogue system utilizing utterance and context embeddings. (arXiv:1710.05780v1 [cs.CL])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05780"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05780", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Finding semantically rich and computer-understandable representations for \ntextual dialogues, utterances and words is crucial for dialogue systems (or \nconversational agents), as their performance mostly depends on understanding \nthe context of conversations. Recent research aims at finding distributed \nvector representations (embeddings) for words, such that semantically similar \nwords are relatively close within the vector-space. Encoding the \"meaning\" of \ntext into vectors is a current trend, and text can range from words, phrases \nand documents to actual human-to-human conversations. In recent research \napproaches, responses have been generated utilizing a decoder architecture, \ngiven the vector representation of the current conversation. In this paper, the \nutilization of embeddings for answer retrieval is explored by using \nLocality-Sensitive Hashing Forest (LSH Forest), an Approximate Nearest Neighbor \n(ANN) model, to find similar conversations in a corpus and rank possible \ncandidates. Experimental results on the well-known Ubuntu Corpus (in English) \nand a customer service chat dataset (in Dutch) show that, in combination with a \ncandidate selection method, retrieval-based approaches outperform generative \nones and reveal promising future research directions towards the usability of \nsuch a system. \n</p>"}, "author": "Alexander Bartl, Gerasimos Spanakis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893757", "id": "tag:google.com,2005:reader/item/00000003208f6318", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sparse Depth Sensing for Resource-Constrained Robots. (arXiv:1703.01398v3 [cs.RO] CROSS LISTED)", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.01398"}], "alternate": [{"href": "http://arxiv.org/abs/1703.01398", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cf64abd\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cf64abd&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We consider the case in which a robot has to navigate in an unknown \nenvironment but does not have enough on-board power or payload to carry a \ntraditional depth sensor (e.g., a 3D lidar) and thus can only acquire a few \n(point-wise) depth measurements. We address the following question: is it \npossible to reconstruct the geometry of an unknown environment using sparse and \nincomplete depth measurements? Reconstruction from incomplete data is not \npossible in general, but when the robot operates in man-made environments, the \ndepth exhibits some regularity (e.g., many planar surfaces with only a few \nedges); we leverage this regularity to infer depth from a small number of \nmeasurements. Our first contribution is a formulation of the depth \nreconstruction problem that bridges robot perception with the compressive \nsensing literature in signal processing. The second contribution includes a set \nof formal results that ascertain the exactness and stability of the depth \nreconstruction in 2D and 3D problems, and completely characterize the geometry \nof the profiles that we can reconstruct. Our third contribution is a set of \npractical algorithms for depth reconstruction: our formulation directly \ntranslates into algorithms for depth estimation based on convex programming. In \nreal-world problems, these convex programs are very large and general-purpose \nsolvers are relatively slow. For this reason, we discuss ad-hoc solvers that \nenable fast depth reconstruction in real problems. The last contribution is an \nextensive experimental evaluation in 2D and 3D problems, including Monte Carlo \nruns on simulated instances and testing on multiple real datasets. Empirical \nresults confirm that the proposed approach ensures accurate depth \nreconstruction, outperforms interpolation-based strategies, and performs well \neven when the assumption of structured environment is violated. \n</p>"}, "author": "Fangchang Ma, Luca Carlone, Ulas Ayaz, Sertac Karaman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893756", "id": "tag:google.com,2005:reader/item/00000003208f6324", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Influence of Personal Preferences on Link Dynamics in Social Networks. (arXiv:1709.07401v1 [cs.SI] CROSS LISTED)", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.07401"}], "alternate": [{"href": "http://arxiv.org/abs/1709.07401", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study a unique network dataset including periodic surveys and electronic \nlogs of dyadic contacts via smartphones. The participants were a sample of \nfreshmen entering university in the Fall 2011. Their opinions on a variety of \npolitical and social issues and lists of activities on campus were regularly \nrecorded at the beginning and end of each semester for the first three years of \nstudy. We identify a behavioral network defined by call and text data, and a \ncognitive network based on friendship nominations in ego-network surveys. Both \nnetworks are limited to study participants. Since a wide range of attributes on \neach node were collected in self-reports, we refer to these networks as \nattribute-rich networks. We study whether student preferences for certain \nattributes of friends can predict formation and dissolution of edges in both \nnetworks. We introduce a method for computing student preferences for different \nattributes which we use to predict link formation and dissolution. We then rank \nthese attributes according to their importance for making predictions. We find \nthat personal preferences, in particular political views, and preferences for \ncommon activities help predict link formation and dissolution in both the \nbehavioral and cognitive networks. \n</p>"}, "author": "Ashwin Bahulkar, Boleslaw K. Szymanski, Nitesh Chawla, Omar Lizardo, Kevin Chan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508233034373", "timestampUsec": "1508233034372876", "id": "tag:google.com,2005:reader/item/000000032079b1c9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A graphical, scalable and intuitive method for the placement and the connection of biological cells. (arXiv:1710.05189v1 [cs.NE])", "published": 1508233035, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05189"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05189", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce a graphical method originating from the computer graphics domain \nthat is used for the arbitrary and intuitive placement of cells over a \ntwo-dimensional manifold. Using a bitmap image as input, where the color \nindicates the identity of the different structures and the alpha channel \nindicates the local cell density, this method guarantees a discrete \ndistribution of cell position respecting the local density function. This \nmethod scales to any number of cells, allows to specify several different \nstructures at once with arbitrary shapes and provides a scalable and versatile \nalternative to the more classical assumption of a uniform non-spatial \ndistribution. Furthermore, several connection schemes can be derived from the \npaired distances between cells using either an automatic mapping or a \nuser-defined local reference frame, providing new computational properties for \nthe underlying model. The method is illustrated on a discrete homogeneous \nneural field, on the distribution of cones and rods in the retina and on a \ncoronal view of the basal ganglia. \n</p>"}, "author": "Nicolas P. Rougier", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508233034373", "timestampUsec": "1508233034372875", "id": "tag:google.com,2005:reader/item/000000032079b1e6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Vector Quantization using the Improved Differential Evolution Algorithm for Image Compression. (arXiv:1710.05311v1 [cs.CV])", "published": 1508233035, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05311"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05311", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Vector Quantization, VQ is a popular image compression technique with a \nsimple decoding architecture and high compression ratio. Codebook designing is \nthe most essential part in Vector Quantization. LindeBuzoGray, LBG is a \ntraditional method of generation of VQ Codebook which results in lower PSNR \nvalue. A Codebook affects the quality of image compression, so the choice of an \nappropriate codebook is a must. Several optimization techniques have been \nproposed for global codebook generation to enhance the quality of image \ncompression. In this paper, a novel algorithm called IDE-LBG is proposed which \nuses Improved Differential Evolution Algorithm coupled with LBG for generating \noptimum VQ Codebooks. The proposed IDE works better than the traditional DE \nwith modifications in the scaling factor and the boundary control mechanism. \nThe IDE generates better solutions by efficient exploration and exploitation of \nthe search space. Then the best optimal solution obtained by the IDE is \nprovided as the initial Codebook for the LBG. This approach produces an \nefficient Codebook with less computational time and the consequences include \nexcellent PSNR values and superior quality reconstructed images. It is observed \nthat the proposed IDE-LBG find better VQ Codebooks as compared to IPSO-LBG, \nBA-LBG and FA-LBG. \n</p>"}, "author": "Sayan Nag", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508233034373", "timestampUsec": "1508233034372874", "id": "tag:google.com,2005:reader/item/000000032079b214", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A systematic study of the class imbalance problem in convolutional neural networks. (arXiv:1710.05381v1 [cs.CV])", "published": 1508233035, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05381"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05381", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this study, we systematically investigate the impact of class imbalance on \nclassification performance of convolutional neural networks (CNNs) and compare \nfrequently used methods to address the issue. Class imbalance is a common \nproblem that has been comprehensively studied in classical machine learning, \nyet very limited systematic research is available in the context of deep \nlearning. In our study, we use three benchmark datasets of increasing \ncomplexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of \nimbalance on classification and perform an extensive comparison of several \nmethods to address the issue: oversampling, undersampling, two-phase training, \nand thresholding that compensates for prior class probabilities. Our main \nevaluation metric is area under the receiver operating characteristic curve \n(ROC AUC) adjusted to multi-class tasks since overall accuracy metric is \nassociated with notable difficulties in the context of imbalanced data. Based \non results from our experiments we conclude that (i) the effect of class \nimbalance on classification performance is detrimental; (ii) the method of \naddressing class imbalance that emerged as dominant in almost all analyzed \nscenarios was oversampling; (iii) oversampling should be applied to the level \nthat totally eliminates the imbalance, whereas undersampling can perform better \nwhen the imbalance is only removed to some extent; (iv) as opposed to some \nclassical machine learning models, oversampling does not necessarily cause \noverfitting of CNNs; (v) thresholding should be applied to compensate for prior \nclass probabilities when overall number of properly classified cases is of \ninterest. \n</p>"}, "author": "Mateusz Buda, Atsuto Maki, Maciej A. Mazurowski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508233034373", "timestampUsec": "1508233034372873", "id": "tag:google.com,2005:reader/item/000000032079b229", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generalization in Deep Learning. (arXiv:1710.05468v1 [stat.ML])", "published": 1508233035, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05468"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05468", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper explains why deep learning can generalize well, despite large \ncapacity and possible algorithmic instability, nonrobustness, and sharp minima, \neffectively addressing an open problem in the literature. Based on our \ntheoretical insight, this paper also proposes a family of new regularization \nmethods. Its simplest member was empirically shown to improve base models and \nachieve state-of-the-art performance on MNIST and CIFAR-10 benchmarks. \nMoreover, this paper presents both data-dependent and data-independent \ngeneralization guarantees with improved convergence rates. Our results suggest \nseveral new open areas of research. \n</p>"}, "author": "Kenji Kawaguchi, Leslie Pack Kaelbling, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489109", "id": "tag:google.com,2005:reader/item/000000032065be68", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Independent Features with Adversarial Nets for Non-linear ICA. (arXiv:1710.05050v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05050"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05050", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Reliable measures of statistical dependence could be useful tools for \nlearning independent features and performing tasks like source separation using \nIndependent Component Analysis (ICA). Unfortunately, many of such measures, \nlike the mutual information, are hard to estimate and optimize directly. We \npropose to learn independent features with adversarial objectives which \noptimize such measures implicitly. These objectives compare samples from the \njoint distribution and the product of the marginals without the need to compute \nany probability densities. We also propose two methods for obtaining samples \nfrom the product of the marginals using either a simple resampling trick or a \nseparate parametric distribution. Our experiments show that this strategy can \neasily be applied to different types of model architectures and solve both \nlinear and non-linear ICA problems. \n</p>"}, "author": "Philemon Brakel, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489108", "id": "tag:google.com,2005:reader/item/000000032065bf4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Automated Scalable Bayesian Inference via Hilbert Coresets. (arXiv:1710.05053v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05053"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05053", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The automation of posterior inference in Bayesian data analysis has enabled \nexperts and nonexperts alike to use more sophisticated models, engage in faster \nexploratory modeling and analysis, and ensure experimental reproducibility. \nHowever, standard automated posterior inference algorithms are not tractable at \nthe scale of massive modern datasets, and modifications to make them so are \ntypically model-specific, require expert tuning, and can break theoretical \nguarantees on inferential quality. Building on the Bayesian coresets framework, \nthis work instead takes advantage of data redundancy to shrink the dataset \nitself as a preprocessing step, providing fully-automated, scalable Bayesian \ninference with theoretical guarantees. We begin with an intuitive reformulation \nof Bayesian coreset construction as sparse vector sum approximation, and \ndemonstrate that its automation and performance-based shortcomings arise from \nthe use of the supremum norm. To address these shortcomings we develop Hilbert \ncoresets, i.e., Bayesian coresets constructed under a norm induced by an \ninner-product on the log-likelihood function space. We propose two Hilbert \ncoreset construction algorithms---one based on importance sampling, and one \nbased on the Frank-Wolfe algorithm---along with theoretical guarantees on \napproximation quality as a function of coreset size. Since the exact \ncomputation of the proposed inner-products is model-specific, we automate the \nconstruction with a random finite-dimensional projection of the log-likelihood \nfunctions. The resulting automated coreset construction algorithm is simple to \nimplement, and experiments on a variety of models with real and synthetic \ndatasets show that it provides high-quality posterior approximations and a \nsignificant reduction in the computational cost of inference. \n</p>"}, "author": "Trevor Campbell, Tamara Broderick", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489107", "id": "tag:google.com,2005:reader/item/000000032065bfc3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "DSCOVR: Randomized Primal-Dual Block Coordinate Algorithms for Asynchronous Distributed Optimization. (arXiv:1710.05080v1 [math.OC])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05080"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05080", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine learning with big data often involves large optimization models. For \ndistributed optimization over a cluster of machines, frequent communication and \nsynchronization of all model parameters (optimization variables) can be very \ncostly. A promising solution is to use parameter servers to store different \nsubsets of the model parameters, and update them asynchronously at different \nmachines using local datasets. In this paper, we focus on distributed \noptimization of large linear models with convex loss functions, and propose a \nfamily of randomized primal-dual block coordinate algorithms that are \nespecially suitable for asynchronous distributed implementation with parameter \nservers. In particular, we work with the saddle-point formulation of such \nproblems which allows simultaneous data and model partitioning, and exploit its \nstructure by doubly stochastic coordinate optimization with variance reduction \n(DSCOVR). Compared with other first-order distributed algorithms, we show that \nDSCOVR may require less amount of overall computation and communication, and \nless or no synchronization. We discuss the implementation details of the DSCOVR \nalgorithms, and present numerical experiments on an industrial distributed \ncomputing system. \n</p>"}, "author": "Lin Xiao, Adams Wei Yu, Qihang Lin, Weizhu Chen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489106", "id": "tag:google.com,2005:reader/item/000000032065c014", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A deep generative model for single-cell RNA sequencing with application to detecting differentially expressed genes. (arXiv:1710.05086v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05086"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05086", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a probabilistic model for interpreting gene expression levels that \nare observed through single-cell RNA sequencing. In the model, each cell has a \nlow-dimensional latent representation. Additional latent variables account for \ntechnical effects that may erroneously set some observations of gene expression \nlevels to zero. Conditional distributions are specified by neural networks, \ngiving the proposed model enough flexibility to fit the data well. We use \nvariational inference and stochastic optimization to approximate the posterior \ndistribution. The inference procedure scales to over one million cells, whereas \ncompeting algorithms do not. Even for smaller datasets, for several tasks, the \nproposed procedure outperforms state-of-the-art methods like ZIFA and \nZINB-WaVE. We also extend our framework to take into account batch effects and \nother confounding factors and propose a natural Bayesian hypothesis framework \nfor differential expression that outperforms tradition DESeq2. \n</p>"}, "author": "Romain Lopez, Jeffrey Regier, Michael Cole, Michael Jordan, Nir Yosef", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489105", "id": "tag:google.com,2005:reader/item/000000032065c07e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Burn-In Demonstrations for Multi-Modal Imitation Learning. (arXiv:1710.05090v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05090"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05090", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cf64d03\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cf64d03&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Recent work on imitation learning has generated policies that reproduce \nexpert behavior from multi-modal data. However, past approaches have focused \nonly on recreating a small number of distinct, expert maneuvers, or have relied \non supervised learning techniques that produce unstable policies. This work \nextends InfoGAIL, an algorithm for multi-modal imitation learning, to reproduce \nbehavior over an extended period of time. Our approach involves reformulating \nthe typical imitation learning setting to include \"burn-in demonstrations\" upon \nwhich policies are conditioned at test time. We demonstrate that our approach \noutperforms standard InfoGAIL in maximizing the mutual information between \npredicted and unseen style labels in road scene simulations, and we show that \nour method leads to policies that imitate expert autonomous driving systems \nover long time horizons. \n</p>"}, "author": "Alex Kuefler, Mykel J. Kochenderfer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489104", "id": "tag:google.com,2005:reader/item/000000032065c0e8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A simple data discretizer. (arXiv:1710.05091v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05091"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05091", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cfe172c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cfe172c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Data discretization is an important step in the process of machine learning, \nsince it is easier for classifiers to deal with discrete attributes rather than \ncontinuous attributes. Over the years, several methods of performing \ndiscretization such as Boolean Reasoning, Equal Frequency Binning, Entropy have \nbeen proposed, explored, and implemented. In this article, a simple supervised \ndiscretization approach is introduced. The prime goal of MIL is to maximize \nclassification accuracy of classifier, minimizing loss of information while \ndiscretization of continuous attributes. The performance of the suggested \napproach is compared with the supervised discretization algorithm Minimum \nInformation Loss (MIL), using the state-of-the-art rule inductive algorithms- \nJ48 (Java implementation of C4.5 classifier). The presented approach is, \nindeed, the modified version of MIL. The empirical results show that the \nmodified approach performs better in several cases in comparison to the \noriginal MIL algorithm and Minimum Description Length Principle (MDLP) . \n</p>"}, "author": "Gourab Mitra, Shashidhar Sundareisan, Bikash Kanti Sarkar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489103", "id": "tag:google.com,2005:reader/item/000000032065c133", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Dropout as a Low-Rank Regularizer for Matrix Factorization. (arXiv:1710.05092v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05092"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05092", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Regularization for matrix factorization (MF) and approximation problems has \nbeen carried out in many different ways. Due to its popularity in deep \nlearning, dropout has been applied also for this class of problems. Despite its \nsolid empirical performance, the theoretical properties of dropout as a \nregularizer remain quite elusive for this class of problems. In this paper, we \npresent a theoretical analysis of dropout for MF, where Bernoulli random \nvariables are used to drop columns of the factors. We demonstrate the \nequivalence between dropout and a fully deterministic model for MF in which the \nfactors are regularized by the sum of the product of squared Euclidean norms of \nthe columns. Additionally, we inspect the case of a variable sized \nfactorization and we prove that dropout achieves the global minimum of a convex \napproximation problem with (squared) nuclear norm regularization. As a result, \nwe conclude that dropout can be used as a low-rank regularizer with data \ndependent singular-value thresholding. \n</p>"}, "author": "Jacopo Cavazza, Pietro Morerio, Benjamin Haeffele, Connor Lane, Vittorio Murino, Rene Vidal", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489102", "id": "tag:google.com,2005:reader/item/000000032065c183", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Unsupervised Real-Time Control through Variational Empowerment. (arXiv:1710.05101v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05101"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05101", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce a methodology for efficiently computing a lower bound to \nempowerment, allowing it to be used as an unsupervised cost function for policy \nlearning in real-time control. Empowerment, being the channel capacity between \nactions and states, maximises the influence of an agent on its near future. It \nhas been shown to be a good model of biological behaviour in the absence of an \nextrinsic goal. But empowerment is also prohibitively hard to compute, \nespecially in nonlinear continuous spaces. We introduce an efficient, amortised \nmethod for learning empowerment-maximising policies. We demonstrate that our \nalgorithm can reliably handle continuous dynamical systems using system \ndynamics learned from raw data. The resulting policies consistently drive the \nagents into states where they can use their full potential. \n</p>"}, "author": "Maximilian Karl, Maximilian Soelch, Philip Becker-Ehmck, Djalel Benbouzid, Patrick van der Smagt, Justin Bayer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489101", "id": "tag:google.com,2005:reader/item/000000032065c1cf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Arbitrage-Free Regularization. (arXiv:1710.05114v1 [q-fin.MF])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05114"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05114", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce a path-dependent geometric framework which generalizes the HJM \nmodeling approach to a wide variety of other asset classes. A machine learning \nregularization framework is developed with the objective of removing arbitrage \nopportunities from models within this general framework. The regularization \nmethod relies on minimal deformations of a model subject to a path-dependent \npenalty that detects arbitrage opportunities. We prove that the solution of \nthis regularization problem is independent of the arbitrage-penalty chosen, \nsubject to a fixed information loss functional. In addition to the general \nproperties of the minimal deformation, we also consider several explicit \nexamples. This paper is focused on placing machine learning methods in finance \non a sound theoretical basis and the techniques developed to achieve this \nobjective may be of interest in other areas of application. \n</p>"}, "author": "Anastasia Kratsios, Cody B. Hyndman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489100", "id": "tag:google.com,2005:reader/item/000000032065c241", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Benefits from Superposed Hawkes Processes. (arXiv:1710.05115v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05115"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05115", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The superposition of temporal point processes has been studied for many \nyears, although the usefulness of such models for practical applications has \nnot be fully developed. We investigate superposed Hawkes process as an \nimportant class of such models, with properties studied in the framework of \nleast squares estimation. The superposition of Hawkes processes is demonstrated \nto be beneficial for tightening the upper bound of excess risk under certain \nconditions, and we show the feasibility of the benefit in typical situations. \nThe usefulness of superposed Hawkes processes is verified on synthetic data, \nand its potential to solve the cold-start problem of recommendation systems is \ndemonstrated on real-world data. \n</p>"}, "author": "Hongteng Xu, Dixin Luo, Xu Chen, Lawrence Carin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489099", "id": "tag:google.com,2005:reader/item/000000032065c280", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "When Point Process Meets RNNs: Predicting Fine-Grained User Interests with Mutual Behavioral Infectivity. (arXiv:1710.05135v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05135"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05135", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Predicting fine-grained interests of users with temporal behavior is \nimportant to personalization and information filtering applications. However, \nexisting interest prediction methods are incapable of capturing the subtle \ndegreed user interests towards particular items, and the internal time-varying \ndrifting attention of individuals is not studied yet. Moreover, the prediction \nprocess can also be affected by inter-personal influence, known as behavioral \nmutual infectivity. Inspired by point process in modeling temporal point \nprocess, in this paper we present a deep prediction method based on two \nrecurrent neural networks (RNNs) to jointly model each user's continuous \nbrowsing history and asynchronous event sequences in the context of inter-user \nbehavioral mutual infectivity. Our model is able to predict the fine-grained \ninterest from a user regarding a particular item and corresponding timestamps \nwhen an occurrence of event takes place. The proposed approach is more flexible \nto capture the dynamic characteristic of event sequences by using the temporal \npoint process to model event data and timely update its intensity function by \nRNNs. Furthermore, to improve the interpretability of the model, the attention \nmechanism is introduced to emphasize both intra-personal and inter-personal \nbehavior influence over time. Experiments on real datasets demonstrate that our \nmodel outperforms the state-of-the-art methods in fine-grained user interest \nprediction. \n</p>"}, "author": "Tong Chen, Lin Wu, Yang Wang, Jun Zhang, Hongxu Chen, Xue Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489098", "id": "tag:google.com,2005:reader/item/000000032065c2c7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Improved Modified Cholesky Decomposition Method for Inverse Covariance Matrix Estimation. (arXiv:1710.05163v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05163"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05163", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The modified Cholesky decomposition is commonly used for inverse covariance \nmatrix estimation given a specified order of random variables. However, the \norder of variables is often not available or cannot be pre-determined. Hence, \nwe propose a novel estimator to address the variable order issue in the \nmodified Cholesky decomposition to estimate the sparse inverse covariance \nmatrix. The key idea is to effectively combine a set of estimates obtained from \nmultiple permutations of variable orders, and to efficiently encourage the \nsparse structure for the resultant estimate by the use of thresholding \ntechnique on the combined Cholesky factor matrix. The consistent property of \nthe proposed estimate is established under some weak regularity conditions. \nSimulation studies show the superior performance of the proposed method in \ncomparison with several existing approaches. We also apply the proposed method \ninto the linear discriminant analysis for analyzing real-data examples for \nclassification. \n</p>"}, "author": "Xiaoning Kang, Xinwei Deng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489097", "id": "tag:google.com,2005:reader/item/000000032065c2ea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Simultaneous Matrix Diagonalization for Structural Brain Networks Classification. (arXiv:1710.05213v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05213"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05213", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper considers the problem of brain disease classification based on \nconnectome data. A connectome is a network representation of a human brain. The \ntypical connectome classification problem is very challenging because of the \nsmall sample size and high dimensionality of the data. We propose to use \nsimultaneous approximate diagonalization of adjacency matrices in order to \ncompute their eigenstructures in more stable way. The obtained approximate \neigenvalues are further used as features for classification. The proposed \napproach is demonstrated to be efficient for detection of Alzheimer's disease, \noutperforming simple baselines and competing with state-of-the-art approaches \nto brain disease classification. \n</p>"}, "author": "Nikita Mokrov, Maxim Panov, Boris A. Gutman, Joshua I. Faskowitz, Neda Jahanshad, Paul M. Thompson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489096", "id": "tag:google.com,2005:reader/item/000000032065c32f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Robust Federated Learning Using ADMM in the Presence of Data Falsifying Byzantines. (arXiv:1710.05241v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05241"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05241", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we consider the problem of federated (or decentralized) \nlearning using ADMM with multiple agents. We consider a scenario where a \ncertain fraction of agents (referred to as Byzantines) provide falsified data \nto the system. In this context, we study the convergence behavior of the \ndecentralized ADMM algorithm. We show that ADMM converges linearly to a \nneighborhood of the solution to the problem under certain conditions. We next \nprovide guidelines for network structure design to achieve faster convergence. \nNext, we provide necessary conditions on the falsified updates for exact \nconvergence to the true solution. To tackle the data falsification problem, we \npropose a robust variant of ADMM. We also provide simulation results to \nvalidate the analysis and show the resilience of the proposed algorithm to \nByzantines. \n</p>"}, "author": "Qunwei Li, Bhavya Kailkhura, Ryan Goldhahn, Priyadip Ray, Pramod K. Varshney", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489095", "id": "tag:google.com,2005:reader/item/000000032065c391", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Infinite RBMs with Frank-Wolfe. (arXiv:1710.05270v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05270"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05270", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cfe1abf\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cfe1abf&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this work, we propose an infinite restricted Boltzmann machine~(RBM), \nwhose maximum likelihood estimation~(MLE) corresponds to a constrained convex \noptimization. We consider the Frank-Wolfe algorithm to solve the program, which \nprovides a sparse solution that can be interpreted as inserting a hidden unit \nat each iteration, so that the optimization process takes the form of a \nsequence of finite models of increasing complexity. As a side benefit, this can \nbe used to easily and efficiently identify an appropriate number of hidden \nunits during the optimization. The resulting model can also be used as an \ninitialization for typical state-of-the-art RBM training algorithms such as \ncontrastive divergence, leading to models with consistently higher test \nlikelihood than random initialization. \n</p>"}, "author": "Wei Ping, Qiang Liu, Alexander Ihler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489094", "id": "tag:google.com,2005:reader/item/000000032065c3ba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Facial Keypoints Detection. (arXiv:1710.05279v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05279"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05279", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Detect facial keypoints is a critical element in face recognition. However, \nthere is difficulty to catch keypoints on the face due to complex influences \nfrom original images, and there is no guidance to suitable algorithms. In this \npaper, we study different algorithms that can be applied to locate keyponits. \nSpecifically: our framework (1)prepare the data for further investigation \n(2)Using PCA and LBP to process the data (3) Apply different algorithms to \nanalysis data, including linear regression models, tree based model, neural \nnetwork and convolutional neural network, etc. Finally we will give our \nconclusion and further research topic. A comprehensive set of experiments on \ndataset demonstrates the effectiveness of our framework. \n</p>"}, "author": "Shenghao Shi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489093", "id": "tag:google.com,2005:reader/item/000000032065c3d5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Accelerated Block Coordinate Proximal Gradients with Applications in High Dimensional Statistics. (arXiv:1710.05338v3 [math.OC] UPDATED)", "published": 1509323746, "updated": 1509323757, "canonical": [{"href": "http://arxiv.org/abs/1710.05338"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05338", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Nonconvex optimization problems arise in different research fields and arouse \nlots of attention in signal processing, statistics and machine learning. In \nthis work, we explore the accelerated proximal gradient method and some of its \nvariants which have been shown to converge under nonconvex context recently. We \nshow that a novel variant proposed here, which exploits adaptive momentum and \nblock coordinate update with specific update rules, further improves the \nperformance of a broad class of nonconvex problems. In applications to sparse \nlinear regression with regularizations like Lasso, grouped Lasso, capped \n$\\ell_1$ and SCAP, the proposed scheme enjoys provable local linear \nconvergence, with experimental justification. \n</p>"}, "author": "Tsz Kit Lau", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489092", "id": "tag:google.com,2005:reader/item/000000032065c412", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Estimation of Squared-Loss Mutual Information from Positive and Unlabeled Data. (arXiv:1710.05359v2 [stat.ML] UPDATED)", "published": 1510139306, "updated": 1510139311, "canonical": [{"href": "http://arxiv.org/abs/1710.05359"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05359", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Capturing input-output dependency is an important task in statistical data \nanalysis. Mutual information (MI) is a vital tool for this purpose, but it is \nknown to be sensitive to outliers. To cope with this problem, a squared-loss \nvariant of MI (SMI) was proposed, and its supervised estimator has been \ndeveloped. On the other hand, in real-world classification problems, it is \nconceivable that only positive and unlabeled (PU) data are available. In this \npaper, we propose a novel estimator of SMI only from PU data, and prove its \noptimal convergence to true SMI. Based on the PU-SMI estimator, we further \npropose a dimension reduction method which can be executed without estimating \nthe class-prior probabilities of unlabeled data. Such PU class-prior estimation \nis often required in PU classification algorithms, but it is unreliable \nparticularly in high-dimensional problems, yielding a biased classifier. Our \ndimension reduction method significantly boosts the accuracy of PU class-prior \nestimation, as demonstrated through experiments. We also develop a method of \nindependent testing based on our PU-SMI estimator and experimentally show its \nsuperiority. \n</p>"}, "author": "Tomoya Sakai, Gang Niu, Masashi Sugiyama", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489091", "id": "tag:google.com,2005:reader/item/000000032065c453", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A systematic study of the class imbalance problem in convolutional neural networks. (arXiv:1710.05381v1 [cs.CV])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05381"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05381", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this study, we systematically investigate the impact of class imbalance on \nclassification performance of convolutional neural networks (CNNs) and compare \nfrequently used methods to address the issue. Class imbalance is a common \nproblem that has been comprehensively studied in classical machine learning, \nyet very limited systematic research is available in the context of deep \nlearning. In our study, we use three benchmark datasets of increasing \ncomplexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of \nimbalance on classification and perform an extensive comparison of several \nmethods to address the issue: oversampling, undersampling, two-phase training, \nand thresholding that compensates for prior class probabilities. Our main \nevaluation metric is area under the receiver operating characteristic curve \n(ROC AUC) adjusted to multi-class tasks since overall accuracy metric is \nassociated with notable difficulties in the context of imbalanced data. Based \non results from our experiments we conclude that (i) the effect of class \nimbalance on classification performance is detrimental; (ii) the method of \naddressing class imbalance that emerged as dominant in almost all analyzed \nscenarios was oversampling; (iii) oversampling should be applied to the level \nthat totally eliminates the imbalance, whereas undersampling can perform better \nwhen the imbalance is only removed to some extent; (iv) as opposed to some \nclassical machine learning models, oversampling does not necessarily cause \noverfitting of CNNs; (v) thresholding should be applied to compensate for prior \nclass probabilities when overall number of properly classified cases is of \ninterest. \n</p>"}, "author": "Mateusz Buda, Atsuto Maki, Maciej A. Mazurowski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489090", "id": "tag:google.com,2005:reader/item/000000032065c47a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Scaling Limit of High-Dimensional Online Independent Component Analysis. (arXiv:1710.05384v2 [cs.LG] UPDATED)", "published": 1510139306, "updated": 1510139311, "canonical": [{"href": "http://arxiv.org/abs/1710.05384"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05384", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We analyze the dynamics of an online algorithm for independent component \nanalysis in the high-dimensional scaling limit. As the ambient dimension tends \nto infinity, and with proper time scaling, we show that the time-varying joint \nempirical measure of the target feature vector and the estimates provided by \nthe algorithm will converge weakly to a deterministic measured-valued process \nthat can be characterized as the unique solution of a nonlinear PDE. Numerical \nsolutions of this PDE, which involves two spatial variables and one time \nvariable, can be efficiently obtained. These solutions provide detailed \ninformation about the performance of the ICA algorithm, as many practical \nperformance metrics are functionals of the joint empirical measures. Numerical \nsimulations show that our asymptotic analysis is accurate even for moderate \ndimensions. In addition to providing a tool for understanding the performance \nof the algorithm, our PDE analysis also provides useful insight. In particular, \nin the high-dimensional limit, the original coupled dynamics associated with \nthe algorithm will be asymptotically \"decoupled\", with each coordinate \nindependently solving a 1-D effective minimization problem via stochastic \ngradient descent. Exploiting this insight to design new algorithms for \nachieving optimal trade-offs between computational and statistical efficiency \nmay prove an interesting line of future research. \n</p>"}, "author": "Chuang Wang, Yue M. Lu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489089", "id": "tag:google.com,2005:reader/item/000000032065c490", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Manifold Regularization for Kernelized LSTD. (arXiv:1710.05387v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05387"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05387", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Policy evaluation or value function or Q-function approximation is a key \nprocedure in reinforcement learning (RL). It is a necessary component of policy \niteration and can be used for variance reduction in policy gradient methods. \nTherefore its quality has a significant impact on most RL algorithms. Motivated \nby manifold regularized learning, we propose a novel kernelized policy \nevaluation method that takes advantage of the intrinsic geometry of the state \nspace learned from data, in order to achieve better sample efficiency and \nhigher accuracy in Q-function approximation. Applying the proposed method in \nthe Least-Squares Policy Iteration (LSPI) framework, we observe superior \nperformance compared to widely used parametric basis functions on two standard \nbenchmarks in terms of policy quality. \n</p>"}, "author": "Xinyan Yan, Krzysztof Choromanski, Byron Boots, Vikas Sindhwani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489088", "id": "tag:google.com,2005:reader/item/000000032065c4e6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "NeuralPower: Predict and Deploy Energy-Efficient Convolutional Neural Networks. (arXiv:1710.05420v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05420"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05420", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>\"How much energy is consumed for an inference made by a convolutional neural \nnetwork (CNN)?\" With the increased popularity of CNNs deployed on the \nwide-spectrum of platforms (from mobile devices to workstations), the answer to \nthis question has drawn significant attention. From lengthening battery life of \nmobile devices to reducing the energy bill of a datacenter, it is important to \nunderstand the energy efficiency of CNNs during serving for making an \ninference, before actually training the model. In this work, we propose \nNeuralPower: a layer-wise predictive framework based on sparse polynomial \nregression, for predicting the serving energy consumption of a CNN deployed on \nany GPU platform. Given the architecture of a CNN, NeuralPower provides an \naccurate prediction and breakdown for power and runtime across all layers in \nthe whole network, helping machine learners quickly identify the power, \nruntime, or energy bottlenecks. We also propose the \"energy-precision ratio\" \n(EPR) metric to guide machine learners in selecting an energy-efficient CNN \narchitecture that better trades off the energy consumption and prediction \naccuracy. The experimental results show that the prediction accuracy of the \nproposed NeuralPower outperforms the best published model to date, yielding an \nimprovement in accuracy of up to 68.5%. We also assess the accuracy of \npredictions at the network level, by predicting the runtime, power, and energy \nof state-of-the-art CNN architectures, achieving an average accuracy of 88.24% \nin runtime, 88.34% in power, and 97.21% in energy. We comprehensively \ncorroborate the effectiveness of NeuralPower as a powerful framework for \nmachine learners by testing it on different GPU platforms and Deep Learning \nsoftware tools. \n</p>"}, "author": "Ermao Cai, Da-Cheng Juan, Dimitrios Stamoulis, Diana Marculescu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489087", "id": "tag:google.com,2005:reader/item/000000032065c515", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generalization in Deep Learning. (arXiv:1710.05468v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05468"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05468", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper explains why deep learning can generalize well, despite large \ncapacity and possible algorithmic instability, nonrobustness, and sharp minima, \neffectively addressing an open problem in the literature. Based on our \ntheoretical insight, this paper also proposes a family of new regularization \nmethods. Its simplest member was empirically shown to improve base models and \nachieve state-of-the-art performance on MNIST and CIFAR-10 benchmarks. \nMoreover, this paper presents both data-dependent and data-independent \ngeneralization guarantees with improved convergence rates. Our results suggest \nseveral new open areas of research. \n</p>"}, "author": "Kenji Kawaguchi, Leslie Pack Kaelbling, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489086", "id": "tag:google.com,2005:reader/item/000000032065c546", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Calibrated Boosting-Forest. (arXiv:1710.05476v2 [stat.ML] UPDATED)", "published": 1508373235, "updated": 1508373237, "canonical": [{"href": "http://arxiv.org/abs/1710.05476"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05476", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Excellent ranking power along with well calibrated probability estimates are \nneeded in many classification tasks. In this paper, we introduce a technique, \nCalibrated Boosting-Forest that captures both. This novel technique is an \nensemble of gradient boosting machines that can support both continuous and \nbinary labels. While offering superior ranking power over any individual \nregression or classification model, Calibrated Boosting-Forest is able to \npreserve well calibrated posterior probabilities. Along with these benefits, we \nprovide an alternative to the tedious step of tuning gradient boosting \nmachines. We demonstrate that tuning Calibrated Boosting-Forest can be reduced \nto a simple hyper-parameter selection. We further establish that increasing \nthis hyper-parameter improves the ranking performance under a diminishing \nreturn. We examine the effectiveness of Calibrated Boosting-Forest on \nligand-based virtual screening where both continuous and binary labels are \navailable and compare the performance of Calibrated Boosting-Forest with \nlogistic regression, gradient boosting machine and deep learning. Calibrated \nBoosting-Forest achieved an approximately 4% improvement compared to a \nstate-of-art deep learning model and has the potential to achieve an 8% \nimprovement after tuning the single hyper-parameter. Moreover, it achieved \naround 98% improvement on probability quality measurement compared to the best \nindividual gradient boosting machine. Calibrated Boosting-Forest offers a \nbenchmark demonstration that in the field of ligand-based virtual screening, \ndeep learning is not the universally dominant machine learning model and good \ncalibrated probabilities can better facilitate virtual screening process. \n</p>"}, "author": "Haozhen Wu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489085", "id": "tag:google.com,2005:reader/item/000000032065c573", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Geometric View of Optimal Transportation and Generative Model. (arXiv:1710.05488v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05488"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05488", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3cfe1d95\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3cfe1d95&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this work, we show the intrinsic relations between optimal transportation \nand convex geometry, especially the variational approach to solve Alexandrov \nproblem: constructing a convex polytope with prescribed face normals and \nvolumes. This leads to a geometric interpretation to generative models, and \nleads to a novel framework for generative models. By using the optimal \ntransportation view of GAN model, we show that the discriminator computes the \nKantorovich potential, the generator calculates the transportation map. For a \nlarge class of transportation costs, the Kantorovich potential can give the \noptimal transportation map by a close-form formula. Therefore, it is sufficient \nto solely optimize the discriminator. This shows the adversarial competition \ncan be avoided, and the computational architecture can be simplified. \nPreliminary experimental results show the geometric method outperforms WGAN for \napproximating probability measures with multiple clusters in low dimensional \nspace. \n</p>"}, "author": "Na Lei, Kehua Su, Li Cui, Shing-Tung Yau, David Xianfeng Gu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489084", "id": "tag:google.com,2005:reader/item/000000032065c5b3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Robust Maximum Likelihood Estimation of Sparse Vector Error Correction Model. (arXiv:1710.05513v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05513"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05513", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3d061f20\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3d061f20&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In econometrics and finance, the vector error correction model (VECM) is an \nimportant time series model for cointegration analysis, which is used to \nestimate the long-run equilibrium variable relationships. The traditional \nanalysis and estimation methodologies assume the underlying Gaussian \ndistribution but, in practice, heavy-tailed data and outliers can lead to the \ninapplicability of these methods. In this paper, we propose a robust model \nestimation method based on the Cauchy distribution to tackle this issue. In \naddition, sparse cointegration relations are considered to realize feature \nselection and dimension reduction. An efficient algorithm based on the \nmajorization-minimization (MM) method is applied to solve the proposed \nnonconvex problem. The performance of this algorithm is shown through numerical \nsimulations. \n</p>"}, "author": "Ziping Zhao, Daniel P. Palomar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489083", "id": "tag:google.com,2005:reader/item/000000032065c5d5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fully adaptive algorithm for pure exploration in linear bandits. (arXiv:1710.05552v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05552"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05552", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose the first fully-adaptive algorithm for pure exploration in linear \nbandits---the task to find the arm with the largest expected reward, which \ndepends on an unknown parameter linearly. While existing methods partially or \nentirely fix sequences of arm selections before observing rewards, our method \nadaptively changes the arm selection strategy based on past observations at \neach round. We show our sample complexity matches the achievable lower bound up \nto a constant factor in an extreme case. Furthermore, we evaluate the \nperformance of the methods by simulations based on both synthetic setting and \nreal-world data, in which our method shows vast improvement over existing \nmethods. \n</p>"}, "author": "Liyuan Xu, Junya Honda, Masashi Sugiyama", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489082", "id": "tag:google.com,2005:reader/item/000000032065c5f6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fair Kernel Learning. (arXiv:1710.05578v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05578"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05578", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>New social and economic activities massively exploit big data and machine \nlearning algorithms to do inference on people's lives. Applications include \nautomatic curricula evaluation, wage determination, and risk assessment for \ncredits and loans. Recently, many governments and institutions have raised \nconcerns about the lack of fairness, equity and ethics in machine learning to \ntreat these problems. It has been shown that not including sensitive features \nthat bias fairness, such as gender or race, is not enough to mitigate the \ndiscrimination when other related features are included. Instead, including \nfairness in the objective function has been shown to be more efficient. \n</p> \n<p>We present novel fair regression and dimensionality reduction methods built \non a previously proposed fair classification framework. Both methods rely on \nusing the Hilbert Schmidt independence criterion as the fairness term. Unlike \nprevious approaches, this allows us to simplify the problem and to use multiple \nsensitive variables simultaneously. Replacing the linear formulation by kernel \nfunctions allows the methods to deal with nonlinear problems. For both linear \nand nonlinear formulations the solution reduces to solving simple matrix \ninversions or generalized eigenvalue problems. This simplifies the evaluation \nof the solutions for different trade-off values between the predictive error \nand fairness terms. We illustrate the usefulness of the proposed methods in toy \nexamples, and evaluate their performance on real world datasets to predict \nincome using gender and/or race discrimination as sensitive variables, and \ncontraceptive method prediction under demographic and socio-economic sensitive \ndescriptors. \n</p>"}, "author": "Adri&#xe1;n P&#xe9;rez-Suay, Valero Laparra, Gonzalo Mateo-Garc&#xed;a, Jordi Mu&#xf1;oz-Mar&#xed;, Luis G&#xf3;mez-Chova, Gustau Camps-Valls", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489081", "id": "tag:google.com,2005:reader/item/000000032065c625", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning from Incomplete Ratings using Nonlinear Multi-layer Semi-Nonnegative Matrix Factorization. (arXiv:1710.05613v2 [cs.LG] UPDATED)", "published": 1510229029, "updated": 1510229038, "canonical": [{"href": "http://arxiv.org/abs/1710.05613"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05613", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recommender systems problems witness a growing interest for finding better \nlearning algorithms for personalized information. Matrix factorization that \nestimates the user liking for an item by taking an inner product on the latent \nfeatures of users and item have been widely studied owing to its better \naccuracy and scalability. However, it is possible that the mapping between the \nlatent features learned from these and the original features contains rather \ncomplex nonlinear hierarchical information, that classical linear matrix \nfactorization can not capture. In this paper, we aim to propose a novel \nmultilayer non-linear approach to a variant of nonnegative matrix factorization \n(NMF) to learn such factors from the incomplete ratings matrix. Firstly, we \nconstruct a user-item matrix with explicit ratings, secondly we learn latent \nfactors for representations of users and items from the designed nonlinear \nmulti-layer approach. Further, the architecture is built with different \nnonlinearities using adaptive gradient optimizer to better learn the latent \nfactors in this space. We show that by doing so, our model is able to learn \nlow-dimensional representations that are better suited for recommender systems \non several benchmark datasets. \n</p>"}, "author": "Vaibhav Krishna, Nino Antulov-Fantulin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489080", "id": "tag:google.com,2005:reader/item/000000032065c659", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Large Scale Graph Learning from Smooth Signals. (arXiv:1710.05654v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05654"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05654", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Graphs are a prevalent tool in data science, as they model the inherent \nstructure of the data. They have been used successfully in unsupervised and \nsemi-supervised learning. Typically they are constructed either by connecting \nnearest samples, or by learning them from data, solving an optimization \nproblem. While graph learning does achieve a better quality, it also comes with \na higher computational cost. In particular, the current state-of-the-art model \ncost is $\\mathcal{O}(n^2)$ for $n$ samples. In this paper, we show how to scale \nit, obtaining an approximation with leading cost of $\\mathcal{O}(n\\log(n))$, \nwith quality that approaches the exact graph learning model. Our algorithm uses \nknown approximate nearest neighbor techniques to reduce the number of \nvariables, and automatically selects the correct parameters of the model, \nrequiring a single intuitive input: the desired edge density. \n</p>"}, "author": "Vassilis Kalofolias, Nathana&#xeb;l Perraudin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489079", "id": "tag:google.com,2005:reader/item/000000032065c683", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the Hardness of Inventory Management with Censored Demand Data. (arXiv:1710.05739v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05739"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05739", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider a repeated newsvendor problem where the inventory manager has no \nprior information about the demand, and can access only censored/sales data. In \nanalogy to multi-armed bandit problems, the manager needs to simultaneously \n\"explore\" and \"exploit\" with her inventory decisions, in order to minimize the \ncumulative cost. We make no probabilistic assumptions---importantly, \nindependence or time stationarity---regarding the mechanism that creates the \ndemand sequence. Our goal is to shed light on the hardness of the problem, and \nto develop policies that perform well with respect to the regret criterion, \nthat is, the difference between the cumulative cost of a policy and that of the \nbest fixed action/static inventory decision in hindsight, uniformly over all \nfeasible demand sequences. We show that a simple randomized policy, termed the \nExponentially Weighted Forecaster, combined with a carefully designed cost \nestimator, achieves optimal scaling of the expected regret (up to logarithmic \nfactors) with respect to all three key primitives: the number of time periods, \nthe number of inventory decisions available, and the demand support. Through \nthis result, we derive an important insight: the benefit from \"information \nstalking\" as well as the cost of censoring are both negligible in this dynamic \nlearning problem, at least with respect to the regret criterion. Furthermore, \nwe modify the proposed policy in order to perform well in terms of the tracking \nregret, that is, using as benchmark the best sequence of inventory decisions \nthat switches a limited number of times. Numerical experiments suggest that the \nproposed approach outperforms existing ones (that are tailored to, or \nfacilitated by, time stationarity) on nonstationary demand models. Finally, we \nextend the proposed approach and its analysis to a \"combinatorial\" version of \nthe repeated newsvendor problem. \n</p>"}, "author": "G&#xe1;bor Lugosi, Mihalis G. Markakis, Gergely Neu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489078", "id": "tag:google.com,2005:reader/item/000000032065c6a5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning. (arXiv:1710.05741v2 [stat.ML] UPDATED)", "published": 1509484038, "updated": 1509484044, "canonical": [{"href": "http://arxiv.org/abs/1710.05741"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05741", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper takes a step towards temporal reasoning in a dynamically changing \nvideo, not in the pixel space that constitutes its frames, but in a latent \nspace that describes the non-linear dynamics of the objects in its world. We \nintroduce the Kalman variational auto-encoder, a framework for unsupervised \nlearning of sequential data that disentangles two latent representations: an \nobject's representation, coming from a recognition model, and a latent state \ndescribing its dynamics. As a result, the evolution of the world can be \nimagined and missing data imputed, both without the need to generate high \ndimensional frames at each time step. The model is trained end-to-end on videos \nof a variety of simulated physical systems, and outperforms competing methods \nin generative and missing data imputation tasks. \n</p>"}, "author": "Marco Fraccaro, Simon Kamronn, Ulrich Paquet, Ole Winther", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489077", "id": "tag:google.com,2005:reader/item/000000032065c6b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Time Series Prediction : Predicting Stock Price. (arXiv:1710.05751v2 [stat.ML] UPDATED)", "published": 1508718990, "updated": 1508718993, "canonical": [{"href": "http://arxiv.org/abs/1710.05751"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05751", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Time series forecasting is widely used in a multitude of domains. In this \npaper, we present four models to predict the stock price using the SPX index as \ninput time series data. The martingale and ordinary linear models require the \nstrongest assumption in stationarity which we use as baseline models. The \ngeneralized linear model requires lesser assumptions but is unable to \noutperform the martingale. In empirical testing, the RNN model performs the \nbest comparing to other two models, because it will update the input through \nLSTM instantaneously, but also does not beat the martingale. In addition, we \nintroduce an online to batch algorithm and discrepancy measure to inform \nreaders the newest research in time series predicting method, which doesn't \nrequire any stationarity or non mixing assumptions in time series data. \nFinally, to apply these forecasting to practice, we introduce basic trading \nstrategies that can create Win win and Zero sum situations. \n</p>"}, "author": "Aaron Elliot, Cheng Hua Hsu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489076", "id": "tag:google.com,2005:reader/item/000000032065c6c9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "TensorQuant - A Simulation Toolbox for Deep Neural Network Quantization. (arXiv:1710.05758v1 [cs.CV])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05758"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05758", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent research implies that training and inference of deep neural networks \n(DNN) can be computed with low precision numerical representations of the \ntraining/test data, weights and gradients without a general loss in accuracy. \nThe benefit of such compact representations is twofold: they allow a \nsignificant reduction of the communication bottleneck in distributed DNN \ntraining and faster neural network implementations on hardware accelerators \nlike FPGAs. Several quantization methods have been proposed to map the original \n32-bit floating point problem to low-bit representations. While most related \npublications validate the proposed approach on a single DNN topology, it \nappears to be evident, that the optimal choice of the quantization method and \nnumber of coding bits is topology dependent. To this end, there is no general \ntheory available, which would allow users to derive the optimal quantization \nduring the design of a DNN topology. In this paper, we present a quantization \ntool box for the TensorFlow framework. TensorQuant allows a transparent \nquantization simulation of existing DNN topologies during training and \ninference. TensorQuant supports generic quantization methods and allows \nexperimental evaluation of the impact of the quantization on single layers as \nwell as on the full topology. In a first series of experiments with \nTensorQuant, we show an analysis of fix-point quantizations of popular CNN \ntopologies. \n</p>"}, "author": "Dominik Marek Loroch, Norbert Wehn, Franz-Josef Pfreundt, Janis Keuper", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489075", "id": "tag:google.com,2005:reader/item/000000032065c6e6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Nonsmooth Frank-Wolfe using Uniform Affine Approximations. (arXiv:1710.05776v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05776"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05776", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3d062305\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3d062305&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Frank-Wolfe methods (FW) have gained significant interest in the machine \nlearning community due to its ability to efficiently solve large problems that \nadmit a sparse structure (e.g. sparse vectors and low-rank matrices). However \nthe performance of the existing FW method hinges on the quality of the linear \napproximation. This typically restricts FW to smooth functions for which the \napproximation quality, indicated by a global curvature measure, is reasonably \ngood. \n</p> \n<p>In this paper, we propose a modified FW algorithm amenable to nonsmooth \nfunctions by optimizing for approximation quality over all affine \napproximations given a neighborhood of interest. We analyze theoretical \nproperties of the proposed algorithm and demonstrate that it overcomes many \nissues associated with existing methods in the context of nonsmooth low-rank \nmatrix estimation. \n</p>"}, "author": "Edward Cheung, Yuying Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489074", "id": "tag:google.com,2005:reader/item/000000032065c70c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A successive difference-of-convex approximation method for a class of nonconvex nonsmooth optimization problems. (arXiv:1710.05778v1 [math.OC])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05778"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05778", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider a class of nonconvex nonsmooth optimization problems whose \nobjective is the sum of a nonnegative smooth function and a bunch of \nnonnegative proper closed possibly nonsmooth functions (whose proximal mappings \nare easy to compute), some of which are further composed with linear maps. This \nkind of problems arises naturally in various applications when different \nregularizers are introduced for inducing simultaneous structures in the \nsolutions. Solving these problems, however, can be challenging because of the \ncoupled nonsmooth functions: the corresponding proximal mapping can be hard to \ncompute so that standard first-order methods such as the proximal gradient \nalgorithm cannot be applied efficiently. In this paper, we propose a successive \ndifference-of-convex approximation method for solving this kind of problems. In \nthis algorithm, we approximate the nonsmooth functions by their Moreau \nenvelopes in each iteration. Making use of the simple observation that Moreau \nenvelopes of nonnegative proper closed functions are continuous \ndifference-of-convex functions, we can then approximately minimize the \napproximation function by first-order methods with suitable majorization \ntechniques. These first-order methods can be implemented efficiently thanks to \nthe fact that the proximal mapping of each nonsmooth function is easy to \ncompute. Under suitable assumptions, we prove that the sequence generated by \nour method is bounded and clusters at a stationary point of the objective. We \nalso discuss how our method can be applied to concrete applications such as \nnonconvex fused regularized optimization problems and simultaneously structured \nmatrix optimization problems, and illustrate the performance numerically for \nthese two specific applications. \n</p>"}, "author": "Tianxiang Liu, Ting Kei Pong, Akiko Takeda", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489073", "id": "tag:google.com,2005:reader/item/000000032065c71f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Densely Connected Convolutional Networks and Signal Quality Analysis to Detect Atrial Fibrillation Using Short Single-Lead ECG Recordings. (arXiv:1710.05817v1 [eess.SP])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05817"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05817", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The development of new technology such as wearables that record high-quality \nsingle channel ECG, provides an opportunity for ECG screening in a larger \npopulation, especially for atrial fibrillation screening. The main goal of this \nstudy is to develop an automatic classification algorithm for normal sinus \nrhythm (NSR), atrial fibrillation (AF), other rhythms (O), and noise from a \nsingle channel short ECG segment (9-60 seconds). For this purpose, signal \nquality index (SQI) along with dense convolutional neural networks was used. \nTwo convolutional neural network (CNN) models (main model that accepts 15 \nseconds ECG and secondary model that processes 9 seconds shorter ECG) were \ntrained using the training data set. If the recording is determined to be of \nlow quality by SQI, it is immediately classified as noisy. Otherwise, it is \ntransformed to a time-frequency representation and classified with the CNN as \nNSR, AF, O, or noise. At the final step, a feature-based post-processing \nalgorithm classifies the rhythm as either NSR or O in case the CNN model's \ndiscrimination between the two is indeterminate. The best result achieved at \nthe official phase of the PhysioNet/CinC challenge on the blind test set was \n0.80 (F1 for NSR, AF, and O were 0.90, 0.80, and 0.70, respectively). \n</p>"}, "author": "Jonathan Rubin, Saman Parvaneh, Asif Rahman, Bryan Conroy, Saeed Babaeizadeh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489072", "id": "tag:google.com,2005:reader/item/000000032065c73c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Geometric Learning and Filtering in Finance. (arXiv:1710.05829v1 [q-fin.MF])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05829"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05829", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We develop a method for incorporating relevant non-Euclidean geometric \ninformation into a broad range of classical filtering and statistical or \nmachine learning algorithms. We apply these techniques to approximate the \nsolution of the non-Euclidean filtering problem to arbitrary precision. We then \nextend the particle filtering algorithm to compute our asymptotic solution to \narbitrary precision. Moreover, we find explicit error bounds measuring the \ndiscrepancy between our locally triangulated filter and the true theoretical \nnon-Euclidean filter. Our methods are motivated by certain fundamental problems \nin mathematical finance. In particular we apply these filtering techniques to \nincorporate the non-Euclidean geometry present in stochastic volatility models \nand optimal Markowitz portfolios. We also extend Euclidean statistical or \nmachine learning algorithms to non-Euclidean problems by using the local \ntriangulation technique, which we show improves the accuracy of the original \nalgorithm. We apply the local triangulation method to obtain improvements of \nthe (sparse) principal component analysis and the principal geodesic analysis \nalgorithms and show how these improved algorithms can be used to parsimoniously \nestimate the evolution of the shape of forward-rate curves. While focused on \nfinancial applications, the non-Euclidean geometric techniques presented in \nthis paper can be employed to provide improvements to a range of other \nstatistical or machine learning algorithms and may be useful in other areas of \napplication. \n</p>"}, "author": "Anastasia Kratsios, Cody B. Hyndman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489071", "id": "tag:google.com,2005:reader/item/000000032065c75e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Spectral Algorithms for Computing Fair Support Vector Machines. (arXiv:1710.05895v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05895"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05895", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Classifiers and rating scores are prone to implicitly codifying biases, which \nmay be present in the training data, against protected classes (i.e., age, \ngender, or race). So it is important to understand how to design classifiers \nand scores that prevent discrimination in predictions. This paper develops \ncomputationally tractable algorithms for designing accurate but fair support \nvector machines (SVM's). Our approach imposes a constraint on the covariance \nmatrices conditioned on each protected class, which leads to a nonconvex \nquadratic constraint in the SVM formulation. We develop iterative algorithms to \ncompute fair linear and kernel SVM's, which solve a sequence of relaxations \nconstructed using a spectral decomposition of the nonconvex constraint. Its \neffectiveness in achieving high prediction accuracy while ensuring fairness is \nshown through numerical experiments on several data sets. \n</p>"}, "author": "Matt Olfat, Anil Aswani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489044", "id": "tag:google.com,2005:reader/item/000000032065cb40", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Stochastic Replica Approach to Machine Learning: Stability and Parameter Optimization. (arXiv:1708.05715v2 [stat.ML] UPDATED)", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.05715"}], "alternate": [{"href": "http://arxiv.org/abs/1708.05715", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce a statistical physics inspired supervised machine learning \nalgorithm for classification and regression problems. The method is based on \nthe invariances or stability of predicted results when known data is \nrepresented as expansions in terms of various stochastic functions. The \nalgorithm predicts the classification/regression values of new data by \ncombining (via voting) the outputs of these numerous linear expansions in \nrandomly chosen functions. The few parameters (typically only one parameter is \nused in all studied examples) that this model has may be automatically \noptimized. The algorithm has been tested on 10 diverse training data sets of \nvarious types and feature space dimensions. It has been shown to consistently \nexhibit high accuracy and readily allow for optimization of parameters, while \nsimultaneously avoiding pitfalls of existing algorithms such as those \nassociated with class imbalance. We very briefly speculate on whether spatial \ncoordinates in physical theories may be viewed as emergent \"features\" that \nenable a robust machine learning type description of data with generic low \norder smooth functions. \n</p>"}, "author": "Patrick Chao, Tahereh Mazaheri, Bo Sun, Nicholas B. Weingartner, Zohar Nussinov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508115936014", "timestampUsec": "1508115936014402", "id": "tag:google.com,2005:reader/item/000000031f99e80f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "STDP Based Pruning of Connections and Weight Quantization in Spiking Neural Networks for Energy Efficient Recognition. (arXiv:1710.04734v1 [cs.NE])", "published": 1508115936, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04734"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04734", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Spiking Neural Networks (SNNs) with a large number of weights and varied \nweight distribution can be difficult to implement in emerging in-memory \ncomputing hardware due to the limitations on crossbar size (implementing dot \nproduct), the constrained number of conductance levels in non-CMOS devices and \nthe power budget. We present a sparse SNN topology where non-critical \nconnections are pruned to reduce the network size and the remaining critical \nsynapses are weight quantized to accommodate for limited conductance levels. \nPruning is based on the power law weight-dependent Spike Timing Dependent \nPlasticity (STDP) model; synapses between pre- and post-neuron with high spike \ncorrelation are retained, whereas synapses with low correlation or uncorrelated \nspiking activity are pruned. The weights of the retained connections are \nquantized to the available number of conductance levels. The process of pruning \nnon-critical connections and quantizing the weights of critical synapses is \nperformed at regular intervals during training. We evaluated our sparse and \nquantized network on MNIST dataset and on a subset of images from Caltech-101 \ndataset. The compressed topology achieved a classification accuracy of 90.1% \n(91.6%) on the MNIST (Caltech-101) dataset with 3.1x (2.2x) and 4x (2.6x) \nimprovement in energy and area, respectively. The compressed topology is energy \nand area efficient while maintaining the same classification accuracy of a \n2-layer fully connected SNN topology. \n</p>"}, "author": "Nitin Rathi, Priyadarshini Panda, Kaushik Roy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508115936014", "timestampUsec": "1508115936014401", "id": "tag:google.com,2005:reader/item/000000031f99e814", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "HyperENTM: Evolving Scalable Neural Turing Machines through HyperNEAT. (arXiv:1710.04748v1 [cs.AI])", "published": 1508115936, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04748"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04748", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent developments within memory-augmented neural networks have solved \nsequential problems requiring long-term memory, which are intractable for \ntraditional neural networks. However, current approaches still struggle to \nscale to large memory sizes and sequence lengths. In this paper we show how \naccess to memory can be encoded geometrically through a HyperNEAT-based Neural \nTuring Machine (HyperENTM). We demonstrate that using the indirect HyperNEAT \nencoding allows for training on small memory vectors in a bit-vector copy task \nand then applying the knowledge gained from such training to speed up training \non larger size memory vectors. Additionally, we demonstrate that in some \ninstances, networks trained to copy bit-vectors of size 9 can be scaled to \nsizes of 1,000 without further training. While the task in this paper is \nsimple, these results could open up the problems amendable to networks with \nexternal memories to problems with larger memory vectors and theoretically \nunbounded memory sizes. \n</p>"}, "author": "Jakob Merrild, Mikkel Angaju Rasmussen, Sebastian Risi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508115936014", "timestampUsec": "1508115936014400", "id": "tag:google.com,2005:reader/item/000000031f99e816", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Efficient Computation in Adaptive Artificial Spiking Neural Networks. (arXiv:1710.04838v1 [cs.NE])", "published": 1508115936, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04838"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04838", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Artificial Neural Networks (ANNs) are bio-inspired models of neural \ncomputation that have proven highly effective. Still, ANNs lack a natural \nnotion of time, and neural units in ANNs exchange analog values in a \nframe-based manner, a computationally and energetically inefficient form of \ncommunication. This contrasts sharply with biological neurons that communicate \nsparingly and efficiently using binary spikes. While artificial Spiking Neural \nNetworks (SNNs) can be constructed by replacing the units of an ANN with \nspiking neurons, the current performance is far from that of deep ANNs on hard \nbenchmarks and these SNNs use much higher firing rates compared to their \nbiological counterparts, limiting their efficiency. Here we show how spiking \nneurons that employ an efficient form of neural coding can be used to construct \nSNNs that match high-performance ANNs and exceed state-of-the-art in SNNs on \nimportant benchmarks, while requiring much lower average firing rates. For \nthis, we use spike-time coding based on the firing rate limiting adaptation \nphenomenon observed in biological spiking neurons. This phenomenon can be \ncaptured in adapting spiking neuron models, for which we derive the effective \ntransfer function. Neural units in ANNs trained with this transfer function can \nbe substituted directly with adaptive spiking neurons, and the resulting \nAdaptive SNNs (AdSNNs) can carry out inference in deep neural networks using up \nto an order of magnitude fewer spikes compared to previous SNNs. Adaptive \nspike-time coding additionally allows for the dynamic control of neural coding \nprecision: we show how a simple model of arousal in AdSNNs further halves the \naverage required firing rate and this notion naturally extends to other forms \nof attention. AdSNNs thus hold promise as a novel and efficient model for \nneural computation that naturally fits to temporally continuous and \nasynchronous applications. \n</p>"}, "author": "Davide Zambrano, Roeland Nusselder, H. Steven Scholte, Sander Bohte", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508115936014", "timestampUsec": "1508115936014399", "id": "tag:google.com,2005:reader/item/000000031f99e81a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Method of Generating Random Weights and Biases in Feedforward Neural Networks with Random Hidden Nodes. (arXiv:1710.04874v1 [cs.NE])", "published": 1508115936, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04874"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04874", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks with random hidden nodes have gained increasing interest from \nresearchers and practical applications. This is due to their unique features \nsuch as very fast training and universal approximation property. In these \nnetworks the weights and biases of hidden nodes determining the nonlinear \nfeature mapping are set randomly and are not learned. Appropriate selection of \nthe intervals from which weights and biases are selected is extremely \nimportant. This topic has not yet been sufficiently explored in the literature. \nIn this work a method of generating random weights and biases is proposed. This \nmethod generates the parameters of the hidden nodes in such a way that \nnonlinear fragments of the activation functions are located in the input space \nregions with data and can be used to construct the surface approximating a \nnonlinear target function. The weights and biases are dependent on the input \ndata range and activation function type. The proposed methods allows us to \ncontrol the generalization degree of the model. These all lead to improvement \nin approximation performance of the network. Several experiments show very \npromising results. \n</p>"}, "author": "Grzegorz Dudek", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361619", "id": "tag:google.com,2005:reader/item/000000031f973003", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Game-Theoretic Design of Secure and Resilient Distributed Support Vector Machines with Adversaries. (arXiv:1710.04677v1 [stat.ML])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04677"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04677", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3d062576\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3d062576&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>With a large number of sensors and control units in networked systems, \ndistributed support vector machines (DSVMs) play a fundamental role in scalable \nand efficient multi-sensor classification and prediction tasks. However, DSVMs \nare vulnerable to adversaries who can modify and generate data to deceive the \nsystem to misclassification and misprediction. This work aims to design defense \nstrategies for DSVM learner against a potential adversary. We establish a \ngame-theoretic framework to capture the conflicting interests between the DSVM \nlearner and the attacker. The Nash equilibrium of the game allows predicting \nthe outcome of learning algorithms in adversarial environments, and enhancing \nthe resilience of the machine learning through dynamic distributed learning \nalgorithms. We show that the DSVM learner is less vulnerable when he uses a \nbalanced network with fewer nodes and higher degree. We also show that adding \nmore training samples is an efficient defense strategy against an attacker. We \npresent secure and resilient DSVM algorithms with verification method and \nrejection method, and show their resiliency against adversary with numerical \nexperiments. \n</p>"}, "author": "Rui Zhang, Quanyan Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361618", "id": "tag:google.com,2005:reader/item/000000031f973012", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hyperparameter Importance Across Datasets. (arXiv:1710.04725v1 [stat.ML])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04725"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04725", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3d0dfcea\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3d0dfcea&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>With the advent of automated machine learning, automated hyperparameter \noptimization methods are by now routinely used. However, this progress is not \nyet matched by equal progress on automatic analyses that yield information \nbeyond performance-optimizing hyperparameter settings. In this work, we aim to \nanswer the following two questions: Given an algorithm, what are generally its \nmost important hyperparameters, and what are good priors over their \nhyperparameters' ranges to draw values from? We present methodology and a \nframework to answer these questions based on meta-learning across many \ndatasets. We apply this methodology using the experimental meta-data available \non OpenML to determine the most important hyperparameters of support vector \nmachines, random forests and Adaboost, and to infer priors for all their \nhyperparameters. Our results, obtained fully automatically, provide a \nquantitative basis to focus efforts in both manual algorithm design and in \nautomated hyperparameter optimization. Our experiments confirm that the \nselected hyperparameters are indeed the most important ones and that our \nobtained priors also lead to improvements in hyperparameter optimization. \n</p>"}, "author": "J. N. van Rijn, F. Hutter", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361617", "id": "tag:google.com,2005:reader/item/000000031f97301b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the Runtime-Efficacy Trade-off of Anomaly Detection Techniques for Real-Time Streaming Data. (arXiv:1710.04735v1 [stat.ML])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04735"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04735", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Ever growing volume and velocity of data coupled with decreasing attention \nspan of end users underscore the critical need for real-time analytics. In this \nregard, anomaly detection plays a key role as an application as well as a means \nto verify data fidelity. Although the subject of anomaly detection has been \nresearched for over 100 years in a multitude of disciplines such as, but not \nlimited to, astronomy, statistics, manufacturing, econometrics, marketing, most \nof the existing techniques cannot be used as is on real-time data streams. \nFurther, the lack of characterization of performance -- both with respect to \nreal-timeliness and accuracy -- on production data sets makes model selection \nvery challenging. To this end, we present an in-depth analysis, geared towards \nreal-time streaming data, of anomaly detection techniques. Given the \nrequirements with respect to real-timeliness and accuracy, the analysis \npresented in this paper should serve as a guide for selection of the \"best\" \nanomaly detection technique. To the best of our knowledge, this is the first \ncharacterization of anomaly detection techniques proposed in very diverse set \nof fields, using production data sets corresponding to a wide set of \napplication domains. \n</p>"}, "author": "Dhruv Choudhary, Arun Kejariwal, Francois Orsini", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361616", "id": "tag:google.com,2005:reader/item/000000031f973021", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Explaining Aviation Safety Incidents Using Deep Learned Precursors. (arXiv:1710.04749v1 [cs.CV])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04749"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04749", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Although aviation accidents are rare, safety incidents occur more frequently \nand require careful analysis for providing actionable recommendations to \nimprove safety. Automatically analyzing safety incidents using flight data is \nchallenging because of the absence of labels on timestep-wise events in a \nflight, complexity of multi-dimensional data, and lack of scalable tools to \nperform analysis over large number of events. In this work, we propose a \nprecursor mining algorithm that identifies correlated patterns in \nmultidimensional time series to explain an adverse event. Precursors are \nvaluable to systems health and safety monitoring in explaining and forecasting \nanomalies. Current precursor mining methods suffer from poor scalability to \nhigh dimensional time series data and in capturing long-term memory. We propose \nan approach by combining multiple-instance learning (MIL) and deep recurrent \nneural networks (DRNN) to take advantage of MIL's ability to model \nweakly-supervised data and DRNN's ability to model long term memory processes, \nto scale well to high dimensional data and to large volumes of data using GPU \nparallelism. We apply the proposed method to find precursors and offer \nexplanations to high speed exceedance safety incidents using commercial flight \ndata. \n</p>"}, "author": "Vijay Manikandan Janakiraman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361615", "id": "tag:google.com,2005:reader/item/000000031f973029", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian Hypernetworks. (arXiv:1710.04759v1 [stat.ML])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04759"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04759", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose Bayesian hypernetworks: a framework for approximate Bayesian \ninference in neural networks. A Bayesian hypernetwork, $h$, is a neural network \nwhich learns to transform a simple noise distribution, $p(\\epsilon) = \n\\mathcal{N}(0,I)$, to a distribution $q(\\theta) \\doteq q(h(\\epsilon))$ over the \nparameters $\\theta$ of another neural network (the \"primary network\"). We train \n$q$ with variational inference, using an invertible $h$ to enable efficient \nestimation of the variational lower bound on the posterior $p(\\theta | \n\\mathcal{D})$ via sampling. In contrast to most methods for Bayesian deep \nlearning, Bayesian hypernets can represent a complex multimodal approximate \nposterior with correlations between parameters, while enabling cheap i.i.d. \nsampling of $q(\\theta)$. We demonstrate these qualitative advantages of \nBayesian hypernets, which also achieve competitive performance on a suite of \ntasks that demonstrate the advantage of estimating model uncertainty, including \nactive learning and anomaly detection. \n</p>"}, "author": "David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, Aaron Courville", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361614", "id": "tag:google.com,2005:reader/item/000000031f973036", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sparse Weighted Canonical Correlation Analysis. (arXiv:1710.04792v1 [cs.LG])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04792"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04792", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Given two data matrices $X$ and $Y$, sparse canonical correlation analysis \n(SCCA) is to seek two sparse canonical vectors $u$ and $v$ to maximize the \ncorrelation between $Xu$ and $Yv$. However, classical and sparse CCA models \nconsider the contribution of all the samples of data matrices and thus cannot \nidentify an underlying specific subset of samples. To this end, we propose a \nnovel sparse weighted canonical correlation analysis (SWCCA), where weights are \nused for regularizing different samples. We solve the $L_0$-regularized SWCCA \n($L_0$-SWCCA) using an alternating iterative algorithm. We apply $L_0$-SWCCA to \nsynthetic data and real-world data to demonstrate its effectiveness and \nsuperiority compared to related methods. Lastly, we consider also SWCCA with \ndifferent penalties like LASSO (Least absolute shrinkage and selection \noperator) and Group LASSO, and extend it for integrating more than three data \nmatrices. \n</p>"}, "author": "Wenwen Min, Juan Liu, Shihua Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361613", "id": "tag:google.com,2005:reader/item/000000031f973042", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning for Case-based Reasoning through Prototypes: A Neural Network that Explains its Predictions. (arXiv:1710.04806v1 [cs.AI])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04806"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04806", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks are widely used for classification. These deep models \noften suffer from a lack of interpretability -- they are particularly difficult \nto understand because of their non-linear nature. As a result, neural networks \nare often treated as \"black box\" models, and in the past, have been trained \npurely to optimize the accuracy of predictions. In this work, we create a novel \nnetwork architecture for deep learning that naturally explains its own \nreasoning for each prediction. This architecture contains an autoencoder and a \nspecial prototype layer, where each unit of that layer stores a weight vector \nthat resembles an encoded training input. The encoder of the autoencoder allows \nus to do comparisons within the latent space, while the decoder allows us to \nvisualize the learned prototypes. The training objective has four terms: an \naccuracy term, a term that encourages every prototype to be similar to at least \none encoded input, a term that encourages every encoded input to be close to at \nleast one prototype, and a term that encourages faithful reconstruction by the \nautoencoder. The distances computed in the prototype layer are used as part of \nthe classification process. Since the prototypes are learned during training, \nthe learned network naturally comes with explanations for each prediction, and \nthe explanations are loyal to what the network actually computes. \n</p>"}, "author": "Oscar Li, Hao Liu, Chaofan Chen, Cynthia Rudin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361612", "id": "tag:google.com,2005:reader/item/000000031f97304a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Machine Learning by Two-Dimensional Hierarchical Tensor Networks: A Quantum Information Theoretic Perspective on Deep Architectures. (arXiv:1710.04833v1 [stat.ML])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04833"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04833", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The resemblance between the methods used in studying quantum-many body \nphysics and in machine learning has drawn considerable attention. In \nparticular, tensor networks (TNs) and deep learning architectures bear striking \nsimilarities to the extent that TNs can be used for machine learning. Previous \nresults used one-dimensional TNs in image recognition, showing limited \nscalability and a high bond dimension. In this work, we train two-dimensional \nhierarchical TNs to solve image recognition problems, using a training \nalgorithm derived from the multipartite entanglement renormalization ansatz \n(MERA). This approach overcomes scalability issues and implies novel \nmathematical connections among quantum many-body physics, quantum information \ntheory, and machine learning. While keeping the TN unitary in the training \nphase, TN states can be defined, which optimally encodes each class of the \nimages into a quantum many-body state. We study the quantum features of the TN \nstates, including quantum entanglement and fidelity. We suggest these \nquantities could be novel properties that characterize the image classes, as \nwell as the machine learning tasks. Our work could be further applied to \nidentifying possible quantum properties of certain artificial intelligence \nmethods. \n</p>"}, "author": "Ding Liu, Shi-Ju Ran, Peter Wittek, Cheng Peng, Raul Bl&#xe1;zquez Garc&#xed;a, Gang Su, Maciej Lewenstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361611", "id": "tag:google.com,2005:reader/item/000000031f973057", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Recent Advances in Zero-shot Recognition. (arXiv:1710.04837v1 [cs.CV])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04837"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04837", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With the recent renaissance of deep convolution neural networks, encouraging \nbreakthroughs have been achieved on the supervised recognition tasks, where \neach class has sufficient training data and fully annotated training data. \nHowever, to scale the recognition to a large number of classes with few or now \ntraining samples for each class remains an unsolved problem. One approach to \nscaling up the recognition is to develop models capable of recognizing unseen \ncategories without any training instances, or zero-shot recognition/ learning. \nThis article provides a comprehensive review of existing zero-shot recognition \ntechniques covering various aspects ranging from representations of models, and \nfrom datasets and evaluation settings. We also overview related recognition \ntasks including one-shot and open set recognition which can be used as natural \nextensions of zero-shot recognition when limited number of class samples become \navailable or when zero-shot recognition is implemented in a real-world setting. \nImportantly, we highlight the limitations of existing approaches and point out \nfuture research directions in this existing new research area. \n</p>"}, "author": "Yanwei Fu, Tao Xiang, Yu-Gang Jiang, Xiangyang Xue, Leonid Sigal, Shaogang Gong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361610", "id": "tag:google.com,2005:reader/item/000000031f973060", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Manifold regularization based on Nystr{\\&quot;o}m type subsampling. (arXiv:1710.04872v1 [stat.ML])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04872"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04872", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we study the Nystr{\\\"o}m type subsampling for large scale \nkernel methods to reduce the computational complexities of big data. We discuss \nthe multi-penalty regularization scheme based on Nystr{\\\"o}m type subsampling \nwhich is motivated from well-studied manifold regularization schemes. We \ndevelop a theoretical analysis of multi-penalty least-square regularization \nscheme under the general source condition in vector-valued function setting, \ntherefore the results can also be applied to multi-task learning problems. We \nachieve the optimal minimax convergence rates of multi-penalty regularization \nusing the concept of effective dimension for the appropriate subsampling size. \nWe discuss an aggregation approach based on linear function strategy to combine \nvarious Nystr{\\\"o}m approximants. Finally, we demonstrate the performance of \nmulti-penalty regularization based on Nystr{\\\"o}m type subsampling on \nCaltech-101 data set for multi-class image classification and NSL-KDD benchmark \ndata set for intrusion detection problem. \n</p>"}, "author": "Abhishake Rastogi, Sivananthan Sampath", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361609", "id": "tag:google.com,2005:reader/item/000000031f973064", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Method of Generating Random Weights and Biases in Feedforward Neural Networks with Random Hidden Nodes. (arXiv:1710.04874v1 [cs.NE])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04874"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04874", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3d0e0171\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3d0e0171&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Neural networks with random hidden nodes have gained increasing interest from \nresearchers and practical applications. This is due to their unique features \nsuch as very fast training and universal approximation property. In these \nnetworks the weights and biases of hidden nodes determining the nonlinear \nfeature mapping are set randomly and are not learned. Appropriate selection of \nthe intervals from which weights and biases are selected is extremely \nimportant. This topic has not yet been sufficiently explored in the literature. \nIn this work a method of generating random weights and biases is proposed. This \nmethod generates the parameters of the hidden nodes in such a way that \nnonlinear fragments of the activation functions are located in the input space \nregions with data and can be used to construct the surface approximating a \nnonlinear target function. The weights and biases are dependent on the input \ndata range and activation function type. The proposed methods allows us to \ncontrol the generalization degree of the model. These all lead to improvement \nin approximation performance of the network. Several experiments show very \npromising results. \n</p>"}, "author": "Grzegorz Dudek", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361608", "id": "tag:google.com,2005:reader/item/000000031f973067", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Graph Convolutional Networks for Classification with a Structured Label Space. (arXiv:1710.04908v1 [cs.LG])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04908"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04908", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is a usual practice to ignore any structural information underlying \nclasses in multi-class classification. In this paper, we propose a graph \nconvolutional network (GCN) augmented neural network classifier to exploit a \nknown, underlying graph structure of labels. The proposed approach resembles an \n(approximate) inference procedure in, for instance, a conditional random field \n(CRF), however without losing any modelling flexibility. The proposed method \ncan easily scale up to thousands of labels. We evaluate the proposed approach \non the problems of document classification and object recognition and report \nboth accuracies and graph-theoretic metrics that correspond to the consistency \nof the model's prediction. The experiment results reveal that the proposed \nmodel outperforms a baseline method which ignores the graph structures of a \nlabel space. \n</p>"}, "author": "Meihao Chen, Zhuoru Lin, Kyunghyun Cho", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361607", "id": "tag:google.com,2005:reader/item/000000031f973075", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Two-stage Algorithm for Fairness-aware Machine Learning. (arXiv:1710.04924v1 [stat.ML])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04924"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04924", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Algorithmic decision making process now affects many aspects of our lives. \nStandard tools for machine learning, such as classification and regression, are \nsubject to the bias in data, and thus direct application of such off-the-shelf \ntools could lead to a specific group being unfairly discriminated. Removing \nsensitive attributes of data does not solve this problem because a \n\\textit{disparate impact} can arise when non-sensitive attributes and sensitive \nattributes are correlated. Here, we study a fair machine learning algorithm \nthat avoids such a disparate impact when making a decision. Inspired by the \ntwo-stage least squares method that is widely used in the field of economics, \nwe propose a two-stage algorithm that removes bias in the training data. The \nproposed algorithm is conceptually simple. Unlike most of existing fair \nalgorithms that are designed for classification tasks, the proposed method is \nable to (i) deal with regression tasks, (ii) combine explanatory attributes to \nremove reverse discrimination, and (iii) deal with numerical sensitive \nattributes. The performance and fairness of the proposed algorithm are \nevaluated in simulations with synthetic and real-world datasets. \n</p>"}, "author": "Junpei Komiyama, Hajime Shimao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361606", "id": "tag:google.com,2005:reader/item/000000031f97307a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "RADNET: Radiologist Level Accuracy using Deep Learning for HEMORRHAGE detection in CT Scans. (arXiv:1710.04934v1 [cs.CV])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04934"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04934", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We describe a deep learning approach for automated brain hemorrhage detection \nfrom computed tomography (CT) scans. Our model emulates the procedure followed \nby radiologists to analyse a 3D CT scan in real-world. Similar to radiologists, \nthe model sifts through 2D cross-sectional slices while paying close attention \nto potential hemorrhagic regions. Further, the model utilizes 3D context from \nneighboring slices to improve predictions at each slice and subsequently, \naggregates the slice-level predictions to provide diagnosis at CT level. We \nrefer to our proposed approach as Recurrent Attention DenseNet (RADnet) as it \nemploys original DenseNet architecture along with adding the components of \nattention for slice level predictions and recurrent neural network layer for \nincorporating 3D context. The real-world performance of RADnet has been \nbenchmarked against independent analysis performed by three senior radiologists \nfor 77 brain CTs. RADnet demonstrates 81.82% hemorrhage prediction accuracy at \nCT level that is comparable to radiologists. Further, RADnet achieves higher \nrecall than two of the three radiologists, which is remarkable. \n</p>"}, "author": "Monika Grewal, Muktabh Mayank Srivastava, Pulkit Kumar, Srikrishna Varadarajan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361605", "id": "tag:google.com,2005:reader/item/000000031f97307d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Potential Conditional Mutual Information: Estimators, Properties and Applications. (arXiv:1710.05012v1 [cs.IT])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05012"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05012", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The conditional mutual information I(X;Y|Z) measures the average information \nthat X and Y contain about each other given Z. This is an important primitive \nin many learning problems including conditional independence testing, graphical \nmodel inference, causal strength estimation and time-series problems. In \nseveral applications, it is desirable to have a functional purely of the \nconditional distribution p_{Y|X,Z} rather than of the joint distribution \np_{X,Y,Z}. We define the potential conditional mutual information as the \nconditional mutual information calculated with a modified joint distribution \np_{Y|X,Z} q_{X,Z}, where q_{X,Z} is a potential distribution, fixed airport. We \ndevelop K nearest neighbor based estimators for this functional, employing \nimportance sampling, and a coupling trick, and prove the finite k consistency \nof such an estimator. We demonstrate that the estimator has excellent practical \nperformance and show an application in dynamical system inference. \n</p>"}, "author": "Arman Rahimzamani, Sreeram Kannan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708618", "id": "tag:google.com,2005:reader/item/000000031f9729ca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Identifying On-time Reward Delivery Projects with Estimating Delivery Duration on Kickstarter. (arXiv:1710.04743v1 [cs.CY])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04743"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04743", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In Crowdfunding platforms, people turn their prototype ideas into real \nproducts by raising money from the crowd, or invest in someone else's projects. \nIn reward-based crowdfunding platforms such as Kickstarter and Indiegogo, \nselecting accurate reward delivery duration becomes crucial for creators, \nbackers, and platform providers to keep the trust between the creators and the \nbackers, and the trust between the platform providers and users. According to \nKickstarter, 35% backers did not receive rewards on time. Unfortunately, little \nis known about on-time and late reward delivery projects, and there is no prior \nwork to estimate reward delivery duration. To fill the gap, in this paper, we \n(i) extract novel features that reveal latent difficulty levels of project \nrewards; (ii) build predictive models to identify whether a creator will \ndeliver all rewards in a project on time or not; and (iii) build a regression \nmodel to estimate accurate reward delivery duration (i.e., how long it will \ntake to produce and deliver all the rewards). Experimental results show that \nour models achieve good performance -- 82.5% accuracy, 78.1 RMSE, and 0.108 \nNRMSE at the first 5% of the longest reward delivery duration. \n</p>"}, "author": "Thanh Tran, Kyumin Lee, Nguyen Vo, Hongkyu Choi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708617", "id": "tag:google.com,2005:reader/item/000000031f9729d1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "HyperENTM: Evolving Scalable Neural Turing Machines through HyperNEAT. (arXiv:1710.04748v1 [cs.AI])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04748"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04748", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent developments within memory-augmented neural networks have solved \nsequential problems requiring long-term memory, which are intractable for \ntraditional neural networks. However, current approaches still struggle to \nscale to large memory sizes and sequence lengths. In this paper we show how \naccess to memory can be encoded geometrically through a HyperNEAT-based Neural \nTuring Machine (HyperENTM). We demonstrate that using the indirect HyperNEAT \nencoding allows for training on small memory vectors in a bit-vector copy task \nand then applying the knowledge gained from such training to speed up training \non larger size memory vectors. Additionally, we demonstrate that in some \ninstances, networks trained to copy bit-vectors of size 9 can be scaled to \nsizes of 1,000 without further training. While the task in this paper is \nsimple, these results could open up the problems amendable to networks with \nexternal memories to problems with larger memory vectors and theoretically \nunbounded memory sizes. \n</p>"}, "author": "Jakob Merrild, Mikkel Angaju Rasmussen, Sebastian Risi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708616", "id": "tag:google.com,2005:reader/item/000000031f9729d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Explaining Aviation Safety Incidents Using Deep Learned Precursors. (arXiv:1710.04749v1 [cs.CV])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04749"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04749", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Although aviation accidents are rare, safety incidents occur more frequently \nand require careful analysis for providing actionable recommendations to \nimprove safety. Automatically analyzing safety incidents using flight data is \nchallenging because of the absence of labels on timestep-wise events in a \nflight, complexity of multi-dimensional data, and lack of scalable tools to \nperform analysis over large number of events. In this work, we propose a \nprecursor mining algorithm that identifies correlated patterns in \nmultidimensional time series to explain an adverse event. Precursors are \nvaluable to systems health and safety monitoring in explaining and forecasting \nanomalies. Current precursor mining methods suffer from poor scalability to \nhigh dimensional time series data and in capturing long-term memory. We propose \nan approach by combining multiple-instance learning (MIL) and deep recurrent \nneural networks (DRNN) to take advantage of MIL's ability to model \nweakly-supervised data and DRNN's ability to model long term memory processes, \nto scale well to high dimensional data and to large volumes of data using GPU \nparallelism. We apply the proposed method to find precursors and offer \nexplanations to high speed exceedance safety incidents using commercial flight \ndata. \n</p>"}, "author": "Vijay Manikandan Janakiraman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708615", "id": "tag:google.com,2005:reader/item/000000031f9729de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian Hypernetworks. (arXiv:1710.04759v1 [stat.ML])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04759"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04759", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose Bayesian hypernetworks: a framework for approximate Bayesian \ninference in neural networks. A Bayesian hypernetwork, $h$, is a neural network \nwhich learns to transform a simple noise distribution, $p(\\epsilon) = \n\\mathcal{N}(0,I)$, to a distribution $q(\\theta) \\doteq q(h(\\epsilon))$ over the \nparameters $\\theta$ of another neural network (the \"primary network\"). We train \n$q$ with variational inference, using an invertible $h$ to enable efficient \nestimation of the variational lower bound on the posterior $p(\\theta | \n\\mathcal{D})$ via sampling. In contrast to most methods for Bayesian deep \nlearning, Bayesian hypernets can represent a complex multimodal approximate \nposterior with correlations between parameters, while enabling cheap i.i.d. \nsampling of $q(\\theta)$. We demonstrate these qualitative advantages of \nBayesian hypernets, which also achieve competitive performance on a suite of \ntasks that demonstrate the advantage of estimating model uncertainty, including \nactive learning and anomaly detection. \n</p>"}, "author": "David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, Aaron Courville", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708614", "id": "tag:google.com,2005:reader/item/000000031f9729eb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Combinatorial Multi-armed Bandits for Real-Time Strategy Games. (arXiv:1710.04805v1 [cs.AI])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04805"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04805", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Games with large branching factors pose a significant challenge for game tree \nsearch algorithms. In this paper, we address this problem with a sampling \nstrategy for Monte Carlo Tree Search (MCTS) algorithms called {\\em na\\\"{i}ve \nsampling}, based on a variant of the Multi-armed Bandit problem called {\\em \nCombinatorial Multi-armed Bandits} (CMAB). We analyze the theoretical \nproperties of several variants of {\\em na\\\"{i}ve sampling}, and empirically \ncompare it against the other existing strategies in the literature for CMABs. \nWe then evaluate these strategies in the context of real-time strategy (RTS) \ngames, a genre of computer games characterized by their very large branching \nfactors. Our results show that as the branching factor grows, {\\em na\\\"{i}ve \nsampling} outperforms the other sampling strategies. \n</p>"}, "author": "Santiago Onta&#xf1;&#xf3;n", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708613", "id": "tag:google.com,2005:reader/item/000000031f9729f2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning for Case-based Reasoning through Prototypes: A Neural Network that Explains its Predictions. (arXiv:1710.04806v1 [cs.AI])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04806"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04806", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3d0e0518\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3d0e0518&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep neural networks are widely used for classification. These deep models \noften suffer from a lack of interpretability -- they are particularly difficult \nto understand because of their non-linear nature. As a result, neural networks \nare often treated as \"black box\" models, and in the past, have been trained \npurely to optimize the accuracy of predictions. In this work, we create a novel \nnetwork architecture for deep learning that naturally explains its own \nreasoning for each prediction. This architecture contains an autoencoder and a \nspecial prototype layer, where each unit of that layer stores a weight vector \nthat resembles an encoded training input. The encoder of the autoencoder allows \nus to do comparisons within the latent space, while the decoder allows us to \nvisualize the learned prototypes. The training objective has four terms: an \naccuracy term, a term that encourages every prototype to be similar to at least \none encoded input, a term that encourages every encoded input to be close to at \nleast one prototype, and a term that encourages faithful reconstruction by the \nautoencoder. The distances computed in the prototype layer are used as part of \nthe classification process. Since the prototypes are learned during training, \nthe learned network naturally comes with explanations for each prediction, and \nthe explanations are loyal to what the network actually computes. \n</p>"}, "author": "Oscar Li, Hao Liu, Chaofan Chen, Cynthia Rudin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708612", "id": "tag:google.com,2005:reader/item/000000031f9729fc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast Top-$\\boldsymbol{k}$ Area Topics Extraction with Knowledge Base. (arXiv:1710.04822v1 [cs.AI])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04822"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04822", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3d15569c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3d15569c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>What are the most popular research topics in Artificial Intelligence (AI)? We \nformulate the problem as extracting top-$k$ topics that can best represent a \ngiven area with the help of knowledge base. We theoretically prove that the \nproblem is NP-hard and propose an optimization model, FastKATE, to address this \nproblem by combining both explicit and latent representations for each topic. \nWe leverage a large-scale knowledge base (Wikipedia) to generate topic \nembeddings using neural networks and use this kind of representations to help \ncapture the representativeness of topics for given areas. We develop a fast \nheuristic algorithm to efficiently solve the problem with a provable error \nbound. We evaluate the proposed model on three real-world datasets. \nExperimental results demonstrate our model's effectiveness, robustness, \nreal-timeness (return results in $&lt;1$s), and its superiority over several \nalternative methods. \n</p>"}, "author": "Fang Zhang, Xiaochen Wang, Jingfei Han, Jie Tang, Shiyin Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708611", "id": "tag:google.com,2005:reader/item/000000031f972a03", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Recent Advances in Zero-shot Recognition. (arXiv:1710.04837v1 [cs.CV])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04837"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04837", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With the recent renaissance of deep convolution neural networks, encouraging \nbreakthroughs have been achieved on the supervised recognition tasks, where \neach class has sufficient training data and fully annotated training data. \nHowever, to scale the recognition to a large number of classes with few or now \ntraining samples for each class remains an unsolved problem. One approach to \nscaling up the recognition is to develop models capable of recognizing unseen \ncategories without any training instances, or zero-shot recognition/ learning. \nThis article provides a comprehensive review of existing zero-shot recognition \ntechniques covering various aspects ranging from representations of models, and \nfrom datasets and evaluation settings. We also overview related recognition \ntasks including one-shot and open set recognition which can be used as natural \nextensions of zero-shot recognition when limited number of class samples become \navailable or when zero-shot recognition is implemented in a real-world setting. \nImportantly, we highlight the limitations of existing approaches and point out \nfuture research directions in this existing new research area. \n</p>"}, "author": "Yanwei Fu, Tao Xiang, Yu-Gang Jiang, Xiangyang Xue, Leonid Sigal, Shaogang Gong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708610", "id": "tag:google.com,2005:reader/item/000000031f972a0d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Two-stage Algorithm for Fairness-aware Machine Learning. (arXiv:1710.04924v1 [stat.ML])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04924"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04924", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Algorithmic decision making process now affects many aspects of our lives. \nStandard tools for machine learning, such as classification and regression, are \nsubject to the bias in data, and thus direct application of such off-the-shelf \ntools could lead to a specific group being unfairly discriminated. Removing \nsensitive attributes of data does not solve this problem because a \n\\textit{disparate impact} can arise when non-sensitive attributes and sensitive \nattributes are correlated. Here, we study a fair machine learning algorithm \nthat avoids such a disparate impact when making a decision. Inspired by the \ntwo-stage least squares method that is widely used in the field of economics, \nwe propose a two-stage algorithm that removes bias in the training data. The \nproposed algorithm is conceptually simple. Unlike most of existing fair \nalgorithms that are designed for classification tasks, the proposed method is \nable to (i) deal with regression tasks, (ii) combine explanatory attributes to \nremove reverse discrimination, and (iii) deal with numerical sensitive \nattributes. The performance and fairness of the proposed algorithm are \nevaluated in simulations with synthetic and real-world datasets. \n</p>"}, "author": "Junpei Komiyama, Hajime Shimao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507857538695", "timestampUsec": "1507857538695388", "id": "tag:google.com,2005:reader/item/000000031dfa2647", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sequence stacking using dual encoder Seq2Seq recurrent networks. (arXiv:1710.04211v1 [cs.LG])", "published": 1507857539, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04211"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04211", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A widely studied non-polynomial (NP) hard problem lies in finding a route \nbetween the two nodes of a graph. Often meta-heuristics algorithms such as \n$A^{*}$ are employed on graphs with a large number of nodes. Here, we propose a \ndeep recurrent neural network architecture based on the Sequence-2-Sequence \nmodel, widely used, for instance in text translation. Particularly, we \nillustrate that utilising a context vector that has been learned from two \ndifferent recurrent networks enables increased accuracies in learning the \nshortest route of a graph. Additionally, we show that one can boost the \nperformance of the Seq2Seq network by smoothing the loss function using a \nhomotopy continuation of the decoder's loss function. \n</p>"}, "author": "Alessandro Bay, Biswa Sengupta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507857538695", "timestampUsec": "1507857538695387", "id": "tag:google.com,2005:reader/item/000000031dfa264c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sum-Product-Quotient Networks. (arXiv:1710.04404v1 [cs.LG])", "published": 1507857539, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04404"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04404", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a novel tractable generative model that extends Sum-Product \nNetworks (SPNs) and significantly boost their power. We call it \nSum-Product-Quotient Networks (SPQNs), whose core concept is to incorporate \nconditional distributions into the model by direct computation using quotient \nnodes, e.g. $P(A|B){=}\\frac{P(A,B)}{P(B)}$. We provide sufficient conditions \nfor the tractability of SPQNs that generalize and relax the decomposable and \ncomplete tractability conditions of SPNs. These relaxed conditions give rise to \nan exponential boost to the expressive efficiency of our model, i.e. we prove \nthat there are distributions which SPQNs can compute efficiently but require \nSPNs to be of exponential size. Thus, we narrow the gap in expressivity between \ntractable graphical models and other Neural Network-based generative models. \n</p>"}, "author": "Or Sharir, Amnon Shashua", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505867", "id": "tag:google.com,2005:reader/item/000000031df63b5c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Measurement Context Extraction from Text: Discovering Opportunities and Gaps in Earth Science. (arXiv:1710.04312v1 [cs.IR])", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04312"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04312", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose Marve, a system for extracting measurement values, units, and \nrelated words from natural language text. Marve uses conditional random fields \n(CRF) to identify measurement values and units, followed by a rule-based system \nto find related entities, descriptors and modifiers within a sentence. Sentence \ntokens are represented by an undirected graphical model, and rules are based on \npart-of-speech and word dependency patterns connecting values and units to \ncontextual words. Marve is unique in its focus on measurement context and early \nexperimentation demonstrates Marve's ability to generate high-precision \nextractions with strong recall. We also discuss Marve's role in refining \nmeasurement requirements for NASA's proposed HyspIRI mission, a hyperspectral \ninfrared imaging satellite that will study the world's ecosystems. In general, \nour work with HyspIRI demonstrates the value of semantic measurement \nextractions in characterizing quantitative discussion contained in large \ncorpuses of natural language text. These extractions accelerate broad, \ncross-cutting research and expose scientists new algorithmic approaches and \nexperimental nuances. They also facilitate identification of scientific \nopportunities enabled by HyspIRI leading to more efficient scientific \ninvestment and research. \n</p>"}, "author": "Kyle Hundman, Chris A. Mattmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505866", "id": "tag:google.com,2005:reader/item/000000031df63b65", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Explaining Trained Neural Networks with Semantic Web Technologies: First Steps. (arXiv:1710.04324v1 [cs.AI])", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04324"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04324", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The ever increasing prevalence of publicly available structured data on the \nWorld Wide Web enables new applications in a variety of domains. In this paper, \nwe provide a conceptual approach that leverages such data in order to explain \nthe input-output behavior of trained artificial neural networks. We apply \nexisting Semantic Web technologies in order to provide an experimental proof of \nconcept. \n</p>"}, "author": "Md Kamruzzaman Sarker, Ning Xie, Derek Doran, Michael Raymer, Pascal Hitzler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505865", "id": "tag:google.com,2005:reader/item/000000031df63b71", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "DisSent: Sentence Representation Learning from Explicit Discourse Relations. (arXiv:1710.04334v1 [cs.CL])", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04334"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04334", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Sentence vectors represent an appealing approach to meaning: learn an \nembedding that encompasses the meaning of a sentence in a single vector, that \ncan be used for a variety of semantic tasks. Existing models for learning \nsentence embeddings either require extensive computational resources to train \non large corpora, or are trained on costly, manually curated datasets of \nsentence relations. We observe that humans naturally annotate the relations \nbetween their sentences with discourse markers like \"but\" and \"because\". These \nwords are deeply linked to the meanings of the sentences they connect. Using \nthis natural signal, we automatically collect a classification dataset from \nunannotated text. Training a model to predict these discourse markers yields \nhigh quality sentence embeddings. Our model captures complementary information \nto existing models and achieves comparable generalization performance to state \nof the art models. \n</p>"}, "author": "Allen Nie, Erin D. Bennett, Noah D. Goodman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505864", "id": "tag:google.com,2005:reader/item/000000031df63b79", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sign-Constrained Regularized Loss Minimization. (arXiv:1710.04380v1 [cs.LG])", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04380"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04380", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In practical analysis, domain knowledge about analysis target has often been \naccumulated, although, typically, such knowledge has been discarded in the \nstatistical analysis stage, and the statistical tool has been applied as a \nblack box. In this paper, we introduce sign constraints that are a handy and \nsimple representation for non-experts in generic learning problems. We have \ndeveloped two new optimization algorithms for the sign-constrained regularized \nloss minimization, called the sign-constrained Pegasos (SC-Pega) and the \nsign-constrained SDCA (SC-SDCA), by simply inserting the sign correction step \ninto the original Pegasos and SDCA, respectively. We present theoretical \nanalyses that guarantee that insertion of the sign correction step does not \ndegrade the convergence rate for both algorithms. Two applications, where the \nsign-constrained learning is effective, are presented. The one is exploitation \nof prior information about correlation between explanatory variables and a \ntarget variable. The other is introduction of the sign-constrained to \nSVM-Pairwise method. Experimental results demonstrate significant improvement \nof generalization performance by introducing sign constraints in both \napplications. \n</p>"}, "author": "Tsuyoshi Kato, Misato Kobayashi, Daisuke Sano", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505863", "id": "tag:google.com,2005:reader/item/000000031df63b87", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Marginal sequential Monte Carlo for doubly intractable models. (arXiv:1710.04382v1 [stat.CO])", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04382"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04382", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3d155a9b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3d155a9b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Bayesian inference for models that have an intractable partition function is \nknown as a doubly intractable problem, where standard Monte Carlo methods are \nnot applicable. The past decade has seen the development of auxiliary variable \nMonte Carlo techniques (M{\\o}ller et al., 2006; Murray et al., 2006) for \ntackling this problem; these approaches being members of the more general class \nof pseudo-marginal, or exact-approximate, Monte Carlo algorithms (Andrieu and \nRoberts, 2009), which make use of unbiased estimates of intractable posteriors. \nEveritt et al. (2017) investigated the use of exact-approximate importance \nsampling (IS) and sequential Monte Carlo (SMC) in doubly intractable problems, \nbut focussed only on SMC algorithms that used data-point tempering. This paper \ndescribes SMC samplers that may use alternative sequences of distributions, and \ndescribes ways in which likelihood estimates may be improved adaptively as the \nalgorithm progresses, building on ideas from Moores et al. (2015). This \napproach is compared with a number of alternative algorithms for doubly \nintractable problems, including approximate Bayesian computation (ABC), which \nwe show is closely related to the method of M{\\o}ller et al. (2006). \n</p>"}, "author": "Richard G. Everitt, Dennis Prangle, Philip Maybank, Mark Bell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505862", "id": "tag:google.com,2005:reader/item/000000031df63b93", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Arguing Machines: Perception-Control System Redundancy and Edge Case Discovery in Real-World Autonomous Driving. (arXiv:1710.04459v1 [cs.AI])", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04459"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04459", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Safe autonomous driving may be one of the most difficult engineering \nchallenges that any artificial intelligence system has been asked to do since \nthe birth of AI over sixty years ago. The difficulty is not within the task \nitself, but rather in the extremely small margin of allowable error given the \nhuman life at stake and the extremely large number of edge cases that have to \nbe accounted for. In other words, we task these systems to expect the \nunexpected with near 100% accuracy, which is a technical challenge for machine \nlearning methods that to date have generally been better at memorizing the \nexpected than predicting the unexpected. In fact, the process of efficiently \nand automatically discovering the edge cases of driving may be the key to \nsolving this engineering challenge. In this work, we propose and evaluate a \nmethod for discovering edge cases by monitoring the disagreement between two \nmonocular-vision-based automated steering systems. The first is a proprietary \nTesla Autopilot system equipped in the first generation of Autopilot-capable \nvehicles. The second is a end-to-end neural network trained on a large-scale \nnaturalistic dataset of 420 hours or 45 million frames of autonomous driving in \nTesla vehicles. \n</p>"}, "author": "Lex Fridman, Benedikt Jenik, Bryan Reimer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505861", "id": "tag:google.com,2005:reader/item/000000031df63b9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Clusters of Driving Behavior from Observational Smartphone Data. (arXiv:1710.04502v2 [cs.AI] UPDATED)", "published": 1509669621, "updated": 1509669622, "canonical": [{"href": "http://arxiv.org/abs/1710.04502"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04502", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Understanding driving behaviors is essential for improving safety and \nmobility of our transportation systems. Data is usually collected via \nsimulator-based studies or naturalistic driving studies. Those techniques allow \nfor understanding relations between demographics, road conditions and safety. \nOn the other hand, they are very costly and time consuming. Thanks to the \nsmartphone data, we have an opportunity to substantially complement more \ntraditional data collection techniques with data extracted from phone sensors, \nsuch as GPS, accelerometer gyroscope and camera. We developed statistical \nmodels that provided insight into driver behavior in the San Francisco metro \narea based on tens of thousands of driver logs. We used a novel data source to \nsupport our work. We used cell phone sensor data drawn from five hundred \ndrivers in San Francisco to understand the speed of traffic across the city as \nwell as the maneuvers of drivers in different areas. Specifically we clustered \ndrivers based on the way they drove around the city. We looked at driver norms \nby street and flagged driving behaviors that deviated from the norm. \n</p>"}, "author": "Josh Warren, Jeff Lipkowitz, Vadim Sokolov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505860", "id": "tag:google.com,2005:reader/item/000000031df63ba4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Is Epicurus the father of Reinforcement Learning?. (arXiv:1710.04582v1 [cs.LG])", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04582"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04582", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Epicurean Philosophy is commonly thought as simplistic and hedonistic. \nHere I discuss how this is a misconception and explore its link to \nReinforcement Learning. Based on the letters of Epicurus, I construct an \nobjective function for hedonism which turns out to be equivalent of the \nReinforcement Learning objective function when omitting the discount factor. I \nthen discuss how Plato and Aristotle 's views that can be also loosely linked \nto Reinforcement Learning, as well as their weaknesses in relationship to it. \nFinally, I emphasise the close affinity of the Epicurean views and the Bellman \nequation. \n</p>"}, "author": "Eleni Vasilaki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505859", "id": "tag:google.com,2005:reader/item/000000031df63baa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards Scalable Spectral Clustering via Spectrum-Preserving Sparsification. (arXiv:1710.04584v2 [cs.LG] UPDATED)", "published": 1510067299, "updated": 1510067302, "canonical": [{"href": "http://arxiv.org/abs/1710.04584"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04584", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The eigendeomposition of nearest-neighbor (NN) graph Laplacian matrices is \nthe main computational bottleneck in spectral clustering. In this work, we \nintroduce a highly-scalable, spectrum-preserving graph sparsification algorithm \nthat enables to build ultra-sparse NN (u-NN) graphs with guaranteed \npreservation of the original graph spectrums, such as the first few \neigenvectors of the original graph Laplacian. Our approach can immediately lead \nto scalable spectral clustering of large data networks without sacrificing \nsolution quality. The proposed method starts from constructing low-stretch \nspanning trees (LSSTs) from the original graphs, which is followed by \niteratively recovering small portions of \"spectrally critical\" off-tree edges \nto the LSSTs by leveraging a spectral off-tree embedding scheme. To determine \nthe suitable amount of off-tree edges to be recovered to the LSSTs, an \neigenvalue stability checking scheme is proposed, which enables to robustly \npreserve the first few Laplacian eigenvectors within the sparsified graph. \nAdditionally, an incremental graph densification scheme is proposed for \nidentifying extra edges that have been missing in the original NN graphs but \ncan still play important roles in spectral clustering tasks. Our experimental \nresults for a variety of well-known data sets show that the proposed method can \ndramatically reduce the complexity of NN graphs, leading to significant \nspeedups in spectral clustering. \n</p>"}, "author": "Yongyu Wang, Zhuo Feng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505854", "id": "tag:google.com,2005:reader/item/000000031df63bba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Construction of FuzzyFind Dictionary using Golay Coding Transformation for Searching Applications. (arXiv:1503.06483v1 [cs.DB] CROSS LISTED)", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1503.06483"}], "alternate": [{"href": "http://arxiv.org/abs/1503.06483", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Searching through a large volume of data is very critical for companies, \nscientists, and searching engines applications due to time complexity and \nmemory complexity. In this paper, a new technique of generating FuzzyFind \nDictionary for text mining was introduced. We simply mapped the 23 bits of the \nEnglish alphabet into a FuzzyFind Dictionary or more than 23 bits by using more \nFuzzyFind Dictionary, and reflecting the presence or absence of particular \nletters. This representation preserves closeness of word distortions in terms \nof closeness of the created binary vectors within Hamming distance of 2 \ndeviations. This paper talks about the Golay Coding Transformation Hash Table \nand how it can be used on a FuzzyFind Dictionary as a new technology for using \nin searching through big data. This method is introduced by linear time \ncomplexity for generating the dictionary and constant time complexity to access \nthe data and update by new data sets, also updating for new data sets is linear \ntime depends on new data points. This technique is based on searching only for \nletters of English that each segment has 23 bits, and also we have more than \n23-bit and also it could work with more segments as reference table. \n</p>"}, "author": "Kamran Kowsari, Maryam Yammahi, Nima Bari, Roman Vichr, Faisal Alsaby, Simon Y. Berkovich", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505853", "id": "tag:google.com,2005:reader/item/000000031df63bc2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Synkhronos: a Multi-GPU Theano Extension for Data Parallelism. (arXiv:1710.04162v1 [cs.DC] CROSS LISTED)", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04162"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04162", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present Synkhronos, an extension to Theano for multi-GPU computations \nleveraging data parallelism. Our framework provides automated execution and \nsynchronization across devices, allowing users to continue to write serial \nprograms without risk of race conditions. The NVIDIA Collective Communication \nLibrary is used for high-bandwidth inter-GPU communication. Further \nenhancements to the Theano function interface include input slicing (with \naggregation) and input indexing, which perform common data-parallel computation \npatterns efficiently. One example use case is synchronous SGD, which has \nrecently been shown to scale well for a growing set of deep learning problems. \nWhen training ResNet-50, we achieve a near-linear speedup of 7.5x on an NVIDIA \nDGX-1 using 8 GPUs, relative to Theano-only code running a single GPU in \nisolation. Yet Synkhronos remains general to any data-parallel computation \nprogrammable in Theano. By implementing parallelism at the level of individual \nTheano functions, our framework uniquely addresses a niche between manual \nmulti-device programming and prescribed multi-GPU training routines. \n</p>"}, "author": "Adam Stooke, Pieter Abbeel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714504", "id": "tag:google.com,2005:reader/item/000000031df6107d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sequence stacking using dual encoder Seq2Seq recurrent networks. (arXiv:1710.04211v1 [cs.LG])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04211"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04211", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A widely studied non-polynomial (NP) hard problem lies in finding a route \nbetween the two nodes of a graph. Often meta-heuristics algorithms such as \n$A^{*}$ are employed on graphs with a large number of nodes. Here, we propose a \ndeep recurrent neural network architecture based on the Sequence-2-Sequence \nmodel, widely used, for instance in text translation. Particularly, we \nillustrate that utilising a context vector that has been learned from two \ndifferent recurrent networks enables increased accuracies in learning the \nshortest route of a graph. Additionally, we show that one can boost the \nperformance of the Seq2Seq network by smoothing the loss function using a \nhomotopy continuation of the decoder's loss function. \n</p>"}, "author": "Alessandro Bay, Biswa Sengupta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714503", "id": "tag:google.com,2005:reader/item/000000031df610a0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Maximum Margin Interval Trees. (arXiv:1710.04234v2 [stat.ML] UPDATED)", "published": 1509323746, "updated": 1509323757, "canonical": [{"href": "http://arxiv.org/abs/1710.04234"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04234", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning a regression function using censored or interval-valued output data \nis an important problem in fields such as genomics and medicine. The goal is to \nlearn a real-valued prediction function, and the training output labels \nindicate an interval of possible values. Whereas most existing algorithms for \nthis task are linear models, in this paper we investigate learning nonlinear \ntree models. We propose to learn a tree by minimizing a margin-based \ndiscriminative objective function, and we provide a dynamic programming \nalgorithm for computing the optimal solution in log-linear time. We show \nempirically that this algorithm achieves state-of-the-art speed and prediction \naccuracy in a benchmark of several data sets. \n</p>"}, "author": "Alexandre Drouin, Toby Dylan Hocking, Fran&#xe7;ois Laviolette", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714502", "id": "tag:google.com,2005:reader/item/000000031df61101", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Local Convergence of Proximal Splitting Methods for Rank Constrained Problems. (arXiv:1710.04248v1 [math.OC])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04248"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04248", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We analyze the local convergence of proximal splitting algorithms to solve \noptimization problems that are convex besides a rank constraint. For this, we \nshow conditions under which the proximal operator of a function involving the \nrank constraint is locally identical to the proximal operator of its convex \nenvelope, hence implying local convergence. The conditions imply that the \nnon-convex algorithms locally converge to a solution whenever a convex \nrelaxation involving the convex envelope can be expected to solve the \nnon-convex problem. \n</p>"}, "author": "Christian Grussler, Pontus Giselsson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714501", "id": "tag:google.com,2005:reader/item/000000031df61131", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Gradient Descent in Continuous Time: A Central Limit Theorem. (arXiv:1710.04273v2 [math.PR] UPDATED)", "published": 1509669387, "updated": 1509669413, "canonical": [{"href": "http://arxiv.org/abs/1710.04273"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04273", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3d155fbd\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3d155fbd&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Stochastic gradient descent in continuous time (SGDCT) provides a \ncomputationally efficient method for the statistical learning of \ncontinuous-time models, which are widely used in science, engineering, and \nfinance. The SGDCT algorithm follows a (noisy) descent direction along a \ncontinuous stream of data. The parameter updates occur in continuous time and \nsatisfy a stochastic differential equation. This paper analyzes the asymptotic \nconvergence rate of the SGDCT algorithm by proving a central limit theorem for \nstrongly convex objective functions and, under slightly stronger conditions, \nfor non-convex objective functions as well. An L$^p$ convergence rate is also \nproven for the algorithm in the strongly convex case. \n</p>"}, "author": "Justin Sirignano, Konstantinos Spiliopoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714500", "id": "tag:google.com,2005:reader/item/000000031df61162", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improved Coresets for Kernel Density Estimates. (arXiv:1710.04325v1 [cs.LG])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04325"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04325", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3d1c6cad\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3d1c6cad&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We study the construction of coresets for kernel density estimates. That is \nwe show how to approximate the kernel density estimate described by a large \npoint set with another kernel density estimate with a much smaller point set. \nFor characteristic kernels (including Gaussian and Laplace kernels), our \napproximation preserves the $L_\\infty$ error between kernel density estimates \nwithin error $\\epsilon$, with coreset size $2/\\epsilon^2$, but no other aspects \nof the data, including the dimension, the diameter of the point set, or the \nbandwidth of the kernel common to other approximations. When the dimension is \nunrestricted, we show this bound is tight for these kernels as well as a much \nbroader set. \n</p> \n<p>This work provides a careful analysis of the iterative Frank-Wolfe algorithm \nadapted to this context, an algorithm called \\emph{kernel herding}. This \nanalysis unites a broad line of work that spans statistics, machine learning, \nand geometry. \n</p> \n<p>When the dimension $d$ is constant, we demonstrate much tighter bounds on the \nsize of the coreset specifically for Gaussian kernels, showing that it is \nbounded by the size of the coreset for axis-aligned rectangles. Currently the \nbest known constructive bound is $O(\\frac{1}{\\epsilon} \\log^d \n\\frac{1}{\\epsilon})$, and non-constructively, this can be improved by \n$\\sqrt{\\log \\frac{1}{\\epsilon}}$. This improves the best constant dimension \nbounds polynomially for $d \\geq 3$. \n</p>"}, "author": "Jeff M. Phillips, Wai Ming Tai", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714499", "id": "tag:google.com,2005:reader/item/000000031df6119c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "What Would a Graph Look Like in This Layout? A Machine Learning Approach to Large Graph Visualization. (arXiv:1710.04328v1 [cs.SI])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04328"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04328", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Using different methods for laying out a graph can lead to very different \nvisual appearances, with which the viewer perceives different information. \nSelecting a \"good\" layout method is thus important for visualizing a graph. The \nselection can be highly subjective and dependent on the given task. A common \napproach to selecting a good layout is to use aesthetic criteria and visual \ninspection. However, fully calculating various layouts and their associated \naesthetic metrics is computationally expensive. In this paper, we present a \nmachine learning approach to large graph visualization based on computing the \ntopological similarity of graphs using graph kernels. For a given graph, our \napproach can show what the graph would look like in different layouts and \nestimate their corresponding aesthetic metrics. An important contribution of \nour work is the development of a new framework to design graph kernels. Our \nexperimental study shows that our estimation calculation is considerably faster \nthan computing the actual layouts and their aesthetic metrics. Also, our graph \nkernels outperform the state-of-the-art ones in both time and accuracy. In \naddition, we conducted a user study to demonstrate that the topological \nsimilarity computed with our graph kernel matches perceptual similarity \nassessed by human users. \n</p>"}, "author": "Oh-Hyun Kwon, Tarik Crnovrsanin, Kwan-Liu Ma", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714498", "id": "tag:google.com,2005:reader/item/000000031df611f4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Efficient Data-Driven Geologic Feature Detection from Pre-stack Seismic Measurements using Randomized Machine-Learning Algorithm. (arXiv:1710.04329v1 [cs.LG])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04329"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04329", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Conventional seismic techniques for detecting the subsurface geologic \nfeatures are challenged by limited data coverage, computational inefficiency, \nand subjective human factors. We developed a novel data-driven geological \nfeature detection approach based on pre-stack seismic measurements. Our \ndetection method employs an efficient and accurate machine-learning detection \napproach to extract useful subsurface geologic features automatically. \nSpecifically, our method is based on kernel ridge regression model. The \nconventional kernel ridge regression can be computationally prohibited because \nof the large volume of seismic measurements. We employ a data reduction \ntechnique in combination with the conventional kernel ridge regression method \nto improve the computational efficiency and reduce memory usage. In particular, \nwe utilize a randomized numerical linear algebra technique, named Nystr\\\"om \nmethod, to effectively reduce the dimensionality of the feature space without \ncompromising the information content required for accurate detection. We \nprovide thorough computational cost analysis to show efficiency of our new \ngeological feature detection methods. We further validate the performance of \nour new subsurface geologic feature detection method using synthetic surface \nseismic data for 2D acoustic and elastic velocity models. Our numerical \nexamples demonstrate that our new detection method significantly improves the \ncomputational efficiency while maintaining comparable accuracy. Interestingly, \nwe show that our method yields a speed-up ratio on the order of $\\sim10^2$ to \n$\\sim 10^3$ in a multi-core computational environment. \n</p>"}, "author": "Youzuo Lin, Shusen Wang, Jayaraman Thiagarajan, George Guthrie, David Coblentz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714497", "id": "tag:google.com,2005:reader/item/000000031df612ce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition. (arXiv:1710.04340v1 [cs.LG])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04340"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04340", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Spectral decomposition of the Koopman operator is attracting attention as a \ntool for the analysis of nonlinear dynamical systems. Dynamic mode \ndecomposition is a popular numerical algorithm for Koopman spectral analysis; \nhowever, we often need to prepare nonlinear observables manually according to \nthe underlying dynamics, which is not always possible since we may not have any \na priori knowledge about them. In this paper, we propose a fully data-driven \nmethod for Koopman spectral analysis based on the principle of learning Koopman \ninvariant subspaces from observed data. To this end, we propose minimization of \nthe residual sum of squares of linear least-squares regression to estimate a \nset of functions that transforms data into a form in which the linear \nregression fits well. We introduce an implementation with neural networks and \nevaluate performance empirically using nonlinear dynamical systems and \napplications. \n</p>"}, "author": "Naoya Takeishi, Yoshinobu Kawahara, Takehisa Yairi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714496", "id": "tag:google.com,2005:reader/item/000000031df61327", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Unified Neural Network Approach for Estimating Travel Time and Distance for a Taxi Trip. (arXiv:1710.04350v1 [stat.ML])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04350"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04350", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In building intelligent transportation systems such as taxi or rideshare \nservices, accurate prediction of travel time and distance is crucial for \ncustomer experience and resource management. Using the NYC taxi dataset, which \ncontains taxi trips data collected from GPS-enabled taxis [23], this paper \ninvestigates the use of deep neural networks to jointly predict taxi trip time \nand distance. We propose a model, called ST-NN (Spatio-Temporal Neural \nNetwork), which first predicts the travel distance between an origin and a \ndestination GPS coordinate, then combines this prediction with the time of day \nto predict the travel time. The beauty of ST-NN is that it uses only the raw \ntrips data without requiring further feature engineering and provides a joint \nestimate of travel time and distance. We compare the performance of ST-NN to \nthat of state-of-the-art travel time estimation methods, and we observe that \nthe proposed approach generalizes better than state-of-the-art methods. We show \nthat ST-NN approach significantly reduces the mean absolute error for both \npredicted travel time and distance, about 17% for travel time prediction. We \nalso observe that the proposed approach is more robust to outliers present in \nthe dataset by testing the performance of ST-NN on the datasets with and \nwithout outliers. \n</p>"}, "author": "Ishan Jindal, Tony (Zhiwei)Qin, Xuewen Chen, Matthew Nokleby, Jieping Ye", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714495", "id": "tag:google.com,2005:reader/item/000000031df61396", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning in Multiple Multistep Time Series Prediction. (arXiv:1710.04373v1 [stat.ML])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04373"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04373", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The project aims to research on combining deep learning specifically \nLong-Short Memory (LSTM) and basic statistics in multiple multistep time series \nprediction. LSTM can dive into all the pages and learn the general trends of \nvariation in a large scope, while the well selected medians for each page can \nkeep the special seasonality of different pages so that the future trend will \nnot fluctuate too much from the reality. A recent Kaggle competition on 145K \nWeb Traffic Time Series Forecasting [1] is used to thoroughly illustrate and \ntest this idea. \n</p>"}, "author": "Chuanyun Zang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714494", "id": "tag:google.com,2005:reader/item/000000031df613d8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Marginal sequential Monte Carlo for doubly intractable models. (arXiv:1710.04382v1 [stat.CO])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04382"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04382", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Bayesian inference for models that have an intractable partition function is \nknown as a doubly intractable problem, where standard Monte Carlo methods are \nnot applicable. The past decade has seen the development of auxiliary variable \nMonte Carlo techniques (M{\\o}ller et al., 2006; Murray et al., 2006) for \ntackling this problem; these approaches being members of the more general class \nof pseudo-marginal, or exact-approximate, Monte Carlo algorithms (Andrieu and \nRoberts, 2009), which make use of unbiased estimates of intractable posteriors. \nEveritt et al. (2017) investigated the use of exact-approximate importance \nsampling (IS) and sequential Monte Carlo (SMC) in doubly intractable problems, \nbut focussed only on SMC algorithms that used data-point tempering. This paper \ndescribes SMC samplers that may use alternative sequences of distributions, and \ndescribes ways in which likelihood estimates may be improved adaptively as the \nalgorithm progresses, building on ideas from Moores et al. (2015). This \napproach is compared with a number of alternative algorithms for doubly \nintractable problems, including approximate Bayesian computation (ABC), which \nwe show is closely related to the method of M{\\o}ller et al. (2006). \n</p>"}, "author": "Richard G. Everitt, Dennis Prangle, Philip Maybank, Mark Bell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714493", "id": "tag:google.com,2005:reader/item/000000031df61434", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sum-Product-Quotient Networks. (arXiv:1710.04404v1 [cs.LG])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04404"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04404", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a novel tractable generative model that extends Sum-Product \nNetworks (SPNs) and significantly boost their power. We call it \nSum-Product-Quotient Networks (SPQNs), whose core concept is to incorporate \nconditional distributions into the model by direct computation using quotient \nnodes, e.g. $P(A|B){=}\\frac{P(A,B)}{P(B)}$. We provide sufficient conditions \nfor the tractability of SPQNs that generalize and relax the decomposable and \ncomplete tractability conditions of SPNs. These relaxed conditions give rise to \nan exponential boost to the expressive efficiency of our model, i.e. we prove \nthat there are distributions which SPQNs can compute efficiently but require \nSPNs to be of exponential size. Thus, we narrow the gap in expressivity between \ntractable graphical models and other Neural Network-based generative models. \n</p>"}, "author": "Or Sharir, Amnon Shashua", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714492", "id": "tag:google.com,2005:reader/item/000000031df6146a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Self-Taught Support Vector Machine. (arXiv:1710.04450v1 [cs.CV])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04450"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04450", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, a new approach for classification of target task using limited \nlabeled target data as well as enormous unlabeled source data is proposed which \nis called self-taught learning. The target and source data can be drawn from \ndifferent distributions. In the previous approaches, covariate shift assumption \nis considered where the marginal distributions p(x) change over domains and the \nconditional distributions p(y|x) remain the same. In our approach, we propose a \nnew objective function which simultaneously learns a common space T(.) where \nthe conditional distributions over domains p(T(x)|y) remain the same and learns \nrobust SVM classifiers for target task using both source and target data in the \nnew representation. Hence, in the proposed objective function, the hidden label \nof the source data is also incorporated. We applied the proposed approach on \nCaltech-256, MSRC+LMO datasets and compared the performance of our algorithm to \nthe available competing methods. Our method has a superior performance to the \nsuccessful existing algorithms. \n</p>"}, "author": "Parvin Razzaghi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714491", "id": "tag:google.com,2005:reader/item/000000031df61498", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Improved Naive Bayes Classifier-based Noise Detection Technique for Classifying User Phone Call Behavior. (arXiv:1710.04461v1 [cs.LG])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04461"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04461", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3d1c6fc4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3d1c6fc4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The presence of noisy instances in mobile phone data is a fundamental issue \nfor classifying user phone call behavior (i.e., accept, reject, missed and \noutgoing), with many potential negative consequences. The classification \naccuracy may decrease and the complexity of the classifiers may increase due to \nthe number of redundant training samples. To detect such noisy instances from a \ntraining dataset, researchers use naive Bayes classifier (NBC) as it identifies \nmisclassified instances by taking into account independence assumption and \nconditional probabilities of the attributes. However, some of these \nmisclassified instances might indicate usages behavioral patterns of individual \nmobile phone users. Existing naive Bayes classifier based noise detection \ntechniques have not considered this issue and, thus, are lacking in \nclassification accuracy. \n</p> \n<p>In this paper, we propose an improved noise detection technique based on \nnaive Bayes classifier for effectively classifying users' phone call behaviors. \nIn order to improve the classification accuracy, we effectively identify noisy \ninstances from the training dataset by analyzing the behavioral patterns of \nindividuals. We dynamically determine a noise threshold according to \nindividual's unique behavioral patterns by using both the naive Bayes \nclassifier and Laplace estimator. We use this noise threshold to identify noisy \ninstances. To measure the effectiveness of our technique in classifying user \nphone call behavior, we employ the most popular classification algorithm (e.g., \ndecision tree). Experimental results on the real phone call log dataset show \nthat our proposed technique more accurately identifies the noisy instances from \nthe training datasets that leads to better classification accuracy. \n</p>"}, "author": "Iqbal H. Sarker, Muhammad Ashad Kabir, Alan Colman, Jun Han", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714490", "id": "tag:google.com,2005:reader/item/000000031df614e8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Effects of Images with Different Levels of Familiarity on EEG. (arXiv:1710.04462v1 [stat.ML])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04462"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04462", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Evaluating human brain potentials during watching different images can be \nused for memory evaluation, information retrieving, guilty-innocent \nidentification and examining the brain response. In this study, the effects of \nwatching images, with different levels of familiarity, on subjects' \nElectroencephalogram (EEG) have been studied. Three different groups of images \nwith three familiarity levels of \"unfamiliar\", \"familiar\" and \"very familiar\" \nhave been considered for this study. EEG signals of 21 subjects (14 men) were \nrecorded. After signal acquisition, pre-processing, including noise and \nartifact removal, were performed on epochs of data. Features, including \nspatial-statistical, wavelet, frequency and harmonic parameters, and also \ncorrelation between recording channels, were extracted from the data. Then, we \nevaluated the efficiency of the extracted features by using p-value and also an \northogonal feature selection method (combination of Gram-Schmitt method and \nFisher discriminant ratio) for feature dimensional reduction. As the final step \nof feature selection, we used 'add-r take-away l' method for choosing the most \ndiscriminative features. For data classification, including all two-class and \nthree-class cases, we applied Support Vector Machine (SVM) on the extracted \nfeatures. The correct classification rates (CCR) for \"unfamiliar-familiar\", \n\"unfamiliar-very familiar\" and \"familiar-very familiar\" cases were 85.6%, \n92.6%, and 70.6%, respectively. The best results of classifications were \nobtained in pre-frontal and frontal regions of brain. Also, wavelet, frequency \nand harmonic features were among the most discriminative features. Finally, in \nthree-class case, the best CCR was 86.8%. \n</p>"}, "author": "Ali Saeedi, Ehsan Arbabi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714489", "id": "tag:google.com,2005:reader/item/000000031df61525", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Dimensionality Reduction Ensembles. (arXiv:1710.04484v1 [stat.ML])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04484"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04484", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Ensemble learning has had many successes in supervised learning, but it has \nbeen rare in unsupervised learning and dimensionality reduction. This study \nexplores dimensionality reduction ensembles, using principal component analysis \nand manifold learning techniques to capture linear, nonlinear, local, and \nglobal features in the original dataset. Dimensionality reduction ensembles are \ntested first on simulation data and then on two real medical datasets using \nrandom forest classifiers; results suggest the efficacy of this approach, with \naccuracies approaching that of the full dataset. Limitations include \ncomputational cost of some algorithms with strong performance, which may be \nameliorated through distributed computing and the development of more efficient \nversions of these algorithms. \n</p>"}, "author": "Colleen M. Farrelly", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714488", "id": "tag:google.com,2005:reader/item/000000031df6156a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multimodal Observation and Interpretation of Subjects Engaged in Problem Solving. (arXiv:1710.04486v1 [cs.HC])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04486"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04486", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we present the first results of a pilot experiment in the \ncapture and interpretation of multimodal signals of human experts engaged in \nsolving challenging chess problems. Our goal is to investigate the extent to \nwhich observations of eye-gaze, posture, emotion and other physiological \nsignals can be used to model the cognitive state of subjects, and to explore \nthe integration of multiple sensor modalities to improve the reliability of \ndetection of human displays of awareness and emotion. We observed chess players \nengaged in problems of increasing difficulty while recording their behavior. \nSuch recordings can be used to estimate a participant's awareness of the \ncurrent situation and to predict ability to respond effectively to challenging \nsituations. Results show that a multimodal approach is more accurate than a \nunimodal one. By combining body posture, visual attention and emotion, the \nmultimodal approach can reach up to 93% of accuracy when determining player's \nchess expertise while unimodal approach reaches 86%. Finally this experiment \nvalidates the use of our equipment as a general and reproducible tool for the \nstudy of participants engaged in screen-based interaction and/or problem \nsolving. \n</p>"}, "author": "Thomas Guntz (LIG), Raffaella Balzarini (LIG), Dominique Vaufreydaz (LIG, UGA), James L. Crowley (Grenoble INP, LIG)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714487", "id": "tag:google.com,2005:reader/item/000000031df615a3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Subjectively Interesting Subgroup Discovery on Real-valued Targets. (arXiv:1710.04521v1 [stat.ML])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04521"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04521", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deriving insights from high-dimensional data is one of the core problems in \ndata mining. The difficulty mainly stems from the fact that there are \nexponentially many variable combinations to potentially consider, and there are \ninfinitely many if we consider weighted combinations, even for linear \ncombinations. Hence, an obvious question is whether we can automate the search \nfor interesting patterns and visualizations. In this paper, we consider the \nsetting where a user wants to learn as efficiently as possible about \nreal-valued attributes. For example, to understand the distribution of crime \nrates in different geographic areas in terms of other (numerical, ordinal \nand/or categorical) variables that describe the areas. We introduce a method to \nfind subgroups in the data that are maximally informative (in the formal \nInformation Theoretic sense) with respect to a single or set of real-valued \ntarget attributes. The subgroup descriptions are in terms of a succinct set of \narbitrarily-typed other attributes. The approach is based on the Subjective \nInterestingness framework FORSIED to enable the use of prior knowledge when \nfinding most informative non-redundant patterns, and hence the method also \nsupports iterative data mining. \n</p>"}, "author": "Jefrey Lijffijt, Bo Kang, Wouter Duivesteijn, Kai Puolam&#xe4;ki, Emilia Oikarinen, Tijl De Bie", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714486", "id": "tag:google.com,2005:reader/item/000000031df615c8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "New efficient algorithms for multiple change-point detection with kernels. (arXiv:1710.04556v1 [math.ST])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04556"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04556", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Several statistical approaches based on reproducing kernels have been \nproposed to detect abrupt changes arising in the full distribution of the \nobservations and not only in the mean or variance. Some of these approaches \nenjoy good statistical properties (oracle inequality, \\ldots). Nonetheless, \nthey have a high computational cost both in terms of time and memory. This \nmakes their application difficult even for small and medium sample sizes ($n&lt; \n10^4$). This computational issue is addressed by first describing a new \nefficient and exact algorithm for kernel multiple change-point detection with \nan improved worst-case complexity that is quadratic in time and linear in \nspace. It allows dealing with medium size signals (up to $n \\approx 10^5$). \nSecond, a faster but approximation algorithm is described. It is based on a \nlow-rank approximation to the Gram matrix. It is linear in time and space. This \napproximation algorithm can be applied to large-scale signals ($n \\geq 10^6$). \nThese exact and approximation algorithms have been implemented in \\texttt{R} \nand \\texttt{C} for various kernels. The computational and statistical \nperformances of these new algorithms have been assessed through empirical \nexperiments. The runtime of the new algorithms is observed to be faster than \nthat of other considered procedures. Finally, simulations confirmed the higher \nstatistical accuracy of kernel-based approaches to detect changes that are not \nonly in the mean. These simulations also illustrate the flexibility of \nkernel-based approaches to analyze complex biological profiles made of DNA copy \nnumber and allele B frequencies. An R package implementing the approach will be \nmade available on github. \n</p>"}, "author": "Alain Celisse (LPP, MODAL), Guillemette Marot (MODAL, CERIM), Morgane Pierre-Jean (LaMME), Guillem Rigaill (URGV)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714485", "id": "tag:google.com,2005:reader/item/000000031df61611", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Additivity of Information in Multilayer Networks via Additive Gaussian Noise Transforms. (arXiv:1710.04580v1 [cs.IT])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04580"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04580", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Multilayer (or deep) networks are powerful probabilistic models based on \nmultiple stages of a linear transform followed by a non-linear (possibly \nrandom) function. In general, the linear transforms are defined by matrices and \nthe non-linear functions are defined by information channels. These models have \ngained great popularity due to their ability to characterize complex \nprobabilistic relationships arising in a wide variety of inference problems. \nThe contribution of this paper is a new method for analyzing the fundamental \nlimits of statistical inference in settings where the model is known. The \nvalidity of our method can be established in a number of settings and is \nconjectured to hold more generally. A key assumption made throughout is that \nthe matrices are drawn randomly from orthogonally invariant distributions. \n</p> \n<p>Our method yields explicit formulas for 1) the mutual information; 2) the \nminimum mean-squared error (MMSE); 3) the existence and locations of certain \nphase-transitions with respect to the problem parameters; and 4) the stationary \npoints for the state evolution of approximate message passing algorithms. When \napplied to the special case of models with multivariate Gaussian channels our \nmethod is rigorous and has close connections to free probability theory for \nrandom matrices. When applied to the general case of non-Gaussian channels, our \nmethod provides a simple alternative to the replica method from statistical \nphysics. A key observation is that the combined effects of the individual \ncomponents in the model (namely the matrices and the channels) are additive \nwhen viewed in a certain transform domain. \n</p>"}, "author": "Galen Reeves", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714484", "id": "tag:google.com,2005:reader/item/000000031df6169a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Is Epicurus the father of Reinforcement Learning?. (arXiv:1710.04582v1 [cs.LG])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04582"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04582", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Epicurean Philosophy is commonly thought as simplistic and hedonistic. \nHere I discuss how this is a misconception and explore its link to \nReinforcement Learning. Based on the letters of Epicurus, I construct an \nobjective function for hedonism which turns out to be equivalent of the \nReinforcement Learning objective function when omitting the discount factor. I \nthen discuss how Plato and Aristotle 's views that can be also loosely linked \nto Reinforcement Learning, as well as their weaknesses in relationship to it. \nFinally, I emphasise the close affinity of the Epicurean views and the Bellman \nequation. \n</p>"}, "author": "Eleni Vasilaki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714483", "id": "tag:google.com,2005:reader/item/000000031df616d6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards Scalable Spectral Clustering via Spectrum-Preserving Sparsification. (arXiv:1710.04584v2 [cs.LG] UPDATED)", "published": 1510049224, "updated": 1510049277, "canonical": [{"href": "http://arxiv.org/abs/1710.04584"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04584", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The eigendeomposition of nearest-neighbor (NN) graph Laplacian matrices is \nthe main computational bottleneck in spectral clustering. In this work, we \nintroduce a highly-scalable, spectrum-preserving graph sparsification algorithm \nthat enables to build ultra-sparse NN (u-NN) graphs with guaranteed \npreservation of the original graph spectrums, such as the first few \neigenvectors of the original graph Laplacian. Our approach can immediately lead \nto scalable spectral clustering of large data networks without sacrificing \nsolution quality. The proposed method starts from constructing low-stretch \nspanning trees (LSSTs) from the original graphs, which is followed by \niteratively recovering small portions of \"spectrally critical\" off-tree edges \nto the LSSTs by leveraging a spectral off-tree embedding scheme. To determine \nthe suitable amount of off-tree edges to be recovered to the LSSTs, an \neigenvalue stability checking scheme is proposed, which enables to robustly \npreserve the first few Laplacian eigenvectors within the sparsified graph. \nAdditionally, an incremental graph densification scheme is proposed for \nidentifying extra edges that have been missing in the original NN graphs but \ncan still play important roles in spectral clustering tasks. Our experimental \nresults for a variety of well-known data sets show that the proposed method can \ndramatically reduce the complexity of NN graphs, leading to significant \nspeedups in spectral clustering. \n</p>"}, "author": "Yongyu Wang, Zhuo Feng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714474", "id": "tag:google.com,2005:reader/item/000000031df61710", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec. (arXiv:1710.02971v2 [cs.SI] CROSS LISTED)", "published": 1507854805, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.02971"}], "alternate": [{"href": "http://arxiv.org/abs/1710.02971", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Since the invention of word2vec, the skip-gram model has significantly \nadvanced the research of network embedding, such as the recent emergence of \nDeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of \nthe aforementioned models with negative sampling can be unified into the matrix \nfactorization framework with closed forms. Our analysis and proofs reveal that: \n(1) DeepWalk empirically produces a low-rank transformation of the normalized \nLaplacian matrix of a network; (2) LINE, in theory, is a special case of \nDeepWalk when the size of vertex context is set to one; (3) As an extension to \nLINE, PTE can be viewed as the joint factorization of multiple Laplacian \nmatrices; (4) node2vec is factorizing a matrix related to the stationary \ndistribution and transition probability tensor of a 2nd-order random walk. We \nfurther provide the theoretical connections between skip-gram based network \nembedding algorithms and the theory of graph Laplacian. Finally, we present the \nNetMF method as well as its approximation algorithm for computing network \nembedding. Our method offers significant improvements over DeepWalk and LINE \n(up to 38% relatively) in several conventional network mining tasks. This work \nlays the theoretical foundation for skip-gram based network embedding methods, \nleading to a better understanding of latent network representations. \n</p>"}, "author": "Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507769090732", "timestampUsec": "1507769090731986", "id": "tag:google.com,2005:reader/item/000000031d4e906f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimizing Long Short-Term Memory Recurrent Neural Networks Using Ant Colony Optimization to Predict Turbine Engine Vibration. (arXiv:1710.03753v1 [cs.NE])", "published": 1507769091, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.03753"}], "alternate": [{"href": "http://arxiv.org/abs/1710.03753", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a06d3d1c7328\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a06d3d1c7328&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This article expands on research that has been done to develop a recurrent \nneural network (RNN) capable of predicting aircraft engine vibrations using \nlong short-term memory (LSTM) neurons. LSTM RNNs can provide a more \ngeneralizable and robust method for prediction over analytical calculations of \nengine vibration, as analytical calculations must be solved iteratively based \non specific empirical engine parameters, making this approach ungeneralizable \nacross multiple engines. In initial work, multiple LSTM RNN architectures were \nproposed, evaluated and compared. This research improves the performance of the \nmost effective LSTM network design proposed in the previous work by using a \npromising neuroevolution method based on ant colony optimization (ACO) to \ndevelop and enhance the LSTM cell structure of the network. A parallelized \nversion of the ACO neuroevolution algorithm has been developed and the evolved \nLSTM RNNs were compared to the previously used fixed topology. The evolved \nnetworks were trained on a large database of flight data records obtained from \nan airline containing flights that suffered from excessive vibration. Results \nwere obtained using MPI (Message Passing Interface) on a high performance \ncomputing (HPC) cluster, evolving 1000 different LSTM cell structures using 168 \ncores over 4 days. The new evolved LSTM cells showed an improvement of 1.35%, \nreducing prediction error from 5.51% to 4.17% when predicting excessive engine \nvibrations 10 seconds in the future, while at the same time dramatically \nreducing the number of weights from 21,170 to 11,810. \n</p>"}, "author": "AbdElRahman ElSaid, Travis Desell, Fatima El Jamiy, James Higgins, Brandon Wild", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}]}