{"all_articles": [{"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215439", "id": "tag:google.com,2005:reader/item/0000000346d96aa0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Enhancing approximation abilities of neural networks by training derivatives. (arXiv:1712.04473v1 [cs.NE])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04473"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04473", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504de5407\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504de5407&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Method for increasing precision of feedforward networks is presented. With \nthe aid of it they can serve as a better tool for describing smooth functions. \nNamely, it is shown that when training uses derivatives of target function up \nto the fourth order, approximation can be nearly machine precise. It is \ndemonstrated in a number of cases: 2D function approximation, training \nautoencoder to compress 3D spiral into 1D, and solving 2D boundary value \nproblem for Poisson equation with nonlinear source. In the first case cost \nfunction in addition to squared difference between output and target contains \nsquared differences between their derivatives with respect to input variables. \nTraining autoencoder is similar, but differentiation is done with respect to \nparameter that generates the spiral. Supplied with derivatives up to the fourth \nthe method is found to be 30-200 times more accurate than regular training \nprovided networks are of sufficient size and depth. Solving PDE is more \npractical since higher derivatives are not calculated beforehand, but \ninformation about them is extracted from the equation itself. Classical \napproach is to put perceptron in place of unknown function, choose the cost as \nsquared residual and to minimize it with respect to weights. This would ensure \nthat equation holds within some margin of error. Additional terms used in cost \nfunction are squared derivatives of the residual with respect to independent \nvariables. Supplied with terms up to the second order the method is found to be \n5 times more accurate. Efficient GPU version of algorithm is proposed. \n</p>"}, "author": "V.I. Avrutskiy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215438", "id": "tag:google.com,2005:reader/item/0000000346d96aba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the organization of grid and place cells: Neural de-noising via subspace learning. (arXiv:1712.04602v1 [q-bio.NC])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04602"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04602", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Place cells in the hippocampus are active when an animal visits a certain \nlocations (referred to as place fields) within an environment and remain silent \notherwise. Grid cells in the medial entorhinal cortex (MEC) respond at multiple \nlocations, with firing fields that exhibit a hexagonally symmetric periodic \npattern. The joint activity of grid and place cell populations, as a function \nof location, forms a neural code for space. An ensemble of codes, for a given \nset of parameters, is generated by selecting grid and place cell population and \ntuning curve parameters. For each ensemble, codewords are generated by \nstimulating a network with a discrete set of locations. In this manuscript, we \ndevelop an understanding of the relationships between coding theoretic \nproperties of these combined populations and code construction parameters. \nThese observations are revisited by measuring the performances of biologically \nrealizable algorithms (e.g. neural bit-flipping) implemented by a network of \nplace and grid cell populations, as well as interneurons, which perform \nde-noising operations. Simulations demonstrate that de-noising mechanisms \nanalyzed here can significantly improve fidelity of this neural representation \nof space. Further, patterns observed in connectivity of each population of \nsimulated cells suggest the existence of heretofore unobserved neurobiological \nphenomena. \n</p>"}, "author": "David M. Schwartz, O. Ozan Koyluoglu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215437", "id": "tag:google.com,2005:reader/item/0000000346d96ad7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Quaternion Networks. (arXiv:1712.04604v1 [cs.NE])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04604"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04604", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The field of deep learning has seen significant advancement in recent years. \nHowever, much of the existing work has been focused on real-valued numbers. \nRecent work has shown that a deep learning system using the complex numbers can \nbe deeper for a set parameter budget compared to its real-valued counterpart. \nIn this work, we explore the benefits of generalizing one step further into the \nhyper-complex numbers, quaternions specifically, and provide the architecture \ncomponents needed to build deep quaternion networks. We go over quaternion \nconvolutions, present a quaternion weight initialization scheme, and present \nalgorithms for quaternion batch-normalization. These pieces are tested by \nend-to-end training on the CIFAR-10 and CIFAR-100 data sets to show the \nimproved convergence to a real-valued network. \n</p>"}, "author": "Chase Gaudet, Anthony Maida", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215436", "id": "tag:google.com,2005:reader/item/0000000346d96ae1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Logarithmic distributions prove that intrinsic learning is Hebbian. (arXiv:1410.5610v3 [q-bio.NC] UPDATED)", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1410.5610"}], "alternate": [{"href": "http://arxiv.org/abs/1410.5610", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we present data for the lognormal distributions of spike \nrates, synaptic weights and intrinsic excitability (gain) for neurons in \nvarious brain areas, such as auditory or visual cortex, hippocampus, \ncerebellum, striatum, midbrain nuclei. We find a remarkable consistency of \nheavy-tailed, specifically lognormal, distributions for rates, weights and \ngains in all brain areas examined. The difference between strongly recurrent \nand feed-forward connectivity (cortex vs. striatum and cerebellum), \nneurotransmitter (GABA (striatum) or glutamate (cortex)) or the level of \nactivation (low in cortex, high in Purkinje cells and midbrain nuclei) turns \nout to be irrelevant for this feature. Logarithmic scale distribution of \nweights and gains appears to be a general, functional property in all cases \nanalyzed. We then created a generic neural model to investigate adaptive \nlearning rules that create and maintain lognormal distributions. We \nconclusively demonstrate that not only weights, but also intrinsic gains, need \nto have strong Hebbian learning in order to produce and maintain the \nexperimentally attested distributions. This provides a solution to the \nlong-standing question about the type of plasticity exhibited by intrinsic \nexcitability. \n</p>"}, "author": "Gabriele Scheler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215435", "id": "tag:google.com,2005:reader/item/0000000346d96aea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates. (arXiv:1708.07120v2 [cs.LG] UPDATED)", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.07120"}], "alternate": [{"href": "http://arxiv.org/abs/1708.07120", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we show a phenomenon, which we named \"super-convergence\", \nwhere residual networks can be trained using an order of magnitude fewer \niterations than is used with standard training methods. The existence of \nsuper-convergence is relevant to understanding why deep networks generalize \nwell. One of the key elements of super-convergence is training with cyclical \nlearning rates and a large maximum learning rate. Furthermore, we present \nevidence that training with large learning rates improves performance by \nregularizing the network. In addition, we show that super-convergence provides \na greater boost in performance relative to standard training when the amount of \nlabeled training data is limited. We also derive a simplification of the \nHessian Free optimization method to compute an estimate of the optimal learning \nrate. The architectures and code to replicate the figures in this paper are \navailable at github.com/lnsmith54/super-convergence. \n</p>"}, "author": "Leslie N. Smith, Nicholay Topin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215434", "id": "tag:google.com,2005:reader/item/0000000346d96aee", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast and Scalable Distributed Deep Convolutional Autoencoder for fMRI Big Data Analytics. (arXiv:1710.08961v2 [cs.DC] UPDATED)", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08961"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08961", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In recent years, analyzing task-based fMRI (tfMRI) data has become an \nessential tool for understanding brain function and networks. However, due to \nthe sheer size of tfMRI data, its intrinsic complex structure, and lack of \nground truth of underlying neural activities, modeling tfMRI data is hard and \nchallenging. Previously proposed data-modeling methods including Independent \nComponent Analysis (ICA) and Sparse Dictionary Learning only provided a weakly \nestablished model based on blind source separation under the strong assumption \nthat original fMRI signals could be linearly decomposed into time series \ncomponents with corresponding spatial maps. Meanwhile, analyzing and learning a \nlarge amount of tfMRI data from a variety of subjects has been shown to be very \ndemanding but yet challenging even with technological advances in computational \nhardware. Given the Convolutional Neural Network (CNN), a robust method for \nlearning high-level abstractions from low-level data such as tfMRI time series, \nin this work we propose a fast and scalable novel framework for distributed \ndeep Convolutional Autoencoder model. This model aims to both learn the complex \nhierarchical structure of the tfMRI data and to leverage the processing power \nof multiple GPUs in a distributed fashion. To implement such a model, we have \ncreated an enhanced processing pipeline on the top of Apache Spark and \nTensorflow library, leveraging from a very large cluster of GPU machines. \nExperimental data from applying the model on the Human Connectome Project (HCP) \nshow that the proposed model is efficient and scalable toward tfMRI big data \nanalytics, thus enabling data-driven extraction of hierarchical neuroscientific \ninformation from massive fMRI big data in the future. \n</p>"}, "author": "Milad Makkie, Heng Huang, Yu Zhao, Athanasios V. Vasilakos, Tianming Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215433", "id": "tag:google.com,2005:reader/item/0000000346d96afc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning. (arXiv:1711.01577v3 [stat.ML] UPDATED)", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01577"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01577", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Long Short-Term Memory (LSTM) is a popular approach to boosting the ability \nof Recurrent Neural Networks to store longer term temporal information. The \ncapacity of an LSTM network can be increased by widening and adding layers. \nHowever, usually the former introduces additional parameters, while the latter \nincreases the runtime. As an alternative we propose the Tensorized LSTM in \nwhich the hidden states are represented by tensors and updated via a \ncross-layer convolution. By increasing the tensor size, the network can be \nwidened efficiently without additional parameters since the parameters are \nshared across different locations in the tensor; by delaying the output, the \nnetwork can be deepened implicitly with little additional runtime since deep \ncomputations for each timestep are merged into temporal computations of the \nsequence. Experiments conducted on five challenging sequence learning tasks \nshow the potential of the proposed model. \n</p>"}, "author": "Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, David Barber", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215432", "id": "tag:google.com,2005:reader/item/0000000346d96b03", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Sequential Prediction of Social Media Popularity with Deep Temporal Context Networks. (arXiv:1712.04443v1 [cs.SI])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04443"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04443", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Prediction of popularity has profound impact for social media, since it \noffers opportunities to reveal individual preference and public attention from \nevolutionary social systems. Previous research, although achieves promising \nresults, neglects one distinctive characteristic of social data, i.e., \nsequentiality. For example, the popularity of online content is generated over \ntime with sequential post streams of social media. To investigate the \nsequential prediction of popularity, we propose a novel prediction framework \ncalled Deep Temporal Context Networks (DTCN) by incorporating both temporal \ncontext and temporal attention into account. Our DTCN contains three main \ncomponents, from embedding, learning to predicting. With a joint embedding \nnetwork, we obtain a unified deep representation of multi-modal user-post data \nin a common embedding space. Then, based on the embedded data sequence over \ntime, temporal context learning attempts to recurrently learn two adaptive \ntemporal contexts for sequential popularity. Finally, a novel temporal \nattention is designed to predict new popularity (the popularity of a new \nuser-post pair) with temporal coherence across multiple time-scales. \nExperiments on our released image dataset with about 600K Flickr photos \ndemonstrate that DTCN outperforms state-of-the-art deep prediction algorithms, \nwith an average of 21.51% relative performance improvement in the popularity \nprediction (Spearman Ranking Correlation). \n</p>"}, "author": "Bo Wu, Wen-Huang Cheng, Yongdong Zhang, Qiushi Huang, Jintao Li, Tao Mei", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215431", "id": "tag:google.com,2005:reader/item/0000000346d96b0d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Consideration on Example 2 of \"An Algorithm of General Fuzzy InferenceWith The Reductive Property\". (arXiv:1712.04596v1 [cs.AI])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04596"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04596", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we will show that (1) the results about the fuzzy reasoning \nalgoritm obtained in the paper \"Computer Sciences Vol. 34, No.4, pp.145-148, \n2007\" according to the paper \"IEEE Transactions On systems, Man and \ncybernetics, 18, pp.1049-1056, 1988\" are correct; (2) example 2 in the paper \n\"An Algorithm of General Fuzzy Inference With The Reductive Property\" presented \nby He Ying-Si, Quan Hai-Jin and Deng Hui-Wen according to the paper \"An \napproximate analogical reasoning approach based on similarity measures\" \npresented by Tursken I.B. and Zhong zhao is incorrect; (3) the mistakes in \ntheir paper are modified and then a calculation example of FMT is supplemented. \n</p>"}, "author": "Son-Il Kwak, Oh-Chol Gwon, Chung-Jin Kwak", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215430", "id": "tag:google.com,2005:reader/item/0000000346d96b15", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multi-focus Attention Network for Efficient Deep Reinforcement Learning. (arXiv:1712.04603v1 [cs.LG])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04603"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04603", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504de5785\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504de5785&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep reinforcement learning (DRL) has shown incredible performance in \nlearning various tasks to the human level. However, unlike human perception, \ncurrent DRL models connect the entire low-level sensory input to the \nstate-action values rather than exploiting the relationship between and among \nentities that constitute the sensory input. Because of this difference, DRL \nneeds vast amount of experience samples to learn. In this paper, we propose a \nMulti-focus Attention Network (MANet) which mimics human ability to spatially \nabstract the low-level sensory input into multiple entities and attend to them \nsimultaneously. The proposed method first divides the low-level input into \nseveral segments which we refer to as partial states. After this segmentation, \nparallel attention layers attend to the partial states relevant to solving the \ntask. Our model estimates state-action values using these attended partial \nstates. In our experiments, MANet attains highest scores with significantly \nless experience samples. Additionally, the model shows higher performance \ncompared to the Deep Q-network and the single attention model as benchmarks. \nFurthermore, we extend our model to attentive communication model for \nperforming multi-agent cooperative tasks. In multi-agent cooperative task \nexperiments, our model shows 20% faster learning than existing state-of-the-art \nmodel. \n</p>"}, "author": "Jinyoung Choi, Beom-Jin Lee, Byoung-Tak Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215429", "id": "tag:google.com,2005:reader/item/0000000346d96b19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Inverse Reinforcement Learning for Marketing. (arXiv:1712.04612v1 [q-fin.CP])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04612"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04612", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning customer preferences from an observed behaviour is an important \ntopic in the marketing literature. Structural models typically model \nforward-looking customers or firms as utility-maximizing agents whose utility \nis estimated using methods of Stochastic Optimal Control. We suggest an \nalternative approach to study dynamic consumer demand, based on Inverse \nReinforcement Learning (IRL). We develop a version of the Maximum Entropy IRL \nthat leads to a highly tractable model formulation that amounts to \nlow-dimensional convex optimization in the search for optimal model parameters. \nUsing simulations of consumer demand, we show that observational noise for \nidentical customers can be easily confused with an apparent consumer \nheterogeneity. \n</p>"}, "author": "Igor Halperin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215428", "id": "tag:google.com,2005:reader/item/0000000346d96b2b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Reasoning in Systems with Elements that Randomly Switch Characteristics. (arXiv:1712.04909v1 [cs.AI])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04909"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04909", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We examine the issue of stability of probability in reasoning about complex \nsystems with uncertainty in structure. Normally, propositions are viewed as \nprobability functions on an abstract random graph where it is implicitly \nassumed that the nodes of the graph have stable properties. But what if some of \nthe nodes change their characteristics? This is a situation that cannot be \ncovered by abstractions of either static or dynamic sets when these changes \ntake place at regular intervals. We propose the use of sets with elements that \nchange, and modular forms are proposed to account for one type of such change. \nAn expression for the dependence of the mean on the probability of the \nswitching elements has been determined. The system is also analyzed from the \nperspective of decision between different hypotheses. Such sets are likely to \nbe of use in complex system queries and in analysis of surveys. \n</p>"}, "author": "Subhash Kak", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215427", "id": "tag:google.com,2005:reader/item/0000000346d96b37", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Logical Induction. (arXiv:1609.03543v4 [cs.AI] UPDATED)", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1609.03543"}], "alternate": [{"href": "http://arxiv.org/abs/1609.03543", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a computable algorithm that assigns probabilities to every logical \nstatement in a given formal language, and refines those probabilities over \ntime. For instance, if the language is Peano arithmetic, it assigns \nprobabilities to all arithmetical statements, including claims about the twin \nprime conjecture, the outputs of long-running computations, and its own \nprobabilities. We show that our algorithm, an instance of what we call a \nlogical inductor, satisfies a number of intuitive desiderata, including: (1) it \nlearns to predict patterns of truth and falsehood in logical statements, often \nlong before having the resources to evaluate the statements, so long as the \npatterns can be written down in polynomial time; (2) it learns to use \nappropriate statistical summaries to predict sequences of statements whose \ntruth values appear pseudorandom; and (3) it learns to have accurate beliefs \nabout its own current beliefs, in a manner that avoids the standard paradoxes \nof self-reference. For example, if a given computer program only ever produces \noutputs in a certain range, a logical inductor learns this fact in a timely \nmanner; and if late digits in the decimal expansion of $\\pi$ are difficult to \npredict, then a logical inductor learns to assign $\\approx 10\\%$ probability to \n\"the $n$th digit of $\\pi$ is a 7\" for large $n$. Logical inductors also learn \nto trust their future beliefs more than their current beliefs, and their \nbeliefs are coherent in the limit (whenever $\\phi \\implies \\psi$, \n$\\mathbb{P}_\\infty(\\phi) \\le \\mathbb{P}_\\infty(\\psi)$, and so on); and logical \ninductors strictly dominate the universal semimeasure in the limit. \n</p> \n<p>These properties and many others all follow from a single logical induction \ncriterion, which is motivated by a series of stock trading analogies. Roughly \nspeaking, each logical sentence $\\phi$ is associated with a stock that is worth \n\\$1 per share if [...] \n</p>"}, "author": "Scott Garrabrant, Tsvi Benson-Tilsen, Andrew Critch, Nate Soares, Jessica Taylor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215426", "id": "tag:google.com,2005:reader/item/0000000346d96b3b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Scalable Co-Optimization of Morphology and Control in Embodied Machines. (arXiv:1706.06133v2 [cs.AI] UPDATED)", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.06133"}], "alternate": [{"href": "http://arxiv.org/abs/1706.06133", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Evolution sculpts both the body plans and nervous systems of agents together \nover time. In contrast, in AI and robotics, a robot's body plan is usually \ndesigned by hand, and control policies are then optimized for that fixed \ndesign. The task of simultaneously co-optimizing the morphology and controller \nof an embodied robot has remained a challenge. In psychology, the theory of \nembodied cognition posits that behavior arises from a close coupling between \nbody plan and sensorimotor control, which suggests why co-optimizing these two \nsubsystems is so difficult: most evolutionary changes to morphology tend to \nadversely impact sensorimotor control, leading to an overall decrease in \nbehavioral performance. Here, we further examine this hypothesis and \ndemonstrate a technique for \"morphological innovation protection\", which \ntemporarily reduces selection pressure on recently morphologically-changed \nindividuals, thus enabling evolution some time to \"readapt\" to the new \nmorphology with subsequent control policy mutations. We show the potential for \nthis method to avoid local optima and converge to similar highly fit \nmorphologies across widely varying initial conditions, while sustaining fitness \nimprovements further into optimization. While this technique is admittedly only \nthe first of many steps that must be taken to achieve scalable optimization of \nembodied machines, we hope that theoretical insight into the cause of \nevolutionary stagnation in current methods will help to enable the automation \nof robot design and behavioral training -- while simultaneously providing a \ntestbed to investigate the theory of embodied cognition. \n</p>"}, "author": "Nick Cheney, Josh Bongard, Vytas SunSpiral, Hod Lipson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215425", "id": "tag:google.com,2005:reader/item/0000000346d96b41", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Closed-Loop Policies for Operational Tests of Safety-Critical Systems. (arXiv:1707.08234v2 [cs.AI] UPDATED)", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.08234"}], "alternate": [{"href": "http://arxiv.org/abs/1707.08234", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Manufacturers of safety-critical systems must make the case that their \nproduct is sufficiently safe for public deployment. Much of this case often \nrelies upon critical event outcomes from real-world testing, requiring \nmanufacturers to be strategic about how they allocate testing resources in \norder to maximize their chances of demonstrating system safety. This work \nframes the partially observable and belief-dependent problem of test scheduling \nas a Markov decision process, which can be solved efficiently to yield \nclosed-loop manufacturer testing policies. By solving for policies over a wide \nrange of problem formulations, we are able to provide high-level guidance for \nmanufacturers and regulators on issues relating to the testing of \nsafety-critical systems. This guidance spans an array of topics, including \ncircumstances under which manufacturers should continue testing despite \nobserved incidents, when manufacturers should test aggressively, and when \nregulators should increase or reduce the real-world testing requirements for an \nautonomous vehicle. \n</p>"}, "author": "Jeremy Morton, Tim A. Wheeler, Mykel J. Kochenderfer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215424", "id": "tag:google.com,2005:reader/item/0000000346d96b4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning with Opponent-Learning Awareness. (arXiv:1709.04326v2 [cs.AI] UPDATED)", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.04326"}], "alternate": [{"href": "http://arxiv.org/abs/1709.04326", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Multi-agent settings are quickly gathering importance in machine learning. \nBeyond a plethora of recent work on deep multi-agent reinforcement learning, \nhierarchical reinforcement learning, generative adversarial networks and \ndecentralized optimization can all be seen as instances of this setting. \nHowever, the presence of multiple learning agents in these settings renders the \ntraining problem non-stationary and often leads to unstable training or \nundesired final results. We present Learning with Opponent-Learning Awareness \n(LOLA), a method that reasons about the anticipated learning of the other \nagents. The LOLA learning rule includes an additional term that accounts for \nthe impact of the agent's policy on the anticipated parameter update of the \nother agents. We show that the LOLA update rule can be efficiently calculated \nusing an extension of the likelihood ratio policy gradient update, making the \nmethod suitable for model-free RL. This method thus scales to large parameter \nand input spaces and nonlinear function approximators. Preliminary results show \nthat the encounter of two LOLA agents leads to the emergence of tit-for-tat and \ntherefore cooperation in the iterated prisoners' dilemma (IPD), while \nindependent learning does not. In this domain, LOLA also receives higher \npayouts compared to a naive learner, and is robust against exploitation by \nhigher order gradient-based methods. Applied to infinitely repeated matching \npennies, LOLA agents converge to the Nash equilibrium. In a round robin \ntournament we show that LOLA agents can successfully shape the learning of a \nrange of multi-agent learning algorithms from literature, resulting in the \nhighest average returns on the IPD. We also apply LOLA to a grid world task \nwith an embedded social dilemma using deep recurrent policies. Again, by \nconsidering the learning of the other agent, LOLA agents learn to cooperate out \nof selfish interests. \n</p>"}, "author": "Jakob N. Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, Igor Mordatch", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215422", "id": "tag:google.com,2005:reader/item/0000000346d96b64", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Backprop as Functor: A compositional perspective on supervised learning. (arXiv:1711.10455v2 [math.CT] UPDATED)", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.10455"}], "alternate": [{"href": "http://arxiv.org/abs/1711.10455", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A supervised learning algorithm searches over a set of functions $A \\to B$ \nparametrised by a space $P$ to find the best approximation to some ideal \nfunction $f\\colon A \\to B$. It does this by taking examples $(a,f(a)) \\in \nA\\times B$, and updating the parameter according to some rule. We define a \ncategory where these update rules may be composed, and show that gradient \ndescent---with respect to a fixed step size and an error function satisfying a \ncertain property---defines a monoidal functor from a category of parametrised \nfunctions to this category of update rules. This provides a structural \nperspective on backpropagation, as well as a broad generalisation of neural \nnetworks. \n</p>"}, "author": "Brendan Fong, David I. Spivak, R&#xe9;my Tuy&#xe9;ras", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215421", "id": "tag:google.com,2005:reader/item/0000000346d96b6a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Sparse Graphs for Prediction and Filtering of Multivariate Data Processes. (arXiv:1712.04542v1 [stat.ML])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04542"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04542", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We address the problem of prediction and filtering of multivariate data \nprocess using an underlying graph model. We develop a method that learns a \nsparse partial correlation graph in a tuning-free and computationally efficient \nmanner. Specifically, the graph structure is learned recursively without the \nneed for cross-validation or parameter tuning by building upon a \nhyperparameter-free framework. Experiments using real-world datasets show that \nthe proposed method offers significant performance gains in prediction and \nfiltering tasks, in comparison with the graphs frequently associated with these \ndatasets. \n</p>"}, "author": "Arun Venkitaraman, Dave Zachariah", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215420", "id": "tag:google.com,2005:reader/item/0000000346d96b71", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Mathematical Programming Approach for Integrated Multiple Linear Regression Subset Selection and Validation. (arXiv:1712.04543v1 [stat.ML])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04543"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04543", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Subset selection for multiple linear regression aims to construct a \nregression model that minimizes errors by selecting a small number of \nexplanatory variables. Once a model is built, various statistical tests and \ndiagnostics are conducted to validate the model and to determine whether \nregression assumptions are met. Most traditional approaches require human \ndecisions at this step, for example, the user adding or removing a variable \nuntil a satisfactory model is obtained. However, this trial-and-error strategy \ncannot guarantee that a subset that minimizes the errors while satisfying all \nregression assumptions will be found. In this paper, we propose a fully \nautomated model building procedure for multiple linear regression subset \nselection that integrates model building and validation based on mathematical \nprogramming. The proposed model minimizes mean squared errors while ensuring \nthat the majority of the important regression assumptions are met. When no \nsubset satisfies all of the considered regression assumptions, our model \nprovides an alternative subset that satisfies most of these assumptions. \nComputational results show that our model yields better solutions (i.e., \nsatisfying more regression assumptions) compared to benchmark models while \nmaintaining similar explanatory power. \n</p>"}, "author": "Seokhyun Chung, Young Woong Park, Taesu Cheong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215419", "id": "tag:google.com,2005:reader/item/0000000346d96b78", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Practical Bayesian optimization in the presence of outliers. (arXiv:1712.04567v1 [cs.LG])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04567"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04567", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504de5a8e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504de5a8e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Inference in the presence of outliers is an important field of research as \noutliers are ubiquitous and may arise across a variety of problems and domains. \nBayesian optimization is method that heavily relies on probabilistic inference. \nThis allows outstanding sample efficiency because the probabilistic machinery \nprovides a memory of the whole optimization process. However, that virtue \nbecomes a disadvantage when the memory is populated with outliers, inducing \nbias in the estimation. In this paper, we present an empirical evaluation of \nBayesian optimization methods in the presence of outliers. The empirical \nevidence shows that Bayesian optimization with robust regression often produces \nsuboptimal results. We then propose a new algorithm which combines robust \nregression (a Gaussian process with Student-t likelihood) with outlier \ndiagnostics to classify data points as outliers or inliers. By using an \nscheduler for the classification of outliers, our method is more efficient and \nhas better convergence over the standard robust regression. Furthermore, we \nshow that even in controlled situations with no expected outliers, our method \nis able to produce better results. \n</p>"}, "author": "Ruben Martinez-Cantin, Kevin Tee, Michael McCourt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215418", "id": "tag:google.com,2005:reader/item/0000000346d96b92", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Low-Rank Bandits. (arXiv:1712.04644v1 [cs.LG])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04644"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04644", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504e52ad5\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504e52ad5&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Many problems in computer vision and recommender systems involve low-rank \nmatrices. In this work, we study the problem of finding the maximum entry of a \nstochastic low-rank matrix from sequential observations. At each step, a \nlearning agent chooses pairs of row and column arms, and receives the noisy \nproduct of their latent values as a reward. The main challenge is that the \nlatent values are unobserved. We identify a class of non-negative matrices \nwhose maximum entry can be found statistically efficiently and propose an \nalgorithm for finding them, which we call LowRankElim. We derive a \n$\\DeclareMathOperator{\\poly}{poly} O((K + L) \\poly(d) \\Delta^{-1} \\log n)$ \nupper bound on its $n$-step regret, where $K$ is the number of rows, $L$ is the \nnumber of columns, $d$ is the rank of the matrix, and $\\Delta$ is the minimum \ngap. The bound depends on other problem-specific constants that clearly do not \ndepend $K L$. To the best of our knowledge, this is the first such result in \nthe literature. \n</p>"}, "author": "Branislav Kveton, Csaba Szepesvari, Anup Rao, Zheng Wen, Yasin Abbasi-Yadkori, S. Muthukrishnan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215417", "id": "tag:google.com,2005:reader/item/0000000346d96ba4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Variance reduction via empirical variance minimization: convergence and complexity. (arXiv:1712.04667v1 [math.NA])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04667"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04667", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we propose and study a generic variance reduction approach. The \nproposed method is based on minimization of the empirical variance over a \nsuitable class of zero mean control functionals. We present the corresponding \nconvergence analysis and analyze complexity. Finally some numerical results \nshowing efficiency of the proposed approach are presented. \n</p>"}, "author": "D. Belomestny, L. Iosipoi, N. Zhivotovskiy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215416", "id": "tag:google.com,2005:reader/item/0000000346d96bb4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stability Selection for Structured Variable Selection. (arXiv:1712.04688v1 [stat.ML])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04688"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04688", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In variable or graph selection problems, finding a right-sized model or \ncontrolling the number of false positives is notoriously difficult. Recently, a \nmeta-algorithm called Stability Selection was proposed that can provide \nreliable finite-sample control of the number of false positives. Its benefits \nwere demonstrated when used in conjunction with the lasso and orthogonal \nmatching pursuit algorithms. \n</p> \n<p>In this paper, we investigate the applicability of stability selection to \nstructured selection algorithms: the group lasso and the structured \ninput-output lasso. We find that using stability selection often increases the \npower of both algorithms, but that the presence of complex structure reduces \nthe reliability of error control under stability selection. We give strategies \nfor setting tuning parameters to obtain a good model size under stability \nselection, and highlight its strengths and weaknesses compared to competing \nmethods screen and clean and cross-validation. We give guidelines about when to \nuse which error control method. \n</p>"}, "author": "George Philipp, Seunghak Lee, Eric P. Xing", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215415", "id": "tag:google.com,2005:reader/item/0000000346d96bbd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Quantum Extension of Variational Bayes Inference. (arXiv:1712.04709v1 [stat.ML])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04709"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04709", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Variational Bayes (VB) inference is one of the most important algorithms in \nmachine learning and widely used in engineering and industry. However, VB is \nknown to suffer from the problem of local optima. In this Letter, we generalize \nVB by using quantum mechanics, and propose a new algorithm, which we call \nquantum annealing variational Bayes (QAVB) inference. We then show that QAVB \ndrastically improve the performance of VB by applying them to a clustering \nproblem described by a Gaussian mixture model. Finally, we discuss an intuitive \nunderstanding on how QAVB works well. \n</p>"}, "author": "Hideyuki Miyahara, Yuki Sughiyama", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215414", "id": "tag:google.com,2005:reader/item/0000000346d96bc4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Exponential convergence of testing error for stochastic gradient methods. (arXiv:1712.04755v1 [cs.LG])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04755"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04755", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider binary classification problems with positive definite kernels and \nsquare loss, and study the convergence rates of stochastic gradient methods. We \nshow that while the excess testing loss (squared loss) converges slowly to zero \nas the number of observations (and thus iterations) goes to infinity, the \ntesting error (classification error) converges exponentially fast if low-noise \nconditions are assumed. \n</p>"}, "author": "Loucas Pillaud-Vivien (SIERRA), Alessandro Rudi (SIERRA), Francis Bach (SIERRA)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215413", "id": "tag:google.com,2005:reader/item/0000000346d96bc9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Multiple testing for outlier detection in functional data. (arXiv:1712.04775v1 [stat.ML])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04775"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04775", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a novel procedure for outlier detection in functional data, in a \nsemi-supervised framework. As the data is functional, we consider the \ncoefficients obtained after projecting the observations onto orthonormal bases \n(wavelet, PCA). A multiple testing procedure based on the two-sample test is \ndefined in order to highlight the levels of the coefficients on which the \noutliers appear as significantly different to the normal data. The selected \ncoefficients are then called features for the outlier detection, on which we \ncompute the Local Outlier Factor to highlight the outliers. This procedure to \nselect the features is applied on simulated data that mimic the behaviour of \nspace telemetries, and compared with existing dimension reduction techniques. \n</p>"}, "author": "Cl&#xe9;mentine Barreyre, B&#xe9;atrice Laurent (IMT), Jean-Michel Loubes (IMT), Bertrand Cabon, Lo&#xef;c Boussouf", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215412", "id": "tag:google.com,2005:reader/item/0000000346d96bd2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments. (arXiv:1712.04802v1 [stat.ML])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04802"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04802", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose strategies to estimate and make inference on key features of \nheterogeneous effects in randomized experiments. These key features include \nbest linear predictors of the effects using machine learning proxies, average \neffects sorted by impact groups, and average characteristics of most and least \nimpacted units. The approach is valid in high dimensional settings, where the \neffects are proxied by machine learning methods. We post-process these proxies \ninto the estimates of the key features. Our approach is agnostic about the \nproperties of the machine learning estimators used to produce proxies, and it \ncompletely avoids making any strong assumption. Estimation and inference relies \non repeated data splitting to avoid overfitting and achieve validity. Our \nvariational inference method is shown to be uniformly valid and quantifies the \nuncertainty coming from both parameter estimation and data splitting. In \nessence, we take medians of p-values and medians of confidence intervals, \nresulting from many different data splits, and then adjust their nominal level \nto guarantee uniform validity. The inference method could be of substantial \nindependent interest in many machine learning applications. Empirical \napplications illustrate the use of the approach. \n</p>"}, "author": "Victor Chernozhukov, Mert Demirer, Esther Duflo, Ivan Fernandez-Val", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215411", "id": "tag:google.com,2005:reader/item/0000000346d96bda", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Ballpark Crowdsourcing: The Wisdom of Rough Group Comparisons. (arXiv:1712.04828v1 [stat.ML])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04828"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04828", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Crowdsourcing has become a popular method for collecting labeled training \ndata. However, in many practical scenarios traditional labeling can be \ndifficult for crowdworkers (for example, if the data is high-dimensional or \nunintuitive, or the labels are continuous). \n</p> \n<p>In this work, we develop a novel model for crowdsourcing that can complement \nstandard practices by exploiting people's intuitions about groups and relations \nbetween them. We employ a recent machine learning setting, called Ballpark \nLearning, that can estimate individual labels given only coarse, aggregated \nsignal over groups of data points. To address the important case of continuous \nlabels, we extend the Ballpark setting (which focused on classification) to \nregression problems. We formulate the problem as a convex optimization problem \nand propose fast, simple methods with an innate robustness to outliers. \n</p> \n<p>We evaluate our methods on real-world datasets, demonstrating how useful \nconstraints about groups can be harnessed from a crowd of non-experts. Our \nmethods can rival supervised models trained on many true labels, and can obtain \nconsiderably better results from the crowd than a standard label-collection \nprocess (for a lower price). By collecting rough guesses on groups of instances \nand using machine learning to infer the individual labels, our lightweight \nframework is able to address core crowdsourcing challenges and train machine \nlearning models in a cost-effective way. \n</p>"}, "author": "Tom Hope, Dafna Shahaf", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215410", "id": "tag:google.com,2005:reader/item/0000000346d96be6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "FFT-Based Deep Learning Deployment in Embedded Systems. (arXiv:1712.04910v1 [cs.LG])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04910"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04910", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning has delivered its powerfulness in many application domains, \nespecially in image and speech recognition. As the backbone of deep learning, \ndeep neural networks (DNNs) consist of multiple layers of various types with \nhundreds to thousands of neurons. Embedded platforms are now becoming essential \nfor deep learning deployment due to their portability, versatility, and energy \nefficiency. The large model size of DNNs, while providing excellent accuracy, \nalso burdens the embedded platforms with intensive computation and storage. \nResearchers have investigated on reducing DNN model size with negligible \naccuracy loss. This work proposes a Fast Fourier Transform (FFT)-based DNN \ntraining and inference model suitable for embedded platforms with reduced \nasymptotic complexity of both computation and storage, making our approach \ndistinguished from existing approaches. We develop the training and inference \nalgorithms based on FFT as the computing kernel and deploy the FFT-based \ninference model on embedded platforms achieving extraordinary processing speed. \n</p>"}, "author": "Sheng Lin, Ning Liu, Mahdi Nazemi, Hongjia Li, Caiwen Ding, Yanzhi Wang, Massoud Pedram", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215409", "id": "tag:google.com,2005:reader/item/0000000346d96be8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Objectives for Treatment Effect Estimation. (arXiv:1712.04912v1 [stat.ML])", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04912"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04912", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504e52cfd\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504e52cfd&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We develop a general class of two-step algorithms for heterogeneous treatment \neffect estimation in observational studies. We first estimate marginal effects \nand treatment propensities to form an objective function that isolates the \nheterogeneous treatment effects, and then optimize the learned objective. This \napproach has several advantages over existing methods. From a practical \nperspective, our method is very flexible and easy to use: In both steps, we can \nuse any method of our choice, e.g., penalized regression, a deep net, or \nboosting; moreover, these methods can be fine-tuned by cross-validating on the \nlearned objective. Meanwhile, in the case of penalized kernel regression, we \nshow that our method has a quasi-oracle property, whereby even if our pilot \nestimates for marginal effects and treatment propensities are not particularly \naccurate, we achieve the same regret bounds as an oracle who has a-priori \nknowledge of these nuisance components. We implement variants of our method \nbased on both penalized regression and convolutional neural networks, and find \npromising performance relative to existing baselines. \n</p>"}, "author": "Xinkun Nie, Stefan Wager", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215408", "id": "tag:google.com,2005:reader/item/0000000346d96bea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Double/Debiased Machine Learning for Treatment and Causal Parameters. (arXiv:1608.00060v6 [stat.ML] UPDATED)", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1608.00060"}], "alternate": [{"href": "http://arxiv.org/abs/1608.00060", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Most modern supervised statistical/machine learning (ML) methods are \nexplicitly designed to solve prediction problems very well. Achieving this goal \ndoes not imply that these methods automatically deliver good estimators of \ncausal parameters. Examples of such parameters include individual regression \ncoefficients, average treatment effects, average lifts, and demand or supply \nelasticities. In fact, estimates of such causal parameters obtained via naively \nplugging ML estimators into estimating equations for such parameters can behave \nvery poorly due to the regularization bias. Fortunately, this regularization \nbias can be removed by solving auxiliary prediction problems via ML tools. \nSpecifically, we can form an orthogonal score for the target low-dimensional \nparameter by combining auxiliary and main ML predictions. The score is then \nused to build a de-biased estimator of the target parameter which typically \nwill converge at the fastest possible 1/root(n) rate and be approximately \nunbiased and normal, and from which valid confidence intervals for these \nparameters of interest may be constructed. The resulting method thus could be \ncalled a \"double ML\" method because it relies on estimating primary and \nauxiliary predictive models. In order to avoid overfitting, our construction \nalso makes use of the K-fold sample splitting, which we call cross-fitting. \nThis allows us to use a very broad set of ML predictive methods in solving the \nauxiliary and main prediction problems, such as random forest, lasso, ridge, \ndeep neural nets, boosted trees, as well as various hybrids and aggregators of \nthese methods. \n</p>"}, "author": "Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, James Robins", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215407", "id": "tag:google.com,2005:reader/item/0000000346d96bf1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning of state-space models with highly informative observations: a tempered Sequential Monte Carlo solution. (arXiv:1702.01618v2 [stat.CO] UPDATED)", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1702.01618"}], "alternate": [{"href": "http://arxiv.org/abs/1702.01618", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Probabilistic (or Bayesian) modeling and learning offers interesting \npossibilities for systematic representation of uncertainty using probability \ntheory. However, probabilistic learning often leads to computationally \nchallenging problems. Some problems of this type that were previously \nintractable can now be solved on standard personal computers thanks to recent \nadvances in Monte Carlo methods. In particular, for learning of unknown \nparameters in nonlinear state-space models, methods based on the particle \nfilter (a Monte Carlo method) have proven very useful. A notoriously \nchallenging problem, however, still occurs when the observations in the \nstate-space model are highly informative, i.e. when there is very little or no \nmeasurement noise present, relative to the amount of process noise. The \nparticle filter will then struggle in estimating one of the basic components \nfor probabilistic learning, namely the likelihood $p($data$|$parameters$)$. To \nthis end we suggest an algorithm which initially assumes that there is \nsubstantial amount of artificial measurement noise present. The variance of \nthis noise is sequentially decreased in an adaptive fashion such that we, in \nthe end, recover the original problem or possibly a very close approximation of \nit. The main component in our algorithm is a sequential Monte Carlo (SMC) \nsampler, which gives our proposed method a clear resemblance to the SMC^2 \nmethod. Another natural link is also made to the ideas underlying the \napproximate Bayesian computation (ABC). We illustrate it with numerical \nexamples, and in particular show promising results for a challenging \nWiener-Hammerstein benchmark problem. \n</p>"}, "author": "Andreas Svensson, Thomas B. Sch&#xf6;n, Fredrik Lindsten", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215406", "id": "tag:google.com,2005:reader/item/0000000346d96bf6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On Quadratic Convergence of DC Proximal Newton Algorithm for Nonconvex Sparse Learning in High Dimensions. (arXiv:1706.06066v2 [stat.ML] UPDATED)", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.06066"}], "alternate": [{"href": "http://arxiv.org/abs/1706.06066", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a DC proximal Newton algorithm for solving nonconvex regularized \nsparse learning problems in high dimensions. Our proposed algorithm integrates \nthe proximal Newton algorithm with multi-stage convex relaxation based on the \ndifference of convex (DC) programming, and enjoys both strong computational and \nstatistical guarantees. Specifically, by leveraging a sophisticated \ncharacterization of sparse modeling structures/assumptions (i.e., local \nrestricted strong convexity and Hessian smoothness), we prove that within each \nstage of convex relaxation, our proposed algorithm achieves (local) quadratic \nconvergence, and eventually obtains a sparse approximate local optimum with \noptimal statistical properties after only a few convex relaxations. Numerical \nexperiments are provided to support our theory. \n</p>"}, "author": "Xingguo Li, Lin F. Yang, Jason Ge, Jarvis Haupt, Tong Zhang, Tuo Zhao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215405", "id": "tag:google.com,2005:reader/item/0000000346d96c23", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "ClustGeo: an R package for hierarchical clustering with spatial constraints. (arXiv:1707.03897v2 [stat.CO] UPDATED)", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.03897"}], "alternate": [{"href": "http://arxiv.org/abs/1707.03897", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a Ward-like hierarchical clustering algorithm \nincluding spatial/geographical constraints. Two dissimilarity matrices $D_0$ \nand $D_1$ are inputted, along with a mixing parameter $\\alpha \\in [0,1]$. The \ndissimilarities can be non-Euclidean and the weights of the observations can be \nnon-uniform. The first matrix gives the dissimilarities in the \"feature space\" \nand the second matrix gives the dissimilarities in the \"constraint space\". The \ncriterion minimized at each stage is a convex combination of the homogeneity \ncriterion calculated with $D_0$ and the homogeneity criterion calculated with \n$D_1$. The idea is then to determine a value of $\\alpha$ which increases the \nspatial contiguity without deteriorating too much the quality of the solution \nbased on the variables of interest i.e. those of the feature space. This \nprocedure is illustrated on a real dataset using the R package ClustGeo. \n</p>"}, "author": "Marie Chavent, Vanessa Kuentz-Simonet, Amaury Labenne, J&#xe9;r&#xf4;me Saracco", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215404", "id": "tag:google.com,2005:reader/item/0000000346d96c42", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Provably Accurate Double-Sparse Coding. (arXiv:1711.03638v2 [stat.ML] UPDATED)", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03638"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03638", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Sparse coding is a crucial subroutine in algorithms for various signal \nprocessing, deep learning, and other machine learning applications. The central \ngoal is to learn an overcomplete dictionary that can sparsely represent a given \ninput dataset. However, a key challenge is that storage, transmission, and \nprocessing of the learned dictionary can be untenably high if the data \ndimension is high. In this paper, we consider the double-sparsity model \nintroduced by Rubinstein et al. (2010b) where the dictionary itself is the \nproduct of a fixed, known basis and a data-adaptive sparse component. First, we \nintroduce a simple algorithm for double-sparse coding that can be amenable to \nefficient implementation via neural architectures. Second, we theoretically \nanalyze its performance and demonstrate asymptotic sample complexity and \nrunning time benefits over existing (provable) approaches for sparse coding. To \nour knowledge, our work introduces the first computationally efficient \nalgorithm for double-sparse coding that enjoys rigorous statistical guarantees. \nFinally, we support our analysis via several numerical experiments on simulated \ndata, confirming that our method can indeed be useful in problem sizes \nencountered in practical applications. \n</p>"}, "author": "Thanh V. Nguyen, Raymond K. W. Wong, Chinmay Hegde", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215403", "id": "tag:google.com,2005:reader/item/0000000346d96c52", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Machine vs Machine: Minimax-Optimal Defense Against Adversarial Examples. (arXiv:1711.04368v2 [cs.LG] UPDATED)", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04368"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04368", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recently, researchers have discovered that the state-of-the-art object \nclassifiers can be fooled easily by small perturbations in the input \nunnoticeable to human eyes. It is known that an attacker can generate strong \nadversarial examples if she knows the classifier parameters. Conversely, a \ndefender can robustify the classifier by retraining if she has the adversarial \nexamples. The cat-and-mouse game nature of attacks and defenses raises the \nquestion of the presence of equilibria in the dynamics. In this paper, we \npresent a neural-network based attack class to approximate a larger but \nintractable class of attacks, and formulate the attacker-defender interaction \nas a zero-sum leader-follower game. We present sensitivity-penalized \noptimization algorithms to find minimax solutions, which are the best \nworst-case defenses against whitebox attacks. Advantages of the learning-based \nattacks and defenses compared to gradient-based attacks and defenses are \ndemonstrated with MNIST and CIFAR-10. \n</p>"}, "author": "Jihun Hamm", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513228697215", "timestampUsec": "1513228697215401", "id": "tag:google.com,2005:reader/item/0000000346d96c57", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Comparing deep neural networks against humans: object recognition when the signal gets weaker. (arXiv:1706.06969v1 [cs.CV] CROSS LISTED)", "published": 1513228697, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.06969"}], "alternate": [{"href": "http://arxiv.org/abs/1706.06969", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Human visual object recognition is typically rapid and seemingly effortless, \nas well as largely independent of viewpoint and object orientation. Until very \nrecently, animate visual systems were the only ones capable of this remarkable \ncomputational feat. This has changed with the rise of a class of computer \nvision algorithms called deep neural networks (DNNs) that achieve human-level \nclassification performance on object recognition tasks. Furthermore, a growing \nnumber of studies report similarities in the way DNNs and the human visual \nsystem process objects, suggesting that current DNNs may be good models of \nhuman visual object recognition. Yet there clearly exist important \narchitectural and processing differences between state-of-the-art DNNs and the \nprimate visual system. The potential behavioural consequences of these \ndifferences are not well understood. We aim to address this issue by comparing \nhuman and DNN generalisation abilities towards image degradations. We find the \nhuman visual system to be more robust to image manipulations like contrast \nreduction, additive noise or novel eidolon-distortions. In addition, we find \nprogressively diverging classification error-patterns between man and DNNs when \nthe signal gets weaker, indicating that there may still be marked differences \nin the way humans and current DNNs perform visual object recognition. We \nenvision that our findings as well as our carefully measured and freely \navailable behavioural datasets provide a new useful benchmark for the computer \nvision community to improve the robustness of DNNs and a motivation for \nneuroscientists to search for mechanisms in the brain that could facilitate \nthis robustness. \n</p>"}, "author": "Robert Geirhos, David H. J. Janssen, Heiko H. Sch&#xfc;tt, Jonas Rauber, Matthias Bethge, Felix A. Wichmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513142281080", "timestampUsec": "1513142281080184", "id": "tag:google.com,2005:reader/item/000000034613311b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Learning Robust Dialog Policies in Noisy Environments. (arXiv:1712.04034v1 [cs.CL])", "published": 1513142281, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04034"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04034", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Modern virtual personal assistants provide a convenient interface for \ncompleting daily tasks via voice commands. An important consideration for these \nassistants is the ability to recover from automatic speech recognition (ASR) \nand natural language understanding (NLU) errors. In this paper, we focus on \nlearning robust dialog policies to recover from these errors. To this end, we \ndevelop a user simulator which interacts with the assistant through voice \ncommands in realistic scenarios with noisy audio, and use it to learn dialog \npolicies through deep reinforcement learning. We show that dialogs generated by \nour simulator are indistinguishable from human generated dialogs, as determined \nby human evaluators. Furthermore, preliminary experimental results show that \nthe learned policies in noisy environments achieve the same execution success \nrate with fewer dialog turns compared to fixed rule-based policies. \n</p>"}, "author": "Maryam Fazel-Zarandi, Shang-Wen Li, Jin Cao, Jared Casale, Peter Henderson, David Whitney, Alborz Geramifard", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml", "title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490941", "id": "tag:google.com,2005:reader/item/0000000346127d8a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neural Component Analysis for Fault Detection. (arXiv:1712.04118v1 [cs.LG])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04118"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04118", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Principal component analysis (PCA) is largely adopted for chemical process \nmonitoring and numerous PCA-based systems have been developed to solve various \nfault detection and diagnosis problems. Since PCA-based methods assume that the \nmonitored process is linear, nonlinear PCA models, such as autoencoder models \nand kernel principal component analysis (KPCA), has been proposed and applied \nto nonlinear process monitoring. However, KPCA-based methods need to perform \neigen-decomposition (ED) on the kernel Gram matrix whose dimensions depend on \nthe number of training data. Moreover, prefixed kernel parameters cannot be \nmost effective for different faults which may need different parameters to \nmaximize their respective detection performances. Autoencoder models lack the \nconsideration of orthogonal constraints which is crucial for PCA-based \nalgorithms. To address these problems, this paper proposes a novel nonlinear \nmethod, called neural component analysis (NCA), which intends to train a \nfeedforward neural work with orthogonal constraints such as those used in PCA. \nNCA can adaptively learn its parameters through backpropagation and the \ndimensionality of the nonlinear features has no relationship with the number of \ntraining samples. Extensive experimental results on the Tennessee Eastman (TE) \nbenchmark process show the superiority of NCA in terms of missed detection rate \n(MDR) and false alarm rate (FAR). The source code of NCA can be found in \nhttps://github.com/haitaozhao/Neural-Component-Analysis.git. \n</p>"}, "author": "Haitao Zhao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490940", "id": "tag:google.com,2005:reader/item/0000000346127d8e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Interpretable Policies for Reinforcement Learning by Genetic Programming. (arXiv:1712.04170v1 [cs.AI])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04170"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04170", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504e52f48\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504e52f48&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The search for interpretable reinforcement learning policies is of high \nacademic and industrial interest. Especially for industrial systems, domain \nexperts are more likely to deploy autonomously learned controllers if they are \nunderstandable and convenient to evaluate. Basic algebraic equations are \nsupposed to meet these requirements, as long as they are restricted to an \nadequate complexity. Here we introduce the genetic programming for \nreinforcement learning (GPRL) approach based on model-based batch reinforcement \nlearning and genetic programming, which autonomously learns policy equations \nfrom pre-existing default state-action trajectory samples. GPRL is compared to \na straight-forward method which utilizes genetic programming for symbolic \nregression, yielding policies imitating an existing well-performing, but \nnon-interpretable policy. Experiments on three reinforcement learning \nbenchmarks, i.e., mountain car, cart-pole balancing, and industrial benchmark, \ndemonstrate the superiority of our GPRL approach compared to the symbolic \nregression method. GPRL is capable of producing well-performing interpretable \nreinforcement learning policies from pre-existing default trajectory data. \n</p>"}, "author": "Daniel Hein, Steffen Udluft, Thomas A. Runkler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490939", "id": "tag:google.com,2005:reader/item/0000000346127d98", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Backpropagation generalized for output derivatives. (arXiv:1712.04185v1 [cs.NE])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04185"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04185", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504eb5f75\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504eb5f75&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Backpropagation algorithm is the cornerstone for neural network analysis. \nPaper extends it for training any derivatives of neural network's output with \nrespect to its input. By the dint of it feedforward networks can be used to \nsolve or verify solutions of partial or simple, linear or nonlinear \ndifferential equations. This method vastly differs from traditional ones like \nfinite differences on a mesh. It contains no approximations, but rather an \nexact form of differential operators. Algorithm is built to train a feed \nforward network with any number of hidden layers and any kind of sufficiently \nsmooth activation functions. It's presented in a form of matrix-vector products \nso highly parallel implementation is readily possible. First part derives the \nmethod for 2D case with first and second order derivatives, second part extends \nit to N-dimensional case with any derivatives. All necessary expressions for \nusing this method to solve most applied PDE can be found in Appendix D. \n</p>"}, "author": "V.I. Avrutskiy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490938", "id": "tag:google.com,2005:reader/item/0000000346127d9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Concept Formation and Dynamics of Repeated Inference in Deep Generative Models. (arXiv:1712.04195v1 [stat.ML])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04195"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04195", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep generative models are reported to be useful in broad applications \nincluding image generation. Repeated inference between data space and latent \nspace in these models can denoise cluttered images and improve the quality of \ninferred results. However, previous studies only qualitatively evaluated image \noutputs in data space, and the mechanism behind the inference has not been \ninvestigated. The purpose of the current study is to numerically analyze \nchanges in activity patterns of neurons in the latent space of a deep \ngenerative model called a \"variational auto-encoder\" (VAE). What kinds of \ninference dynamics the VAE demonstrates when noise is added to the input data \nare identified. The VAE embeds a dataset with clear cluster structures in the \nlatent space and the center of each cluster of multiple correlated data points \n(memories) is referred as the concept. Our study demonstrated that transient \ndynamics of inference first approaches a concept, and then moves close to a \nmemory. Moreover, the VAE revealed that the inference dynamics approaches a \nmore abstract concept to the extent that the uncertainty of input data \nincreases due to noise. It was demonstrated that by increasing the number of \nthe latent variables, the trend of the inference dynamics to approach a concept \ncan be enhanced, and the generalization ability of the VAE can be improved. \n</p>"}, "author": "Yoshihiro Nagano, Ryo Karakida, Masato Okada", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490937", "id": "tag:google.com,2005:reader/item/0000000346127da1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models. (arXiv:1712.04248v1 [stat.ML])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04248"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04248", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many machine learning algorithms are vulnerable to almost imperceptible \nperturbations of their inputs. So far it was unclear how much risk adversarial \nperturbations carry for the safety of real-world machine learning applications \nbecause most methods used to generate such perturbations rely either on \ndetailed model information (gradient-based attacks) or on confidence scores \nsuch as class probabilities (score-based attacks), neither of which are \navailable in most real-world scenarios. In many such cases one currently needs \nto retreat to transfer-based attacks which rely on cumbersome substitute \nmodels, need access to the training data and can be defended against. Here we \nemphasise the importance of attacks which solely rely on the final model \ndecision. Such decision-based attacks are (1) applicable to real-world \nblack-box models such as autonomous cars, (2) need less knowledge and are \neasier to apply than transfer-based attacks and (3) are more robust to simple \ndefences than gradient- or score-based attacks. Previous attacks in this \ncategory were limited to simple models or simple datasets. Here we introduce \nthe Boundary Attack, a decision-based attack that starts from a large \nadversarial perturbation and then seeks to reduce the perturbation while \nstaying adversarial. The attack is conceptually simple, requires close to no \nhyperparameter tuning, does not rely on substitute models and is competitive \nwith the best gradient-based attacks in standard computer vision tasks like \nImageNet. We apply the attack on two black-box algorithms from Clarifai.com. \nThe Boundary Attack in particular and the class of decision-based attacks in \ngeneral open new avenues to study the robustness of machine learning models and \nraise new questions regarding the safety of deployed machine learning systems. \nAn implementation of the attack is available as part of Foolbox at \nhttps://github.com/bethgelab/foolbox . \n</p>"}, "author": "Wieland Brendel, Jonas Rauber, Matthias Bethge", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490936", "id": "tag:google.com,2005:reader/item/0000000346127da4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Robustness, Evolvability and Phenotypic Complexity: Insights from Evolving Digital Circuits. (arXiv:1712.04254v1 [cs.NE])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04254"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04254", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We show how the characteristics of the evolutionary algorithm influence the \nevolvability of candidate solutions, i.e. the propensity of evolving \nindividuals to generate better solutions as a result of genetic variation. More \nspecifically, (1+{\\lambda}) evolutionary strategies largely outperform \n({\\mu}+1) evolutionary strategies in the context of the evolution of digital \ncircuits --- a domain characterized by a high level of neutrality. This \ndifference is due to the fact that the competition for robustness to mutations \namong the circuits evolved with ({\\mu}+1) evolutionary strategies leads to the \nselection of phenotypically simple but low evolvable circuits. These circuits \nachieve robustness by minimizing the number of functional genes rather than by \nrelying on redundancy or degeneracy to buffer the effects of mutations. The \nanalysis of these factors enabled us to design a new evolutionary algorithm, \nnamed Parallel Stochastic Hill Climber (PSHC), which outperforms the other two \nmethods considered. \n</p>"}, "author": "Nicola Milano, Paolo Pagliuca, Stefano Nolfi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490935", "id": "tag:google.com,2005:reader/item/0000000346127dbb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation. (arXiv:1703.09902v2 [cs.CL] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.09902"}], "alternate": [{"href": "http://arxiv.org/abs/1703.09902", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper surveys the current state of the art in Natural Language \nGeneration (NLG), defined as the task of generating text or speech from \nnon-linguistic input. A survey of NLG is timely in view of the changes that the \nfield has undergone over the past decade or so, especially in relation to new \n(usually data-driven) methods, as well as new applications of NLG technology. \nThis survey therefore aims to (a) give an up-to-date synthesis of research on \nthe core tasks in NLG and the architectures adopted in which such tasks are \norganised; (b) highlight a number of relatively recent research topics that \nhave arisen partly as a result of growing synergies between NLG and other areas \nof artificial intelligence; (c) draw attention to the challenges in NLG \nevaluation, relating them to similar challenges faced in other areas of Natural \nLanguage Processing, with an emphasis on different evaluation methods and the \nrelationships between them. \n</p>"}, "author": "Albert Gatt, Emiel Krahmer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490934", "id": "tag:google.com,2005:reader/item/0000000346127de5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Super-polynomial and exponential improvements for quantum-enhanced reinforcement learning. (arXiv:1710.11160v2 [quant-ph] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11160"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11160", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent work on quantum machine learning has demonstrated that quantum \ncomputers can offer dramatic improvements over classical devices for data \nmining, prediction and classification. However, less is known about the \nadvantages using quantum computers may bring in the more general setting of \nreinforcement learning, where learning is achieved via interaction with a task \nenvironment that provides occasional rewards. Reinforcement learning can \nincorporate data-analysis-oriented learning settings as special cases, but also \nincludes more complex situations where, e.g., reinforcing feedback is delayed. \nIn a few recent works, Grover-type amplification has been utilized to construct \nquantum agents that achieve up-to-quadratic improvements in learning \nefficiency. These encouraging results have left open the key question of \nwhether super-polynomial improvements in learning times are possible for \ngenuine reinforcement learning problems, that is problems that go beyond the \nother more restricted learning paradigms. In this work, we provide a family of \nsuch genuine reinforcement learning tasks. We construct quantum-enhanced \nlearners which learn super-polynomially, and even exponentially faster than any \nclassical reinforcement learning model, and we discuss the potential impact our \nresults may have on future technologies. \n</p>"}, "author": "Vedran Dunjko, Yi-Kai Liu, Xingyao Wu, Jacob M. Taylor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490933", "id": "tag:google.com,2005:reader/item/0000000346127df7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning for Real-time Gravitational Wave Detection and Parameter Estimation with LIGO Data. (arXiv:1711.07966v2 [gr-qc] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07966"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07966", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The recent Nobel-prize-winning detections of gravitational waves from merging \nblack holes and the subsequent detection of the collision of two neutron stars \nin coincidence with electromagnetic observations have inaugurated a new era of \nmultimessenger astrophysics. To enhance the scope of this emergent science, we \nproposed the use of deep convolutional neural networks for the detection and \ncharacterization of gravitational wave signals in real-time. This method, Deep \nFiltering, was initially demonstrated using simulated LIGO noise. In this \narticle, we present the extension of Deep Filtering using real data from the \nfirst observing run of LIGO, for both detection and parameter estimation of \ngravitational waves from binary black hole mergers with continuous data streams \nfrom multiple LIGO detectors. We show for the first time that machine learning \ncan detect and estimate the true parameters of a real GW event observed by \nLIGO. Our comparisons show that Deep Filtering is far more computationally \nefficient than matched-filtering, while retaining similar sensitivity and lower \nerrors, allowing real-time processing of weak time-series signals in \nnon-stationary non-Gaussian noise, with minimal resources, and also enables the \ndetection of new classes of gravitational wave sources that may go unnoticed \nwith existing detection algorithms. This approach is uniquely suited to enable \ncoincident detection campaigns of gravitational waves and their multimessenger \ncounterparts in real-time. \n</p>"}, "author": "Daniel George, E. A. Huerta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490931", "id": "tag:google.com,2005:reader/item/0000000346127e17", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations. (arXiv:1712.02779v2 [cs.LG] CROSS LISTED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02779"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02779", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent work has shown that neural network-based vision classifiers exhibit a \nsignificant vulnerability to misclassifications caused by imperceptible but \nadversarial perturbations of their inputs. These perturbations, however, are \npurely pixel-wise and built out of loss function gradients of either the \nattacked model or its surrogate. As a result, they tend to be contrived and \nlook pretty artificial. This might suggest that such vulnerability to slight \ninput perturbations can only arise in a truly adversarial setting and thus is \nunlikely to be an issue in more \"natural\" contexts. \n</p> \n<p>In this paper, we provide evidence that such belief might be incorrect. We \ndemonstrate that significantly simpler, and more likely to occur naturally, \ntransformations of the input - namely, rotations and translations alone, \nsuffice to significantly degrade the classification performance of neural \nnetwork-based vision models across a spectrum of datasets. This remains to be \nthe case even when these models are trained using appropriate data \naugmentation. Finding such \"fooling\" transformations does not require having \nany special access to the model - just trying out a small number of random \nrotation and translation combinations already has a significant effect. These \nfindings suggest that our current neural network-based vision models might not \nbe as reliable as we tend to assume. \n</p> \n<p>Finally, we consider a new class of perturbations that combines rotations and \ntranslations with the standard pixel-wise attacks. We observe that these two \ntypes of input transformations are, in a sense, orthogonal to each other. Their \neffect on the performance of the model seems to be additive, while robustness \nto one type does not seem to affect the robustness to the other type. This \nsuggests that this combined class of transformations is a more complete notion \nof similarity in the context of adversarial robustness of vision models. \n</p>"}, "author": "Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, Aleksander Madry", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490930", "id": "tag:google.com,2005:reader/item/0000000346127e35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Investigation on How Data Volume Affects Transfer Learning Performances in Business Applications. (arXiv:1712.04008v1 [cs.CV])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04008"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04008", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Transfer Learning helps to build a system to recognize and apply knowledge \nand experience learned in previous tasks (source task) to new tasks or new \ndomains (target task), which share some commonality. The two important factors \nthat impact the performance of transfer learning models are: (a) the size of \nthe target dataset and (b) the similarity in distribution between source and \ntarget domains. Thus far there has been little investigation into just how \nimportant these factors are. In this paper, we investigated the impact of \ntarget dataset size and source/target domain similarity on model performance \nthrough a series of experiments. We found that more data is always beneficial, \nand that model performance improved linearly with the log of data size, until \nwe were out of data. As source/target domains differ, more data is required and \nfine tuning will render better performance than feature extraction. When \nsource/target domains are similar and data size is small, fine tuning and \nfeature extraction renders equivalent performance. We hope that our study \ninspires further work in transfer learning, which continues to be a very \nimportant technique for developing practical machine learning applications in \nbusiness domains. \n</p>"}, "author": "Michael Bernico, Yuntao Li, Dingchao Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490929", "id": "tag:google.com,2005:reader/item/0000000346127e3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Detecting Qualia in Natural and Artificial Agents. (arXiv:1712.04020v1 [cs.AI])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04020"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04020", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504eb61a6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504eb61a6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The Hard Problem of consciousness has been dismissed as an illusion. By \nshowing that computers are capable of experiencing, we show that they are at \nleast rudimentarily conscious with potential to eventually reach \nsuperconsciousness. The main contribution of the paper is a test for confirming \ncertain subjective experiences in a tested agent. We follow with analysis of \nbenefits and problems with conscious machines and implications of such \ncapability on future of computing, machine rights and artificial intelligence \nsafety. \n</p>"}, "author": "Roman V. Yampolskiy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490928", "id": "tag:google.com,2005:reader/item/0000000346127e44", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Eigenoption-Critic Framework. (arXiv:1712.04065v1 [cs.AI])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04065"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04065", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Eigenoptions (EOs) have been recently introduced as a promising idea for \ngenerating a diverse set of options through the graph Laplacian, having been \nshown to allow efficient exploration. Despite its initial promising results, a \ncouple of issues in current algorithms limit its application, namely: (1) EO \nmethods require two separate steps (eigenoption discovery and reward \nmaximization) to learn a control policy, which can incur a significant amount \nof storage and computation; (2) EOs are only defined for problems with discrete \nstate-spaces and; (3) it is not easy to take the environment's reward function \ninto consideration when discovering EOs. To addresses these issues, we \nintroduce an algorithm termed eigenoption-critic (EOC) based on the \nOption-critic (OC) framework [Bacon17], a general hierarchical reinforcement \nlearning (RL) algorithm that allows learning the intra-option policies \nsimultaneously with the policy over options. We also propose a generalization \nof EOC to problems with continuous state-spaces through the Nystr\\\"om \napproximation. EOC can also be seen as extending OC to nonstationary settings, \nwhere the discovered options are not tailored for a single task. \n</p>"}, "author": "Miao Liu, Marlos C. Machado, Gerald Tesauro, Murray Campbell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490927", "id": "tag:google.com,2005:reader/item/0000000346127e49", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "In a Nutshell: Sequential Parameter Optimization. (arXiv:1712.04076v1 [cs.MS])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04076"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04076", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The performance of optimization algorithms relies crucially on their \nparameterizations. Finding good parameter settings is called algorithm tuning. \nUsing a simple simulated annealing algorithm, we will demonstrate how \noptimization algorithms can be tuned using the sequential parameter \noptimization toolbox (SPOT). SPOT provides several tools for automated and \ninteractive tuning. The underling concepts of the SPOT approach are explained. \nThis includes key techniques such as exploratory fitness landscape analysis and \nresponse surface methodology. Many examples illustrate how SPOT can be used for \nunderstanding the performance of algorithms and gaining insight into \nalgorithm's behavior. Furthermore, we demonstrate how SPOT can be used as an \noptimizer and how a sophisticated ensemble approach is able to combine several \nmeta models via stacking. \n</p>"}, "author": "Thomas Bartz-Beielstein, Lorenzo Gentile, Martin Zaefferer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490926", "id": "tag:google.com,2005:reader/item/0000000346127e4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "RESIDE: A Benchmark for Single Image Dehazing. (arXiv:1712.04143v1 [cs.CV])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04143"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04143", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we present a comprehensive study and evaluation of existing \nsingle image dehazing algorithms, using a new large-scale benchmark consisting \nof both synthetic and real-world hazy images, called REalistic Single Image \nDEhazing (RESIDE). RESIDE highlights diverse data sources and image contents, \nand is divided into five subsets, each serving different training or evaluation \npurposes. We further provide a rich variety of criteria for dehazing algorithm \nevaluation, ranging from full-reference metrics, to no-reference metrics, to \nsubjective evaluation and the novel task-driven evaluation. Experiments on \nRESIDE sheds light on the comparisons and limitations of state-of-the-art \ndehazing algorithms, and suggest promising future directions. \n</p>"}, "author": "Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, Zhangyang Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490925", "id": "tag:google.com,2005:reader/item/0000000346127e53", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Toward `verifying' a Water Treatment System. (arXiv:1712.04155v1 [cs.AI])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04155"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04155", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Modeling and verifying real-world cyber-physical systems are challenging, \nespecially so for complex systems where manually modeling is infeasible. In \nthis work, we report our experience on combining model learning and abstraction \nrefinement to analyze a challenging system, i.e., a real-world Secure Water \nTreatment (SWaT) system. Given a set of safety requirements, the objective is \nto either show that the system is safe with a high probability (so that a \nsystem shutdown is rarely triggered due to safety violation) or otherwise. As \nthe system is too complicated to be manually modelled, we apply latest \nautomatic model learning techniques to construct a set of Markov chains through \nabstraction and refinement, based on two long system execution logs (one for \ntraining and the other for testing). For each probabilistic property, we either \nreport it does not hold with a certain level of probabilistic confidence, or \nreport that it holds by showing the evidence in the form of an abstract Markov \nchain. The Markov chains can subsequently be implemented as runtime monitors in \nSWaT. This is the first case study of applying model learning techniques to a \nreal-world system as far as we know. \n</p>"}, "author": "Jingyi Wang, Jun Sun, Yifan Jia", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490924", "id": "tag:google.com,2005:reader/item/0000000346127e5b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Mining Non-Redundant Sets of Generalizing Patterns from Sequence Databases. (arXiv:1712.04159v1 [cs.DS])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04159"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04159", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Sequential pattern mining techniques extract patterns corresponding to \nfrequent subsequences from a sequence database. A practical limitation of these \ntechniques is that they overload the user with too many patterns. Local Process \nModel (LPM) mining is an alternative approach coming from the field of process \nmining. While in traditional sequential pattern mining, a pattern describes one \nsubsequence, an LPM captures a set of subsequences. Also, while traditional \nsequential patterns only match subsequences that are observed in the sequence \ndatabase, an LPM may capture subsequences that are not explicitly observed, but \nthat are related to observed subsequences. In other words, LPMs generalize the \nbehavior observed in the sequence database. These properties make it possible \nfor a set of LPMs to cover the behavior of a much larger set of sequential \npatterns. Yet, existing LPM mining techniques still suffer from the pattern \nexplosion problem because they produce sets of redundant LPMs. In this paper, \nwe propose several heuristics to mine a set of non-redundant LPMs either from a \nset of redundant LPMs or from a set of sequential patterns. We empirically \ncompare the proposed heuristics between them and against existing (local) \nprocess mining techniques in terms of coverage, precision, and complexity of \nthe produced sets of LPMs. \n</p>"}, "author": "Niek Tax, Marlon Dumas", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490923", "id": "tag:google.com,2005:reader/item/0000000346127e62", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents. (arXiv:1712.04172v1 [cs.AI])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04172"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04172", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes a low-cost, easily realizable strategy to equip a \nreinforcement learning (RL) agent the capability of behaving ethically. Our \nmodel allows the designers of RL agents to solely focus on the task to achieve, \nwithout having to worry about the implementation of multiple trivial ethical \npatterns to follow. Based on the assumption that the majority of human \nbehavior, regardless which goals they are achieving, is ethical, our design \nintegrates human policy with the RL policy to achieve the target objective with \nless chance of violating the ethical code that human beings normally obey. \n</p>"}, "author": "Yueh-Hua Wu, Shou-De Lin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490922", "id": "tag:google.com,2005:reader/item/0000000346127e69", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Contradiction-Centricity: A Uniform Model for Formation of Swarm Intelligence and its Simulations. (arXiv:1712.04182v1 [cs.AI])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04182"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04182", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is a grand challenge to model the emergence of swarm intelligence and many \nprinciples or models had been proposed. However, existing models do not catch \nthe nature of swarm intelligence and they are not generic enough to describe \nvarious types of emergence phenomena. In this work, we propose a \ncontradiction-centric model for emergence of swarm intelligence, in which \nindividuals' contradictions dominate their appearances whilst they are \nassociated and interacting to update their contradictions. This model \nhypothesizes that 1) the emergence of swarm intelligence is rooted in the \ndevelopment of contradictions of individuals and the interactions among \nassociated individuals and 2) swarm intelligence is essentially a combinative \nreflection of the configurations of contradictions inside individuals and the \ndistributions of contradictions among individuals. To verify the feasibility of \nthe model, we simulate four types of swarm intelligence. As the simulations \nshow, our model is truly generic and can describe the emergence of a variety of \nswarm intelligence, and it is also very simple and can be easily applied to \ndemonstrate the emergence of swarm intelligence without needing complicated \ncomputations. \n</p>"}, "author": "Wenpin Jiao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490921", "id": "tag:google.com,2005:reader/item/0000000346127e71", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning for IoT Big Data and Streaming Analytics: A Survey. (arXiv:1712.04301v1 [cs.LG])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04301"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04301", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In the era of the Internet of Things (IoT), an enormous amount of sensing \ndevices collect and/or generate various sensory data over time for a wide range \nof fields and applications. Based on the nature of the application, these \ndevices will result in big or fast/real-time data streams. Applying analytics \nover such data streams to discover new information, predict future insights, \nand make control decisions is a crucial process that makes IoT a worthy \nparadigm for businesses and a quality-of-life improving technology. In this \npaper, we provide a thorough overview on using a class of advanced machine \nlearning techniques, namely Deep Learning (DL), to facilitate the analytics and \nlearning in the IoT domain. We start by articulating IoT data characteristics \nand identifying two major treatments for IoT data from a machine learning \nperspective, namely IoT big data analytics and IoT streaming data analytics. We \nalso discuss why DL is a promising approach to achieve the desired analytics in \nthese types of data and applications. The potential of using emerging DL \ntechniques for IoT data analytics are then discussed, and its promises and \nchallenges are introduced. We present a comprehensive background on different \nDL architectures and algorithms. We also analyze and summarize major reported \nresearch attempts that leveraged DL in the IoT domain. The smart IoT devices \nthat have incorporated DL in their intelligence background are also discussed. \nDL implementation approaches on the fog and cloud centers in support of IoT \napplications are also surveyed. Finally, we shed light on some challenges and \npotential directions for future research. At the end of each section, we \nhighlight the lessons learned based on our experiments and review of the recent \nliterature. \n</p>"}, "author": "Mehdi Mohammadi, Ala Al-Fuqaha, Sameh Sorour, Mohsen Guizani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490920", "id": "tag:google.com,2005:reader/item/0000000346127e75", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "In folly ripe. In reason rotten. Putting machine theology to rest. (arXiv:1712.04306v1 [cs.AI])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04306"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04306", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Computation has changed the world more than any previous expressions of \nknowledge. In its particular algorithmic embodiment, it offers a perspective, \nwithin which the digital computer (one of many possible) exercises a role \nreminiscent of theology. Since it is closed to meaning, algorithmic digital \ncomputation can at most mimic the creative aspects of life. AI, in the \nperspective of time, proved to be less an acronym for artificial intelligence \nand more of automating tasks associated with intelligence. The entire \ndevelopment led to the hypostatized role of the machine: outputting nothing \nelse but reality, including that of the humanity that made the machine happen. \nThe convergence machine called deep learning is only the latest form through \nwhich the deterministic theology of the machine claims more than what extremely \neffective data processing actually is. A new understanding of complexity, as \nwell as the need to distinguish between the reactive nature of the artificial \nand the anticipatory nature of the living are suggested as practical responses \nto the challenges posed by machine theology. \n</p>"}, "author": "Mihai Nadin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490919", "id": "tag:google.com,2005:reader/item/0000000346127e7d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "AI Safety and Reproducibility: Establishing Robust Foundations for the Neuroscience of Human Values. (arXiv:1712.04307v1 [cs.AI])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04307"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04307", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504eb63c2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504eb63c2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose the creation of a systematic effort to identify and replicate key \nfindings in neuroscience and allied fields related to understanding human \nvalues. Our aim is to ensure that research underpinning the value alignment \nproblem of artificial intelligence has been sufficiently validated to play a \nrole in the design of AI systems. \n</p>"}, "author": "Gopal P. Sarma, Nick J. Hay, Adam Safron", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490918", "id": "tag:google.com,2005:reader/item/0000000346127e84", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Echo State Network (DeepESN): A Brief Survey. (arXiv:1712.04323v1 [cs.LG])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04323"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04323", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504f22a74\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504f22a74&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The study of deep recurrent neural networks (RNNs) and, in particular, of \ndeep Reservoir Computing (RC) is gaining an increasing research attention in \nthe neural networks community. The recently introduced deep Echo State Network \n(deepESN) model opened the way to an extremely efficient approach for designing \ndeep neural networks for temporal data. At the same time, the study of deepESNs \nallowed to shed light on the intrinsic properties of state dynamics developed \nby hierarchical compositions of recurrent layers, i.e. on the bias of depth in \nRNNs architectural design. In this paper, we summarize the advancements in the \ndevelopment, analysis and applications of deepESNs. \n</p>"}, "author": "Claudio Gallicchio, Alessio Micheli", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490917", "id": "tag:google.com,2005:reader/item/0000000346127e8a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Simulated Autonomous Driving on Realistic Road Networks using Deep Reinforcement Learning. (arXiv:1712.04363v1 [cs.AI])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04363"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04363", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Using Deep Reinforcement Learning (DRL) can be a promising approach to handle \ntasks in the field of (simulated) autonomous driving, whereby recent \npublications only consider learning in unusual driving environments. This paper \noutlines a developed software, which instead can be used for evaluating DRL \nalgorithms based on realistic road networks and therefore in more usual driving \nenvironments. Furthermore, we identify difficulties when DRL algorithms are \napplied to tasks, in which it is not only important to reach a goal, but also \nhow this goal is reached. We conclude this paper by presenting the results of \nan application of a new DRL algorithm, which can partly solve these problems. \n</p>"}, "author": "Patrick Klose, Rudolf Mester", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490916", "id": "tag:google.com,2005:reader/item/0000000346127e95", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hawkes Processes for Invasive Species Modeling and Management. (arXiv:1712.04386v1 [q-bio.PE])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04386"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04386", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The spread of invasive species to new areas threatens the stability of \necosystems and causes major economic losses in agriculture and forestry. We \npropose a novel approach to minimizing the spread of an invasive species given \na limited intervention budget. We first model invasive species propagation \nusing Hawkes processes, and then derive closed-form expressions for \ncharacterizing the effect of an intervention action on the invasion process. We \nuse this to obtain an optimal intervention plan based on an integer programming \nformulation, and compare the optimal plan against several \necologically-motivated heuristic strategies used in practice. We present an \nempirical study of two variants of the invasive control problem: minimizing the \nfinal rate of invasions, and minimizing the number of invasions at the end of a \ngiven time horizon. Our results show that the optimized intervention achieves \nnearly the same level of control that would be attained by completely \neradicating the species, with a 20% cost saving. Additionally, we design a \nheuristic intervention strategy based on a combination of the density and life \nstage of the invasive individuals, and find that it comes surprisingly close to \nthe optimized strategy, suggesting that this could serve as a good rule of \nthumb in invasive species management. \n</p>"}, "author": "Amrita Gupta, Mehrdad Farajtabar, Bistra Dilkina, Hongyuan Zha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490915", "id": "tag:google.com,2005:reader/item/0000000346127e9b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Android Malware Characterization using Metadata and Machine Learning Techniques. (arXiv:1712.04402v1 [cs.CR])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04402"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04402", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Android Malware has emerged as a consequence of the increasing popularity of \nsmartphones and tablets. While most previous work focuses on inherent \ncharacteristics of Android apps to detect malware, this study analyses indirect \nfeatures and meta-data to identify patterns in malware applications. Our \nexperiments show that: (1) the permissions used by an application offer only \nmoderate performance results; (2) other features publicly available at Android \nMarkets are more relevant in detecting malware, such as the application \ndeveloper and certificate issuer, and (3) compact and efficient classifiers can \nbe constructed for the early detection of malware applications prior to code \ninspection or sandboxing. \n</p>"}, "author": "Ignacio Mart&#xed;n, Jos&#xe9; Alberto Hern&#xe1;ndez, Alfonso Mu&#xf1;oz, Antonio Guzm&#xe1;n", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490914", "id": "tag:google.com,2005:reader/item/0000000346127ea4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deception Detection in Videos. (arXiv:1712.04415v1 [cs.AI])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04415"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04415", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a system for covert automated deception detection in real-life \ncourtroom trial videos. We study the importance of different modalities like \nvision, audio and text for this task. On the vision side, our system uses \nclassifiers trained on low level video features which predict human \nmicro-expressions. We show that predictions of high-level micro-expressions can \nbe used as features for deception prediction. Surprisingly, IDT (Improved Dense \nTrajectory) features which have been widely used for action recognition, are \nalso very good at predicting deception in videos. We fuse the score of \nclassifiers trained on IDT features and high-level micro-expressions to improve \nperformance. MFCC (Mel-frequency Cepstral Coefficients) features from the audio \ndomain also provide a significant boost in performance, while information from \ntranscripts is not very beneficial for our system. Using various classifiers, \nour automated system obtains an AUC of 0.877 (10-fold cross-validation) when \nevaluated on subjects which were not part of the training set. Even though \nstate-of-the-art methods use human annotations of micro-expressions for \ndeception detection, our fully automated approach outperforms them by 5%. When \ncombined with human annotations of micro-expressions, our AUC improves to \n0.922. We also present results of a user-study to analyze how well do average \nhumans perform on this task, what modalities they use for deception detection \nand how they perform if only one modality is accessible. Our project page can \nbe found at \\url{https://doubaibai.github.io/DARE/}. \n</p>"}, "author": "Zhe Wu, Bharat Singh, Larry S. Davis, V. S. Subrahmanian", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490913", "id": "tag:google.com,2005:reader/item/0000000346127eab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Reset-free Trial-and-Error Learning for Robot Damage Recovery. (arXiv:1610.04213v4 [cs.RO] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1610.04213"}], "alternate": [{"href": "http://arxiv.org/abs/1610.04213", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The high probability of hardware failures prevents many advanced robots \n(e.g., legged robots) from being confidently deployed in real-world situations \n(e.g., post-disaster rescue). Instead of attempting to diagnose the failures, \nrobots could adapt by trial-and-error in order to be able to complete their \ntasks. In this situation, damage recovery can be seen as a Reinforcement \nLearning (RL) problem. However, the best RL algorithms for robotics require the \nrobot and the environment to be reset to an initial state after each episode, \nthat is, the robot is not learning autonomously. In addition, most of the RL \nmethods for robotics do not scale well with complex robots (e.g., walking \nrobots) and either cannot be used at all or take too long to converge to a \nsolution (e.g., hours of learning). In this paper, we introduce a novel \nlearning algorithm called \"Reset-free Trial-and-Error\" (RTE) that (1) breaks \nthe complexity by pre-generating hundreds of possible behaviors with a dynamics \nsimulator of the intact robot, and (2) allows complex robots to quickly recover \nfrom damage while completing their tasks and taking the environment into \naccount. We evaluate our algorithm on a simulated wheeled robot, a simulated \nsix-legged robot, and a real six-legged walking robot that are damaged in \nseveral ways (e.g., a missing leg, a shortened leg, faulty motor, etc.) and \nwhose objective is to reach a sequence of targets in an arena. Our experiments \nshow that the robots can recover most of their locomotion abilities in an \nenvironment with obstacles, and without any human intervention. \n</p>"}, "author": "Konstantinos Chatzilygeroudis, Vassilis Vassiliades, Jean-Baptiste Mouret", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490912", "id": "tag:google.com,2005:reader/item/0000000346127eb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Continual Learning with Deep Generative Replay. (arXiv:1705.08690v3 [cs.AI] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.08690"}], "alternate": [{"href": "http://arxiv.org/abs/1705.08690", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Attempts to train a comprehensive artificial intelligence capable of solving \nmultiple tasks have been impeded by a chronic problem called catastrophic \nforgetting. Although simply replaying all previous data alleviates the problem, \nit requires large memory and even worse, often infeasible in real world \napplications where the access to past data is limited. Inspired by the \ngenerative nature of hippocampus as a short-term memory system in primate \nbrain, we propose the Deep Generative Replay, a novel framework with a \ncooperative dual model architecture consisting of a deep generative model \n(\"generator\") and a task solving model (\"solver\"). With only these two models, \ntraining data for previous tasks can easily be sampled and interleaved with \nthose for a new task. We test our methods in several sequential learning \nsettings involving image classification tasks. \n</p>"}, "author": "Hanul Shin, Jung Kwon Lee, Jaehong Kim, Jiwon Kim", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490911", "id": "tag:google.com,2005:reader/item/0000000346127eb9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On a Formal Model of Safe and Scalable Self-driving Cars. (arXiv:1708.06374v3 [cs.RO] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.06374"}], "alternate": [{"href": "http://arxiv.org/abs/1708.06374", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In recent years, car makers and tech companies have been racing towards self \ndriving cars. It seems that the main parameter in this race is who will have \nthe first car on the road. The goal of this paper is to add to the equation two \nadditional crucial parameters. The first is standardization of safety assurance \n--- what are the minimal requirements that every self-driving car must satisfy, \nand how can we verify these requirements. The second parameter is scalability \n--- engineering solutions that lead to unleashed costs will not scale to \nmillions of cars, which will push interest in this field into a niche academic \ncorner, and drive the entire field into a \"winter of autonomous driving\". In \nthe first part of the paper we propose a white-box, interpretable, mathematical \nmodel for safety assurance, which we call Responsibility-Sensitive Safety \n(RSS). In the second part we describe a design of a system that adheres to our \nsafety assurance requirements and is scalable to millions of cars. \n</p>"}, "author": "Shai Shalev-Shwartz, Shaked Shammah, Amnon Shashua", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490910", "id": "tag:google.com,2005:reader/item/0000000346127ec1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Synonym Discovery with Etymology-based Word Embeddings. (arXiv:1709.10445v2 [cs.CL] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.10445"}], "alternate": [{"href": "http://arxiv.org/abs/1709.10445", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a novel approach to learn word embeddings based on an extended \nversion of the distributional hypothesis. Our model derives word embedding \nvectors using the etymological composition of words, rather than the context in \nwhich they appear. It has the strength of not requiring a large text corpus, \nbut instead it requires reliable access to etymological roots of words, making \nit specially fit for languages with logographic writing systems. The model \nconsists on three steps: (1) building an etymological graph, which is a \nbipartite network of words and etymological roots, (2) obtaining the \nbiadjacency matrix of the etymological graph and reducing its dimensionality, \n(3) using columns/rows of the resulting matrices as embedding vectors. We test \nour model in the Chinese and Sino-Korean vocabularies. Our graphs are formed by \na set of 117,000 Chinese words, and a set of 135,000 Sino-Korean words. In both \ncases we show that our model performs well in the task of synonym discovery. \n</p>"}, "author": "Seunghyun Yoon, Pablo Estrada, Kyomin Jung", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490909", "id": "tag:google.com,2005:reader/item/0000000346127ec7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "ADA: A Game-Theoretic Perspective on Data Augmentation for Object Detection. (arXiv:1710.07735v2 [cs.CV] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07735"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07735", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504f22e64\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504f22e64&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The use of random perturbations of ground truth data, such as random \ntranslation or scaling of bounding boxes, is a common heuristic used for data \naugmentation that has been shown to prevent overfitting and improve \ngeneralization. Since the design of data augmentation is largely guided by \nreported best practices, it is difficult to understand if those design choices \nare optimal. To provide a more principled perspective, we develop a \ngame-theoretic interpretation of data augmentation in the context of object \ndetection. We aim to find an optimal adversarial perturbations of the ground \ntruth data (i.e., the worst case perturbations) that forces the object bounding \nbox predictor to learn from the hardest distribution of perturbed examples for \nbetter test-time performance. We establish that the game theoretic solution, \nthe Nash equilibrium, provides both an optimal predictor and optimal data \naugmentation distribution. We show that our adversarial method of training a \npredictor can significantly improve test time performance for the task of \nobject detection. On the ImageNet object detection task, our adversarial \napproach improves performance by over 16\\% compared to the best performing data \naugmentation method \n</p>"}, "author": "Sima Behpour, Kris M. Kitani, Brian D. Ziebart", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490908", "id": "tag:google.com,2005:reader/item/0000000346127ecd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Modular Continual Learning in a Unified Visual Environment. (arXiv:1711.07425v2 [cs.LG] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07425"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07425", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A core aspect of human intelligence is the ability to learn new tasks quickly \nand switch between them flexibly. Here, we describe a modular continual \nreinforcement learning paradigm inspired by these abilities. We first introduce \na visual interaction environment that allows many types of tasks to be unified \nin a single framework. We then describe a reward map prediction scheme that \nlearns new tasks robustly in the very large state and action spaces required by \nsuch an environment. We investigate how properties of module architecture \ninfluence efficiency of task learning, showing that a module motif \nincorporating specific design principles (e.g. early bottlenecks, low-order \npolynomial nonlinearities, and symmetry) significantly outperforms more \nstandard neural network motifs, needing fewer training examples and fewer \nneurons to achieve high levels of performance. Finally, we present a \nmeta-controller architecture for task switching based on a dynamic neural \nvoting scheme, which allows new modules to use information learned from \npreviously-seen tasks to substantially improve their own learning efficiency. \n</p>"}, "author": "Kevin T. Feigelis, Blue Sheffer, Daniel L. K. Yamins", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490907", "id": "tag:google.com,2005:reader/item/0000000346127ed5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Allocation Problems in Ride-Sharing Platforms: Online Matching with Offline Reusable Resources. (arXiv:1711.08345v2 [cs.AI] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08345"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08345", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Bipartite matching markets pair agents on one side of a market with agents, \nitems, or contracts on the opposing side. Prior work addresses online bipartite \nmatching markets, where agents arrive over time and are dynamically matched to \na known set of disposable resources. In this paper, we propose a new model, \nOnline Matching with (offline) Reusable Resources under Known Adversarial \nDistributions (OM-RR-KAD), in which resources on the offline side are reusable \ninstead of disposable; that is, once matched, resources become available again \nat some point in the future. We show that our model is tractable by presenting \nan LP-based adaptive algorithm that achieves an online competitive ratio of 1/2 \n- eps for any given eps greater than 0. We also show that no non-adaptive \nalgorithm can achieve a ratio of 1/2 + o(1) based on the same benchmark LP. \nThrough a data-driven analysis on a massive openly-available dataset, we show \nour model is robust enough to capture the application of taxi dispatching \nservices and ride-sharing systems. We also present heuristics that perform well \nin practice. \n</p>"}, "author": "John P Dickerson, Karthik A Sankararaman, Aravind Srinivasan, Pan Xu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490906", "id": "tag:google.com,2005:reader/item/0000000346127edb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "DeepConfig: Automating Data Center Network Topologies Management with Machine Learning. (arXiv:1712.03890v1 [cs.NI] CROSS LISTED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03890"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03890", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In recent years, many techniques have been developed to improve the \nperformance and efficiency of data center networks. While these techniques \nprovide high accuracy, they are often designed using heuristics that leverage \ndomain-specific properties of the workload or hardware. \n</p> \n<p>In this vision paper, we argue that many data center networking techniques, \ne.g., routing, topology augmentation, energy savings, with diverse goals \nactually share design and architectural similarity. We present a design for \ndeveloping general intermediate representations of network topologies using \ndeep learning that is amenable to solving classes of data center problems. We \ndevelop a framework, DeepConfig, that simplifies the processing of configuring \nand training deep learning agents that use the intermediate representation to \nlearns different tasks. To illustrate the strength of our approach, we \nconfigured, implemented, and evaluated a DeepConfig-Agent that tackles the data \ncenter topology augmentation problem. Our initial results are promising --- \nDeepConfig performs comparably to the optimal. \n</p>"}, "author": "Christopher Streiffer, Huan Chen, Theophilus Benson, Asim Kadav", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490905", "id": "tag:google.com,2005:reader/item/0000000346127ee4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Eye In-Painting with Exemplar Generative Adversarial Networks. (arXiv:1712.03999v1 [cs.CV])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03999"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03999", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper introduces a novel approach to in-painting where the identity of \nthe object to remove or change is preserved and accounted for at inference \ntime: Exemplar GANs (ExGANs). ExGANs are a type of conditional GAN that utilize \nexemplar information to produce high-quality, personalized in painting results. \nWe propose using exemplar information in the form of a reference image of the \nregion to in-paint, or a perceptual code describing that object. Unlike \nprevious conditional GAN formulations, this extra information can be inserted \nat multiple points within the adversarial network, thus increasing its \ndescriptive power. We show that ExGANs can produce photo-realistic personalized \nin-painting results that are both perceptually and semantically plausible by \napplying them to the task of closed to-open eye in-painting in natural \npictures. A new benchmark dataset is also introduced for the task of eye \nin-painting for future comparisons. \n</p>"}, "author": "Brian Dolhansky, Cristian Canton Ferrer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490904", "id": "tag:google.com,2005:reader/item/0000000346127eeb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Attention networks for image-to-text. (arXiv:1712.04046v1 [cs.CV])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04046"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04046", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The paper approaches the problem of image-to-text with attention-based \nencoder-decoder networks that are trained to handle sequences of characters \nrather than words. We experiment on lines of text from a popular handwriting \ndatabase with different attention mechanisms for the decoder. The model trained \nwith softmax attention achieves the lowest test error, outperforming several \nother RNN-based models. Our results show that softmax attention is able to \nlearn a linear alignment whereas the alignment generated by sigmoid attention \nis linear but much less precise. \n</p>"}, "author": "Jason Poulos, Rafael Valle", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490903", "id": "tag:google.com,2005:reader/item/0000000346127ef6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "PacGAN: The power of two samples in generative adversarial networks. (arXiv:1712.04086v1 [cs.LG])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04086"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04086", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative adversarial networks (GANs) are innovative techniques for learning \ngenerative models of complex data distributions from samples. Despite \nremarkable recent improvements in generating realistic images, one of their \nmajor shortcomings is the fact that in practice, they tend to produce samples \nwith little diversity, even when trained on diverse datasets. This phenomenon, \nknown as mode collapse, has been the main focus of several recent advances in \nGANs. Yet there is little understanding of why mode collapse happens and why \nexisting approaches are able to mitigate mode collapse. We propose a principled \napproach to handling mode collapse, which we call packing. The main idea is to \nmodify the discriminator to make decisions based on multiple samples from the \nsame class, either real or artificially generated. We borrow analysis tools \nfrom binary hypothesis testing---in particular the seminal result of Blackwell \n[Bla53]---to prove a fundamental connection between packing and mode collapse. \nWe show that packing naturally penalizes generators with mode collapse, thereby \nfavoring generator distributions with less mode collapse during the training \nprocess. Numerical experiments on benchmark datasets suggests that packing \nprovides significant improvements in practice as well. \n</p>"}, "author": "Zinan Lin, Ashish Khetan, Giulia Fanti, Sewoong Oh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490902", "id": "tag:google.com,2005:reader/item/0000000346127f04", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "GibbsNet: Iterative Adversarial Inference for Deep Graphical Models. (arXiv:1712.04120v1 [stat.ML])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04120"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04120", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Directed latent variable models that formulate the joint distribution as \n$p(x,z) = p(z) p(x \\mid z)$ have the advantage of fast and exact sampling. \nHowever, these models have the weakness of needing to specify $p(z)$, often \nwith a simple fixed prior that limits the expressiveness of the model. \nUndirected latent variable models discard the requirement that $p(z)$ be \nspecified with a prior, yet sampling from them generally requires an iterative \nprocedure such as blocked Gibbs-sampling that may require many steps to draw \nsamples from the joint distribution $p(x, z)$. We propose a novel approach to \nlearning the joint distribution between the data and a latent code which uses \nan adversarially learned iterative procedure to gradually refine the joint \ndistribution, $p(x, z)$, to better match with the data distribution on each \nstep. GibbsNet is the best of both worlds both in theory and in practice. \nAchieving the speed and simplicity of a directed latent variable model, it is \nguaranteed (assuming the adversarial game reaches the virtual training criteria \nglobal minimum) to produce samples from $p(x, z)$ with only a few sampling \niterations. Achieving the expressiveness and flexibility of an undirected \nlatent variable model, GibbsNet does away with the need for an explicit $p(z)$ \nand has the ability to do attribute prediction, class-conditional generation, \nand joint image-attribute modeling in a single model which is not trained for \nany of these specific tasks. We show empirically that GibbsNet is able to learn \na more complex $p(z)$ and show that this leads to improved inpainting and \niterative refinement of $p(x, z)$ for dozens of steps and stable generation \nwithout collapse for thousands of steps, despite being trained on only a few \nsteps. \n</p>"}, "author": "Alex Lamb, Devon Hjelm, Yaroslav Ganin, Joseph Paul Cohen, Aaron Courville, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490901", "id": "tag:google.com,2005:reader/item/0000000346127f0e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Outlier Detection by Consistent Data Selection Method. (arXiv:1712.04129v1 [cs.LG])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04129"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04129", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Often the challenge associated with tasks like fraud and spam detection[1] is \nthe lack of all likely patterns needed to train suitable supervised learning \nmodels. In order to overcome this limitation, such tasks are attempted as \noutlier or anomaly detection tasks. We also hypothesize that out- liers have \nbehavioral patterns that change over time. Limited data and continuously \nchanging patterns makes learning significantly difficult. In this work we are \nproposing an approach that detects outliers in large data sets by relying on \ndata points that are consistent. The primary contribution of this work is that \nit will quickly help retrieve samples for both consistent and non-outlier data \nsets and is also mindful of new outlier patterns. No prior knowledge of each \nset is required to extract the samples. The method consists of two phases, in \nthe first phase, consistent data points (non- outliers) are retrieved by an \nensemble method of unsupervised clustering techniques and in the second phase a \none class classifier trained on the consistent data point set is ap- plied on \nthe remaining sample set to identify the outliers. The approach is tested on \nthree publicly available data sets and the performance scores are competitive. \n</p>"}, "author": "Utkarsh Porwal, Smruthi Mukund", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490900", "id": "tag:google.com,2005:reader/item/0000000346127f1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning for Reliable Mobile Edge Analytics in Intelligent Transportation Systems. (arXiv:1712.04135v1 [cs.IT])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04135"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04135", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Intelligent transportation systems (ITSs) will be a major component of \ntomorrow's smart cities. However, realizing the true potential of ITSs requires \nultra-low latency and reliable data analytics solutions that can combine, in \nreal-time, a heterogeneous mix of data stemming from the ITS network and its \nenvironment. Such data analytics capabilities cannot be provided by \nconventional cloud-centric data processing techniques whose communication and \ncomputing latency can be high. Instead, edge-centric solutions that are \ntailored to the unique ITS environment must be developed. In this paper, an \nedge analytics architecture for ITSs is introduced in which data is processed \nat the vehicle or roadside smart sensor level in order to overcome the ITS \nlatency and reliability challenges. With a higher capability of passengers' \nmobile devices and intra-vehicle processors, such a distributed edge computing \narchitecture can leverage deep learning techniques for reliable mobile sensing \nin ITSs. In this context, the ITS mobile edge analytics challenges pertaining \nto heterogeneous data, autonomous control, vehicular platoon control, and \ncyber-physical security are investigated. Then, different deep learning \nsolutions for such challenges are proposed. The proposed deep learning \nsolutions will enable ITS edge analytics by endowing the ITS devices with \npowerful computer vision and signal processing functions. Preliminary results \nshow that the proposed edge analytics architecture, coupled with the power of \ndeep learning algorithms, can provide a reliable, secure, and truly smart \ntransportation environment. \n</p>"}, "author": "Aidin Ferdowsi, Ursula Challita, Walid Saad", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490899", "id": "tag:google.com,2005:reader/item/0000000346127f31", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Information Perspective to Probabilistic Modeling: Boltzmann Machines versus Born Machines. (arXiv:1712.04144v1 [physics.data-an])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04144"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04144", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504f231b5\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504f231b5&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We compare and contrast the statistical physics and quantum physics inspired \napproaches for unsupervised generative modeling of classical data. The two \napproaches represent probabilities of observed data using energy-based models \nand quantum states respectively.Classical and quantum information patterns of \nthe target datasets therefore provide principled guidelines for structural \ndesign and learning in these two approaches. Taking the restricted Boltzmann \nmachines (RBM) as an example, we analyze the information theoretical bounds of \nthe two approaches. We verify our reasonings by comparing the performance of \nRBMs of various architectures on the standard MNIST datasets. \n</p>"}, "author": "Song Cheng, Jing Chen, Lei Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490898", "id": "tag:google.com,2005:reader/item/0000000346127f38", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Transportation analysis of denoising autoencoders: a novel method for analyzing deep neural networks. (arXiv:1712.04145v1 [cs.LG])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04145"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04145", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504f880f6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504f880f6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The feature map obtained from the denoising autoencoder (DAE) is investigated \nby determining transportation dynamics of the DAE, which is a cornerstone for \ndeep learning. Despite the rapid development in its application, deep neural \nnetworks remain analytically unexplained, because the feature maps are nested \nand parameters are not faithful. In this paper, we address the problem of the \nformulation of nested complex of parameters by regarding the feature map as a \ntransport map. Even when a feature map has different dimensions between input \nand output, we can regard it as a transportation map by considering that both \nthe input and output spaces are embedded in a common high-dimensional space. In \naddition, the trajectory is a geometric object and thus, is independent of \nparameterization. In this manner, transportation can be regarded as a universal \ncharacter of deep neural networks. By determining and analyzing the \ntransportation dynamics, we can understand the behavior of a deep neural \nnetwork. In this paper, we investigate a fundamental case of deep neural \nnetworks: the DAE. We derive the transport map of the DAE, and reveal that the \ninfinitely deep DAE transports mass to decrease a certain quantity, such as \nentropy, of the data distribution. These results though analytically simple, \nshed light on the correspondence between deep neural networks and the \nWasserstein gradient flows. \n</p>"}, "author": "Sho Sonoda, Noboru Murata", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490897", "id": "tag:google.com,2005:reader/item/0000000346127f4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Random Sample Partition Data Model for Big Data Analysis. (arXiv:1712.04146v1 [cs.DC])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04146"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04146", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Big data sets must be carefully partitioned into statistically similar data \nsubsets that can be used as representative samples for big data analysis tasks. \nIn this paper, we propose the random sample partition (RSP) to represent a big \ndata set as a set of non-overlapping data subsets, i.e. RSP data blocks, where \neach RSP data block has the same probability distribution with the whole big \ndata set. Then, the block-based sampling is used to directly select \nrepresentative samples for a variety of data analysis tasks. We show how RSP \ndata blocks can be employed to estimate statistics and build models which are \nequivalent (or approximate) to those from the whole big data set. \n</p>"}, "author": "Salman Salloum, Yulin He, Joshua Zhexue Huang, Xiaoliang Zhang, Tamer Z. Emara", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490896", "id": "tag:google.com,2005:reader/item/0000000346127f4f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Temporal Stability in Predictive Process Monitoring. (arXiv:1712.04165v1 [cs.LG])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04165"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04165", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Predictive business process monitoring is concerned with the analysis of \nevents produced during the execution of a business process in order to predict \nas early as possible the final outcome of an ongoing case. Traditionally, \npredictive process monitoring methods are optimized with respect to accuracy. \nHowever, in environments where users make decisions and take actions in \nresponse to the predictions they receive, it is equally important to optimize \nthe stability of the successive predictions made for each case. To this end, \nthis paper defines a notion of temporal stability for predictive process \nmonitoring and evaluates existing methods with respect to both temporal \nstability and accuracy. We find that methods based on XGBoost and LSTM neural \nnetworks exhibit the highest temporal stability. We then show that temporal \nstability can be enhanced by hyperparameter-optimizing random forests and \nXGBoost classifiers with respect to inter-run stability. Finally, we show that \ntime series smoothing techniques can further enhance temporal stability at the \nexpense of slightly lower accuracy. \n</p>"}, "author": "Irene Teinemaa, Marlon Dumas, Anna Leontjeva, Fabrizio Maria Maggi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490895", "id": "tag:google.com,2005:reader/item/0000000346127f58", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Causal Patterns: Extraction of multiple causal relationships by Mixture of Probabilistic Partial Canonical Correlation Analysis. (arXiv:1712.04221v1 [stat.ME])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04221"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04221", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a mixture of probabilistic partial canonical \ncorrelation analysis (MPPCCA) that extracts the Causal Patterns from two \nmultivariate time series. Causal patterns refer to the signal patterns within \ninteractions of two elements having multiple types of mutually causal \nrelationships, rather than a mixture of simultaneous correlations or the \nabsence of presence of a causal relationship between the elements. In \nmultivariate statistics, partial canonical correlation analysis (PCCA) \nevaluates the correlation between two multivariates after subtracting the \neffect of the third multivariate. PCCA can calculate the Granger Causal- ity \nIndex (which tests whether a time-series can be predicted from an- other \ntime-series), but is not applicable to data containing multiple partial \ncanonical correlations. After introducing the MPPCCA, we propose an \nexpectation-maxmization (EM) algorithm that estimates the parameters and latent \nvariables of the MPPCCA. The MPPCCA is expected to ex- tract multiple partial \ncanonical correlations from data series without any supervised signals to split \nthe data as clusters. The method was then eval- uated in synthetic data \nexperiments. In the synthetic dataset, our method estimated the multiple \npartial canonical correlations more accurately than the existing method. To \ndetermine the types of patterns detectable by the method, experiments were also \nconducted on real datasets. The method estimated the communication patterns In \nmotion-capture data. The MP- PCCA is applicable to various type of signals such \nas brain signals, human communication and nonlinear complex multibody systems. \n</p>"}, "author": "Hiroki Mori, Keisuke Kawano, Hiroki Yokoyama", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490894", "id": "tag:google.com,2005:reader/item/0000000346127f5e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multi-Speaker Localization Using Convolutional Neural Network Trained with Noise. (arXiv:1712.04276v1 [cs.SD])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04276"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04276", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The problem of multi-speaker localization is formulated as a multi-class \nmulti-label classification problem, which is solved using a convolutional \nneural network (CNN) based source localization method. Utilizing the common \nassumption of disjoint speaker activities, we propose a novel method to train \nthe CNN using synthesized noise signals. The proposed localization method is \nevaluated for two speakers and compared to a well-known steered response power \nmethod. \n</p>"}, "author": "Soumitro Chakrabarty, Emanu&#xeb;l A. P. Habets", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490893", "id": "tag:google.com,2005:reader/item/0000000346127f65", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Scaling Limit: Exact and Tractable Analysis of Online Learning Algorithms with Applications to Regularized Regression and PCA. (arXiv:1712.04332v1 [cs.LG])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04332"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04332", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a framework for analyzing the exact dynamics of a class of online \nlearning algorithms in the high-dimensional scaling limit. Our results are \napplied to two concrete examples: online regularized linear regression and \nprincipal component analysis. As the ambient dimension tends to infinity, and \nwith proper time scaling, we show that the time-varying joint empirical \nmeasures of the target feature vector and its estimates provided by the \nalgorithms will converge weakly to a deterministic measured-valued process that \ncan be characterized as the unique solution of a nonlinear PDE. Numerical \nsolutions of this PDE can be efficiently obtained. These solutions lead to \nprecise predictions of the performance of the algorithms, as many practical \nperformance metrics are linear functionals of the joint empirical measures. In \naddition to characterizing the dynamic performance of online learning \nalgorithms, our asymptotic analysis also provides useful insights. In \nparticular, in the high-dimensional limit, and due to exchangeability, the \noriginal coupled dynamics associated with the algorithms will be asymptotically \n\"decoupled\", with each coordinate independently solving a 1-D effective \nminimization problem via stochastic gradient descent. Exploiting this insight \nfor nonconvex optimization problems may prove an interesting line of future \nresearch. \n</p>"}, "author": "Chuang Wang, Jonathan Mattingly, Yue M. Lu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490892", "id": "tag:google.com,2005:reader/item/0000000346127f70", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Predicting Yelp Star Reviews Based on Network Structure with Deep Learning. (arXiv:1712.04350v1 [cs.LG])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04350"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04350", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we tackle the real-world problem of predicting Yelp \nstar-review rating based on business features (such as images, descriptions), \nuser features (average previous ratings), and, of particular interest, network \nproperties (which businesses has a user rated before). We compare multiple \nmodels on different sets of features -- from simple linear regression on \nnetwork features only to deep learning models on network and item features. \n</p> \n<p>In recent years, breakthroughs in deep learning have led to increased \naccuracy in common supervised learning tasks, such as image classification, \ncaptioning, and language understanding. However, the idea of combining deep \nlearning with network feature and structure appears to be novel. While the \nproblem of predicting future interactions in a network has been studied at \nlength, these approaches have often ignored either node-specific data or global \nstructure. \n</p> \n<p>We demonstrate that taking a mixed approach combining both node-level \nfeatures and network information can effectively be used to predict Yelp-review \nstar ratings. We evaluate on the Yelp dataset by splitting our data along the \ntime dimension (as would naturally occur in the real-world) and comparing our \nmodel against others which do no take advantage of the network structure and/or \ndeep learning. \n</p>"}, "author": "Luis Perez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490891", "id": "tag:google.com,2005:reader/item/0000000346127f78", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "CUSBoost: Cluster-based Under-sampling with Boosting for Imbalanced Classification. (arXiv:1712.04356v1 [cs.LG])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04356"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04356", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Class imbalance classification is a challenging research problem in data \nmining and machine learning, as most of the real-life datasets are often \nimbalanced in nature. Existing learning algorithms maximise the classification \naccuracy by correctly classifying the majority class, but misclassify the \nminority class. However, the minority class instances are representing the \nconcept with greater interest than the majority class instances in real-life \napplications. Recently, several techniques based on sampling methods \n(under-sampling of the majority class and over-sampling the minority class), \ncost-sensitive learning methods, and ensemble learning have been used in the \nliterature for classifying imbalanced datasets. In this paper, we introduce a \nnew clustering-based under-sampling approach with boosting (AdaBoost) \nalgorithm, called CUSBoost, for effective imbalanced classification. The \nproposed algorithm provides an alternative to RUSBoost (random under-sampling \nwith AdaBoost) and SMOTEBoost (synthetic minority over-sampling with AdaBoost) \nalgorithms. We evaluated the performance of CUSBoost algorithm with the \nstate-of-the-art methods based on ensemble learning like AdaBoost, RUSBoost, \nSMOTEBoost on 13 imbalance binary and multi-class datasets with various \nimbalance ratios. The experimental results show that the CUSBoost is a \npromising and effective approach for dealing with highly imbalanced datasets. \n</p>"}, "author": "Farshid Rayhan, Sajid Ahmed, Asif Mahbub, Md. Rafsan Jani, Swakkhar Shatabda, Dewan Md. Farid", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490890", "id": "tag:google.com,2005:reader/item/0000000346127f7e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Logo Synthesis and Manipulation with Clustered Generative Adversarial Networks. (arXiv:1712.04407v1 [cs.CV])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04407"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04407", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Designing a logo for a new brand is a lengthy and tedious back-and-forth \nprocess between a designer and a client. In this paper we explore to what \nextent machine learning can solve the creative task of the designer. For this, \nwe build a dataset -- LLD -- of 600k+ logos crawled from the world wide web. \nTraining Generative Adversarial Networks (GANs) for logo synthesis on such \nmulti-modal data is not straightforward and results in mode collapse for some \nstate-of-the-art methods. We propose the use of synthetic labels obtained \nthrough clustering to disentangle and stabilize GAN training. We are able to \ngenerate a high diversity of plausible logos and we demonstrate latent space \nexploration techniques to ease the logo design task in an interactive manner. \nMoreover, we validate the proposed clustered GAN training on CIFAR 10, \nachieving state-of-the-art Inception scores when using synthetic labels \nobtained via clustering the features of an ImageNet classifier. GANs can cope \nwith multi-modal data by means of synthetic labels achieved through clustering, \nand our results show the creative potential of such techniques for logo \nsynthesis and manipulation. Our dataset and models will be made publicly \navailable at https://data.vision.ee.ethz.ch/cvl/lld/. \n</p>"}, "author": "Alexander Sage, Eirikur Agustsson, Radu Timofte, Luc Van Gool", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490889", "id": "tag:google.com,2005:reader/item/0000000346127f85", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Integrated Model and Data Parallelism in Training Neural Networks. (arXiv:1712.04432v1 [cs.LG])", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04432"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04432", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504f8833a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504f8833a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a new integrated method of exploiting both model and data \nparallelism for the training of deep neural networks (DNNs) on large \ndistributed-memory computers using mini-batch stochastic gradient descent \n(SGD). Our goal is to find an efficient parallelization strategy for a fixed \nbatch size using $P$ processes. Our method is inspired by the \ncommunication-avoiding algorithms in numerical linear algebra. We see $P$ \nprocesses as logically divided into a $P_r \\times P_c$ grid where the $P_r$ \ndimension is implicitly responsible for model parallelism and the $P_c$ \ndimension is implicitly responsible for data parallelism. In practice, the \nintegrated matrix-based parallel algorithm encapsulates both types of \nparallelism automatically. We analyze the communication complexity and \nanalytically demonstrate that the lowest communication costs are often achieved \nneither with pure model parallelism nor with pure data parallelism. We also \nshow the positive effect of our approach in the computational performance of \nSGD based DNN training where the reduced number of processes responsible for \ndata parallelism result in \"fatter\" matrices that enable higher-throughput \nmatrix multiplication. \n</p>"}, "author": "Amir Gholami, Ariful Azad, Kurt Keutzer, Aydin Buluc", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490888", "id": "tag:google.com,2005:reader/item/0000000346127f8c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Is the Bellman residual a bad proxy?. (arXiv:1606.07636v3 [cs.LG] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1606.07636"}], "alternate": [{"href": "http://arxiv.org/abs/1606.07636", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper aims at theoretically and empirically comparing two standard \noptimization criteria for Reinforcement Learning: i) maximization of the mean \nvalue and ii) minimization of the Bellman residual. For that purpose, we place \nourselves in the framework of policy search algorithms, that are usually \ndesigned to maximize the mean value, and derive a method that minimizes the \nresidual $\\|T_* v_\\pi - v_\\pi\\|_{1,\\nu}$ over policies. A theoretical analysis \nshows how good this proxy is to policy optimization, and notably that it is \nbetter than its value-based counterpart. We also propose experiments on \nrandomly generated generic Markov decision processes, specifically designed for \nstudying the influence of the involved concentrability coefficient. They show \nthat the Bellman residual is generally a bad proxy to policy optimization and \nthat directly maximizing the mean value is much better, despite the current \nlack of deep theoretical analysis. This might seem obvious, as directly \naddressing the problem of interest is usually better, but given the prevalence \nof (projected) Bellman residual minimization in value-based reinforcement \nlearning, we believe that this question is worth to be considered. \n</p>"}, "author": "Matthieu Geist, Bilal Piot, Olivier Pietquin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490887", "id": "tag:google.com,2005:reader/item/0000000346127f9b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Forward and Reverse Gradient-Based Hyperparameter Optimization. (arXiv:1703.01785v3 [stat.ML] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.01785"}], "alternate": [{"href": "http://arxiv.org/abs/1703.01785", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study two procedures (reverse-mode and forward-mode) for computing the \ngradient of the validation error with respect to the hyperparameters of any \niterative learning algorithm such as stochastic gradient descent. These \nprocedures mirror two methods of computing gradients for recurrent neural \nnetworks and have different trade-offs in terms of running time and space \nrequirements. Our formulation of the reverse-mode procedure is linked to \nprevious work by Maclaurin et al. [2015] but does not require reversible \ndynamics. The forward-mode procedure is suitable for real-time hyperparameter \nupdates, which may significantly speed up hyperparameter optimization on large \ndatasets. We present experiments on data cleaning and on learning task \ninteractions. We also present one large-scale experiment where the use of \nprevious gradient-based methods would be prohibitive. \n</p>"}, "author": "Luca Franceschi, Michele Donini, Paolo Frasconi, Massimiliano Pontil", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490886", "id": "tag:google.com,2005:reader/item/0000000346127fa6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Broadband DOA estimation using Convolutional neural networks trained with noise signals. (arXiv:1705.00919v2 [cs.SD] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.00919"}], "alternate": [{"href": "http://arxiv.org/abs/1705.00919", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A convolution neural network (CNN) based classification method for broadband \nDOA estimation is proposed, where the phase component of the short-time Fourier \ntransform coefficients of the received microphone signals are directly fed into \nthe CNN and the features required for DOA estimation are learnt during \ntraining. Since only the phase component of the input is used, the CNN can be \ntrained with synthesized noise signals, thereby making the preparation of the \ntraining data set easier compared to using speech signals. Through experimental \nevaluation, the ability of the proposed noise trained CNN framework to \ngeneralize to speech sources is demonstrated. In addition, the robustness of \nthe system to noise, small perturbations in microphone positions, as well as \nits ability to adapt to different acoustic conditions is investigated using \nexperiments with simulated and real data. \n</p>"}, "author": "Soumitro Chakrabarty, Emanu&#xeb;l. A. P. Habets", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490885", "id": "tag:google.com,2005:reader/item/0000000346127faf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "FDR-Corrected Sparse Canonical Correlation Analysis with Applications to Imaging Genomics. (arXiv:1705.04312v3 [stat.ME] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.04312"}], "alternate": [{"href": "http://arxiv.org/abs/1705.04312", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Reducing the number of false positive discoveries is presently one of the \nmost pressing issues in the life sciences. It is of especially great importance \nfor many applications in neuroimaging and genomics, where datasets are \ntypically high-dimensional, which means that the number of explanatory \nvariables exceeds the sample size. The false discovery rate (FDR) is a \ncriterion that can be employed to address that issue. Thus it has gained great \npopularity as a tool for testing multiple hypotheses. Canonical correlation \nanalysis (CCA) is a statistical technique that is used to make sense of the \ncross-correlation of two sets of measurements collected on the same set of \nsamples (e.g., brain imaging and genomic data for the same mental illness \npatients), and sparse CCA extends the classical method to high-dimensional \nsettings. Here we propose a way of applying the FDR concept to sparse CCA, and \na method to control the FDR. The proposed FDR correction directly influences \nthe sparsity of the solution, adapting it to the unknown true sparsity level. \nTheoretical derivation as well as simulation studies show that our procedure \nindeed keeps the FDR of the canonical vectors below a user-specified target \nlevel. We apply the proposed method to an imaging genomics dataset from the \nPhiladelphia Neurodevelopmental Cohort. Our results link the brain connectivity \nprofiles derived from brain activity during an emotion identification task, as \nmeasured by functional magnetic resonance imaging (fMRI), to the corresponding \nsubjects' genomic data. \n</p>"}, "author": "Alexej Gossmann, Pascal Zille, Vince Calhoun, Yu-Ping Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490884", "id": "tag:google.com,2005:reader/item/0000000346127fb9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Online Factorization and Partition of Complex Networks From Random Walks. (arXiv:1705.07881v4 [cs.LG] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.07881"}], "alternate": [{"href": "http://arxiv.org/abs/1705.07881", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Finding the reduced-dimensional structure is critical to understanding \ncomplex networks. Existing approaches such as spectral clustering are \napplicable only when the full network is explicitly observed. In this paper, we \nfocus on the online factorization and partition of implicit large-scale \nnetworks based on observations from an associated random walk. We formulate \nthis into a nonconvex stochastic factorization problem and propose an efficient \nand scalable stochastic generalized Hebbian algorithm. The algorithm is able to \nprocess dependent state-transition data dynamically generated by the underlying \nnetwork and learn a low-dimensional representation for each vertex. By applying \na diffusion approximation analysis, we show that the continuous-time limiting \nprocess of the stochastic algorithm converges globally to the \"principal \ncomponents\" of the Markov chain and achieves a nearly optimal sample \ncomplexity. Once given the learned low-dimensional representations, we further \napply clustering techniques to recover the network partition. We show that when \nthe associated Markov process is lumpable, one can recover the partition \nexactly with high probability. We apply the proposed approach to model the \ntraffic flow of Manhattan as city-wide random walks. By using our algorithm to \nanalyze the taxi trip data, we discover a latent partition of the Manhattan \ncity that closely matches the traffic dynamics. \n</p>"}, "author": "Lin F. Yang, Vladimir Braverman, Tuo Zhao, Mengdi Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490883", "id": "tag:google.com,2005:reader/item/0000000346127fbf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Variational approach for learning Markov processes from time series data. (arXiv:1707.04659v2 [stat.ML] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.04659"}], "alternate": [{"href": "http://arxiv.org/abs/1707.04659", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Inference, prediction and control of complex dynamical systems from time \nseries is important in many areas, including financial markets, power grid \nmanagement, climate and weather modeling, or molecular dynamics. The analysis \nof such highly nonlinear dynamical systems is facilitated by the fact that we \ncan often find a (generally nonlinear) transformation of the system coordinates \nto features in which the dynamics can be excellently approximated by a linear \nMarkovian model. Moreover, the large number of system variables often change \ncollectively on large time- and length-scales, facilitating a low-dimensional \nanalysis in feature space. In this paper, we introduce a variational approach \nfor Markov processes (VAMP) that allows us to find optimal feature mappings and \noptimal Markovian models of the dynamics from given time series data. The key \ninsight is that the best linear model can be obtained from the top singular \ncomponents of the Koopman operator. This leads to the definition of a family of \nscore functions called VAMP-r which can be calculated from data, and can be \nemployed to optimize a Markovian model. In addition, based on the relationship \nbetween the variational scores and approximation errors of Koopman operators, \nwe propose a new VAMP-E score, which can be applied to cross-validation for \nhyper-parameter optimization and model selection in VAMP. VAMP is valid for \nboth reversible and nonreversible processes and for stationary and \nnon-stationary processes or realizations. \n</p>"}, "author": "Hao Wu, Frank No&#xe9;", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490882", "id": "tag:google.com,2005:reader/item/0000000346127fc8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Primal-Dual Proximal ExtraGradient Descent for Compositely Regularized Optimization. (arXiv:1708.05978v3 [cs.LG] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.05978"}], "alternate": [{"href": "http://arxiv.org/abs/1708.05978", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider a wide range of regularized stochastic minimization problems with \ntwo regularization terms, one of which is composed with a linear function. This \noptimization model abstracts a number of important applications in artificial \nintelligence and machine learning, such as fused Lasso, fused logistic \nregression, and a class of graph-guided regularized minimization. The \ncomputational challenges of this model are in two folds. On one hand, the \nclosed-form solution of the proximal mapping associated with the composed \nregularization term or the expected objective function is not available. On the \nother hand, the calculation of the full gradient of the expectation in the \nobjective is very expensive when the number of input data samples is \nconsiderably large. To address these issues, we propose a stochastic variant of \nextra-gradient type methods, namely \\textsf{Stochastic Primal-Dual Proximal \nExtraGradient descent (SPDPEG)}, and analyze its convergence property for both \nconvex and strongly convex objectives. For general convex objectives, the \nuniformly average iterates generated by \\textsf{SPDPEG} converge in expectation \nwith $O(1/\\sqrt{t})$ rate. While for strongly convex objectives, the uniformly \nand non-uniformly average iterates generated by \\textsf{SPDPEG} converge with \n$O(\\log(t)/t)$ and $O(1/t)$ rates, respectively. The order of the rate of the \nproposed algorithm is known to match the best convergence rate for first-order \nstochastic algorithms. Experiments on fused logistic regression and \ngraph-guided regularized logistic regression problems show that the proposed \nalgorithm performs very efficiently and consistently outperforms other \ncompeting algorithms. \n</p>"}, "author": "Tianyi Lin, Linbo Qiao, Teng Zhang, Jiashi Feng, Bofeng Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490881", "id": "tag:google.com,2005:reader/item/0000000346127fd1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Merging Path Plot: adaptive fusing of k-groups with likelihood-based model selection. (arXiv:1709.04412v2 [stat.ML] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.04412"}], "alternate": [{"href": "http://arxiv.org/abs/1709.04412", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>There are many statistical tests that verify the null hypothesis: the \nvariable of interest has the same distribution among k-groups. But once the \nnull hypothesis is rejected, how to present the structure of dissimilarity \nbetween groups? In this article, we introduce The Merging Path Plot - a \nmethodology, and factorMerger - an R package, for exploration and visualization \nof k-group dissimilarities. Comparison of k-groups is one of the most important \nissues in exploratory analyses and it has zillions of applications. The \nclassical solution is to test a~null hypothesis that observations from all \ngroups come from the same distribution. If the global null hypothesis is \nrejected, a~more detailed analysis of differences among pairs of groups is \nperformed. The traditional approach is to use pairwise post hoc tests in order \nto verify which groups differ significantly. However, this approach fails with \na large number of groups in both interpretation and visualization layer. \nThe~Merging Path Plot methodology solves this problem by using an \neasy-to-understand description of dissimilarity among groups based on \nLikelihood Ratio Test (LRT) statistic. \n</p>"}, "author": "Agnieszka Sitko, Przemyslaw Biecek", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490880", "id": "tag:google.com,2005:reader/item/0000000346127fd8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec. (arXiv:1710.02971v3 [cs.SI] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.02971"}], "alternate": [{"href": "http://arxiv.org/abs/1710.02971", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Since the invention of word2vec, the skip-gram model has significantly \nadvanced the research of network embedding, such as the recent emergence of the \nDeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of \nthe aforementioned models with negative sampling can be unified into the matrix \nfactorization framework with closed forms. Our analysis and proofs reveal that: \n(1) DeepWalk empirically produces a low-rank transformation of a network's \nnormalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk \nwhen the size of vertices' context is set to one; (3) As an extension of LINE, \nPTE can be viewed as the joint factorization of multiple networks' Laplacians; \n(4) node2vec is factorizing a matrix related to the stationary distribution and \ntransition probability tensor of a 2nd-order random walk. We further provide \nthe theoretical connections between skip-gram based network embedding \nalgorithms and the theory of graph Laplacian. Finally, we present the NetMF \nmethod as well as its approximation algorithm for computing network embedding. \nOur method offers significant improvements over DeepWalk and LINE for \nconventional network mining tasks. This work lays the theoretical foundation \nfor skip-gram based network embedding methods, leading to a better \nunderstanding of latent network representation learning. \n</p>"}, "author": "Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490879", "id": "tag:google.com,2005:reader/item/0000000346127fdf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Grouping-By-ID: Guarding Against Adversarial Domain Shifts. (arXiv:1710.11469v2 [stat.ML] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11469"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11469", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504f88564\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504f88564&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>When training a deep network for image classification, one can broadly \ndistinguish between two types of latent features that will drive the \nclassification. Following Gong et al. (2016), we can divide features into (i) \n\"core\" features $X^{ci}$ whose distribution $P(X^{ci} | Y)$ does not change \nsubstantially across domains and (ii) \"style\" or \"orthogonal\" features \n$X^\\perp$ whose distribution $P(X^\\perp | Y)$ can change substantially across \ndomains. These latter orthogonal features would generally include features such \nas position or brightness but also more complex ones like hair color or posture \nfor images of persons. We try to guard against future adversarial domain shifts \nby ideally just using the \"core\" features for classification. In contrast to \nprevious work, we assume that the domain itself is not observed and hence a \nlatent variable, i.e. we cannot directly see the distributional change of \nfeatures across different domains. We do assume, however, that we can sometimes \nobserve a so-called ID variable. E.g. we might know that two images show the \nsame person, with ID referring to the identity of the person. The method \nrequires only a small fraction of images to have an ID variable. We provide a \ncausal framework for the problem by adding the ID variable to the model of Gong \net al. (2016). If two or more samples share the same class and identifier, then \nwe treat those samples as counterfactuals under different interventions on the \northogonal features. Using this grouping-by-ID approach, we regularize the \nnetwork to provide near constant output across samples that share the same ID \nby penalizing with an appropriate graph Laplacian. This substantially improves \nperformance in settings where domains change in terms of image quality, \nbrightness, color or posture and movement. We show links to questions of \ninterpretability, fairness, transfer learning and adversarial examples. \n</p>"}, "author": "Christina Heinze-Deml, Nicolai Meinshausen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490878", "id": "tag:google.com,2005:reader/item/0000000346127fe3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Oversampling for Imbalanced Learning Based on K-Means and SMOTE. (arXiv:1711.00837v2 [cs.LG] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00837"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00837", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504fedb2a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504fedb2a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Learning from class-imbalanced data continues to be a common and challenging \nproblem in supervised learning as standard classification algorithms are \ndesigned to handle balanced class distributions. While different strategies \nexist to tackle this problem, methods which generate artificial data to achieve \na balanced class distribution are more versatile than modifications to the \nclassification algorithm. Such techniques, called oversamplers, modify the \ntraining data, allowing any classifier to be used with class-imbalanced \ndatasets. Many algorithms have been proposed for this task, but most are \ncomplex and tend to generate unnecessary noise. This work presents a simple and \neffective oversampling method based on k-means clustering and SMOTE \noversampling, which avoids the generation of noise and effectively overcomes \nimbalances between and within classes. Empirical results of extensive \nexperiments with 71 datasets show that training data oversampled with the \nproposed method improves classification results. Moreover, k-means SMOTE \nconsistently outperforms other popular oversampling methods. An implementation \nis made available in the python programming language. \n</p>"}, "author": "Felix Last, Georgios Douzas, Fernando Bacao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490877", "id": "tag:google.com,2005:reader/item/0000000346127fe9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Virtual Adversarial Ladder Networks For Semi-supervised Learning. (arXiv:1711.07476v2 [cs.LG] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07476"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07476", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Semi-supervised learning (SSL) partially circumvents the high cost of \nlabeling data by augmenting a small labeled dataset with a large and relatively \ncheap unlabeled dataset drawn from the same distribution. This paper offers a \nnovel interpretation of two deep learning-based SSL approaches, ladder networks \nand virtual adversarial training (VAT), as applying distributional smoothing to \ntheir respective latent spaces. We propose a class of models that fuse these \napproaches. We achieve near-supervised accuracy with high consistency on the \nMNIST dataset using just 5 labels per class: our best model, ladder with \nlayer-wise virtual adversarial noise (LVAN-LW), achieves 1.42% +/- 0.12 average \nerror rate on the MNIST test set, in comparison with 1.62% +/- 0.65 reported \nfor the ladder network. On adversarial examples generated with L2-normalized \nfast gradient method, LVAN-LW trained with 5 examples per class achieves \naverage error rate 2.4% +/- 0.3 compared to 68.6% +/- 6.5 for the ladder \nnetwork and 9.9% +/- 7.5 for VAT. \n</p>"}, "author": "Saki Shinoda, Daniel E. Worrall, Gabriel J. Brostow", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490876", "id": "tag:google.com,2005:reader/item/0000000346127ff0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Regret Analysis for Continuous Dueling Bandit. (arXiv:1711.07693v2 [stat.ML] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07693"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07693", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The dueling bandit is a learning framework wherein the feedback information \nin the learning process is restricted to a noisy comparison between a pair of \nactions. In this research, we address a dueling bandit problem based on a cost \nfunction over a continuous space. We propose a stochastic mirror descent \nalgorithm and show that the algorithm achieves an $O(\\sqrt{T\\log T})$-regret \nbound under strong convexity and smoothness assumptions for the cost function. \nSubsequently, we clarify the equivalence between regret minimization in dueling \nbandit and convex optimization for the cost function. Moreover, when \nconsidering a lower bound in convex optimization, our algorithm is shown to \nachieve the optimal convergence rate in convex optimization and the optimal \nregret in dueling bandit except for a logarithmic factor. \n</p>"}, "author": "Wataru Kumagai", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513141858491", "timestampUsec": "1513141858490875", "id": "tag:google.com,2005:reader/item/0000000346127ff6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Proceedings of NIPS 2017 Workshop on Machine Learning for the Developing World. (arXiv:1711.09522v2 [stat.ML] UPDATED)", "published": 1513141859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.09522"}], "alternate": [{"href": "http://arxiv.org/abs/1711.09522", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This is the Proceedings of NIPS 2017 Workshop on Machine Learning for the \nDeveloping World, held in Long Beach, California, USA on December 8, 2017 \n</p>"}, "author": "Maria De-Arteaga, William Herlands", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337172", "id": "tag:google.com,2005:reader/item/00000003454dcee2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Peephole: Predicting Network Performance Before Training. (arXiv:1712.03351v1 [cs.LG])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03351"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03351", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The quest for performant networks has been a significant force that drives \nthe advancements of deep learning in recent years. While rewarding, improving \nnetwork design has never been an easy journey. The large design space combined \nwith the tremendous cost required for network training poses a major obstacle \nto this endeavor. In this work, we propose a new approach to this problem, \nnamely, predicting the performance of a network before training, based on its \narchitecture. Specifically, we develop a unified way to encode individual \nlayers into vectors and bring them together to form an integrated description \nvia LSTM. Taking advantage of the recurrent network's strong expressive power, \nthis method can reliably predict the performances of various network \narchitectures. Our empirical studies showed that it not only achieved accurate \npredictions but also produced consistent rankings across datasets -- a key \ndesideratum in performance prediction. \n</p>"}, "author": "Boyang Deng, Junjie Yan, Dahua Lin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337171", "id": "tag:google.com,2005:reader/item/00000003454dcee5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "An Architecture Combining Convolutional Neural Network (CNN) and Support Vector Machine (SVM) for Image Classification. (arXiv:1712.03541v1 [cs.CV])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03541"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03541", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolutional neural networks (CNNs) are similar to \"ordinary\" neural \nnetworks in the sense that they are made up of hidden layers consisting of \nneurons with \"learnable\" parameters. These neurons receive inputs, performs a \ndot product, and then follows it with a non-linearity. The whole network \nexpresses the mapping between raw image pixels and their class scores. \nConventionally, the Softmax function is the classifier used at the last layer \nof this network. However, there have been studies (Alalshekmubarak and Smith, \n2013; Agarap, 2017; Tang, 2013) conducted to challenge this norm. The cited \nstudies introduce the usage of linear support vector machine (SVM) in an \nartificial neural network architecture. This project is yet another take on the \nsubject, and is inspired by (Tang, 2013). Empirical data has shown that the \nCNN-SVM model was able to achieve a test accuracy of ~99.04% using the MNIST \ndataset (LeCun, Cortes, and Burges, 2010). On the other hand, the CNN-Softmax \nwas able to achieve a test accuracy of ~99.23% using the same dataset. Both \nmodels were also tested on the recently-published Fashion-MNIST dataset (Xiao, \nRasul, and Vollgraf, 2017), which is suppose to be a more difficult image \nclassification dataset than MNIST (Zalandoresearch, 2017). This proved to be \nthe case as CNN-SVM reached a test accuracy of ~90.72%, while the CNN-Softmax \nreached a test accuracy of ~91.86%. The said results may be improved if data \npreprocessing techniques were employed on the datasets, and if the base CNN \nmodel was a relatively more sophisticated than the one used in this study. \n</p>"}, "author": "Abien Fred Agarap", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337170", "id": "tag:google.com,2005:reader/item/00000003454dcee8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "On Convergence and Stability of GANs. (arXiv:1705.07215v5 [cs.AI] UPDATED)", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.07215"}], "alternate": [{"href": "http://arxiv.org/abs/1705.07215", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose studying GAN training dynamics as regret minimization, which is in \ncontrast to the popular view that there is consistent minimization of a \ndivergence between real and generated distributions. We analyze the convergence \nof GAN training from this new point of view to understand why mode collapse \nhappens. We hypothesize the existence of undesirable local equilibria in this \nnon-convex game to be responsible for mode collapse. We observe that these \nlocal equilibria often exhibit sharp gradients of the discriminator function \naround some real data points. We demonstrate that these degenerate local \nequilibria can be avoided with a gradient penalty scheme called DRAGAN. We show \nthat DRAGAN enables faster training, achieves improved stability with fewer \nmode collapses, and leads to generator networks with better modeling \nperformance across a variety of architectures and objective functions. \n</p>"}, "author": "Naveen Kodali, Jacob Abernethy, James Hays, Zsolt Kira", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337169", "id": "tag:google.com,2005:reader/item/00000003454dcef1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "A Flow Model of Neural Networks. (arXiv:1708.06257v2 [cs.LG] UPDATED)", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.06257"}], "alternate": [{"href": "http://arxiv.org/abs/1708.06257", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Based on a natural connection between ResNet and transport equation or its \ncharacteristic equation, we propose a continuous flow model for both ResNet and \nplain net. Through this continuous model, a ResNet can be explicitly \nconstructed as a refinement of a plain net. The flow model provides an \nalternative perspective to understand phenomena in deep neural networks, such \nas why it is necessary and sufficient to use 2-layer blocks in ResNets, why \ndeeper is better, and why ResNets are even deeper, and so on. It also opens a \ngate to bring in more tools from the huge area of differential equations. \n</p>"}, "author": "Zhen Li, Zuoqiang Shi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337168", "id": "tag:google.com,2005:reader/item/00000003454dcef6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "S-Shaped vs. V-Shaped Transfer Functions for Antlion Optimization Algorithm in Feature Selection Problems. (arXiv:1712.03223v1 [cs.AI])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03223"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03223", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Feature selection is an important preprocessing step for classification \nproblems. It deals with selecting near optimal features in the original \ndataset. Feature selection is an NP-hard problem, so meta-heuristics can be \nmore efficient than exact methods. In this work, Ant Lion Optimizer (ALO), \nwhich is a recent metaheuristic algorithm, is employed as a wrapper feature \nselection method. Six variants of ALO are proposed where each employ a transfer \nfunction to map a continuous search space to a discrete search space. The \nperformance of the proposed approaches is tested on eighteen UCI datasets and \ncompared to a number of existing approaches in the literature: Particle Swarm \nOptimization, Gravitational Search Algorithm, and two existing ALO-based \napproaches. Computational experiments show that the proposed approaches \nefficiently explore the feature space and select the most informative features, \nwhich help to improve the classification accuracy. \n</p>"}, "author": "Majdi Mafarja, Seyedali Mirjalili", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337167", "id": "tag:google.com,2005:reader/item/00000003454dcefd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Social Emotion Mining Techniques for Facebook Posts Reaction Prediction. (arXiv:1712.03249v1 [cs.AI])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03249"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03249", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504fedece\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504fedece&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>As of February 2016 Facebook allows users to express their experienced \nemotions about a post by using five so-called `reactions'. This research paper \nproposes and evaluates alternative methods for predicting these reactions to \nuser posts on public pages of firms/companies (like supermarket chains). For \nthis purpose, we collected posts (and their reactions) from Facebook pages of \nlarge supermarket chains and constructed a dataset which is available for other \nresearches. In order to predict the distribution of reactions of a new post, \nneural network architectures (convolutional and recurrent neural networks) were \ntested using pretrained word embeddings. Results of the neural networks were \nimproved by introducing a bootstrapping approach for sentiment and emotion \nmining on the comments for each post. The final model (a combination of neural \nnetwork and a baseline emotion miner) is able to predict the reaction \ndistribution on Facebook posts with a mean squared error (or misclassification \nrate) of 0.135. \n</p>"}, "author": "Florian Krebs, Bruno Lubascher, Tobias Moers, Pieter Schaap, Gerasimos Spanakis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337166", "id": "tag:google.com,2005:reader/item/00000003454dcf01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Nintendo Super Smash Bros. Melee: An \"Untouchable\" Agent. (arXiv:1712.03280v1 [cs.AI])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03280"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03280", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Nintendo's Super Smash Bros. Melee fighting game can be emulated on modern \nhardware allowing us to inspect internal memory states, such as character \npositions. We created an AI that avoids being hit by training using these \ninternal memory states and outputting controller button presses. After training \non a month's worth of Melee matches, our best agent learned to avoid the \ntoughest AI built into the game for a full minute 74.6% of the time. \n</p>"}, "author": "Ben Parr, Deepak Dilipkumar, Yuan Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337165", "id": "tag:google.com,2005:reader/item/00000003454dcf07", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian Q-learning with Assumed Density Filtering. (arXiv:1712.03333v1 [cs.LG])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03333"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03333", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>While off-policy temporal difference methods have been broadly used in \nreinforcement learning due to their efficiency and simple implementation, their \nBayesian counterparts have been relatively understudied. This is mainly because \nthe max operator in the Bellman optimality equation brings non-linearity and \ninconsistent distributions over value function. In this paper, we introduce a \nnew Bayesian approach to off-policy TD methods using Assumed Density Filtering, \ncalled ADFQ, which updates beliefs on action-values (Q) through an online \nBayesian inference method. Uncertainty measures in the beliefs not only are \nused in exploration but they provide a natural regularization in the belief \nupdates. We also present a connection between ADFQ and Q-learning. Our \nempirical results show the proposed ADFQ algorithms outperform comparing \nalgorithms in several task domains. Moreover, our algorithms improve general \ndrawbacks in BRL such as computational complexity, usage of uncertainty, and \nnonlinearity. \n</p>"}, "author": "Heejin Jeong, Daniel D. Lee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337164", "id": "tag:google.com,2005:reader/item/00000003454dcf0c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "NAG: Network for Adversary Generation. (arXiv:1712.03390v1 [cs.CV])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03390"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03390", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Adversarial perturbations can pose a serious threat for deploying machine \nlearning systems. Recent works have shown existence of image-agnostic \nperturbations that can fool classifiers over most natural images. Existing \nmethods present optimization approaches that solve for a fooling objective with \nan imperceptibility constraint to craft the perturbations. However, for a given \nclassifier, they generate one perturbation at a time, which is a single \ninstance from the manifold of adversarial perturbations. Also, in order to \nbuild robust models, it is essential to explore the manifold of adversarial \nperturbations. In this paper, we propose for the first time, a generative \napproach to model the distribution of adversarial perturbations. The \narchitecture of the proposed model is inspired from that of GANs and is trained \nusing fooling and diversity objectives. Our trained generator network attempts \nto capture the distribution of adversarial perturbations for a given classifier \nand readily generates a wide variety of such perturbations. Our experimental \nevaluation demonstrates that perturbations crafted by our model (i) achieve \nstate-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver \nexcellent cross model generalizability. Our work can be deemed as an important \nstep in the process of inferring about the complex manifolds of adversarial \nperturbations. \n</p>"}, "author": "Konda Reddy Mopuri, Utkarsh Ojha, Utsav Garg, R. Venkatesh Babu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337163", "id": "tag:google.com,2005:reader/item/00000003454dcf12", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Efficient Implementation of the Room Simulator for Training Deep Neural Network Acoustic Models. (arXiv:1712.03439v1 [cs.SD])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03439"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03439", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we describe how to efficiently implement an acoustic room \nsimulator to generate large-scale simulated data for training deep neural \nnetworks. Even though Google Room Simulator in [1] was shown to be quite \neffective in reducing the Word Error Rates (WERs) for far-field applications by \ngenerating simulated far-field training sets, it requires a very large number \nof Fast Fourier Transforms (FFTs) of large size. Room Simulator in [1] used \napproximately 80 percent of Central Processing Unit (CPU) usage in our CPU + \nGraphics Processing Unit (GPU) training architecture [2]. In this work, we \nimplement an efficient OverLap Addition (OLA) based filtering using the \nopen-source FFTW3 library. Further, we investigate the effects of the Room \nImpulse Response (RIR) lengths. Experimentally, we conclude that we can cut the \ntail portions of RIRs whose power is less than 20 dB below the maximum power \nwithout sacrificing the speech recognition accuracy. However, we observe that \ncutting RIR tail more than this threshold harms the speech recognition accuracy \nfor rerecorded test sets. Using these approaches, we were able to reduce CPU \nusage for the room simulator portion down to 9.69 percent in CPU/GPU training \narchitecture. Profiling result shows that we obtain 22.4 times speed-up on a \nsingle machine and 37.3 times speed up on Google's distributed training \ninfrastructure. \n</p>"}, "author": "Chanwoo Kim, Ehsan Variani, Arun Narayanan, Michiel Bacchiani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337162", "id": "tag:google.com,2005:reader/item/00000003454dcf15", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "A Cascade Architecture for Keyword Spotting on Mobile Devices. (arXiv:1712.03603v1 [cs.SD])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03603"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03603", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a cascade architecture for keyword spotting with speaker \nverification on mobile devices. By pairing a small computational footprint with \nspecialized digital signal processing (DSP) chips, we are able to achieve low \npower consumption while continuously listening for a keyword. \n</p>"}, "author": "Alexander Gruenstein, Raziel Alvarez, Chris Thornton, Mohammadali Ghodrat", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337161", "id": "tag:google.com,2005:reader/item/00000003454dcf19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Towards Fully Environment-Aware UAVs: Real-Time Path Planning with Online 3D Wind Field Prediction in Complex Terrain. (arXiv:1712.03608v1 [cs.RO])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03608"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03608", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Today, low-altitude fixed-wing Unmanned Aerial Vehicles (UAVs) are largely \nlimited to primitively follow user-defined waypoints. To allow fully-autonomous \nremote missions in complex environments, real-time environment-aware navigation \nis required both with respect to terrain and strong wind drafts. This paper \npresents two relevant initial contributions: First, the literature's first-ever \n3D wind field prediction method which can run in real time onboard a UAV is \npresented. The approach retrieves low-resolution global weather data, and uses \npotential flow theory to adjust the wind field such that terrain boundaries, \nmass conservation, and the atmospheric stratification are observed. A \ncomparison with 1D LIDAR data shows an overall wind error reduction of 23% with \nrespect to the zero-wind assumption that is mostly used for UAV path planning \ntoday. However, given that the vertical winds are not resolved accurately \nenough further research is required and identified. Second, a sampling-based \npath planner that considers the aircraft dynamics in non-uniform wind \niteratively via Dubins airplane paths is presented. Performance optimizations, \ne.g. obstacle-aware sampling and fast 2.5D-map collision checks, render the \nplanner 50% faster than the Open Motion Planning Library (OMPL) implementation. \nTest cases in Alpine terrain show that the wind-aware planning performs up to \n50x less iterations than shortest-path planning and is thus slower in low \nwinds, but that it tends to deliver lower-cost paths in stronger winds. More \nimportantly, in contrast to the shortest-path planner, it always delivers \ncollision-free paths. Overall, our initial research demonstrates the \nfeasibility of 3D wind field prediction from a UAV and the advantages of \nwind-aware planning. This paves the way for follow-up research on \nfully-autonomous environment-aware navigation of UAVs in real-life missions and \ncomplex terrain. \n</p>"}, "author": "Philipp Oettershagen, Florian Achermann, Benjamin M&#xfc;ller, Daniel Schneider, Roland Siegwart", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337160", "id": "tag:google.com,2005:reader/item/00000003454dcf28", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Robust Deep Reinforcement Learning with Adversarial Attacks. (arXiv:1712.03632v1 [cs.LG])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03632"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03632", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes adversarial attacks for Reinforcement Learning (RL) and \nthen improves the robustness of Deep Reinforcement Learning algorithms (DRL) to \nparameter uncertainties with the help of these attacks. We show that even a \nnaively engineered attack successfully degrades the performance of DRL \nalgorithm. We further improve the attack using gradient information of an \nengineered loss function which leads to further degradation in performance. \nThese attacks are then leveraged during training to improve the robustness of \nRL within robust control framework. We show that this adversarial training of \nDRL algorithms like Deep Double Q learning and Deep Deterministic Policy \nGradients leads to significant increase in robustness to parameter variations \nfor RL benchmarks such as Cart-pole, Mountain Car, Hopper and Half Cheetah \nenvironment. \n</p>"}, "author": "Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, Girish Chowdhary", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337159", "id": "tag:google.com,2005:reader/item/00000003454dcf30", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Novel model-based heuristics for energy optimal motion planning of an autonomous vehicle using A*. (arXiv:1712.03719v1 [math.OC])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03719"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03719", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Predictive motion planning is the key to achieve energy-efficient driving, \nwhich is one of the main benefits of automated driving. Researchers have been \nstudying the planning of velocity trajectories, a simpler form of motion \nplanning, for over a decade now and many different methods are available. \nDynamic programming has shown to be the most common choice due to its numerical \nbackground and ability to include nonlinear constraints and models. Although \nplanning of optimal trajectory is done in a systematic way, dynamic programming \ndoesn't use any knowledge about the considered problem to guide the exploration \nand therefore explores all possible trajectories. A* is an algorithm which \nenables using knowledge about the problem to guide the exploration to the most \npromising solutions first. Knowledge has to be represented in a form of a \nheuristic function, which gives an optimistic estimate of cost for \ntransitioning between two states, which is not a straightforward task. This \npaper presents a novel heuristics incorporating air drag and auxiliary power as \nwell as operational costs of the vehicle, besides kinetic and potential energy \nand rolling resistance known in the literature. Furthermore, optimal cruising \nvelocity, which depends on vehicle aerodynamic properties and auxiliary power, \nis derived. Results are compared for different variants of heuristic functions \nand dynamic programming as well. \n</p>"}, "author": "Zlatan Ajanovic, Michael Stolz, Martin Horn", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337158", "id": "tag:google.com,2005:reader/item/00000003454dcf35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Cogniculture: Towards a Better Human-Machine Co-evolution. (arXiv:1712.03724v1 [cs.CY])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03724"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03724", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Research in Artificial Intelligence is breaking technology barriers every \nday. New algorithms and high performance computing are making things possible \nwhich we could only have imagined earlier. Though the enhancements in AI are \nmaking life easier for human beings day by day, there is constant fear that AI \nbased systems will pose a threat to humanity. People in AI community have \ndiverse set of opinions regarding the pros and cons of AI mimicking human \nbehavior. Instead of worrying about AI advancements, we propose a novel idea of \ncognitive agents, including both human and machines, living together in a \ncomplex adaptive ecosystem, collaborating on human computation for producing \nessential social goods while promoting sustenance, survival and evolution of \nthe agents' life cycle. We highlight several research challenges and technology \nbarriers in achieving this goal. We propose a governance mechanism around this \necosystem to ensure ethical behaviors of all cognitive agents. Along with a \nnovel set of use-cases of Cogniculture, we discuss the road map ahead for this \njourney. \n</p>"}, "author": "Rakesh R Pimplikar, Kushal Mukherjee, Gyana Parija, Harit Vishwakarma, Ramasuri Narayanam, Sarthak Ahuja, Rohith D Vallam, Ritwik Chaudhuri, Joydeep Mondal", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337157", "id": "tag:google.com,2005:reader/item/00000003454dcf3b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Artificial Intelligence and Statistics. (arXiv:1712.03779v1 [stat.ML])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03779"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03779", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32504fee161\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32504fee161&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Artificial intelligence (AI) is intrinsically data-driven. It calls for the \napplication of statistical concepts through human-machine collaboration during \ngeneration of data, development of algorithms, and evaluation of results. This \npaper discusses how such human-machine collaboration can be approached through \nthe statistical concepts of population, question of interest, \nrepresentativeness of training data, and scrutiny of results (PQRS). The PQRS \nworkflow provides a conceptual framework for integrating statistical ideas with \nhuman input into AI products and research. These ideas include experimental \ndesign principles of randomization and local control as well as the principle \nof stability to gain reproducibility and interpretability of algorithms and \ndata results. We discuss the use of these principles in the contexts of \nself-driving cars, automated medical diagnoses, and examples from the authors' \ncollaborative research. \n</p>"}, "author": "Bin Yu, Karl Kumbier", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337156", "id": "tag:google.com,2005:reader/item/00000003454dcf41", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Learning Nested Sparse Structures in Deep Neural Networks. (arXiv:1712.03781v1 [cs.CV])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03781"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03781", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250507278a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250507278a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Recently, there have been increasing demands to construct compact deep \narchitectures to remove unnecessary redundancy and to improve the inference \nspeed. While many recent works focus on reducing the redundancy by eliminating \nunneeded weight parameters, it is not possible to apply a single deep \narchitecture for multiple devices with different resources. When a new device \nor circumstantial condition requires a new deep architecture, it is necessary \nto construct and train a new network from scratch. In this work, we propose a \nnovel deep learning framework, called a nested sparse network, which exploits \nan n-in-1-type nested structure in a neural network. A nested sparse network \nconsists of multiple levels of networks with a different sparsity ratio \nassociated with each level, and higher level networks share parameters with \nlower level networks to enable stable nested learning. The proposed framework \nrealizes a resource-aware versatile architecture as the same network can meet \ndiverse resource requirements. Moreover, the proposed nested network can learn \ndifferent forms of knowledge in its internal networks at different levels, \nenabling multiple tasks using a single network, such as coarse-to-fine \nhierarchical classification. In order to train the proposed nested sparse \nnetwork, we propose efficient weight connection learning and channel and layer \nscheduling strategies. We evaluate our network in multiple tasks, including \nadaptive deep compression, knowledge distillation, and learning class \nhierarchy, and demonstrate that nested sparse networks perform competitively, \nbut more efficiently, than existing methods. \n</p>"}, "author": "Eunwoo Kim, Chanho Ahn, Songhwai Oh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337155", "id": "tag:google.com,2005:reader/item/00000003454dcf4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments. (arXiv:1712.03931v1 [cs.LG])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03931"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03931", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present MINOS, a simulator designed to support the development of \nmultisensory models for goal-directed navigation in complex indoor \nenvironments. The simulator leverages large datasets of complex 3D environments \nand supports flexible configuration of multimodal sensor suites. We use MINOS \nto benchmark deep-learning-based navigation methods, to analyze the influence \nof environmental complexity on navigation performance, and to carry out a \ncontrolled study of multimodality in sensorimotor learning. The experiments \nshow that current deep reinforcement learning approaches fail in large \nrealistic environments. The experiments also indicate that multimodality is \nbeneficial in learning to navigate cluttered scenes. MINOS is released \nopen-source to the research community at <a href=\"http://minosworld.org\">this http URL</a> . A video that \nshows MINOS can be found at https://youtu.be/c0mL9K64q84 \n</p>"}, "author": "Manolis Savva, Angel X. Chang, Alexey Dosovitskiy, Thomas Funkhouser, Vladlen Koltun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337154", "id": "tag:google.com,2005:reader/item/00000003454dcf4f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the Benefit of Combining Neural, Statistical and External Features for Fake News Identification. (arXiv:1712.03935v1 [cs.AI])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03935"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03935", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Identifying the veracity of a news article is an interesting problem while \nautomating this process can be a challenging task. Detection of a news article \nas fake is still an open question as it is contingent on many factors which the \ncurrent state-of-the-art models fail to incorporate. In this paper, we explore \na subtask to fake news identification, and that is stance detection. Given a \nnews article, the task is to determine the relevance of the body and its claim. \nWe present a novel idea that combines the neural, statistical and external \nfeatures to provide an efficient solution to this problem. We compute the \nneural embedding from the deep recurrent model, statistical features from the \nweighted n-gram bag-of-words model and handcrafted external features with the \nhelp of feature engineering heuristics. Finally, using deep neural layer all \nthe features are combined, thereby classifying the headline-body news pair as \nagree, disagree, discuss, or unrelated. We compare our proposed technique with \nthe current state-of-the-art models on the fake news challenge dataset. Through \nextensive experiments, we find that the proposed model outperforms all the \nstate-of-the-art techniques including the submissions to the fake news \nchallenge. \n</p>"}, "author": "Gaurav Bhatt, Aman Sharma, Shivam Sharma, Ankush Nagpal, Balasubramanian Raman, Ankush Mittal", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337153", "id": "tag:google.com,2005:reader/item/00000003454dcf54", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Reactive Multi-Context Systems: Heterogeneous Reasoning in Dynamic Environments. (arXiv:1609.03438v3 [cs.LO] UPDATED)", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1609.03438"}], "alternate": [{"href": "http://arxiv.org/abs/1609.03438", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Managed multi-context systems (mMCSs) allow for the integration of \nheterogeneous knowledge sources in a modular and very general way. They were, \nhowever, mainly designed for static scenarios and are therefore not well-suited \nfor dynamic environments in which continuous reasoning over such heterogeneous \nknowledge with constantly arriving streams of data is necessary. In this paper, \nwe introduce reactive multi-context systems (rMCSs), a framework for reactive \nreasoning in the presence of heterogeneous knowledge sources and data streams. \nWe show that rMCSs are indeed well-suited for this purpose by illustrating how \nseveral typical problems arising in the context of stream reasoning can be \nhandled using them, by showing how inconsistencies possibly occurring in the \nintegration of multiple knowledge sources can be handled, and by arguing that \nthe potential non-determinism of rMCSs can be avoided if needed using an \nalternative, more skeptical well-founded semantics instead with beneficial \ncomputational properties. We also investigate the computational complexity of \nvarious reasoning problems related to rMCSs. Finally, we discuss related work, \nand show that rMCSs do not only generalize mMCSs to dynamic settings, but also \ncapture/extend relevant approaches w.r.t. dynamics in knowledge representation \nand stream reasoning. \n</p>"}, "author": "Gerhard Brewka, Stefan Ellmauthaler, Ricardo Gon&#xe7;alves, Matthias Knorr, Jo&#xe3;o Leite, J&#xf6;rg P&#xfc;hrer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337151", "id": "tag:google.com,2005:reader/item/00000003454dcf61", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generating Visual Representations for Zero-Shot Classification. (arXiv:1708.06975v3 [cs.CV] UPDATED)", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.06975"}], "alternate": [{"href": "http://arxiv.org/abs/1708.06975", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper addresses the task of learning an image clas-sifier when some \ncategories are defined by semantic descriptions only (e.g. visual attributes) \nwhile the others are defined by exemplar images as well. This task is often \nreferred to as the Zero-Shot classification task (ZSC). Most of the previous \nmethods rely on learning a common embedding space allowing to compare visual \nfeatures of unknown categories with semantic descriptions. This paper argues \nthat these approaches are limited as i) efficient discrimi-native classifiers \ncan't be used ii) classification tasks with seen and unseen categories \n(Generalized Zero-Shot Classification or GZSC) can't be addressed efficiently. \nIn contrast , this paper suggests to address ZSC and GZSC by i) learning a \nconditional generator using seen classes ii) generate artificial training \nexamples for the categories without exemplars. ZSC is then turned into a \nstandard supervised learning problem. Experiments with 4 generative models and \n5 datasets experimentally validate the approach, giving state-of-the-art \nresults on both ZSC and GZSC. \n</p>"}, "author": "Maxime Bucher (1), St&#xe9;phane Herbin (1), Fr&#xe9;d&#xe9;ric Jurie ((1) Palaiseau)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337150", "id": "tag:google.com,2005:reader/item/00000003454dcf66", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Complexity of Graph-Based Reductions for Reachability in Markov Decision Processes. (arXiv:1710.07903v2 [cs.LO] UPDATED)", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07903"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07903", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the never-worse relation (NWR) for Markov decision processes with an \ninfinite-horizon reachability objective. A state q is never worse than a state \np if the maximal probability of reaching the target set of states from p is at \nmost the same value from q, regardless of the probabilities labelling the \ntransitions. Extremal-probability states, end components, and essential states \nare all special cases of the equivalence relation induced by the NWR. Using the \nNWR, states in the same equivalence class can be collapsed. Then, actions \nleading to sub-optimal states can be removed. We show the natural decision \nproblem associated to computing the NWR is coNP-complete. Finally, we extend a \nknown incomplete polynomial-time iterative algorithm to under-approximate the \nNWR. \n</p>"}, "author": "Stephane Le Roux, Guillermo A. Perez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337149", "id": "tag:google.com,2005:reader/item/00000003454dcf6e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Learning K-way D-dimensional Discrete Code For Compact Embedding Representations. (arXiv:1711.03067v3 [cs.LG] UPDATED)", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03067"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03067", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Embedding methods such as word embedding have become pillars for many \napplications containing discrete structures. Conventional embedding methods \ndirectly associate each symbol with a continuous embedding vector, which is \nequivalent to applying linear transformation based on \"one-hot\" encoding of the \ndiscrete symbols. Despite its simplicity, such approach yields number of \nparameters that grows linearly with the vocabulary size and can lead to \noverfitting. In this work we propose a much more compact K-way D-dimensional \ndiscrete encoding scheme to replace the \"one-hot\" encoding. In \"KD encoding\", \neach symbol is represented by a $D$-dimensional code, and each of its dimension \nhas a cardinality of $K$. The final symbol embedding vector can be generated by \ncomposing the code embedding vectors. To learn the semantically meaningful \ncode, we derive a relaxed discrete optimization technique based on stochastic \ngradient descent. By adopting the new coding system, the efficiency of \nparameterization can be significantly improved (from linear to logarithmic), \nand this can also mitigate the over-fitting problem. In our experiments with \nlanguage modeling, the number of embedding parameters can be reduced by 97\\% \nwhile achieving similar or better performance. \n</p>"}, "author": "Ting Chen, Martin Renqiang Min, Yizhou Sun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337148", "id": "tag:google.com,2005:reader/item/00000003454dcf76", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Exploring Approximations for Floating-Point Arithmetic using UppSAT. (arXiv:1711.08859v2 [cs.LO] UPDATED)", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08859"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08859", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of solving floating-point constraints obtained from \nsoftware verification. We present UppSAT --- a new implementation of a \nsystematic approximation refinement framework [ZWR17] as an abstract SMT \nsolver. Provided with an approximation and a decision procedure (implemented in \nan off-the-shelf SMT solver), UppSAT yields an approximating SMT solver. \nAdditionally, UppSAT includes a library of predefined approximation components \nwhich can be combined and extended to define new encodings, orderings and \nsolving strategies. We propose that UppSAT can be used as a sandbox for easy \nand flexible exploration of new approximations. To substantiate this, we \nexplore several approximations of floating-point arithmetic. Approximations can \nbe viewed as a composition of an encoding into a target theory, a precision \nordering, and a number of strategies for model reconstruction and precision (or \napproximation) refinement. We present encodings of floating-point arithmetic \ninto reduced precision floating-point arithmetic, real-arithmetic, and \nfixed-point arithmetic (encoded in the theory of bit-vectors). In an \nexperimental evaluation, we compare the advantages and disadvantages of \napproximating solvers obtained by combining various encodings and decision \nprocedures (based on existing state-of-the-art SMT solvers for floating-point, \nreal, and bit-vector arithmetic). \n</p>"}, "author": "Aleksandar Zeljic, Peter Backeman, Christoph M. Wintersteiger, Philipp Ruemmer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337145", "id": "tag:google.com,2005:reader/item/00000003454dcf83", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast Low-Rank Matrix Estimation without the Condition Number. (arXiv:1712.03281v1 [stat.ML])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03281"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03281", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we study the general problem of optimizing a convex function \n$F(L)$ over the set of $p \\times p$ matrices, subject to rank constraints on \n$L$. However, existing first-order methods for solving such problems either are \ntoo slow to converge, or require multiple invocations of singular value \ndecompositions. On the other hand, factorization-based non-convex algorithms, \nwhile being much faster, require stringent assumptions on the \\emph{condition \nnumber} of the optimum. In this paper, we provide a novel algorithmic framework \nthat achieves the best of both worlds: asymptotically as fast as factorization \nmethods, while requiring no dependency on the condition number. \n</p> \n<p>We instantiate our general framework for three important matrix estimation \nproblems that impact several practical applications; (i) a \\emph{nonlinear} \nvariant of affine rank minimization, (ii) logistic PCA, and (iii) precision \nmatrix estimation in probabilistic graphical model learning. We then derive \nexplicit bounds on the sample complexity as well as the running time of our \napproach, and show that it achieves the best possible bounds for both cases. We \nalso provide an extensive range of experimental results, and demonstrate that \nour algorithm provides a very attractive tradeoff between estimation accuracy \nand running time. \n</p>"}, "author": "Mohammadreza Soltani, Chinmay Hegde", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337144", "id": "tag:google.com,2005:reader/item/00000003454dcf91", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks. (arXiv:1712.03298v1 [cs.LG])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03298"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03298", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325050729d1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325050729d1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Progress in deep learning is slowed by the days or weeks it takes to train \nlarge models. The natural solution of using more hardware is limited by \ndiminishing returns, and leads to inefficient use of additional resources. In \nthis paper, we present a large batch, stochastic optimization algorithm that is \nboth faster than widely used algorithms for fixed amounts of computation, and \nalso scales up substantially better as more computational resources become \navailable. Our algorithm implicitly computes the inverse Hessian of each \nmini-batch to produce descent directions; we do so without either an explicit \napproximation to the Hessian or Hessian-vector products. We demonstrate the \neffectiveness of our algorithm by successfully training large ImageNet models \n(Inception-V3, Resnet-50, Resnet-101 and Inception-Resnet-V2) with mini-batch \nsizes of up to 32000 with no loss in validation error relative to current \nbaselines, and no increase in the total number of steps. At smaller mini-batch \nsizes, our optimizer improves the validation error in these models by 0.8-0.9%. \nAlternatively, we can trade off this accuracy to reduce the number of training \nsteps needed by roughly 10-30%. Our work is practical and easily usable by \nothers -- only one hyperparameter (learning rate) needs tuning, and \nfurthermore, the algorithm is as computationally cheap as the commonly used \nAdam optimizer. \n</p>"}, "author": "Shankar Krishnan, Ying Xiao, Rif A. Saurous", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337143", "id": "tag:google.com,2005:reader/item/00000003454dcf98", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian Joint Matrix Decomposition for Data Integration with Heterogeneous Noise. (arXiv:1712.03337v1 [cs.CV])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03337"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03337", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Matrix decomposition is a popular and fundamental approach in machine \nlearning and data mining. It has been successfully applied into various fields. \nMost matrix decomposition methods focus on decomposing a data matrix from one \nsingle source. However, it is common that data are from different sources with \nheterogeneous noise. A few of matrix decomposition methods have been extended \nfor such multi-view data integration and pattern discovery. While only few \nmethods were designed to consider the heterogeneity of noise in such multi-view \ndata for data integration explicitly. To this end, we propose a joint matrix \ndecomposition framework (BJMD), which models the heterogeneity of noise by \nGaussian distribution in a Bayesian framework. We develop two algorithms to \nsolve this model: one is a variational Bayesian inference algorithm, which \nmakes full use of the posterior distribution; and another is a maximum a \nposterior algorithm, which is more scalable and can be easily paralleled. \nExtensive experiments on synthetic and real-world datasets demonstrate that \nBJMD considering the heterogeneity of noise is superior or competitive to the \nstate-of-the-art methods. \n</p>"}, "author": "Chihao Zhang, Shihua Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337142", "id": "tag:google.com,2005:reader/item/00000003454dcf9f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Variational Inference over Non-differentiable Cardiac Simulators using Bayesian Optimization. (arXiv:1712.03353v1 [stat.ML])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03353"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03353", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Performing inference over simulators is generally intractable as their \nruntime means we cannot compute a marginal likelihood. We develop a \nlikelihood-free inference method to infer parameters for a cardiac simulator, \nwhich replicates electrical flow through the heart to the body surface. We \nimprove the fit of a state-of-the-art simulator to an electrocardiogram (ECG) \nrecorded from a real patient. \n</p>"}, "author": "Adam McCarthy, Blanca Rodriguez, Ana Minchole", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337141", "id": "tag:google.com,2005:reader/item/00000003454dcfa5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Elastic-net regularized High-dimensional Negative Binomial Regression: Consistency and Weak Signals Detection. (arXiv:1712.03412v1 [stat.ML])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03412"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03412", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study sparse high-dimensional negative binomial regression problem for \ncount data regression by showing non-asymptotic merits of the Elastic-net \nregularized estimator. With the KKT conditions, we derive two types of \nnon-asymptotic oracle inequalities for the elastic net estimates of negative \nbinomial regression by utilizing Compatibility factor and Stabil Condition, \nrespectively. Based on oracle inequalities we proposed, we firstly show the \nsign consistency property of the Elastic-net estimators provided that the \nnon-zero components in sparse true vector are large than a proper choice of the \nweakest signal detection threshold, and the second application is that we give \nan oracle inequality for bounding the grouping effect with high probability, \nthirdly, under some assumptions of design matrix, we can recover the true \nvariable set with high probability if the weakest signal detection threshold is \nlarge than 3 times the value of turning parameter, at last, we briefly discuss \nthe de-biased Elastic-net estimator. \n</p>"}, "author": "Huiming Zhang, Jinzhu Jia", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337140", "id": "tag:google.com,2005:reader/item/00000003454dcfa9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Cost-Sensitive Approach to Batch Size Adaptation for Gradient Descent. (arXiv:1712.03428v1 [cs.LG])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03428"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03428", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a novel approach to automatically determine the \nbatch size in stochastic gradient descent methods. The choice of the batch size \ninduces a trade-off between the accuracy of the gradient estimate and the cost \nin terms of samples of each update. We propose to determine the batch size by \noptimizing the ratio between a lower bound to a linear or quadratic Taylor \napproximation of the expected improvement and the number of samples used to \nestimate the gradient. The performance of the proposed approach is empirically \ncompared with related methods on popular classification tasks. \n</p> \n<p>The work was presented at the NIPS workshop on Optimizing the Optimizers. \nBarcelona, Spain, 2016. \n</p>"}, "author": "Matteo Pirotta, Marcello Restelli", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337139", "id": "tag:google.com,2005:reader/item/00000003454dcfaf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Identifiability of Kronecker-structured Dictionaries for Tensor Data. (arXiv:1712.03471v1 [stat.ML])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03471"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03471", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper derives sufficient conditions for reliable recovery of coordinate \ndictionaries comprising a Kronecker-structured dictionary that is used for \nrepresenting $K$th-order tensor data. Tensor observations are generated by a \nKronecker-structured dictionary and sparse coefficient tensors that follow the \nseparable sparsity model. This work provides sufficient conditions on the \nunderlying coordinate dictionaries, coefficient and noise distributions, and \nnumber of samples that guarantee recovery of the individual coordinate \ndictionaries up to a specified error with high probability. In particular, the \nsample complexity to recover $K$ coordinate dictionaries with dimensions \n$m_k\\times p_k$ up to estimation error $r_k$ is shown to be $\\max_{k \\in \n[K]}\\mathcal{O}(m_kp_k^3r_k^{-2})$. \n</p>"}, "author": "Zahra Shakeri, Anand D. Sarwate, Waheed U. Bajwa", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337138", "id": "tag:google.com,2005:reader/item/00000003454dcfb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Capsule Network Performance on Complex Data. (arXiv:1712.03480v1 [stat.ML])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03480"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03480", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In recent years, convolutional neural networks (CNN) have played an important \nrole in the field of deep learning. Variants of CNN's have proven to be very \nsuccessful in classification tasks across different domains. However, there are \ntwo big drawbacks to CNN's: their failure to take into account of important \nspatial hierarchies between features, and their lack of rotational invariance. \nAs long as certain key features of an object are present in the test data, \nCNN's classify the test data as the object, disregarding features' relative \nspatial orientation to each other. This causes false positives. The lack of \nrotational invariance in CNN's would cause the network to incorrectly assign \nthe object another label, causing false negatives. To address this concern, \nHinton et al. propose a novel type of neural network using the concept of \ncapsules in a recent paper. With the use of dynamic routing and reconstruction \nregularization, the capsule network model would be both rotation invariant and \nspatially aware. The capsule network has shown its potential by achieving a \nstate-of-the-art result of 0.25% test error on MNIST without data augmentation \nsuch as rotation and scaling, better than the previous baseline of 0.39%. To \nfurther test out the application of capsule networks on data with higher \ndimensionality, we attempt to find the best set of configurations that yield \nthe optimal test error on CIFAR10 dataset. \n</p>"}, "author": "Edgar Xi, Selina Bing, Yang Jin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337137", "id": "tag:google.com,2005:reader/item/00000003454dcfb8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improving Malware Detection Accuracy by Extracting Icon Information. (arXiv:1712.03483v1 [cs.CR])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03483"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03483", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Detecting PE malware files is now commonly approached using statistical and \nmachine learning models. While these models commonly use features extracted \nfrom the structure of PE files, we propose that icons from these files can also \nhelp better predict malware. We propose an innovative machine learning approach \nto extract information from icons. Our proposed approach consists of two steps: \n1) extracting icon features using summary statics, histogram of gradients \n(HOG), and a convolutional autoencoder, 2) clustering icons based on the \nextracted icon features. Using publicly available data and by using machine \nlearning experiments, we show our proposed icon clusters significantly boost \nthe efficacy of malware prediction models. In particular, our experiments show \nan average accuracy increase of 10% when icon clusters are used in the \nprediction model. \n</p>"}, "author": "Pedro Silva, Sepehr Akhavan-Masouleh, Li Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337136", "id": "tag:google.com,2005:reader/item/00000003454dcfc1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Causal Inference for Observational Time-Series with Encoder-Decoder Networks. (arXiv:1712.03553v1 [stat.ML])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03553"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03553", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes a method for estimating the causal effect of a discrete \nintervention in observational time-series data using encoder-decoder recurrent \nneural networks (RNNs). Encoder-decoder networks, which are special class of \nRNNs suitable for handling variable-length sequential data, are used to predict \na counterfactual time-series of treated unit outcomes. The proposed method does \nnot rely on pretreatment covariates and encoder-decoder networks are capable of \nlearning nonconvex combinations of control unit outcomes to construct a \ncounterfactual. To demonstrate the proposed method, I extend a field experiment \nstudying the effect of radio advertisements on electoral competition to \nobservational time-series. \n</p>"}, "author": "Jason Poulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337135", "id": "tag:google.com,2005:reader/item/00000003454dcfc5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "DGCNN: Disordered Graph Convolutional Neural Network Based on the Gaussian Mixture Model. (arXiv:1712.03563v1 [cs.LG])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03563"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03563", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolutional neural networks (CNNs) can be applied to graph similarity \nmatching, in which case they are called graph CNNs. Graph CNNs are attracting \nincreasing attention due to their effectiveness and efficiency. However, the \nexisting convolution approaches focus only on regular data forms and require \nthe transfer of the graph or key node neighborhoods of the graph into the same \nfixed form. During this transfer process, structural information of the graph \ncan be lost, and some redundant information can be incorporated. To overcome \nthis problem, we propose the disordered graph convolutional neural network \n(DGCNN) based on the mixed Gaussian model, which extends the CNN by adding a \npreprocessing layer called the disordered graph convolutional layer (DGCL). The \nDGCL uses a mixed Gaussian function to realize the mapping between the \nconvolution kernel and the nodes in the neighborhood of the graph. The output \nof the DGCL is the input of the CNN. We further implement a \nbackward-propagation optimization process of the convolutional layer by which \nwe incorporate the feature-learning model of the irregular node neighborhood \nstructure into the network. Thereafter, the optimization of the convolution \nkernel becomes part of the neural network learning process. The DGCNN can \naccept arbitrary scaled and disordered neighborhood graph structures as the \nreceptive fields of CNNs, which reduces information loss during graph \ntransformation. Finally, we perform experiments on multiple standard graph \ndatasets. The results show that the proposed method outperforms the \nstate-of-the-art methods in graph classification and retrieval. \n</p>"}, "author": "Bo Wu, Yang Liu, Bo Lang, Lei Huang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337134", "id": "tag:google.com,2005:reader/item/00000003454dcfcd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sensitivity Analysis for Predictive Uncertainty in Bayesian Neural Networks. (arXiv:1712.03605v1 [stat.ML])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03605"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03605", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32505072bdd\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32505072bdd&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We derive a novel sensitivity analysis of input variables for predictive \nepistemic and aleatoric uncertainty. We use Bayesian neural networks with \nlatent variables as a model class and illustrate the usefulness of our \nsensitivity analysis on real-world datasets. Our method increases the \ninterpretability of complex black-box probabilistic models. \n</p>"}, "author": "Stefan Depeweg, Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato, Steffen Udluft, Thomas Runkler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337133", "id": "tag:google.com,2005:reader/item/00000003454dcfd2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Gradient Normalization & Depth Based Decay For Deep Learning. (arXiv:1712.03607v1 [cs.LG])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03607"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03607", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325050dd533\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325050dd533&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper we introduce a novel method of gradient normalization and decay \nwith respect to depth. Our method leverages the simple concept of normalizing \nall gradients in a deep neural network, and then decaying said gradients with \nrespect to their depth in the network. Our proposed normalization and decay \ntechniques can be used in conjunction with most current state of the art \noptimizers and are a very simple addition to any network. This method, although \nsimple, showed improvements in convergence time on state of the art networks \nsuch as DenseNet and ResNet on image classification tasks, as well as on an \nLSTM for natural language processing tasks. \n</p>"}, "author": "Robert Kwiatkowski, Oscar Chang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337132", "id": "tag:google.com,2005:reader/item/00000003454dcfda", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The PhaseLift for Non-quadratic Gaussian Measurements. (arXiv:1712.03638v1 [stat.ML])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03638"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03638", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of recovering a structured signal $\\mathbf{x}_0$ from \nhigh-dimensional measurements of the form $y=f(\\mathbf{a}^T\\mathbf{x}_0)$ for \nsome nonlinear function $f$. When the measurement vector $\\mathbf a$ is iid \nGaussian, Brillinger observed in his 1982 paper that $\\mu_\\ell\\cdot\\mathbf{x}_0 \n= \\min_{\\mathbf{x}}\\mathbb{E}(y - \\mathbf{a}^T\\mathbf{x})^2$, where \n$\\mu_\\ell=\\mathbb{E}_{\\gamma}[\\gamma f(\\gamma)]$ with $\\gamma$ being a standard \nGaussian random variable. Based on this simple observation, he showed that, in \nthe classical statistical setting, the least-squares method is consistent. More \nrecently, Plan \\&amp; Vershynin extended this result to the high-dimensional \nsetting and derived error bounds for the generalized Lasso. Unfortunately, both \nleast-squares and the Lasso fail to recover $\\mathbf{x}_0$ when $\\mu_\\ell=0$. \nFor example, this includes all even link functions. We resolve this issue by \nproposing and analyzing an appropriate generic semidefinite-optimization based \nmethod. In a nutshell, our idea is to treat such link functions as if they were \nlinear in a lifted space of higher-dimension. An appealing feature of our error \nanalysis is that it captures the effect of the nonlinearity in a few simple \nsummary parameters, which can be particularly useful in system design. \n</p>"}, "author": "Christos Thrampoulidis, Ankit Singh Rawat", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337131", "id": "tag:google.com,2005:reader/item/00000003454dcfe6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Distributed Mapper. (arXiv:1712.03660v1 [cs.CV])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03660"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03660", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The construction of Mapper has emerged in the last decade as a powerful and \neffective topological data analysis tool that approximates and generalizes \nother topological summaries, such as the Reeb graph, the contour tree, split, \nand joint trees. In this paper we study the parallel analysis of the \nconstruction of Mapper. We give a provably correct algorithm to distribute \nMapper on a set of processors and discuss the performance results that compare \nour approach to a reference sequential Mapper implementation. We report the \nperformance experiments that demonstrate the efficiency of our method. \n</p>"}, "author": "Mustafa Hajij, Basem Assiri, Paul Rosen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337130", "id": "tag:google.com,2005:reader/item/00000003454dcffa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Crime prediction through urban metrics and statistical learning. (arXiv:1712.03834v1 [physics.soc-ph])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03834"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03834", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Understanding the causes of crime is a longstanding issue in researcher's \nagenda. While it is a hard task to extract causality from data, several linear \nmodels have been proposed to predict crime through the existing correlations \nbetween crime and urban metrics. However, because of non-Gaussian distributions \nand multicollinearity in urban indicators, it is common to find controversial \nconclusions about the influence of some urban indicators on crime. Machine \nlearning ensemble-based algorithms can handle well such problems. Here, we use \na random forest regressor to predict crime and quantify the influence of urban \nindicators on homicides. Our approach can have up to $97\\%$ of accuracy on \ncrime prediction and the importance of urban indicators is ranked and clustered \nin groups of equal influence, which are robust under slightly changes in the \ndata sample analyzed. Our results determine the rank of importance of urban \nindicators to predict crime, unveiling that unemployment and illiteracy are the \nmost important variables for describing homicides in Brazilian cities. We \nfurther believe that our approach helps in producing more robust conclusions \nregarding the effects of urban indicators on crime, having potential \napplications for guiding public policies for crime control. \n</p>"}, "author": "Luiz G A Alves, Haroldo V Ribeiro, Francisco A Rodrigues", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337129", "id": "tag:google.com,2005:reader/item/00000003454dd005", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On Quadratic Penalties in Elastic Weight Consolidation. (arXiv:1712.03847v1 [stat.ML])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03847"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03847", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Elastic weight consolidation (EWC, Kirkpatrick et al, 2017) is a novel \nalgorithm designed to safeguard against catastrophic forgetting in neural \nnetworks. EWC can be seen as an approximation to Laplace propagation (Eskin et \nal, 2004), and this view is consistent with the motivation given by Kirkpatrick \net al (2017). In this note, I present an extended derivation that covers the \ncase when there are more than two tasks. I show that the quadratic penalties in \nEWC are inconsistent with this derivation and might lead to double-counting \ndata from earlier tasks. \n</p>"}, "author": "Ferenc Husz&#xe1;r", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337128", "id": "tag:google.com,2005:reader/item/00000003454dd00a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Generalized Zero-Shot Learning via Synthesized Examples. (arXiv:1712.03878v1 [cs.LG])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03878"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03878", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a generative framework for generalized zero-shot learning where \nthe training and test classes are not necessarily disjoint. Built upon a \nvariational autoencoder based architecture, consisting of a probabilistic \nencoder and a probabilistic conditional decoder, our model can generate novel \nexemplars from seen/unseen classes, given their respective class attributes. \nThese exemplars can subsequently be used to train any off-the-shelf \nclassification model. One of the key aspects of our encoder-decoder \narchitecture is a feedback-driven mechanism in which a discriminator (a \nmultivariate regressor) learns to map the generated exemplars to the \ncorresponding class attribute vectors, leading to an improved generator. Our \nmodel's ability to generate and leverage examples from unseen classes to train \nthe classification model naturally helps to mitigate the bias towards \npredicting seen classes in generalized zero-shot learning settings. Through a \ncomprehensive set of experiments, we show that our model outperforms several \nstate-of-the-art methods, on several benchmark datasets, for both standard as \nwell as generalized zero-shot learning. \n</p>"}, "author": "Gundeep Arora, Vinay Kumar Verma, Ashish Mishra, Piyush Rai", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337127", "id": "tag:google.com,2005:reader/item/00000003454dd013", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards reduction of autocorrelation in HMC by machine learning. (arXiv:1712.03893v1 [hep-lat])", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03893"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03893", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we propose new algorithm to reduce autocorrelation in Markov \nchain Monte-Carlo algorithms for euclidean field theories on the lattice. Our \nproposing algorithm is the Hybrid Monte-Carlo algorithm (HMC) with restricted \nBoltzmann machine. We examine the validity of the algorithm by employing the \nphi-fourth theory in three dimension. We observe reduction of the \nautocorrelation both in symmetric and broken phase as well. Our proposing \nalgorithm provides consistent central values of expectation values of the \naction density and one-point Green's function with ones from the original HMC \nin both the symmetric phase and broken phase within the statistical error. On \nthe other hand, two-point Green's functions have slight difference between one \ncalculated by the HMC and one by our proposing algorithm in the symmetric \nphase. Furthermore, near the criticality, the distribution of the one-point \nGreen's function differs from the one from HMC. We discuss the origin of \ndiscrepancies and its improvement. \n</p>"}, "author": "Akinori Tanaka, Akio Tomiya", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337126", "id": "tag:google.com,2005:reader/item/00000003454dd01e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Regulating Greed Over Time. (arXiv:1505.05629v3 [stat.ML] UPDATED)", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1505.05629"}], "alternate": [{"href": "http://arxiv.org/abs/1505.05629", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In retail, there are predictable yet dramatic time-dependent patterns in \ncustomer behavior, such as periodic changes in the number of visitors, or \nincreases in visitors just before major holidays. The current paradigm of \nmulti-armed bandit analysis does not take these known patterns into account. \nThis means that for applications in retail, where prices are fixed for periods \nof time, current bandit algorithms will not suffice. This work provides a \nremedy that takes the time-dependent patterns into account, and we show how \nthis remedy is implemented in the UCB and {\\epsilon}-greedy methods and we \nintroduce a new policy called the variable arm pool method. In the corrected \nmethods, exploitation (greed) is regulated over time, so that more exploitation \noccurs during higher reward periods, and more exploration occurs in periods of \nlow reward. In order to understand why regret is reduced with the corrected \nmethods, we present a set of bounds that provide insight into why we would want \nto exploit during periods of high reward, and discuss the impact on regret. Our \nproposed methods perform well in experiments, and were inspired by a \nhigh-scoring entry in the Exploration and Exploitation 3 contest using data \nfrom Yahoo! Front Page. That entry heavily used time-series methods to regulate \ngreed over time, which was substantially more effective than other contextual \nbandit methods. \n</p>"}, "author": "Stefano Trac&#xe0;, Cynthia Rudin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337125", "id": "tag:google.com,2005:reader/item/00000003454dd027", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Discriminative k-shot learning using probabilistic models. (arXiv:1706.00326v2 [stat.ML] UPDATED)", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.00326"}], "alternate": [{"href": "http://arxiv.org/abs/1706.00326", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper introduces a probabilistic framework for k-shot image \nclassification. The goal is to generalise from an initial large-scale \nclassification task to a separate task comprising new classes and small numbers \nof examples. The new approach not only leverages the feature-based \nrepresentation learned by a neural network from the initial task \n(representational transfer), but also information about the classes (concept \ntransfer). The concept information is encapsulated in a probabilistic model for \nthe final layer weights of the neural network which acts as a prior for \nprobabilistic k-shot learning. We show that even a simple probabilistic model \nachieves state-of-the-art on a standard k-shot learning dataset by a large \nmargin. Moreover, it is able to accurately model uncertainty, leading to well \ncalibrated classifiers, and is easily extensible and flexible, unlike many \nrecent approaches to k-shot learning. \n</p>"}, "author": "Matthias Bauer, Mateo Rojas-Carulla, Jakub Bart&#x142;omiej &#x15a;wi&#x105;tkowski, Bernhard Sch&#xf6;lkopf, Richard E. Turner", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337124", "id": "tag:google.com,2005:reader/item/00000003454dd031", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Tight Semi-Nonnegative Matrix Factorization. (arXiv:1709.04395v3 [stat.ML] UPDATED)", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.04395"}], "alternate": [{"href": "http://arxiv.org/abs/1709.04395", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325050dd8e7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325050dd8e7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The nonnegative matrix factorization is a widely used, flexible matrix \ndecomposition, finding applications in biology, image and signal processing and \ninformation retrieval, among other areas. Here we present a related matrix \nfactorization. A multi-objective optimization problem finds conical \ncombinations of templates that approximate a given data matrix. The templates \nare chosen so that as far as possible only the initial data set can be \nrepresented this way. However, the templates are not required to be nonnegative \nnor convex combinations of the original data. \n</p>"}, "author": "David W Dreisigmeyer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337123", "id": "tag:google.com,2005:reader/item/00000003454dd03c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Conditions for Stability and Convergence of Set-Valued Stochastic Approximations: Applications to Approximate Value and Fixed point Iterations. (arXiv:1709.04673v2 [cs.SY] UPDATED)", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.04673"}], "alternate": [{"href": "http://arxiv.org/abs/1709.04673", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The main aim of this paper is the development of easily verifiable sufficient \nconditions for stability (almost sure boundedness) and convergence of \nstochastic approximation algorithms (SAAs) with set-valued mean-fields, a class \nof model-free algorithms that have become important in recent times. In this \npaper we provide a complete analysis of such algorithms under three different, \nyet related sets of sufficient conditions, based on the existence of an \nassociated global/local Lyapunov function. Unlike previous Lyapunov function \nbased approaches, we provide a simple recipe for explicitly constructing the \nLyapunov function, needed for analysis. Our work builds on the works of \nAbounadi, Bertsekas and Borkar (2002), Munos (2005), and Ramaswamy and \nBhatnagar (2016). An important motivation for the flavor of our assumptions \ncomes from the need to understand dynamic programming and reinforcement \nlearning algorithms, that use deep neural networks (DNNs) for function \napproximations and parameterizations. These algorithms are popularly known as \ndeep learning algorithms. As an important application of our theory, we provide \na complete analysis of the stochastic approximation counterpart of approximate \nvalue iteration (AVI), an important dynamic programming method designed to \ntackle Bellman's curse of dimensionality. Further, the assumptions involved are \nsignificantly weaker, easily verifiable and truly model-free. The theory \npresented in this paper is also used to develop and analyze the first SAA for \nfinding fixed points of contractive set-valued maps. \n</p>"}, "author": "Arunselvan Ramaswamy, Shalabh Bhatnagar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337122", "id": "tag:google.com,2005:reader/item/00000003454dd04a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Theoretical and Computational Guarantees of Mean Field Variational Inference for Community Detection. (arXiv:1710.11268v3 [math.ST] UPDATED)", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11268"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11268", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The mean field variational Bayes method is becoming increasingly popular in \nstatistics and machine learning. Its iterative Coordinate Ascent Variational \nInference algorithm has been widely applied to large scale Bayesian inference. \nSee Blei et al. (2017) for a recent comprehensive review. Despite the \npopularity of the mean field method there exist remarkably little fundamental \ntheoretical justifications. To the best of our knowledge, the iterative \nalgorithm has never been investigated for any high dimensional and complex \nmodel. In this paper, we study the mean field method for community detection \nunder the Stochastic Block Model. For an iterative Batch Coordinate Ascent \nVariational Inference algorithm, we show that it has a linear convergence rate \nand converges to the minimax rate within $\\log n$ iterations. This complements \nthe results of Bickel et al. (2013) which studied the global minimum of the \nmean field variational Bayes and obtained asymptotic normal estimation of \nglobal model parameters. In addition, we obtain similar optimality results for \nGibbs sampling and an iterative procedure to calculate maximum likelihood \nestimation, which can be of independent interest. \n</p>"}, "author": "Anderson Y. Zhang, Harrison H. Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337121", "id": "tag:google.com,2005:reader/item/00000003454dd04f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Measuring Territorial Control in Civil Wars Using Hidden Markov Models: A Data Informatics-Based Approach. (arXiv:1711.06786v2 [stat.AP] UPDATED)", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06786"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06786", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Territorial control is a key aspect shaping the dynamics of civil war. \nDespite its importance, we lack data on territorial control that are \nfine-grained enough to account for subnational spatio-temporal variation and \nthat cover a large set of conflicts. To resolve this issue, we propose a \ntheoretical model of the relationship between territorial control and tactical \nchoice in civil war and outline how Hidden Markov Models (HMMs) are suitable to \ncapture theoretical intuitions and estimate levels of territorial control. We \ndiscuss challenges of using HMMs in this application and mitigation strategies \nfor future work. \n</p>"}, "author": "Therese Anders, Hong Xu, Cheng Cheng, T. K. Satish Kumar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337120", "id": "tag:google.com,2005:reader/item/00000003454dd060", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Glitch Classification and Clustering for LIGO with Deep Transfer Learning. (arXiv:1711.07468v2 [astro-ph.IM] UPDATED)", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07468"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07468", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The detection of gravitational waves with LIGO and Virgo requires a detailed \nunderstanding of the response of these instruments in the presence of \nenvironmental and instrumental noise. Of particular interest is the study of \nanomalous non-Gaussian noise transients known as glitches, since their high \noccurrence rate in LIGO/Virgo data can obscure or even mimic true gravitational \nwave signals. Therefore, successfully identifying and excising glitches is of \nutmost importance to detect and characterize gravitational waves. In this \narticle, we present the first application of Deep Learning combined with \nTransfer Learning for glitch classification, using real data from LIGO's first \ndiscovery campaign labeled by Gravity Spy, showing that knowledge from \npre-trained models for real-world object recognition can be transferred for \nclassifying spectrograms of glitches. We demonstrate that this method enables \nthe optimal use of very deep convolutional neural networks for glitch \nclassification given small unbalanced training datasets, significantly reduces \nthe training time, and achieves state-of-the-art accuracy above 98.8%. Once \ntrained via transfer learning, we show that the networks can be truncated and \nused as feature extractors for unsupervised clustering to automatically group \ntogether new classes of glitches and anomalies. This novel capability is of \ncritical importance to identify and remove new types of glitches which will \noccur as the LIGO/Virgo detectors gradually attain design sensitivity. \n</p>"}, "author": "Daniel George, Hongyu Shen, E. A. Huerta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1513055249337", "timestampUsec": "1513055249337119", "id": "tag:google.com,2005:reader/item/00000003454dd069", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SolarisNet: A Deep Regression Network for Solar Radiation Prediction. (arXiv:1711.08413v2 [cs.CV] UPDATED)", "published": 1513055249, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08413"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08413", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Effective utilization of photovoltaic (PV) plants requires weather \nvariability robust global solar radiation (GSR) forecasting models. Random \nweather turbulence phenomena coupled with assumptions of clear sky model as \nsuggested by Hottel pose significant challenges to parametric &amp; non-parametric \nmodels in GSR conversion rate estimation. Also, a decent GSR estimate requires \ncostly high-tech radiometer and expert dependent instrument handling and \nmeasurements, which are subjective. As such, a computer aided monitoring (CAM) \nsystem to evaluate PV plant operation feasibility by employing smart grid past \ndata analytics and deep learning is developed. Our algorithm, SolarisNet is a \n6-layer deep neural network trained on data collected at two weather stations \nlocated near Kalyani metrological site, West Bengal, India. The daily GSR \nprediction performance using SolarisNet outperforms the existing state of art \nand its efficacy in inferring past GSR data insights to comprehend daily and \nseasonal GSR variability along with its competence for short term forecasting \nis discussed. \n</p>"}, "author": "Subhadip Dey, Sawon Pratiher, Saon Banerjee, Chanchal Kumar Mukherjee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619087", "id": "tag:google.com,2005:reader/item/0000000344902761", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Building competitive direct acoustics-to-word models for English conversational speech recognition. (arXiv:1712.03133v1 [cs.CL])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03133"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03133", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Direct acoustics-to-word (A2W) models in the end-to-end paradigm have \nreceived increasing attention compared to conventional sub-word based automatic \nspeech recognition models using phones, characters, or context-dependent hidden \nMarkov model states. This is because A2W models recognize words from speech \nwithout any decoder, pronunciation lexicon, or externally-trained language \nmodel, making training and decoding with such models simple. Prior work has \nshown that A2W models require orders of magnitude more training data in order \nto perform comparably to conventional models. Our work also showed this \naccuracy gap when using the English Switchboard-Fisher data set. This paper \ndescribes a recipe to train an A2W model that closes this gap and is at-par \nwith state-of-the-art sub-word based models. We achieve a word error rate of \n8.8%/13.9% on the Hub5-2000 Switchboard/CallHome test sets without any decoder \nor language model. We find that model initialization, training data order, and \nregularization have the most impact on the A2W model performance. Next, we \npresent a joint word-character A2W model that learns to first spell the word \nand then recognize it. This model provides a rich output to the user instead of \nsimple word hypotheses, making it especially useful in the case of words unseen \nor rarely-seen during training. \n</p>"}, "author": "Kartik Audhkhasi, Brian Kingsbury, Bhuvana Ramabhadran, George Saon, Michael Picheny", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619086", "id": "tag:google.com,2005:reader/item/0000000344902782", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improving Brain Storm Optimization Algorithm via Simplex Search. (arXiv:1712.03166v1 [cs.NE])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03166"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03166", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Through modeling human's brainstorming process, the brain storm optimization \n(BSO) algorithm has become a promising population based evolution algorithm. \nHowever, BSO is often good at global exploration but not good enough at local \nexploitation, just like most global optimization algorithms. In this paper, the \nNelder-Mead's Simplex (NMS) method is adopted in a simple version of BSO. Our \ngoal is to combine BSO's exploration ability and NMS's exploitation ability \ntogether, and develop an enhanced BSO via a better balance between global \nexploration and local exploitation. Large number of experimental results are \nreported, and the proposed algorithm is shown to perform better than both BSO \nand NMS. \n</p>"}, "author": "Wei Chen, YingYing Cao, Yifei Sun, Qunfeng Liu, Yun Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619084", "id": "tag:google.com,2005:reader/item/00000003449027ab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "A Deep Network Model for Paraphrase Detection in Short Text Messages. (arXiv:1712.02820v1 [cs.IR])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02820"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02820", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper is concerned with paraphrase detection. The ability to detect \nsimilar sentences written in natural language is crucial for several \napplications, such as text mining, text summarization, plagiarism detection, \nauthorship authentication and question answering. Given two sentences, the \nobjective is to detect whether they are semantically identical. An important \ninsight from this work is that existing paraphrase systems perform well when \napplied on clean texts, but they do not necessarily deliver good performance \nagainst noisy texts. Challenges with paraphrase detection on user generated \nshort texts, such as Twitter, include language irregularity and noise. To cope \nwith these challenges, we propose a novel deep neural network-based approach \nthat relies on coarse-grained sentence modeling using a convolutional neural \nnetwork and a long short-term memory model, combined with a specific \nfine-grained word-level similarity matching model. Our experimental results \nshow that the proposed approach outperforms existing state-of-the-art \napproaches on user-generated noisy social media data, such as Twitter texts, \nand achieves highly competitive performance on a cleaner corpus. \n</p>"}, "author": "Basant Agarwal, Heri Ramampiaro, Helge Langseth, Massimiliano Ruocco", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619083", "id": "tag:google.com,2005:reader/item/00000003449027b4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "End-to-End Offline Goal-Oriented Dialog Policy Learning via Policy Gradient. (arXiv:1712.02838v1 [cs.AI])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02838"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02838", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning a goal-oriented dialog policy is generally performed offline with \nsupervised learning algorithms or online with reinforcement learning (RL). \nAdditionally, as companies accumulate massive quantities of dialog transcripts \nbetween customers and trained human agents, encoder-decoder methods have gained \npopularity as agent utterances can be directly treated as supervision without \nthe need for utterance-level annotations. However, one potential drawback of \nsuch approaches is that they myopically generate the next agent utterance \nwithout regard for dialog-level considerations. To resolve this concern, this \npaper describes an offline RL method for learning from unannotated corpora that \ncan optimize a goal-oriented policy at both the utterance and dialog level. We \nintroduce a novel reward function and use both on-policy and off-policy policy \ngradient to learn a policy offline without requiring online user interaction or \nan explicit state space definition. \n</p>"}, "author": "Li Zhou, Kevin Small, Oleg Rokhlenko, Charles Elkan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619082", "id": "tag:google.com,2005:reader/item/00000003449027c8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Per-Pixel Feedback for improving Semantic Segmentation. (arXiv:1712.02861v1 [cs.AI])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02861"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02861", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325050ddcba\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325050ddcba&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Semantic segmentation is the task of assigning a label to each pixel in the \nimage.In recent years, deep convolutional neural networks have been driving \nadvances in multiple tasks related to cognition. Although, DCNNs have resulted \nin unprecedented visual recognition performances, they offer little \ntransparency. To understand how DCNN based models work at the task of semantic \nsegmentation, we try to analyze the DCNN models in semantic segmentation. We \ntry to find the importance of global image information for labeling pixels. \n</p> \n<p>Based on the experiments on discriminative regions, and modeling of \nfixations, we propose a set of new training loss functions for fine-tuning DCNN \nbased models. The proposed training regime has shown improvement in performance \nof DeepLab Large FOV(VGG-16) Segmentation model for PASCAL VOC 2012 dataset. \nHowever, further test remains to conclusively evaluate the benefits due to the \nproposed loss functions across models, and data-sets. \n</p> \n<p>Submitted in part fulfillment of the requirements for the degree of \nIntegrated Masters of Science in Applied Mathematics. \n</p> \n<p>Update: Further Experiment showed minimal benefits. \n</p> \n<p>Code Available [here](https://github.com/BardOfCodes/Seg-Unravel). \n</p>"}, "author": "Aditya Ganeshan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619081", "id": "tag:google.com,2005:reader/item/00000003449027ea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Perspectival Knowledge in PSOA RuleML: Representation, Model Theory, and Translation. (arXiv:1712.02869v1 [cs.AI])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02869"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02869", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32505151fa3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32505151fa3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In Positional-Slotted Object-Applicative (PSOA) RuleML, a predicate \napplication (atom) can have an Object IDentifier (OID) and descriptors that may \nbe positional arguments (tuples) or attribute-value pairs (slots). PSOA RuleML \n1.0 specifies for each descriptor whether it is to be interpreted under the \nperspective of the predicate in whose scope it occurs. This perspectivity \ndimension refines the space between oidless, positional atoms (relationships) \nand oidful, slotted atoms (frames): While relationships use only a \npredicate-scope-sensitive (predicate-dependent) tuple and frames use only \npredicate-scope-insensitive (predicate-independent) slots, PSOA RuleML 1.0 uses \na systematics of orthogonal constructs also permitting atoms with \n(predicate-)independent tuples and atoms with (predicate-)dependent slots. This \nsupports data and knowledge representation where a slot attribute can have \ndifferent values depending on the predicate. PSOA thus extends object-oriented \nmulti-membership and multiple inheritance. Based on objectification, PSOA laws \nare given: Besides unscoping and centralization, the semantic restriction and \ntransformation of describution permits rescoping of one atom's independent \ndescriptors to another atom with the same OID but a different predicate. For \ninheritance, default descriptors are realized by rules. On top of a metamodel \nand a Grailog visualization, PSOA's atom systematics for facts, queries, and \nrules is explained. The presentation and (XML-)serialization syntaxes of PSOA \nRuleML 1.0 are introduced. Its model-theoretic semantics is formalized by \nextending the earlier interpretation functions for dependent descriptors. The \nopen-source PSOATransRun 1.3 system realizes PSOA RuleML 1.0 by a translator to \nruntime predicates, including for dependent tuples (prdtupterm) and slots \n(prdsloterm). Our tests show efficiency advantages of dependent and tupled \nmodeling. \n</p>"}, "author": "Harold Boley, Gen Zou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619080", "id": "tag:google.com,2005:reader/item/00000003449027ff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Recruitment Market Trend Analysis with Sequential Latent Variable Models. (arXiv:1712.02975v1 [cs.AI])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02975"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02975", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recruitment market analysis provides valuable understanding of \nindustry-specific economic growth and plays an important role for both \nemployers and job seekers. With the rapid development of online recruitment \nservices, massive recruitment data have been accumulated and enable a new \nparadigm for recruitment market analysis. However, traditional methods for \nrecruitment market analysis largely rely on the knowledge of domain experts and \nclassic statistical models, which are usually too general to model large-scale \ndynamic recruitment data, and have difficulties to capture the fine-grained \nmarket trends. To this end, in this paper, we propose a new research paradigm \nfor recruitment market analysis by leveraging unsupervised learning techniques \nfor automatically discovering recruitment market trends based on large-scale \nrecruitment data. Specifically, we develop a novel sequential latent variable \nmodel, named MTLVM, which is designed for capturing the sequential dependencies \nof corporate recruitment states and is able to automatically learn the latent \nrecruitment topics within a Bayesian generative framework. In particular, to \ncapture the variability of recruitment topics over time, we design hierarchical \ndirichlet processes for MTLVM. These processes allow to dynamically generate \nthe evolving recruitment topics. Finally, we implement a prototype system to \nempirically evaluate our approach based on real-world recruitment data in \nChina. Indeed, by visualizing the results from MTLVM, we can successfully \nreveal many interesting findings, such as the popularity of LBS related jobs \nreached the peak in the 2nd half of 2014, and decreased in 2015. \n</p>"}, "author": "Chen Zhu, Hengshu Zhu, Hui Xiong, Pengliang Ding, Fang Xie", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619079", "id": "tag:google.com,2005:reader/item/0000000344902809", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Dual Coordinate Descent with Bandit Sampling. (arXiv:1712.03010v1 [cs.LG])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03010"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03010", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Coordinate descent methods minimize a cost function by updating a single \ndecision variable (corresponding to one coordinate) at a time. Ideally, one \nwould update the decision variable that yields the largest marginal decrease in \nthe cost function. However, finding this coordinate would require checking all \nof them, which is not computationally practical. We instead propose a new \nadaptive method for coordinate descent. First, we define a lower bound on the \ndecrease of the cost function when a coordinate is updated and, instead of \ncalculating this lower bound for all coordinates, we use a multi-armed bandit \nalgorithm to learn which coordinates result in the largest marginal decrease \nwhile simultaneously performing coordinate descent. We show that our approach \nimproves the convergence of the coordinate methods (including parallel \nversions) both theoretically and experimentally. \n</p>"}, "author": "Farnood Salehi, Patrick Thiran, L. Elisa Celis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619078", "id": "tag:google.com,2005:reader/item/0000000344902814", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Heuristic Search Algorithm Using the Stability of Learning Algorithms as the Fitness Function in Certain Scenarios: An Artificial General Intelligence Engineering Approach. (arXiv:1712.03043v1 [cs.AI])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03043"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03043", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a non-manual design engineering method based on heuristic \nsearch algorithm to search for candidate agents in the solution space which \nformed by artificial intelligence agents modeled on the base of \nbionics.Compared with the artificial design method represented by meta-learning \nand the bionics method represented by the neural architecture chip,this method \nis more feasible for realizing artificial general intelligence,and it has a \nmuch better interaction with cognitive neuroscience;at the same time,the \nengineering method is based on the theoretical hypothesis that the final \nlearning algorithm is stable in certain scenarios,and has generalization \nability in various scenarios.The paper discusses the theory preliminarily and \nproposes the possible correlation between the theory and the fixed-point \ntheorem in the field of mathematics.Limited by the author's knowledge \nlevel,this correlation is proposed only as a kind of conjecture. \n</p>"}, "author": "Zengkun Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619077", "id": "tag:google.com,2005:reader/item/0000000344902818", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "FlagIt: A System for Minimally Supervised Human Trafficking Indicator Mining. (arXiv:1712.03086v1 [cs.CY])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03086"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03086", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we describe and study the indicator mining problem in the \nonline sex advertising domain. We present an in-development system, FlagIt \n(Flexible and adaptive generation of Indicators from text), which combines the \nbenefits of both a lightweight expert system and classical semi-supervision \n(heuristic re-labeling) with recently released state-of-the-art unsupervised \ntext embeddings to tag millions of sentences with indicators that are highly \ncorrelated with human trafficking. The FlagIt technology stack is open source. \nOn preliminary evaluations involving five indicators, FlagIt illustrates \npromising performance compared to several alternatives. The system is being \nactively developed, refined and integrated into a domain-specific search system \nused by over 200 law enforcement agencies to combat human trafficking, and is \nbeing aggressively extended to mine at least six more indicators with minimal \nprogramming effort. FlagIt is a good example of a system that operates in \nlimited label settings, and that requires creative combinations of established \nmachine learning techniques to produce outputs that could be used by real-world \nnon-technical analysts. \n</p>"}, "author": "Mayank Kejriwal, Jiayuan Ding, Runqi Shao, Anoop Kumar, Pedro Szekely", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619076", "id": "tag:google.com,2005:reader/item/0000000344902822", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Class of Logistic Functions for Approximating State-Inclusive Koopman Operators. (arXiv:1712.03132v1 [cs.LG])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03132"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03132", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>An outstanding challenge in nonlinear systems theory is identification or \nlearning of a given nonlinear system's Koopman operator directly from data or \nmodels. Advances in extended dynamic mode decomposition approaches and machine \nlearning methods have enabled data-driven discovery of Koopman operators, for \nboth continuous and discrete-time systems. Since Koopman operators are often \ninfinite-dimensional, they are approximated in practice using \nfinite-dimensional systems. The fidelity and convergence of a given \nfinite-dimensional Koopman approximation is a subject of ongoing research. In \nthis paper we introduce a class of Koopman observable functions that confer an \napproximate closure property on their corresponding finite-dimensional \napproximations of the Koopman operator. We derive error bounds for the fidelity \nof this class of observable functions, as well as identify two key learning \nparameters which can be used to tune performance. We illustrate our approach on \ntwo classical nonlinear system models: the Van Der Pol oscillator and the \nbistable toggle switch. \n</p>"}, "author": "Charles A. Johnson, Enoch Yeung", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619075", "id": "tag:google.com,2005:reader/item/0000000344902832", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Empirical Evaluation of Abstract Argumentation: Supporting the Need for Bipolar and Probabilistic Approaches. (arXiv:1707.09324v2 [cs.AI] UPDATED)", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.09324"}], "alternate": [{"href": "http://arxiv.org/abs/1707.09324", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In dialogical argumentation it is often assumed that the involved parties \nalways correctly identify the intended statements posited by each other, \nrealize all of the associated relations, conform to the three acceptability \nstates (accepted, rejected, undecided), adjust their views when new and correct \ninformation comes in, and that a framework handling only attack relations is \nsufficient to represent their opinions. Although it is natural to make these \nassumptions as a starting point for further research, removing them or even \nacknowledging that such removal should happen is more challenging for some of \nthese concepts than for others. Probabilistic argumentation is one of the \napproaches that can be harnessed for more accurate user modelling. The \nepistemic approach allows us to represent how much a given argument is believed \nby a given person, offering us the possibility to express more than just three \nagreement states. It is equipped with a wide range of postulates, including \nthose that do not make any restrictions concerning how initial arguments should \nbe viewed, thus potentially being more adequate for handling beliefs of the \npeople that have not fully disclosed their opinions in comparison to Dung's \nsemantics. The constellation approach can be used to represent the views of \ndifferent people concerning the structure of the framework we are dealing with, \nincluding cases in which not all relations are acknowledged or when they are \nseen differently than intended. Finally, bipolar argumentation frameworks can \nbe used to express both positive and negative relations between arguments. In \nthis paper we describe the results of an experiment in which participants \njudged dialogues in terms of agreement and structure. We compare our findings \nwith the aforementioned assumptions as well as with the constellation and \nepistemic approaches to probabilistic argumentation and bipolar argumentation. \n</p>"}, "author": "Sylwia Polberg, Anthony Hunter", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619074", "id": "tag:google.com,2005:reader/item/0000000344902846", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Prosocial learning agents solve generalized Stag Hunts better than selfish ones. (arXiv:1709.02865v2 [cs.AI] UPDATED)", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.02865"}], "alternate": [{"href": "http://arxiv.org/abs/1709.02865", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep reinforcement learning has become an important paradigm for constructing \nagents that can enter complex multi-agent situations and improve their policies \nthrough experience. One commonly used technique is reactive training - applying \nstandard RL methods while treating other agents as a part of the learner's \nenvironment. It is known that in general-sum games reactive training can lead \ngroups of agents to converge to inefficient outcomes. We focus on one such \nclass of environments: Stag Hunt games. Here agents either choose a risky \ncooperative policy (which leads to high payoffs if both choose it but low \npayoffs to an agent who attempts it alone) or a safe one (which leads to a safe \npayoff no matter what). We ask how we can change the learning rule of a single \nagent to improve its outcomes in Stag Hunts that include other reactive \nlearners. We extend existing work on reward-shaping in multi-agent \nreinforcement learning and show that that making a single agent prosocial, that \nis, making them care about the rewards of their partners can increase the \nprobability that groups converge to good outcomes. Thus, even if we control a \nsingle agent in a group making that agent prosocial can increase our agent's \nlong-run payoff. We show experimentally that this result carries over to a \nvariety of more complex environments with Stag Hunt-like dynamics including \nones where agents must learn from raw input pixels. \n</p>"}, "author": "Alexander Peysakhovich, Adam Lerer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619073", "id": "tag:google.com,2005:reader/item/0000000344902858", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Long Text Generation via Adversarial Training with Leaked Information. (arXiv:1709.08624v2 [cs.CL] UPDATED)", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.08624"}], "alternate": [{"href": "http://arxiv.org/abs/1709.08624", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Automatically generating coherent and semantically meaningful text has many \napplications in machine translation, dialogue systems, image captioning, etc. \nRecently, by combining with policy gradient, Generative Adversarial Nets (GAN) \nthat use a discriminative model to guide the training of the generative model \nas a reinforcement learning policy has shown promising results in text \ngeneration. However, the scalar guiding signal is only available after the \nentire text has been generated and lacks intermediate information about text \nstructure during the generative process. As such, it limits its success when \nthe length of the generated text samples is long (more than 20 words). In this \npaper, we propose a new framework, called LeakGAN, to address the problem for \nlong text generation. We allow the discriminative net to leak its own \nhigh-level extracted features to the generative net to further help the \nguidance. The generator incorporates such informative signals into all \ngeneration steps through an additional Manager module, which takes the \nextracted features of current generated words and outputs a latent vector to \nguide the Worker module for next-word generation. Our extensive experiments on \nsynthetic data and various real-world tasks with Turing test demonstrate that \nLeakGAN is highly effective in long text generation and also improves the \nperformance in short text generation scenarios. More importantly, without any \nsupervision, LeakGAN would be able to implicitly learn sentence structures only \nthrough the interaction between Manager and Worker. \n</p>"}, "author": "Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, Jun Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619072", "id": "tag:google.com,2005:reader/item/0000000344902869", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Deep Neural Network Representations for Koopman Operators of Nonlinear Dynamical Systems. (arXiv:1708.06850v2 [cs.LG] CROSS LISTED)", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.06850"}], "alternate": [{"href": "http://arxiv.org/abs/1708.06850", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32505152200\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32505152200&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The Koopman operator has recently garnered much attention for its value in \ndynamical systems analysis and data-driven model discovery. However, its \napplication has been hindered by the computational complexity of extended \ndynamic mode decomposition; this requires a combinatorially large basis set to \nadequately describe many nonlinear systems of interest, e.g. cyber-physical \ninfrastructure systems, biological networks, social systems, and fluid \ndynamics. Often the dictionaries generated for these problems are manually \ncurated, requiring domain-specific knowledge and painstaking tuning. In this \npaper we introduce a deep learning framework for learning Koopman operators of \nnonlinear dynamical systems. We show that this novel method automatically \nselects efficient deep dictionaries, outperforming state-of-the-art methods. We \nbenchmark this method on partially observed nonlinear systems, including the \nglycolytic oscillator and show it is able to predict quantitatively 100 steps \ninto the future, using only a single timepoint, and qualitative oscillatory \nbehavior 400 steps into the future. \n</p>"}, "author": "Enoch Yeung, Soumya Kundu, Nathan Hodas", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619071", "id": "tag:google.com,2005:reader/item/0000000344902883", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "RelNN: A Deep Neural Model for Relational Learning. (arXiv:1712.02831v1 [stat.ML])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02831"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02831", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Statistical relational AI (StarAI) aims at reasoning and learning in noisy \ndomains described in terms of objects and relationships by combining \nprobability with first-order logic. With huge advances in deep learning in the \ncurrent years, combining deep networks with first-order logic has been the \nfocus of several recent studies. Many of the existing attempts, however, only \nfocus on relations and ignore object properties. The attempts that do consider \nobject properties are limited in terms of modelling power or scalability. In \nthis paper, we develop relational neural networks (RelNNs) by adding hidden \nlayers to relational logistic regression (the relational counterpart of \nlogistic regression). We learn latent properties for objects both directly and \nthrough general rules. Back-propagation is used for training these models. A \nmodular, layer-wise architecture facilitates utilizing the techniques developed \nwithin deep learning community to our architecture. Initial experiments on \neight tasks over three real-world datasets show that RelNNs are promising \nmodels for relational learning. \n</p>"}, "author": "Seyed Mehran Kazemi, David Poole", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619070", "id": "tag:google.com,2005:reader/item/0000000344902893", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic reconstruction of an oolitic limestone by generative adversarial networks. (arXiv:1712.02854v1 [cs.CV])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02854"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02854", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Stochastic image reconstruction is a key part of modern digital rock physics \nand materials analysis that aims to create numerous representative samples of \nmaterial micro-structures for upscaling, numerical computation of effective \nproperties and uncertainty quantification. We present a method of \nthree-dimensional stochastic image reconstruction based on generative \nadversarial neural networks (GANs). GANs represent a framework of unsupervised \nlearning methods that require no a priori inference of the probability \ndistribution associated with the training data. Using a fully convolutional \nneural network allows fast sampling of large volumetric images.We apply a GAN \nbased workflow of network training and image generation to an oolitic Ketton \nlimestone micro-CT dataset. Minkowski functionals, effective permeability as \nwell as velocity distributions of simulated flow within the acquired images are \ncompared with the synthetic reconstructions generated by the deep neural \nnetwork. While our results show that GANs allow a fast and accurate \nreconstruction of the evaluated image dataset, we address a number of open \nquestions and challenges involved in the evaluation of generative network-based \nmethods. \n</p>"}, "author": "Lukas Mosser, Olivier Dubrule, Martin J. Blunt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619069", "id": "tag:google.com,2005:reader/item/00000003449028b6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multiple Adaptive Bayesian Linear Regression for Scalable Bayesian Optimization with Warm Start. (arXiv:1712.02902v1 [stat.ML])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02902"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02902", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Bayesian optimization (BO) is a model-based approach for gradient-free \nblack-box function optimization. Typically, BO is powered by a Gaussian process \n(GP), whose algorithmic complexity is cubic in the number of evaluations. \nHence, GP-based BO cannot leverage large amounts of past or related function \nevaluations, for example, to warm start the BO procedure. We develop a multiple \nadaptive Bayesian linear regression model as a scalable alternative whose \ncomplexity is linear in the number of observations. The multiple Bayesian \nlinear regression models are coupled through a shared feedforward neural \nnetwork, which learns a joint representation and transfers knowledge across \nmachine learning problems. \n</p>"}, "author": "Valerio Perrone, Rodolphe Jenatton, Matthias Seeger, Cedric Archambeau", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619068", "id": "tag:google.com,2005:reader/item/00000003449028cb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Blind Multi-class Ensemble Learning with Unequally Reliable Classifiers. (arXiv:1712.02903v1 [stat.ML])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02903"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02903", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The rising interest in pattern recognition and data analytics has spurred the \ndevelopment of innovative machine learning algorithms and tools. However, as \neach algorithm has its strengths and limitations, one is motivated to \njudiciously fuse multiple algorithms in order to find the \"best\" performing \none, for a given dataset. Ensemble learning aims at such high-performance \nmeta-algorithm, by combining the outputs from multiple algorithms. The present \nwork introduces a blind scheme for learning from ensembles of classifiers, \nusing a moment matching method that leverages joint tensor and matrix \nfactorization. Blind refers to the combiner who has no knowledge of the \nground-truth labels that each classifier has been trained on. A rigorous \nperformance analysis is derived and the proposed scheme is evaluated on \nsynthetic and real datasets. \n</p>"}, "author": "Panagiotis A. Traganitis, Alba Pag&#xe8;s-Zamora, Georgios B. Giannakis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619067", "id": "tag:google.com,2005:reader/item/00000003449028d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "CycleGAN: a Master of Steganography. (arXiv:1712.02950v1 [cs.CV])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02950"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02950", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>CycleGAN is one of the latest successful approaches to learn a correspondence \nbetween two image distributions. In a series of experiments, we demonstrate an \nintriguing property of the model: CycleGAN learns to \"hide\" information about a \nsource image inside the generated image in nearly imperceptible, high-frequency \nnoise. This trick ensures that the complementary generator can recover the \noriginal sample and thus satisfy the cyclic consistency requirement, but the \ngenerated image remains realistic. We connect this phenomenon with adversarial \nattacks by viewing CycleGAN's training procedure as training a generator of \nadversarial examples, thereby showing that adversarial attacks are not limited \nto classifiers but also may target generative models. \n</p>"}, "author": "Casey Chu, Andrey Zhmoginov, Mark Sandler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619066", "id": "tag:google.com,2005:reader/item/00000003449028dc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On Adaptive Estimation for Dynamic Bernoulli Bandits. (arXiv:1712.03134v1 [stat.ML])", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.03134"}], "alternate": [{"href": "http://arxiv.org/abs/1712.03134", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The multi-armed bandit (MAB) problem is a classic example of the \nexploration-exploitation dilemma. It is concerned with maximising the total \nrewards for a gambler by sequentially pulling an arm from a multi-armed slot \nmachine where each arm is associated with a reward distribution. In static \nMABs, the reward distributions do not change over time, while in dynamic MABs, \neach arm's reward distribution can change, and the optimal arm can switch over \ntime. Motivated by many real applications where rewards are binary counts, we \nfocus on dynamic Bernoulli bandits. Standard methods like $\\epsilon$-Greedy and \nUpper Confidence Bound (UCB), which rely on the sample mean estimator, often \nfail to track the changes in underlying reward for dynamic problems. In this \npaper, we overcome the shortcoming of slow response to change by deploying \nadaptive estimation in the standard methods and propose a new family of \nalgorithms, which are adaptive versions of $\\epsilon$-Greedy, UCB, and Thompson \nsampling. These new methods are simple and easy to implement. Moreover, they do \nnot require any prior knowledge about the data, which is important for real \napplications. We examine the new algorithms numerically in different scenarios \nand find out that the results show solid improvements of our algorithms in \ndynamic environments. \n</p>"}, "author": "Xue Lu, Niall Adams, Nikolas Kantas", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619065", "id": "tag:google.com,2005:reader/item/00000003449028e9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Exponential Machines. (arXiv:1605.03795v3 [stat.ML] UPDATED)", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1605.03795"}], "alternate": [{"href": "http://arxiv.org/abs/1605.03795", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Modeling interactions between features improves the performance of machine \nlearning solutions in many domains (e.g. recommender systems or sentiment \nanalysis). In this paper, we introduce Exponential Machines (ExM), a predictor \nthat models all interactions of every order. The key idea is to represent an \nexponentially large tensor of parameters in a factorized format called Tensor \nTrain (TT). The Tensor Train format regularizes the model and lets you control \nthe number of underlying parameters. To train the model, we develop a \nstochastic Riemannian optimization procedure, which allows us to fit tensors \nwith 2^160 entries. We show that the model achieves state-of-the-art \nperformance on synthetic data with high-order interactions and that it works on \npar with high-order factorization machines on a recommender system dataset \nMovieLens 100K. \n</p>"}, "author": "Alexander Novikov, Mikhail Trofimov, Ivan Oseledets", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619064", "id": "tag:google.com,2005:reader/item/00000003449028f1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Projected Power Method: An Efficient Algorithm for Joint Alignment from Pairwise Differences. (arXiv:1609.05820v3 [cs.IT] UPDATED)", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1609.05820"}], "alternate": [{"href": "http://arxiv.org/abs/1609.05820", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Various applications involve assigning discrete label values to a collection \nof objects based on some pairwise noisy data. Due to the discrete---and hence \nnonconvex---structure of the problem, computing the optimal assignment \n(e.g.~maximum likelihood assignment) becomes intractable at first sight. This \npaper makes progress towards efficient computation by focusing on a concrete \njoint alignment problem---that is, the problem of recovering $n$ discrete \nvariables $x_i \\in \\{1,\\cdots, m\\}$, $1\\leq i\\leq n$ given noisy observations \nof their modulo differences $\\{x_i - x_j~\\mathsf{mod}~m\\}$. We propose a \nlow-complexity and model-free procedure, which operates in a lifted space by \nrepresenting distinct label values in orthogonal directions, and which attempts \nto optimize quadratic functions over hypercubes. Starting with a first guess \ncomputed via a spectral method, the algorithm successively refines the iterates \nvia projected power iterations. We prove that for a broad class of statistical \nmodels, the proposed projected power method makes no error---and hence \nconverges to the maximum likelihood estimate---in a suitable regime. Numerical \nexperiments have been carried out on both synthetic and real data to \ndemonstrate the practicality of our algorithm. We expect this algorithmic \nframework to be effective for a broad range of discrete assignment problems. \n</p>"}, "author": "Yuxin Chen, Emmanuel Candes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619063", "id": "tag:google.com,2005:reader/item/00000003449028f9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs by Selective Execution. (arXiv:1701.00299v2 [cs.LG] UPDATED)", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1701.00299"}], "alternate": [{"href": "http://arxiv.org/abs/1701.00299", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce Dynamic Deep Neural Networks (D2NN), a new type of feed-forward \ndeep neural network that allows selective execution. Given an input, only a \nsubset of D2NN neurons are executed, and the particular subset is determined by \nthe D2NN itself. By pruning unnecessary computation depending on input, D2NNs \nprovide a way to improve computational efficiency. To achieve dynamic selective \nexecution, a D2NN augments a feed-forward deep neural network (directed acyclic \ngraph of differentiable modules) with controller modules. Each controller \nmodule is a sub-network whose output is a decision that controls whether other \nmodules can execute. A D2NN is trained end to end. Both regular and controller \nmodules in a D2NN are learnable and are jointly trained to optimize both \naccuracy and efficiency. Such training is achieved by integrating \nbackpropagation with reinforcement learning. With extensive experiments of \nvarious D2NN architectures on image classification tasks, we demonstrate that \nD2NNs are general and flexible, and can effectively optimize \naccuracy-efficiency trade-offs. \n</p>"}, "author": "Lanlan Liu, Jia Deng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619062", "id": "tag:google.com,2005:reader/item/0000000344902907", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Analyzing the Robustness of Nearest Neighbors to Adversarial Examples. (arXiv:1706.03922v2 [stat.ML] UPDATED)", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.03922"}], "alternate": [{"href": "http://arxiv.org/abs/1706.03922", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32505152447\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32505152447&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Motivated by applications such as autonomous vehicles, test-time attacks via \nadversarial examples have received a great deal of recent attention. In this \nsetting, an adversary is capable of making queries to a classifier, and \nperturbs a test example by a small amount in order to force the classifier to \nreport an incorrect label. While a long line of work has explored a number of \nattacks, not many reliable defenses are known, and there is an overall lack of \ngeneral understanding about the foundations of designing machine learning \nalgorithms robust to adversarial examples. \n</p> \n<p>In this paper, we take a step towards addressing this challenging question by \nintroducing a new theoretical framework, analogous to bias-variance theory, \nwhich we can use to tease out the causes of vulnerability. We apply our \nframework to a simple classification algorithm: nearest neighbors, and analyze \nits robustness to adversarial examples. Motivated by our analysis, we propose a \nmodified version of the nearest neighbor algorithm, and demonstrate both \ntheoretically and empirically that it has superior robustness to standard \nnearest neighbors. \n</p>"}, "author": "Yizhen Wang, Somesh Jha, Kamalika Chaudhuri", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619061", "id": "tag:google.com,2005:reader/item/000000034490290c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Improving Negative Sampling for Word Representation using Self-embedded Features. (arXiv:1710.09805v2 [cs.LG] UPDATED)", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09805"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09805", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325051be89b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325051be89b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Although the word-popularity based negative sampler has shown superb \nperformance in the skip-gram model, the theoretical motivation behind \noversampling popular (non-observed) words as negative samples is still not well \nunderstood. In this paper, we start from an investigation of the gradient \nvanishing issue in the skip-gram model without a proper negative sampler. By \nperforming an insightful analysis from the stochastic gradient descent (SGD) \nlearning perspective, we demonstrate that, both theoretically and intuitively, \nnegative samples with larger inner product scores are more informative than \nthose with lower scores for the SGD learner in terms of both convergence rate \nand accuracy. Understanding this, we propose an alternative sampling algorithm \nthat dynamically selects informative negative samples during each SGD update. \nMore importantly, the proposed sampler accounts for multi-dimensional \nself-embedded features during the sampling process, which essentially makes it \nmore effective than the original popularity-based (one-dimensional) sampler. \nEmpirical experiments further verify our observations, and show that our \nfine-grained samplers gain significant improvement over the existing ones \nwithout increasing computational complexity. \n</p>"}, "author": "Long Chen, Fajie Yuan, Joemon M. Jose, Weinan Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619060", "id": "tag:google.com,2005:reader/item/0000000344902913", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian Paragraph Vectors. (arXiv:1711.03946v2 [cs.CL] UPDATED)", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03946"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03946", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Word2vec (Mikolov et al., 2013) has proven to be successful in natural \nlanguage processing by capturing the semantic relationships between different \nwords. Built on top of single-word embeddings, paragraph vectors (Le and \nMikolov, 2014) find fixed-length representations for pieces of text with \narbitrary lengths, such as documents, paragraphs, and sentences. In this work, \nwe propose a novel interpretation for neural-network-based paragraph vectors by \ndeveloping an unsupervised generative model whose maximum likelihood solution \ncorresponds to traditional paragraph vectors. This probabilistic formulation \nallows us to go beyond point estimates of parameters and to perform Bayesian \nposterior inference. We find that the entropy of paragraph vectors decreases \nwith the length of documents, and that information about posterior uncertainty \nimproves performance in supervised learning tasks such as sentiment analysis \nand paraphrase detection. \n</p>"}, "author": "Geng Ji, Robert Bamler, Erik B. Sudderth, Stephan Mandt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619059", "id": "tag:google.com,2005:reader/item/0000000344902917", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "BoostJet: Towards Combining Statistical Aggregates with Neural Embeddings for Recommendations. (arXiv:1711.05828v2 [cs.IR] UPDATED)", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05828"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05828", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recommenders have become widely popular in recent years because of their \nbroader applicability in many e-commerce applications. These applications rely \non recommenders for generating advertisements for various offers or providing \ncontent recommendations. However, the quality of the generated recommendations \ndepends on user features (like demography, temporality), offer features (like \npopularity, price), and user-offer features (like implicit or explicit \nfeedback). Current state-of-the-art recommenders do not explore such diverse \nfeatures concurrently while generating the recommendations. \n</p> \n<p>In this paper, we first introduce the notion of Trackers which enables us to \ncapture the above-mentioned features and thus incorporate users' online \nbehaviour through statistical aggregates of different features (demography, \ntemporality, popularity, price). We also show how to capture offer-to-offer \nrelations, based on their consumption sequence, leveraging neural embeddings \nfor offers in our Offer2Vec algorithm. We then introduce BoostJet, a novel \nrecommender which integrates the Trackers along with the neural embeddings \nusing MatrixNet, an efficient distributed implementation of gradient boosted \ndecision tree, to improve the recommendation quality significantly. We provide \nan in-depth evaluation of BoostJet on Yandex's dataset, collecting online \nbehaviour from tens of millions of online users, to demonstrate the \npracticality of BoostJet in terms of recommendation quality as well as \nscalability. \n</p>"}, "author": "Rhicheek Patra, Egor Samosvat, Michael Roizner, Andrei Mishchenko", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619058", "id": "tag:google.com,2005:reader/item/0000000344902938", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sequences, Items And Latent Links: Recommendation With Consumed Item Packs. (arXiv:1711.06100v2 [cs.IR] UPDATED)", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06100"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06100", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recommenders personalize the web content by typically using collaborative \nfiltering to relate users (or items) based on explicit feedback, e.g., ratings. \nThe difficulty of collecting this feedback has recently motivated to consider \nimplicit feedback (e.g., item consumption along with the corresponding time). \n</p> \n<p>In this paper, we introduce the notion of consumed item pack (CIP) which \nenables to link users (or items) based on their implicit analogous consumption \nbehavior. Our proposal is generic, and we show that it captures three novel \nimplicit recommenders: a user-based (CIP-U), an item-based (CIP-I), and a word \nembedding-based (DEEPCIP), as well as a state-of-the-art technique using \nimplicit feedback (FISM). We show that our recommenders handle incremental \nupdates incorporating freshly consumed items. We demonstrate that all three \nrecommenders provide a recommendation quality that is competitive with \nstate-of-the-art ones, including one incorporating both explicit and implicit \nfeedback. \n</p>"}, "author": "Rachid Guerraoui, Erwan Le Merrer, Rhicheek Patra, Jean-Ronan Vigouroux", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512969981619", "timestampUsec": "1512969981619057", "id": "tag:google.com,2005:reader/item/0000000344902944", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Video Generation, Prediction and Completion of Human Action Sequences. (arXiv:1711.08682v3 [cs.CV] UPDATED)", "published": 1512969982, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08682"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08682", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Current deep learning results on video generation are limited while there are \nonly a few first results on video prediction and no relevant significant \nresults on video completion. This is due to the severe ill-posedness inherent \nin these three problems. In this paper, we focus on human action videos, and \npropose a general, two-stage deep framework to generate human action videos \nwith no constraints or arbitrary number of constraints, which uniformly address \nthe three problems: video generation given no input frames, video prediction \ngiven the first few frames, and video completion given the first and last \nframes. To make the problem tractable, in the first stage we train a deep \ngenerative model that generates a human pose sequence from random noise. In the \nsecond stage, a skeleton-to-image network is trained, which is used to generate \na human action video given the complete human pose sequence generated in the \nfirst stage. By introducing the two-stage strategy, we sidestep the original \nill-posed problems while producing for the first time high-quality video \ngeneration/prediction/completion results of much longer duration. We present \nquantitative and qualitative evaluation to show that our two-stage approach \noutperforms state-of-the-art methods in video generation, prediction and video \ncompletion. Our video result demonstration can be viewed at \nhttps://iamacewhite.github.io/supp/index.html \n</p>"}, "author": "Haoye Cai, Chunyan Bai, Yu-Wing Tai, Chi-Keung Tang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983976", "id": "tag:google.com,2005:reader/item/0000000342b7c7c7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "CNNs are Globally Optimal Given Multi-Layer Support. (arXiv:1712.02501v1 [cs.LG])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02501"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02501", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Stochastic Gradient Descent (SGD) is the central workhorse for training \nmodern CNNs. Although giving impressive empirical performance it can be slow to \nconverge. In this paper we explore a novel strategy for training a CNN using an \nalternation strategy that offers substantial speedups during training. We make \nthe following contributions: (i) replace the ReLU non-linearity within a CNN \nwith positive hard-thresholding, (ii) reinterpret this non-linearity as a \nbinary state vector making the entire CNN linear if the multi-layer support is \nknown, and (iii) demonstrate that under certain conditions a global optima to \nthe CNN can be found through local descent. We then employ a novel alternation \nstrategy (between weights and support) for CNN training that leads to \nsubstantially faster convergence rates, nice theoretical properties, and \nachieving state of the art results across large scale datasets (e.g. ImageNet) \nas well as other standard benchmarks. \n</p>"}, "author": "Chen Huang, Chen Kong, Simon Lucey", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983975", "id": "tag:google.com,2005:reader/item/0000000342b7c7cb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Symbol Grounding Association in Multimodal Sequences with Missing Elements. (arXiv:1511.04401v5 [cs.CV] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1511.04401"}], "alternate": [{"href": "http://arxiv.org/abs/1511.04401", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we extend a symbolic association framework for being able to \nhandle missing elements in multimodal sequences. The general scope of the work \nis the symbolic associations of object-word mappings as it happens in language \ndevelopment in infants. In other words, two different representations of the \nsame abstract concepts can associate in both directions. This scenario has been \nlong interested in Artificial Intelligence, Psychology, and Neuroscience. In \nthis work, we extend a recent approach for multimodal sequences (visual and \naudio) to also cope with missing elements in one or both modalities. Our method \nuses two parallel Long Short-Term Memories (LSTMs) with a learning rule based \non EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We \npropose to include an extra step for the combination with the max operation for \nexploiting the common elements between both sequences. The motivation behind is \nthat the combination acts as a condition selector for choosing the best \nrepresentation from both LSTMs. We evaluated the proposed extension in the \nfollowing scenarios: missing elements in one modality (visual or audio) and \nmissing elements in both modalities (visual and sound). The performance of our \nextension reaches better results than the original model and similar results to \nindividual LSTM trained in each modality. \n</p>"}, "author": "Federico Raue, Andreas Dengel, Thomas M. Breuel, Marcus Liwicki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983974", "id": "tag:google.com,2005:reader/item/0000000342b7c7d1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "New Ideas for Brain Modelling 3. (arXiv:1612.00369v6 [cs.NE] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1612.00369"}], "alternate": [{"href": "http://arxiv.org/abs/1612.00369", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper considers a process for the creation and subsequent firing of \nsequences of neuronal patterns, as might be found in the human brain. The scale \nis one of larger patterns emerging from an ensemble mass, possibly through some \ntype of energy equation and a reduction procedure. The links between the \npatterns can be formed naturally, as a residual effect of the pattern creation \nitself. This paper follows-on closely from the earlier research, including two \nearlier papers in the series and uses the ideas of entropy and cohesion. With a \nsmall addition, it is possible to show how the inter-pattern links can be \ndetermined. A new compact Grid form of an earlier Counting Mechanism is also \ndemonstrated. It is possible to explain how a very basic repeating structure \ncan form the arbitrary patterns and activation sequences between them, and a \nkey question of how nodes synchronise may even be answerable. The paper \nfinishes with an implementation architecture, for the realisation and storage \nof knowledge and memory, as part of a general design, based on distributed \nneural components. \n</p>"}, "author": "Kieran Greer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983973", "id": "tag:google.com,2005:reader/item/0000000342b7c7d6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Born to Learn: the Inspiration, Progress, and Future of Evolved Plastic Artificial Neural Networks. (arXiv:1703.10371v2 [cs.NE] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.10371"}], "alternate": [{"href": "http://arxiv.org/abs/1703.10371", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Biological plastic neural networks are systems of extraordinary computational \ncapabilities shaped by evolution, development, and lifetime learning. The \ninterplay of these elements leads to the emergence of adaptive behavior and \nintelligence. Inspired by such intricate natural phenomena, Evolved Plastic \nArtificial Neural Networks (EPANNs) use simulated evolution in-silico to breed \nplastic neural networks with a large variety of dynamics, architectures, and \nplasticity rules: these artificial systems are composed of inputs, outputs, and \nplastic components that change in response to experiences in an environment. \nThese systems may autonomously discover novel adaptive algorithms, and lead to \nhypotheses on the emergence of biological adaptation. EPANNs have seen \nconsiderable progress over the last two decades. Current scientific and \ntechnological advances in artificial neural networks are now setting the \nconditions for radically new approaches and results. In particular, the \nlimitations of hand-designed networks could be overcome by more flexible and \ninnovative solutions. This paper brings together a variety of inspiring ideas \nthat define the field of EPANNs. The main methods and results are reviewed. \nFinally, new opportunities and developments are presented. \n</p>"}, "author": "Andrea Soltoggio, Kenneth O. Stanley, Sebastian Risi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983972", "id": "tag:google.com,2005:reader/item/0000000342b7c7db", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Forecasting day-ahead electricity prices in Europe: the importance of considering market integration. (arXiv:1708.07061v3 [q-fin.ST] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.07061"}], "alternate": [{"href": "http://arxiv.org/abs/1708.07061", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325051beabd\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325051beabd&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Motivated by the increasing integration among electricity markets, in this \npaper we propose two different methods to incorporate market integration in \nelectricity price forecasting and to improve the predictive performance. First, \nwe propose a deep neural network that considers features from connected markets \nto improve the predictive accuracy in a local market. To measure the importance \nof these features, we propose a novel feature selection algorithm that, by \nusing Bayesian optimization and functional analysis of variance, evaluates the \neffect of the features on the algorithm performance. In addition, using market \nintegration, we propose a second model that, by simultaneously predicting \nprices from two markets, improves the forecasting accuracy even further. As a \ncase study, we consider the electricity market in Belgium and the improvements \nin forecasting accuracy when using various French electricity features. We show \nthat the two proposed models lead to improvements that are statistically \nsignificant. Particularly, due to market integration, the predictive accuracy \nis improved from 15.7% to 12.5% sMAPE (symmetric mean absolute percentage \nerror). In addition, we show that the proposed feature selection algorithm is \nable to perform a correct assessment, i.e. to discard the irrelevant features. \n</p>"}, "author": "Jesus Lago, Fjo De Ridder, Peter Vrancx, Bart De Schutter", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983971", "id": "tag:google.com,2005:reader/item/0000000342b7c7e1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep learning with convolutional neural networks for decoding and visualization of EEG pathology. (arXiv:1708.08012v2 [cs.LG] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.08012"}], "alternate": [{"href": "http://arxiv.org/abs/1708.08012", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We apply convolutional neural networks (ConvNets) to the task of \ndistinguishing pathological from normal EEG recordings in the Temple University \nHospital EEG Abnormal Corpus. We use two basic, shallow and deep ConvNet \narchitectures recently shown to decode task-related information from EEG at \nleast as well as established algorithms designed for this purpose. In decoding \nEEG pathology, both ConvNets reached substantially better accuracies (about 6% \nbetter, ~85% vs. ~79%) than the only published result for this dataset, and \nwere still better when using only 1 minute of each recording for training and \nonly six seconds of each recording for testing. We used automated methods to \noptimize architectural hyperparameters and found intriguingly different ConvNet \narchitectures, e.g., with max pooling as the only nonlinearity. Visualizations \nof the ConvNet decoding behavior showed that they used spectral power changes \nin the delta (0-4 Hz) and theta (4-8 Hz) frequency range, possibly alongside \nother features, consistent with expectations derived from spectral analysis of \nthe EEG data and from the textual medical reports. Analysis of the textual \nmedical reports also highlighted the potential for accuracy increases by \nintegrating contextual information, such as the age of subjects. In summary, \nthe ConvNets and visualization techniques used in this study constitute a next \nstep towards clinically useful automated EEG diagnosis and establish a new \nbaseline for future work on this topic. \n</p>"}, "author": "Robin Tibor Schirrmeister, Lukas Gemein, Katharina Eggensperger, Frank Hutter, Tonio Ball", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983970", "id": "tag:google.com,2005:reader/item/0000000342b7c7e5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Geometric Semantic Genetic Programming Algorithm and Slump Prediction. (arXiv:1709.06114v2 [cs.NE] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.06114"}], "alternate": [{"href": "http://arxiv.org/abs/1709.06114", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Research on the performance of recycled concrete as building material in the \ncurrent world is an important subject. Given the complex composition of \nrecycled concrete, conventional methods for forecasting slump scarcely obtain \nsatisfactory results. Based on theory of nonlinear prediction method, we \npropose a recycled concrete slump prediction model based on geometric semantic \ngenetic programming (GSGP) and combined it with recycled concrete features. \nTests show that the model can accurately predict the recycled concrete slump by \nusing the established prediction model to calculate the recycled concrete slump \nwith different mixing ratios in practical projects and by comparing the \npredicted values with the experimental values. By comparing the model with \nseveral other nonlinear prediction models, we can conclude that GSGP has higher \naccuracy and reliability than conventional methods. \n</p>"}, "author": "Juncai Xu, Zhenzhong Shen, Qingwen Ren, Xin Xie, Zhengyu Yang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983969", "id": "tag:google.com,2005:reader/item/0000000342b7c7eb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Avoiding Your Teacher's Mistakes: Training Neural Networks with Controlled Weak Supervision. (arXiv:1711.00313v2 [cs.LG] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00313"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00313", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Training deep neural networks requires massive amounts of training data, but \nfor many tasks only limited labeled data is available. This makes weak \nsupervision attractive, using weak or noisy signals like the output of \nheuristic methods or user click-through data for training. In a semi-supervised \nsetting, we can use a large set of data with weak labels to pretrain a neural \nnetwork and then fine-tune the parameters with a small amount of data with true \nlabels. This feels intuitively sub-optimal as these two independent stages \nleave the model unaware about the varying label quality. What if we could \nsomehow inform the model about the label quality? In this paper, we propose a \nsemi-supervised learning method where we train two neural networks in a \nmulti-task fashion: a \"target network\" and a \"confidence network\". The target \nnetwork is optimized to perform a given task and is trained using a large set \nof unlabeled data that are weakly annotated. We propose to weight the gradient \nupdates to the target network using the scores provided by the second \nconfidence network, which is trained on a small amount of supervised data. Thus \nwe avoid that the weight updates computed from noisy labels harm the quality of \nthe target network model. We evaluate our learning strategy on two different \ntasks: document ranking and sentiment classification. The results demonstrate \nthat our approach not only enhances the performance compared to the baselines \nbut also speeds up the learning process from weak labels. \n</p>"}, "author": "Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, Jaap Kamps", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983967", "id": "tag:google.com,2005:reader/item/0000000342b7c7f2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Adversarial Examples that Fool Detectors. (arXiv:1712.02494v1 [cs.CV])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02494"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02494", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>An adversarial example is an example that has been adjusted to produce a \nwrong label when presented to a system at test time. To date, adversarial \nexample constructions have been demonstrated for classifiers, but not for \ndetectors. If adversarial examples that could fool a detector exist, they could \nbe used to (for example) maliciously create security hazards on roads populated \nwith smart vehicles. In this paper, we demonstrate a construction that \nsuccessfully fools two standard detectors, Faster RCNN and YOLO. The existence \nof such examples is surprising, as attacking a classifier is very different \nfrom attacking a detector, and that the structure of detectors - which must \nsearch for their own bounding box, and which cannot estimate that box very \naccurately - makes it quite likely that adversarial patterns are strongly \ndisrupted. We show that our construction produces adversarial examples that \ngeneralize well across sequences digitally, even though large perturbations are \nneeded. We also show that our construction yields physical objects that are \nadversarial. \n</p>"}, "author": "Jiajun Lu, Hussein Sibai, Evan Fabry", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983966", "id": "tag:google.com,2005:reader/item/0000000342b7c7fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "ChemNet: A Transferable and Generalizable Deep Neural Network for Small-Molecule Property Prediction. (arXiv:1712.02734v1 [stat.ML])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02734"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02734", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With access to large datasets, deep neural networks (DNN) have achieved \nhuman-level accuracy in image and speech recognition tasks. However, in \nchemistry, availability of large standardized and labelled datasets is scarce, \nand many chemical properties of research interest, chemical data is inherently \nsmall and fragmented. In this work, we explore transfer learning techniques in \nconjunction with the existing Chemception CNN model, to create a transferable \nand generalizable deep neural network for small-molecule property prediction. \nOur latest model, ChemNet learns in a semi-supervised manner from inexpensive \nlabels computed from the ChEMBL database. When fine-tuned to the Tox21, HIV and \nFreeSolv dataset, which are 3 separate chemical properties that ChemNet was not \noriginally trained on, we demonstrate that ChemNet exceeds the performance of \nexisting Chemception models and other contemporary DNN models. Furthermore, as \nChemNet has been pre-trained on a large diverse chemical database, it can be \nused as a general-purpose plug-and-play deep neural network for the prediction \nof novel small-molecule chemical properties. \n</p>"}, "author": "Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983965", "id": "tag:google.com,2005:reader/item/0000000342b7c802", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Measuring Relations Between Concepts In Conceptual Spaces. (arXiv:1707.02292v2 [cs.AI] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.02292"}], "alternate": [{"href": "http://arxiv.org/abs/1707.02292", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The highly influential framework of conceptual spaces provides a geometric \nway of representing knowledge. Instances are represented by points in a \nhigh-dimensional space and concepts are represented by regions in this space. \nOur recent mathematical formalization of this framework is capable of \nrepresenting correlations between different domains in a geometric way. In this \npaper, we extend our formalization by providing quantitative mathematical \ndefinitions for the notions of concept size, subsethood, implication, \nsimilarity, and betweenness. This considerably increases the representational \npower of our formalization by introducing measurable ways of describing \nrelations between concepts. \n</p>"}, "author": "Lucas Bechberger, Kai-Uwe K&#xfc;hnberger", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983964", "id": "tag:google.com,2005:reader/item/0000000342b7c807", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "IKBT: solving closed-form Inverse Kinematics with Behavior Tree. (arXiv:1711.05412v3 [cs.RO] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05412"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05412", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Serial robot arms have complicated kinematic equations which must be solved \nto write effective arm planning and control software (the Inverse Kinematics \nProblem). Existing software packages for inverse kinematics often rely on \nnumerical methods which have significant shortcomings. Here we report a new \nsymbolic inverse kinematics solver which overcomes the limitations of numerical \nmethods, and the shortcomings of previous symbolic software packages. We \nintegrate Behavior Trees, an execution planning framework previously used for \ncontrolling intelligent robot behavior, to organize the equation solving \nprocess, and a modular architecture for each solution technique. The system \nsuccessfully solved, generated a LaTex report, and generated a Python code \ntemplate for 18 out of 19 example robots of 4-6 DOF. The system is readily \nextensible, maintainable, and multi-platform with few dependencies. The \ncomplete package is available with a Modified BSD license on Github. \n</p>"}, "author": "Dianmu Zhang, Blake Hannaford", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983963", "id": "tag:google.com,2005:reader/item/0000000342b7c812", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Posterior Sampling for Large Scale Reinforcement Learning. (arXiv:1711.07979v2 [cs.LG] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07979"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07979", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Posterior sampling for reinforcement learning (PSRL) is a popular algorithm \nfor learning to control an unknown Markov decision process (MDP). PSRL \nmaintains a distribution over MDP parameters and in an episodic fashion samples \nMDP parameters, computes the optimal policy for them and executes it. A special \ncase of PSRL is where at the end of each episode the MDP resets to the initial \nstate distribution. Extensions of this idea to general MDPs without state \nresetting has so far produced non-practical algorithms and in some cases buggy \ntheoretical analysis. This is due to the difficulty of analyzing regret under \nepisode switching schedules that depend on random variables of the true \nunderlying model. We propose a solution to this problem that involves using a \ndeterministic, model-independent episode switching schedule, and establish a \nBayes regret bound under mild assumptions. Our algorithm termed deterministic \nschedule PSRL (DS-PSRL) is efficient in terms of time, sample, and space \ncomplexity. Our result is more generally applicable to continuous state action \nproblems. We demonstrate how this algorithm is well suited for sequential \nrecommendation problems such as points of interest (POI). We derive a general \nprocedure for parameterizing the underlying MDPs, to create action condition \ndynamics from passive data, that do not contain actions. We prove that such \nparameterization satisfies the assumptions of our analysis. \n</p>"}, "author": "Georgios Theocharous, Zheng Wen, Yasin Abbasi-Yadkori, Nikos Vlassis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983961", "id": "tag:google.com,2005:reader/item/0000000342b7c815", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Achieving the time of $1$-NN, but the accuracy of $k$-NN. (arXiv:1712.02369v1 [math.ST])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02369"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02369", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a simple approach which, given distributed computing resources, \ncan nearly achieve the accuracy of $k$-NN prediction, while matching (or \nimproving) the faster prediction time of $1$-NN. The approach consists of \naggregating denoised $1$-NN predictors over a small number of distributed \nsubsamples. We show, both theoretically and experimentally, that small \nsubsample sizes suffice to attain similar performance as $k$-NN, without \nsacrificing the computational efficiency of $1$-NN. \n</p>"}, "author": "Lirong Xue, Samory Kpotufe", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983960", "id": "tag:google.com,2005:reader/item/0000000342b7c81e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Noisy Natural Gradient as Variational Inference. (arXiv:1712.02390v1 [cs.LG])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02390"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02390", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325051becee\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325051becee&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Combining the flexibility of deep learning with Bayesian uncertainty \nestimation has long been a goal in our field, and many modern approaches are \nbased on variational Bayes. Unfortunately, one is forced to choose between \noverly simplistic variational families (e.g. fully factorized) or expensive and \ncomplicated inference procedures. We show that natural gradient ascent with \nadaptive weight noise can be interpreted as fitting a variational posterior to \nmaximize the evidence lower bound (ELBO). This insight allows us to train full \ncovariance, fully factorized, and matrix variate Gaussian variational \nposteriors using noisy versions of natural gradient, Adam, and K-FAC, \nrespectively. On standard regression benchmarks, our noisy K-FAC algorithm \nmakes better predictions and matches HMC's predictive variances better than \nexisting methods. Its improved uncertainty estimates lead to more efficient \nexploration in the settings of active learning and intrinsic motivation for \nreinforcement learning. \n</p>"}, "author": "Guodong Zhang, Shengyang Sun, David Duvenaud, Roger Grosse", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983959", "id": "tag:google.com,2005:reader/item/0000000342b7c824", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Estimating the error variance in a high-dimensional linear model. (arXiv:1712.02412v1 [stat.ME])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02412"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02412", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325052437b8\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325052437b8&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The lasso has been studied extensively as a tool for estimating the \ncoefficient vector in the high-dimensional linear model; however, considerably \nless is known about estimating the error variance. Indeed, most well-known \ntheoretical properties of the lasso, including recent advances in selective \ninference with the lasso, are established under the assumption that the \nunderlying error variance is known. Yet the error variance in practice is, of \ncourse, unknown. In this paper, we propose the natural lasso estimator for the \nerror variance, which maximizes a penalized likelihood objective. A key aspect \nof the natural lasso is that the likelihood is expressed in terms of the \nnatural parameterization of the multiparameter exponential family of a Gaussian \nwith unknown mean and variance. The result is a remarkably simple estimator \nwith provably good performance in terms of mean squared error. These \ntheoretical results do not require placing any assumptions on the design matrix \nor the true regression coefficients. We also propose a companion estimator, \ncalled the organic lasso, which theoretically does not require tuning of the \nregularization parameter. Both estimators do well compared to preexisting \nmethods, especially in settings where successful recovery of the true support \nof the coefficient vector is hard. \n</p>"}, "author": "Guo Yu, Jacob Bien", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983958", "id": "tag:google.com,2005:reader/item/0000000342b7c829", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sparse learning of stochastic dynamic equations. (arXiv:1712.02432v1 [stat.ML])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02432"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02432", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With the rapid increase of available data for complex systems, there is great \ninterest in the extraction of physically relevant information from massive \ndatasets. Recently, a framework called Sparse Identification of Nonlinear \nDynamics (SINDy) has been introduced to identify the governing equations of \ndynamical systems from simulation data. In this study, we extend SINDy to \nstochastic dynamical systems, which are frequently used to model biophysical \nprocesses. We prove the asymptotic correctness of stochastics SINDy in the \ninfinite data limit, both in the original and projected variables. We discuss \nalgorithms to solve the sparse regression problem arising from the practical \nimplementation of SINDy, and show that cross validation is an essential tool to \ndetermine the right level of sparsity. We demonstrate the proposed methodology \non two test systems, namely, the diffusion in a one-dimensional potential, and \nthe projected dynamics of a two-dimensional diffusion process. \n</p>"}, "author": "Lorenzo Boninsegna, Feliks N&#xfc;ske, Cecilia Clementi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983957", "id": "tag:google.com,2005:reader/item/0000000342b7c82e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Novel Model for Arbitration between Planning and Habitual Control Systems. (arXiv:1712.02441v1 [cs.SY])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02441"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02441", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is well established that humans decision making and instrumental control \nuses multiple systems, some which use habitual action selection and some which \nrequire deliberate planning. Deliberate planning systems use predictions of \naction-outcomes using an internal model of the agent's environment, while \nhabitual action selection systems learn to automate by repeating previously \nrewarded actions. Habitual control is computationally efficient but may be \ninflexible in changing environments. Conversely, deliberate planning may be \ncomputationally expensive, but flexible in dynamic environments. This paper \nproposes a general architecture comprising both control paradigms by \nintroducing an arbitrator that controls which subsystem is used at any time. \nThis system is implemented for a target-reaching task with a simulated \ntwo-joint robotic arm that comprises a supervised internal model and deep \nreinforcement learning. Through permutation of target-reaching conditions, we \ndemonstrate that the proposed is capable of rapidly learning kinematics of the \nsystem without a priori knowledge, and is robust to (A) changing environmental \nreward and kinematics, and (B) occluded vision. The arbitrator model is \ncompared to exclusive deliberate planning with the internal model and exclusive \nhabitual control instances of the model. The results show how such a model can \nharness the benefits of both systems, using fast decisions in reliable \ncircumstances while optimizing performance in changing environments. In \naddition, the proposed model learns very fast. Finally, the system which \nincludes internal models is able to reach the target under the visual \nocclusion, while the pure habitual system is unable to operate sufficiently \nunder such conditions. \n</p>"}, "author": "Farzaneh S. Fard, Thomas P. Trappenberg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983956", "id": "tag:google.com,2005:reader/item/0000000342b7c834", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Cost-sensitive detection with variational autoencoders for environmental acoustic sensing. (arXiv:1712.02488v1 [stat.ML])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02488"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02488", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Environmental acoustic sensing involves the retrieval and processing of audio \nsignals to better understand our surroundings. While large-scale acoustic data \nmake manual analysis infeasible, they provide a suitable playground for machine \nlearning approaches. Most existing machine learning techniques developed for \nenvironmental acoustic sensing do not provide flexible control of the trade-off \nbetween the false positive rate and the false negative rate. This paper \npresents a cost-sensitive classification paradigm, in which the \nhyper-parameters of classifiers and the structure of variational autoencoders \nare selected in a principled Neyman-Pearson framework. We examine the \nperformance of the proposed approach using a dataset from the HumBug project \nwhich aims to detect the presence of mosquitoes using sound collected by simple \nembedded devices. \n</p>"}, "author": "Yunpeng Li, Ivan Kiskin, Davide Zilli, Marianne Sinka, Henry Chan, Kathy Willis, Stephen Roberts", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983955", "id": "tag:google.com,2005:reader/item/0000000342b7c83c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Gini-regularized Optimal Transport with an Application to Spatio-Temporal Forecasting. (arXiv:1712.02512v1 [stat.ML])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02512"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02512", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Rapidly growing product lines and services require a finer-granularity \nforecast that considers geographic locales. However the open question remains, \nhow to assess the quality of a spatio-temporal forecast? In this manuscript we \nintroduce a metric to evaluate spatio-temporal forecasts. This metric is based \non an Opti- mal Transport (OT) problem. The metric we propose is a constrained \nOT objec- tive function using the Gini impurity function as a regularizer. We \ndemonstrate through computer experiments both the qualitative and the \nquantitative charac- teristics of the Gini regularized OT problem. Moreover, we \nshow that the Gini regularized OT problem converges to the classical OT \nproblem, when the Gini regularized problem is considered as a function of \n{\\lambda}, the regularization parame-ter. The convergence to the classical OT \nsolution is faster than the state-of-the-art Entropic-regularized OT[Cuturi, \n2013] and results in a numerically more stable algorithm. \n</p>"}, "author": "Lucas Roberts, Leo Razoumov, Lin Su, Yuyang Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983954", "id": "tag:google.com,2005:reader/item/0000000342b7c845", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convergence Rates of Variational Posterior Distributions. (arXiv:1712.02519v1 [math.ST])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02519"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02519", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study convergence rates of variational posterior distributions for \nnonparametric and high-dimensional inference. We formulate general conditions \non prior, likelihood, and variational class that characterize the convergence \nrates. Under similar \"prior mass and testing\" conditions considered in the \nliterature, the rate is found to be the sum of two terms. The first term stands \nfor the convergence rate of the true posterior distribution, and the second \nterm is contributed by the variational approximation error. For a class of \npriors that admit the structure of a mixture of product measures, we propose a \nnovel prior mass condition, under which the variational approximation error of \nthe generalized mean-field class is dominated by convergence rate of the true \nposterior. We demonstrate the applicability of our general results for various \nmodels, prior distributions and variational classes by deriving convergence \nrates of the corresponding variational posteriors. \n</p>"}, "author": "Fengshuo Zhang, Chao Gao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983953", "id": "tag:google.com,2005:reader/item/0000000342b7c84b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Random Fourier Features by Hybrid Constrained Optimization. (arXiv:1712.02527v1 [stat.ML])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02527"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02527", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The kernel embedding algorithm is an important component for adapting kernel \nmethods to large datasets. Since the algorithm consumes a major computation \ncost in the testing phase, we propose a novel teacher-learner framework of \nlearning computation-efficient kernel embeddings from specific data. In the \nframework, the high-precision embeddings (teacher) transfer the data \ninformation to the computation-efficient kernel embeddings (learner). We \njointly select informative embedding functions and pursue an orthogonal \ntransformation between two embeddings. We propose a novel approach of \nconstrained variational expectation maximization (CVEM), where the alternate \ndirection method of multiplier (ADMM) is applied over a nonconvex domain in the \nmaximization step. We also propose two specific formulations based on the \nprevalent Random Fourier Feature (RFF), the masked and blocked version of \nComputation-Efficient RFF (CERF), by imposing a random binary mask or a block \nstructure on the transformation matrix. By empirical studies of several \napplications on different real-world datasets, we demonstrate that the CERF \nsignificantly improves the performance of kernel methods upon the RFF, under \ncertain arithmetic operation requirements, and suitable for structured matrix \nmultiplication in Fastfood type algorithms. \n</p>"}, "author": "Jianqiao Wangni, Jingwei Zhuo, Jun Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983952", "id": "tag:google.com,2005:reader/item/0000000342b7c853", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Solving internal covariate shift in deep learning with linked neurons. (arXiv:1712.02609v1 [stat.ML])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02609"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02609", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This work proposes a novel solution to the problem of internal covariate \nshift and dying neurons using the concept of linked neurons. We define the \nneuron linkage in terms of two constraints: first, all neuron activations in \nthe linkage must have the same operating point. That is to say, all of them \nshare input weights. Secondly, a set of neurons is linked if and only if there \nis at least one member of the linkage that has a non-zero gradient in regard to \nthe input of the activation function. This means that for any input in the \nactivation function, there is at least one member of the linkage that operates \nin a non-flat and non-zero area. This simple change has profound implications \nin the network learning dynamics. In this article we explore the consequences \nof this proposal and show that by using this kind of units, internal covariate \nshift is implicitly solved. As a result of this, the use of linked neurons \nallows to train arbitrarily large networks without any architectural or \nalgorithmic trick, effectively removing the need of using re-normalization \nschemes such as Batch Normalization, which leads to halving the required \ntraining time. It also solves the problem of the need for standarized input \ndata. Results show that the units using the linkage not only do effectively \nsolve the aforementioned problems, but are also a competitive alternative with \nrespect to state-of-the-art with very promising results. \n</p>"}, "author": "Carles Roger Riera Molina, Oriol Pujol Vila", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983951", "id": "tag:google.com,2005:reader/item/0000000342b7c85a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Differentially Private Variational Dropout. (arXiv:1712.02629v1 [stat.ML])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02629"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02629", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks with their large number of parameters are highly \nflexible learning systems. The high flexibility in such networks brings with \nsome serious problems such as overfitting, and regularization is used to \naddress this problem. A currently popular and effective regularization \ntechnique for controlling the overfitting is dropout. Often, large data \ncollections required for neural networks contain sensitive information such as \nthe medical histories of patients, and the privacy of the training data should \nbe protected. In this paper, we modify the recently proposed variational \ndropout technique which provided an elegant Bayesian interpretation to dropout, \nand show that the intrinsic noise in the variational dropout can be exploited \nto obtain a degree of differential privacy. The iterative nature of training \nneural networks presents a challenge for privacy-preserving estimation since \nmultiple iterations increase the amount of noise added. We overcome this by \nusing a relaxed notion of differential privacy, called concentrated \ndifferential privacy, which provides tighter estimates on the overall privacy \nloss. We demonstrate the accuracy of our privacy-preserving variational dropout \nalgorithm on benchmark datasets. \n</p>"}, "author": "Beyza Ermis, Ali Taylan Cemgil", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983950", "id": "tag:google.com,2005:reader/item/0000000342b7c85f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "High-dimensional robust regression and outliers detection with SLOPE. (arXiv:1712.02640v1 [stat.ML])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02640"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02640", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325052439e3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325052439e3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The problems of outliers detection and robust regression in a \nhigh-dimensional setting are fundamental in statistics, and have numerous \napplications. Following a recent set of works providing methods for \nsimultaneous robust regression and outliers detection, we consider in this \npaper a model of linear regression with individual intercepts, in a \nhigh-dimensional setting. We introduce a new procedure for simultaneous \nestimation of the linear regression coefficients and intercepts, using two \ndedicated sorted-$\\ell_1$ penalizations, also called SLOPE. We develop a \ncomplete theory for this problem: first, we provide sharp upper bounds on the \nstatistical estimation error of both the vector of individual intercepts and \nregression coefficients. Second, we give an asymptotic control on the False \nDiscovery Rate (FDR) and statistical power for support selection of the \nindividual intercepts. As a consequence, this paper is the first to introduce a \nprocedure with guaranteed FDR and statistical power control for outliers \ndetection under the mean-shift model. Numerical illustrations, with a \ncomparison to recent alternative approaches, are provided on both simulated and \nseveral real-world datasets. Experiments are conducted using an open-source \nsoftware written in Python and C++. \n</p>"}, "author": "Alain Virouleau, Agathe Guilloux, St&#xe9;phane Ga&#xef;ffas, Malgorzata Bogdan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983949", "id": "tag:google.com,2005:reader/item/0000000342b7c864", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Using SVDD in SimpleMKL for 3D-Shapes Filtering. (arXiv:1712.02658v1 [stat.ML])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02658"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02658", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes the adaptation of Support Vector Data Description (SVDD) \nto the multiple kernel case (MK-SVDD), based on SimpleMKL. It also introduces a \nvariant called Slim-MK-SVDD that is able to produce a tighter frontier around \nthe data. For the sake of comparison, the equivalent methods are also developed \nfor One-Class SVM, known to be very similar to SVDD for certain shapes of \nkernels. \n</p> \n<p>Those algorithms are illustrated in the context of 3D-shapes filtering and \noutliers detection. For the 3D-shapes problem, the objective is to be able to \nselect a sub-category of 3D-shapes, each sub-category being learned with our \nalgorithm in order to create a filter. For outliers detection, we apply the \nproposed algorithms for unsupervised outliers detection as well as for the \nsupervised case. \n</p>"}, "author": "Ga&#xeb;lle Loosli, Hattoibe Aboubacar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983948", "id": "tag:google.com,2005:reader/item/0000000342b7c86d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Is My Model Flexible Enough? Information-Theoretic Model Check. (arXiv:1712.02675v1 [stat.ML])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02675"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02675", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The choice of model class is fundamental in statistical learning and system \nidentification, no matter whether the class is derived from physical principles \nor is a generic black-box. We develop a method to evaluate the specified model \nclass by assessing its capability of reproducing data that is similar to the \nobserved data record. This model check is based on the information-theoretic \nproperties of models viewed as data generators and is applicable to e.g. \nsequential data and nonlinear dynamical models. The method can be understood as \na specific two-sided posterior predictive test. We apply the \ninformation-theoretic model check to both synthetic and real data and compare \nit with a classical whiteness test. \n</p>"}, "author": "Andreas Svensson, Dave Zachariah, Thomas B. Sch&#xf6;n", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983947", "id": "tag:google.com,2005:reader/item/0000000342b7c873", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "AdaComp : Adaptive Residual Gradient Compression for Data-Parallel Distributed Training. (arXiv:1712.02679v1 [cs.LG])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02679"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02679", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Highly distributed training of Deep Neural Networks (DNNs) on future compute \nplatforms (offering 100 of TeraOps/s of computational capacity) is expected to \nbe severely communication constrained. To overcome this limitation, new \ngradient compression techniques are needed that are computationally friendly, \napplicable to a wide variety of layers seen in Deep Neural Networks and \nadaptable to variations in network architectures as well as their \nhyper-parameters. In this paper we introduce a novel technique - the Adaptive \nResidual Gradient Compression (AdaComp) scheme. AdaComp is based on localized \nselection of gradient residues and automatically tunes the compression rate \ndepending on local activity. We show excellent results on a wide spectrum of \nstate of the art Deep Learning models in multiple domains (vision, speech, \nlanguage), datasets (MNIST, CIFAR10, ImageNet, BN50, Shakespeare), optimizers \n(SGD with momentum, Adam) and network parameters (number of learners, \nminibatch-size etc.). Exploiting both sparsity and quantization, we demonstrate \nend-to-end compression rates of ~200X for fully-connected and recurrent layers, \nand ~40X for convolutional layers, without any noticeable degradation in model \naccuracies. \n</p>"}, "author": "Chia-Yu Chen, Jungwook Choi, Daniel Brand, Ankur Agrawal, Wei Zhang, Kailash Gopalakrishnan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983946", "id": "tag:google.com,2005:reader/item/0000000342b7c879", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "End-to-end Learning of Deterministic Decision Trees. (arXiv:1712.02743v1 [stat.ML])", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02743"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02743", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Conventional decision trees have a number of favorable properties, including \ninterpretability, a small computational footprint and the ability to learn from \nlittle training data. However, they lack a key quality that has helped fuel the \ndeep learning revolution: that of being end-to-end trainable, and to learn from \nscratch those features that best allow to solve a given supervised learning \nproblem. Recent work (Kontschieder 2015) has addressed this deficit, but at the \ncost of losing a main attractive trait of decision trees: the fact that each \nsample is routed along a small subset of tree nodes only. We here propose a \nmodel and Expectation-Maximization training scheme for decision trees that are \nfully probabilistic at train time, but after a deterministic annealing process \nbecome deterministic at test time. We also analyze the learned oblique split \nparameters on image datasets and show that Neural Networks can be trained at \neach split node. In summary, we present the first end-to-end learning scheme \nfor deterministic decision trees and present results on par with or superior to \npublished standard oblique decision tree algorithms. \n</p>"}, "author": "Thomas Hehn, Fred A. Hamprecht", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983945", "id": "tag:google.com,2005:reader/item/0000000342b7c881", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "BDgraph: An R Package for Bayesian Structure Learning in Graphical Models. (arXiv:1501.05108v5 [stat.ML] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1501.05108"}], "alternate": [{"href": "http://arxiv.org/abs/1501.05108", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Graphical models provide powerful tools to uncover complicated patterns in \nmultivariate data and are commonly used in Bayesian statistics and machine \nlearning. In this paper, we introduce an R package BDgraph which performs \nBayesian structure learning for general undirected graphical models with either \ncontinuous or discrete variables. The package efficiently implements recent \nimprovements in the Bayesian literature. To speed up computations, the \ncomputationally intensive tasks have been implemented in C++ and interfaced \nwith R. In addition, the package contains several functions for simulation and \nvisualization, as well as two multivariate datasets taken from the literature \nand are used to describe the package capabilities. The paper includes a brief \noverview of the statistical methods which have been implemented in the package. \nThe main body of the paper explains how to use the package. Furthermore, we \nillustrate the package's functionality in both real and artificial examples, as \nwell as in an extensive simulation study. \n</p>"}, "author": "Abdolreza Mohammadi, Ernst C. Wit", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983944", "id": "tag:google.com,2005:reader/item/0000000342b7c887", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast Rates for General Unbounded Loss Functions: from ERM to Generalized Bayes. (arXiv:1605.00252v2 [cs.LG] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1605.00252"}], "alternate": [{"href": "http://arxiv.org/abs/1605.00252", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present new excess risk bounds for general unbounded loss functions \nincluding log loss and squared loss, where the distribution of the losses may \nbe heavy-tailed. The bounds hold for general estimators, but they are optimized \nwhen applied to $\\eta$-generalized Bayesian, MDL, and ERM estimators. When \napplied with log loss, the bounds imply convergence rates for generalized \nBayesian inference under misspecification in terms of a generalization of the \nHellinger metric as long as the learning rate $\\eta$ is set correctly. For \ngeneral loss functions, our bounds rely on two separate conditions: the \n$v$-GRIP (generalized reversed information projection) conditions, which \ncontrol the lower tail of the excess loss; and the newly introduced witness \ncondition, which controls the upper tail. The parameter $v$ in the $v$-GRIP \nconditions determines the achievable rate and is akin to the exponent in the \nwell-known Tsybakov margin condition and the Bernstein condition for bounded \nlosses, which the $v$-GRIP conditions generalize; favorable $v$ in combination \nwith small model complexity leads to $\\tilde{O}(1/n)$ rates. The witness \ncondition allows us to connect the excess risk to an 'annealed' version \nthereof, by which we generalize several previous results connecting Hellinger \nand R\\'enyi divergence to KL divergence. \n</p>"}, "author": "Peter D. Gr&#xfc;nwald, Nishant A. Mehta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983943", "id": "tag:google.com,2005:reader/item/0000000342b7c88b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Complex-valued Gaussian Process Regression for Time Series Analysis. (arXiv:1611.10073v2 [stat.ML] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1611.10073"}], "alternate": [{"href": "http://arxiv.org/abs/1611.10073", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The construction of synthetic complex-valued signals from real-valued \nobservations is an important step in many time series analysis techniques. The \nmost widely used approach is based on the Hilbert transform, which maps the \nreal-valued signal into its quadrature component. In this paper, we define a \nprobabilistic generalization of this approach. We model the observable \nreal-valued signal as the real part of a latent complex-valued Gaussian \nprocess. In order to obtain the appropriate statistical relationship between \nits real and imaginary parts, we define two new classes of complex-valued \ncovariance functions. Through an analysis of simulated chirplets and stochastic \noscillations, we show that the resulting Gaussian process complex-valued signal \nprovides a better estimate of the instantaneous amplitude and frequency than \nthe established approaches. Furthermore, the complex-valued Gaussian process \nregression allows to incorporate prior information about the structure in \nsignal and noise and thereby to tailor the analysis to the features of the \nsignal. As a example, we analyze the non-stationary dynamics of brain \noscillations in the alpha band, as measured using magneto-encephalography. \n</p>"}, "author": "Luca Ambrogioni, Eric Maris", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983942", "id": "tag:google.com,2005:reader/item/0000000342b7c896", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Properties and Bayesian fitting of restricted Boltzmann machines. (arXiv:1612.01158v2 [stat.ML] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1612.01158"}], "alternate": [{"href": "http://arxiv.org/abs/1612.01158", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A restricted Boltzmann machine (RBM) is an undirected graphical model \nconstructed for discrete or continuous random variables, with two layers, one \nhidden and one visible, and no conditional dependency within a layer. In recent \nyears, RBMs have risen to prominence due to their connection to deep learning. \nBy treating a hidden layer of one RBM as the visible layer in a second RBM, a \ndeep architecture can be created. RBMs are thought to thereby have the ability \nto encode very complex and rich structures in data, making them attractive for \nsupervised learning. However, the generative behavior of RBMs is largely \nunexplored. In this paper, we discuss the relationship between RBM parameter \nspecification in the binary case and model properties such as degeneracy, \ninstability and uninterpretability. We also describe the difficulties that \narise in likelihood-based and Bayes fitting of such (highly flexible) models, \nespecially as Gibbs sampling (quasi-Bayes) methods are often advocated for the \nRBM model structure. \n</p>"}, "author": "Andee Kaplan, Daniel Nordman, Stephen Vardeman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983941", "id": "tag:google.com,2005:reader/item/0000000342b7c8a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Wasserstein GAN. (arXiv:1701.07875v3 [stat.ML] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1701.07875"}], "alternate": [{"href": "http://arxiv.org/abs/1701.07875", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce a new algorithm named WGAN, an alternative to traditional GAN \ntraining. In this new model, we show that we can improve the stability of \nlearning, get rid of problems like mode collapse, and provide meaningful \nlearning curves useful for debugging and hyperparameter searches. Furthermore, \nwe show that the corresponding optimization problem is sound, and provide \nextensive theoretical work highlighting the deep connections to other distances \nbetween distributions. \n</p>"}, "author": "Martin Arjovsky, Soumith Chintala, L&#xe9;on Bottou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983940", "id": "tag:google.com,2005:reader/item/0000000342b7c8b8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Integral Transforms from Finite Data: An Application of Gaussian Process Regression to Fourier Analysis. (arXiv:1704.02828v2 [stat.ML] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1704.02828"}], "alternate": [{"href": "http://arxiv.org/abs/1704.02828", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32505243c04\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32505243c04&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Computing accurate estimates of the Fourier transform of analog signals from \ndiscrete data points is important in many fields of science and engineering. \nThe conventional approach of performing the discrete Fourier transform of the \ndata implicitly assumes periodicity and bandlimitedness of the signal. In this \npaper, we use Gaussian process regression to estimate the Fourier transform (or \nany other integral transform) without making these assumptions. This is \npossible because the posterior expectation of Gaussian process regression maps \na finite set of samples to a function defined on the whole real line, expressed \nas a linear combination of covariance functions. We estimate the covariance \nfunction from the data using an appropriately designed gradient ascent method \nthat constrains the solution to a linear combination of tractable kernel \nfunctions. This procedure results in a posterior expectation of the analog \nsignal whose Fourier transform can be obtained analytically by exploiting \nlinearity. Our simulations show that the new method leads to sharper and more \nprecise estimation of the spectral density both in noise-free and \nnoise-corrupted signals. We further validate the method in two real-world \napplications: the analysis of the yearly fluctuation in atmospheric CO2 level \nand the analysis of the spectral content of brain signals. \n</p>"}, "author": "Luca Ambrogioni, Eric Maris", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983939", "id": "tag:google.com,2005:reader/item/0000000342b7c8ca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Limitations on Variance-Reduction and Acceleration Schemes for Finite Sum Optimization. (arXiv:1706.01686v2 [math.OC] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.01686"}], "alternate": [{"href": "http://arxiv.org/abs/1706.01686", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325052b9f42\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325052b9f42&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We study the conditions under which one is able to efficiently apply \nvariance-reduction and acceleration schemes on finite sum optimization \nproblems. First, we show that, perhaps surprisingly, the finite sum structure \nby itself, is not sufficient for obtaining a complexity bound of \n$\\tilde{\\cO}((n+L/\\mu)\\ln(1/\\epsilon))$ for $L$-smooth and $\\mu$-strongly \nconvex individual functions - one must also know which individual function is \nbeing referred to by the oracle at each iteration. Next, we show that for a \nbroad class of first-order and coordinate-descent finite sum algorithms \n(including, e.g., SDCA, SVRG, SAG), it is not possible to get an `accelerated' \ncomplexity bound of $\\tilde{\\cO}((n+\\sqrt{n L/\\mu})\\ln(1/\\epsilon))$, unless \nthe strong convexity parameter is given explicitly. Lastly, we show that when \nthis class of algorithms is used for minimizing $L$-smooth and convex finite \nsums, the optimal complexity bound is $\\tilde{\\cO}(n+L/\\epsilon)$, assuming \nthat (on average) the same update rule is used in every iteration, and \n$\\tilde{\\cO}(n+\\sqrt{nL/\\epsilon})$, otherwise. \n</p>"}, "author": "Yossi Arjevani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983938", "id": "tag:google.com,2005:reader/item/0000000342b7c8d9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Compressive Statistical Learning with Random Feature Moments. (arXiv:1706.07180v2 [stat.ML] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.07180"}], "alternate": [{"href": "http://arxiv.org/abs/1706.07180", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We describe a general framework --compressive statistical learning-- for \nresource-efficient large-scale learning: the training collection is compressed \nin one pass into a low-dimensional sketch (a vector of random empirical \ngeneralized moments) that captures the information relevant to the considered \nlearning task. A near-minimizer of the risk is computed from the sketch through \nthe solution of a nonlinear least squares problem. We investigate sufficient \nsketch sizes to control the generalization error of this procedure. The \nframework is illustrated on compressive clustering, compressive Gaussian \nmixture Modeling with fixed known variance, and compressive PCA. \n</p>"}, "author": "R&#xe9;mi Gribonval (PANAMA), Gilles Blanchard, Nicolas Keriven (PANAMA, UR1), Yann Traonmilin (PANAMA)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983937", "id": "tag:google.com,2005:reader/item/0000000342b7c8e4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Algorithmic learning of probability distributions from random data in the limit. (arXiv:1710.11303v2 [cs.LG] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11303"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11303", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of identifying a probability distribution for some given \nrandomly sampled data in the limit, in the context of algorithmic learning \ntheory as proposed recently by Vinanyi and Chater. We show that there exists a \ncomputable partial learner for the computable probability measures, while by \nBienvenu, Monin and Shen it is known that there is no computable learner for \nthe computable probability measures. Our main result is the characterization of \nthe oracles that compute explanatory learners for the computable (continuous) \nprobability measures as the high oracles. This provides an analogue of a \nwell-known result of Adleman and Blum in the context of learning computable \nprobability distributions. We also discuss related learning notions such as \nbehaviorally correct learning and orther variations of explanatory learning, in \nthe context of learning probability distributions from data. \n</p>"}, "author": "George Barmpalias, Frank Stephan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983936", "id": "tag:google.com,2005:reader/item/0000000342b7c8f3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improved Bayesian Compression. (arXiv:1711.06494v2 [stat.ML] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06494"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06494", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Compression of Neural Networks (NN) has become a highly studied topic in \nrecent years. The main reason for this is the demand for industrial scale usage \nof NNs such as deploying them on mobile devices, storing them efficiently, \ntransmitting them via band-limited channels and most importantly doing \ninference at scale. In this work, we propose to join the Soft-Weight Sharing \nand Variational Dropout approaches that show strong results to define a new \nstate-of-the-art in terms of model compression. \n</p>"}, "author": "Marco Federici, Karen Ullrich, Max Welling", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512709730984", "timestampUsec": "1512709730983935", "id": "tag:google.com,2005:reader/item/0000000342b7c8f7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "\"I know it when I see it\". Visualization and Intuitive Interpretability. (arXiv:1711.08042v2 [stat.ML] UPDATED)", "published": 1512709731, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08042"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08042", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Most research on the interpretability of machine learning systems focuses on \nthe development of a more rigorous notion of interpretability. I suggest that a \nbetter understanding of the deficiencies of the intuitive notion of \ninterpretability is needed as well. I show that visualization enables but also \nimpedes intuitive interpretability, as it presupposes two levels of technical \npre-interpretation: dimensionality reduction and regularization. Furthermore, I \nargue that the use of positive concepts to emulate the distributed semantic \nstructure of machine learning models introduces a significant human bias into \nthe model. As a consequence, I suggest that, if intuitive interpretability is \nneeded, singular representations of internal model states should be avoided. \n</p>"}, "author": "Fabian Offert", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381662", "id": "tag:google.com,2005:reader/item/0000000341fb7c37", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Single-trial P300 Classification using PCA with LDA, QDA and Neural Networks. (arXiv:1712.01977v1 [cs.NE])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01977"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01977", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The P300 event-related potential (ERP), evoked in scalp-recorded \nelectroencephalography (EEG) by external stimuli, has proven to be a reliable \nresponse for controlling a BCI. The P300 component of an event related \npotential is thus widely used in brain-computer interfaces to translate the \nsubjects' intent by mere thoughts into commands to control artificial devices. \nThe main challenge in the classification of P300 trials in \nelectroencephalographic (EEG) data is the low signal-to-noise ratio (SNR) of \nthe P300 response. To overcome the low SNR of individual trials, it is common \npractice to average together many consecutive trials, which effectively \ndiminishes the random noise. Unfortunately, when more repeated trials are \nrequired for applications such as the P300 speller, the communication rate is \ngreatly reduced. This has resulted in a need for better methods to improve \nsingle-trial classification accuracy of P300 response. In this work, we use \nPrincipal Component Analysis (PCA) as a preprocessing method and use Linear \nDiscriminant Analysis (LDA)and neural networks for classification. The results \nshow that a combination of PCA with these methods provided as high as 13\\% \naccuracy gain for single-trial classification while using only 3 to 4 principal \ncomponents. \n</p>"}, "author": "Nand Sharma", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381661", "id": "tag:google.com,2005:reader/item/0000000341fb7c43", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Scalable Deep Neural Network Architecture for Multi-Building and Multi-Floor Indoor Localization Based on Wi-Fi Fingerprinting. (arXiv:1712.01990v1 [cs.NI])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01990"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01990", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>One of the key technologies for future large-scale location-aware services \ncovering a complex of multi-story buildings --- e.g., a big shopping mall and a \nuniversity campus --- is a scalable indoor localization technique. In this \npaper, we report the current status of our investigation on the use of deep \nneural networks (DNNs) for scalable building/floor classification and \nfloor-level position estimation based on Wi-Fi fingerprinting. Exploiting the \nhierarchical nature of the building/floor estimation and floor-level \ncoordinates estimation of a location, we propose a new DNN architecture \nconsisting of a stacked autoencoder for the reduction of feature space \ndimension and a feed-forward classifier for multi-label classification of \nbuilding/floor/location, on which the multi-building and multi-floor indoor \nlocalization system based on Wi-Fi fingerprinting is built. Experimental \nresults for the performance of building/floor estimation and floor-level \ncoordinates estimation of a given location demonstrate the feasibility of the \nproposed DNN-based indoor localization system, which can provide near \nstate-of-the-art performance using a single DNN, for the implementation with \nlower complexity and energy consumption at mobile devices. \n</p>"}, "author": "Kyeong Soo Kim, Sanghyuk Lee, Kaizhu Huang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381660", "id": "tag:google.com,2005:reader/item/0000000341fb7c49", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Named Entity Sequence Classification. (arXiv:1712.02316v1 [cs.NE])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02316"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02316", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Named Entity Recognition (NER) aims at locating and classifying named \nentities in text. In some use cases of NER, including cases where detected \nnamed entities are used in creating content recommendations, it is crucial to \nhave a reliable confidence level for the detected named entities. In this work \nwe study the problem of finding confidence levels for detected named entities. \nWe refer to this problem as Named Entity Sequence Classification (NESC). We \nframe NESC as a binary classification problem and we use NER as well as \nrecurrent neural networks to find the probability of candidate named entity is \na real named entity. We apply this approach to Tweet texts and we show how we \ncould find named entities with high confidence levels from Tweets. \n</p>"}, "author": "Mahdi Namazifar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381659", "id": "tag:google.com,2005:reader/item/0000000341fb7c4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generative Adversarial Perturbations. (arXiv:1712.02328v1 [cs.CV])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02328"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02328", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose novel generative models for creating adversarial \nexamples, slightly perturbed images resembling natural images but maliciously \ncrafted to fool pre-trained models. We present trainable deep neural networks \nfor transforming images to adversarial perturbations. Our proposed models can \nproduce image-agnostic and image-dependent perturbations for both targeted and \nnon-targeted attacks. We also demonstrate that similar architectures can \nachieve impressive results in fooling classification and semantic segmentation \nmodels, obviating the need for hand-crafting attack methods for each task. \nUsing extensive experiments on challenging high-resolution datasets such as \nImageNet and Cityscapes, we show that our perturbations achieve high fooling \nrates with small perturbation norms. Moreover, our attacks are considerably \nfaster than current iterative methods at inference time. \n</p>"}, "author": "Omid Poursaeed, Isay Katsman, Bicheng Gao, Serge Belongie", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381658", "id": "tag:google.com,2005:reader/item/0000000341fb7c55", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Predicting Demographics, Moral Foundations, and Human Values from Digital Behaviors. (arXiv:1712.01930v1 [cs.CY])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01930"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01930", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325052ba22f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325052ba22f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Personal electronic devices such as smartphones give access to a broad range \nof behavioral signals that can be used to learn about the characteristics and \npreferences of individuals. In this study we explore the connection between \ndemographic and psychological attributes and digital records for a cohort of \n7,633 people, closely representative of the US population with respect to \ngender, age, geographical distribution, education, and income. We collected \nself-reported assessments on validated psychometric questionnaires based on \nboth the Moral Foundations and Basic Human Values theories, and combined this \ninformation with passively-collected multi-modal digital data from web browsing \nbehavior, smartphone usage and demographic data. Then, we designed a machine \nlearning framework to infer both the demographic and psychological attributes \nfrom the behavioral data. In a cross-validated setting, our model is found to \npredict demographic attributes with good accuracy (weighted AUC scores of 0.90 \nfor gender, 0.71 for age, 0.74 for ethnicity). Our weighted AUC scores for \nMoral Foundation attributes (0.66) and Human Values attributes (0.60) suggest \nthat accurate prediction of complex psychometric attributes is more challenging \nbut feasible. This connection might prove useful for designing personalized \nservices, communication strategies, and interventions, and can be used to \nsketch a portrait of people with similar worldviews. \n</p>"}, "author": "Kyriaki Kalimeri, Mariano G. Beiro, Matteo Delfino, Robert Raleigh, Ciro Cattuto", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381657", "id": "tag:google.com,2005:reader/item/0000000341fb7c5c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Recognizing Plans by Learning Embeddings from Observed Action Distributions. (arXiv:1712.01949v1 [cs.AI])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01949"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01949", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent advances in visual activity recognition have raised the possibility of \napplications such as automated video surveillance. Effective approaches for \nsuch problems however require the ability to recognize the plans of the agents \nfrom video information. Although traditional plan recognition algorithms depend \non access to sophisticated domain models, one recent promising direction \ninvolves learning shallow models directly from the observed activity sequences, \nand using them to recognize/predict plans. One limitation of such approaches is \nthat they expect observed action sequences as training data. In many cases \ninvolving vision or sensing from raw data, there is considerably uncertainty \nabout the specific action at any given time point. The most we can expect in \nsuch cases is probabilistic information about the action at that point. The \ntraining data will then be sequences of such observed action distributions. In \nthis paper, we focus on doing effective plan recognition with such uncertain \nobservations. Our contribution is a novel extension of word vector embedding \ntechniques to directly handle such observation distributions as input. This \ninvolves computing embeddings by minimizing the distance between distributions \n(measured as KL-divergence). We will show that our approach has superior \nperformance when the perception error rate (PER) is higher, and competitive \nperformance when the PER is lower. We will also explore the possibility of \nusing importance sampling techniques to handle observed action distributions \nwith traditional word vector embeddings. We will show that although such \napproaches can give good recognition accuracy, they take significantly longer \ntraining time and their performance will degrade significantly at higher \nperception error rate. \n</p>"}, "author": "Yantian Zha, Yikang Li, Sriram Gopalakrishnan, Baoxin Li, Subbarao Kambhampati", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381656", "id": "tag:google.com,2005:reader/item/0000000341fb7c65", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An analysis of incorporating an external language model into a sequence-to-sequence model. (arXiv:1712.01996v1 [eess.AS])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01996"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01996", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Attention-based sequence-to-sequence models for automatic speech recognition \njointly train an acoustic model, language model, and alignment mechanism. Thus, \nthe language model component is only trained on transcribed audio-text pairs. \nThis leads to the use of shallow fusion with an external language model at \ninference time. Shallow fusion refers to log-linear interpolation with a \nseparately trained language model at each step of the beam search. In this \nwork, we investigate the behavior of shallow fusion across a range of \nconditions: different types of language models, different decoding units, and \ndifferent tasks. On Google Voice Search, we demonstrate that the use of shallow \nfusion with a neural LM with wordpieces yields a 9.1% relative word error rate \nreduction (WERR) over our competitive attention-based sequence-to-sequence \nmodel, obviating the need for second-pass rescoring. \n</p>"}, "author": "Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N. Sainath, Zhifeng Chen, Rohit Prabhavalkar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381655", "id": "tag:google.com,2005:reader/item/0000000341fb7c6f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for Predicting Chemical Properties. (arXiv:1712.02034v1 [stat.ML])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02034"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02034", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Chemical databases store information in text representations, and the SMILES \nformat is a universal standard used in many cheminformatics software. Encoded \nin each SMILES string is structural information that can be used to predict \ncomplex chemical properties. In this work, we develop SMILES2Vec, a deep RNN \nthat automatically learns features from SMILES strings to predict chemical \nproperties, without the need for additional explicit chemical information, or \nthe \"grammar\" of how SMILES encode structural data. Using Bayesian optimization \nmethods to tune the network architecture, we show that an optimized SMILES2Vec \nmodel can serve as a general-purpose neural network for learning a range of \ndistinct chemical properties including toxicity, activity, solubility and \nsolvation energy, while outperforming contemporary MLP networks that uses \nengineered features. Furthermore, we demonstrate proof-of-concept of \ninterpretability by developing an explanation mask that localizes on the most \nimportant characters used in making a prediction. When tested on the solubility \ndataset, this localization identifies specific parts of a chemical that is \nconsistent with established first-principles knowledge of solubility with an \naccuracy of 88%, demonstrating that neural networks can learn technically \naccurate chemical concepts. The fact that SMILES2Vec validates established \nchemical facts, while providing state-of-the-art accuracy, makes it a potential \ntool for widespread adoption of interpretable deep learning by the chemistry \ncommunity. \n</p>"}, "author": "Garrett B. Goh, Nathan O. Hodas, Charles Siegel, Abhinav Vishnu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381654", "id": "tag:google.com,2005:reader/item/0000000341fb7c7b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning General Latent-Variable Graphical Models with Predictive Belief Propagation and Hilbert Space Embeddings. (arXiv:1712.02046v1 [cs.LG])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02046"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02046", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a new algorithm for learning general \nlatent-variable probabilistic graphical models using the techniques of \npredictive state representation, instrumental variable regression, and \nreproducing-kernel Hilbert space embeddings of distributions. Under this new \nlearning framework, we first convert latent-variable graphical models into \ncorresponding latent-variable junction trees, and then reduce the hard \nparameter learning problem into a pipeline of supervised learning problems, \nwhose results will then be used to perform predictive belief propagation over \nthe latent junction tree during the actual inference procedure. We then give \nproofs of our algorithm's correctness, and demonstrate its good performance in \nexperiments on one synthetic dataset and two real-world tasks from \ncomputational biology and computer vision - classifying DNA splice junctions \nand recognizing human actions in videos. \n</p>"}, "author": "Borui Wang, Geoffrey Gordon", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381653", "id": "tag:google.com,2005:reader/item/0000000341fb7c82", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Distance-based Self-Attention Network for Natural Language Inference. (arXiv:1712.02047v1 [cs.CL])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02047"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02047", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Attention mechanism has been used as an ancillary means to help RNN or CNN. \nHowever, the Transformer (Vaswani et al., 2017) recently recorded the \nstate-of-the-art performance in machine translation with a dramatic reduction \nin training time by solely using attention. Motivated by the Transformer, \nDirectional Self Attention Network (Shen et al., 2017), a fully attention-based \nsentence encoder, was proposed. It showed good performance with various data by \nusing forward and backward directional information in a sentence. But in their \nstudy, not considered at all was the distance between words, an important \nfeature when learning the local dependency to help understand the context of \ninput text. We propose Distance-based Self-Attention Network, which considers \nthe word distance by using a simple distance mask in order to model the local \ndependency without losing the ability of modeling global dependency which \nattention has inherent. Our model shows good performance with NLI data, and it \nrecords the new state-of-the-art result with SNLI data. Additionally, we show \nthat our model has a strength in long sentences or documents. \n</p>"}, "author": "Jinbae Im, Sungzoon Cho", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381652", "id": "tag:google.com,2005:reader/item/0000000341fb7c89", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Human Perception of Performance. (arXiv:1712.02224v1 [physics.soc-ph])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02224"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02224", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Humans are routinely asked to evaluate the performance of other individuals, \nseparating success from failure and affecting outcomes from science to \neducation and sports. Yet, in many contexts, the metrics driving the human \nevaluation process remain unclear. Here we analyse a massive dataset capturing \nplayers' evaluations by human judges to explore human perception of performance \nin soccer, the world's most popular sport. We use machine learning to design an \nartificial judge which accurately reproduces human evaluation, allowing us to \ndemonstrate how human observers are biased towards diverse contextual features. \nBy investigating the structure of the artificial judge, we uncover the aspects \nof the players' behavior which attract the attention of human judges, \ndemonstrating that human evaluation is based on a noticeability heuristic where \nonly feature values far from the norm are considered to rate an individual's \nperformance. \n</p>"}, "author": "Luca Pappalardo, Paolo Cintia, Dino Pedreschi, Fosca Giannotti, Albert-Laszlo Barabasi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381651", "id": "tag:google.com,2005:reader/item/0000000341fb7c94", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Pose-Normalized Image Generation for Person Re-identification. (arXiv:1712.02225v1 [cs.CV])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02225"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02225", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Person Re-identification (re-id) faces two major challenges: the lack of \ncross-view paired training data and learning discriminative identity-sensitive \nand view-invariant features in the presence of large pose variations. In this \nwork, we address both problems by proposing a novel deep person image \ngeneration model for synthesizing realistic person images conditional on pose. \nThe model is based on a generative adversarial network (GAN) and used \nspecifically for pose normalization in re-id, thus termed pose-normalization \nGAN (PN-GAN). With the synthesized images, we can learn a new type of deep \nre-id feature free of the influence of pose variations. We show that this \nfeature is strong on its own and highly complementary to features learned with \nthe original images. Importantly, we now have a model that generalizes to any \nnew re-id dataset without the need for collecting any training data for model \nfine-tuning, thus making a deep re-id model truly scalable. Extensive \nexperiments on five benchmarks show that our model outperforms the \nstate-of-the-art models, often significantly. In particular, the features \nlearned on Market-1501 can achieve a Rank-1 accuracy of 68.67% on VIPeR without \nany model fine-tuning, beating almost all existing models fine-tuned on the \ndataset. \n</p>"}, "author": "Xuelin Qian, Yanwei Fu, Wenxuan Wang, Tao Xiang, Yang Wu, Yu-Gang Jiang, Xiangyang Xue", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381650", "id": "tag:google.com,2005:reader/item/0000000341fb7c9c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization. (arXiv:1003.3967v5 [cs.LG] UPDATED)", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1003.3967"}], "alternate": [{"href": "http://arxiv.org/abs/1003.3967", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Solving stochastic optimization problems under partial observability, where \none needs to adaptively make decisions with uncertain outcomes, is a \nfundamental but notoriously difficult challenge. In this paper, we introduce \nthe concept of adaptive submodularity, generalizing submodular set functions to \nadaptive policies. We prove that if a problem satisfies this property, a simple \nadaptive greedy algorithm is guaranteed to be competitive with the optimal \npolicy. In addition to providing performance guarantees for both stochastic \nmaximization and coverage, adaptive submodularity can be exploited to \ndrastically speed up the greedy algorithm by using lazy evaluations. We \nillustrate the usefulness of the concept by giving several examples of adaptive \nsubmodular objectives arising in diverse applications including sensor \nplacement, viral marketing and active learning. Proving adaptive submodularity \nfor these problems allows us to recover existing results in these applications \nas special cases, improve approximation guarantees and handle natural \ngeneralizations. \n</p>"}, "author": "Daniel Golovin, Andreas Krause", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381649", "id": "tag:google.com,2005:reader/item/0000000341fb7ca7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Using Options and Covariance Testing for Long Horizon Off-Policy Policy Evaluation. (arXiv:1703.03453v2 [cs.AI] UPDATED)", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.03453"}], "alternate": [{"href": "http://arxiv.org/abs/1703.03453", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Evaluating a policy by deploying it in the real world can be risky and \ncostly. Off-policy policy evaluation (OPE) algorithms use historical data \ncollected from running a previous policy to evaluate a new policy, which \nprovides a means for evaluating a policy without requiring it to ever be \ndeployed. Importance sampling is a popular OPE method because it is robust to \npartial observability and works with continuous states and actions. However, \nthe amount of historical data required by importance sampling can scale \nexponentially with the horizon of the problem: the number of sequential \ndecisions that are made. We propose using policies over temporally extended \nactions, called options, and show that combining these policies with importance \nsampling can significantly improve performance for long-horizon problems. In \naddition, we can take advantage of special cases that arise due to \noptions-based policies to further improve the performance of importance \nsampling. We further generalize these special cases to a general covariance \ntesting rule that can be used to decide which weights to drop in an IS \nestimate, and derive a new IS algorithm called Incremental Importance Sampling \nthat can provide significantly more accurate estimates for a broad class of \ndomains. \n</p>"}, "author": "Zhaohan Daniel Guo, Philip S. Thomas, Emma Brunskill", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381648", "id": "tag:google.com,2005:reader/item/0000000341fb7cad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Real-time On-Demand Crowd-powered Entity Extraction. (arXiv:1704.03627v2 [cs.HC] UPDATED)", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1704.03627"}], "alternate": [{"href": "http://arxiv.org/abs/1704.03627", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325052ba571\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325052ba571&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Output-agreement mechanisms such as ESP Game have been widely used in human \ncomputation to obtain reliable human-generated labels. In this paper, we argue \nthat a \"time-limited\" output-agreement mechanism can be used to create a fast \nand robust crowd-powered component in interactive systems, particularly \ndialogue systems, to extract key information from user utterances on the fly. \nOur experiments on Amazon Mechanical Turk using the Airline Travel Information \nSystem (ATIS) dataset showed that the proposed approach achieves high-quality \nresults with an average response time shorter than 9 seconds. \n</p>"}, "author": "Ting-Hao &#x27;Kenneth&#x27; Huang, Yun-Nung Chen, Jeffrey P. Bigham", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381647", "id": "tag:google.com,2005:reader/item/0000000341fb7cb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations. (arXiv:1704.04683v5 [cs.CL] UPDATED)", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1704.04683"}], "alternate": [{"href": "http://arxiv.org/abs/1704.04683", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250532f6e5\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250532f6e5&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present RACE, a new dataset for benchmark evaluation of methods in the \nreading comprehension task. Collected from the English exams for middle and \nhigh school Chinese students in the age range between 12 to 18, RACE consists \nof near 28,000 passages and near 100,000 questions generated by human experts \n(English instructors), and covers a variety of topics which are carefully \ndesigned for evaluating the students' ability in understanding and reasoning. \nIn particular, the proportion of questions that requires reasoning is much \nlarger in RACE than that in other benchmark datasets for reading comprehension, \nand there is a significant gap between the performance of the state-of-the-art \nmodels (43%) and the ceiling human performance (95%). We hope this new dataset \ncan serve as a valuable resource for research and evaluation in machine \ncomprehension. The dataset is freely available at \n<a href=\"http://www.cs.cmu.edu/~glai1/data/race/\">this http URL</a> and the code is available at \nhttps://github.com/qizhex/RACE_AR_baselines. \n</p>"}, "author": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381646", "id": "tag:google.com,2005:reader/item/0000000341fb7cbb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Autonomous Quadrotor Landing using Deep Reinforcement Learning. (arXiv:1709.03339v2 [cs.AI] UPDATED)", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.03339"}], "alternate": [{"href": "http://arxiv.org/abs/1709.03339", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Landing an unmanned aerial vehicle (UAV) on a ground marker is an open \nproblem despite the effort of the research community. Previous attempts mostly \nfocused on the analysis of hand-crafted geometric features and the use of \nexternal sensors in order to allow the vehicle to approach the land-pad. In \nthis article, we propose a method based on deep reinforcement learning that \nonly requires low-resolution images taken from a down-looking camera in order \nto identify the position of the marker and land the UAV on it. The proposed \napproach is based on a hierarchy of Deep Q-Networks (DQNs) used as high-level \ncontrol policy for the navigation toward the marker. We implemented different \ntechnical solutions, such as the combination of vanilla and double DQNs trained \nusing a partitioned buffer replay.The results show that policies trained on \nuniform textures can accomplish autonomous landing on a large variety of \nsimulated environments. The overall performance is comparable with a \nstate-of-the-art algorithm and human pilots. \n</p>"}, "author": "Riccardo Polvara, Massimiliano Patacchiola, Sanjay Sharma, Jian Wan, Andrew Manning, Robert Sutton, Angelo Cangelosi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381644", "id": "tag:google.com,2005:reader/item/0000000341fb7cbf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimizing Human Learning. (arXiv:1712.01856v1 [stat.ML])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01856"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01856", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Spaced repetition is a technique for efficient memorization which uses \nrepeated, spaced review of content to improve long-term retention. Can we find \nthe optimal reviewing schedule to maximize the benefits of spaced repetition? \nIn this paper, we introduce a novel, flexible representation of spaced \nrepetition using the framework of marked temporal point processes and then \naddress the above question as an optimal control problem for stochastic \ndifferential equations with jumps. For two well-known human memory models, we \nshow that the optimal reviewing schedule is given by the recall probability of \nthe content to be learned. As a result, we can then develop a simple, scalable \nonline algorithm, Memorize, to sample the optimal reviewing times. Experiments \non both synthetic and real data gathered from Duolingo, a popular \nlanguage-learning online platform, show that our algorithm may be able to help \nlearners memorize more effectively than alternatives. \n</p>"}, "author": "Behzad Tabibian, Utkarsh Upadhyay, Abir De, Ali Zarezade, Bernhard Schoelkopf, Manuel Gomez-Rodriguez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381643", "id": "tag:google.com,2005:reader/item/0000000341fb7cc7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models. (arXiv:1712.01864v1 [cs.CL])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01864"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01864", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>For decades, context-dependent phonemes have been the dominant sub-word unit \nfor conventional acoustic modeling systems. This status quo has begun to be \nchallenged recently by end-to-end models which seek to combine acoustic, \npronunciation, and language model components into a single neural network. Such \nsystems, which typically predict graphemes or words, simplify the recognition \nprocess since they remove the need for a separate expert-curated pronunciation \nlexicon to map from phoneme-based units to words. However, there has been \nlittle previous work comparing phoneme-based versus grapheme-based sub-word \nunits in the end-to-end modeling framework, to determine whether the gains from \nsuch approaches are primarily due to the new probabilistic model, or from the \njoint learning of the various components with grapheme-based units. \n</p> \n<p>In this work, we conduct detailed experiments which are aimed at quantifying \nthe value of phoneme-based pronunciation lexica in the context of end-to-end \nmodels. We examine phoneme-based end-to-end models, which are contrasted \nagainst grapheme-based ones on a large vocabulary English Voice-search task, \nwhere we find that graphemes do indeed outperform phonemes. We also compare \ngrapheme and phoneme-based approaches on a multi-dialect English task, which \nonce again confirm the superiority of graphemes, greatly simplifying the system \nfor recognizing multiple dialects. \n</p>"}, "author": "Tara N. Sainath, Rohit Prabhavalkar, Shankar Kumar, Seungji Lee, Anjuli Kannan, David Rybach, Vlad Schogol, Patrick Nguyen, Bo Li, Yonghui Wu, Zhifeng Chen, Chung-Cheng Chiu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381642", "id": "tag:google.com,2005:reader/item/0000000341fb7cd1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training. (arXiv:1712.01887v1 [cs.CV])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01887"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01887", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Large-scale distributed training requires significant communication bandwidth \nfor gradient exchange that limits the scalability of multi-node training, and \nrequires expensive high-bandwidth network infrastructure. The situation gets \neven worse with distributed training on mobile devices (federated learning), \nwhich suffers from higher latency, lower throughput, and intermittent poor \nconnections. In this paper, we find 99.9% of the gradient exchange in \ndistributed SGD is redundant, and propose Deep Gradient Compression (DGC) to \ngreatly reduce the communication bandwidth. To preserve accuracy during \ncompression, DGC employs four methods: momentum correction, local gradient \nclipping, momentum factor masking, and warm-up training. We have applied Deep \nGradient Compression to image classification, speech recognition, and language \nmodeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and \nLibrispeech Corpus. On these scenarios, Deep Gradient Compression achieves a \ngradient compression ratio from 270x to 600x without losing accuracy, cutting \nthe gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from \n488MB to 0.74MB. Deep gradient compression enables large-scale distributed \ntraining on inexpensive commodity 1Gbps Ethernet and facilitates distributed \ntraining on mobile. \n</p>"}, "author": "Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381641", "id": "tag:google.com,2005:reader/item/0000000341fb7cdd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Concentration of weakly dependent Banach-valued sums and applications to kernel learning methods. (arXiv:1712.01934v1 [stat.ML])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01934"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01934", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We obtain a new Bernstein-type inequality for sums of Banach-valued random \nvariables satisfying a weak dependence assumption of general type and under \ncertain smoothness assumptions of the underlying Banach norm. We use this \ninequality in order to investigate in asymptotical regime the error upper \nbounds for the broad family of spectral regularization methods for reproducing \nkernel decision rules, when trained on a sample coming from a $\\tau-$mixing \nprocess. \n</p>"}, "author": "Gilles Blanchard, Oleksandr Zadorozhnyi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381640", "id": "tag:google.com,2005:reader/item/0000000341fb7cf1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the nonparametric maximum likelihood estimator for Gaussian location mixture densities with application to Gaussian denoising. (arXiv:1712.02009v1 [math.ST])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02009"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02009", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the Nonparametric Maximum Likelihood Estimator (NPMLE) for \nestimating Gaussian location mixture densities in $d$-dimensions from \nindependent observations. Unlike usual likelihood-based methods for fitting \nmixtures, NPMLEs are based on convex optimization. We prove finite sample \nresults on the Hellinger accuracy of every NPMLE. Our results imply, in \nparticular, that every NPMLE achieves near parametric risk (up to logarithmic \nmultiplicative factors) when the true density is a discrete Gaussian mixture \nwithout any prior information on the number of mixture components. NPMLEs can \nnaturally be used to yield empirical Bayes estimates of the Oracle Bayes \nestimator in the Gaussian denoising problem. We prove bounds for the accuracy \nof the empirical Bayes estimate as an approximation to the Oracle Bayes \nestimator. Here our results imply that the empirical Bayes estimator performs \nat nearly the optimal level (up to logarithmic multiplicative factors) for \ndenoising in clustering situations without any prior knowledge of the number of \nclusters. \n</p>"}, "author": "Sujayam Saha, Adityanand Guntuboyina", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381639", "id": "tag:google.com,2005:reader/item/0000000341fb7cf7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks. (arXiv:1712.02029v1 [cs.LG])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02029"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02029", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Training deep neural networks with Stochastic Gradient Descent, or its \nvariants, requires careful choice of both learning rate and batch size. While \nsmaller batch sizes generally converge in fewer training epochs, larger batch \nsizes offer more parallelism and hence better computational efficiency. We have \ndeveloped a new training approach that, rather than statically choosing a \nsingle batch size for all epochs, adaptively increases the batch size during \nthe training process. Our method delivers the convergence rate of small batch \nsizes while achieving performance similar to large batch sizes. We analyse our \napproach using the standard AlexNet, ResNet, and VGG networks operating on the \npopular CIFAR-10, CIFAR-100, and ImageNet datasets. Our results demonstrate \nthat learning with adaptive batch sizes can improve performance by factors of \nup to 6.25 on 4 NVIDIA Tesla P100 GPUs while changing accuracy by less than 1% \nrelative to training with fixed batch sizes. \n</p>"}, "author": "Aditya Devarakonda, Maxim Naumov, Michael Garland", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381638", "id": "tag:google.com,2005:reader/item/0000000341fb7d01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Local Analysis of Block Coordinate Descent for Gaussian Phase Retrieval. (arXiv:1712.02083v1 [cs.IT])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02083"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02083", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>While convergence of the Alternating Direction Method of Multipliers (ADMM) \non convex problems is well studied, convergence on nonconvex problems is only \npartially understood. In this paper, we consider the Gaussian phase retrieval \nproblem, formulated as a linear constrained optimization problem with a \nbiconvex objective. The particular structure allows for a novel application of \nthe ADMM. It can be shown that the dual variable is zero at the global \nminimizer. This motivates the analysis of a block coordinate descent algorithm, \nwhich is equivalent to the ADMM with the dual variable fixed to be zero. We \nshow that the block coordinate descent algorithm converges to the global \nminimizer at a linear rate, when starting from a deterministically achievable \ninitialization point. \n</p>"}, "author": "David Barmherzig, Ju Sun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381637", "id": "tag:google.com,2005:reader/item/0000000341fb7d06", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Guided Labeling using Convolutional Neural Networks. (arXiv:1712.02154v1 [cs.CV])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02154"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02154", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250532fa8e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250532fa8e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Over the last couple of years, deep learning and especially convolutional \nneural networks have become one of the work horses of computer vision. One \nlimiting factor for the applicability of supervised deep learning to more areas \nis the need for large, manually labeled datasets. In this paper we propose an \neasy to implement method we call guided labeling, which automatically \ndetermines which samples from an unlabeled dataset should be labeled. We show \nthat using this procedure, the amount of samples that need to be labeled is \nreduced considerably in comparison to labeling images arbitrarily. \n</p>"}, "author": "Sebastian Stabinger, Antonio Rodriguez-Sanchez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381636", "id": "tag:google.com,2005:reader/item/0000000341fb7d14", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A trans-disciplinary review of deep learning research for water resources scientists. (arXiv:1712.02162v2 [stat.ML] UPDATED)", "published": 1512709731, "updated": 1512709735, "canonical": [{"href": "http://arxiv.org/abs/1712.02162"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02162", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning (DL), a new-generation artificial neural network research, has \nmade profound strides in recent years. This review paper is intended to provide \nwater resources scientists with a simple technical overview, trans-disciplinary \nprogress update, and potentially inspirations about DL. Effective \narchitectures, more accessible data, advances in regularization, and new \ncomputing power enabled the success of DL. A trans-disciplinary review reveals \nthat DL is rapidly transforming myriad scientific disciplines including \nhigh-energy physics, astronomy, chemistry, genomics and remote sensing, where \nsystematic DL toolkits, innovative customizations, and sub-disciplines have \nemerged. However, with a few exceptions, its adoption in hydrology has so far \nbeen gradual. The literature suggests that novel regularization techniques can \neffectively prevent high-capacity deep networks from overfitting. As a result, \nin most scientific disciplines, DL models demonstrated superior predictive and \ngeneralization performance to conventional methods. Meanwhile, less noticed is \nthat DL may also serve as a scientific exploratory tool. A new area termed \"AI \nneuroscience\", has been born. This budding sub-discipline is accumulating a \nsignificant body of work, e.g., distilling knowledge obtained in DL networks to \ninterpretable models, attributing decisions to inputs via back-propagation of \nrelevance, or visualization of activations. These methods are designed to \ninterpret the decision process of deep networks and derive insights. While \nscientists so far have mostly been using customized, ad-hoc methods for \ninterpretation, vast opportunities await for DL to propel advancement in water \nscience. \n</p>"}, "author": "Chaopeng Shen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381635", "id": "tag:google.com,2005:reader/item/0000000341fb7d1f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast spatial inference in the homogeneous Ising model. (arXiv:1712.02195v1 [stat.ME])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02195"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02195", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Ising model is important in statistical modeling and inference in many \napplications, however its normalizing constant, mean number of active vertices \nand mean spin interaction are intractable. We provide accurate approximations \nthat make it possible to calculate these quantities numerically. Simulation \nstudies indicate good performance when compared to Markov Chain Monte Carlo \nmethods and at a tiny fraction of the time. The methodology is also used to \nperform Bayesian inference in a functional Magnetic Resonance Imaging \nactivation detection experiment. \n</p>"}, "author": "Alejandro Murua, Ranjan Maitra", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381634", "id": "tag:google.com,2005:reader/item/0000000341fb7d2d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An innovative solution for breast cancer textual big data analysis. (arXiv:1712.02259v1 [stat.ML])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02259"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02259", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The digitalization of stored information in hospitals now allows for the \nexploitation of medical data in text format, as electronic health records \n(EHRs), initially gathered for other purposes than epidemiology. Manual search \nand analysis operations on such data become tedious. In recent years, the use \nof natural language processing (NLP) tools was highlighted to automatize the \nextraction of information contained in EHRs, structure it and perform \nstatistical analysis on this structured information. The main difficulties with \nthe existing approaches is the requirement of synonyms or ontology \ndictionaries, that are mostly available in English only and do not include \nlocal or custom notations. In this work, a team composed of oncologists as \ndomain experts and data scientists develop a custom NLP-based system to process \nand structure textual clinical reports of patients suffering from breast \ncancer. The tool relies on the combination of standard text mining techniques \nand an advanced synonym detection method. It allows for a global analysis by \nretrieval of indicators such as medical history, tumor characteristics, \ntherapeutic responses, recurrences and prognosis. The versatility of the method \nallows to obtain easily new indicators, thus opening up the way for \nretrospective studies with a substantial reduction of the amount of manual \nwork. With no need for biomedical annotators or pre-defined ontologies, this \nlanguage-agnostic method reached an good extraction accuracy for several \nconcepts of interest, according to a comparison with a manually structured \nfile, without requiring any existing corpus with local or new notations. \n</p>"}, "author": "Nicolas Thiebaut, Antoine Simoulin, Karl Neuberger, Issam Ibnoushein, Nicolas Bousquet, Nathalie Reix, S&#xe9;bastien Moli&#xe8;re, Carole Mathelin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381633", "id": "tag:google.com,2005:reader/item/0000000341fb7d3a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Attention based convolutional neural network for predicting RNA-protein binding sites. (arXiv:1712.02270v1 [q-bio.GN])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02270"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02270", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>RNA-binding proteins (RBPs) play crucial roles in many biological processes, \ne.g. gene regulation. Computational identification of RBP binding sites on RNAs \nare urgently needed. In particular, RBPs bind to RNAs by recognizing sequence \nmotifs. Thus, fast locating those motifs on RNA sequences is crucial and \ntime-efficient for determining whether the RNAs interact with the RBPs or not. \nIn this study, we present an attention based convolutional neural network, \niDeepA, to predict RNA-protein binding sites from raw RNA sequences. We first \nencode RNA sequences into one-hot encoding. Next, we design a deep learning \nmodel with a convolutional neural network (CNN) and an attention mechanism, \nwhich automatically search for important positions, e.g. binding motifs, to \nlearn discriminant high-level features for predicting RBP binding sites. We \nevaluate iDeepA on publicly gold-standard RBP binding sites derived from \nCLIP-seq data. The results demonstrate iDeepA achieves comparable performance \nwith other state-of-the-art methods. \n</p>"}, "author": "Xiaoyong Pan, Junchi Yan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381632", "id": "tag:google.com,2005:reader/item/0000000341fb7d42", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Exchangeable modelling of relational data: checking sparsity, train-test splitting, and sparse exchangeable Poisson matrix factorization. (arXiv:1712.02311v1 [stat.ML])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02311"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02311", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A variety of machine learning tasks---e.g., matrix factorization, topic \nmodelling, and feature allocation---can be viewed as learning the parameters of \na probability distribution over bipartite graphs. Recently, a new class of \nmodels for networks, the sparse exchangeable graphs, have been introduced to \nresolve some important pathologies of traditional approaches to statistical \nnetwork modelling; most notably, the inability to model sparsity (in the \nasymptotic sense). The present paper explains some practical insights arising \nfrom this work. We first show how to check if sparsity is relevant for \nmodelling a given (fixed size) dataset by using network subsampling to identify \na simple signature of sparsity. We discuss the implications of the (sparse) \nexchangeable subsampling theory for test-train dataset splitting; we argue \ncommon approaches can lead to biased results, and we propose a principled \nalternative. Finally, we study sparse exchangeable Poisson matrix factorization \nas a worked example. In particular, we show how to adapt mean field variational \ninference to the sparse exchangeable setting, allowing us to scale inference to \nhuge datasets. \n</p>"}, "author": "Victor Veitch, Ekansh Sharma, Zacharie Naulet, Daniel M. Roy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381631", "id": "tag:google.com,2005:reader/item/0000000341fb7d4c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SGAN: An Alternative Training of Generative Adversarial Networks. (arXiv:1712.02330v1 [stat.ML])", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.02330"}], "alternate": [{"href": "http://arxiv.org/abs/1712.02330", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Generative Adversarial Networks (GANs) have demonstrated impressive \nperformance for data synthesis, and are now used in a wide range of computer \nvision tasks. In spite of this success, they gained a reputation for being \ndifficult to train, what results in a time-consuming and human-involved \ndevelopment process to use them. \n</p> \n<p>We consider an alternative training process, named SGAN, in which several \nadversarial \"local\" pairs of networks are trained independently so that a \n\"global\" supervising pair of networks can be trained against them. The goal is \nto train the global pair with the corresponding ensemble opponent for improved \nperformances in terms of mode coverage. This approach aims at increasing the \nchances that learning will not stop for the global pair, preventing both to be \ntrapped in an unsatisfactory local minimum, or to face oscillations often \nobserved in practice. To guarantee the latter, the global pair never affects \nthe local ones. \n</p> \n<p>The rules of SGAN training are thus as follows: the global generator and \ndiscriminator are trained using the local discriminators and generators, \nrespectively, whereas the local networks are trained with their fixed local \nopponent. \n</p> \n<p>Experimental results on both toy and real-world problems demonstrate that \nthis approach outperforms standard training in terms of better mitigating mode \ncollapse, stability while converging and that it surprisingly, increases the \nconvergence speed as well. \n</p>"}, "author": "Tatjana Chavdarova, Fran&#xe7;ois Fleuret", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381630", "id": "tag:google.com,2005:reader/item/0000000341fb7d52", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Kernel clustering: density biases and solutions. (arXiv:1705.05950v5 [stat.ML] UPDATED)", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.05950"}], "alternate": [{"href": "http://arxiv.org/abs/1705.05950", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Kernel methods are popular in clustering due to their generality and \ndiscriminating power. However, we show that many kernel clustering criteria \nhave density biases theoretically explaining some practically significant \nartifacts empirically observed in the past. For example, we provide conditions \nand formally prove the density mode isolation bias in kernel K-means for a \ncommon class of kernels. We call it Breiman's bias due to its similarity to the \nhistogram mode isolation previously discovered by Breiman in decision tree \nlearning with Gini impurity. We also extend our analysis to other popular \nkernel clustering methods, e.g. average/normalized cut or dominant sets, where \ndensity biases can take different forms. For example, splitting isolated points \nby cut-based criteria is essentially the sparsest subset bias, which is the \nopposite of the density mode bias. Our findings suggest that a principled \nsolution for density biases in kernel clustering should directly address data \ninhomogeneity. We show that density equalization can be implicitly achieved \nusing either locally adaptive weights or locally adaptive kernels. Moreover, \ndensity equalization makes many popular kernel clustering objectives \nequivalent. Our synthetic and real data experiments illustrate density biases \nand proposed solutions. We anticipate that theoretical understanding of kernel \nclustering limitations and their principled solutions will be important for a \nbroad spectrum of data analysis applications across the disciplines. \n</p>"}, "author": "Dmitrii Marin, Meng Tang, Ismail Ben Ayed, Yuri Boykov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381629", "id": "tag:google.com,2005:reader/item/0000000341fb7d5c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Robust Photometric Stereo via Dictionary Learning. (arXiv:1710.08873v2 [cs.CV] UPDATED)", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08873"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08873", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Photometric stereo is a method that seeks to reconstruct the normal vectors \nof an object from a set of images of the object illuminated under different \nlight sources. While effective in some situations, classical photometric stereo \nrelies on a diffuse surface model that cannot handle objects with complex \nreflectance patterns, and it is sensitive to non-idealities in the images. In \nthis work, we propose a novel approach to photometric stereo that relies on \ndictionary learning to produce robust normal vector reconstructions. \nSpecifically, we develop three formulations for applying dictionary learning to \nphotometric stereo. We propose a preprocessing step that utilizes dictionary \nlearning to denoise the images. We also present a model that applies dictionary \nlearning to regularize and reconstruct the normal vectors from the images under \nthe classic Lambertian reflectance model. Finally, we generalize the latter \nmodel to explicitly model non-Lambertian objects. We investigate all three \napproaches through extensive experimentation on synthetic and real benchmark \ndatasets and observe state-of-the-art performance compared to existing robust \nphotometric stereo methods. \n</p>"}, "author": "Andrew J. Wagenmaker, Brian E. Moore, Raj Rao Nadakuditi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381628", "id": "tag:google.com,2005:reader/item/0000000341fb7d64", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Cubic Regularization for Fast Nonconvex Optimization. (arXiv:1711.02838v2 [cs.LG] UPDATED)", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02838"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02838", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes a stochastic variant of a classic algorithm---the \ncubic-regularized Newton method [Nesterov and Polyak 2006]. The proposed \nalgorithm efficiently escapes saddle points and finds approximate local minima \nfor general smooth, nonconvex functions in only \n$\\mathcal{\\tilde{O}}(\\epsilon^{-3.5})$ stochastic gradient and stochastic \nHessian-vector product evaluations. The latter can be computed as efficiently \nas stochastic gradients. This improves upon the \n$\\mathcal{\\tilde{O}}(\\epsilon^{-4})$ rate of stochastic gradient descent. Our \nrate matches the best-known result for finding local minima without requiring \nany delicate acceleration or variance-reduction techniques. \n</p>"}, "author": "Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, Michael I. Jordan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381627", "id": "tag:google.com,2005:reader/item/0000000341fb7d6e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On consistent vertex nomination schemes. (arXiv:1711.05610v2 [stat.ML] UPDATED)", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05610"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05610", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250532fe57\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250532fe57&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Given a vertex of interest in a network $G_1$, the vertex nomination problem \nseeks to find the corresponding vertex of interest (if it exists) in a second \nnetwork $G_2$. Although the vertex nomination problem and related tasks have \nattracted much attention in the machine learning literature, with applications \nto social and biological networks, the framework has so far been confined to a \ncomparatively small class of network models, and the concept of statistically \nconsistent vertex nomination schemes has been only shallowly explored. In this \npaper, we extend the vertex nomination problem to a very general statistical \nmodel of graphs. Further, drawing inspiration from the long-established \nclassification framework in the pattern recognition literature, we provide \ndefinitions for the key notions of Bayes optimality and consistency in our \nextended vertex nomination framework, including a derivation of the Bayes \noptimal vertex nomination scheme. In addition, we prove that no universally \nconsistent vertex nomination schemes exist. Illustrative examples are provided \nthroughout. \n</p>"}, "author": "Vince Lyzinski, Keith Levin, Carey E. Priebe", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381626", "id": "tag:google.com,2005:reader/item/0000000341fb7d78", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Mosquito detection with low-cost smartphones: data acquisition for malaria research. (arXiv:1711.06346v3 [stat.ML] UPDATED)", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06346"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06346", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250539355f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250539355f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Mosquitoes are a major vector for malaria, causing hundreds of thousands of \ndeaths in the developing world each year. Not only is the prevention of \nmosquito bites of paramount importance to the reduction of malaria transmission \ncases, but understanding in more forensic detail the interplay between malaria, \nmosquito vectors, vegetation, standing water and human populations is crucial \nto the deployment of more effective interventions. Typically the presence and \ndetection of malaria-vectoring mosquitoes is only quantified by hand-operated \ninsect traps or signified by the diagnosis of malaria. If we are to gather \ntimely, large-scale data to improve this situation, we need to automate the \nprocess of mosquito detection and classification as much as possible. In this \npaper, we present a candidate mobile sensing system that acts as both a \nportable early warning device and an automatic acoustic data acquisition \npipeline to help fuel scientific inquiry and policy. The machine learning \nalgorithm that powers the mobile system achieves excellent off-line \nmulti-species detection performance while remaining computationally efficient. \nFurther, we have conducted preliminary live mosquito detection tests using \nlow-cost mobile phones and achieved promising results. The deployment of this \nsystem for field usage in Southeast Asia and Africa is planned in the near \nfuture. In order to accelerate processing of field recordings and labelling of \ncollected data, we employ a citizen science platform in conjunction with \nautomated methods, the former implemented using the Zooniverse platform, \nallowing crowdsourcing on a grand scale. \n</p>"}, "author": "Yunpeng Li, Davide Zilli, Henry Chan, Ivan Kiskin, Marianne Sinka, Stephen Roberts, Kathy Willis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512624568382", "timestampUsec": "1512624568381625", "id": "tag:google.com,2005:reader/item/0000000341fb7d83", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Arbitrary Facial Attribute Editing: Only Change What You Want. (arXiv:1711.10678v2 [cs.CV] UPDATED)", "published": 1512624569, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.10678"}], "alternate": [{"href": "http://arxiv.org/abs/1711.10678", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Facial attribute editing aims to modify either single or multiple attributes \non a face image. Since it is practically infeasible to collect images with \narbitrarily specified attributes for each person, the generative adversarial \nnet (GAN) and the encoder-decoder architecture are usually incorporated to \nhandle this task. With the encoder-decoder architecture, arbitrary attribute \nediting can then be conducted by decoding the latent representation of the face \nimage conditioned on the specified attributes. A few existing methods attempt \nto establish attribute-independent latent representation for arbitrarily \nchanging the attributes. However, since the attributes portray the \ncharacteristics of the face image, the attribute-independent constraint on the \nlatent representation is excessive. Such constraint may result in information \nloss and unexpected distortion on the generated images (e.g. over-smoothing), \nespecially for those identifiable attributes such as gender, race etc. Instead \nof imposing the attribute-independent constraint on the latent representation, \nwe introduce an attribute classification constraint on the generated image, \njust requiring the correct change of the attributes. Meanwhile, reconstruction \nlearning is introduced in order to guarantee the preservation of all other \nattribute-excluding details on the generated image, and adversarial learning is \nemployed for visually realistic generation. Moreover, our method can be \nnaturally extended to attribute intensity manipulation. Experiments on the \nCelebA dataset show that our method outperforms the state-of-the-arts on \ngenerating realistic attribute editing results with facial details well \npreserved. \n</p>"}, "author": "Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan, Xilin Chen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473962", "id": "tag:google.com,2005:reader/item/0000000341394e91", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks. (arXiv:1712.01507v1 [cs.NE])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01507"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01507", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Hardware acceleration of Deep Neural Networks (DNNs) aims to tame their \nenormous compute intensity. Fully realizing the potential of acceleration in \nthis domain requires understanding and leveraging algorithmic properties of \nDNNs. This paper builds upon the algorithmic insight that bitwidth of \noperations in DNNs can be reduced without compromising their accuracy. However, \nto prevent accuracy loss, the bitwidth varies significantly across DNNs and it \nmay even be adjusted for each layer individually. Thus, a fixed-bitwidth \naccelerator would either offer limited benefits to accommodate the worst-case \nbitwidth, or inevitably lead to a degradation in final accuracy. To alleviate \nthese deficiencies, this work introduces dynamic bit-level fusion/decomposition \nas a new dimension in the design of DNN accelerators. We explore this dimension \nby designing Bit Fusion, a bit-flexible accelerator, that constitutes an array \nof bit-level processing elements that dynamically fuse to match the bitwidth of \nindividual DNN layers. This flexibility in the architecture minimizes the \ncomputation and the communication at the finest granularity possible with no \nloss in accuracy. We evaluate the benefits of Bit Fusion using eight real-world \nfeed-forward and recurrent DNNs. The proposed microarchitecture is implemented \nin Verilog and synthesized in 45 nm technology. Using the synthesis results and \ncycle accurate simulation, we compare the benefits of Bit Fusion to two \nstate-of-the-art DNN accelerators, Eyeriss and Stripes. In the same area, \nfrequency, and technology node, Bit Fusion offers 4.3x speedup and 9.6x energy \nsavings over Eyeriss. Bit Fusion provides 2.4x speedup and 4.1x energy \nreduction over Stripes at 45 nm node when Bit Fusion area and frequency are set \nto those of Stripes. Compared to Jetson-TX2, Bit Fusion offers 4.3x speedup and \nalmost matches the performance of TitanX, which is 4.6x faster than TX2. \n</p>"}, "author": "Hardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Joon Kyung Kim, Vikas Chandra, Hadi Esmaeilzadeh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473961", "id": "tag:google.com,2005:reader/item/0000000341394e9d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Autonomous development and learning in artificial intelligence and robotics: Scaling up deep learning to human--like learning. (arXiv:1712.01626v1 [cs.AI])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01626"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01626", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Autonomous lifelong development and learning is a fundamental capability of \nhumans, differentiating them from current deep learning systems. However, other \nbranches of artificial intelligence have designed crucial ingredients towards \nautonomous learning: curiosity and intrinsic motivation, social learning and \nnatural interaction with peers, and embodiment. These mechanisms guide \nexploration and autonomous choice of goals, and integrating them with deep \nlearning opens stimulating perspectives. Deep learning (DL) approaches made \ngreat advances in artificial intelligence, but are still far away from human \nlearning. As argued convincingly by Lake et al., differences include human \ncapabilities to learn causal models of the world from very little data, \nleveraging compositional representations and priors like intuitive physics and \npsychology. However, there are other fundamental differences between current DL \nsystems and human learning, as well as technical ingredients to fill this gap, \nthat are either superficially, or not adequately, discussed by Lake et al. \nThese fundamental mechanisms relate to autonomous development and learning. \nThey are bound to play a central role in artificial intelligence in the future. \nCurrent DL systems require engineers to manually specify a task-specific \nobjective function for every new task, and learn through off-line processing of \nlarge training databases. On the contrary, humans learn autonomously open-ended \nrepertoires of skills, deciding for themselves which goals to pursue or value, \nand which skills to explore, driven by intrinsic motivation/curiosity and \nsocial learning through natural interaction with peers. Such learning processes \nare incremental, online, and progressive. Human child development involves a \nprogressive increase of complexity in a curriculum of learning where skills are \nexplored, acquired, and built on each other, through particular ordering and \ntiming. Finally, human learning happens in the physical world, and through \nbodily and physical experimentation, under severe constraints on energy, time, \nand computational resources. In the two last decades, the field of \nDevelopmental and Cognitive Robotics (Cangelosi and Schlesinger, 2015, Asada et \nal., 2009), in strong interaction with developmental psychology and \nneuroscience, has achieved significant advances in computational \n</p>"}, "author": "Pierre-Yves Oudeyer (Flowers)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473960", "id": "tag:google.com,2005:reader/item/0000000341394ea3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generalization of Deep Neural Networks for Chest Pathology Classification in X-Rays Using Generative Adversarial Networks. (arXiv:1712.01636v1 [cs.CV])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01636"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01636", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Medical datasets are often highly imbalanced, over representing common \nmedical problems, and sparsely representing rare problems. We propose \nsimulation of pathology in images to overcome the above limitations. Using \nchest Xrays as a model medical image, we implement a generative adversarial \nnetwork (GAN) to create artificial images based upon a modest sized labeled \ndataset. We employ a combination of real and artificial images to train a deep \nconvolutional neural network (DCNN) to detect pathology across five classes of \ndisease. We furthermore demonstrate that augmenting the original imbalanced \ndataset with GAN generated images improves performance of chest pathology \nclassification using the proposed DCNN in comparison to the same DCNN trained \nwith the original dataset alone. This improved performance is largely \nattributed to balancing of the dataset using GAN generated images, where image \nclasses that are lacking in example images are preferentially augmented. \n</p>"}, "author": "Hojjat Salehinejad, Shahrokh Valaee, Tim Dowdell, Errol Colak, Joseph Barfett", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473959", "id": "tag:google.com,2005:reader/item/0000000341394eab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Artificial intelligence in peer review: How can evolutionary computation support journal editors?. (arXiv:1712.01682v1 [cs.DL])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01682"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01682", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With the volume of manuscripts submitted for publication growing every year, \nthe deficiencies of peer review (e.g. long review times) are becoming more \napparent. Editorial strategies, sets of guidelines designed to speed up the \nprocess and reduce editors workloads, are treated as trade secrets by \npublishing houses and are not shared publicly. To improve the effectiveness of \ntheir strategies, editors in small publishing groups are faced with undertaking \nan iterative trial-and-error approach. We show that Cartesian Genetic \nProgramming, a nature-inspired evolutionary algorithm, can dramatically improve \neditorial strategies. The artificially evolved strategy reduced the duration of \nthe peer review process by 30%, without increasing the pool of reviewers (in \ncomparison to a typical human-developed strategy). Evolutionary computation has \ntypically been used in technological processes or biological ecosystems. Our \nresults demonstrate that genetic programs can improve real-world social systems \nthat are usually much harder to understand and control than physical systems. \n</p>"}, "author": "Maciej J. Mrowinski, Piotr Fronczak, Agata Fronczak, Marcel Ausloos, Olgica Nedic", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473958", "id": "tag:google.com,2005:reader/item/0000000341394eae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fuzzy-Based Dialectical Non-Supervised Image Classification and Clustering. (arXiv:1712.01694v1 [cs.CV])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01694"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01694", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The materialist dialectical method is a philosophical investigative method to \nanalyze aspects of reality. These aspects are viewed as complex processes \ncomposed by basic units named poles, which interact with each other. Dialectics \nhas experienced considerable progress in the 19th century, with Hegel's \ndialectics and, in the 20th century, with the works of Marx, Engels, and \nGramsci, in Philosophy and Economics. The movement of poles through their \ncontradictions is viewed as a dynamic process with intertwined phases of \nevolution and revolutionary crisis. In order to build a computational process \nbased on dialectics, the interaction between poles can be modeled using fuzzy \nmembership functions. Based on this assumption, we introduce the Objective \nDialectical Classifier (ODC), a non-supervised map for classification based on \nmaterialist dialectics and designed as an extension of fuzzy c-means \nclassifier. As a case study, we used ODC to classify 181 magnetic resonance \nsynthetic multispectral images composed by proton density, $T_1$- and \n$T_2$-weighted synthetic brain images. Comparing ODC to k-means, fuzzy c-means, \nand Kohonen's self-organized maps, concerning with image fidelity indexes as \nestimatives of quantization distortion, we proved that ODC can reach almost the \nsame quantization performance as optimal non-supervised classifiers like \nKohonen's self-organized maps. \n</p>"}, "author": "Wellington Pinheiro dos Santos, Francisco Marcos de Assis, Ricardo Emmanuel de Souza, Priscilla B. Mendes, Henrique S. S. Monteiro, Havana Diogo Alves", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473957", "id": "tag:google.com,2005:reader/item/0000000341394ebb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Triagem virtual de imagens de imuno-histoqu\\'imica usando redes neurais artificiais e espectro de padr\\~oes. (arXiv:1712.01695v1 [cs.CV])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01695"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01695", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The importance of organizing medical images according to their nature, \napplication and relevance is increasing. Furhermore, a previous selection of \nmedical images can be useful to accelerate the task of analysis by \npathologists. Herein this work we propose an image classifier to integrate a \nCBIR (Content-Based Image Retrieval) selection system. This classifier is based \non pattern spectra and neural networks. Feature selection is performed using \npattern spectra and principal component analysis, whilst image classification \nis based on multilayer perceptrons and a composition of self-organizing maps \nand learning vector quantization. These methods were applied for content \nselection of immunohistochemical images of placenta and newdeads lungs. Results \ndemonstrated that this approach can reach reasonable classification \nperformance. \n</p>"}, "author": "Higor Neto Lima, Wellington Pinheiro dos Santos, M&#xea;user Jorge Silva Valen&#xe7;a", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473956", "id": "tag:google.com,2005:reader/item/0000000341394ec8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Dialectical Multispectral Classification of Diffusion-Weighted Magnetic Resonance Images as an Alternative to Apparent Diffusion Coefficients Maps to Perform Anatomical Analysis. (arXiv:1712.01697v1 [cs.CV])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01697"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01697", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Multispectral image analysis is a relatively promising field of research with \napplications in several areas, such as medical imaging and satellite \nmonitoring. A considerable number of current methods of analysis are based on \nparametric statistics. Alternatively, some methods in Computational \nIntelligence are inspired by biology and other sciences. Here we claim that \nPhilosophy can be also considered as a source of inspiration. This work \nproposes the Objective Dialectical Method (ODM): a method for classification \nbased on the Philosophy of Praxis. ODM is instrumental in assembling evolvable \nmathematical tools to analyze multispectral images. In the case study described \nin this paper, multispectral images are composed of diffusion-weighted (DW) \nmagnetic resonance (MR) images. The results are compared to ground-truth images \nproduced by polynomial networks using a morphological similarity index. The \nclassification results are used to improve the usual analysis of the apparent \ndiffusion coefficient map. Such results proved that gray and white matter can \nbe distinguished in DW-MR multispectral analysis and, consequently, DW-MR \nimages can also be used to furnish anatomical information. \n</p>"}, "author": "Wellington Pinheiro dos Santos, Francisco Marcos de Assis, Ricardo Emmanuel de Souza, Pl&#xed;nio Batista dos Santos Filho, Fernando Buarque de Lima Neto", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473955", "id": "tag:google.com,2005:reader/item/0000000341394ecd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Design Automation for Binarized Neural Networks: A Quantum Leap Opportunity?. (arXiv:1712.01743v1 [cs.OH])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01743"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01743", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32505393842\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32505393842&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Design automation in general, and in particular logic synthesis, can play a \nkey role in enabling the design of application-specific Binarized Neural \nNetworks (BNN). This paper presents the hardware design and synthesis of a \npurely combinational BNN for ultra-low power near-sensor processing. We \nleverage the major opportunities raised by BNN models, which consist mostly of \nlogical bit-wise operations and integer counting and comparisons, for pushing \nultra-low power deep learning circuits close to the sensor and coupling it with \nbinarized mixed-signal image sensor data. We analyze area, power and energy \nmetrics of BNNs synthesized as combinational networks. Our synthesis results in \nGlobalFoundries 22nm SOI technology shows a silicon area of 2.61mm2 for \nimplementing a combinational BNN with 32x32 binary input sensor receptive field \nand weight parameters fixed at design time. This is 2.2x smaller than a \nsynthesized network with re-configurable parameters. With respect to other \ncomparable techniques for deep learning near-sensor processing, our approach \nfeatures a 10x higher energy efficiency. \n</p>"}, "author": "Manuele Rusci, Lukas Cavigelli, Luca Benini", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473954", "id": "tag:google.com,2005:reader/item/0000000341394edb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Robustly representing inferential uncertainty in deep neural networks through sampling. (arXiv:1611.01639v5 [cs.LG] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1611.01639"}], "alternate": [{"href": "http://arxiv.org/abs/1611.01639", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>As deep neural networks (DNNs) are applied to increasingly challenging \nproblems, they will need to be able to represent their own uncertainty. \nModeling uncertainty is one of the key features of Bayesian methods. Using \nBernoulli dropout with sampling at prediction time has recently been proposed \nas an efficient and well performing variational inference method for DNNs. \nHowever, sampling from other multiplicative noise based variational \ndistributions has not been investigated in depth. We evaluated Bayesian DNNs \ntrained with Bernoulli or Gaussian multiplicative masking of either the units \n(dropout) or the weights (dropconnect). We tested the calibration of the \nprobabilistic predictions of Bayesian convolutional neural networks (CNNs) on \nMNIST and CIFAR-10. Sampling at prediction time increased the calibration of \nthe DNNs' probabalistic predictions. Sampling weights, whether Gaussian or \nBernoulli, led to more robust representation of uncertainty compared to \nsampling of units. However, using either Gaussian or Bernoulli dropout led to \nincreased test set classification accuracy. Based on these findings we used \nboth Bernoulli dropout and Gaussian dropconnect concurrently, which we show \napproximates the use of a spike-and-slab variational distribution without \nincreasing the number of learned parameters. We found that spike-and-slab \nsampling had higher test set performance than Gaussian dropconnect and more \nrobustly represented its uncertainty compared to Bernoulli dropout. \n</p>"}, "author": "Patrick McClure, Nikolaus Kriegeskorte", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473953", "id": "tag:google.com,2005:reader/item/0000000341394ee4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "One-Shot Imitation Learning. (arXiv:1703.07326v3 [cs.AI] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.07326"}], "alternate": [{"href": "http://arxiv.org/abs/1703.07326", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Imitation learning has been commonly applied to solve different tasks in \nisolation. This usually requires either careful feature engineering, or a \nsignificant number of samples. This is far from what we desire: ideally, robots \nshould be able to learn from very few demonstrations of any given task, and \ninstantly generalize to new situations of the same task, without requiring \ntask-specific engineering. In this paper, we propose a meta-learning framework \nfor achieving such capability, which we call one-shot imitation learning. \n</p> \n<p>Specifically, we consider the setting where there is a very large set of \ntasks, and each task has many instantiations. For example, a task could be to \nstack all blocks on a table into a single tower, another task could be to place \nall blocks on a table into two-block towers, etc. In each case, different \ninstances of the task would consist of different sets of blocks with different \ninitial states. At training time, our algorithm is presented with pairs of \ndemonstrations for a subset of all tasks. A neural net is trained that takes as \ninput one demonstration and the current state (which initially is the initial \nstate of the other demonstration of the pair), and outputs an action with the \ngoal that the resulting sequence of states and actions matches as closely as \npossible with the second demonstration. At test time, a demonstration of a \nsingle instance of a new task is presented, and the neural net is expected to \nperform well on new instances of this new task. The use of soft attention \nallows the model to generalize to conditions and tasks unseen in the training \ndata. We anticipate that by training this model on a much greater variety of \ntasks and settings, we will obtain a general system that can turn any \ndemonstrations into robust policies that can accomplish an overwhelming variety \nof tasks. \n</p> \n<p>Videos available at https://bit.ly/nips2017-oneshot . \n</p>"}, "author": "Yan Duan, Marcin Andrychowicz, Bradly C. Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, Wojciech Zaremba", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473952", "id": "tag:google.com,2005:reader/item/0000000341394ef0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Spectrally-normalized margin bounds for neural networks. (arXiv:1706.08498v2 [cs.LG] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.08498"}], "alternate": [{"href": "http://arxiv.org/abs/1706.08498", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a margin-based multiclass generalization bound for neural \nnetworks that scales with their margin-normalized \"spectral complexity\": their \nLipschitz constant, meaning the product of the spectral norms of the weight \nmatrices, times a certain correction factor. This bound is empirically \ninvestigated for a standard AlexNet network trained with SGD on the mnist and \ncifar10 datasets, with both original and random labels; the bound, the \nLipschitz constants, and the excess risks are all in direct correlation, \nsuggesting both that SGD selects predictors whose complexity scales with the \ndifficulty of the learning task, and secondly that the presented bound is \nsensitive to this complexity. \n</p>"}, "author": "Peter Bartlett, Dylan J. Foster, Matus Telgarsky", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473951", "id": "tag:google.com,2005:reader/item/0000000341394efb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Slope Stability Analysis with Geometric Semantic Genetic Programming. (arXiv:1708.09116v2 [cs.NE] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.09116"}], "alternate": [{"href": "http://arxiv.org/abs/1708.09116", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Genetic programming has been widely used in the engineering field. Compared \nwith the conventional genetic programming and artificial neural network, \ngeometric semantic genetic programming (GSGP) is superior in astringency and \ncomputing efficiency. In this paper, GSGP is adopted for the classification and \nregression analysis of a sample dataset. Furthermore, a model for slope \nstability analysis is established on the basis of geometric semantics. \nAccording to the results of the study based on GSGP, the method can analyze \nslope stability objectively and is highly precise in predicting slope stability \nand safety factors. Hence, the predicted results can be used as a reference for \nslope safety design. \n</p>"}, "author": "Juncai Xu, Zhenzhong Shen, Qingwen Ren, Xin Xie, Zhengyu Yang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473950", "id": "tag:google.com,2005:reader/item/0000000341394f14", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Compatibility Family Learning for Item Recommendation and Generation. (arXiv:1712.01262v1 [cs.LG])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01262"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01262", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Compatibility between items, such as clothes and shoes, is a major factor \namong customer's purchasing decisions. However, learning \"compatibility\" is \nchallenging due to (1) broader notions of compatibility than those of \nsimilarity, (2) the asymmetric nature of compatibility, and (3) only a small \nset of compatible and incompatible items are observed. We propose an end-to-end \ntrainable system to embed each item into a latent vector and project a query \nitem into K compatible prototypes in the same space. These prototypes reflect \nthe broad notions of compatibility. We refer to both the embedding and \nprototypes as \"Compatibility Family\". In our learned space, we introduce a \nnovel Projected Compatibility Distance (PCD) function which is differentiable \nand ensures diversity by aiming for at least one prototype to be close to a \ncompatible item, whereas none of the prototypes are close to an incompatible \nitem. We evaluate our system on a toy dataset, two Amazon product datasets, and \nPolyvore outfit dataset. Our method consistently achieves state-of-the-art \nperformance. Finally, we show that we can visualize the candidate compatible \nprototypes using a Metric-regularized Conditional Generative Adversarial \nNetwork (MrCGAN), where the input is a projected prototype and the output is a \ngenerated image of a compatible item. We ask human evaluators to judge the \nrelative compatibility between our generated images and images generated by \nCGANs conditioned directly on query items. Our generated images are \nsignificantly preferred, with roughly twice the number of votes as others. \n</p>"}, "author": "Yong-Siang Shih, Kai-Yueh Chang, Hsuan-Tien Lin, Min Sun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473949", "id": "tag:google.com,2005:reader/item/0000000341394f21", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Deeper Look at Experience Replay. (arXiv:1712.01275v1 [cs.LG])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01275"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01275", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Experience replay plays an important role in the success of deep \nreinforcement learning (RL) by helping stabilize the neural networks. It has \nbecome a new norm in deep RL algorithms. In this paper, however, we showcase \nthat varying the size of the experience replay buffer can hurt the performance \neven in very simple tasks. The size of the replay buffer is actually a \nhyper-parameter which needs careful tuning. Moreover, our study of experience \nreplay leads to the formulation of the Combined DQN algorithm, which can \nsignificantly outperform primitive DQN in some tasks. \n</p>"}, "author": "Shangtong Zhang, Richard S. Sutton", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473948", "id": "tag:google.com,2005:reader/item/0000000341394f28", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning User Intent from Action Sequences on Interactive Systems. (arXiv:1712.01328v1 [cs.AI])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01328"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01328", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Interactive systems have taken over the web and mobile space with increasing \nparticipation from users. Applications across every marketing domain can now be \naccessed through mobile or web where users can directly perform certain actions \nand reach a desired outcome. Actions of user on a system, though, can be \nrepresentative of a certain intent. Ability to learn this intent through user's \nactions can help draw certain insight into the behavior of users on a system. \n</p> \n<p>In this paper, we present models to optimize interactive systems by learning \nand analyzing user intent through their actions on the system. We present a \nfour phased model that uses time-series of interaction actions sequentially \nusing a Long Short-Term Memory (LSTM) based sequence learning system that helps \nbuild a model for intent recognition. Our system then provides an objective \nspecific maximization followed by analysis and contrasting methods in order to \nidentify spaces of improvement in the interaction system. We discuss deployment \nscenarios for such a system and present results from evaluation on an online \nmarketplace using user clickstream data. \n</p>"}, "author": "Rakshit Agrawal, Anwar Habeeb, Chih-Hsin Hsueh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473947", "id": "tag:google.com,2005:reader/item/0000000341394f33", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Examining Cooperation in Visual Dialog Models. (arXiv:1712.01329v1 [cs.CV])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01329"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01329", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work we propose a blackbox intervention method for visual dialog \nmodels, with the aim of assessing the contribution of individual linguistic or \nvisual components. Concretely, we conduct structured or randomized \ninterventions that aim to impair an individual component of the model, and \nobserve changes in task performance. We reproduce a state-of-the-art visual \ndialog model and demonstrate that our methodology yields surprising insights, \nnamely that both dialog and image information have minimal contributions to \ntask performance. The intervention method presented here can be applied as a \nsanity check for the strength and robustness of each component in visual dialog \nsystems. \n</p>"}, "author": "Mircea Mironenco, Dana Kianfar, Ke Tran, Evangelos Kanoulas, Efstratios Gavves", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473946", "id": "tag:google.com,2005:reader/item/0000000341394f3f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multimodal Storytelling via Generative Adversarial Imitation Learning. (arXiv:1712.01455v1 [cs.AI])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01455"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01455", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deriving event storylines is an effective summarization method to succinctly \norganize extensive information, which can significantly alleviate the pain of \ninformation overload. The critical challenge is the lack of widely recognized \ndefinition of storyline metric. Prior studies have developed various approaches \nbased on different assumptions about users' interests. These works can extract \ninteresting patterns, but their assumptions do not guarantee that the derived \npatterns will match users' preference. On the other hand, their exclusiveness \nof single modality source misses cross-modality information. This paper \nproposes a method, multimodal imitation learning via generative adversarial \nnetworks(MIL-GAN), to directly model users' interests as reflected by various \ndata. In particular, the proposed model addresses the critical challenge by \nimitating users' demonstrated storylines. Our proposed model is designed to \nlearn the reward patterns given user-provided storylines and then applies the \nlearned policy to unseen data. The proposed approach is demonstrated to be \ncapable of acquiring the user's implicit intent and outperforming competing \nmethods by a substantial margin with a user study. \n</p>"}, "author": "Zhiqian Chen, Xuchao Zhang, Arnold P. Boedihardjo, Jing Dai, Chang-Tien Lu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473945", "id": "tag:google.com,2005:reader/item/0000000341394f4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Determinism in the Certification of UNSAT Proofs. (arXiv:1712.01488v1 [cs.LO])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01488"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01488", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32505393b10\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32505393b10&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The search for increased trustworthiness of SAT solvers is very active and \nuses various methods. Some of these methods obtain a proof from the provers \nthen check it, normally by replicating the search based on the proof's \ninformation. Because the certification process involves another nontrivial \nproof search, the trust we can place in it is decreased. Some attempts to amend \nthis use certifiers which have been verified by proofs assistants such as \nIsabelle/HOL and Coq. Our approach is different because it is based on an \nextremely simplified certifier. This certifier enjoys a very high level of \ntrust but is very inefficient. In this paper, we experiment with this approach \nand conclude that by placing some restrictions on the formats, one can mostly \neliminate the need for search and in principle, can certify proofs of arbitrary \nsize. \n</p>"}, "author": "Tomer Libal (Inria, Paris), Xaviera Steele (American University of Paris)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473944", "id": "tag:google.com,2005:reader/item/0000000341394f50", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Discriminant Projection Representation-based Classification for Vision Recognition. (arXiv:1712.01643v1 [cs.CV])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01643"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01643", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325054012d3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325054012d3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Representation-based classification methods such as sparse \nrepresentation-based classification (SRC) and linear regression classification \n(LRC) have attracted a lot of attentions. In order to obtain the better \nrepresentation, a novel method called projection representation-based \nclassification (PRC) is proposed for image recognition in this paper. PRC is \nbased on a new mathematical model. This model denotes that the 'ideal \nprojection' of a sample point $x$ on the hyper-space $H$ may be gained by \niteratively computing the projection of $x$ on a line of hyper-space $H$ with \nthe proper strategy. Therefore, PRC is able to iteratively approximate the \n'ideal representation' of each subject for classification. Moreover, the \ndiscriminant PRC (DPRC) is further proposed, which obtains the discriminant \ninformation by maximizing the ratio of the between-class reconstruction error \nover the within-class reconstruction error. Experimental results on five \ntypical databases show that the proposed PRC and DPRC are effective and \noutperform other state-of-the-art methods on several vision recognition tasks. \n</p>"}, "author": "Qingxiang Feng, Yicong Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473943", "id": "tag:google.com,2005:reader/item/0000000341394f54", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Dilated FCN for Multi-Agent 2D/3D Medical Image Registration. (arXiv:1712.01651v1 [cs.CV])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01651"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01651", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>2D/3D image registration to align a 3D volume and 2D X-ray images is a \nchallenging problem due to its ill-posed nature and various artifacts presented \nin 2D X-ray images. In this paper, we propose a multi-agent system with an auto \nattention mechanism for robust and efficient 2D/3D image registration. \nSpecifically, an individual agent is trained with dilated Fully Convolutional \nNetwork (FCN) to perform registration in a Markov Decision Process (MDP) by \nobserving a local region, and the final action is then taken based on the \nproposals from multiple agents and weighted by their corresponding confidence \nlevels. The contributions of this paper are threefold. First, we formulate \n2D/3D registration as a MDP with observations, actions, and rewards properly \ndefined with respect to X-ray imaging systems. Second, to handle various \nartifacts in 2D X-ray images, multiple local agents are employed efficiently \nvia FCN-based structures, and an auto attention mechanism is proposed to favor \nthe proposals from regions with more reliable visual cues. Third, a dilated \nFCN-based training mechanism is proposed to significantly reduce the Degree of \nFreedom in the simulation of registration environment, and drastically improve \ntraining efficiency by an order of magnitude compared to standard CNN-based \ntraining method. We demonstrate that the proposed method achieves high \nrobustness on both spine cone beam Computed Tomography data with a low \nsignal-to-noise ratio and data from minimally invasive spine surgery where \nsevere image artifacts and occlusions are presented due to metal screws and \nguide wires, outperforming other state-of-the-art methods (single agent-based \nand optimization-based) by a large margin. \n</p>"}, "author": "Shun Miao, Sebastien Piat, Peter Fischer, Ahmet Tuysuzoglu, Philip Mewes, Tommaso Mansi, Rui Liao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473942", "id": "tag:google.com,2005:reader/item/0000000341394f5b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Novel Brain Decoding Method: a Correlation Network Framework for Revealing Brain Connections. (arXiv:1712.01668v1 [cs.CV])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01668"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01668", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Brain decoding is a hot spot in cognitive science, which focuses on \nreconstructing perceptual images from brain activities. Analyzing the \ncorrelations of collected data from human brain activities and representing \nactivity patterns are two problems in brain decoding based on functional \nmagnetic resonance imaging (fMRI) signals. However, existing correlation \nanalysis methods mainly focus on the strength information of voxel, which \nreveals functional connectivity in the cerebral cortex. They tend to neglect \nthe structural information that implies the intracortical or intrinsic \nconnections; that is, structural connectivity. Hence, the effective \nconnectivity inferred by these methods is relatively unilateral. Therefore, we \nproposed a correlation network (CorrNet) framework that could be flexibly \ncombined with diverse pattern representation models. In the CorrNet framework, \nthe topological correlation was introduced to reveal structural information. \nRich correlations were obtained, which contributed to specifying the underlying \neffective connectivity. We also combined the CorrNet framework with a linear \nsupport vector machine (SVM) and a dynamic evolving spike neuron network (SNN) \nfor pattern representation separately, thus providing a novel method for \ndecoding cognitive activity patterns. Experimental results verified the \nreliability and robustness of our CorrNet framework and demonstrated that the \nnew method achieved significant improvement in brain decoding over comparable \nmethods. \n</p>"}, "author": "Siyu Yu, Nanning Zheng, Yongqiang Ma, Hao Wu, Badong Chen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473941", "id": "tag:google.com,2005:reader/item/0000000341394f5f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm. (arXiv:1712.01815v1 [cs.AI])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01815"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01815", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The game of chess is the most widely-studied domain in the history of \nartificial intelligence. The strongest programs are based on a combination of \nsophisticated search techniques, domain-specific adaptations, and handcrafted \nevaluation functions that have been refined by human experts over several \ndecades. In contrast, the AlphaGo Zero program recently achieved superhuman \nperformance in the game of Go, by tabula rasa reinforcement learning from games \nof self-play. In this paper, we generalise this approach into a single \nAlphaZero algorithm that can achieve, tabula rasa, superhuman performance in \nmany challenging domains. Starting from random play, and given no domain \nknowledge except the game rules, AlphaZero achieved within 24 hours a \nsuperhuman level of play in the games of chess and shogi (Japanese chess) as \nwell as Go, and convincingly defeated a world-champion program in each case. \n</p>"}, "author": "David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473940", "id": "tag:google.com,2005:reader/item/0000000341394f6b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "WNtags: A Web-Based Tool For Image Labeling And Retrieval With Lexical Ontologies. (arXiv:1302.2223v2 [cs.IR] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1302.2223"}], "alternate": [{"href": "http://arxiv.org/abs/1302.2223", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Ever growing number of image documents available on the Internet continuously \nmotivates research in better annotation models and more efficient retrieval \nmethods. Formal knowledge representation of objects and events in pictures, \ntheir interaction as well as context complexity becomes no longer an option for \na quality image repository, but a necessity. We present an ontology-based \nonline image annotation tool WNtags and demonstrate its usefulness in several \ntypical multimedia retrieval tasks using International Affective Picture System \nemotionally annotated image database. WNtags is built around WordNet lexical \nontology but considers Suggested Upper Merged Ontology as the preferred \nlabeling formalism. WNtags uses sets of weighted WordNet synsets as high-level \nimage semantic descriptors and query matching is performed with word stemming \nand node distance metrics. We also elaborate our near future plans to expand \nimage content description with induced affect as in stimuli for research of \nhuman emotion and attention. \n</p>"}, "author": "Marko Horvat, Anton Grbin, Gordan Gledec", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473939", "id": "tag:google.com,2005:reader/item/0000000341394f7b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Network Structure Inference, A Survey: Motivations, Methods, and Applications. (arXiv:1610.00782v3 [cs.SI] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1610.00782"}], "alternate": [{"href": "http://arxiv.org/abs/1610.00782", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Networks represent relationships between entities in many complex systems, \nspanning from online social interactions to biological cell development and \nbrain connectivity. In many cases, relationships between entities are \nunambiguously known: are two users 'friends' in a social network? Do two \nresearchers collaborate on a published paper? Do two road segments in a \ntransportation system intersect? These are directly observable in the system in \nquestion. In most cases, relationship between nodes are not directly observable \nand must be inferred: does one gene regulate the expression of another? Do two \nanimals who physically co-locate have a social bond? Who infected whom in a \ndisease outbreak in a population? Existing approaches for inferring networks \nfrom data are found across many application domains, and use specialized \nknowledge to infer and measure the quality of inferred network for a specific \ntask or hypothesis. However, current research lacks a rigorous methodology \nwhich employs standard statistical validation on inferred models. In this \nsurvey, we examine (1) how network representations are constructed from \nunderlying data, (2) the variety of questions and tasks on these \nrepresentations over several domains, and (3) validation strategies for \nmeasuring the inferred network's capability of answering questions on the \nsystem of interest. \n</p>"}, "author": "Ivan Brugere, Brian Gallagher, Tanya Y. Berger-Wolf", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473938", "id": "tag:google.com,2005:reader/item/0000000341394f83", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning. (arXiv:1611.04717v3 [cs.AI] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1611.04717"}], "alternate": [{"href": "http://arxiv.org/abs/1611.04717", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Count-based exploration algorithms are known to perform near-optimally when \nused in conjunction with tabular reinforcement learning (RL) methods for \nsolving small discrete Markov decision processes (MDPs). It is generally \nthought that count-based methods cannot be applied in high-dimensional state \nspaces, since most states will only occur once. Recent deep RL exploration \nstrategies are able to deal with high-dimensional continuous state spaces \nthrough complex heuristics, often relying on optimism in the face of \nuncertainty or intrinsic motivation. In this work, we describe a surprising \nfinding: a simple generalization of the classic count-based approach can reach \nnear state-of-the-art performance on various high-dimensional and/or continuous \ndeep RL benchmarks. States are mapped to hash codes, which allows to count \ntheir occurrences with a hash table. These counts are then used to compute a \nreward bonus according to the classic count-based exploration theory. We find \nthat simple hash functions can achieve surprisingly good results on many \nchallenging tasks. Furthermore, we show that a domain-dependent learned hash \ncode may further improve these results. Detailed analysis reveals important \naspects of a good hash function: 1) having appropriate granularity and 2) \nencoding information relevant to solving the MDP. This exploration strategy \nachieves near state-of-the-art performance on both continuous control tasks and \nAtari 2600 games, hence providing a simple yet powerful baseline for solving \nMDPs that require considerable exploration. \n</p>"}, "author": "Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman, Filip De Turck, Pieter Abbeel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473937", "id": "tag:google.com,2005:reader/item/0000000341394f92", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Explicablility as Minimizing Distance from Expected Behavior. (arXiv:1611.05497v2 [cs.AI] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1611.05497"}], "alternate": [{"href": "http://arxiv.org/abs/1611.05497", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In order to have effective human AI collaboration, it is not simply enough to \naddress the question of autonomy; an equally important question is, how the \nAI's behavior is being perceived by their human counterparts. When AI agent's \ntask plans are generated without such considerations, they may often \ndemonstrate inexplicable behavior from the human's point of view. This problem \narises due to the human's partial or inaccurate understanding of the agent's \nplanning process and/or the model. This may have serious implications on \nhuman-AI collaboration, from increased cognitive load and reduced trust in the \nagent, to more serious concerns of safety in interactions with physical agent. \nIn this paper, we address this issue by modeling the notion of plan \nexplicability as a function of the distance between a plan that agent makes and \nthe plan that human expects it to make. To this end, we learn a distance \nfunction based on different plan distance measures that can accurately model \nthis notion of plan explicability, and develop an anytime search algorithm that \ncan use this distance as a heuristic to come up with progressively explicable \nplans. We evaluate the effectiveness of our approach in a simulated autonomous \ncar domain and a physical service robot domain. We provide empirical \nevaluations that demonstrate the usefulness of our approach in making the \nplanning process of an autonomous agent conform to human expectations. \n</p>"}, "author": "Anagha Kulkarni, Yantian Zha, Tathagata Chakraborti, Satya Gautam Vadlamudi, Yu Zhang, Subbarao Kambhampati", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473936", "id": "tag:google.com,2005:reader/item/0000000341394fa3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "R2-D2: ColoR-inspired Convolutional NeuRal Network (CNN)-based AndroiD Malware Detections. (arXiv:1705.04448v4 [cs.CR] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.04448"}], "alternate": [{"href": "http://arxiv.org/abs/1705.04448", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine Learning (ML) has found it particularly useful in malware detection. \nHowever, as the malware evolves very fast, the stability of the feature \nextracted from malware serves as a critical issue in malware detection. Recent \nsuccess of deep learning in image recognition, natural language processing, and \nmachine translation indicate a potential solution for stabilizing the malware \ndetection effectiveness. We present a coloR-inspired convolutional neuRal \nnetwork-based AndroiD malware Detection (R2-D2), which can detect malware \nwithout extracting pre-selected features (e.g., the control-flow of op-code, \nclasses, methods of functions and the timing they are invoked etc.) from \nAndroid apps. In particular, we develop a color representation for translating \nAndroid apps into RGB color code and transform them to a fixed-sized encoded \nimage. After that, the encoded image is fed to convolutional neural network for \nautomatic feature extraction and learning, reducing the expert's intervention. \nWe have collected over 1 million malware samples and 1 million benign samples \naccording to the data provided by Leopard Mobile Inc. from its core product \nSecurity Master (which has 623 million monthly active users and 10k new malware \nsamples per day). It is shown that R2-D2 can effectively detect the malware. \nFurthermore, we keep our research results and release experiment material on \n<a href=\"http://R2D2.TWMAN.ORG\">this http URL</a> if there is any update. \n</p>"}, "author": "TonTon Hsien-De Huang, Hung-Yu Kao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473935", "id": "tag:google.com,2005:reader/item/0000000341394fac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Vehicle Traffic Driven Camera Placement for Better Metropolis Security Surveillance. (arXiv:1705.08508v3 [cs.CY] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.08508"}], "alternate": [{"href": "http://arxiv.org/abs/1705.08508", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32505401741\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32505401741&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Security surveillance is one of the most important issues in smart cities, \nespecially in an era of terrorism. Deploying a number of (video) cameras is a \ncommon surveillance approach. Given the never-ending power offered by vehicles \nto metropolises, exploiting vehicle traffic to design camera placement \nstrategies could potentially facilitate security surveillance. This article \nconstitutes the first effort toward building the linkage between vehicle \ntraffic and security surveillance, which is a critical problem for smart \ncities. We expect our study could influence the decision making of surveillance \ncamera placement, and foster more research of principled ways of security \nsurveillance beneficial to our physical-world life. \n</p>"}, "author": "Yihui He, Xiaobo Ma, Xiapu Luo, Jianfeng Li, Mengchen Zhao, Bo An, Xiaohong Guan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473934", "id": "tag:google.com,2005:reader/item/0000000341394fb5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference. (arXiv:1705.08850v2 [cs.LG] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.08850"}], "alternate": [{"href": "http://arxiv.org/abs/1705.08850", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Semi-supervised learning methods using Generative Adversarial Networks (GANs) \nhave shown promising empirical success recently. Most of these methods use a \nshared discriminator/classifier which discriminates real examples from fake \nwhile also predicting the class label. Motivated by the ability of the GANs \ngenerator to capture the data manifold well, we propose to estimate the tangent \nspace to the data manifold using GANs and employ it to inject invariances into \nthe classifier. In the process, we propose enhancements over existing methods \nfor learning the inverse mapping (i.e., the encoder) which greatly improves in \nterms of semantic similarity of the reconstructed sample with the input sample. \nWe observe considerable empirical gains in semi-supervised learning over \nbaselines, particularly in the cases when the number of labeled examples is \nlow. We also provide insights into how fake examples influence the \nsemi-supervised learning procedure. \n</p>"}, "author": "Abhishek Kumar, Prasanna Sattigeri, P. Thomas Fletcher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473933", "id": "tag:google.com,2005:reader/item/0000000341394fb8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Pose Grammar to Encode Human Body Configuration for 3D Pose Estimation. (arXiv:1710.06513v5 [cs.CV] UPDATED)", "published": 1513228697, "updated": 1513228699, "canonical": [{"href": "http://arxiv.org/abs/1710.06513"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06513", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a pose grammar to tackle the problem of 3D human \npose estimation. Our model directly takes 2D pose as input and learns a \ngeneralized 2D-3D mapping function. The proposed model consists of a base \nnetwork which efficiently captures pose-aligned features and a hierarchy of \nBi-directional RNNs (BRNN) on the top to explicitly incorporate a set of \nknowledge regarding human body configuration (i.e., kinematics, symmetry, motor \ncoordination). The proposed model thus enforces high-level constraints over \nhuman poses. In learning, we develop a pose sample simulator to augment \ntraining samples in virtual camera views, which further improves our model \ngeneralizability. We validate our method on public 3D human pose benchmarks and \npropose a new evaluation protocol working on cross-view setting to verify the \ngeneralization capability of different methods. We empirically observe that \nmost state-of-the-art methods encounter difficulty under such setting while our \nmethod can well handle such challenges. \n</p>"}, "author": "Haoshu Fang, Yuanlu Xu, Wenguan Wang, Xiaobai Liu, Song-Chun Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473932", "id": "tag:google.com,2005:reader/item/0000000341394fc1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Clustering with feature selection using alternating minimization, Application to computational biology. (arXiv:1711.02974v2 [cs.LG] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02974"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02974", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper deals with unsupervised clustering with feature selection. The \nproblem is to estimate both labels and a sparse projection matrix of weights. \nTo address this combinatorial non-convex problem maintaining a strict control \non the sparsity of the matrix of weights, we propose an alternating \nminimization of the Frobenius norm criterion. We provide a new efficient \nalgorithm named K-sparse which alternates k-means with projection-gradient \nminimization. The projection-gradient step is a method of splitting type, with \nexact projection on the $\\ell^1$ ball to promote sparsity. The convergence of \nthe gradient-projection step is addressed, and a preliminary analysis of the \nalternating minimization is made. The Frobenius norm criterion converges as the \nnumber of iterates in Algorithm K-sparse goes to infinity. Experiments on \nSingle Cell RNA sequencing datasets show that our method significantly improves \nthe results of PCA k-means, spectral clustering, SIMLR, and Sparcl methods, and \nachieves a relevant selection of genes. The complexity of K-sparse is linear in \nthe number of samples (cells), so that the method scales up to large datasets. \n</p>"}, "author": "Cyprien Gilet, Marie Deprez, Jean-Baptiste Caillau, Michel Barlaud", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473931", "id": "tag:google.com,2005:reader/item/0000000341394fc6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improved Neural Text Attribute Transfer with Non-parallel Data. (arXiv:1711.09395v2 [cs.CL] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.09395"}], "alternate": [{"href": "http://arxiv.org/abs/1711.09395", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Text attribute transfer using non-parallel data requires methods that can \nperform disentanglement of content and linguistic attributes. In this work, we \npropose multiple improvements over the existing approaches that enable the \nencoder-decoder framework to cope with the text attribute transfer from \nnon-parallel data. We perform experiments on the sentiment transfer task using \ntwo datasets. For both datasets, our proposed method outperforms a strong \nbaseline in two of the three employed evaluation metrics. \n</p>"}, "author": "Igor Melnyk, Cicero Nogueira dos Santos, Kahini Wadhawan, Inkit Padhi, Abhishek Kumar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473928", "id": "tag:google.com,2005:reader/item/0000000341394fd3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Sparse Neural Networks through $L_0$ Regularization. (arXiv:1712.01312v1 [stat.ML])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01312"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01312", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a practical method for $L_0$ norm regularization for neural \nnetworks: pruning the network during training by encouraging weights to become \nexactly zero. Such regularization is interesting since (1) it can greatly speed \nup training and inference, and (2) it can improve generalization. AIC and BIC, \nwell-known model selection criteria, are special cases of $L_0$ regularization. \nHowever, since the $L_0$ norm of weights is non-differentiable, we cannot \nincorporate it directly as a regularization term in the objective function. We \npropose a solution through the inclusion of a collection of non-negative \nstochastic gates, which collectively determine which weights to set to zero. We \nshow that, somewhat surprisingly, for certain distributions over the gates, the \nexpected $L_0$ norm of the resulting gated weights is differentiable with \nrespect to the distribution parameters. We further propose the \\emph{hard \nconcrete} distribution for the gates, which is obtained by \"stretching\" a \nbinary concrete distribution and then transforming its samples with a \nhard-sigmoid. The parameters of the distribution over the gates can then be \njointly optimized with the original network parameters. As a result our method \nallows for straightforward and efficient learning of model structures with \nstochastic gradient descent and allows for conditional computation in a \nprincipled way. We perform various experiments to demonstrate the effectiveness \nof the resulting approach and regularizer. \n</p>"}, "author": "Christos Louizos, Max Welling, Diederik P. Kingma", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473927", "id": "tag:google.com,2005:reader/item/0000000341394fd7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Modelling collective motion based on the principle of agency. (arXiv:1712.01334v1 [q-bio.PE])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01334"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01334", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Collective motion is an intriguing phenomenon, especially considering that it \narises from a set of simple rules governing local interactions between \nindividuals. In theoretical models, these rules are normally \\emph{assumed} to \ntake a particular form, possibly constrained by heuristic arguments. We propose \na new class of models, which describe the individuals as \\emph{agents}, capable \nof deciding for themselves how to act and learning from their experiences. The \nlocal interaction rules do not need to be postulated in this model, since they \n\\emph{emerge} from the learning process. We apply this ansatz to a concrete \nscenario involving marching locusts, in order to model the phenomenon of \ndensity-dependent alignment. We show that our learning agent-based model can \naccount for a Fokker-Planck equation that describes the collective motion and, \nmost notably, that the agents can learn the appropriate local interactions, \nrequiring no strong previous assumptions on their form. These results suggest \nthat learning agent-based models are a powerful tool for studying a broader \nclass of problems involving collective motion and animal agency in general. \n</p>"}, "author": "Katja Ried, Thomas M&#xfc;ller, Hans J. Briegel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473926", "id": "tag:google.com,2005:reader/item/0000000341394fdd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Linearly-Recurrent Autoencoder Networks for Learning Dynamics. (arXiv:1712.01378v1 [math.DS])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01378"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01378", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper describes a method for learning low-dimensional approximations of \nnonlinear dynamical systems, based on neural-network approximations of the \nunderlying Koopman operator. Extended Dynamic Mode Decomposition (EDMD) \nprovides a useful data-driven approximation of the Koopman operator for \nanalyzing dynamical systems. This paper addresses a fundamental problem \nassociated with EDMD: a trade-off between representational capacity of the \ndictionary and over-fitting due to insufficient data. A new neural network \narchitecture combining an autoencoder with linear recurrent dynamics in the \nencoded state is used to learn a low-dimensional and highly informative \nKoopman-invariant subspace of observables. A method is also presented for \nbalanced model reduction of over-specified EDMD systems in feature space. \nNonlinear reconstruction using partially linear multi-kernel regression aims to \nimprove reconstruction accuracy from the low-dimensional state when the data \nhas complex but intrinsically low-dimensional structure. The techniques \ndemonstrate the ability to identify Koopman eigenfunctions of the unforced \nDuffing equation, create accurate low-dimensional models of an unstable \ncylinder wake flow, and make short-time predictions of the chaotic \nKuramoto-Sivashinsky equation. \n</p>"}, "author": "Samuel E. Otto, Clarence W. Rowley", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473925", "id": "tag:google.com,2005:reader/item/0000000341394fe4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Gaussian Process bandits with adaptive discretization. (arXiv:1712.01447v1 [stat.ML])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01447"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01447", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, the problem of maximizing a black-box function $f:\\mathcal{X} \n\\to \\mathbb{R}$ is studied in the Bayesian framework with a Gaussian Process \n(GP) prior. In particular, a new algorithm for this problem is proposed, and \nhigh probability bounds on its simple and cumulative regret are established. \nThe query point selection rule in most existing methods involves an exhaustive \nsearch over an increasingly fine sequence of uniform discretizations of \n$\\mathcal{X}$. The proposed algorithm, in contrast, adaptively refines \n$\\mathcal{X}$ which leads to a lower computational complexity, particularly \nwhen $\\mathcal{X}$ is a subset of a high dimensional Euclidean space. In \naddition to the computational gains, sufficient conditions are identified under \nwhich the regret bounds of the new algorithm improve upon the known results. \nFinally an extension of the algorithm to the case of contextual bandits is \nproposed, and high probability bounds on the contextual regret are presented. \n</p>"}, "author": "Shubhanshu Shekhar, Tara Javidi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473924", "id": "tag:google.com,2005:reader/item/0000000341394feb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep linear neural networks with arbitrary loss: All local minima are global. (arXiv:1712.01473v1 [cs.LG])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01473"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01473", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider deep linear networks with arbitrary differentiable loss. We \nprovide a short and elementary proof of the following fact: all local minima \nare global minima if each hidden layer is wider than either the input or output \nlayer. \n</p>"}, "author": "Thomas Laurent, James von Brecht", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473923", "id": "tag:google.com,2005:reader/item/0000000341394ff0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Pain from Action Unit Combinations: A Weakly Supervised Approach via Multiple Instance Learning. (arXiv:1712.01496v1 [cs.LG])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01496"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01496", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32505401abd\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32505401abd&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Facial pain expression is an important modality for assessing pain, \nespecially when a patient's verbal ability to communicate is impaired. A set of \neight facial muscle based action units (AUs), which are defined by the Facial \nAction Coding System (FACS), have been widely studied and are highly reliable \nfor pain detection through facial expressions. However, using FACS is a very \ntime consuming task that makes its clinical use prohibitive. An automated \nfacial expression recognition system (AFER) reliably detecting pain-related AUs \nwould be highly beneficial for efficient and practical pain monitoring. \nAutomated pain detection under clinical settings is viewed as a weakly \nsupervised problem, which is not suitable general AFER system that trained on \nwell labeled data. Existing pain oriented AFER research either focus on the \nindividual pain-related AU recognition or bypassing the AU detection procedure \nby training a binary pain classifier from pain intensity data. In this paper, \nwe decouple pain detection into two consecutive tasks: the AFER based AU \nlabeling at video frame level and a probabilistic measure of pain at sequence \nlevel from AU combination scores. Our work is distinguished in the following \naspects, 1) State of the art AFER tools Emotient is applied on pain oriented \ndata sets for single AU labeling. 2) Two different data structures are proposed \nto encode AU combinations from single AU scores, which forms low-dimensional \nfeature vectors for the learning framework. 3) Two weakly supervised learning \nframeworks namely multiple instance learning and multiple clustered instance \nlearning are employed corresponding to each feature structure to learn pain \nfrom video sequences. The results shows 87% pain recognition accuracy with 0.94 \nAUC on UNBC-McMaster dataset. Tests on Wilkie's dataset suggests the potential \nvalue of the proposed system for pain monitoring task under clinical settings. \n</p>"}, "author": "Zhanli Chen, Rashid Ansari, Diana J. Wilkie", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473922", "id": "tag:google.com,2005:reader/item/000000034139500b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Online Algorithm for Nonparametric Correlations. (arXiv:1712.01521v1 [stat.AP])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01521"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01521", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325054673f4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325054673f4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Nonparametric correlations such as Spearman's rank correlation and Kendall's \ntau correlation are widely applied in scientific and engineering fields. This \npaper investigates the problem of computing nonparametric correlations on the \nfly for streaming data. Standard batch algorithms are generally too slow to \nhandle real-world big data applications. They also require too much memory \nbecause all the data need to be stored in the memory before processing. This \npaper proposes a novel online algorithm for computing nonparametric \ncorrelations. The algorithm has O(1) time complexity and O(1) memory cost and \nis quite suitable for edge devices, where only limited memory and processing \npower are available. You can seek a balance between speed and accuracy by \nchanging the number of cutpoints specified in the algorithm. The online \nalgorithm can compute the nonparametric correlations 10 to 1,000 times faster \nthan the corresponding batch algorithm, and it can compute them based either on \nall past observations or on fixed-size sliding windows. \n</p>"}, "author": "Wei Xiao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473921", "id": "tag:google.com,2005:reader/item/000000034139506d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Manifold-valued Image Generation with Wasserstein Adversarial Networks. (arXiv:1712.01551v1 [cs.CV])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01551"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01551", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Unsupervised image generation has recently received an increasing amount of \nattention thanks to the great success of generative adversarial networks \n(GANs), particularly Wasserstein GANs. Inspired by the paradigm of real-valued \nimage generation, this paper makes the first attempt to formulate the problem \nof generating manifold-valued images, which are frequently encountered in \nreal-world applications. For the study, we specially exploit three typical \nmanifold-valued image generation tasks: hue-saturation-value (HSV) color image \ngeneration, chromaticity-brightness (CB) color image generation, and \ndiffusion-tensor (DT) image generation. In order to produce such kinds of \nimages as realistic as possible, we generalize the state-of-the-art technique \nof Wasserstein GANs to the manifold context with exploiting Riemannian \ngeometry. For the proposed manifold-valued image generation problem, we \nrecommend three benchmark datasets that are CIFAR-10 HSV/CB color images, \nImageNet HSV/CB color images, UCL DT image datasets. On the three datasets, we \nexperimentally demonstrate the proposed manifold-aware Wasserestein GAN can \ngenerate high quality manifold-valued images. \n</p>"}, "author": "Zhiwu Huang, Jiqing Wu, Luc Van Gool", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473920", "id": "tag:google.com,2005:reader/item/0000000341395083", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Eigendecompositions of Transfer Operators in Reproducing Kernel Hilbert Spaces. (arXiv:1712.01572v1 [math.DS])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01572"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01572", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Transfer operators such as the Perron-Frobenius or Koopman operator play an \nimportant role in the global analysis of complex dynamical systems. The \neigenfunctions of these operators can be used to detect metastable sets, to \nproject the dynamics onto the dominant slow processes, or to separate \nsuperimposed signals. We extend transfer operator theory to reproducing kernel \nHilbert spaces and show that these operators are related to Hilbert space \nrepresentations of conditional distributions, known as conditional mean \nembeddings in the machine learning community. Moreover, numerical methods to \ncompute empirical estimates of these embeddings are akin to data-driven methods \nfor the approximation of transfer operators such as extended dynamic mode \ndecomposition and its variants. In fact, most of the existing methods can be \nderived from our framework, providing a unifying view on the approximation of \ntransfer operators. One main benefit of the presented kernel-based approaches \nis that these methods can be applied to any domain where a similarity measure \ngiven by a kernel is available. We illustrate the results with the aid of \nguiding examples and highlight potential applications in molecular dynamics as \nwell as video and text data analysis. \n</p>"}, "author": "Stefan Klus, Ingmar Schuster, Krikamol Muandet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473919", "id": "tag:google.com,2005:reader/item/000000034139509e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning a Generative Model for Validity in Complex Discrete Structures. (arXiv:1712.01664v1 [stat.ML])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01664"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01664", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep generative models have been successfully used to learn representations \nfor high-dimensional discrete spaces by representing discrete objects as \nsequences, for which powerful sequence-based deep models can be employed. \nUnfortunately, these techniques are significantly hindered by the fact that \nthese generative models often produce invalid sequences: sequences which do not \nrepresent any underlying discrete structure. As a step towards solving this \nproblem, we propose to learn a deep recurrent validator model, which can \nestimate whether a partial sequence can function as the beginning of a full, \nvalid sequence. This model not only discriminates between valid and invalid \nsequences, but also provides insight as to how individual sequence elements \ninfluence the validity of the overall sequence, and the existence of a \ncorresponding discrete object. To learn this model we propose a reinforcement \nlearning approach, where an oracle which can evaluate validity of complete \nsequences provides a sparse reward signal. We believe this is a key step toward \nlearning generative models that faithfully produce valid sequences which \nrepresent discrete objects. We demonstrate its effectiveness in evaluating the \nvalidity of Python 3 source code for mathematical expressions, and improving \nthe ability of a variational autoencoder trained on SMILES strings to decode \nvalid molecular structures. \n</p>"}, "author": "David Janz, Jos van der Westhuizen, Brooks Paige, Matt J. Kusner, Jose Miguel Hernandez-Labato", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473918", "id": "tag:google.com,2005:reader/item/00000003413950b2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Differentially Private Dropout. (arXiv:1712.01665v1 [stat.ML])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01665"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01665", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Large data collections required for the training of neural networks often \ncontain sensitive information such as the medical histories of patients, and \nthe privacy of the training data must be preserved. In this paper, we introduce \na dropout technique that provides an elegant Bayesian interpretation to \ndropout, and show that the intrinsic noise added, with the primary goal of \nregularization, can be exploited to obtain a degree of differential privacy. \nThe iterative nature of training neural networks presents a challenge for \nprivacy-preserving estimation since multiple iterations increase the amount of \nnoise added. We overcome this by using a relaxed notion of differential \nprivacy, called concentrated differential privacy, which provides tighter \nestimates on the overall privacy loss. We demonstrate the accuracy of our \nprivacy-preserving dropout algorithm on benchmark datasets. \n</p>"}, "author": "Beyza Ermis, Ali Taylan Cemgil", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473917", "id": "tag:google.com,2005:reader/item/00000003413950bc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "OL\\'E: Orthogonal Low-rank Embedding, A Plug and Play Geometric Loss for Deep Learning. (arXiv:1712.01727v1 [cs.CV])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01727"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01727", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks trained using a softmax layer at the top and the \ncross-entropy loss are ubiquitous tools for image classification. Yet, this \ndoes not naturally enforce intra-class similarity nor inter-class margin of the \nlearned deep representations. To simultaneously achieve these two goals, \ndifferent solutions have been proposed in the literature, such as the pairwise \nor triplet losses. However, such solutions carry the extra task of selecting \npairs or triplets, and the extra computational burden of computing and learning \nfor many combinations of them. In this paper, we propose a plug-and-play loss \nterm for deep networks that explicitly reduces intra-class variance and \nenforces inter-class margin simultaneously, in a simple and elegant geometric \nmanner. For each class, the deep features are collapsed into a learned linear \nsubspace, or union of them, and inter-class subspaces are pushed to be as \northogonal as possible. Our proposed Orthogonal Low-rank Embedding (OL\\'E) does \nnot require carefully crafting pairs or triplets of samples for training, and \nworks standalone as a classification loss, being the first reported deep metric \nlearning framework of its kind. Because of the improved margin between features \nof different classes, the resulting deep networks generalize better, are more \ndiscriminative, and more robust. We demonstrate improved classification \nperformance in general object recognition, plugging the proposed loss term into \nexisting off-the-shelf architectures. In particular, we show the advantage of \nthe proposed loss in the small data/model scenario, and we significantly \nadvance the state-of-the-art on the Stanford STL-10 benchmark. \n</p>"}, "author": "Jos&#xe9; Lezama, Qiang Qiu, Pablo Mus&#xe9;, Guillermo Sapiro", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473916", "id": "tag:google.com,2005:reader/item/00000003413950cd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Improving the Performance of Online Neural Transducer Models. (arXiv:1712.01807v1 [cs.CL])", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01807"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01807", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Having a sequence-to-sequence model which can operate in an online fashion is \nimportant for streaming applications such as Voice Search. Neural transducer is \na streaming sequence-to-sequence model, but has shown a significant degradation \nin performance compared to non-streaming models such as Listen, Attend and \nSpell (LAS). In this paper, we present various improvements to NT. \nSpecifically, we look at increasing the window over which NT computes \nattention, mainly by looking backwards in time so the model still remains \nonline. In addition, we explore initializing a NT model from a LAS-trained \nmodel so that it is guided with a better alignment. Finally, we explore \nincluding stronger language models such as using wordpiece models, and applying \nan external LM during the beam search. On a Voice Search task, we find with \nthese improvements we can get NT to match the performance of LAS. \n</p>"}, "author": "Tara N. Sainath, Chung-Cheng Chiu, Rohit Prabhavalkar, Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Zhifeng Chen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473915", "id": "tag:google.com,2005:reader/item/00000003413950da", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Variational Inference: A Review for Statisticians. (arXiv:1601.00670v7 [stat.CO] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1601.00670"}], "alternate": [{"href": "http://arxiv.org/abs/1601.00670", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>One of the core problems of modern statistics is to approximate \ndifficult-to-compute probability densities. This problem is especially \nimportant in Bayesian statistics, which frames all inference about unknown \nquantities as a calculation involving the posterior density. In this paper, we \nreview variational inference (VI), a method from machine learning that \napproximates probability densities through optimization. VI has been used in \nmany applications and tends to be faster than classical methods, such as Markov \nchain Monte Carlo sampling. The idea behind VI is to first posit a family of \ndensities and then to find the member of that family which is close to the \ntarget. Closeness is measured by Kullback-Leibler divergence. We review the \nideas behind mean-field variational inference, discuss the special case of VI \napplied to exponential family models, present a full example with a Bayesian \nmixture of Gaussians, and derive a variant that uses stochastic optimization to \nscale up to massive data. We discuss modern research in VI and highlight \nimportant open problems. VI is powerful, but it is not yet well understood. Our \nhope in writing this paper is to catalyze statistical research on this class of \nalgorithms. \n</p>"}, "author": "David M. Blei, Alp Kucukelbir, Jon D. McAuliffe", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473914", "id": "tag:google.com,2005:reader/item/00000003413950ff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "DGM: A deep learning algorithm for solving partial differential equations. (arXiv:1708.07469v2 [q-fin.MF] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.07469"}], "alternate": [{"href": "http://arxiv.org/abs/1708.07469", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>}High-dimensional PDEs have been a longstanding computational challenge. We \npropose to solve high-dimensional PDEs by approximating the solution with a \ndeep neural network which is trained to satisfy the differential operator, \ninitial condition, and boundary conditions. We prove that the neural network \nconverges to the solution of the partial differential equation as the number of \nhidden units increases. Our algorithm is meshfree, which is key since meshes \nbecome infeasible in higher dimensions. Instead of forming a mesh, the neural \nnetwork is trained on batches of randomly sampled time and space points. We \nimplement the approach for American options (a type of free-boundary PDE which \nis widely used in finance) in up to $200$ dimensions. We call the algorithm a \n\"Deep Galerkin Method (DGM)\" since it is similar in spirit to Galerkin methods, \nwith the solution approximated by a neural network instead of a linear \ncombination of basis functions. \n</p>"}, "author": "Justin Sirignano, Konstantinos Spiliopoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473913", "id": "tag:google.com,2005:reader/item/0000000341395110", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Boosting Adversarial Attacks with Momentum. (arXiv:1710.06081v2 [cs.LG] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06081"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06081", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250546763d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250546763d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep neural networks are vulnerable to adversarial examples, which poses \nsecurity concerns on these algorithms due to the potentially severe \nconsequences. Adversarial attacks serve as an important surrogate to evaluate \nthe robustness of deep learning models before they are deployed. However, most \nof the existing adversarial attacks can only fool a black-box model with a low \nsuccess rate because of the coupling of the attack ability and the \ntransferability. To address this issue, we propose a broad class of \nmomentum-based iterative algorithms to boost adversarial attacks. By \nintegrating the momentum term into the iterative process for attacks, our \nmethods can stabilize update directions and escape from poor local maxima \nduring the iterations, resulting in more transferable adversarial examples. To \nfurther improve the success rates for black-box attacks, we apply momentum \niterative algorithms to an ensemble of models, and show that the adversarially \ntrained models with a strong defense ability are also vulnerable to our \nblack-box attacks. We hope that the proposed methods will serve as a benchmark \nfor evaluating the robustness of various deep models and defense methods. We \nwon the first places in NIPS 2017 Non-targeted Adversarial Attack and Targeted \nAdversarial Attack competitions. \n</p>"}, "author": "Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Xiaolin Hu, Jianguo Li, Jun Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473912", "id": "tag:google.com,2005:reader/item/0000000341395124", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning to Warm-Start Bayesian Hyperparameter Optimization. (arXiv:1710.06219v2 [stat.ML] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06219"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06219", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Hyperparameter optimization undergoes extensive evaluations of validation \nerrors in order to find its best configuration. Bayesian optimization is now \npopular for hyperparameter optimization, since it reduces the number of \nvalidation error evaluations required. Suppose that we are given a collection \nof datasets on which hyperparameters are already tuned by either humans with \ndomain expertise or extensive trials of cross-validation. When a model is \napplied to a new dataset, it is desirable to let Bayesian optimization start \nfrom configurations that were successful on similar datasets. To this end, we \nconstruct a Siamese network with convolutional layers followed by \nbi-directional LSTM layers, to learn meta-features over image datasets. Learned \nmeta-features are used to select a few datasets that are similar to the new \ndataset, so that a set of configurations in similar datasets is adopted as \ninitialization to warm-start Bayesian hyperparameter optimization. Experiments \non image datasets demonstrate that our learned meta-features are useful in \noptimizing hyperparameters in deep residual networks for image classification. \n</p>"}, "author": "Jungtaek Kim, Saehoon Kim, Seungjin Choi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512537801474", "timestampUsec": "1512537801473911", "id": "tag:google.com,2005:reader/item/000000034139513b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Dependent relevance determination for smooth and structured sparse regression. (arXiv:1711.10058v2 [stat.ML] UPDATED)", "published": 1512537802, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.10058"}], "alternate": [{"href": "http://arxiv.org/abs/1711.10058", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In many problem settings, parameter vectors are not merely sparse, but \ndependent in such a way that non-zero coefficients tend to cluster together. We \nrefer to this form of dependency as \"region sparsity\". Classical sparse \nregression methods, such as the lasso and automatic relevance determination \n(ARD), which model parameters as independent a priori, and therefore do not \nexploit such dependencies. Here we introduce a hierarchical model for smooth, \nregion-sparse weight vectors and tensors in a linear regression setting. Our \napproach represents a hierarchical extension of the relevance determination \nframework, where we add a transformed Gaussian process to model the \ndependencies between the prior variances of regression weights. We combine this \nwith a structured model of the prior variances of Fourier coefficients, which \neliminates unnecessary high frequencies. The resulting prior encourages weights \nto be region-sparse in two different bases simultaneously. We develop Laplace \napproximation and Monte Carlo Markov Chain (MCMC) sampling to provide efficient \ninference for the posterior. Furthermore, a two-stage convex relaxation of the \nLaplace approximation approach is also provided to relax the inevitable \nnon-convexity during the optimization. We finally show substantial improvements \nover comparable methods for both simulated and real datasets from brain \nimaging. \n</p>"}, "author": "Anqi Wu, Oluwasanmi Koyejo, Jonathan W. Pillow", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512536858997", "timestampUsec": "1512536858997297", "id": "tag:google.com,2005:reader/item/000000034137931c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "State-of-the-art Speech Recognition With Sequence-to-Sequence Models. (arXiv:1712.01769v1 [cs.CL])", "published": 1512536859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01769"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01769", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Attention-based encoder-decoder architectures such as Listen, Attend, and \nSpell (LAS), subsume the acoustic, pronunciation and language model components \nof a traditional automatic speech recognition (ASR) system into a single neural \nnetwork. In our previous work, we have shown that such architectures are \ncomparable to state-of-the-art ASR systems on dictation tasks, but it was not \nclear if such architectures would be practical for more challenging tasks such \nas voice search. In this work, we explore a variety of structural and \noptimization improvements to our LAS model which significantly improve \nperformance. On the structural side, we show that word piece models can be used \ninstead of graphemes. We introduce a novel multi-head attention architecture, \nwhich offers improvements over the commonly-used single-head attention. On the \noptimization side, we explore techniques such as synchronous training, \nscheduled sampling, label smoothing, and applying minimum word error rate \noptimization, which are all shown to improve accuracy. We present results with \na unidirectional LSTM encoder for streaming recognition. On a 12,500~hour voice \nsearch task, we find that the proposed changes improve the WER of the LAS \nsystem from 9.2% to 5.8%, which corresponds to a 13% relative improvement over \nthe best conventional system which achieves 6.7% WER. \n</p>"}, "author": "Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Katya Gonina, Navdeep Jaitly, Bo Li, Jan Chorowski, Michiel Bacchiani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml", "title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512536858997", "timestampUsec": "1512536858997296", "id": "tag:google.com,2005:reader/item/0000000341379320", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Minimum Word Error Rate Training for Attention-based Sequence-to-Sequence Models. (arXiv:1712.01818v1 [cs.CL])", "published": 1512536859, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01818"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01818", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Sequence-to-sequence models, such as attention-based models in automatic \nspeech recognition (ASR), are typically trained to optimize the cross-entropy \ncriterion which corresponds to improving the log-likelihood of the data. \nHowever, system performance is usually measured in terms of word error rate \n(WER), not log-likelihood. Traditional ASR systems benefit from discriminative \nsequence training which optimizes criteria such as the state-level minimum \nBayes risk (sMBR) which are more closely related to WER. In the present work, \nwe explore techniques to train attention-based models to directly minimize \nexpected word error rate. We consider two loss functions which approximate the \nexpected number of word errors: either by sampling from the model, or by using \nN-best lists of decoded hypotheses, which we find to be more effective than the \nsampling-based method. In experimental evaluations, we find that the proposed \ntraining procedure improves performance by up to 8.2% relative to the baseline \nsystem. This allows us to train grapheme-based, uni-directional attention-based \nmodels which match the performance of a traditional, state-of-the-art, \ndiscriminative sequence-trained system on a mobile voice-search task. \n</p>"}, "author": "Rohit Prabhavalkar, Tara N. Sainath, Yonghui Wu, Patrick Nguyen, Zhifeng Chen, Chung-Cheng Chiu, Anjuli Kannan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml", "title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841025", "id": "tag:google.com,2005:reader/item/0000000340763912", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Elementary Analysis of the Probability That a Binomial Random Variable Exceeds Its Expectation. (arXiv:1712.00519v2 [math.PR] UPDATED)", "published": 1512767377, "updated": 1512767377, "canonical": [{"href": "http://arxiv.org/abs/1712.00519"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00519", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We give an elementary proof of the fact that a binomial random variable $X$ \nwith parameters $n$ and $0.29/n \\le p &lt; 1$ with probability at least $1/4$ \nstrictly exceeds its expectation. We also show that for $1/n \\le p &lt; 1 - 1/n$, \n$X$ exceeds its expectation by more than one with probability at least \n$0.0370$. Both probabilities approach $1/2$ when $np$ and $n(1-p)$ tend to \ninfinity. \n</p>"}, "author": "Benjamin Doerr", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841024", "id": "tag:google.com,2005:reader/item/0000000340763916", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Recurrent Neural Network Language Models for Open Vocabulary Event-Level Cyber Anomaly Detection. (arXiv:1712.00557v1 [cs.NE])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00557"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00557", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Automated analysis methods are crucial aids for monitoring and defending a \nnetwork to protect the sensitive or confidential data it hosts. This work \nintroduces a flexible, powerful, and unsupervised approach to detecting \nanomalous behavior in computer and network logs, one that largely eliminates \ndomain-dependent feature engineering employed by existing methods. By treating \nsystem logs as threads of interleaved \"sentences\" (event log lines) to train \nonline unsupervised neural network language models, our approach provides an \nadaptive model of normal network behavior. We compare the effectiveness of both \nstandard and bidirectional recurrent neural network language models at \ndetecting malicious activity within network log data. Extending these models, \nwe introduce a tiered recurrent architecture, which provides context by \nmodeling sequences of users' actions over time. Compared to Isolation Forest \nand Principal Components Analysis, two popular anomaly detection algorithms, we \nobserve superior performance on the Los Alamos National Laboratory Cyber \nSecurity dataset. For log-line-level red team detection, our best performing \ncharacter-based model provides test set area under the receiver operator \ncharacteristic curve of 0.98, demonstrating the strong fine-grained anomaly \ndetection performance of this approach on open vocabulary logging sources. \n</p>"}, "author": "Aaron Tuor, Ryan Baerwolf, Nicolas Knowles, Brian Hutchinson, Nicole Nichols, Rob Jasper", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841023", "id": "tag:google.com,2005:reader/item/000000034076391b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Evaluation of Alzheimer's Disease by Analysis of MR Images using Multilayer Perceptrons and Kohonen SOM Classifiers as an Alternative to the ADC Maps. (arXiv:1712.00712v1 [eess.IV])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00712"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00712", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Alzheimer's disease is the most common cause of dementia, yet hard to \ndiagnose precisely without invasive techniques, particularly at the onset of \nthe disease. This work approaches image analysis and classification of \nsynthetic multispectral images composed by diffusion-weighted magnetic \nresonance (MR) cerebral images for the evaluation of cerebrospinal fluid area \nand measuring the advance of Alzheimer's disease. A clinical 1.5 T MR imaging \nsystem was used to acquire all images presented. The classification methods are \nbased on multilayer perceptrons and Kohonen Self-Organized Map classifiers. We \nassume the classes of interest can be separated by hyperquadrics. Therefore, a \n2-degree polynomial network is used to classify the original image, generating \nthe ground truth image. The classification results are used to improve the \nusual analysis of the apparent diffusion coefficient map. \n</p>"}, "author": "Wellington Pinheiro dos Santos, Ricardo Emmanuel de Souza, Pl&#xed;nio B. dos Santos Filho", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841022", "id": "tag:google.com,2005:reader/item/0000000340763922", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems. (arXiv:1712.00754v1 [cs.NE])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00754"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00754", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A new class of non-homogeneous state-affine systems is introduced. Sufficient \nconditions are identified that guarantee first, that the associated reservoir \ncomputers with linear readouts are causal, time-invariant, and satisfy the \nfading memory property and second, that a subset of this class is universal in \nthe category of fading memory filters with stochastic almost surely bounded \ninputs. This means that any discrete-time filter that satisfies the fading \nmemory property with random inputs of that type can be uniformly approximated \nby elements in the non-homogeneous state-affine family. \n</p>"}, "author": "Lyudmila Grigoryeva, Juan-Pablo Ortega", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841021", "id": "tag:google.com,2005:reader/item/0000000340763928", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Reconstruction of Electrical Impedance Tomography Using Fish School Search, Non-Blind Search, and Genetic Algorithm. (arXiv:1712.00789v1 [physics.med-ph])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00789"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00789", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Electrical Impedance Tomography (EIT) is a noninvasive imaging technique that \ndoes not use ionizing radiation, with application both in environmental \nsciences and in health. Image reconstruction is performed by solving an inverse \nproblem and ill-posed. Evolutionary Computation and Swarm Intelligence have \nbecome a source of methods for solving inverse problems. Fish School Search \n(FSS) is a promising search and optimization method, based on the dynamics of \nschools of fish. In this article the authors present a method for \nreconstruction of EIT images based on FSS and Non-Blind Search (NBS). The \nmethod was evaluated using numerical phantoms consisting of electrical \nconductivity images with subjects in the center, between the center and the \nedge and on the edge of a circular section, with meshes of 415 finite elements. \nThe authors performed 20 simulations for each configuration. Results showed \nthat both FSS and FSS-NBS were able to converge faster than genetic algorithms. \n</p>"}, "author": "Valter Augusto de Freitas Barbosa, Reiga Ramalho Ribeiro, Allan Rivalles Souza Feitosa, Victor Luiz Bezerra Ara&#xfa;jo da Silva, Arthur Diego Dias Rocha, Rafaela Covello de Freitas, Ricardo Emmanuel de Souza, Wellington Pinheiro dos Santos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841020", "id": "tag:google.com,2005:reader/item/0000000340763933", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hierarchical Actor-Critic. (arXiv:1712.00948v1 [cs.AI])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00948"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00948", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32505467882\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32505467882&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present a novel approach to hierarchical reinforcement learning called \nHierarchical Actor-Critic (HAC). HAC aims to make learning tasks with sparse \nbinary rewards more efficient by enabling agents to learn how to break down \ntasks from scratch. The technique uses of a set of actor-critic networks that \nlearn to decompose tasks into a hierarchy of subgoals. We demonstrate that HAC \nsignificantly improves sample efficiency in a series of tasks that involve \nsparse binary rewards and require behavior over a long time horizon. \n</p>"}, "author": "Andrew Levy, Robert Platt, Kate Saenko", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841019", "id": "tag:google.com,2005:reader/item/0000000340763941", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Drift Analysis. (arXiv:1712.00964v1 [cs.NE])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00964"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00964", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250550d318\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250550d318&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Drift analysis is one of the major tools for analysing evolutionary \nalgorithms and nature-inspired search heuristics. In this chapter we give an \nintroduction to drift analysis and give some examples of how to use it for the \nanalysis of evolutionary algorithms. \n</p>"}, "author": "Johannes Lengler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841018", "id": "tag:google.com,2005:reader/item/000000034076394a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "NEURAghe: Exploiting CPU-FPGA Synergies for Efficient and Flexible CNN Inference Acceleration on Zynq SoCs. (arXiv:1712.00994v1 [cs.NE])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00994"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00994", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep convolutional neural networks (CNNs) obtain outstanding results in tasks \nthat require human-level understanding of data, like image or speech \nrecognition. However, their computational load is significant, motivating the \ndevelopment of CNN-specialized accelerators. This work presents NEURAghe, a \nflexible and efficient hardware/software solution for the acceleration of CNNs \non Zynq SoCs. NEURAghe leverages the synergistic usage of Zynq ARM cores and of \na powerful and flexible Convolution-Specific Processor deployed on the \nreconfigurable logic. The Convolution-Specific Processor embeds both a \nconvolution engine and a programmable soft core, releasing the ARM processors \nfrom most of the supervision duties and allowing the accelerator to be \ncontrolled by software at an ultra-fine granularity. This methodology opens the \nway for cooperative heterogeneous computing: while the accelerator takes care \nof the bulk of the CNN workload, the ARM cores can seamlessly execute \nhard-to-accelerate parts of the computational graph, taking advantage of the \nNEON vector engines to further speed up computation. Through the companion \nNeuDNN SW stack, NEURAghe supports end-to-end CNN-based classification with a \npeak performance of 169 Gops/s, and an energy efficiency of 17 Gops/W. Thanks \nto our heterogeneous computing model, our platform improves upon the \nstate-of-the-art, achieving a frame rate of 5.5 fps on the end-to-end execution \nof VGG-16, and 6.6 fps on ResNet-18. \n</p>"}, "author": "Paolo Meloni, Alessandro Capotondi, Gianfranco Deriu, Michele Brian, Francesco Conti, Davide Rossi, Luigi Raffo, Luca Benini", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841017", "id": "tag:google.com,2005:reader/item/0000000340763952", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Natural Langevin Dynamics for Neural Networks. (arXiv:1712.01076v1 [stat.ML])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01076"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01076", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>One way to avoid overfitting in machine learning is to use model parameters \ndistributed according to a Bayesian posterior given the data, rather than the \nmaximum likelihood estimator. Stochastic gradient Langevin dynamics (SGLD) is \none algorithm to approximate such Bayesian posteriors for large models and \ndatasets. SGLD is a standard stochastic gradient descent to which is added a \ncontrolled amount of noise, specifically scaled so that the parameter converges \nin law to the posterior distribution [WT11, TTV16]. The posterior predictive \ndistribution can be approximated by an ensemble of samples from the trajectory. \n</p> \n<p>Choice of the variance of the noise is known to impact the practical behavior \nof SGLD: for instance, noise should be smaller for sensitive parameter \ndirections. Theoretically, it has been suggested to use the inverse Fisher \ninformation matrix of the model as the variance of the noise, since it is also \nthe variance of the Bayesian posterior [PT13, AKW12, GC11]. But the Fisher \nmatrix is costly to compute for large- dimensional models. \n</p> \n<p>Here we use the easily computed Fisher matrix approximations for deep neural \nnetworks from [MO16, Oll15]. The resulting natural Langevin dynamics combines \nthe advantages of Amari's natural gradient descent and Fisher-preconditioned \nLangevin dynamics for large neural networks. \n</p> \n<p>Small-scale experiments on MNIST show that Fisher matrix preconditioning \nbrings SGLD close to dropout as a regularizing technique. \n</p>"}, "author": "Ga&#xe9;tan Marceau-Caron, Yann Ollivier", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841016", "id": "tag:google.com,2005:reader/item/0000000340763964", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Case for Learned Index Structures. (arXiv:1712.01208v2 [cs.DB] UPDATED)", "published": 1513141859, "updated": 1513141860, "canonical": [{"href": "http://arxiv.org/abs/1712.01208"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01208", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Indexes are models: a B-Tree-Index can be seen as a model to map a key to the \nposition of a record within a sorted array, a Hash-Index as a model to map a \nkey to a position of a record within an unsorted array, and a BitMap-Index as a \nmodel to indicate if a data record exists or not. In this exploratory research \npaper, we start from this premise and posit that all existing index structures \ncan be replaced with other types of models, including deep-learning models, \nwhich we term learned indexes. The key idea is that a model can learn the sort \norder or structure of lookup keys and use this signal to effectively predict \nthe position or existence of records. We theoretically analyze under which \nconditions learned indexes outperform traditional index structures and describe \nthe main challenges in designing learned index structures. Our initial results \nshow, that by using neural nets we are able to outperform cache-optimized \nB-Trees by up to 70% in speed while saving an order-of-magnitude in memory over \nseveral real-world data sets. More importantly though, we believe that the idea \nof replacing core components of a data management system through learned models \nhas far reaching implications for future systems designs and that this work \njust provides a glimpse of what might be possible. \n</p>"}, "author": "Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, Neoklis Polyzotis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841015", "id": "tag:google.com,2005:reader/item/0000000340763969", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "End-to-End Differentiable Proving. (arXiv:1705.11040v2 [cs.NE] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.11040"}], "alternate": [{"href": "http://arxiv.org/abs/1705.11040", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce neural networks for end-to-end differentiable proving of queries \nto knowledge bases by operating on dense vector representations of symbols. \nThese neural networks are constructed recursively by taking inspiration from \nthe backward chaining algorithm as used in Prolog. Specifically, we replace \nsymbolic unification with a differentiable computation on vector \nrepresentations of symbols using a radial basis function kernel, thereby \ncombining symbolic reasoning with learning subsymbolic vector representations. \nBy using gradient descent, the resulting neural network can be trained to infer \nfacts from a given incomplete knowledge base. It learns to (i) place \nrepresentations of similar symbols in close proximity in a vector space, (ii) \nmake use of such similarities to prove queries, (iii) induce logical rules, and \n(iv) use provided and induced logical rules for multi-hop reasoning. We \ndemonstrate that this architecture outperforms ComplEx, a state-of-the-art \nneural link prediction model, on three out of four benchmark knowledge bases \nwhile at the same time inducing interpretable function-free first-order logic \nrules. \n</p>"}, "author": "Tim Rockt&#xe4;schel, Sebastian Riedel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841014", "id": "tag:google.com,2005:reader/item/0000000340763970", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Visual Features for Context-Aware Speech Recognition. (arXiv:1712.00489v1 [cs.CL])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00489"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00489", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Automatic transcriptions of consumer-generated multi-media content such as \n\"Youtube\" videos still exhibit high word error rates. Such data typically \noccupies a very broad domain, has been recorded in challenging conditions, with \ncheap hardware and a focus on the visual modality, and may have been \npost-processed or edited. In this paper, we extend our earlier work on adapting \nthe acoustic model of a DNN-based speech recognition system to an RNN language \nmodel and show how both can be adapted to the objects and scenes that can be \nautomatically detected in the video. We are working on a corpus of \"how-to\" \nvideos from the web, and the idea is that an object that can be seen (\"car\"), \nor a scene that is being detected (\"kitchen\") can be used to condition both \nmodels on the \"context\" of the recording, thereby reducing perplexity and \nimproving transcription. We achieve good improvements in both cases and compare \nand analyze the respective reductions in word error rate. We expect that our \nresults can be used for any type of speech processing in which \"context\" \ninformation is available, for example in robotics, man-machine interaction, or \nwhen indexing large audio-visual archives, and should ultimately help to bring \ntogether the \"video-to-text\" and \"speech-to-text\" communities. \n</p>"}, "author": "Abhinav Gupta, Yajie Miao, Leonardo Neves, Florian Metze", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841013", "id": "tag:google.com,2005:reader/item/0000000340763978", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences. (arXiv:1712.00547v2 [cs.AI] UPDATED)", "published": 1512537802, "updated": 1512537804, "canonical": [{"href": "http://arxiv.org/abs/1712.00547"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00547", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In his seminal book `The Inmates are Running the Asylum: Why High-Tech \nProducts Drive Us Crazy And How To Restore The Sanity' [2004, Sams \nIndianapolis, IN, USA], Alan Cooper argues that a major reason why software is \noften poorly designed (from a user perspective) is that programmers are in \ncharge of design decisions, rather than interaction designers. As a result, \nprogrammers design software for themselves, rather than for their target \naudience, a phenomenon he refers to as the `inmates running the asylum'. This \npaper argues that explainable AI risks a similar fate. While the re-emergence \nof explainable AI is positive, this paper argues most of us as AI researchers \nare building explanatory agents for ourselves, rather than for the intended \nusers. But explainable AI is more likely to succeed if researchers and \npractitioners understand, adopt, implement, and improve models from the vast \nand valuable bodies of research in philosophy, psychology, and cognitive \nscience, and if evaluation of these models is focused more on people than on \ntechnology. From a light scan of literature, we demonstrate that there is \nconsiderable scope to infuse more results from the social and behavioural \nsciences into explainable AI, and present some key results from these fields \nthat are relevant to explainable AI. \n</p>"}, "author": "Tim Miller, Piers Howe, Liz Sonenberg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841012", "id": "tag:google.com,2005:reader/item/000000034076397e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Interactive Reinforcement Learning for Object Grounding via Self-Talking. (arXiv:1712.00576v1 [cs.AI])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00576"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00576", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Humans are able to identify a referred visual object in a complex scene via a \nfew rounds of natural language communications. Success communication requires \nboth parties to engage and learn to adapt for each other. In this paper, we \nintroduce an interactive training method to improve the natural language \nconversation system for a visual grounding task. During interactive training, \nboth agents are reinforced by the guidance from a common reward function. The \nparametrized reward function also cooperatively updates itself via \ninteractions, and contribute to accomplishing the task. We evaluate the method \non GuessWhat?! visual grounding task, and significantly improve the task \nsuccess rate. However, we observe language drifting problem during training and \npropose to use reward engineering to improve the interpretability for the \ngenerated conversations. Our result also indicates evaluating goal-ended visual \nconversation tasks require semantic relevant metrics beyond task success rate. \n</p>"}, "author": "Yan Zhu, Shaoting Zhang, Dimitris Metaxas", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841011", "id": "tag:google.com,2005:reader/item/000000034076399a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "MAgent: A Many-Agent Reinforcement Learning Platform for Artificial Collective Intelligence. (arXiv:1712.00600v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00600"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00600", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce MAgent, a platform to support research and development of \nmany-agent reinforcement learning. Unlike previous research platforms on single \nor multi-agent reinforcement learning, MAgent focuses on supporting the tasks \nand the applications that require hundreds to millions of agents. Within the \ninteractions among a population of agents, it enables not only the study of \nlearning algorithms for agents' optimal polices, but more importantly, the \nobservation and understanding of individual agent's behaviors and social \nphenomena emerging from the AI society, including communication languages, \nleaderships, altruism. MAgent is highly scalable and can host up to one million \nagents on a single GPU server. MAgent also provides flexible configurations for \nAI researchers to design their customized environments and agents. In this \ndemo, we present three environments designed on MAgent and show emerged \ncollective intelligence by learning from scratch. \n</p>"}, "author": "Lianmin Zheng, Jiacheng Yang, Han Cai, Weinan Zhang, Jun Wang, Yong Yu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841010", "id": "tag:google.com,2005:reader/item/00000003407639bc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "PFAx: Predictable Feature Analysis to Perform Control. (arXiv:1712.00634v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00634"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00634", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250550d652\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250550d652&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Predictable Feature Analysis (PFA) (Richthofer, Wiskott, ICMLA 2015) is an \nalgorithm that performs dimensionality reduction on high dimensional input \nsignal. It extracts those subsignals that are most predictable according to a \ncertain prediction model. We refer to these extracted signals as predictable \nfeatures. \n</p> \n<p>In this work we extend the notion of PFA to take supplementary information \ninto account for improving its predictions. Such information can be a \nmultidimensional signal like the main input to PFA, but is regarded external. \nThat means it won't participate in the feature extraction - no features get \nextracted or composed of it. Features will be exclusively extracted from the \nmain input such that they are most predictable based on themselves and the \nsupplementary information. We refer to this enhanced PFA as PFAx (PFA \nextended). \n</p> \n<p>Even more important than improving prediction quality is to observe the \neffect of supplementary information on feature selection. PFAx transparently \nprovides insight how the supplementary information adds to prediction quality \nand whether it is valuable at all. Finally we show how to invert that relation \nand can generate the supplementary information such that it would yield a \ncertain desired outcome of the main signal. \n</p> \n<p>We apply this to a setting inspired by reinforcement learning and let the \nalgorithm learn how to control an agent in an environment. With this method it \nis feasible to locally optimize the agent's state, i.e. reach a certain goal \nthat is near enough. We are preparing a follow-up paper that extends this \nmethod such that also global optimization is feasible. \n</p>"}, "author": "Stefan Richthofer, Laurenz Wiskott", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841009", "id": "tag:google.com,2005:reader/item/00000003407639d1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "From knowledge-based to data-driven modeling of fuzzy rule-based systems: A critical reflection. (arXiv:1712.00646v1 [cs.AI])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00646"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00646", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper briefly elaborates on a development in (applied) fuzzy logic that \nhas taken place in the last couple of decades, namely, the complementation or \neven replacement of the traditional knowledge-based approach to fuzzy \nrule-based systems design by a data-driven one. It is argued that the classical \nrule-based modeling paradigm is actually more amenable to the knowledge-based \napproach, for which it has originally been conceived, while being less apt to \ndata-driven model design. An important reason that prevents fuzzy (rule-based) \nsystems from being leveraged in large-scale applications is the flat structure \nof rule bases, along with the local nature of fuzzy rules and their limited \nability to express complex dependencies between variables. This motivates \nalternative approaches to fuzzy systems modeling, in which functional \ndependencies can be represented more flexibly and more compactly in terms of \nhierarchical structures. \n</p>"}, "author": "Eyke H&#xfc;llermeier", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841008", "id": "tag:google.com,2005:reader/item/00000003407639e6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Simulated Annealing Algorithm for Graph Coloring. (arXiv:1712.00709v1 [cs.AI])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00709"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00709", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The goal of this Random Walks project is to code and experiment the Markov \nChain Monte Carlo (MCMC) method for the problem of graph coloring. In this \nreport, we present the plots of cost function \\(\\mathbf{H}\\) by varying the \nparameters like \\(\\mathbf{q}\\) (Number of colors that can be used in coloring) \nand \\(\\mathbf{c}\\) (Average node degree). The results are obtained by using \nsimulated annealing scheme, where the temperature (inverse of \n\\(\\mathbf{\\beta}\\)) parameter in the MCMC is lowered progressively. \n</p>"}, "author": "Alper Kose, Berke Aral Sonmez, Metin Balaban", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841007", "id": "tag:google.com,2005:reader/item/00000003407639fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sentiment Classification using Images and Label Embeddings. (arXiv:1712.00725v1 [cs.CL] CROSS LISTED)", "published": 1512767377, "updated": 1512767377, "canonical": [{"href": "http://arxiv.org/abs/1712.00725"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00725", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this project we analysed how much semantic information images carry, and \nhow much value image data can add to sentiment analysis of the text associated \nwith the images. To better understand the contribution from images, we compared \nmodels which only made use of image data, models which only made use of text \ndata, and models which combined both data types. We also analysed if this \napproach could help sentiment classifiers generalize to unknown sentiments. \n</p>"}, "author": "Laura Graesser, Abhinav Gupta, Lakshay Sharma, Evelina Bakhturina", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841006", "id": "tag:google.com,2005:reader/item/0000000340763a13", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of Spurious Local Minima. (arXiv:1712.00779v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00779"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00779", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of learning a one-hidden-layer neural network with \nnon-overlapping convolutional layer and ReLU activation function, i.e., \n$f(\\mathbf{Z}; \\mathbf{w}, \\mathbf{a}) = \\sum_j \na_j\\sigma(\\mathbf{w}^\\top\\mathbf{Z}_j)$, in which both the convolutional \nweights $\\mathbf{w}$ and the output weights $\\mathbf{a}$ are parameters to be \nlearned. We prove that with Gaussian input $\\mathbf{Z}$, there is a spurious \nlocal minimum that is not a global mininum. Surprisingly, in the presence of \nlocal minimum, starting from randomly initialized weights, gradient descent \nwith weight normalization can still be proven to recover the true parameters \nwith constant probability (which can be boosted to arbitrarily high accuracy \nwith multiple restarts). We also show that with constant probability, the same \nprocedure could also converge to the spurious local minimum, showing that the \nlocal minimum plays a non-trivial role in the dynamics of gradient descent. \nFurthermore, a quantitative analysis shows that the gradient descent dynamics \nhas two phases: it starts off slow, but converges much faster after several \niterations. \n</p>"}, "author": "Simon S. Du, Jason D. Lee, Yuandong Tian, Barnabas Poczos, Aarti Singh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841005", "id": "tag:google.com,2005:reader/item/0000000340763a24", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Visual Explanation by High-Level Abduction: On Answer-Set Programming Driven Reasoning about Moving Objects. (arXiv:1712.00840v1 [cs.AI])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00840"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00840", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a hybrid architecture for systematically computing robust visual \nexplanation(s) encompassing hypothesis formation, belief revision, and default \nreasoning with video data. The architecture consists of two tightly integrated \nsynergistic components: (1) (functional) answer set programming based abductive \nreasoning with space-time tracklets as native entities; and (2) a visual \nprocessing pipeline for detection based object tracking and motion analysis. \n</p> \n<p>We present the formal framework, its general implementation as a \n(declarative) method in answer set programming, and an example application and \nevaluation based on two diverse video datasets: the MOTChallenge benchmark \ndeveloped by the vision community, and a recently developed Movie Dataset. \n</p>"}, "author": "Jakob Suchan, Mehul Bhatt, Przemys&#x142;aw Wa&#x142;&#x119;ga, Carl Schultz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841004", "id": "tag:google.com,2005:reader/item/0000000340763a2a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Always Lurking: Understanding and Mitigating Bias in Online Human Trafficking Detection. (arXiv:1712.00846v1 [cs.AI])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00846"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00846", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Web-based human trafficking activity has increased in recent years but it \nremains sparsely dispersed among escort advertisements and difficult to \nidentify due to its often-latent nature. The use of intelligent systems to \ndetect trafficking can thus have a direct impact on investigative resource \nallocation and decision-making, and, more broadly, help curb a widespread \nsocial problem. Trafficking detection involves assigning a normalized score to \na set of escort advertisements crawled from the Web -- a higher score indicates \na greater risk of trafficking-related (involuntary) activities. In this paper, \nwe define and study the problem of trafficking detection and present a \ntrafficking detection pipeline architecture developed over three years of \nresearch within the DARPA Memex program. Drawing on multi-institutional data, \nsystems, and experiences collected during this time, we also conduct post hoc \nbias analyses and present a bias mitigation plan. Our findings show that, while \nautomatic trafficking detection is an important application of AI for social \ngood, it also provides cautionary lessons for deploying predictive machine \nlearning algorithms without appropriate de-biasing. This ultimately led to \nintegration of an interpretable solution into a search system that contains \nover 100 million advertisements and is used by over 200 law enforcement \nagencies to investigate leads. \n</p>"}, "author": "Kyle Hundman, Thamme Gowda, Mayank Kejriwal, Benedikt Boecking", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841003", "id": "tag:google.com,2005:reader/item/0000000340763a30", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Proceedings of the Fifth Workshop on Proof eXchange for Theorem Proving. (arXiv:1712.00898v1 [cs.LO])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00898"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00898", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This volume of EPTCS contains the proceedings of the Fifth Workshop on Proof \nExchange for Theorem Proving (PxTP 2017), held on September 23-24, 2017 as part \nof the Tableaux, FroCoS and ITP conferences in Brasilia, Brazil. The PxTP \nworkshop series brings together researchers working on various aspects of \ncommunication, integration, and cooperation between reasoning systems and \nformalisms, with a special focus on proofs. The progress in computer-aided \nreasoning, both automated and interactive, during the past decades, made it \npossible to build deduction tools that are increasingly more applicable to a \nwider range of problems and are able to tackle larger problems progressively \nfaster. In recent years, cooperation between such tools in larger systems has \ndemonstrated the potential to reduce the amount of manual intervention. \nCooperation between reasoning systems relies on availability of theoretical \nformalisms and practical tools to exchange problems, proofs, and models. The \nPxTP workshop series strives to encourage such cooperation by inviting \ncontributions on all aspects of cooperation between reasoning tools, whether \nautomatic or interactive. \n</p>"}, "author": "Catherine Dubois, Bruno Woltzenlogel Paleo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841002", "id": "tag:google.com,2005:reader/item/0000000340763a37", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning Can Reverse Photon Migration for Diffuse Optical Tomography. (arXiv:1712.00912v1 [cs.CV])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00912"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00912", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Can artificial intelligence (AI) learn complicated non-linear physics? Here \nwe propose a novel deep learning approach that learns non-linear photon \nscattering physics and obtains accurate 3D distribution of optical anomalies. \nIn contrast to the traditional black-box deep learning approaches to inverse \nproblems, our deep network learns to invert the Lippmann-Schwinger integral \nequation which describes the essential physics of photon migration of diffuse \nnear-infrared (NIR) photons in turbid media. As an example for clinical \nrelevance, we applied the method to our prototype diffuse optical tomography \n(DOT). We show that our deep neural network, trained with only simulation data, \ncan accurately recover the location of anomalies within biomimetic phantoms and \nlive animals without the use of an exogenous contrast agent. \n</p>"}, "author": "Jaejun Yoo, Sohail Sabir, Duchang Heo, Kee Hyun Kim, Abdul Wahab, Yoonseok Choi, Seul-I Lee, Eun Young Chae, Hak Hee Kim, Young Min Bae, Young-wook Choi, Seungryong Cho, Jong Chul Ye", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841001", "id": "tag:google.com,2005:reader/item/0000000340763a46", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SERKET: An Architecture For Connecting Stochastic Models to Realize a Large-Scale Cognitive Model. (arXiv:1712.00929v1 [cs.AI])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00929"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00929", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>To realize human-like robot intelligence, a large-scale cognitive \narchitecture is required for robots to understand the environment through a \nvariety of sensors with which they are equipped. In this paper, we propose a \nnovel framework named Serket that enables the construction of a large-scale \ngenerative model and its inference easily by connecting sub-modules to allow \nthe robots to acquire various capabilities through interaction with their \nenvironments and others. We consider that large-scale cognitive models can be \nconstructed by connecting smaller fundamental models hierarchically while \nmaintaining their programmatic independence. Moreover, connected modules are \ndependent on each other, and parameters are required to be optimized as a \nwhole. Conventionally, the equations for parameter estimation have to be \nderived and implemented depending on the models. However, it becomes harder to \nderive and implement those of a larger scale model. To solve these problems, in \nthis paper, we propose a method for parameter estimation by communicating the \nminimal parameters between various modules while maintaining their programmatic \nindependence. Therefore, Serket makes it easy to construct large-scale models \nand estimate their parameters via the connection of modules. Experimental \nresults demonstrated that the model can be constructed by connecting modules, \nthe parameters can be optimized as a whole, and they are comparable with the \noriginal models that we have proposed. \n</p>"}, "author": "Tomoaki Nakamura, Takayuki Nagai, Tadahiro Taniguchi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843841000", "id": "tag:google.com,2005:reader/item/0000000340763a58", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "End-to-End Relation Extraction using Markov Logic Networks. (arXiv:1712.00988v1 [cs.AI])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00988"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00988", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250550d8ea\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250550d8ea&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The task of end-to-end relation extraction consists of two sub-tasks: i) \nidentifying entity mentions along with their types and ii) recognizing semantic \nrelations among the entity mention pairs. %Identifying entity mentions along \nwith their types and recognizing semantic relations among the entity mentions, \nare two very important problems in Information Extraction. It has been shown \nthat for better performance, it is necessary to address these two sub-tasks \njointly. We propose an approach for simultaneous extraction of entity mentions \nand relations in a sentence, by using inference in Markov Logic Networks (MLN). \nWe learn three different classifiers : i) local entity classifier, ii) local \nrelation classifier and iii) \"pipeline\" relation classifier which uses \npredictions of the local entity classifier. Predictions of these classifiers \nmay be inconsistent with each other. We represent these predictions along with \nsome domain knowledge using weighted first-order logic rules in an MLN and \nperform joint inference over the MLN to obtain a global output with minimum \ninconsistencies. Experiments on the ACE (Automatic Content Extraction) 2004 \ndataset demonstrate that our approach of joint extraction using MLNs \noutperforms the baselines of individual classifiers. Our end-to-end relation \nextraction performance is better than 2 out of 3 previous results reported on \nthe ACE 2004 dataset. \n</p>"}, "author": "Sachin Pawar, Pushpak Bhattacharya, Girish K. Palshikar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840999", "id": "tag:google.com,2005:reader/item/0000000340763a65", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals. (arXiv:1712.00991v1 [cs.CL])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00991"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00991", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250557a1a6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250557a1a6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Performance appraisal (PA) is an important HR process to periodically measure \nand evaluate every employee's performance vis-a-vis the goals established by \nthe organization. A PA process involves purposeful multi-step multi-modal \ncommunication between employees, their supervisors and their peers, such as \nself-appraisal, supervisor assessment and peer feedback. Analysis of the \nstructured data and text produced in PA is crucial for measuring the quality of \nappraisals and tracking actual improvements. In this paper, we apply text \nmining techniques to produce insights from PA text. First, we perform sentence \nclassification to identify strengths, weaknesses and suggestions of \nimprovements found in the supervisor assessments and then use clustering to \ndiscover broad categories among them. Next we use multi-class multi-label \nclassification techniques to match supervisor assessments to predefined broad \nperspectives on performance. Finally, we propose a short-text summarization \ntechnique to produce a summary of peer feedback comments for a given employee \nand compare it with manual summaries. All techniques are illustrated using a \nreal-life dataset of supervisor assessment and peer feedback text produced \nduring the PA of 4528 employees in a large multi-national IT company. \n</p>"}, "author": "Girish Keshav Palshikar, Sachin Pawar, Saheb Chourasia, Nitin Ramrakhiyani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840998", "id": "tag:google.com,2005:reader/item/0000000340763a6c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Characterizing and Computing Causes for Query Answers in Databases from Database Repairs and Repair Programs. (arXiv:1712.01001v1 [cs.DB])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01001"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01001", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A correspondence between database tuples as causes for query answers in \ndatabases and tuple-based repairs of inconsistent databases with respect to \ndenial constraints has already been established. In this work, answer-set \nprograms that specify repairs of databases are used as a basis for solving \ncomputational and reasoning problems about causes. Here, causes are also \nintroduced at the attribute level by appealing to a both null-based and \nattribute-based repair semantics. The corresponding repair programs are \npresented, and they are used as a basis for computation and reasoning about \nattribute-level causes. \n</p>"}, "author": "Leopoldo Bertossi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840997", "id": "tag:google.com,2005:reader/item/0000000340763a74", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "The mind as a computational system. (arXiv:1712.01093v1 [cs.AI])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01093"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01093", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The present document is an excerpt of an essay that I wrote as part of my \napplication material to graduate school in Computer Science (with a focus on \nArtificial Intelligence), in 1986. I was not invited by any of the schools that \nreceived it, so I became a theoretical physicist instead. The essay's full \ntitle was \"Some Topics in Philosophy and Computer Science\". I am making this \ntext (unchanged from 1985, preserving the typesetting as much as possible) \navailable now in memory of Jerry Fodor, whose writings had influenced me \nsignificantly at the time (even though I did not always agree). \n</p>"}, "author": "Christoph Adami", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840996", "id": "tag:google.com,2005:reader/item/0000000340763a7c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Transferring Autonomous Driving Knowledge on Simulated and Real Intersections. (arXiv:1712.01106v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01106"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01106", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We view intersection handling on autonomous vehicles as a reinforcement \nlearning problem, and study its behavior in a transfer learning setting. We \nshow that a network trained on one type of intersection generally is not able \nto generalize to other intersections. However, a network that is pre-trained on \none intersection and fine-tuned on another performs better on the new task \ncompared to training in isolation. This network also retains knowledge of the \nprior task, even though some forgetting occurs. Finally, we show that the \nbenefits of fine-tuning hold when transferring simulated intersection handling \nknowledge to a real autonomous vehicle. \n</p>"}, "author": "David Isele, Akansel Cosgun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840995", "id": "tag:google.com,2005:reader/item/0000000340763a86", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the Real-time Vehicle Placement Problem. (arXiv:1712.01235v1 [cs.AI])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01235"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01235", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Motivated by ride-sharing platforms' efforts to reduce their riders' wait \ntimes for a vehicle, this paper introduces a novel problem of placing vehicles \nto fulfill real-time pickup requests in a spatially and temporally changing \nenvironment. The real-time nature of this problem makes it fundamentally \ndifferent from other placement and scheduling problems, as it requires not only \nreal-time placement decisions but also handling real-time request dynamics, \nwhich are influenced by human mobility patterns. We use a dataset of ten \nmillion ride requests from four major U.S. cities to show that the requests \nexhibit significant self-similarity. We then propose distributed online \nlearning algorithms for the real-time vehicle placement problem and bound their \nexpected performance under this observed self-similarity. \n</p>"}, "author": "Abhinav Jauhri, Carlee Joe-Wong, John Paul Shen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840994", "id": "tag:google.com,2005:reader/item/0000000340763a8e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Equivalence of Fully Connected Layer and Convolutional Layer. (arXiv:1712.01252v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01252"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01252", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This article demonstrates that convolutional operation can be converted to \nmatrix multiplication, which has the same calculation way with fully connected \nlayer. The article is helpful for the beginners of the neural network to \nunderstand how fully connected layer and the convolutional layer work in the \nbackend. To be concise and to make the article more readable, we only consider \nthe linear case. It can be extended to the non-linear case easily through \nplugging in a non-linear encapsulation to the values like this $\\sigma(x)$ \ndenoted as $x^{\\prime}$. \n</p>"}, "author": "Wei Ma, Jun Lu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840993", "id": "tag:google.com,2005:reader/item/0000000340763a96", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Classical Planning in Deep Latent Space: Bridging the Subsymbolic-Symbolic Boundary. (arXiv:1705.00154v3 [cs.AI] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.00154"}], "alternate": [{"href": "http://arxiv.org/abs/1705.00154", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Current domain-independent, classical planners require symbolic models of the \nproblem domain and instance as input, resulting in a knowledge acquisition \nbottleneck. Meanwhile, although deep learning has achieved significant success \nin many fields, the knowledge is encoded in a subsymbolic representation which \nis incompatible with symbolic systems such as planners. We propose LatPlan, an \nunsupervised architecture combining deep learning and classical planning. Given \nonly an unlabeled set of image pairs showing a subset of transitions allowed in \nthe environment (training inputs), and a pair of images representing the \ninitial and the goal states (planning inputs), LatPlan finds a plan to the goal \nstate in a symbolic latent space and returns a visualized plan execution. The \ncontribution of this paper is twofold: (1) State Autoencoder, which finds a \npropositional state representation of the environment using a Variational \nAutoencoder. It generates a discrete latent vector from the images, based on \nwhich a PDDL model can be constructed and then solved by an off-the-shelf \nplanner. (2) Action Autoencoder / Discriminator, a neural architecture which \njointly finds the action symbols and the implicit action models \n(preconditions/effects), and provides a successor function for the implicit \ngraph search. We evaluate LatPlan using image-based versions of 3 planning \ndomains: 8-puzzle, Towers of Hanoi and LightsOut. \n</p>"}, "author": "Masataro Asai, Alex Fukunaga", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840992", "id": "tag:google.com,2005:reader/item/0000000340763aa0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Thinking Fast and Slow with Deep Learning and Tree Search. (arXiv:1705.08439v4 [cs.AI] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.08439"}], "alternate": [{"href": "http://arxiv.org/abs/1705.08439", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Sequential decision making problems, such as structured prediction, robotic \ncontrol, and game playing, require a combination of planning policies and \ngeneralisation of those plans. In this paper, we present Expert Iteration \n(ExIt), a novel reinforcement learning algorithm which decomposes the problem \ninto separate planning and generalisation tasks. Planning new policies is \nperformed by tree search, while a deep neural network generalises those plans. \nSubsequently, tree search is improved by using the neural network policy to \nguide search, increasing the strength of new plans. In contrast, standard deep \nReinforcement Learning algorithms rely on a neural network not only to \ngeneralise plans, but to discover them too. We show that ExIt outperforms \nREINFORCE for training a neural network to play the board game Hex, and our \nfinal tree search agent, trained tabula rasa, defeats MoHex 1.0, the most \nrecent Olympiad Champion player to be publicly released. \n</p>"}, "author": "Thomas Anthony, Zheng Tian, David Barber", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840991", "id": "tag:google.com,2005:reader/item/0000000340763aa7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning. (arXiv:1708.02596v2 [cs.LG] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.02596"}], "alternate": [{"href": "http://arxiv.org/abs/1708.02596", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Model-free deep reinforcement learning algorithms have been shown to be \ncapable of learning a wide range of robotic skills, but typically require a \nvery large number of samples to achieve good performance. Model-based \nalgorithms, in principle, can provide for much more efficient learning, but \nhave proven difficult to extend to expressive, high-capacity models such as \ndeep neural networks. In this work, we demonstrate that medium-sized neural \nnetwork models can in fact be combined with model predictive control (MPC) to \nachieve excellent sample complexity in a model-based reinforcement learning \nalgorithm, producing stable and plausible gaits to accomplish various complex \nlocomotion tasks. We also propose using deep neural network dynamics models to \ninitialize a model-free learner, in order to combine the sample efficiency of \nmodel-based approaches with the high task-specific performance of model-free \nmethods. We empirically demonstrate on MuJoCo locomotion tasks that our pure \nmodel-based approach trained on just random action data can follow arbitrary \ntrajectories with excellent sample efficiency, and that our hybrid algorithm \ncan accelerate model-free learning on high-speed benchmark tasks, achieving \nsample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents. \nVideos can be found at https://sites.google.com/view/mbmf \n</p>"}, "author": "Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, Sergey Levine", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840990", "id": "tag:google.com,2005:reader/item/0000000340763aaf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neural Block Sampling. (arXiv:1708.06040v2 [cs.AI] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.06040"}], "alternate": [{"href": "http://arxiv.org/abs/1708.06040", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250557a3d9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250557a3d9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Efficient Monte Carlo inference often requires manual construction of \nmodel-specific proposals. We propose an approach to automated proposal \nconstruction by training neural networks to provide fast approximations to \nblock Gibbs conditionals. The learned proposals generalize to occurrences of \ncommon structural motifs both within a given model and across models, allowing \nfor the construction of a library of learned inference primitives that can \naccelerate inference on unseen models with no model-specific training required. \nWe explore several applications including open-universe Gaussian mixture \nmodels, in which our learned proposals outperform a hand-tuned sampler, and a \nreal-world named entity recognition task, in which our sampler's ability to \nescape local modes yields higher final F1 scores than single-site Gibbs. \n</p>"}, "author": "Tongzhou Wang, Yi Wu, David A. Moore, Stuart J. Russell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840989", "id": "tag:google.com,2005:reader/item/0000000340763ab8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning 6-DOF Grasping Interaction with Deep Geometry-aware 3D Representations. (arXiv:1708.07303v3 [cs.RO] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.07303"}], "alternate": [{"href": "http://arxiv.org/abs/1708.07303", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper focuses on the problem of learning 6-DOF grasping with a parallel \njaw gripper in simulation. We propose the notion of a geometry-aware \nrepresentation in grasping based on the assumption that knowledge of 3D \ngeometry is at the heart of interaction. Our key idea is constraining and \nregularizing grasping interaction learning through 3D geometry prediction. \nSpecifically, we formulate the learning of deep geometry-aware grasping model \nin two steps: First, we learn to build mental geometry-aware representation by \nreconstructing the scene (i.e., 3D occupancy grid) from RGBD input via \ngenerative 3D shape modeling. Second, we learn to predict grasping outcome with \nits internal geometry-aware representation. The learned outcome prediction \nmodel is used to sequentially propose grasping solutions via \nanalysis-by-synthesis optimization. Our contributions are fourfold: (1) To best \nof our knowledge, we are presenting for the first time a method to learn a \n6-DOF grasping net from RGBD input; (2) We build a grasping dataset from \ndemonstrations in virtual reality with rich sensory and interaction \nannotations. This dataset includes 101 everyday objects spread across 7 \ncategories, additionally, we propose a data augmentation strategy for effective \nlearning; (3) We demonstrate that the learned geometry-aware representation \nleads to about 10 percent relative performance improvement over the baseline \nCNN on grasping objects from our dataset. (4) We further demonstrate that the \nmodel generalizes to novel viewpoints and object instances. \n</p>"}, "author": "Xinchen Yan, Jasmine Hsu, Mohi Khansari, Yunfei Bai, Arkanath Pathak, Abhinav Gupta, James Davidson, Honglak Lee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840988", "id": "tag:google.com,2005:reader/item/0000000340763abc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network. (arXiv:1709.04555v2 [cs.LG] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.04555"}], "alternate": [{"href": "http://arxiv.org/abs/1709.04555", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The prediction of organic reaction outcomes is a fundamental problem in \ncomputational chemistry. Since a reaction may involve hundreds of atoms, fully \nexploring the space of possible transformations is intractable. The current \nsolution utilizes reaction templates to limit the space, but it suffers from \ncoverage and efficiency issues. In this paper, we propose a template-free \napproach to efficiently explore the space of product molecules by first \npinpointing the reaction center -- the set of nodes and edges where graph edits \noccur. Since only a small number of atoms contribute to reaction center, we can \ndirectly enumerate candidate products. The generated candidates are scored by a \nWeisfeiler-Lehman Difference Network that models high-order interactions \nbetween changes occurring at nodes across the molecule. Our framework \noutperforms the top-performing template-based approach with a 10\\% margin, \nwhile running orders of magnitude faster. Finally, we demonstrate that the \nmodel accuracy rivals the performance of domain experts. \n</p>"}, "author": "Wengong Jin, Connor W. Coley, Regina Barzilay, Tommi Jaakkola", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840987", "id": "tag:google.com,2005:reader/item/0000000340763ac2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Motif-based Rule Discovery for Predicting Real-valued Time Series. (arXiv:1709.04763v4 [cs.AI] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.04763"}], "alternate": [{"href": "http://arxiv.org/abs/1709.04763", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Time series prediction is of great significance in many applications and has \nattracted extensive attention from the data mining community. Existing work \nsuggests that for many problems, the shape in the current time series may \ncorrelate an upcoming shape in the same or another series. Therefore, it is a \npromising strategy to associate two recurring patterns as a rule's antecedent \nand consequent: the occurrence of the antecedent can foretell the occurrence of \nthe consequent, and the learned shape of consequent will give accurate \npredictions. Earlier work employs symbolization methods, but the symbolized \nrepresentation maintains too little information of the original series to mine \nvalid rules. The state-of-the-art work, though directly manipulating the \nseries, fails to segment the series precisely for seeking \nantecedents/consequents, resulting in inaccurate rules in common scenarios. In \nthis paper, we propose a novel motif-based rule discovery method, which \nutilizes motif discovery to accurately extract frequently occurring consecutive \nsubsequences, i.e. motifs, as antecedents/consequents. It then investigates the \nunderlying relationships between motifs by matching motifs as rule candidates \nand ranking them based on the similarities. Experimental results on real open \ndatasets show that the proposed approach outperforms the baseline method by \n23.9%. Furthermore, it extends the applicability from single time series to \nmultiple ones. \n</p>"}, "author": "Yuanduo He, Xu Chu, Juguang Peng, Jingyue Gao, Yasha Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840986", "id": "tag:google.com,2005:reader/item/0000000340763ace", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast Top-k Area Topics Extraction with Knowledge Base. (arXiv:1710.04822v2 [cs.AI] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04822"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04822", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>What are the most popular research topics in Artificial Intelligence (AI)? We \nformulate the problem as extracting top-$k$ topics that can best represent a \ngiven area with the help of knowledge base. We theoretically prove that the \nproblem is NP-hard and propose an optimization model, FastKATE, to address this \nproblem by combining both explicit and latent representations for each topic. \nWe leverage a large-scale knowledge base (Wikipedia) to generate topic \nembeddings using neural networks and use this kind of representations to help \ncapture the representativeness of topics for given areas. We develop a fast \nheuristic algorithm to efficiently solve the problem with a provable error \nbound. We evaluate the proposed model on three real-world datasets. \nExperimental results demonstrate our model's effectiveness, robustness, \nreal-timeness (return results in $&lt;1$s), and its superiority over several \nalternative methods. \n</p>"}, "author": "Fang Zhang, Xiaochen Wang, Jingfei Han, Jie Tang, Shiyin Wang, Marie-Francine Moens", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840985", "id": "tag:google.com,2005:reader/item/0000000340763ad2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Novel Stochastic Stratified Average Gradient Method: Convergence Rate and Its Complexity. (arXiv:1710.07783v3 [cs.LG] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07783"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07783", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>SGD (Stochastic Gradient Descent) is a popular algorithm for large scale \noptimization problems due to its low iterative cost. However, SGD can not \nachieve linear convergence rate as FGD (Full Gradient Descent) because of the \ninherent gradient variance. To attack the problem, mini-batch SGD was proposed \nto get a trade-off in terms of convergence rate and iteration cost. In this \npaper, a general CVI (Convergence-Variance Inequality) equation is presented to \nstate formally the interaction of convergence rate and gradient variance. Then \na novel algorithm named SSAG (Stochastic Stratified Average Gradient) is \nintroduced to reduce gradient variance based on two techniques, stratified \nsampling and averaging over iterations that is a key idea in SAG (Stochastic \nAverage Gradient). Furthermore, SSAG can achieve linear convergence rate of \n$\\mathcal {O}((1-\\frac{\\mu}{8CL})^k)$ at smaller storage and iterative costs, \nwhere $C\\geq 2$ is the category number of training data. This convergence rate \ndepends mainly on the variance between classes, but not on the variance within \nthe classes. In the case of $C\\ll N$ ($N$ is the training data size), SSAG's \nconvergence rate is much better than SAG's convergence rate of $\\mathcal \n{O}((1-\\frac{\\mu}{8NL})^k)$. Our experimental results show SSAG outperforms SAG \nand many other algorithms. \n</p>"}, "author": "Aixiang Chen, Bingchuan Chen, Xiaolong Chai, Rui Bian, Hengguang Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840984", "id": "tag:google.com,2005:reader/item/0000000340763adb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Safety-Aware Apprenticeship Learning. (arXiv:1710.07983v2 [cs.AI] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07983"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07983", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Apprenticeship learning (AL) is a class of \"learning from demonstrations\" \ntechniques where the reward function of a Markov Decision Process (MDP) is \nunknown to the learning agent and the agent has to derive a good policy by \nobserving an expert's demonstrations. In this paper, we study the problem of \nhow to make AL algorithms inherently safe while still meeting its learning \nobjective. We consider a setting where the unknown reward function is assumed \nto be a linear combination of a set of state features, and the safety property \nis specified in Probabilistic Computation Tree Logic (PCTL). By embedding \nprobabilistic model checking inside AL, we propose a novel \ncounterexample-guided approach that can ensure both safety and performance of \nthe learned policy. We demonstrate the effectiveness of our approach on several \nchallenging AL scenarios where safety is essential. \n</p>"}, "author": "Weichao Zhou, Wenchao Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840983", "id": "tag:google.com,2005:reader/item/0000000340763ae3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Context-Aware Generative Adversarial Privacy. (arXiv:1710.09549v3 [cs.LG] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09549"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09549", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Preserving the utility of published datasets while simultaneously providing \nprovable privacy guarantees is a well-known challenge. On the one hand, \ncontext-free privacy solutions, such as differential privacy, provide strong \nprivacy guarantees, but often lead to a significant reduction in utility. On \nthe other hand, context-aware privacy solutions, such as information theoretic \nprivacy, achieve an improved privacy-utility tradeoff, but assume that the data \nholder has access to dataset statistics. We circumvent these limitations by \nintroducing a novel context-aware privacy framework called generative \nadversarial privacy (GAP). GAP leverages recent advancements in generative \nadversarial networks (GANs) to allow the data holder to learn privatization \nschemes from the dataset itself. Under GAP, learning the privacy mechanism is \nformulated as a constrained minimax game between two players: a privatizer that \nsanitizes the dataset in a way that limits the risk of inference attacks on the \nindividuals' private variables, and an adversary that tries to infer the \nprivate variables from the sanitized dataset. To evaluate GAP's performance, we \ninvestigate two simple (yet canonical) statistical dataset models: (a) the \nbinary data model, and (b) the binary Gaussian mixture model. For both models, \nwe derive game-theoretically optimal minimax privacy mechanisms, and show that \nthe privacy mechanisms learned from data (in a generative adversarial fashion) \nmatch the theoretically optimal ones. This demonstrates that our framework can \nbe easily applied in practice, even in the absence of dataset statistics. \n</p>"}, "author": "Chong Huang, Peter Kairouz, Xiao Chen, Lalitha Sankar, Ram Rajagopal", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840982", "id": "tag:google.com,2005:reader/item/0000000340763ae9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning with Options that Terminate Off-Policy. (arXiv:1711.03817v2 [cs.AI] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03817"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03817", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A temporally abstract action, or an option, is specified by a policy and a \ntermination condition: the policy guides option behavior, and the termination \ncondition roughly determines its length. Generally, learning with longer \noptions (like learning with multi-step returns) is known to be more efficient. \nHowever, if the option set for the task is not ideal, and cannot express the \nprimitive optimal policy exactly, shorter options offer more flexibility and \ncan yield a better solution. Thus, the termination condition puts learning \nefficiency at odds with solution quality. We propose to resolve this dilemma by \ndecoupling the behavior and target terminations, just like it is done with \npolicies in off-policy learning. To this end, we give a new algorithm, \nQ(\\beta), that learns the solution with respect to any termination condition, \nregardless of how the options actually terminate. We derive Q(\\beta) by casting \nlearning with options into a common framework with well-studied multi-step \noff-policy learning. We validate our algorithm empirically, and show that it \nholds up to its motivating claims. \n</p>"}, "author": "Anna Harutyunyan, Peter Vrancx, Pierre-Luc Bacon, Doina Precup, Ann Nowe", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840981", "id": "tag:google.com,2005:reader/item/0000000340763af2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improvised Comedy as a Turing Test. (arXiv:1711.08819v2 [cs.AI] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08819"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08819", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The best improvisational theatre actors can make any scene partner, of any \nskill level or ability, appear talented and proficient in the art form, and \nthus \"make them shine\". To challenge this improvisational paradigm, we built an \nartificial intelligence (AI) trained to perform live shows alongside human \nactors for human audiences. Over the course of 30 performances to a combined \naudience of almost 3000 people, we have refined theatrical games which involve \ncombinations of human and (at times, adversarial) AI actors. We have developed \nspecific scene structures to include audience participants in interesting ways. \nFinally, we developed a complete show structure that submitted the audience to \na Turing test and observed their suspension of disbelief, which we believe is \nkey for human/non-human theatre co-creation. \n</p>"}, "author": "Kory Wallace Mathewson, Piotr Mirowski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840980", "id": "tag:google.com,2005:reader/item/0000000340763af9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Butterfly Effect: Bidirectional Control of Classification Performance by Small Additive Perturbation. (arXiv:1711.09681v2 [cs.LG] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.09681"}], "alternate": [{"href": "http://arxiv.org/abs/1711.09681", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250557a601\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250557a601&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper proposes a new algorithm for controlling classification results by \ngenerating a small additive perturbation without changing the classifier \nnetwork. Our work is inspired by existing works generating adversarial \nperturbation that worsens classification performance. In contrast to the \nexisting methods, our work aims to generate perturbations that can enhance \noverall classification performance. To solve this performance enhancement \nproblem, we newly propose a perturbation generation network (PGN) influenced by \nthe adversarial learning strategy. In our problem, the information in a large \nexternal dataset is summarized by a small additive perturbation, which helps to \nimprove the performance of the classifier trained with the target dataset. In \naddition to this performance enhancement problem, we show that the proposed PGN \ncan be adopted to solve the classical adversarial problem without utilizing the \ninformation on the target classifier. The mentioned characteristics of our \nmethod are verified through extensive experiments on publicly available visual \ndatasets. \n</p>"}, "author": "YoungJoon Yoo, Seonguk Park, Junyoung Choi, Sangdoo Yun, Nojun Kwak", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840979", "id": "tag:google.com,2005:reader/item/0000000340763b00", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "How linguistic descriptions of data can help to the teaching-learning process in higher education, case of study: artificial intelligence. (arXiv:1711.09744v2 [cs.AI] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.09744"}], "alternate": [{"href": "http://arxiv.org/abs/1711.09744", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325055e0c4a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325055e0c4a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Artificial Intelligence is a central topic in the computer science \ncurriculum. From the year 2011 a project-based learning methodology based on \ncomputer games has been designed and implemented into the intelligence \nartificial course at the University of the Bio-Bio. The project aims to develop \nsoftware-controlled agents (bots) which are programmed by using heuristic \nalgorithms seen during the course. This methodology allows us to obtain good \nlearning results, however several challenges have been founded during its \nimplementation. \n</p> \n<p>In this paper we show how linguistic descriptions of data can help to provide \nstudents and teachers with technical and personalized feedback about the \nlearned algorithms. Algorithm behavior profile and a new Turing test for \ncomputer games bots based on linguistic modelling of complex phenomena are also \nproposed in order to deal with such challenges. \n</p> \n<p>In order to show and explore the possibilities of this new technology, a web \nplatform has been designed and implemented by one of authors and its \nincorporation in the process of assessment allows us to improve the teaching \nlearning process. \n</p>"}, "author": "Clemente Rubio-Manzano, Tomas Lermanda Senoceain", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840977", "id": "tag:google.com,2005:reader/item/0000000340763b08", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning to Rank based on Analogical Reasoning. (arXiv:1711.10207v1 [stat.ML] CROSS LISTED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.10207"}], "alternate": [{"href": "http://arxiv.org/abs/1711.10207", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Object ranking or \"learning to rank\" is an important problem in the realm of \npreference learning. On the basis of training data in the form of a set of \nrankings of objects represented as feature vectors, the goal is to learn a \nranking function that predicts a linear order of any new set of objects. In \nthis paper, we propose a new approach to object ranking based on principles of \nanalogical reasoning. More specifically, our inference pattern is formalized in \nterms of so-called analogical proportions and can be summarized as follows: \nGiven objects $A,B,C,D$, if object $A$ is known to be preferred to $B$, and $C$ \nrelates to $D$ as $A$ relates to $B$, then $C$ is (supposedly) preferred to \n$D$. Our method applies this pattern as a main building block and combines it \nwith ideas and techniques from instance-based learning and rank aggregation. \nBased on first experimental results for data sets from various domains (sports, \neducation, tourism, etc.), we conclude that our approach is highly competitive. \nIt appears to be specifically interesting in situations in which the objects \nare coming from different subdomains, and which hence require a kind of \nknowledge transfer. \n</p>"}, "author": "Mohsen Ahmadi Fahandar, Eyke H&#xfc;llermeier", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840976", "id": "tag:google.com,2005:reader/item/0000000340763b11", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Subject Selection on a Riemannian Manifold for Unsupervised Cross-subject Seizure Detection. (arXiv:1712.00465v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00465"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00465", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Inter-subject variability between individuals poses a challenge in \ninter-subject brain signal analysis problems. A new algorithm for \nsubject-selection based on clustering covariance matrices on a Riemannian \nmanifold is proposed. After unsupervised selection of the subsets of relevant \nsubjects, data in a cluster is mapped to a tangent space at the mean point of \ncovariance matrices in that cluster and an SVM classifier on labeled data from \nrelevant subjects is trained. Experiment on an EEG seizure database shows that \nthe proposed method increases the accuracy over state-of-the-art from 86.83% to \n89.84% and specificity from 87.38% to 89.64% while reducing the false positive \nrate/hour from 0.8/hour to 0.77/hour. \n</p>"}, "author": "Samaneh Nasiri Ghosheh Bolagh, Gari. D. Clifford", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840975", "id": "tag:google.com,2005:reader/item/0000000340763b1f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Intelligent EHRs: Predicting Procedure Codes From Diagnosis Codes. (arXiv:1712.00481v1 [stat.ML])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00481"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00481", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In order to submit a claim to insurance companies, a doctor needs to code a \npatient encounter with both the diagnosis (ICDs) and procedures performed \n(CPTs) in an Electronic Health Record (EHR). Identifying and applying relevant \nprocedures code is a cumbersome and time-consuming task as a doctor has to \nchoose from around 13,000 procedure codes with no predefined one-to-one \nmapping. In this paper, we propose a state-of-the-art deep learning method for \nautomatic and intelligent coding of procedures (CPTs) from the diagnosis codes \n(ICDs) entered by the doctor. Precisely, we cast the learning problem as a \nmulti-label classification problem and use distributed representation to learn \nthe input mapping of high-dimensional sparse ICDs codes. Our final model \ntrained on 2.3 million claims is able to outperform existing rule-based \nprobabilistic and association-rule mining based methods and has a recall of \n90@3. \n</p>"}, "author": "Hasham Ul Haq, Rameel Ahmad, Sibt Ul Hussain", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840974", "id": "tag:google.com,2005:reader/item/0000000340763b29", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Prediction-Constrained Topic Models for Antidepressant Recommendation. (arXiv:1712.00499v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00499"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00499", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Supervisory signals can help topic models discover low-dimensional data \nrepresentations that are more interpretable for clinical tasks. We propose a \nframework for training supervised latent Dirichlet allocation that balances two \ngoals: faithful generative explanations of high-dimensional data and accurate \nprediction of associated class labels. Existing approaches fail to balance \nthese goals by not properly handling a fundamental asymmetry: the intended task \nis always predicting labels from data, not data from labels. Our new \nprediction-constrained objective trains models that predict labels from heldout \ndata well while also producing good generative likelihoods and interpretable \ntopic-word parameters. In a case study on predicting depression medications \nfrom electronic health records, we demonstrate improved recommendations \ncompared to previous supervised topic models and high- dimensional logistic \nregression from words alone. \n</p>"}, "author": "Michael C. Hughes, Gabriel Hope, Leah Weiner, Thomas H. McCoy, Roy H. Perlis, Erik B. Sudderth, Finale Doshi-Velez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840973", "id": "tag:google.com,2005:reader/item/0000000340763b2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Neural Stochastic Volatility Model. (arXiv:1712.00504v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00504"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00504", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we show that the recent integration of statistical models with \ndeep recurrent neural networks provides a new way of formulating volatility \n(the degree of variation of time series) models that have been widely used in \ntime series analysis and prediction in finance. The model comprises a pair of \ncomplementary stochastic recurrent neural networks: the generative network \nmodels the joint distribution of the stochastic volatility process; the \ninference network approximates the conditional distribution of the latent \nvariables given the observables. Our focus here is on the formulation of \ntemporal dynamics of volatility over time under a stochastic recurrent neural \nnetwork framework. Experiments on real-world stock price datasets demonstrate \nthat the proposed model generates a better volatility estimation and prediction \nthat outperforms stronge baseline methods, including the deterministic models, \nsuch as GARCH and its variants, and the stochastic MCMC-based models, and the \nGaussian-process-based, on the average negative log-likelihood measure. \n</p>"}, "author": "Rui Luo, Weinan Zhang, Xiaojun Xu, Jun Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840972", "id": "tag:google.com,2005:reader/item/0000000340763b38", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian Semi-nonnegative Tri-matrix Factorization to Identify Pathways Associated with Cancer Types. (arXiv:1712.00520v1 [stat.ML])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00520"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00520", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Identifying altered pathways that are associated with specific cancer types \ncan potentially bring a significant impact on cancer patient treatment. \nAccurate identification of such key altered pathways information can be used to \ndevelop novel therapeutic agents as well as to understand the molecular \nmechanisms of various types of cancers better. Tri-matrix factorization is an \nefficient tool to learn associations between two different entities (e.g., \ncancer types and pathways in our case) from data. To successfully apply \ntri-matrix factorization methods to biomedical problems, biological prior \nknowledge such as pathway databases or protein-protein interaction (PPI) \nnetworks, should be taken into account in the factorization model. However, it \nis not straightforward in the Bayesian setting even though Bayesian methods are \nmore appealing than point estimate methods, such as a maximum likelihood or a \nmaximum posterior method, in the sense that they calculate distributions over \nvariables and are robust against overfitting. We propose a Bayesian \n(semi-)nonnegative matrix factorization model for human cancer genomic data, \nwhere the biological prior knowledge represented by a pathway database and a \nPPI network is taken into account in the factorization model through a finite \ndependent Beta-Bernoulli prior. We tested our method on The Cancer Genome Atlas \n(TCGA) dataset and found that the pathways identified by our method can be used \nas a prognostic biomarkers for patient subgroup identification. \n</p>"}, "author": "Sunho Park, Tae Hyun Hwang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840971", "id": "tag:google.com,2005:reader/item/0000000340763b41", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Survival-Supervised Topic Modeling with Anchor Words: Characterizing Pancreatitis Outcomes. (arXiv:1712.00535v1 [stat.ML])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00535"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00535", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce a new approach for topic modeling that is supervised by survival \nanalysis. Specifically, we build on recent work on unsupervised topic modeling \nwith so-called anchor words by providing supervision through an elastic-net \nregularized Cox proportional hazards model. In short, an anchor word being \npresent in a document provides strong indication that the document is partially \nabout a specific topic. For example, by seeing \"gallstones\" in a document, we \nare fairly certain that the document is partially about medicine. Our proposed \nmethod alternates between learning a topic model and learning a survival model \nto find a local minimum of a block convex optimization problem. We apply our \nproposed approach to predicting how long patients with pancreatitis admitted to \nan intensive care unit (ICU) will stay in the ICU. Our approach is as accurate \nas the best of a variety of baselines while being more interpretable than any \nof the baselines. \n</p>"}, "author": "George H. Chen, Jeremy C. Weiss", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840970", "id": "tag:google.com,2005:reader/item/0000000340763b53", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A global feature extraction model for the effective computer aided diagnosis of mild cognitive impairment using structural MRI images. (arXiv:1712.00556v1 [stat.ML])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00556"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00556", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Multiple modalities of biomarkers have been proved to be very sensitive in \nassessing the progression of Alzheimer's disease (AD), and using these \nmodalities and machine learning algorithms, several approaches have been \nproposed to assist in the early diagnosis of AD. Among the recent investigated \nstate-of-the-art approaches, Gaussian discriminant analysis (GDA)-based \napproaches have been demonstrated to be more effective and accurate in the \nclassification of AD, especially for delineating its prodromal stage of mild \ncognitive impairment (MCI). Moreover, among those binary classification \ninvestigations, the local feature extraction methods were mostly used, which \nmade them hardly be applied to a practical computer aided diagnosis system. \nTherefore, this study presents a novel global feature extraction model taking \nadvantage of the recent proposed GDA-based dual high-dimensional decision \nspaces, which can significantly improve the early diagnosis performance \ncomparing to those local feature extraction methods. In the true test using 20% \nheld-out data, for discriminating the most challenging MCI group from the \ncognitively normal control (CN) group, an F1 score of 91.06%, an accuracy of \n88.78%, a sensitivity of 91.80%, and a specificity of 83.78% were achieved that \ncan be considered as the best performance obtained so far. \n</p>"}, "author": "Chen Fang, Panuwat Janwattanapong, Chunfei Li, Malek Adjouadi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840969", "id": "tag:google.com,2005:reader/item/0000000340763b5b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Where Classification Fails, Interpretation Rises. (arXiv:1712.00558v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00558"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00558", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325055e0ffa\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325055e0ffa&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>An intriguing property of deep neural networks is their inherent \nvulnerability to adversarial inputs, which significantly hinders their \napplication in security-critical domains. Most existing detection methods \nattempt to use carefully engineered patterns to distinguish adversarial inputs \nfrom their genuine counterparts, which however can often be circumvented by \nadaptive adversaries. In this work, we take a completely different route by \nleveraging the definition of adversarial inputs: while deceiving for deep \nneural networks, they are barely discernible for human visions. Building upon \nrecent advances in interpretable models, we construct a new detection framework \nthat contrasts an input's interpretation against its classification. We \nvalidate the efficacy of this framework through extensive experiments using \nbenchmark datasets and attacks. We believe that this work opens a new direction \nfor designing adversarial input detection methods. \n</p>"}, "author": "Chanh Nguyen, Georgi Georgiev, Yujie Ji, Ting Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840968", "id": "tag:google.com,2005:reader/item/0000000340763b64", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Progressive Neural Architecture Search. (arXiv:1712.00559v1 [cs.CV])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00559"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00559", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a method for learning CNN structures that is more efficient than \nprevious approaches: instead of using reinforcement learning (RL) or genetic \nalgorithms (GA), we use a sequential model-based optimization (SMBO) strategy, \nin which we search for architectures in order of increasing complexity, while \nsimultaneously learning a surrogate function to guide the search, similar to A* \nsearch. On the CIFAR-10 dataset, our method finds a CNN structure with the same \nclassification accuracy (3.41% error rate) as the RL method of Zoph et al. \n(2017), but 2 times faster (in terms of number of models evaluated). It also \noutperforms the GA method of Liu et al. (2017), which finds a model with worse \nperformance (3.63% error rate), and takes 5 times longer. Finally we show that \nthe model we learned on CIFAR also works well at the task of ImageNet \nclassification. In particular, we match the state-of-the-art performance of \n82.9% top-1 and 96.1% top-5 accuracy. \n</p>"}, "author": "Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, Kevin Murphy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840967", "id": "tag:google.com,2005:reader/item/0000000340763b7d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Anesthesiologist-level forecasting of hypoxemia with only SpO2 data using deep learning. (arXiv:1712.00563v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00563"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00563", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We use a deep learning model trained only on a patient's blood oxygenation \ndata (measurable with an inexpensive fingertip sensor) to predict impending \nhypoxemia (low blood oxygen) more accurately than trained anesthesiologists \nwith access to all the data recorded in a modern operating room. We also \nprovide a simple way to visualize the reason why a patient's risk is low or \nhigh by assigning weight to the patient's past blood oxygen values. This work \nhas the potential to provide cutting-edge clinical decision support in \nlow-resource settings, where rates of surgical complication and death are \nsubstantially greater than in high-resource areas. \n</p>"}, "author": "Gabriel Erion, Hugh Chen, Scott M. Lundberg, Su-In Lee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840966", "id": "tag:google.com,2005:reader/item/0000000340763b8c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Supervised Hashing based on Energy Minimization. (arXiv:1712.00573v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00573"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00573", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recently, supervised hashing methods have attracted much attention since they \ncan optimize retrieval speed and storage cost while preserving semantic \ninformation. Because hashing codes learning is NP-hard, many methods resort to \nsome form of relaxation technique. But the performance of these methods can \neasily deteriorate due to the relaxation. Luckily, many supervised hashing \nformulations can be viewed as energy functions, hence solving hashing codes is \nequivalent to learning marginals in the corresponding conditional random field \n(CRF). By minimizing the KL divergence between a fully factorized distribution \nand the Gibbs distribution of this CRF, a set of consistency equations can be \nobtained, but updating them in parallel may not yield a local optimum since the \nvariational lower bound is not guaranteed to increase. In this paper, we use a \nlinear approximation of the sigmoid function to convert these consistency \nequations to linear systems, which have a closed-form solution. By applying \nthis novel technique to two classical hashing formulations KSH and SPLH, we \nobtain two new methods called EM (energy minimizing based)-KSH and EM-SPLH. \nExperimental results on three datasets show the superiority of our methods. \n</p>"}, "author": "Zihao Hu, Xiyi Luo, Hongtao Lu, Yong Yu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840965", "id": "tag:google.com,2005:reader/item/0000000340763b96", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Short-term Mortality Prediction for Elderly Patients Using Medicare Claims Data. (arXiv:1712.00644v1 [stat.ML])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00644"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00644", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Risk prediction is central to both clinical medicine and public health. While \nmany machine learning models have been developed to predict mortality, they are \nrarely applied in the clinical literature, where classification tasks typically \nrely on logistic regression. One reason for this is that existing machine \nlearning models often seek to optimize predictions by incorporating features \nthat are not present in the databases readily available to providers and policy \nmakers, limiting generalizability and implementation. Here we tested a number \nof machine learning classifiers for prediction of six-month mortality in a \npopulation of elderly Medicare beneficiaries, using an administrative claims \ndatabase of the kind available to the majority of health care payers and \nproviders. We show that machine learning classifiers substantially outperform \ncurrent widely-used methods of risk prediction but only when used with an \nimproved feature set incorporating insights from clinical medicine, developed \nfor this study. Our work has applications to supporting patient and provider \ndecision making at the end of life, as well as population health-oriented \nefforts to identify patients at high risk of poor outcomes. \n</p>"}, "author": "Maggie Makar, Marzyeh Ghassemi, David Cutler, Ziad Obermeyer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840964", "id": "tag:google.com,2005:reader/item/0000000340763b9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards Robust Neural Networks via Random Self-ensemble. (arXiv:1712.00673v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00673"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00673", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent studies have revealed the vulnerability of deep neural networks - A \nsmall adversarial perturbation that is imperceptible to human can easily make a \nwell-trained deep neural network mis-classify. This makes it unsafe to apply \nneural networks in security-critical applications. In this paper, we propose a \nnew defensive algorithm called Random Self-Ensemble (RSE) by combining two \nimportant concepts: ${\\bf randomness}$ and ${\\bf ensemble}$. To protect a \ntargeted model, RSE adds random noise layers to the neural network to prevent \nfrom state-of-the-art gradient-based attacks, and ensembles the prediction over \nrandom noises to stabilize the performance. We show that our algorithm is \nequivalent to ensemble an infinite number of noisy models $f_\\epsilon$ without \nany additional memory overhead, and the proposed training procedure based on \nnoisy stochastic gradient descent can ensure the ensemble model has good \npredictive capability. Our algorithm significantly outperforms previous defense \ntechniques on real datasets. For instance, on CIFAR-10 with VGG network (which \nhas $92\\%$ accuracy without any attack), under the state-of-the-art C&amp;W attack \nwithin a certain distortion tolerance, the accuracy of unprotected model drops \nto less than $10\\%$, the best previous defense technique has $48\\%$ accuracy, \nwhile our method still has $86\\%$ prediction accuracy under the same level of \nattack. Finally, our method is simple and easy to integrate into any neural \nnetwork. \n</p>"}, "author": "Xuanqing Liu, Minhao Cheng, Huan Zhang, Cho-Jui Hsieh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840963", "id": "tag:google.com,2005:reader/item/0000000340763baa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "GANGs: Generative Adversarial Network Games. (arXiv:1712.00679v1 [stat.ML])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00679"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00679", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative Adversarial Networks (GAN) have become one of the most successful \nframeworks for unsupervised generative modeling. As GANs are difficult to train \nmuch research has focused on this. However, very little of this research has \ndirectly exploited game-theoretic techniques. We introduce Generative \nAdversarial Network Games (GANGs), which explicitly model a finite zero-sum \ngame between a generator ($G$) and classifier ($C$) that use mixed strategies. \nThe size of these games precludes exact solution methods, therefore we define \nresource-bounded best responses (RBBRs), and a resource-bounded Nash \nEquilibrium (RB-NE) as a pair of mixed strategies such that neither $G$ or $C$ \ncan find a better RBBR. The RB-NE solution concept is richer than the notion of \n`local Nash equilibria' in that it captures not only failures of escaping local \noptima of gradient descent, but applies to any approximate best response \ncomputations, including methods with random restarts. To validate our approach, \nwe solve GANGs with the Parallel Nash Memory algorithm, which provably \nmonotonically converges to an RB-NE. We compare our results to standard GAN \nsetups, and demonstrate that our method deals well with typical GAN problems \nsuch as mode collapse, partial mode coverage and forgetting. \n</p>"}, "author": "Frans A. Oliehoek, Rahul Savani, Jose Gallego-Posada, Elise van der Pol, Edwin D. de Jong, Roderich Gross", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840962", "id": "tag:google.com,2005:reader/item/0000000340763bc5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improving Network Robustness against Adversarial Attacks with Compact Convolution. (arXiv:1712.00699v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00699"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00699", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Though Convolutional Neural Networks (CNNs) have surpassed human-level \nperformance on tasks such as object classification and face verification, they \ncan easily be fooled by adversarial attacks. These attacks add a small \nperturbation to the input image that causes the network to mis-classify the \nsample. In this paper, we focus on neutralizing adversarial attacks by \nexploring the effect of different loss functions such as CenterLoss and \nL2-Softmax Loss for enhanced robustness to adversarial perturbations. \nAdditionally, we propose power convolution, a novel method of convolution that \nwhen incorporated in conventional CNNs improve their robustness. Power \nconvolution ensures that features at every layer are bounded and close to each \nother. Extensive experiments show that Power Convolutional Networks (PCNs) \nneutralize multiple types of attacks, and perform better than existing methods \nfor defending adversarial attacks. \n</p>"}, "author": "Rajeev Ranjan, Swami Sankaranarayanan, Carlos D. Castillo, Rama Chellappa", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840961", "id": "tag:google.com,2005:reader/item/0000000340763bd3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convolutional Phase Retrieval via Gradient Descent. (arXiv:1712.00716v1 [stat.CO])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00716"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00716", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the convolutional phase retrieval problem, which considers \nrecovering an unknown signal $\\mathbf x \\in \\mathbb C^n $ from $m$ measurements \nconsisting of the magnitude of its cyclic convolution with a known kernel \n$\\mathbf a \\in \\mathbb C^m $. This model is motivated by applications such as \nchannel estimation, optics, and underwater acoustic communication, where the \nsignal of interest is acted on by a given channel/filter, and phase information \nis difficult or impossible to acquire. We show that when $\\mathbf a$ is random \nand the sample number $m$ is sufficiently large, with high probability $\\mathbf \nx$ can be efficiently recovered up to a global phase using a combination of \nspectral initialization and generalized gradient descent. The main challenge is \ncoping with dependencies in the measurement operator. We overcome this \nchallenge by using ideas from decoupling theory, suprema of chaos processes and \nthe restricted isometry property of random circulant matrices, and recent \nanalysis for alternating minimization methods. \n</p>"}, "author": "Qing Qu, Yuqian Zhang, Yonina C. Eldar, John Wright", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840960", "id": "tag:google.com,2005:reader/item/0000000340763bd8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Joint Topic-Semantic-aware Social Recommendation for Online Voting. (arXiv:1712.00731v1 [stat.ML])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00731"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00731", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Online voting is an emerging feature in social networks, in which users can \nexpress their attitudes toward various issues and show their unique interest. \nOnline voting imposes new challenges on recommendation, because the propagation \nof votings heavily depends on the structure of social networks as well as the \ncontent of votings. In this paper, we investigate how to utilize these two \nfactors in a comprehensive manner when doing voting recommendation. First, due \nto the fact that existing text mining methods such as topic model and semantic \nmodel cannot well process the content of votings that is typically short and \nambiguous, we propose a novel Topic-Enhanced Word Embedding (TEWE) method to \nlearn word and document representation by jointly considering their topics and \nsemantics. Then we propose our Joint Topic-Semantic-aware social Matrix \nFactorization (JTS-MF) model for voting recommendation. JTS-MF model calculates \nsimilarity among users and votings by combining their TEWE representation and \nstructural information of social networks, and preserves this \ntopic-semantic-social similarity during matrix factorization. To evaluate the \nperformance of TEWE representation and JTS-MF model, we conduct extensive \nexperiments on real online voting dataset. The results prove the efficacy of \nour approach against several state-of-the-art baselines. \n</p>"}, "author": "Hongwei Wang, Jia Wang, Miao Zhao, Jiannong Cao, Minyi Guo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840959", "id": "tag:google.com,2005:reader/item/0000000340763be1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SHINE: Signed Heterogeneous Information Network Embedding for Sentiment Link Prediction. (arXiv:1712.00732v1 [stat.ML])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00732"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00732", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325055e138f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325055e138f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In online social networks people often express attitudes towards others, \nwhich forms massive sentiment links among users. Predicting the sign of \nsentiment links is a fundamental task in many areas such as personal \nadvertising and public opinion analysis. Previous works mainly focus on textual \nsentiment classification, however, text information can only disclose the \"tip \nof the iceberg\" about users' true opinions, of which the most are unobserved \nbut implied by other sources of information such as social relation and users' \nprofile. To address this problem, in this paper we investigate how to predict \npossibly existing sentiment links in the presence of heterogeneous information. \nFirst, due to the lack of explicit sentiment links in mainstream social \nnetworks, we establish a labeled heterogeneous sentiment dataset which consists \nof users' sentiment relation, social relation and profile knowledge by \nentity-level sentiment extraction method. Then we propose a novel and flexible \nend-to-end Signed Heterogeneous Information Network Embedding (SHINE) framework \nto extract users' latent representations from heterogeneous networks and \npredict the sign of unobserved sentiment links. SHINE utilizes multiple deep \nautoencoders to map each user into a low-dimension feature space while \npreserving the network structure. We demonstrate the superiority of SHINE over \nstate-of-the-art baselines on link prediction and node recommendation in two \nreal-world datasets. The experimental results also prove the efficacy of SHINE \nin cold start scenario. \n</p>"}, "author": "Hongwei Wang, Fuzheng Zhang, Min Hou, Xing Xie, Minyi Guo, Qi Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840958", "id": "tag:google.com,2005:reader/item/0000000340763bf6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Tensor Train Neighborhood Preserving Embedding. (arXiv:1712.00828v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00828"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00828", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32505652eea\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32505652eea&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we propose a Tensor Train Neighborhood Preserving Embedding \n(TTNPE) to embed multi-dimensional tensor data into low dimensional tensor \nsubspace. Novel approaches to solve the optimization problem in TTNPE are \nproposed. For this embedding, we evaluate novel trade-off gain among \nclassification, computation, and dimensionality reduction (storage) for \nsupervised learning. It is shown that compared to the state-of-the-arts tensor \nembedding methods, TTNPE achieves superior trade-off in classification, \ncomputation, and dimensionality reduction in MNIST handwritten digits and \nWeizmann face datasets. \n</p>"}, "author": "Wenqi Wang, Vaneet Aggarwal, Shuchin Aeron", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840957", "id": "tag:google.com,2005:reader/item/0000000340763c11", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Gaussian Process Regression for Arctic Coastal Erosion Forecasting. (arXiv:1712.00867v1 [physics.geo-ph])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00867"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00867", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Arctic coastal morphology is governed by multiple factors, many of which are \naffected by climatological changes. As the season length for shorefast ice \ndecreases and temperatures warm permafrost soils, coastlines are more \nsusceptible to erosion from storm waves. Such coastal erosion is a concern, \nsince the majority of the population centers and infrastructure in the Arctic \nare located near the coasts. Stakeholders and decision makers increasingly need \nmodels capable of scenario-based predictions to assess and mitigate the effects \nof coastal morphology on infrastructure and land use. Our research uses \nGaussian process models to forecast Arctic coastal erosion along the Beaufort \nSea near Drew Point, AK. Gaussian process regression is a data-driven modeling \nmethodology capable of extracting patterns and trends from data-sparse \nenvironments such as remote Arctic coastlines. To train our model, we use \nannual coastline positions and near-shore summer temperature averages from \nexisting datasets and extend these data by extracting additional coastlines \nfrom satellite imagery. We combine our calibrated models with future climate \nmodels to generate a range of plausible future erosion scenarios. Our results \nshow that the Gaussian process methodology substantially improves yearly \npredictions compared to linear and nonlinear least squares methods, and is \ncapable of generating detailed forecasts suitable for use by decision makers. \n</p>"}, "author": "Matthew Kupilik, Frank Witmer, Euan-Angus MacLeod, Caixia Wang, Tom Ravens", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840956", "id": "tag:google.com,2005:reader/item/0000000340763c29", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Data Dropout in Arbitrary Basis for Deep Network Regularization. (arXiv:1712.00891v2 [cs.CV] UPDATED)", "published": 1512537802, "updated": 1512537806, "canonical": [{"href": "http://arxiv.org/abs/1712.00891"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00891", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>An important problem in training deep networks with high capacity is to \nensure that the trained network works well when presented with new inputs \noutside the training dataset. Dropout is an effective regularization technique \nto boost the network generalization in which a random subset of the elements of \nthe given data and the extracted features are set to zero during the training \nprocess. In this paper, a new randomized regularization technique in which we \nwithhold a random part of the data without necessarily turning off the \nneurons/data-elements is proposed. In the proposed method, of which the \nconventional dropout is shown to be a special case, random data dropout is \nperformed in an arbitrary basis, hence the designation Generalized Dropout. We \nalso present a framework whereby the proposed technique can be applied \nefficiently to convolutional neural networks. The presented numerical \nexperiments demonstrate that the proposed technique yields notable performance \ngain. Generalized Dropout provides new insight into the idea of dropout, shows \nthat we can achieve different performance gains by using different bases \nmatrices, and opens up a new research question as of how to choose optimal \nbases matrices that achieve maximal performance gain. \n</p>"}, "author": "Mostafa Rahmani, George Atia", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840955", "id": "tag:google.com,2005:reader/item/0000000340763c3f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Independent Causal Mechanisms. (arXiv:1712.00961v2 [cs.LG] UPDATED)", "published": 1513098583, "updated": 1513098583, "canonical": [{"href": "http://arxiv.org/abs/1712.00961"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00961", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Independent causal mechanisms are a central concept in the study of causality \nwith implications for machine learning tasks. In this work we develop an \nalgorithm to recover a set of (inverse) independent mechanisms relating a \ndistribution transformed by the mechanisms to a reference distribution. The \napproach is fully unsupervised and based on a set of experts that compete for \ndata to specialize and extract the mechanisms. We test and analyze the proposed \nmethod on a series of experiments based on image transformations. Each expert \nsuccessfully maps a subset of the transformed data to the original domain, and \nthe learned mechanisms generalize to other domains. We discuss implications for \ndomain transfer and links to recent trends in generative modeling. \n</p>"}, "author": "Giambattista Parascandolo, Mateo Rojas-Carulla, Niki Kilbertus, Bernhard Sch&#xf6;lkopf", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840954", "id": "tag:google.com,2005:reader/item/0000000340763c4b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning to detect chest radiographs containing lung nodules using visual attention networks. (arXiv:1712.00996v1 [stat.ML])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00996"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00996", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine learning approaches hold great potential for the automated detection \nof lung nodules in chest radiographs, but training the algorithms requires vary \nlarge amounts of manually annotated images, which are difficult to obtain. Weak \nlabels indicating whether a radiograph is likely to contain pulmonary nodules \nare typically easier to obtain at scale by parsing historical free-text \nradiological reports associated to the radiographs. Using a repositotory of \nover 700,000 chest radiographs, in this study we demonstrate that promising \nnodule detection performance can be achieved using weak labels through \nconvolutional neural networks for radiograph classification. We propose two \nnetwork architectures for the classification of images likely to contain \npulmonary nodules using both weak labels and manually-delineated bounding \nboxes, when these are available. Annotated nodules are used at training time to \ndeliver a visual attention mechanism informing the model about its localisation \nperformance. The first architecture extracts saliency maps from high-level \nconvolutional layers and compares the estimated position of a nodule against \nthe ground truth, when this is available. A corresponding localisation error is \nthen back-propagated along with the softmax classification error. The second \napproach consists of a recurrent attention model that learns to observe a short \nsequence of smaller image portions through reinforcement learning. When a \nnodule annotation is available at training time, the reward function is \nmodified accordingly so that exploring portions of the radiographs away from a \nnodule incurs a larger penalty. Our empirical results demonstrate the potential \nadvantages of these architectures in comparison to competing methodologies. \n</p>"}, "author": "Emanuele Pesce, Petros-Pavlos Ypsilantis, Samuel Withey, Robert Bakewell, Vicky Goh, Giovanni Montana", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840953", "id": "tag:google.com,2005:reader/item/0000000340763c74", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "NEON+: Accelerated Gradient Methods for Extracting Negative Curvature for Non-Convex Optimization. (arXiv:1712.01033v1 [math.OC])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01033"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01033", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Accelerated gradient (AG) methods are breakthroughs in convex optimization, \nimproving the convergence rate of the gradient descent method for optimization \nwith smooth functions. However, the analysis of AG methods for non-convex \noptimization is still limited. It remains an open question whether AG methods \nfrom convex optimization can accelerate the convergence of the gradient descent \nmethod for finding local minimum of non-convex optimization problems. This \npaper provides an affirmative answer to this question. In particular, we \nanalyze two renowned variants of AG methods (namely Polyak's Heavy Ball method \nand Nesterov's Accelerated Gradient method) for extracting the negative \ncurvature from random noise, which is central to escaping from saddle points. \nBy leveraging the proposed AG methods for extracting the negative curvature, we \npresent a new AG algorithm with double loops for non-convex \noptimization~\\footnote{this is in contrast to a single-loop AG algorithm \nproposed in a recent manuscript~\\citep{AGNON}, which directly analyzed the \nNesterov's AG method for non-convex optimization and appeared online on \nNovember 29, 2017. However, we emphasize that our work is an independent work, \nwhich is inspired by our earlier work~\\citep{NEON17} and is based on a \ndifferent novel analysis.}, which converges to second-order stationary point \n$\\x$ such that $\\|\\nabla f(\\x)\\|\\leq \\epsilon$ and $\\nabla^2 f(\\x)\\geq \n-\\sqrt{\\epsilon} I$ with $\\widetilde O(1/\\epsilon^{1.75})$ iteration \ncomplexity, improving that of gradient descent method by a factor of \n$\\epsilon^{-0.25}$ and matching the best iteration complexity of second-order \nHessian-free methods for non-convex optimization. \n</p>"}, "author": "Yi Xu, Rong Jin, Tianbao Yang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840952", "id": "tag:google.com,2005:reader/item/0000000340763c94", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Vprop: Variational Inference using RMSprop. (arXiv:1712.01038v1 [stat.ML])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01038"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01038", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many computationally-efficient methods for Bayesian deep learning rely on \ncontinuous optimization algorithms, but the implementation of these methods \nrequires significant changes to existing code-bases. In this paper, we propose \nVprop, a method for Gaussian variational inference that can be implemented with \ntwo minor changes to the off-the-shelf RMSprop optimizer. Vprop also reduces \nthe memory requirements of Black-Box Variational Inference by half. We derive \nVprop using the conjugate-computation variational inference method, and \nestablish its connections to Newton's method, natural-gradient methods, and \nextended Kalman filters. Overall, this paper presents Vprop as a principled, \ncomputationally-efficient, and easy-to-implement method for Bayesian deep \nlearning. \n</p>"}, "author": "Mohammad Emtiyaz Khan, Zuozhu Liu, Voot Tangkaratt, Yarin Gal", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840951", "id": "tag:google.com,2005:reader/item/0000000340763ca9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Adaptive Quantization for Deep Neural Network. (arXiv:1712.01048v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01048"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01048", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In recent years Deep Neural Networks (DNNs) have been rapidly developed in \nvarious applications, together with increasingly complex architectures. The \nperformance gain of these DNNs generally comes with high computational costs \nand large memory consumption, which may not be affordable for mobile platforms. \nDeep model quantization can be used for reducing the computation and memory \ncosts of DNNs, and deploying complex DNNs on mobile equipment. In this work, we \npropose an optimization framework for deep model quantization. First, we \npropose a measurement to estimate the effect of parameter quantization errors \nin individual layers on the overall model prediction accuracy. Then, we propose \nan optimization process based on this measurement for finding optimal \nquantization bit-width for each layer. This is the first work that \ntheoretically analyse the relationship between parameter quantization errors of \nindividual layers and model accuracy. Our new quantization algorithm \noutperforms previous quantization optimization methods, and achieves 20-40% \nhigher compression rate compared to equal bit-width quantization at the same \nmodel prediction accuracy. \n</p>"}, "author": "Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, Pascal Frossard", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840950", "id": "tag:google.com,2005:reader/item/0000000340763cbe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Determinants of Mobile Money Adoption in Pakistan. (arXiv:1712.01081v1 [stat.ML])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01081"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01081", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work, we analyze the problem of adoption of mobile money in Pakistan \nby using the call detail records of a major telecom company as our input. Our \nresults highlight the fact that different sections of the society have \ndifferent patterns of adoption of digital financial services but user mobility \nrelated features are the most important one when it comes to adopting and using \nmobile money services. \n</p>"}, "author": "Muhammad Raza Khan, Joshua Blumenstock", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840949", "id": "tag:google.com,2005:reader/item/0000000340763cd5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Inferring agent objectives at different scales of a complex adaptive system. (arXiv:1712.01137v1 [q-fin.TR])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01137"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01137", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32505653212\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32505653212&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We introduce a framework to study the effective objectives at different time \nscales of financial market microstructure. The financial market can be regarded \nas a complex adaptive system, where purposeful agents collectively and \nsimultaneously create and perceive their environment as they interact with it. \nIt has been suggested that multiple agent classes operate in this system, with \na non-trivial hierarchy of top-down and bottom-up causation classes with \ndifferent effective models governing each level. We conjecture that agent \nclasses may in fact operate at different time scales and thus act differently \nin response to the same perceived market state. Given scale-specific temporal \nstate trajectories and action sequences estimated from aggregate market \nbehaviour, we use Inverse Reinforcement Learning to compute the effective \nreward function for the aggregate agent class at each scale, allowing us to \nassess the relative attractiveness of feature vectors across different scales. \nDifferences in reward functions for feature vectors may indicate different \nobjectives of market participants, which could assist in finding the scale \nboundary for agent classes. This has implications for learning algorithms \noperating in this domain. \n</p>"}, "author": "Dieter Hendricks, Adam Cobb, Richard Everett, Jonathan Downing, Stephen J. Roberts", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840948", "id": "tag:google.com,2005:reader/item/0000000340763cdf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Maximum Likelihood Optimization via Hypernetworks. (arXiv:1712.01141v1 [stat.ML])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01141"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01141", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This work explores maximum likelihood optimization of neural networks through \nhypernetworks. A hypernetwork initializes the weights of another network, which \nin turn can be employed for typical functional tasks such as regression and \nclassification. We optimize hypernetworks to directly maximize the conditional \nlikelihood of target variables given input. Using this approach we obtain \ncompetitive empirical results on regression and classification benchmarks. \n</p>"}, "author": "Abdul-Saboor Sheikh, Kashif Rasul, Andreas Merentitis, Urs Bergmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840947", "id": "tag:google.com,2005:reader/item/0000000340763cf6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Statistical Inference for Incomplete Ranking Data: The Case of Rank-Dependent Coarsening. (arXiv:1712.01158v1 [stat.ML])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01158"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01158", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of statistical inference for ranking data, \nspecifically rank aggregation, under the assumption that samples are incomplete \nin the sense of not comprising all choice alternatives. In contrast to most \nexisting methods, we explicitly model the process of turning a full ranking \ninto an incomplete one, which we call the coarsening process. To this end, we \npropose the concept of rank-dependent coarsening, which assumes that incomplete \nrankings are produced by projecting a full ranking to a random subset of ranks. \nFor a concrete instantiation of our model, in which full rankings are drawn \nfrom a Plackett-Luce distribution and observations take the form of pairwise \npreferences, we study the performance of various rank aggregation methods. In \naddition to predictive accuracy in the finite sample setting, we address the \ntheoretical question of consistency, by which we mean the ability to recover a \ntarget ranking when the sample size goes to infinity, despite a potential bias \nin the observations caused by the (unknown) coarsening. \n</p>"}, "author": "Mohsen Ahmadi Fahandar, Eyke H&#xfc;llermeier, In&#xe9;s Couso", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840946", "id": "tag:google.com,2005:reader/item/0000000340763cfd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Episodic memory for continual model learning. (arXiv:1712.01169v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01169"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01169", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Both the human brain and artificial learning agents operating in real-world \nor comparably complex environments are faced with the challenge of online model \nselection. In principle this challenge can be overcome: hierarchical Bayesian \ninference provides a principled method for model selection and it converges on \nthe same posterior for both off-line (i.e. batch) and online learning. However, \nmaintaining a parameter posterior for each model in parallel has in general an \neven higher memory cost than storing the entire data set and is consequently \nclearly unfeasible. Alternatively, maintaining only a limited set of models in \nmemory could limit memory requirements. However, sufficient statistics for one \nmodel will usually be insufficient for fitting a different kind of model, \nmeaning that the agent loses information with each model change. We propose \nthat episodic memory can circumvent the challenge of limited memory-capacity \nonline model selection by retaining a selected subset of data points. We design \na method to compute the quantities necessary for model selection even when the \ndata is discarded and only statistics of one (or few) learnt models are \navailable. We demonstrate on a simple model that a limited-sized episodic \nmemory buffer, when the content is optimised to retain data with statistics not \nmatching the current representation, can resolve the fundamental challenge of \nonline model selection. \n</p>"}, "author": "David G. Nagy, Gerg&#x151; Orb&#xe1;n", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840945", "id": "tag:google.com,2005:reader/item/0000000340763d19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Dual Framework for Low-rank Tensor Completion. (arXiv:1712.01193v1 [cs.LG])", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.01193"}], "alternate": [{"href": "http://arxiv.org/abs/1712.01193", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a novel formulation of the low-rank tensor completion problem that \nis based on the duality theory and a particular choice of low-rank regularizer. \nThis low-rank regularizer along with the dual perspective provides a simple \ncharacterization of the solution to the tensor completion problem. Motivated by \nlarge-scale setting, we next derive a rank-constrained reformulation of the \nproposed optimization problem, which is shown to lie on the Riemannian \nspectrahedron manifold. We exploit the versatile Riemannian optimization \nframework to develop computationally efficient conjugate gradient and \ntrust-region algorithms. The experiments confirm the benefits of our choice of \nregularization and the proposed algorithms outperform state-of-the-art \nalgorithms on several real-world data sets in different applications. \n</p>"}, "author": "Madhav Nimishakavi, Pratik Jawanpuria, Bamdev Mishra", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840944", "id": "tag:google.com,2005:reader/item/0000000340763d27", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Distributional Equivalence and Structure Learning for Bow-free Acyclic Path Diagrams. (arXiv:1508.01717v4 [stat.ML] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1508.01717"}], "alternate": [{"href": "http://arxiv.org/abs/1508.01717", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of structure learning for bow-free acyclic path \ndiagrams (BAPs). BAPs can be viewed as a generalization of linear Gaussian DAG \nmodels that allow for certain hidden variables. We present a first method for \nthis problem using a greedy score-based search algorithm. We also prove some \nnecessary and some sufficient conditions for distributional equivalence of BAPs \nwhich are used in an algorithmic ap- proach to compute (nearly) equivalent \nmodel structures. This allows us to infer lower bounds of causal effects. We \nalso present applications to real and simulated datasets using our publicly \navailable R-package. \n</p>"}, "author": "Christopher Nowzohour, Marloes H. Maathuis, Robin J. Evans, Peter B&#xfc;hlmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840943", "id": "tag:google.com,2005:reader/item/0000000340763d3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sparse Recovery Guarantees from Extreme Eigenvalues Small Deviations. (arXiv:1604.01171v3 [math.ST] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1604.01171"}], "alternate": [{"href": "http://arxiv.org/abs/1604.01171", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This article provides a new toolbox to derive sparse recovery guarantees from \nsmall deviations on extreme singular values or extreme eigenvalues obtained in \nRandom Matrix Theory. This work is based on Restricted Isometry Constants \n(RICs) which are a pivotal notion in Compressed Sensing and High-Dimensional \nStatistics as these constants finely assess how a linear operator is \nconditioned on the set of sparse vectors and hence how it performs in SRSR. \nWhile it is an open problem to construct deterministic matrices with apposite \nRICs, one can prove that such matrices exist using random matrices models. In \nthis paper, we show upper bounds on RICs for Gaussian and Rademacher matrices \nusing state-of-the-art small deviation estimates on their extreme eigenvalues. \nThis allows us to derive a lower bound on the probability of getting SRSR. One \nbenefit of this paper is a direct and explicit derivation of upper bounds on \nRICs and lower bounds on SRSR from small deviations on the extreme eigenvalues \ngiven by Random Matrix theory. \n</p>"}, "author": "Sandrine Dallaporta, Yohann De Castro", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840942", "id": "tag:google.com,2005:reader/item/0000000340763d42", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Saturating Splines and Feature Selection. (arXiv:1609.06764v3 [stat.ML] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1609.06764"}], "alternate": [{"href": "http://arxiv.org/abs/1609.06764", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We extend the adaptive regression spline model by incorporating saturation, \nthe natural requirement that a function extend as a constant outside a certain \nrange. We fit saturating splines to data using a convex optimization problem \nover a space of measures, which we solve using an efficient algorithm based on \nthe conditional gradient method. Unlike many existing approaches, our algorithm \nsolves the original infinite-dimensional (for splines of degree at least two) \noptimization problem without pre-specified knot locations. We then adapt our \nalgorithm to fit generalized additive models with saturating splines as \ncoordinate functions and show that the saturation requirement allows our model \nto simultaneously perform feature selection and nonlinear function fitting. \nFinally, we briefly sketch how the method can be extended to higher order \nsplines and to different requirements on the extension outside the data range. \n</p>"}, "author": "Nicholas Boyd, Trevor Hastie, Stephen Boyd, Benjamin Recht, Michael Jordan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840941", "id": "tag:google.com,2005:reader/item/0000000340763d46", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge. (arXiv:1611.10277v3 [cs.CL] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1611.10277"}], "alternate": [{"href": "http://arxiv.org/abs/1611.10277", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>While generative models such as Latent Dirichlet Allocation (LDA) have proven \nfruitful in topic modeling, they often require detailed assumptions and careful \nspecification of hyperparameters. Such model complexity issues only compound \nwhen trying to generalize generative models to incorporate human input. We \nintroduce Correlation Explanation (CorEx), an alternative approach to topic \nmodeling that does not assume an underlying generative model, and instead \nlearns maximally informative topics through an information-theoretic framework. \nThis framework naturally generalizes to hierarchical and semi-supervised \nextensions with no additional modeling assumptions. In particular, word-level \ndomain knowledge can be flexibly incorporated within CorEx through anchor \nwords, allowing topic separability and representation to be promoted with \nminimal human intervention. Across a variety of datasets, metrics, and \nexperiments, we demonstrate that CorEx produces topics that are comparable in \nquality to those produced by unsupervised and semi-supervised variants of LDA. \n</p>"}, "author": "Ryan J. Gallagher, Kyle Reing, David Kale, Greg Ver Steeg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840940", "id": "tag:google.com,2005:reader/item/0000000340763d4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Linear-Complexity Exponentially-Consistent Tests for Universal Outlying Sequence Detection. (arXiv:1701.06084v4 [cs.IT] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1701.06084"}], "alternate": [{"href": "http://arxiv.org/abs/1701.06084", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The problem of universal outlying sequence detection is studied, where the \ngoal is to detect outlying sequences among $M$ sequences of samples. A sequence \nis considered as outlying if the observations therein are generated by a \ndistribution different from those generating the observations in the majority \nof the sequences. In the universal setting, we are interested in identifying \nall the outlying sequences without knowing the underlying generating \ndistributions. In this paper, a class of tests based on distribution clustering \nis proposed. These tests are shown to be exponentially consistent with linear \ntime complexity in $M$. Numerical results demonstrate that our clustering-based \ntests achieve similar performance to existing tests, while being considerably \nmore computationally efficient. \n</p>"}, "author": "Yuheng Bu, Shaofeng Zou, Venugopal V. Veeravalli", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840939", "id": "tag:google.com,2005:reader/item/0000000340763d51", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "On the Effects of Batch and Weight Normalization in Generative Adversarial Networks. (arXiv:1704.03971v4 [stat.ML] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1704.03971"}], "alternate": [{"href": "http://arxiv.org/abs/1704.03971", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32505653594\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32505653594&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Generative adversarial networks (GANs) are highly effective unsupervised \nlearning frameworks that can generate very sharp data, even for data such as \nimages with complex, highly multimodal distributions. However GANs are known to \nbe very hard to train, suffering from problems such as mode collapse and \ndisturbing visual artifacts. Batch normalization (BN) techniques have been \nintroduced to address the training. Though BN accelerates the training in the \nbeginning, our experiments show that the use of BN can be unstable and \nnegatively impact the quality of the trained model. The evaluation of BN and \nnumerous other recent schemes for improving GAN training is hindered by the \nlack of an effective objective quality measure for GAN models. To address these \nissues, we first introduce a weight normalization (WN) approach for GAN \ntraining that significantly improves the stability, efficiency and the quality \nof the generated samples. To allow a methodical evaluation, we introduce \nsquared Euclidean reconstruction error on a test set as a new objective \nmeasure, to assess training performance in terms of speed, stability, and \nquality of generated samples. Our experiments with a standard DCGAN \narchitecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10) \nindicate that training using WN is generally superior to BN for GANs, achieving \n10% lower mean squared loss for reconstruction and significantly better \nqualitative results than BN. We further demonstrate the stability of WN on a \n21-layer ResNet trained with the CelebA data set. The code for this paper is \navailable at https://github.com/stormraiser/gan-weightnorm-resnet \n</p>"}, "author": "Sitao Xiang, Hao Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840938", "id": "tag:google.com,2005:reader/item/0000000340763d63", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Segmentation of the Proximal Femur from MR Images using Deep Convolutional Neural Networks. (arXiv:1704.06176v2 [cs.CV] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1704.06176"}], "alternate": [{"href": "http://arxiv.org/abs/1704.06176", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325056c6c0e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325056c6c0e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Magnetic resonance imaging (MRI) has been proposed as a complimentary method \nto measure bone quality and assess fracture risk. However, manual segmentation \nof MR images of bone is time-consuming, limiting the use of MRI measurements in \nthe clinical practice. The purpose of this paper is to present an automatic \nproximal femur segmentation method that is based on deep convolutional neural \nnetworks (CNNs). This study had institutional review board approval and written \ninformed consent was obtained from all subjects. A dataset of volumetric \nstructural MR images of the proximal femur from 86 subject were \nmanually-segmented by an expert. We performed experiments by training two \ndifferent CNN architectures with multiple number of initial feature maps and \nlayers, and tested their segmentation performance against the gold standard of \nmanual segmentations using four-fold cross-validation. Automatic segmentation \nof the proximal femur achieved a high dice similarity score of 0.94$\\pm$0.05 \nwith precision = 0.95$\\pm$0.02, and recall = 0.94$\\pm$0.08 using a CNN \narchitecture based on 3D convolution exceeding the performance of 2D CNNs. The \nhigh segmentation accuracy provided by CNNs has the potential to help bring the \nuse of structural MRI measurements of bone quality into clinical practice for \nmanagement of osteoporosis. \n</p>"}, "author": "Cem M. Deniz, Siyuan Xiang, Spencer Hallyburton, Arakua Welbeck, Stephen Honig, Kyunghyun Cho, Gregory Chang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840937", "id": "tag:google.com,2005:reader/item/0000000340763d70", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs. (arXiv:1706.02633v2 [stat.ML] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.02633"}], "alternate": [{"href": "http://arxiv.org/abs/1706.02633", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative Adversarial Networks (GANs) have shown remarkable success as a \nframework for training models to produce realistic-looking data. In this work, \nwe propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to \nproduce realistic real-valued multi-dimensional time series, with an emphasis \non their application to medical data. RGANs make use of recurrent neural \nnetworks in the generator and the discriminator. In the case of RCGANs, both of \nthese RNNs are conditioned on auxiliary information. We demonstrate our models \nin a set of toy datasets, where we show visually and quantitatively (using \nsample likelihood and maximum mean discrepancy) that they can successfully \ngenerate realistic time-series. We also describe novel evaluation methods for \nGANs, where we generate a synthetic labelled training dataset, and evaluate on \na real test set the performance of a model trained on the synthetic data, and \nvice-versa. We illustrate with these metrics that RCGANs can generate \ntime-series data useful for supervised training, with only minor degradation in \nperformance on real test data. This is demonstrated on digit classification \nfrom 'serialised' MNIST and by training an early warning system on a medical \ndataset of 17,000 patients from an intensive care unit. We further discuss and \nanalyse the privacy concerns that may arise when using RCGANs to generate \nrealistic synthetic medical time series data. \n</p>"}, "author": "Crist&#xf3;bal Esteban, Stephanie L. Hyland, Gunnar R&#xe4;tsch", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840936", "id": "tag:google.com,2005:reader/item/0000000340763d79", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Labeled Memory Networks for Online Model Adaptation. (arXiv:1707.01461v3 [cs.LG] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.01461"}], "alternate": [{"href": "http://arxiv.org/abs/1707.01461", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Augmenting a neural network with memory that can grow without growing the \nnumber of trained parameters is a recent powerful concept with many exciting \napplications. We propose a design of memory augmented neural networks (MANNs) \ncalled Labeled Memory Networks (LMNs) suited for tasks requiring online \nadaptation in classification models. LMNs organize the memory with classes as \nthe primary key.The memory acts as a second boosted stage following a regular \nneural network thereby allowing the memory and the primary network to play \ncomplementary roles. Unlike existing MANNs that write to memory for every \ninstance and use LRU based memory replacement, LMNs write only for instances \nwith non-zero loss and use label-based memory replacement. We demonstrate \nsignificant accuracy gains on various tasks including word-modelling and \nfew-shot learning. In this paper, we establish their potential in online \nadapting a batch trained neural network to domain-relevant labeled data at \ndeployment time. We show that LMNs are better than other MANNs designed for \nmeta-learning. We also found them to be more accurate and faster than \nstate-of-the-art methods of retuning model parameters for adapting to \ndomain-specific labeled data. \n</p>"}, "author": "Shiv Shankar, Sunita Sarawagi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840935", "id": "tag:google.com,2005:reader/item/0000000340763d81", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Accelerating Kernel Classifiers Through Borders Mapping. (arXiv:1708.05917v2 [stat.ML] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.05917"}], "alternate": [{"href": "http://arxiv.org/abs/1708.05917", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Support vector machines (SVM) and other kernel techniques represent a family \nof powerful statistical classification methods with high accuracy and broad \napplicability. Because they use all or a significant portion of the training \ndata, however, they can be slow, especially for large problems. Piecewise \nlinear classifiers are similarly versatile, yet have the additional advantages \nof simplicity, ease of interpretation and, if the number of component linear \nclassifiers is not too large, speed. Here we show how a simple, piecewise \nlinear classifier can be trained from a kernel-based classifier in order to \nimprove the classification speed. The method works by finding the root of the \ndifference in conditional probabilities between pairs of opposite classes to \nbuild up a representation of the decision boundary. When tested on 17 different \ndatasets, it succeeded in improving the classification speed of a SVM for 9 of \nthem by factors as high as 88 times or more. The method is best suited to \nproblems with continuum features data and smooth probability functions. Because \nthe component linear classifiers are built up individually from an existing \nclassifier, rather than through a simultaneous optimization procedure, the \nclassifier is also fast to train. \n</p>"}, "author": "Peter Mills", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840934", "id": "tag:google.com,2005:reader/item/0000000340763d8e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Asymptotic Bayesian Generalization Error in a General Stochastic Matrix Factorization for Markov Chain and Bayesian Network. (arXiv:1709.04212v3 [math.ST] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.04212"}], "alternate": [{"href": "http://arxiv.org/abs/1709.04212", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Stochastic matrix factorization (SMF) can be regarded as a restriction of \nnon-negative matrix factorization (NMF). SMF is useful for inference of topic \nmodels, NMF for binary matrices data, Markov chains, and Bayesian networks. \nHowever, SMF needs strong assumptions to reach a unique factorization and its \ntheoretical prediction accuracy has not yet been clarified. In this paper, we \nstudy the maximum the pole of zeta function (real log canonical threshold) of a \ngeneral SMF and derive an upper bound of the generalization error in Bayesian \ninference. The results give a foundation for a widely applicable and rigorous \nfactorization method of SMF and mean that the generalization error in SMF \nbecomes smaller than regular statistical models by Bayesian inference. \n</p>"}, "author": "Naoki Hayashi, Sumio Watanabe", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840933", "id": "tag:google.com,2005:reader/item/0000000340763d9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Accelerated Block Coordinate Proximal Gradients with Applications in High Dimensional Statistics. (arXiv:1710.05338v7 [math.OC] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05338"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05338", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Nonconvex optimization problems arise in different research fields and arouse \nlots of attention in signal processing, statistics and machine learning. In \nthis work, we explore the accelerated proximal gradient method and some of its \nvariants which have been shown to converge under nonconvex context recently. We \nshow that a novel variant proposed here, which exploits adaptive momentum and \nblock coordinate update with specific update rules, further improves the \nperformance of a broad class of nonconvex problems. In applications to sparse \nlinear regression with regularizations like Lasso, grouped Lasso, capped \n$\\ell_1$ and SCAP, the proposed scheme enjoys provable local linear \nconvergence, with experimental justification. \n</p>"}, "author": "Tsz Kit Lau, Yuan Yao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840932", "id": "tag:google.com,2005:reader/item/0000000340763db0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks. (arXiv:1711.02213v2 [cs.LG] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02213"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02213", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks are commonly developed and trained in 32-bit floating \npoint format. Significant gains in performance and energy efficiency could be \nrealized by training and inference in numerical formats optimized for deep \nlearning. Despite advances in limited precision inference in recent years, \ntraining of neural networks in low bit-width remains a challenging problem. \nHere we present the Flexpoint data format, aiming at a complete replacement of \n32-bit floating point format training and inference, designed to support modern \ndeep network topologies without modifications. Flexpoint tensors have a shared \nexponent that is dynamically adjusted to minimize overflows and maximize \navailable dynamic range. We validate Flexpoint by training AlexNet, a deep \nresidual network and a generative adversarial network, using a simulator \nimplemented with the neon deep learning framework. We demonstrate that 16-bit \nFlexpoint closely matches 32-bit floating point in training all three models, \nwithout any need for tuning of model hyperparameters. Our results suggest \nFlexpoint as a promising numerical format for future hardware for training and \ninference. \n</p>"}, "author": "Urs K&#xf6;ster, Tristan J. Webb, Xin Wang, Marcel Nassar, Arjun K. Bansal, William H. Constable, O&#x11f;uz H. Elibol, Scott Gray, Stewart Hall, Luke Hornof, Amir Khosrowshahi, Carey Kloss, Ruby J. Pai, Naveen Rao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840931", "id": "tag:google.com,2005:reader/item/0000000340763dc5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Feature importance scores and lossless feature pruning using Banzhaf power indices. (arXiv:1711.04992v2 [stat.ML] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04992"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04992", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Understanding the influence of features in machine learning is crucial to \ninterpreting models and selecting the best features for classification. In this \nwork we propose the use of principles from coalitional game theory to reason \nabout importance of features. In particular, we propose the use of the Banzhaf \npower index as a measure of influence of features on the outcome of a \nclassifier. We show that features having Banzhaf power index of zero can be \nlosslessly pruned without damage to classifier accuracy. Computing the power \nindices does not require having access to data samples. However, if samples are \navailable, the indices can be empirically estimated. We compute Banzhaf power \nindices for a neural network classifier on real-life data, and compare the \nresults with gradient-based feature saliency, and coefficients of a logistic \nregression model with $L_1$ regularization. \n</p>"}, "author": "Bogdan Kulynych, Carmela Troncoso", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840930", "id": "tag:google.com,2005:reader/item/0000000340763dd0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "A Resizable Mini-batch Gradient Descent based on a Randomized Weighted Majority. (arXiv:1711.06424v2 [stat.ML] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06424"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06424", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Determining the appropriate batch size for mini-batch gradient descent is \nalways time consuming as it often relies on grid search. This paper considers a \nresizable mini-batch gradient descent (RMGD) algorithm-inspired by the \nrandomized weighted majority algorithm-for achieving best performance in grid \nsearch by selecting an appropriate batch size at each epoch with a probability \ndefined as a function of its previous success/failure and the validation error. \nThis probability encourages exploration of different batch size and then later \nexploitation of batch size with history of success. At each epoch, the RMGD \nsamples a batch size from its probability distribution, then uses the selected \nbatch size for mini-batch gradient descent. After obtaining the validation \nerror at each epoch, the probability distribution is updated to incorporate the \neffectiveness of the sampled batch size. The RMGD essentially assists the \nlearning process to explore the possible domain of the batch size and exploit \nsuccessful batch size. Experimental results show that the RMGD achieves \nperformance better than the best performing single batch size. Furthermore, it \nattains this performance in a shorter amount of time than that of the best \nperforming. It is surprising that the RMGD achieves better performance than \ngrid search. \n</p>"}, "author": "Seong Jin Cho, Sunghun Kang, Chang D. Yoo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840929", "id": "tag:google.com,2005:reader/item/0000000340763ddf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Variational Encoding of Complex Dynamics. (arXiv:1711.08576v2 [stat.ML] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08576"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08576", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325056c70b1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325056c70b1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Often the analysis of time-dependent chemical and biophysical systems \nproduces high-dimensional time-series data for which it can be difficult to \ninterpret which individual features are most salient. While recent work from \nour group and others has demonstrated the utility of time-lagged co-variate \nmodels to study such systems, linearity assumptions can limit the compression \nof inherently nonlinear dynamics into just a few characteristic components. \nRecent work in the field of deep learning has led to the development of \nvariational autoencoders (VAE), which are able to compress complex datasets \ninto simpler manifolds. We present the use of a time-lagged VAE, or variational \ndynamics encoder (VDE), to reduce complex, nonlinear processes to a single \nembedding with high fidelity to the underlying dynamics. We demonstrate how the \nVDE is able to capture nontrivial dynamics in a variety of examples, including \nBrownian dynamics and atomistic protein folding. Additionally, we demonstrate a \nmethod for analyzing the VDE model, inspired by saliency mapping, to determine \nwhat features are selected by the VDE model to describe dynamics. The VDE \npresents an important step in applying techniques from deep learning to more \naccurately model and interpret complex biophysics. \n</p>"}, "author": "Carlos X. Hern&#xe1;ndez, Hannah K. Wayment-Steele, Mohammad M. Sultan, Brooke E. Husic, Vijay S. Pande", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512450843841", "timestampUsec": "1512450843840928", "id": "tag:google.com,2005:reader/item/0000000340763df3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning with Biased Complementary Labels. (arXiv:1711.09535v2 [stat.ML] UPDATED)", "published": 1512450844, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.09535"}], "alternate": [{"href": "http://arxiv.org/abs/1711.09535", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we study the classification problem in which we have access to \neasily obtainable surrogate for the true labels, namely complementary labels, \nwhich specify classes that observations do \\textbf{not} belong to. For example, \nif one is familiar with monkeys but not meerkats, a meerkat is easily \nidentified as not a monkey, so \"monkey\" is annotated to the meerkat as a \ncomplementary label. Specifically, let $Y$ and $\\bar{Y}$ be the true and \ncomplementary labels, respectively. We first model the annotation of \ncomplementary labels via the transition probabilities $P(\\bar{Y}=i|Y=j), i\\neq \nj\\in\\{1,\\cdots,c\\}$, where $c$ is the number of classes. All the previous \nmethods implicitly assume that the transition probabilities $P(\\bar{Y}=i|Y=j)$ \nare identical, which is far from true in practice because humans are biased \ntoward their own experience. For example, if a person is more familiar with \nmonkey than prairie dog when providing complementary labels for meerkats, \nhe/she is more likely to employ \"monkey\" as a complementary label. We therefore \nreason that the transition probabilities will be different. In this paper, we \naddress three fundamental problems raised by learning with biased complementary \nlabels. (1) How to estimate the transition probabilities? (2) How to modify the \ntraditional loss functions and extend standard deep neural network classifiers \nto learn with biased complementary labels? (3) Does the classifier learned from \nexamples with complementary labels by our proposed method converge to the \noptimal one learned from examples with true labels? Comprehensive experiments \non MNIST, CIFAR10, CIFAR100, and Tiny ImageNet empirically validate the \nsuperiority of the proposed method to the current state-of-the-art methods with \naccuracy gains of over 10\\%. \n</p>"}, "author": "Xiyu Yu, Tongliang Liu, Mingming Gong, Dacheng Tao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369392", "id": "tag:google.com,2005:reader/item/000000033fb640d9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Algorithmically probable mutations reproduce aspects of evolution such as convergence rate, genetic memory, modularity, diversity explosions, and mass extinction. (arXiv:1709.00268v5 [cs.NE] UPDATED)", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.00268"}], "alternate": [{"href": "http://arxiv.org/abs/1709.00268", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Natural selection explains how life has evolved over millions of years from \nmore primitive forms. The speed at which this happens, however, has sometimes \ndefied explanations based on random (uniformly distributed) mutations. Here we \ninvestigate the application of algorithmic mutations (no recombination) to \nbinary matrices drawn from numerical approximations to algorithmic probability \nin order to compare evolutionary convergence rates against the null hypothesis \n(uniformly distributed mutations). Results both on synthetic and a small \nbiological examples lead to an accelerated rate of convergence when using the \nalgorithmic probability. We also show that algorithmically evolved modularity \nprovides an advantage that produces a genetic memory. We demonstrate that \nregular structures are preserved and carried on when they first occur and can \nlead to an accelerated production of diversity and extinction, possibly \nexplaining naturally occurring phenomena such as diversity explosions (e.g. the \nCambrian) and massive extinctions (e.g. the End Triassic) whose causes have \neluded researchers and are a cause for debate. The approach introduced here \nappears to be a better approximation to biological evolution than models based \nexclusively upon random uniform mutations, and it also approaches better a \nformal version of open-ended evolution based on previous results. The results \nvalidate the motivations and results of Chaitin's Metabiology programme and \nprevious suggestions that computation may be an equally important driver of \nevolution together, and even before, the action and result of natural \nselection. We also show that inducing the method on problems of optimization, \nsuch as genetic algorithms, has the potential to significantly accelerate \nconvergence of artificial evolutionary algorithms. \n</p>"}, "author": "Santiago Hern&#xe1;ndez-Orozco, Narsis A. Kiani, Hector Zenil", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369391", "id": "tag:google.com,2005:reader/item/000000033fb64101", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "WSNet: Compact and Efficient Networks with Weight Sampling. (arXiv:1711.10067v2 [cs.CV] UPDATED)", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.10067"}], "alternate": [{"href": "http://arxiv.org/abs/1711.10067", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a new approach and a novel architecture, termed WSNet, for \nlearning compact and efficient deep neural networks. Existing approaches \nconventionally learn full model parameters independently at first and then \ncompress them via ad hoc processing like model pruning or filter factorization. \nDifferent from them, WSNet proposes learning model parameters by sampling from \na compact set of learnable parameters, which naturally enforces parameter \nsharing throughout the learning process. We show that such novel weight \nsampling approach (and induced WSNet) promotes both weights and computation \nsharing favorably. It can more efficiently learn much smaller networks with \ncompetitive performance, compared to baseline networks with equal number of \nconvolution filters. Specifically, we consider learning compact and efficient \n1D convolutional neural networks for audio classification. Extensive \nexperiments on multiple audio classification datasets verify the effectiveness \nof WSNet. Combined with weight quantization, the resulted models are up to 180x \nsmaller and theoretically up to 16x faster than the well-established baselines, \nwithout noticeable performance drop. \n</p>"}, "author": "Xiaojie Jin, Yingzhen Yang, Ning Xu, Jianchao Yang, Jiashi Feng, Shuicheng Yan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369390", "id": "tag:google.com,2005:reader/item/000000033fb64116", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "PSIque: Next Sequence Prediction of Satellite Images using a Convolutional Sequence-to-Sequence Network. (arXiv:1711.10644v2 [cs.CV] UPDATED)", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.10644"}], "alternate": [{"href": "http://arxiv.org/abs/1711.10644", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Predicting unseen weather phenomena is an important issue for disaster \nmanagement. In this paper, we suggest a model for a convolutional \nsequence-to-sequence autoencoder for predicting undiscovered weather situations \nfrom previous satellite images. We also propose a symmetric skip connection \nbetween encoder and decoder modules to produce more comprehensive image \npredictions. To examine our model performance, we conducted experiments for \neach suggested model to predict future satellite images from historical \nsatellite images. A specific combination of skip connection and \nsequence-to-sequence autoencoder was able to generate closest prediction from \nthe ground truth image. \n</p>"}, "author": "Seungkyun Hong, Seongchan Kim, Minsu Joh, Sa-kwang Song", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369389", "id": "tag:google.com,2005:reader/item/000000033fb64134", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learnings Options End-to-End for Continuous Action Tasks. (arXiv:1712.00004v1 [cs.LG])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00004"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00004", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present new results on learning temporally extended actions for \ncontinuoustasks, using the options framework (Suttonet al.[1999b], Precup \n[2000]). In orderto achieve this goal we work with the option-critic \narchitecture (Baconet al.[2017])using a deliberation cost and train it with \nproximal policy optimization (Schulmanet al.[2017]) instead of vanilla policy \ngradient. Results on Mujoco domains arepromising, but lead to interesting \nquestions aboutwhena given option should beused, an issue directly connected to \nthe use of initiation sets. \n</p>"}, "author": "Martin Klissarov, Pierre-Luc Bacon, Jean Harb, Doina Precup", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369388", "id": "tag:google.com,2005:reader/item/000000033fb64164", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Comparing Deep Reinforcement Learning and Evolutionary Methods in Continuous Control. (arXiv:1712.00006v1 [cs.LG])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00006"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00006", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Reinforcement learning and evolutionary strategy are two major approaches in \naddressing complicated control problems. Both have strong biological basis and \nthere have been recently many advanced techniques in both domains. In this \npaper, we present a thorough comparison between the state of the art techniques \nin both domains in complex continuous control tasks. We also formulate the \nparallelized version of the Proximal Policy Optimization method and the Deep \nDeterministic Policy Gradient method. \n</p>"}, "author": "Shangtong Zhang, Osmar R. Zaiane", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369387", "id": "tag:google.com,2005:reader/item/000000033fb6418d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "New Techniques for Inferring L-Systems Using Genetic Algorithm. (arXiv:1712.00180v1 [cs.AI])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00180"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00180", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Lindenmayer systems (L-systems) are a formal grammar system that iteratively \nrewrites all symbols of a string, in parallel. When visualized with a graphical \ninterpretation, the images have self-similar shapes that appear frequently in \nnature, and they have been particularly successful as a concise, reusable \ntechnique for simulating plants. The L-system inference problem is to find an \nL-system to simulate a given plant. This is currently done mainly by experts, \nbut this process is limited by the availability of experts, the complexity that \nmay be solved by humans, and time. This paper introduces the Plant Model \nInference Tool (PMIT) that infers deterministic context-free L-systems from an \ninitial sequence of strings generated by the system using a genetic algorithm. \nPMIT is able to infer more complex systems than existing approaches. Indeed, \nwhile existing approaches are limited to L-systems with a total sum of 20 \ncombined symbols in the productions, PMIT can infer almost all L-systems tested \nwhere the total sum is 140 symbols. This was validated using a test bed of 28 \npreviously developed L-system models, in addition to models created \nartificially by bootstrapping larger models. \n</p>"}, "author": "Jason Bernard, Ian McQuillan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369386", "id": "tag:google.com,2005:reader/item/000000033fb641ac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improving Smiling Detection with Race and Gender Diversity. (arXiv:1712.00193v1 [cs.CV])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00193"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00193", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent progress in deep learning has been accompanied by a growing concern \nfor whether models are fair for users, with equally good performance across \ndifferent demographics. In computer vision research, such questions are \nrelevant to face detection and the related task of face attribute detection, \namong others. We measure race and gender inclusion in the context of smiling \ndetection, and introduce a method for improving smiling detection across \ndemographic groups. Our method introduces several modifications over existing \ndetection methods, leveraging twofold transfer learning to better model facial \ndiversity. Results show that this technique improves accuracy against strong \nbaselines for most demographic groups as well as overall. Our best-performing \nmodel defines a new state-of-the-art for smiling detection, reaching 91% on the \nFaces of the World dataset. The accompanying multi-head diversity classifier \nalso defines a new state-of-the-art for gender classification, reaching 93.87% \non the Faces of the World dataset. This research demonstrates the utility of \nmodeling race and gender to improve a face attribute detection task, using a \ntwofold transfer learning framework that allows for privacy towards individuals \nin a target dataset. \n</p>"}, "author": "Hee Jung Ryu, Margaret Mitchell, Hartwig Adam", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369385", "id": "tag:google.com,2005:reader/item/000000033fb641ba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A double competitive strategy based learning automata algorithm. (arXiv:1712.00222v1 [cs.AI])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00222"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00222", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning Automata (LA) are considered as one of the most powerful tools in \nthe field of reinforcement learning. The family of estimator algorithms is \nproposed to improve the convergence rate of LA and has made great achievements. \nHowever, the estimators perform poorly on estimating the reward probabilities \nof actions in the initial stage of the learning process of LA. In this \nsituation, a lot of rewards would be added to the probabilities of non-optimal \nactions. Thus, a large number of extra iterations are needed to compensate for \nthese wrong rewards. In order to improve the speed of convergence, we propose a \nnew P-model absorbing learning automaton by utilizing a double competitive \nstrategy which is designed for updating the action probability vector. In this \nway, the wrong rewards can be corrected instantly. Hence, the proposed Double \nCompetitive Algorithm overcomes the drawbacks of existing estimator algorithms. \nA refined analysis is presented to show the $\\epsilon-optimality$ of the \nproposed scheme. The extensive experimental results in benchmark environments \ndemonstrate that our proposed learning automata perform more efficiently than \nthe most classic LA $SE_{RI}$ and the current fastest LA $DGCPA^{*}$. \n</p>"}, "author": "Chong Di", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369384", "id": "tag:google.com,2005:reader/item/000000033fb641d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering. (arXiv:1712.00377v1 [cs.CV])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00377"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00377", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325056c74bd\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325056c74bd&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A number of studies have found that today's Visual Question Answering (VQA) \nmodels are heavily driven by superficial correlations in the training data and \nlack sufficient image grounding. To encourage development of models geared \ntowards the latter, we propose a new setting for VQA where for every question \ntype, train and test sets have different prior distributions of answers. \nSpecifically, we present new splits of the VQA v1 and VQA v2 datasets, which we \ncall Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 \nrespectively). First, we evaluate several existing VQA models under this new \nsetting and show that their performance degrades significantly compared to the \noriginal VQA setting. Second, we propose a novel Grounded Visual Question \nAnswering model (GVQA) that contains inductive biases and restrictions in the \narchitecture specifically designed to prevent the model from 'cheating' by \nprimarily relying on priors in the training data. Specifically, GVQA explicitly \ndisentangles the recognition of visual concepts present in the image from the \nidentification of plausible answer space for a given question, enabling the \nmodel to more robustly generalize across different distributions of answers. \nGVQA is built off an existing VQA model -- Stacked Attention Networks (SAN). \nOur experiments demonstrate that GVQA significantly outperforms SAN on both \nVQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more \npowerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in \nseveral cases. GVQA offers strengths complementary to SAN when trained and \nevaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more \ntransparent and interpretable than existing VQA models. \n</p>"}, "author": "Aishwarya Agrawal, Dhruv Batra, Devi Parikh, Aniruddha Kembhavi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369383", "id": "tag:google.com,2005:reader/item/000000033fb641e7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Novel Exploration Techniques (NETs) for Malaria Policy Interventions. (arXiv:1712.00428v1 [cs.AI])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00428"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00428", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250575ac44\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250575ac44&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The task of decision-making under uncertainty is daunting, especially for \nproblems which have significant complexity. Healthcare policy makers across the \nglobe are facing problems under challenging constraints, with limited tools to \nhelp them make data driven decisions. In this work we frame the process of \nfinding an optimal malaria policy as a stochastic multi-armed bandit problem, \nand implement three agent based strategies to explore the policy space. We \napply a Gaussian Process regression to the findings of each agent, both for \ncomparison and to account for stochastic results from simulating the spread of \nmalaria in a fixed population. The generated policy spaces are compared with \npublished results to give a direct reference with human expert decisions for \nthe same simulated population. Our novel approach provides a powerful resource \nfor policy makers, and a platform which can be readily extended to capture \nfuture more nuanced policy spaces. \n</p>"}, "author": "Oliver Bent, Sekou L. Remy, Stephen Roberts, Aisha Walcott-Bryant", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369381", "id": "tag:google.com,2005:reader/item/000000033fb64205", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Constraint Answer Set Solver EZCSP and Why Integration Schemas Matter. (arXiv:1702.04047v3 [cs.AI] UPDATED)", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1702.04047"}], "alternate": [{"href": "http://arxiv.org/abs/1702.04047", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Researchers in answer set programming and constraint programming have spent \nsignificant efforts in the development of hybrid languages and solving \nalgorithms combining the strengths of these traditionally separate fields. \nThese efforts resulted in a new research area: constraint answer set \nprogramming. Constraint answer set programming languages and systems proved to \nbe successful at providing declarative, yet efficient solutions to problems \ninvolving hybrid reasoning tasks. One of the main contributions of this paper \nis the first comprehensive account of the constraint answer set language and \nsolver EZCSP, a mainstream representative of this research area that has been \nused in various successful applications. We also develop an extension of the \ntransition systems proposed by Nieuwenhuis et al. in 2006 to capture Boolean \nsatisfiability solvers. We use this extension to describe the EZCSP algorithm \nand prove formal claims about it. The design and algorithmic details behind \nEZCSP clearly demonstrate that the development of the hybrid systems of this \nkind is challenging. Many questions arise when one faces various design choices \nin an attempt to maximize system's benefits. One of the key decisions that a \ndeveloper of a hybrid solver makes is settling on a particular integration \nschema within its implementation. Thus, another important contribution of this \npaper is a thorough case study based on EZCSP, focused on the various \nintegration schemas that it provides. \n</p> \n<p>Under consideration in Theory and Practice of Logic Programming (TPLP). \n</p>"}, "author": "Marcello Balduccini, Yuliya Lierler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369380", "id": "tag:google.com,2005:reader/item/000000033fb64241", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Beyond Parity: Fairness Objectives for Collaborative Filtering. (arXiv:1705.08804v2 [cs.IR] UPDATED)", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.08804"}], "alternate": [{"href": "http://arxiv.org/abs/1705.08804", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study fairness in collaborative-filtering recommender systems, which are \nsensitive to discrimination that exists in historical data. Biased data can \nlead collaborative-filtering methods to make unfair predictions for users from \nminority groups. We identify the insufficiency of existing fairness metrics and \npropose four new metrics that address different forms of unfairness. These \nfairness metrics can be optimized by adding fairness terms to the learning \nobjective. Experiments on synthetic and real data show that our new metrics can \nbetter measure fairness than the baseline, and that the fairness objectives \neffectively help reduce unfairness. \n</p>"}, "author": "Sirui Yao, Bert Huang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369379", "id": "tag:google.com,2005:reader/item/000000033fb6425b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Computing LPMLN Using ASP and MLN Solvers. (arXiv:1707.06325v3 [cs.AI] UPDATED)", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.06325"}], "alternate": [{"href": "http://arxiv.org/abs/1707.06325", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>LPMLN is a recent addition to probabilistic logic programming languages. Its \nmain idea is to overcome the rigid nature of the stable model semantics by \nassigning a weight to each rule in a way similar to Markov Logic is defined. We \npresent two implementations of LPMLN, $\\text{LPMLN2ASP}$ and \n$\\text{LPMLN2MLN}$. System $\\text{LPMLN2ASP}$ translates LPMLN programs into \nthe input language of answer set solver $\\text{CLINGO}$, and using weak \nconstraints and stable model enumeration, it can compute most probable stable \nmodels as well as exact conditional and marginal probabilities. System \n$\\text{LPMLN2MLN}$ translates LPMLN programs into the input language of Markov \nLogic solvers, such as $\\text{ALCHEMY}$, $\\text{TUFFY}$, and $\\text{ROCKIT}$, \nand allows for performing approximate probabilistic inference on LPMLN \nprograms. We also demonstrate the usefulness of the LPMLN systems for computing \nother languages, such as ProbLog and Pearl's Causal Models, that are shown to \nbe translatable into LPMLN. (Under consideration for acceptance in TPLP) \n</p>"}, "author": "Joohyung Lee, Samidh Talsania, Yi Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369378", "id": "tag:google.com,2005:reader/item/000000033fb64271", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Self-supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation. (arXiv:1709.10489v2 [cs.LG] UPDATED)", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.10489"}], "alternate": [{"href": "http://arxiv.org/abs/1709.10489", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Enabling robots to autonomously navigate complex environments is essential \nfor real-world deployment. Prior methods approach this problem by having the \nrobot maintain an internal map of the world, and then use a localization and \nplanning method to navigate through the internal map. However, these approaches \noften include a variety of assumptions, are computationally intensive, and do \nnot learn from failures. In contrast, learning-based methods improve as the \nrobot acts in the environment, but are difficult to deploy in the real-world \ndue to their high sample complexity. To address the need to learn complex \npolicies with few samples, we propose a generalized computation graph that \nsubsumes value-based model-free methods and model-based methods, with specific \ninstantiations interpolating between model-free and model-based. We then \ninstantiate this graph to form a navigation model that learns from raw images \nand is sample efficient. Our simulated car experiments explore the design \ndecisions of our navigation model, and show our approach outperforms \nsingle-step and $N$-step double Q-learning. We also evaluate our approach on a \nreal-world RC car and show it can learn to navigate through a complex indoor \nenvironment with a few hours of fully autonomous, self-supervised training. \nVideos of the experiments and code can be found at github.com/gkahn13/gcg \n</p>"}, "author": "Gregory Kahn, Adam Villaflor, Bosen Ding, Pieter Abbeel, Sergey Levine", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369377", "id": "tag:google.com,2005:reader/item/000000033fb64282", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Prediction Under Uncertainty with Error-Encoding Networks. (arXiv:1711.04994v3 [cs.AI] UPDATED)", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04994"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04994", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work we introduce a new framework for performing temporal predictions \nin the presence of uncertainty. It is based on a simple idea of disentangling \ncomponents of the future state which are predictable from those which are \ninherently unpredictable, and encoding the unpredictable components into a \nlow-dimensional latent variable which is fed into a forward model. Our method \nuses a supervised training objective which is fast and easy to train. We \nevaluate it in the context of video prediction on multiple datasets and show \nthat it is able to consistently generate diverse predictions without the need \nfor alternating minimization over a latent space or adversarial training. \n</p>"}, "author": "Mikael Henaff, Junbo Zhao, Yann LeCun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369376", "id": "tag:google.com,2005:reader/item/000000033fb64291", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Pedagogical learning. (arXiv:1711.09401v2 [cs.AI] UPDATED)", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.09401"}], "alternate": [{"href": "http://arxiv.org/abs/1711.09401", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A common assumption in machine learning is that training data are i.i.d. \nsamples from some distribution. Processes that generate i.i.d. samples are, in \na sense, uninformative---they produce data without regard to how good this data \nis for learning. By contrast, cognitive science research has shown that when \npeople generate training data for others (i.e., teaching), they deliberately \nselect examples that are helpful for learning. Because the data is more \ninformative, learning can require less data. Interestingly, such examples are \nmost effective when learners know that the data were pedagogically generated \n(as opposed to randomly generated). We call this pedagogical learning---when a \nlearner assumes that evidence comes from a helpful teacher. In this work, we \nask how pedagogical learning might work for machine learning algorithms. \nStudying this question requires understanding how people actually teach complex \nconcepts with examples, so we conducted a behavioral study examining how people \nteach regular expressions using example strings. We found that teachers' \nexamples contain powerful clustering structure that can greatly facilitate \nlearning. We then develop a model of teaching and show a proof of concept that \nusing this model inside of a learner can improve performance. \n</p>"}, "author": "Long Ouyang, Michael C. Frank", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369374", "id": "tag:google.com,2005:reader/item/000000033fb642b4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Modeling Information Flow Through Deep Neural Networks. (arXiv:1712.00003v1 [cs.LG])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00003"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00003", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes a principled information theoretic analysis of \nclassification for deep neural network structures, e.g. convolutional neural \nnetworks (CNN). The output of convolutional filters is modeled as a random \nvariable Y conditioned on the object class C and network filter bank F. The \nconditional entropy (CENT) H(Y |C,F) is shown in theory and experiments to be a \nhighly compact and class-informative code, that can be computed from the filter \noutputs throughout an existing CNN and used to obtain higher classification \nresults than the original CNN itself. Experiments demonstrate the effectiveness \nof CENT feature analysis in two separate CNN classification contexts. 1) In the \nclassification of neurodegeneration due to Alzheimer's disease (AD) and natural \naging from 3D magnetic resonance image (MRI) volumes, 3 CENT features result in \nan AUC=94.6% for whole-brain AD classification, the highest reported accuracy \non the public OASIS dataset used and 12% higher than the softmax output of the \noriginal CNN trained for the task. 2) In the context of visual object \nclassification from 2D photographs, transfer learning based on a small set of \nCENT features identified throughout an existing CNN leads to AUC values \ncomparable to the 1000-feature softmax output of the original network when \nclassifying previously unseen object categories. The general information \ntheoretical analysis explains various recent CNN design successes, e.g. densely \nconnected CNN architectures, and provides insights for future research \ndirections in deep learning. \n</p>"}, "author": "Ahmad Chaddad, Behnaz Naisiri, Marco Pedersoli, Eric Granger, Christian Desrosiers, Matthew Toews", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369373", "id": "tag:google.com,2005:reader/item/000000033fb642c0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Highrisk Prediction from Electronic Medical Records via Deep Attention Networks. (arXiv:1712.00010v1 [cs.LG])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00010"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00010", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Predicting highrisk vascular diseases is a significant issue in the medical \ndomain. Most predicting methods predict the prognosis of patients from \npathological and radiological measurements, which are expensive and require \nmuch time to be analyzed. Here we propose deep attention models that predict \nthe onset of the high risky vascular disease from symbolic medical histories \nsequence of hypertension patients such as ICD-10 and pharmacy codes only, \nMedical History-based Prediction using Attention Network (MeHPAN). We \ndemonstrate two types of attention models based on 1) bidirectional gated \nrecurrent unit (R-MeHPAN) and 2) 1D convolutional multilayer model (C-MeHPAN). \nTwo MeHPAN models are evaluated on approximately 50,000 hypertension patients \nwith respect to precision, recall, f1-measure and area under the curve (AUC). \nExperimental results show that our MeHPAN methods outperform standard \nclassification models. Comparing two MeHPANs, R-MeHPAN provides more better \ndiscriminative capability with respect to all metrics while C-MeHPAN presents \nmuch shorter training time with competitive accuracy. \n</p>"}, "author": "You Jin Kim (1), Yun-Geun Lee (1), Jeong Whun Kim (2), Jin Joo Park (2), Borim Ryu (2), Jung-Woo Ha (1) ((1) Clova AI Research, NAVER Corp., (2) Seoul National University Bundang Hospital)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369372", "id": "tag:google.com,2005:reader/item/000000033fb642cf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Feature discovery and visualization of robot mission data using convolutional autoencoders and Bayesian nonparametric topic models. (arXiv:1712.00028v1 [cs.LG])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00028"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00028", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250575afee\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250575afee&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The gap between our ability to collect interesting data and our ability to \nanalyze these data is growing at an unprecedented rate. Recent algorithmic \nattempts to fill this gap have employed unsupervised tools to discover \nstructure in data. Some of the most successful approaches have used \nprobabilistic models to uncover latent thematic structure in discrete data. \nDespite the success of these models on textual data, they have not generalized \nas well to image data, in part because of the spatial and temporal structure \nthat may exist in an image stream. \n</p> \n<p>We introduce a novel unsupervised machine learning framework that \nincorporates the ability of convolutional autoencoders to discover features \nfrom images that directly encode spatial information, within a Bayesian \nnonparametric topic model that discovers meaningful latent patterns within \ndiscrete data. By using this hybrid framework, we overcome the fundamental \ndependency of traditional topic models on rigidly hand-coded data \nrepresentations, while simultaneously encoding spatial dependency in our topics \nwithout adding model complexity. We apply this model to the motivating \napplication of high-level scene understanding and mission summarization for \nexploratory marine robots. Our experiments on a seafloor dataset collected by a \nmarine robot show that the proposed hybrid framework outperforms current \nstate-of-the-art approaches on the task of unsupervised seafloor terrain \ncharacterization. \n</p>"}, "author": "Genevieve Flaspohler, Nicholas Roy, Yogesh Girdhar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369371", "id": "tag:google.com,2005:reader/item/000000033fb642fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Paris-Lille-3D: a large and high-quality ground truth urban point cloud dataset for automatic segmentation and classification. (arXiv:1712.00032v1 [cs.LG])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00032"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00032", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper introduces a new Urban Point Cloud Dataset for Automatic \nSegmentation and Classification acquired by Mobile Laser Scanning (MLS). We \ndescribe how the dataset is obtained from acquisition to post-processing and \nlabeling. This dataset can be used to learn classification algorithm, however, \ngiven that a great attention has been paid to the split between the different \nobjects, this dataset can also be used to learn the segmentation. The dataset \nconsists of around 2km of MLS point cloud acquired in two cities. The number of \npoints and range of classes make us consider that it can be used to train \nDeep-Learning methods. Besides we show some results of automatic segmentation \nand classification. The dataset is available at: \n<a href=\"http://caor-mines-paristech.fr/fr/paris-lille-3d-dataset/\">this http URL</a> \n</p>"}, "author": "Xavier Roynard, Jean-Emmanuel Deschaud, Fran&#xe7;ois Goulette", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369370", "id": "tag:google.com,2005:reader/item/000000033fb6430e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards Personalized Modeling of the Female Hormonal Cycle: Experiments with Mechanistic Models and Gaussian Processes. (arXiv:1712.00117v1 [stat.ML])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00117"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00117", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we introduce a novel task for machine learning in healthcare, \nnamely personalized modeling of the female hormonal cycle. The motivation for \nthis work is to model the hormonal cycle and predict its phases in time, both \nfor healthy individuals and for those with disorders of the reproductive \nsystem. Because there are individual differences in the menstrual cycle, we are \nparticularly interested in personalized models that can account for individual \nidiosyncracies, towards identifying phenotypes of menstrual cycles. As a first \nstep, we consider the hormonal cycle as a set of observations through time. We \nuse a previously validated mechanistic model to generate realistic hormonal \npatterns, and experiment with Gaussian process regression to estimate their \nvalues over time. Specifically, we are interested in the feasibility of \npredicting menstrual cycle phases under varying learning conditions: number of \ncycles used for training, hormonal measurement noise and sampling rates, and \ninformed vs. agnostic sampling of hormonal measurements. Our results indicate \nthat Gaussian processes can help model the female menstrual cycle. We discuss \nthe implications of our experiments in the context of modeling the female \nmenstrual cycle. \n</p>"}, "author": "I&#xf1;igo Urteaga, David J. Albers, Marija Vlajic Wheeler, Anna Druet, Hans Raffauf, No&#xe9;mie Elhadad", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369369", "id": "tag:google.com,2005:reader/item/000000033fb6432a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Label Efficient Learning of Transferable Representations across Domains and Tasks. (arXiv:1712.00123v1 [stat.ML])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00123"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00123", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a framework that learns a representation transferable across \ndifferent domains and tasks in a label efficient manner. Our approach battles \ndomain shift with a domain adversarial loss, and generalizes the embedding to \nnovel task using a metric learning-based approach. Our model is simultaneously \noptimized on labeled source data and unlabeled or sparsely labeled data in the \ntarget domain. Our method shows compelling results on novel classes within a \nnew domain even when only a few labeled examples per class are available, \noutperforming the prevalent fine-tuning approach. In addition, we demonstrate \nthe effectiveness of our framework on the transfer learning task from image \nobject recognition to video action recognition. \n</p>"}, "author": "Zelun Luo, Yuliang Zou, Judy Hoffman, Li Fei-Fei", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369368", "id": "tag:google.com,2005:reader/item/000000033fb6434b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An interpretable latent variable model for attribute applicability in the Amazon catalogue. (arXiv:1712.00126v2 [stat.ML] UPDATED)", "published": 1512450844, "updated": 1512450854, "canonical": [{"href": "http://arxiv.org/abs/1712.00126"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00126", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning attribute applicability of products in the Amazon catalog (e.g., \npredicting that a shoe should have a value for size, but not for battery-type \nat scale is a challenge. The need for an interpretable model is contingent on \n(1) the lack of ground truth training data, (2) the need to utilise prior \ninformation about the underlying latent space and (3) the ability to understand \nthe quality of predictions on new, unseen data. To this end, we develop the \nMaxMachine, a probabilistic latent variable model that learns distributed \nbinary representations, associated to sets of features that are likely to \nco-occur in the data. Layers of MaxMachines can be stacked such that higher \nlayers encode more abstract information. Any set of variables can be clamped to \nencode prior information. We develop fast sampling based posterior inference. \nPreliminary results show that the model improves over the baseline in 17 out of \n19 product groups and provides qualitatively reasonable predictions. \n</p>"}, "author": "Tammo Rukat, Dustin Lange, C&#xe9;dric Archambeau", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369367", "id": "tag:google.com,2005:reader/item/000000033fb64371", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Susceptibility Propagation by Using Diagonal Consistency. (arXiv:1712.00155v1 [cond-mat.stat-mech])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00155"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00155", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A susceptibility propagation that is constructed by combining a belief \npropagation and a linear response method is used for approximate computation \nfor Markov random fields. Herein, we formulate a new, improved susceptibility \npropagation by using the concept of a diagonal matching method that is based on \nmean-field approaches to inverse Ising problems. The proposed susceptibility \npropagation is robust for various network structures, and it is reduced to the \nordinary susceptibility propagation and to the adaptive \nThouless-Anderson-Palmer equation in special cases. \n</p>"}, "author": "Muneki Yasuda, Kazuyuki Tanaka", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369366", "id": "tag:google.com,2005:reader/item/000000033fb643cd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generative Adversarial Networks for Electronic Health Records: A Framework for Exploring and Evaluating Methods for Predicting Drug-Induced Laboratory Test Trajectories. (arXiv:1712.00164v1 [cs.LG])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00164"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00164", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative Adversarial Networks (GANs) represent a promising class of \ngenerative networks that combine neural networks with game theory. From \ngenerating realistic images and videos to assisting musical creation, GANs are \ntransforming many fields of arts and sciences. However, their application to \nhealthcare has not been fully realized, more specifically in generating \nelectronic health records (EHR) data. In this paper, we propose a framework for \nexploring the value of GANs in the context of continuous laboratory time series \ndata. We devise an unsupervised evaluation method that measures the predictive \npower of synthetic laboratory test time series. Further, we show that when it \ncomes to predicting the impact of drug exposure on laboratory test data, \nincorporating representation learning of the training cohorts prior to training \nGAN models is beneficial. \n</p>"}, "author": "Alexandre Yahi, Rami Vanguri, No&#xe9;mie Elhadad, Nicholas P. Tatonetti", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369365", "id": "tag:google.com,2005:reader/item/000000033fb643f1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Speaker identification from the sound of the human breath. (arXiv:1712.00171v1 [cs.SD])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00171"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00171", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper examines the speaker identification potential of breath sounds in \ncontinuous speech. Speech is largely produced during exhalation. In order to \nreplenish air in the lungs, speakers must periodically inhale. When inhalation \noccurs in the midst of continuous speech, it is generally through the mouth. \nIntra-speech breathing behavior has been the subject of much study, including \nthe patterns, cadence, and variations in energy levels. However, an often \nignored characteristic is the {\\em sound} produced during the inhalation phase \nof this cycle. Intra-speech inhalation is rapid and energetic, performed with \nopen mouth and glottis, effectively exposing the entire vocal tract to enable \nmaximum intake of air. This results in vocal tract resonances evoked by \nturbulence that are characteristic of the speaker's speech-producing apparatus. \nConsequently, the sounds of inhalation are expected to carry information about \nthe speaker's identity. Moreover, unlike other spoken sounds which are subject \nto active control, inhalation sounds are generally more natural and less \naffected by voluntary influences. The goal of this paper is to demonstrate that \nbreath sounds are indeed bio-signatures that can be used to identify speakers. \nWe show that these sounds by themselves can yield remarkably accurate speaker \nrecognition with appropriate feature representations and classification \nframeworks. \n</p>"}, "author": "Wenbo Zhao, Yang Gao, Rita Singh, Ming Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369364", "id": "tag:google.com,2005:reader/item/000000033fb64419", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Rapid point-of-care Hemoglobin measurement through low-cost optics and Convolutional Neural Network based validation. (arXiv:1712.00174v1 [physics.med-ph])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00174"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00174", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A low-cost, robust, and simple mechanism to measure hemoglobin would play a \ncritical role in the modern health infrastructure. Consistent sample \nacquisition has been a long-standing technical hurdle for photometer-based \nportable hemoglobin detectors which rely on micro cuvettes and dry chemistry. \nAny particulates (e.g. intact red blood cells (RBCs), microbubbles, etc.) in a \ncuvette's sensing area drastically impact optical absorption profile, and \ncommercial hemoglobinometers lack the ability to automatically detect faulty \nsamples. We present the ground-up development of a portable, low-cost and open \nplatform with equivalent accuracy to medical-grade devices, with the addition \nof CNN-based image processing for rapid sample viability prechecks. The \ndeveloped platform has demonstrated precision to the nearest $0.18[g/dL]$ of \nhemoglobin, an R^2 = 0.945 correlation to hemoglobin absorption curves reported \nin literature, and a 97% detection accuracy of poorly-prepared samples. We see \nthe developed hemoglobin device/ML platform having massive implications in \nrural medicine, and consider it an excellent springboard for robust deep \nlearning optical spectroscopy: a currently untapped source of data for \ndetection of countless analytes. \n</p>"}, "author": "Chris Wu, Tanay Tandon", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369363", "id": "tag:google.com,2005:reader/item/000000033fb6442e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Personalized Gaussian Processes for Future Prediction of Alzheimer's Disease Progression. (arXiv:1712.00181v1 [cs.LG])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00181"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00181", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we introduce the use of a personalized Gaussian Process model \n(pGP) to predict the key metrics of Alzheimer's Disease progression (MMSE, \nADAS-Cog13, CDRSB and CS) based on each patient's previous visits. We start by \nlearning a population-level model using multi-modal data from previously seen \npatients using the base Gaussian Process (GP) regression. Then, this model is \nadapted sequentially over time to a new patient using domain adaptive GPs to \nform the patient's pGP. We show that this new approach, together with an \nauto-regressive formulation, leads to significant improvements in forecasting \nfuture clinical status and cognitive scores for target patients when compared \nto modeling the population with traditional GPs. \n</p>"}, "author": "Kelly Peterson, Ognjen (Oggi) Rudovic, Ricardo Guerrero, Rosalind W. Picard", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369362", "id": "tag:google.com,2005:reader/item/000000033fb64443", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Tensors, Learning, and 'Kolmogorov Extension' for Finite-alphabet Random Vectors. (arXiv:1712.00205v1 [eess.SP])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00205"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00205", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250575b364\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250575b364&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Estimating the joint probability mass function (PMF) of a set of random \nvariables lies at the heart of statistical learning and signal processing. \nWithout structural assumptions, such as modeling the variables as a Markov \nchain, tree, or other graphical model, joint PMF estimation is often considered \nmission impossible - the number of unknowns grows exponentially with the number \nof variables. But who gives us the structural model? Is there a generic, \n'non-parametric' way to control joint PMF complexity without relying on a \npriori structural assumptions regarding the underlying probability model? Is it \npossible to discover the operational structure without biasing the analysis up \nfront? What if we only observe random subsets of the variables, can we still \nreliably estimate the joint PMF of all? This paper shows, perhaps surprisingly, \nthat if the joint PMF of any three variables can be estimated, then the joint \nPMF of all the variables can be provably recovered under relatively mild \nconditions. The result is reminiscent of Kolmogorov's extension theorem - \nconsistent specification of lower-order distributions induces a unique \nprobability measure for the entire process. The difference is that for \nprocesses of limited complexity (rank of the high-order PMF) it is possible to \nobtain complete characterization from only third-order distributions. In fact \nnot all third order PMFs are needed; and under more stringent conditions even \nsecond-order will do. Exploiting multilinear (tensor) algebra, this paper \nproves that such higher-order PMF completion can be guaranteed - several \npertinent identifiability results are derived. It also provides a practical and \nefficient algorithm to carry out the recovery task. Judiciously designed \nsimulations and real-data experiments on movie recommendation and data \nclassification are presented to showcase the effectiveness of the approach. \n</p>"}, "author": "Nikos Kargas, Nicholas D. Sidiropoulos, Xiao Fu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369361", "id": "tag:google.com,2005:reader/item/000000033fb64465", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimal Algorithms for Distributed Optimization. (arXiv:1712.00232v1 [math.OC])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00232"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00232", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325057c7752\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325057c7752&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we study the optimal convergence rate for distributed convex \noptimization problems in networks. We model the communication restrictions \nimposed by the network as a set of affine constraints and provide optimal \ncomplexity bounds for four different setups, namely: the function $F(\\xb) \n\\triangleq \\sum_{i=1}^{m}f_i(\\xb)$ is strongly convex and smooth, either \nstrongly convex or smooth or just convex. Our results show that Nesterov's \naccelerated gradient descent on the dual problem can be executed in a \ndistributed manner and obtains the same optimal rates as in the centralized \nversion of the problem (up to constant or logarithmic factors) with an \nadditional cost related to the spectral gap of the interaction matrix. Finally, \nwe discuss some extensions to the proposed setup such as proximal friendly \nfunctions, time-varying graphs, improvement of the condition numbers. \n</p>"}, "author": "C&#xe9;sar A. Uribe, Soomin Lee, Alexander Gasnikov, Angelia Nedi&#x107;", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369360", "id": "tag:google.com,2005:reader/item/000000033fb64476", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Utilizing Domain Knowledge in End-to-End Audio Processing. (arXiv:1712.00254v1 [cs.SD])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00254"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00254", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>End-to-end neural network based approaches to audio modelling are generally \noutperformed by models trained on high-level data representations. In this \npaper we present preliminary work that shows the feasibility of training the \nfirst layers of a deep convolutional neural network (CNN) model to learn the \ncommonly-used log-scaled mel-spectrogram transformation. Secondly, we \ndemonstrate that upon initializing the first layers of an end-to-end CNN \nclassifier with the learned transformation, convergence and performance on the \nESC-50 environmental sound classification dataset are similar to a CNN-based \nmodel trained on the highly pre-processed log-scaled mel-spectrogram features. \n</p>"}, "author": "Tycho Max Sylvester Tax, Jose Luis Diez Antich, Hendrik Purwins, Lars Maal&#xf8;e", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369359", "id": "tag:google.com,2005:reader/item/000000033fb64488", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "GANosaic: Mosaic Creation with Generative Texture Manifolds. (arXiv:1712.00269v1 [cs.CV])", "published": 1512364308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00269"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00269", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a novel framework for generating texture mosaics with \nconvolutional neural networks. Our method is called GANosaic and performs \noptimization in the latent noise space of a generative texture model, which \nallows the transformation of a content image into a mosaic exhibiting the \nvisual properties of the underlying texture manifold. To represent that \nmanifold, we use a state-of-the-art generative adversarial method for texture \nsynthesis, which can learn expressive texture representations from data and \nproduce mosaic images with very high resolution. This fully convolutional model \ngenerates smooth (without any visible borders) mosaic images which morph and \nblend different textures locally. In addition, we develop a new type of \ndifferentiable statistical regularization appropriate for optimization over the \nprior noise space of the PSGAN model. \n</p>"}, "author": "Nikolay Jetchev, Urs Bergmann, Calvin Seward", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369358", "id": "tag:google.com,2005:reader/item/000000033fb64494", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Faithful Model Inversion Substantially Improves Auto-encoding Variational Inference. (arXiv:1712.00287v1 [stat.ML])", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00287"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00287", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In learning deep generative models, the encoder for variational inference is \ntypically formed in an ad hoc manner with a structure and parametrization \nanalogous to the forward model. Our chief insight is that this results in \ncoarse approximations to the posterior, and that the d-separation properties of \nthe BN structure of the forward model should be used, in a principled way, to \nproduce ones that are faithful to the posterior, for which we introduce the \nnovel Compact Minimal I-map (CoMI) algorithm. Applying our method to common \nmodels reveals that standard encoder design choices lack many important edges, \nand through experiments we demonstrate that modelling these edges is important \nfor optimal learning. We show how using a faithful encoder is crucial when \nmodelling with continuous relaxations of categorical distributions. \n</p>"}, "author": "Stefan Webb, Adam Golinski, Robert Zinkov, N. Siddharth, Yee Whye Teh, Frank Wood", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369357", "id": "tag:google.com,2005:reader/item/000000033fb644a9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Prior and Likelihood Choices for Bayesian Matrix Factorisation on Small Datasets. (arXiv:1712.00288v1 [stat.ML])", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00288"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00288", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we study the effects of different prior and likelihood choices \nfor Bayesian matrix factorisation, focusing on small datasets. These choices \ncan greatly influence the predictive performance of the methods. We identify \nfour groups of approaches: Gaussian-likelihood with real-valued priors, \nnonnegative priors, semi-nonnegative models, and finally Poisson-likelihood \napproaches. For each group we review several models from the literature, \nconsidering sixteen in total, and discuss the relations between different \npriors and matrix norms. We extensively compare these methods on eight \nreal-world datasets across three application areas, giving both inter- and \nintra-group comparisons. We measure convergence runtime speed, cross-validation \nperformance, sparse and noisy prediction performance, and model selection \nrobustness. We offer several insights into the trade-offs between prior and \nlikelihood choices for Bayesian matrix factorisation on small datasets - such \nas that Poisson models give poor predictions, and that nonnegative models are \nmore constrained than real-valued ones. \n</p>"}, "author": "Thomas Brouwer, Pietro Lio&#x27;", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369356", "id": "tag:google.com,2005:reader/item/000000033fb644b3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning with Permutation-invariant Operator for Multi-instance Histopathology Classification. (arXiv:1712.00310v2 [cs.LG] UPDATED)", "published": 1512537802, "updated": 1512537806, "canonical": [{"href": "http://arxiv.org/abs/1712.00310"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00310", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The computer-aided analysis of medical scans is a longstanding goal in the \nmedical imaging field. Currently, deep learning has became a dominant \nmethodology for supporting pathologists and radiologist. Deep learning \nalgorithms have been successfully applied to digital pathology and radiology, \nnevertheless, there are still practical issues that prevent these tools to be \nwidely used in practice. The main obstacles are low number of available cases \nand large size of images (a.k.a. the small n, large p problem in machine \nlearning), and a very limited access to annotation at a pixel level that can \nlead to severe overfitting and large computational requirements. We propose to \nhandle these issues by introducing a framework that processes a medical image \nas a collection of small patches using a single, shared neural network. The \nfinal diagnosis is provided by combining scores of individual patches using a \npermutation-invariant operator (combination). In machine learning community \nsuch approach is called a multi-instance learning (MIL). \n</p>"}, "author": "Jakub M. Tomczak, Maximilian Ilse, Max Welling", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369355", "id": "tag:google.com,2005:reader/item/000000033fb644bf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Folded Recurrent Neural Networks for Future Video Prediction. (arXiv:1712.00311v1 [cs.CV])", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00311"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00311", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Future video prediction is an ill-posed Computer Vision problem that recently \nreceived much attention. Its main challenges are the high variability in video \ncontent, the propagation of errors through time, and the non-specificity of the \nfuture frames: given a sequence of past frames there is a continuous \ndistribution of possible futures. This work introduces bijective Gated \nRecurrent Units, a double mapping between the input and output of a GRU layer. \nThis allows for recurrent auto-encoders with state sharing between encoder and \ndecoder, stratifying the sequence representation and helping to prevent \ncapacity problems. We show how with this topology only the encoder or decoder \nneeds to be applied for input encoding and prediction, respectively. This \nreduces the computational cost and avoids re-encoding the predictions when \ngenerating a sequence of frames, mitigating the propagation of errors. \nFurthermore, it is possible to remove layers from an already trained model, \ngiving an insight to the role performed by each layer and making the model more \nexplainable. We evaluate our approach on three video datasets, outperforming \nstate of the art prediction results on MMNIST and UCF101, and obtaining \ncompetitive results on KTH with 2 and 3 times less memory usage and \ncomputational cost than the best scored approach. \n</p>"}, "author": "Marc Oliu, Javier Selva, Sergio Escalera", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369354", "id": "tag:google.com,2005:reader/item/000000033fb644c5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Group Sparse Bayesian Learning for Active Surveillance on Epidemic Dynamics. (arXiv:1712.00328v1 [stat.ML])", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00328"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00328", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Predicting epidemic dynamics is of great value in understanding and \ncontrolling diffusion processes, such as infectious disease spread and \ninformation propagation. This task is intractable, especially when surveillance \nresources are very limited. To address the challenge, we study the problem of \nactive surveillance, i.e., how to identify a small portion of system components \nas sentinels to effect monitoring, such that the epidemic dynamics of an entire \nsystem can be readily predicted from the partial data collected by such \nsentinels. We propose a novel measure, the gamma value, to identify the \nsentinels by modeling a sentinel network with row sparsity structure. We design \na flexible group sparse Bayesian learning algorithm to mine the sentinel \nnetwork suitable for handling both linear and non-linear dynamical systems by \nusing the expectation maximization method and variational approximation. The \nefficacy of the proposed algorithm is theoretically analyzed and empirically \nvalidated using both synthetic and real-world data. \n</p>"}, "author": "Hongbin Pei, Bo Yang, Jiming Liu, Lei Dong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369353", "id": "tag:google.com,2005:reader/item/000000033fb644ca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hierarchical Bayesian image analysis: from low-level modeling to robust supervised learning. (arXiv:1712.00368v1 [cs.CV])", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00368"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00368", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Within a supervised classification framework, labeled data are used to learn \nclassifier parameters. Prior to that, it is generally required to perform \ndimensionality reduction via feature extraction. These preprocessing steps have \nmotivated numerous research works aiming at recovering latent variables in an \nunsupervised context. This paper proposes a unified framework to perform \nclassification and low-level modeling jointly. The main objective is to use the \nestimated latent variables as features for classification and to incorporate \nsimultaneously supervised information to help latent variable extraction. The \nproposed hierarchical Bayesian model is divided into three stages: a first \nlow-level modeling stage to estimate latent variables, a second stage \nclustering these features into statistically homogeneous groups and a last \nclassification stage exploiting the (possibly badly) labeled data. Performance \nof the model is assessed in the specific context of hyperspectral image \ninterpretation, unifying two standard analysis techniques, namely unmixing and \nclassification. \n</p>"}, "author": "Adrien Lagrange, Mathieu Fauvel, St&#xe9;phane May, Nicolas Dobigeon", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369352", "id": "tag:google.com,2005:reader/item/000000033fb644d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning Scaling is Predictable, Empirically. (arXiv:1712.00409v1 [cs.LG])", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00409"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00409", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325057c7b1a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325057c7b1a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep learning (DL) creates impactful advances following a virtuous recipe: \nmodel architecture search, creating large training data sets, and scaling \ncomputation. It is widely believed that growing training sets and models should \nimprove accuracy and result in better products. As DL application domains grow, \nwe would like a deeper understanding of the relationships between training set \nsize, computational scale, and model accuracy improvements to advance the \nstate-of-the-art. \n</p> \n<p>This paper presents a large scale empirical characterization of \ngeneralization error and model size growth as training sets grow. We introduce \na methodology for this measurement and test four machine learning domains: \nmachine translation, language modeling, image processing, and speech \nrecognition. Our empirical results show power-law generalization error scaling \nacross a breadth of factors, resulting in power-law exponents---the \"steepness\" \nof the learning curve---yet to be explained by theoretical work. Further, model \nimprovements only shift the error but do not appear to affect the power-law \nexponent. We also show that model size scales sublinearly with data size. These \nscaling relationships have significant implications on deep learning research, \npractice, and systems. They can assist model debugging, setting accuracy \ntargets, and decisions about data set growth. They can also guide computing \nsystem design and underscore the importance of continued computational scaling. \n</p>"}, "author": "Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, Yanqi Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369351", "id": "tag:google.com,2005:reader/item/000000033fb644db", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The reparameterization trick for acquisition functions. (arXiv:1712.00424v1 [stat.ML])", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00424"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00424", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Bayesian optimization is a sample-efficient approach to solving global \noptimization problems. Along with a surrogate model, this approach relies on \ntheoretically motivated value heuristics (acquisition functions) to guide the \nsearch process. Maximizing acquisition functions yields the best performance; \nunfortunately, this ideal is difficult to achieve since optimizing acquisition \nfunctions per se is frequently non-trivial. This statement is especially true \nin the parallel setting, where acquisition functions are routinely non-convex, \nhigh-dimensional, and intractable. Here, we demonstrate how many popular \nacquisition functions can be formulated as Gaussian integrals amenable to the \nreparameterization trick and, ensuingly, gradient-based optimization. Further, \nwe use this reparameterized representation to derive an efficient Monte Carlo \nestimator for the upper confidence bound acquisition function in the context of \nparallel selection. \n</p>"}, "author": "James T. Wilson, Riccardo Moriconi, Frank Hutter, Marc Peter Deisenroth", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369350", "id": "tag:google.com,2005:reader/item/000000033fb644e5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Neural Network Architectures for Modulation Classification. (arXiv:1712.00443v1 [cs.LG])", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.00443"}], "alternate": [{"href": "http://arxiv.org/abs/1712.00443", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work, we investigate the value of employing deep learning for the \ntask of wireless signal modulation recognition. Recently in [1], a framework \nhas been introduced by generating a dataset using GNU radio that mimics the \nimperfections in a real wireless channel, and uses 11 different modulation \ntypes. Further, a convolutional neural network (CNN) architecture was developed \nand shown to deliver performance that exceeds that of expert-based approaches. \nHere, we follow the framework of [1] and find deep neural network architectures \nthat deliver higher accuracy than the state of the art. We tested the \narchitecture of [1] and found it to achieve an accuracy of approximately 75% of \ncorrectly recognizing the modulation type. We first tune the CNN architecture \nof [1] and find a design with four convolutional layers and two dense layers \nthat gives an accuracy of approximately 83.8% at high SNR. We then develop \narchitectures based on the recently introduced ideas of Residual Networks \n(ResNet [2]) and Densely Connected Networks (DenseNet [3]) to achieve high SNR \naccuracies of approximately 83.5% and 86.6%, respectively. Finally, we \nintroduce a Convolutional Long Short-term Deep Neural Network (CLDNN [4]) to \nachieve an accuracy of approximately 88.5% at high SNR. \n</p>"}, "author": "Xiaoyu Liu, Diyu Yang, Aly El Gamal", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369349", "id": "tag:google.com,2005:reader/item/000000033fb644eb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian inference for spatio-temporal spike-and-slab priors. (arXiv:1509.04752v3 [stat.ML] UPDATED)", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1509.04752"}], "alternate": [{"href": "http://arxiv.org/abs/1509.04752", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work, we address the problem of solving a series of underdetermined \nlinear inverse problems subject to a sparsity constraint. We generalize the \nspike-and-slab prior distribution to encode a priori correlation of the support \nof the solution in both space and time by imposing a transformed Gaussian \nprocess on the spike-and-slab probabilities. An expectation propagation (EP) \nalgorithm for posterior inference under the proposed model is derived. For \nlarge scale problems, the standard EP algorithm can be prohibitively slow. We \ntherefore introduce three different approximation schemes to reduce the \ncomputational complexity. Finally, we demonstrate the proposed model using \nnumerical experiments based on both synthetic and real data sets. \n</p>"}, "author": "Michael Riis Andersen, Aki Vehtari, Ole Winther, Lars Kai Hansen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369348", "id": "tag:google.com,2005:reader/item/000000033fb644fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hierarchical Sparse Modeling: A Choice of Two Group Lasso Formulations. (arXiv:1512.01631v4 [stat.ME] UPDATED)", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1512.01631"}], "alternate": [{"href": "http://arxiv.org/abs/1512.01631", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Demanding sparsity in estimated models has become a routine practice in \nstatistics. In many situations, we wish to require that the sparsity patterns \nattained honor certain problem-specific constraints. Hierarchical sparse \nmodeling (HSM) refers to situations in which these constraints specify that one \nset of parameters be set to zero whenever another is set to zero. In recent \nyears, numerous papers have developed convex regularizers for this form of \nsparsity structure, which arises in many areas of statistics including \ninteraction modeling, time series analysis, and covariance estimation. In this \npaper, we observe that these methods fall into two frameworks, the group lasso \n(GL) and latent overlapping group lasso (LOG), which have not been \nsystematically compared in the context of HSM. The purpose of this paper is to \nprovide a side-by-side comparison of these two frameworks for HSM in terms of \ntheir statistical properties and computational efficiency. We call special \nattention to GL's more aggressive shrinkage of parameters deep in the \nhierarchy, a property not shared by LOG. In terms of computation, we introduce \na finite-step algorithm that exactly solves the proximal operator of LOG for a \ncertain simple HSM structure; we later exploit this to develop a novel \npath-based block coordinate descent scheme for general HSM structures. Both \nalgorithms greatly improve the computational performance of LOG. Finally, we \ncompare the two methods in the context of covariance estimation, where we \nintroduce a new sparsely-banded estimator using LOG, which we show achieves the \nstatistical advantages of an existing GL-based method but is simpler to express \nand more efficient to compute. \n</p>"}, "author": "Xiaohan Yan, Jacob Bien", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369347", "id": "tag:google.com,2005:reader/item/000000033fb6451a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Universal Joint Image Clustering and Registration using Partition Information. (arXiv:1701.02776v2 [cs.IT] UPDATED)", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1701.02776"}], "alternate": [{"href": "http://arxiv.org/abs/1701.02776", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of universal joint clustering and registration of \nimages and define algorithms using multivariate information functionals. We \nfirst study registering two images using maximum mutual information and prove \nits asymptotic optimality. We then show the shortcomings of pairwise \nregistration in multi-image registration, and design an asymptotically optimal \nalgorithm based on multiinformation. Further, we define a novel multivariate \ninformation functional to perform joint clustering and registration of images, \nand prove consistency of the algorithm. Finally, we consider registration and \nclustering of numerous limited-resolution images, defining algorithms that are \norder-optimal in scaling of number of pixels in each image with the number of \nimages. \n</p>"}, "author": "Ravi Kiran Raman, Lav R. Varshney", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369346", "id": "tag:google.com,2005:reader/item/000000033fb64538", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Audio-Visual Speech Enhancement based on Multimodal Deep Convolutional Neural Networks. (arXiv:1703.10893v4 [cs.SD] UPDATED)", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.10893"}], "alternate": [{"href": "http://arxiv.org/abs/1703.10893", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Speech enhancement (SE) aims to reduce noise in speech signals. Most SE \ntechniques focus only on addressing audio information. In this work, inspired \nby multimodal learning, which utilizes data from different modalities, and the \nrecent success of convolutional neural networks (CNNs) in SE, we propose an \naudio-visual deep CNNs (AVDCNN) SE model, which incorporates audio and visual \nstreams into a unified network model. We also propose a multi-task learning \nframework for reconstructing audio and visual signals at the output layer. \nPrecisely speaking, the proposed AVDCNN model is structured as an audio-visual \nencoder-decoder network, in which audio and visual data are first processed \nusing individual CNNs, and then fused into a joint network to generate enhanced \nspeech (the primary task) and reconstructed images (the secondary task) at the \noutput layer. The model is trained in an end-to-end manner, and parameters are \njointly learned through back-propagation. We evaluate enhanced speech using \nfive instrumental criteria. Results show that the AVDCNN model yields a notably \nsuperior performance compared with an audio-only CNN-based SE model and two \nconventional SE approaches, confirming the effectiveness of integrating visual \ninformation into the SE process. In addition, the AVDCNN model also outperforms \nan existing audio-visual SE model, confirming its capability of effectively \ncombining audio and visual information in SE. \n</p>"}, "author": "Jen-Cheng Hou, Syu-Siang Wang, Ying-Hui Lai, Yu Tsao, Hsiu-Wen Chang, Hsin-Min Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369345", "id": "tag:google.com,2005:reader/item/000000033fb6455b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Certifiably Optimal Rule Lists for Categorical Data. (arXiv:1704.01701v2 [stat.ML] UPDATED)", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1704.01701"}], "alternate": [{"href": "http://arxiv.org/abs/1704.01701", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present the design and implementation of a custom discrete optimization \ntechnique for building rule lists over a categorical feature space. Our \nalgorithm produces rule lists with optimal training performance, according to \nthe regularized empirical risk, with a certificate of optimality. By leveraging \nalgorithmic bounds, efficient data structures, and computational reuse, we \nachieve several orders of magnitude speedup in time and a massive reduction of \nmemory consumption. We demonstrate that our approach produces optimal rule \nlists on practical problems in seconds. Our results indicate that it is \npossible to construct optimal sparse rule lists that are approximately as \naccurate as the COMPAS proprietary risk prediction tool on data from Broward \nCounty, Florida, but that are completely interpretable. This framework is a \nnovel alternative to CART and other decision tree methods for interpretable \nmodeling. \n</p>"}, "author": "Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, Cynthia Rudin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369344", "id": "tag:google.com,2005:reader/item/000000033fb6456e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Expected Policy Gradients. (arXiv:1706.05374v4 [stat.ML] UPDATED)", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.05374"}], "alternate": [{"href": "http://arxiv.org/abs/1706.05374", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose expected policy gradients (EPG), which unify stochastic policy \ngradients (SPG) and deterministic policy gradients (DPG) for reinforcement \nlearning. Inspired by expected sarsa, EPG integrates across the action when \nestimating the gradient, instead of relying only on the action in the sampled \ntrajectory. We establish a new general policy gradient theorem, of which the \nstochastic and deterministic policy gradient theorems are special cases. We \nalso prove that EPG reduces the variance of the gradient estimates without \nrequiring deterministic policies and, for the Gaussian case, with no \ncomputational overhead. Finally, we show that it is optimal in a certain sense \nto explore with a Gaussian policy such that the covariance is proportional to \nthe exponential of the scaled Hessian of the critic with respect to the \nactions. We present empirical results confirming that this new form of \nexploration substantially outperforms DPG with the Ornstein-Uhlenbeck heuristic \nin four challenging MuJoCo domains. \n</p>"}, "author": "Kamil Ciosek, Shimon Whiteson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369343", "id": "tag:google.com,2005:reader/item/000000033fb64586", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Transferable Architectures for Scalable Image Recognition. (arXiv:1707.07012v3 [cs.CV] UPDATED)", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.07012"}], "alternate": [{"href": "http://arxiv.org/abs/1707.07012", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Developing neural network image classification models often requires \nsignificant architecture engineering. In this paper, we attempt to automate \nthis engineering process by learning the model architectures directly on the \ndataset of interest. As this approach is expensive when the dataset is large, \nwe propose to search for an architectural building block on a small dataset and \nthen transfer the block to a larger dataset. Our key contribution is the design \nof a new search space which enables transferability. In our experiments, we \nsearch for the best convolutional layer (or \"cell\") on the CIFAR-10 dataset and \nthen apply this cell to the ImageNet dataset by stacking together more copies \nof this cell, each with their own parameters. Although the cell is not searched \nfor directly on ImageNet, an architecture constructed from the best cell \nachieves, among the published works, state-of-the-art accuracy of 82.7% top-1 \nand 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than \nthe best human-invented architectures while having 9 billion fewer FLOPS -- a \nreduction of 28% in computational demand from the previous state-of-the-art \nmodel. When evaluated at different levels of computational cost, accuracies of \nour models exceed those of the state-of-the-art human-designed models. For \ninstance, a smaller network constructed from the best cell also achieves 74% \ntop-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art \nmodels for mobile platforms. On CIFAR-10, an architecture constructed from the \nbest cell achieves 2.4% error rate, which is also state-of-the-art. Finally, \nthe image features learned from image classification can also be transferred to \nother computer vision problems. On the task of object detection, the learned \nfeatures used with the Faster-RCNN framework surpass state-of-the-art by 4.0% \nachieving 43.1% mAP on the COCO dataset. \n</p>"}, "author": "Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369342", "id": "tag:google.com,2005:reader/item/000000033fb6459d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Inverse Ising problem in continuous time: A latent variable approach. (arXiv:1709.04495v2 [stat.ML] UPDATED)", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.04495"}], "alternate": [{"href": "http://arxiv.org/abs/1709.04495", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325057c7f19\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325057c7f19&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We consider the inverse Ising problem, i.e. the inference of network \ncouplings from observed spin trajectories for a model with continuous time \nGlauber dynamics. By introducing two sets of auxiliary latent random variables \nwe render the likelihood into a form, which allows for simple iterative \ninference algorithms with analytical updates. The variables are: (1) Poisson \nvariables to linearise an exponential term which is typical for point process \nlikelihoods and (2) P\\'olya-Gamma variables, which make the likelihood \nquadratic in the coupling parameters. Using the augmented likelihood, we derive \nan expectation-maximization (EM) algorithm to obtain the maximum likelihood \nestimate of network parameters. Using a third set of latent variables we extend \nthe EM algorithm to sparse couplings via L1 regularization. Finally, we develop \nan efficient approximate Bayesian inference algorithm using a variational \napproach. We demonstrate the performance of our algorithms on data simulated \nfrom an Ising model. For data which are simulated from a more biologically \nplausible network with spiking neurons, we show that the Ising model captures \nwell the low order statistics of the data and how the Ising couplings are \nrelated to the underlying synaptic structure of the simulated network. \n</p>"}, "author": "Christian Donner, Manfred Opper", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369341", "id": "tag:google.com,2005:reader/item/000000033fb645b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Improved Naive Bayes Classifier-based Noise Detection Technique for Classifying User Phone Call Behavior. (arXiv:1710.04461v2 [cs.LG] UPDATED)", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04461"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04461", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32505846d68\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32505846d68&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The presence of noisy instances in mobile phone data is a fundamental issue \nfor classifying user phone call behavior (i.e., accept, reject, missed and \noutgoing), with many potential negative consequences. The classification \naccuracy may decrease and the complexity of the classifiers may increase due to \nthe number of redundant training samples. To detect such noisy instances from a \ntraining dataset, researchers use naive Bayes classifier (NBC) as it identifies \nmisclassified instances by taking into account independence assumption and \nconditional probabilities of the attributes. However, some of these \nmisclassified instances might indicate usages behavioral patterns of individual \nmobile phone users. Existing naive Bayes classifier based noise detection \ntechniques have not considered this issue and, thus, are lacking in \nclassification accuracy. In this paper, we propose an improved noise detection \ntechnique based on naive Bayes classifier for effectively classifying users' \nphone call behaviors. In order to improve the classification accuracy, we \neffectively identify noisy instances from the training dataset by analyzing the \nbehavioral patterns of individuals. We dynamically determine a noise threshold \naccording to individual's unique behavioral patterns by using both the naive \nBayes classifier and Laplace estimator. We use this noise threshold to identify \nnoisy instances. To measure the effectiveness of our technique in classifying \nuser phone call behavior, we employ the most popular classification algorithm \n(e.g., decision tree). Experimental results on the real phone call log dataset \nshow that our proposed technique more accurately identifies the noisy instances \nfrom the training datasets that leads to better classification accuracy. \n</p>"}, "author": "Iqbal H. Sarker, Muhammad Ashad Kabir, Alan Colman, Jun Han", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369340", "id": "tag:google.com,2005:reader/item/000000033fb645cd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples. (arXiv:1711.09325v2 [stat.ML] UPDATED)", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.09325"}], "alternate": [{"href": "http://arxiv.org/abs/1711.09325", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The problem of detecting whether a test sample is from in-distribution (i.e., \ntraining distribution by a classifier) or out-of-distribution sufficiently \ndifferent from it arises in many real-world machine learning applications. \nHowever, the state-of-art deep neural networks are known to be highly \noverconfident in their predictions, i.e., do not distinguish in- and \nout-of-distributions. Recently, to handle this issue, several threshold-based \ndetectors have been proposed given pre-trained neural classifiers. However, the \nperformance of prior works highly depends on how to train the classifiers since \nthey only focus on improving inference procedures. In this paper, we develop a \nnovel training method for classifiers so that such inference algorithms can \nwork better. In particular, we suggest two additional terms added to the \noriginal loss (e.g., cross entropy). The first one forces samples from \nout-of-distribution less confident by the classifier and the second one is for \n(implicitly) generating most effective training samples for the first one. In \nessence, our method jointly trains both classification and generative neural \nnetworks for out-of-distribution. We demonstrate its effectiveness using deep \nconvolutional neural networks on various popular image datasets. \n</p>"}, "author": "Kimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512364308369", "timestampUsec": "1512364308369339", "id": "tag:google.com,2005:reader/item/000000033fb645da", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Predicting Adolescent Suicide Attempts with Neural Networks. (arXiv:1711.10057v2 [stat.ML] UPDATED)", "published": 1512364309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.10057"}], "alternate": [{"href": "http://arxiv.org/abs/1711.10057", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Though suicide is a major public health problem in the US, machine learning \nmethods are not commonly used to predict an individual's risk of \nattempting/committing suicide. In the present work, starting with an anonymized \ncollection of electronic health records for 522,056 unique, California-resident \nadolescents, we develop neural network models to predict suicide attempts. We \nframe the problem as a binary classification problem in which we use a \npatient's data from 2006-2009 to predict either the presence (1) or absence (0) \nof a suicide attempt in 2010. After addressing issues such as severely \nimbalanced classes and the variable length of a patient's history, we build \nneural networks with depths varying from two to eight hidden layers. For test \nset observations where we have at least five ED/hospital visits' worth of data \non a patient, our depth-4 model achieves a sensitivity of 0.703, specificity of \n0.980, and AUC of 0.958. \n</p>"}, "author": "Harish S. Bhat, Sidra J. Goldman-Mellor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936910", "id": "tag:google.com,2005:reader/item/000000033e2c9fea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Gaussian Process Neurons Learn Stochastic Activation Functions. (arXiv:1711.11059v1 [stat.ML])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11059"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11059", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose stochastic, non-parametric activation functions that are fully \nlearnable and individual to each neuron. Complexity and the risk of overfitting \nare controlled by placing a Gaussian process prior over these functions. The \nresult is the Gaussian process neuron, a probabilistic unit that can be used as \nthe basic building block for probabilistic graphical models that resemble the \nstructure of neural networks. The proposed model can intrinsically handle \nuncertainties in its inputs and self-estimate the confidence of its \npredictions. Using variational Bayesian inference and the central limit \ntheorem, a fully deterministic loss function is derived, allowing it to be \ntrained as efficiently as a conventional neural network using mini-batch \ngradient descent. The posterior distribution of activation functions is \ninferred from the training data alongside the weights of the network. \n</p> \n<p>The proposed model favorably compares to deep Gaussian processes, both in \nmodel complexity and efficiency of inference. It can be directly applied to \nrecurrent or convolutional network structures, allowing its use in audio and \nimage processing tasks. \n</p> \n<p>As an preliminary empirical evaluation we present experiments on regression \nand classification tasks, in which our model achieves performance comparable to \nor better than a Dropout regularized neural network with a fixed activation \nfunction. Experiments are ongoing and results will be added as they become \navailable. \n</p>"}, "author": "Sebastian Urban, Marcus Basalla, Patrick van der Smagt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936909", "id": "tag:google.com,2005:reader/item/000000033e2c9ff5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Quantum Neuron: an elementary building block for machine learning on quantum computers. (arXiv:1711.11240v1 [quant-ph])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11240"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11240", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Even the most sophisticated artificial neural networks are built by \naggregating substantially identical units called neurons. A neuron receives \nmultiple signals, internally combines them, and applies a non-linear function \nto the resulting weighted sum. Several attempts to generalize neurons to the \nquantum regime have been proposed, but all proposals collided with the \ndifficulty of implementing non-linear activation functions, which is essential \nfor classical neurons, due to the linear nature of quantum mechanics. Here we \npropose a solution to this roadblock in the form of a small quantum circuit \nthat naturally simulates neurons with threshold activation. Our quantum circuit \ndefines a building block, the \"quantum neuron\", that can reproduce a variety of \nclassical neural network constructions while maintaining the ability to process \nsuperpositions of inputs and preserve quantum coherence and entanglement. In \nthe construction of feedforward networks of quantum neurons, we provide \nnumerical evidence that the network not only can learn a function when trained \nwith superposition of inputs and the corresponding output, but that this \ntraining suffices to learn the function on all individual inputs separately. \nWhen arranged to mimic Hopfield networks, quantum neural networks exhibit \nproperties of associative memory. Patterns are encoded using the simple Hebbian \nrule for the weights and we demonstrate attractor dynamics from corrupted \ninputs. Finally, the fact that our quantum model closely captures (traditional) \nneural network dynamics implies that the vast body of literature and results on \nneural networks becomes directly relevant in the context of quantum machine \nlearning. \n</p>"}, "author": "Yudong Cao, Gian Giacomo Guerreschi, Al&#xe1;n Aspuru-Guzik", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936908", "id": "tag:google.com,2005:reader/item/000000033e2ca004", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Uncertainty Estimates for Efficient Neural Network-based Dialogue Policy Optimisation. (arXiv:1711.11486v1 [stat.ML])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11486"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11486", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In statistical dialogue management, the dialogue manager learns a policy that \nmaps a belief state to an action for the system to perform. Efficient \nexploration is key to successful policy optimisation. Current deep \nreinforcement learning methods are very promising but rely on epsilon-greedy \nexploration, thus subjecting the user to a random choice of action during \nlearning. Alternative approaches such as Gaussian Process SARSA (GPSARSA) \nestimate uncertainties and are sample efficient, leading to better user \nexperience, but on the expense of a greater computational complexity. This \npaper examines approaches to extract uncertainty estimates from deep Q-networks \n(DQN) in the context of dialogue management. We perform an extensive benchmark \nof deep Bayesian methods to extract uncertainty estimates, namely \nBayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble and \nalpha-divergences, combining it with DQN algorithm. \n</p>"}, "author": "Christopher Tegho, Pawe&#x142; Budzianowski, Milica Ga&#x161;i&#x107;", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936907", "id": "tag:google.com,2005:reader/item/000000033e2ca010", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. (arXiv:1703.01780v3 [cs.NE] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.01780"}], "alternate": [{"href": "http://arxiv.org/abs/1703.01780", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The recently proposed Temporal Ensembling has achieved state-of-the-art \nresults in several semi-supervised learning benchmarks. It maintains an \nexponential moving average of label predictions on each training example, and \npenalizes predictions that are inconsistent with this target. However, because \nthe targets change only once per epoch, Temporal Ensembling becomes unwieldy \nwhen learning large datasets. To overcome this problem, we propose Mean \nTeacher, a method that averages model weights instead of label predictions. As \nan additional benefit, Mean Teacher improves test accuracy and enables training \nwith fewer labels than Temporal Ensembling. Without changing the network \narchitecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250 \nlabels, outperforming Temporal Ensembling trained with 1000 labels. We also \nshow that a good network architecture is crucial to performance. Combining Mean \nTeacher and Residual Networks, we improve the state of the art on CIFAR-10 with \n4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels \nfrom 35.24% to 9.11%. \n</p>"}, "author": "Antti Tarvainen, Harri Valpola", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936906", "id": "tag:google.com,2005:reader/item/000000033e2ca01f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Happiness Pursuit: Personality Learning in a Society of Agents. (arXiv:1711.11068v2 [cs.MA] UPDATED)", "published": 1513055249, "updated": 1513055251, "canonical": [{"href": "http://arxiv.org/abs/1711.11068"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11068", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Modeling personality is a challenging problem with applications spanning \ncomputer games, virtual assistants, online shopping and education. Many \ntechniques have been tried, ranging from neural networks to computational \ncognitive architectures. However, most approaches rely on examples with \nhand-crafted features and scenarios. Here, we approach learning a personality \nby training agents using a Deep Q-Network (DQN) model on rewards based on \npsychoanalysis, against hand-coded AI in the game of Pong. As a result, we \nobtain 4 agents, each with its own personality. Then, we define happiness of an \nagent, which can be seen as a measure of alignment with agent's objective \nfunction, and study it when agents play both against hand-coded AI, and against \neach other. We find that the agents that achieve higher happiness during \ntesting against hand-coded AI, have lower happiness when competing against each \nother. This suggests that higher happiness in testing is a sign of overfitting \nin learning to interact with hand-coded AI, and leads to worse performance \nagainst agents with different personalities. \n</p>"}, "author": "Rafa&#x142; Muszy&#x144;ski, Jun Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936905", "id": "tag:google.com,2005:reader/item/000000033e2ca029", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improving Latent User Models in Online Social Media. (arXiv:1711.11124v1 [cs.SI])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11124"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11124", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Modern social platforms are characterized by the presence of rich \nuser-behavior data associated with the publication, sharing and consumption of \ntextual content. Users interact with content and with each other in a complex \nand dynamic social environment while simultaneously evolving over time. In \norder to effectively characterize users and predict their future behavior in \nsuch a setting, it is necessary to overcome several challenges. Content \nheterogeneity and temporal inconsistency of behavior data result in severe \nsparsity at the user level. In this paper, we propose a novel \nmutual-enhancement framework to simultaneously partition and learn latent \nactivity profiles of users. We propose a flexible user partitioning approach to \neffectively discover rare behaviors and tackle user-level sparsity. We \nextensively evaluate the proposed framework on massive datasets from real-world \nplatforms including Q&amp;A networks and interactive online courses (MOOCs). Our \nresults indicate significant gains over state-of-the-art behavior models ( 15% \navg ) in a varied range of tasks and our gains are further magnified for users \nwith limited interaction data. The proposed algorithms are amenable to \nparallelization, scale linearly in the size of datasets, and provide \nflexibility to model diverse facets of user behavior. \n</p>"}, "author": "Adit Krishnan, Ashish Sharma, Hari Sundaram", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936904", "id": "tag:google.com,2005:reader/item/000000033e2ca032", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Video Captioning via Hierarchical Reinforcement Learning. (arXiv:1711.11135v1 [cs.CV])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11135"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11135", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32505847176\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32505847176&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Video captioning is the task of automatically generating a textual \ndescription of the actions in a video. Although previous work (e.g. \nsequence-to-sequence model) has shown promising results in abstracting a coarse \ndescription of a short video, it is still very challenging to caption a video \ncontaining multiple fine-grained actions with a detailed description. This \npaper aims to address the challenge by proposing a novel hierarchical \nreinforcement learning framework for video captioning, where a high-level \nManager module learns to design sub-goals and a low-level Worker module \nrecognizes the primitive actions to fulfill the sub-goal. With this \ncompositional framework to reinforce video captioning at different levels, our \napproach significantly outperforms all the baseline methods on a newly \nintroduced large-scale dataset for fine-grained video captioning. Furthermore, \nour non-ensemble model has already achieved the state-of-the-art results on the \nwidely-used MSR-VTT dataset. \n</p>"}, "author": "Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang, William Yang Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936903", "id": "tag:google.com,2005:reader/item/000000033e2ca03d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Semantic Loss Function for Deep Learning with Symbolic Knowledge. (arXiv:1711.11157v1 [cs.AI])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11157"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11157", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper develops a novel methodology for using symbolic knowledge in deep \nlearning. From first principles, we derive a semantic loss function that \nbridges between neural output vectors and logical constraints. This loss \nfunction captures how close the neural network is to satisfying the constraints \non its output. An experimental evaluation shows that our semantic loss function \neffectively guides the learner to achieve (near-)state-of-the-art results on \nsemi-supervised multi-class classification. Moreover, it significantly \nincreases the ability of the neural network to predict structured objects, such \nas rankings and paths. These discrete concepts are tremendously difficult to \nlearn, and benefit from a tight integration of deep learning and symbolic \nreasoning methods. \n</p>"}, "author": "Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, Guy Van den Broeck", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936902", "id": "tag:google.com,2005:reader/item/000000033e2ca04e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards Data Quality Assessment in Online Advertising. (arXiv:1711.11175v1 [cs.AI])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11175"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11175", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In online advertising, our aim is to match the advertisers with the most \nrelevant users to optimize the campaign performance. In the pursuit of \nachieving this goal, multiple data sources provided by the advertisers or \nthird-party data providers are utilized to choose the set of users according to \nthe advertisers' targeting criteria. In this paper, we present a framework that \ncan be applied to assess the quality of such data sources in large scale. This \nframework efficiently evaluates the similarity of a specific data source \ncategorization to that of the ground truth, especially for those cases when the \nground truth is accessible only in aggregate, and the user-level information is \nanonymized or unavailable due to privacy reasons. We propose multiple \nmethodologies within this framework, present some preliminary assessment \nresults, and evaluate how the methodologies compare to each other. We also \npresent two use cases where we can utilize the data quality assessment results: \nthe first use case is targeting specific user categories, and the second one is \nforecasting the desirable audiences we can reach for an online advertising \ncampaign with pre-set targeting criteria. \n</p>"}, "author": "Sahin Cem Geyik, Jianqiang Shen, Shahriar Shariat, Ali Dasdan, Santanu Kolay", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936901", "id": "tag:google.com,2005:reader/item/000000033e2ca060", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improved Learning in Evolution Strategies via Sparser Inter-Agent Network Topologies. (arXiv:1711.11180v1 [cs.AI])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11180"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11180", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We draw upon a previously largely untapped literature on human collective \nintelligence as a source of inspiration for improving deep learning. Implicit \nin many algorithms that attempt to solve Deep Reinforcement Learning (DRL) \ntasks is the network of processors along which parameter values are shared. So \nfar, existing approaches have implicitly utilized fully-connected networks, in \nwhich all processors are connected. However, the scientific literature on human \ncollective intelligence suggests that complete networks may not always be the \nmost effective information network structures for distributed search through \ncomplex spaces. Here we show that alternative topologies can improve deep \nneural network training: we find that sparser networks learn higher rewards \nfaster, leading to learning improvements at lower communication costs. \n</p>"}, "author": "Dhaval Adjodah, Dan Calacci, Yan Leng, Peter Krafft, Esteban Moro, Alex Pentland", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936900", "id": "tag:google.com,2005:reader/item/000000033e2ca07b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Embedded Real-Time Fall Detection Using Deep Learning For Elderly Care. (arXiv:1711.11200v1 [stat.ML])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11200"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11200", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes a real-time embedded fall detection system using a \nDVS(Dynamic Vision Sensor) that has never been used for traditional fall \ndetection, a dataset for fall detection using that, and a DVS-TN(DVS-Temporal \nNetwork). The first contribution is building a DVS Falls Dataset, which made \nour network to recognize a much greater variety of falls than the existing \ndatasets that existed before and solved privacy issues using the DVS. Secondly, \nwe introduce the DVS-TN : optimized deep learning network to detect falls using \nDVS. Finally, we implemented a fall detection system which can run on \nlow-computing H/W with real-time, and tested on DVS Falls Dataset that takes \ninto account various falls situations. Our approach achieved 95.5% on the \nF1-score and operates at 31.25 FPS on NVIDIA Jetson TX1 board. \n</p>"}, "author": "Hyunwoo Lee, Jooyoung Kim, Dojun Yang, Joon-Ho Kim", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936899", "id": "tag:google.com,2005:reader/item/000000033e2ca094", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Variational Deep Q Network. (arXiv:1711.11225v1 [cs.LG])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11225"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11225", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a framework that directly tackles the probability distribution of \nthe value function parameters in Deep Q Network (DQN), with powerful \nvariational inference subroutines to approximate the posterior of the \nparameters. We will establish the equivalence between our proposed surrogate \nobjective and variational inference loss. Our new algorithm achieves efficient \nexploration and performs well on large scale chain Markov Decision Process \n(MDP). \n</p>"}, "author": "Yunhao Tang, Alp Kucukelbir", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936898", "id": "tag:google.com,2005:reader/item/000000033e2ca0a9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Knowledge Graph Embedding with Iterative Guidance from Soft Rules. (arXiv:1711.11231v1 [cs.AI])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11231"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11231", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of \ncurrent research. Combining such an embedding model with logic rules has \nrecently attracted increasing attention. Most previous attempts made a one-time \ninjection of logic rules, ignoring the interactive nature between embedding \nlearning and logical inference. And they focused only on hard rules, which \nalways hold with no exception and usually require extensive manual effort to \ncreate or validate. In this paper, we propose Rule-Guided Embedding (RUGE), a \nnovel paradigm of KG embedding with iterative guidance from soft rules. RUGE \nenables an embedding model to learn simultaneously from 1) labeled triples that \nhave been directly observed in a given KG, 2) unlabeled triples whose labels \nare going to be predicted iteratively, and 3) soft rules with various \nconfidence levels extracted automatically from the KG. In the learning process, \nRUGE iteratively queries rules to obtain soft labels for unlabeled triples, and \nintegrates such newly labeled triples to update the embedding model. Through \nthis iterative procedure, knowledge embodied in logic rules may be better \ntransferred into the learned embeddings. We evaluate RUGE in link prediction on \nFreebase and YAGO. Experimental results show that: 1) with rule knowledge \ninjected iteratively, RUGE achieves significant and consistent improvements \nover state-of-the-art baselines; and 2) despite their uncertainties, \nautomatically extracted soft rules are highly beneficial to KG embedding, even \nthose with moderate confidence levels. The code and data used for this paper \ncan be obtained from https://github.com/iieir-km/RUGE. \n</p>"}, "author": "Shu Guo, Quan Wang, Lihong Wang, Bin Wang, Li Guo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936897", "id": "tag:google.com,2005:reader/item/000000033e2ca0bf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning to Compose Skills. (arXiv:1711.11289v1 [cs.AI])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11289"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11289", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a differentiable framework capable of learning a wide variety of \ncompositions of simple policies that we call skills. By recursively composing \nskills with themselves, we can create hierarchies that display complex \nbehavior. Skill networks are trained to generate skill-state embeddings that \nare provided as inputs to a trainable composition function, which in turn \noutputs a policy for the overall task. Our experiments on an environment \nconsisting of multiple collect and evade tasks show that this architecture is \nable to quickly build complex skills from simpler ones. Furthermore, the \nlearned composition function displays some transfer to unseen combinations of \nskills, allowing for zero-shot generalizations. \n</p>"}, "author": "Himanshu Sahni, Saurabh Kumar, Farhan Tejani, Charles Isbell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936896", "id": "tag:google.com,2005:reader/item/000000033e2ca0d2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning to Learn from Weak Supervision by Full Supervision. (arXiv:1711.11383v1 [stat.ML])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11383"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11383", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a method for training neural networks when we have \na large set of data with weak labels and a small amount of data with true \nlabels. In our proposed model, we train two neural networks: a target network, \nthe learner and a confidence network, the meta-learner. The target network is \noptimized to perform a given task and is trained using a large set of unlabeled \ndata that are weakly annotated. We propose to control the magnitude of the \ngradient updates to the target network using the scores provided by the second \nconfidence network, which is trained on a small amount of supervised data. Thus \nwe avoid that the weight updates computed from noisy labels harm the quality of \nthe target network model. \n</p>"}, "author": "Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, Jaap Kamps", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936895", "id": "tag:google.com,2005:reader/item/000000033e2ca0e2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "ConvNets and ImageNet Beyond Accuracy: Explanations, Bias Detection, Adversarial Examples and Model Criticism. (arXiv:1711.11443v1 [cs.LG])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11443"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11443", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>ConvNets and Imagenet have driven the recent success of deep learning for \nimage classification. However, the marked slowdown in performance improvement, \nthe recent studies on the lack of robustness of neural networks to adversarial \nexamples and their tendency to exhibit undesirable biases (e.g racial biases) \nquestioned the reliability and the sustained development of these methods. This \nwork investigates these questions from the perspective of the end-user by using \nhuman subject studies and explanations. We experimentally demonstrate that the \naccuracy and robustness of ConvNets measured on Imagenet are underestimated. We \nshow that explanations can mitigate the impact of misclassified adversarial \nexamples from the perspective of the end-user and we introduce a novel tool for \nuncovering the undesirable biases learned by a model. These contributions also \nshow that explanations are a promising tool for improving our understanding of \nConvNets' predictions and for designing more reliable models \n</p>"}, "author": "Pierre Stock, Moustapha Cisse", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936894", "id": "tag:google.com,2005:reader/item/000000033e2ca0fc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Calculating Semantic Similarity between Academic Articles using Topic Event and Ontology. (arXiv:1711.11508v1 [cs.CL])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11508"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11508", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a32505847522\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a32505847522&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Determining semantic similarity between academic documents is crucial to many \ntasks such as plagiarism detection, automatic technical survey and semantic \nsearch. Current studies mostly focus on semantic similarity between concepts, \nsentences and short text fragments. However, document-level semantic matching \nis still based on statistical information in surface level, neglecting article \nstructures and global semantic meanings, which may cause the deviation in \ndocument understanding. In this paper, we focus on the document-level semantic \nsimilarity issue for academic literatures with a novel method. We represent \nacademic articles with topic events that utilize multiple information profiles, \nsuch as research purposes, methodologies and domains to integrally describe the \nresearch work, and calculate the similarity between topic events based on the \ndomain ontology to acquire the semantic similarity between articles. \nExperiments show that our approach achieves significant performance compared to \nstate-of-the-art methods. \n</p>"}, "author": "Ming Liu, Bo Lang, Zepeng Gu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936893", "id": "tag:google.com,2005:reader/item/000000033e2ca11a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Embodied Question Answering. (arXiv:1711.11543v2 [cs.CV] UPDATED)", "published": 1512364308, "updated": 1512364312, "canonical": [{"href": "http://arxiv.org/abs/1711.11543"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11543", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325058ad03a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325058ad03a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where \nan agent is spawned at a random location in a 3D environment and asked a \nquestion (\"What color is the car?\"). In order to answer, the agent must first \nintelligently navigate to explore the environment, gather information through \nfirst-person (egocentric) vision, and then answer the question (\"orange\"). \n</p> \n<p>This challenging task requires a range of AI skills -- active perception, \nlanguage understanding, goal-driven navigation, commonsense reasoning, and \ngrounding of language into actions. In this work, we develop the environments, \nend-to-end-trained reinforcement learning agents, and evaluation protocols for \nEmbodiedQA. \n</p>"}, "author": "Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936892", "id": "tag:google.com,2005:reader/item/000000033e2ca127", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Neural Networks for Multiple Speaker Detection and Localization. (arXiv:1711.11565v1 [cs.SD])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11565"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11565", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose to use neural networks (NNs) for simultaneous detection and \nlocalization of multiple sound sources in Human-Robot Interaction (HRI). Unlike \nconventional signal processing techniques, NN-based Sound Source Localization \n(SSL) methods are relatively straightforward and require no or fewer \nassumptions that hardly hold in real HRI scenarios. Previously, NN-based \nmethods have been successfully applied to single SSL problems, which do not \nextend to multiple sources in terms of detection and localization. In this \npaper, we thus propose a likelihood-based encoding of the network output, which \nnaturally allows the detection of an arbitrary number of sources. In addition, \nwe investigate the use of sub-band cross-correlation information as features \nfor better localization in sound mixtures, as well as three different NN \narchitectures based on different processing motivations. Experiments on real \ndata recorded from the robot show that our NN-based methods significantly \noutperform the popular spatial spectrum-based approaches. \n</p>"}, "author": "Weipeng He, Petr Motlicek, Jean-Marc Odobez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936891", "id": "tag:google.com,2005:reader/item/000000033e2ca141", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "KSR: A Semantic Representation of Knowledge Graph within a Novel Unsupervised Paradigm. (arXiv:1608.07685v4 [cs.LG] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1608.07685"}], "alternate": [{"href": "http://arxiv.org/abs/1608.07685", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Knowledge representation is a long-history topic in AI, which is very \nimportant. A variety of models have been proposed for knowledge graph \nembedding, which projects symbolic entities and relations into continuous \nvector space. However, most related methods merely focus on the data-fitting of \nknowledge graph, and ignore the interpretable semantic expression. Thus, \ntraditional embedding methods are not friendly for applications that require \nsemantic analysis, such as question answering and entity retrieval. To this \nend, this paper proposes a semantic representation method for knowledge graph \n\\textbf{(KSR)}, which imposes a two-level hierarchical generative process that \nglobally extracts many aspects and then locally assigns a specific category in \neach aspect for every triple. Since both aspects and categories are \nsemantics-relevant, the collection of categories in each aspect is treated as \nthe semantic representation of this triple. Extensive experiments show that our \nmodel outperforms other state-of-the-art baselines substantially. \n</p>"}, "author": "Han Xiao, Minlie Huang, Xiaoyan Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936890", "id": "tag:google.com,2005:reader/item/000000033e2ca155", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning to Optimize Neural Nets. (arXiv:1703.00441v2 [cs.LG] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.00441"}], "alternate": [{"href": "http://arxiv.org/abs/1703.00441", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning to Optimize is a recently proposed framework for learning \noptimization algorithms using reinforcement learning. In this paper, we explore \nlearning an optimization algorithm for training shallow neural nets. Such \nhigh-dimensional stochastic optimization problems present interesting \nchallenges for existing reinforcement learning algorithms. We develop an \nextension that is suited to learning optimization algorithms in this setting \nand demonstrate that the learned optimization algorithm consistently \noutperforms other known optimization algorithms even on unseen tasks and is \nrobust to changes in stochasticity of gradients and the neural net \narchitecture. More specifically, we show that an optimization algorithm trained \nwith the proposed method on the problem of training a neural net on MNIST \ngeneralizes to the problems of training neural nets on the Toronto Faces \nDataset, CIFAR-10 and CIFAR-100. \n</p>"}, "author": "Ke Li, Jitendra Malik", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936889", "id": "tag:google.com,2005:reader/item/000000033e2ca15e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Finite Sample Analyses for TD(0) with Function Approximation. (arXiv:1704.01161v4 [cs.AI] UPDATED)", "published": 1513055249, "updated": 1513055251, "canonical": [{"href": "http://arxiv.org/abs/1704.01161"}], "alternate": [{"href": "http://arxiv.org/abs/1704.01161", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>TD(0) is one of the most commonly used algorithms in reinforcement learning. \nDespite this, there is no existing finite sample analysis for TD(0) with \nfunction approximation, even for the linear case. Our work is the first to \nprovide such results. Existing convergence rates for Temporal Difference (TD) \nmethods apply only to somewhat modified versions, e.g., projected variants or \nones where stepsizes depend on unknown problem parameters. Our analyses obviate \nthese artificial alterations by exploiting strong properties of TD(0). We \nprovide convergence rates both in expectation and with high-probability. The \ntwo are obtained via different approaches that use relatively unknown, recently \ndeveloped stochastic approximation techniques. \n</p>"}, "author": "Gal Dalal, Bal&#xe1;zs Sz&#xf6;r&#xe9;nyi, Gugan Thoppe, Shie Mannor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936888", "id": "tag:google.com,2005:reader/item/000000033e2ca174", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Pig, an Angel and a Cactus Walk Into a Blender: A Descriptive Approach to Visual Blending. (arXiv:1706.09076v2 [cs.AI] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.09076"}], "alternate": [{"href": "http://arxiv.org/abs/1706.09076", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A descriptive approach for automatic generation of visual blends is \npresented. The implemented system, the Blender, is composed of two components: \nthe Mapper and the Visual Blender. The approach uses structured visual \nrepresentations along with sets of visual relations which describe how the \nelements (in which the visual representation can be decomposed) relate among \neach other. Our system is a hybrid blender, as the blending process starts at \nthe Mapper (conceptual level) and ends at the Visual Blender (visual \nrepresentation level). The experimental results show that the Blender is able \nto create analogies from input mental spaces and produce well-composed blends, \nwhich follow the rules imposed by its base-analogy and its relations. The \nresulting blends are visually interesting and some can be considered as \nunexpected. \n</p>"}, "author": "Jo&#xe3;o M. Cunha, Jo&#xe3;o Gon&#xe7;alves, Pedro Martins, Penousal Machado, Am&#xed;lcar Cardoso", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936887", "id": "tag:google.com,2005:reader/item/000000033e2ca17f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Teacher-Student Curriculum Learning. (arXiv:1707.00183v2 [cs.LG] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.00183"}], "alternate": [{"href": "http://arxiv.org/abs/1707.00183", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose Teacher-Student Curriculum Learning (TSCL), a framework for \nautomatic curriculum learning, where the Student tries to learn a complex task \nand the Teacher automatically chooses subtasks from a given set for the Student \nto train on. We describe a family of Teacher algorithms that rely on the \nintuition that the Student should practice more those tasks on which it makes \nthe fastest progress, i.e. where the slope of the learning curve is highest. In \naddition, the Teacher algorithms address the problem of forgetting by also \nchoosing tasks where the Student's performance is getting worse. We demonstrate \nthat TSCL matches or surpasses the results of carefully hand-crafted curricula \nin two tasks: addition of decimal numbers with LSTM and navigation in \nMinecraft. Using our automatically generated curriculum enabled to solve a \nMinecraft maze that could not be solved at all when training directly on \nsolving the maze, and the learning was an order of magnitude faster than \nuniform sampling of subtasks. \n</p>"}, "author": "Tambet Matiisen, Avital Oliver, Taco Cohen, John Schulman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936886", "id": "tag:google.com,2005:reader/item/000000033e2ca18c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Acting Thoughts: Towards a Mobile Robotic Service Assistant for Users with Limited Communication Skills. (arXiv:1707.06633v3 [cs.AI] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.06633"}], "alternate": [{"href": "http://arxiv.org/abs/1707.06633", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>As autonomous service robots become more affordable and thus available also \nfor the general public, there is a growing need for user friendly interfaces to \ncontrol the robotic system. Currently available control modalities typically \nexpect users to be able to express their desire through either touch, speech or \ngesture commands. While this requirement is fulfilled for the majority of \nusers, paralyzed users may not be able to use such systems. In this paper, we \npresent a novel framework, that allows these users to interact with a robotic \nservice assistant in a closed-loop fashion, using only thoughts. The \nbrain-computer interface (BCI) system is composed of several interacting \ncomponents, i.e., non-invasive neuronal signal recording and decoding, \nhigh-level task planning, motion and manipulation planning as well as \nenvironment perception. In various experiments, we demonstrate its \napplicability and robustness in real world scenarios, considering \nfetch-and-carry tasks and tasks involving human-robot interaction. As our \nresults demonstrate, our system is capable of adapting to frequent changes in \nthe environment and reliably completing given tasks within a reasonable amount \nof time. Combined with high-level planning and autonomous robotic systems, \ninteresting new perspectives open up for non-invasive BCI-based human-robot \ninteractions. \n</p>"}, "author": "Felix Burget, Lukas Dominique Josef Fiederer, Daniel Kuhner, Martin V&#xf6;lker, Johannes Aldinger, Robin Tibor Schirrmeister, Chau Do, Joschka Boedecker, Bernhard Nebel, Tonio Ball, Wolfram Burgard", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936885", "id": "tag:google.com,2005:reader/item/000000033e2ca1a1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Analysis of Italian Word Embeddings. (arXiv:1707.08783v4 [cs.CL] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.08783"}], "alternate": [{"href": "http://arxiv.org/abs/1707.08783", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work we analyze the performances of two of the most used word \nembeddings algorithms, skip-gram and continuous bag of words on Italian \nlanguage. These algorithms have many hyper-parameter that have to be carefully \ntuned in order to obtain accurate word representation in vectorial space. We \nprovide an accurate analysis and an evaluation, showing what are the best \nconfiguration of parameters for specific tasks. \n</p>"}, "author": "Rocco Tripodi, Stefano Li Pira", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936884", "id": "tag:google.com,2005:reader/item/000000033e2ca1b4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Voice 3: 2000-Speaker Neural Text-to-Speech. (arXiv:1710.07654v2 [cs.SD] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07654"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07654", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325058ad291\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325058ad291&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present Deep Voice 3, a fully-convolutional attention-based neural \ntext-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural \nspeech synthesis systems in naturalness while training ten times faster. We \nscale Deep Voice 3 to data set sizes unprecedented for TTS, training on more \nthan eight hundred hours of audio from over two thousand speakers. In addition, \nwe identify common error modes of attention-based speech synthesis networks, \ndemonstrate how to mitigate them, and compare several different waveform \nsynthesis methods. We also describe how to scale inference to ten million \nqueries per day on one single-GPU server. \n</p>"}, "author": "Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman, John Miller", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936883", "id": "tag:google.com,2005:reader/item/000000033e2ca1c9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer. (arXiv:1612.01895v2 [cs.CV] CROSS LISTED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1612.01895"}], "alternate": [{"href": "http://arxiv.org/abs/1612.01895", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Transferring artistic styles onto everyday photographs has become an \nextremely popular task in both academia and industry. Recently, offline \ntraining has replaced on-line iterative optimization, enabling nearly real-time \nstylization. When those stylization networks are applied directly to \nhigh-resolution images, however, the style of localized regions often appears \nless similar to the desired artistic style. This is because the transfer \nprocess fails to capture small, intricate textures and maintain correct texture \nscales of the artworks. Here we propose a multimodal convolutional neural \nnetwork that takes into consideration faithful representations of both color \nand luminance channels, and performs stylization hierarchically with multiple \nlosses of increasing scales. Compared to state-of-the-art networks, our network \ncan also perform style transfer in nearly real-time by conducting much more \nsophisticated training offline. By properly handling style and texture cues at \nmultiple scales using several modalities, we can transfer not just large-scale, \nobvious style cues but also subtle, exquisite ones. That is, our scheme can \ngenerate results that are visually pleasing and more similar to multiple \ndesired artistic styles with color and texture cues at multiple scales. \n</p>"}, "author": "Xin Wang, Geoffrey Oxholm, Da Zhang, Yuan-Fang Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936882", "id": "tag:google.com,2005:reader/item/000000033e2ca1d6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Wisdom of the crowd from unsupervised dimension reduction. (arXiv:1711.11034v1 [stat.ML])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11034"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11034", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Wisdom of the crowd, the collective intelligence derived from responses of \nmultiple human or machine individuals to the same questions, can be more \naccurate than each individual, and improve social decision-making and \nprediction accuracy. This can also integrate multiple programs or datasets, \neach as an individual, for the same predictive questions. Crowd wisdom \nestimates each individual's independent error level arising from their limited \nknowledge, and finds the crowd consensus that minimizes the overall error. \nHowever, previous studies have merely built isolated, problem-specific models \nwith limited generalizability, and mainly for binary (yes/no) responses. Here \nwe show with simulation and real-world data that the crowd wisdom problem is \nanalogous to one-dimensional unsupervised dimension reduction in machine \nlearning. This provides a natural class of crowd wisdom solutions, such as \nprincipal component analysis and Isomap, which can handle binary and also \ncontinuous responses, like confidence levels, and consequently can be more \naccurate than existing solutions. They can even outperform \nsupervised-learning-based collective intelligence that is calibrated on \nhistorical performance of individuals, e.g. penalized linear regression and \nrandom forest. This study unifies crowd wisdom and unsupervised dimension \nreduction, and thereupon introduces a broad range of highly-performing and \nwidely-applicable crowd wisdom methods. As the costs for data acquisition and \nprocessing rapidly decrease, this study will promote and guide crowd wisdom \napplications in the social and natural sciences, including data fusion, \nmeta-analysis, crowd-sourcing, and committee decision making. \n</p>"}, "author": "Lingfei Wang, Tom Michoel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936881", "id": "tag:google.com,2005:reader/item/000000033e2ca1ed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Multi-Horizon Quantile Recurrent Forecaster. (arXiv:1711.11053v1 [stat.ML])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11053"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11053", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a framework for general probabilistic multi-step time series \nregression. Specifically, we exploit the expressiveness and temporal nature of \nRecurrent Neural Networks, the nonparametric nature of Quantile Regression and \nthe efficiency of Direct Multi-Horizon Forecasting. A new training scheme for \nrecurrent nets is designed to boost stability and performance. We show that the \napproach accommodates both temporal and static covariates, learning across \nmultiple related series, shifting seasonality, future planned event spikes and \ncold-starts in real life large-scale forecasting. The performance of the \nframework is demonstrated in an application to predict the future demand of \nitems sold on Amazon.com, and in a public probabilistic forecasting competition \nto predict electricity price and load. \n</p>"}, "author": "Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936880", "id": "tag:google.com,2005:reader/item/000000033e2ca1f9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the use of bootstrap with variational inference: Theory, interpretation, and a two-sample test example. (arXiv:1711.11057v1 [stat.ME])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11057"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11057", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Variational inference is a general approach for approximating complex density \nfunctions, such as those arising in latent variable models, popular in machine \nlearning. It has been applied to approximate the maximum likelihood estimator \nand to carry out Bayesian inference, however, quantification of uncertainty \nwith variational inference remains challenging from both theoretical and \npractical perspectives. This paper is concerned with developing uncertainty \nmeasures for variational inference by using bootstrap procedures. We first \ndevelop two general bootstrap approaches for assessing the uncertainty of a \nvariational estimate and the study the underlying bootstrap theory in both \nfixed- and increasing-dimension settings. We then use the bootstrap approach \nand our theoretical results in the context of mixed membership modeling with \nmultivariate binary data on functional disability from the National Long Term \nCare Survey. We carry out a two-sample approach to test for changes in the \nrepeated measures of functional disability for the subset of individuals \npresent in 1984 and 1994 waves. \n</p>"}, "author": "Yen-Chi Chen, Y. Samuel Wang, Elena A. Erosheva", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936879", "id": "tag:google.com,2005:reader/item/000000033e2ca1ff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "GANs for LIFE: Generative Adversarial Networks for Likelihood Free Inference. (arXiv:1711.11139v1 [cs.LG])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11139"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11139", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce a framework using Generative Adversarial Networks (GANs) for \nlikelihood--free inference (LFI) and Approximate Bayesian Computation (ABC). \nOur approach addresses both the key problems in likelihood--free inference, \nnamely how to compare distributions and how to efficiently explore the \nparameter space. Our framework allows one to use the simulator model as a black \nbox and leverage the power of deep networks to generate a rich set of features \nin a data driven fashion (as opposed to previous ad hoc approaches). Thereby it \nis a step towards a powerful alternative approach to LFI and ABC. On benchmark \ndata sets, our approach improves on others with respect to scalability, ability \nto handle high dimensional data and complex probability distributions. \n</p>"}, "author": "Vinay Jethava, Devdatt Dubhashi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936878", "id": "tag:google.com,2005:reader/item/000000033e2ca209", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Phase Transitions in Approximate Ranking. (arXiv:1711.11189v1 [math.ST])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11189"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11189", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of approximate ranking from observations of pairwise \ninteractions. The goal is to estimate the underlying ranks of $n$ objects from \ndata through interactions of comparison or collaboration. Under a general \nframework of approximate ranking models, we characterize the exact optimal \nstatistical error rates of estimating the underlying ranks. We discover \nimportant phase transition boundaries of the optimal error rates. Depending on \nthe value of the signal-to-noise ratio (SNR) parameter, the optimal rate, as a \nfunction of SNR, is either trivial, polynomial, exponential or zero. The four \ncorresponding regimes thus have completely different error behaviors. To the \nbest of our knowledge, this phenomenon, especially the phase transition between \nthe polynomial and the exponential rates, has not been discovered before. \n</p>"}, "author": "Chao Gao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936877", "id": "tag:google.com,2005:reader/item/000000033e2ca213", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Riemannian Stein Variational Gradient Descent for Bayesian Inference. (arXiv:1711.11216v1 [stat.ML])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11216"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11216", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We develop Riemannian Stein Variational Gradient Descent (RSVGD), a Bayesian \ninference method that generalizes Stein Variational Gradient Descent (SVGD) to \nRiemann manifold. The benefits are two-folds: (i) for inference tasks in \nEuclidean spaces, RSVGD has the advantage over SVGD of utilizing information \ngeometry, and (ii) for inference tasks on Riemann manifolds, RSVGD brings the \nunique advantages of SVGD to the Riemannian world. To appropriately transfer to \nRiemann manifolds, we conceive novel and non-trivial techniques for RSVGD, \nwhich are required by the intrinsically different characteristics of general \nRiemann manifolds from Euclidean spaces. We also discover Riemannian Stein's \nIdentity and Riemannian Kernelized Stein Discrepancy. Experimental results show \nthe advantages over SVGD of exploring distribution geometry and the advantages \nof particle-efficiency, iteration-effectiveness and approximation flexibility \nover other inference methods on Riemann manifolds. \n</p>"}, "author": "Chang Liu, Jun Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936876", "id": "tag:google.com,2005:reader/item/000000033e2ca21d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "TCAV: Relative concept importance testing with Linear Concept Activation Vectors. (arXiv:1711.11279v1 [stat.ML])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11279"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11279", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks commonly offer high utility but remain difficult to \ninterpret. Developing methods to explain their decisions is challenging due to \ntheir large size, complex structure, and inscrutable internal representations. \nThis work argues that the language of explanations should be expanded from that \nof input features (e.g., assigning importance weightings to pixels) to include \nthat of higher-level, human-friendly concepts. For example, an understandable \nexplanation of why an image classifier outputs the label \"zebra\" would ideally \nrelate to concepts such as \"stripes\" rather than a set of particular pixel \nvalues. This paper introduces the \"concept activation vector\" (CAV) which \nallows quantitative analysis of a concept's relative importance to \nclassification, with a user-provided set of input data examples defining the \nconcept. CAVs may be easily used by non-experts, who need only provide \nexamples, and with CAVs the high-dimensional structure of neural networks turns \ninto an aid to interpretation, rather than an obstacle. Using the domain of \nimage classification as a testing ground, we describe how CAVs may be used to \ntest hypotheses about classifiers and also generate insights into the \ndeficiencies and correlations in training data. CAVs also provide us a directed \napproach to choose the combinations of neurons to visualize with the DeepDream \ntechnique, which traditionally has chosen neurons or linear combinations of \nneurons at random to visualize. \n</p>"}, "author": "Been Kim, Justin Gilmer, Fernanda Viegas, Ulfar Erlingsson, Martin Wattenberg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936875", "id": "tag:google.com,2005:reader/item/000000033e2ca228", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards Accurate Binary Convolutional Neural Network. (arXiv:1711.11294v1 [cs.LG])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11294"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11294", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce a novel scheme to train binary convolutional neural networks \n(CNNs) -- CNNs with weights and activations constrained to {-1,+1} at run-time. \nIt has been known that using binary weights and activations drastically reduce \nmemory size and accesses, and can replace arithmetic operations with more \nefficient bitwise operations, leading to much faster test-time inference and \nlower power consumption. However, previous works on binarizing CNNs usually \nresult in severe prediction accuracy degradation. In this paper, we address \nthis issue with two major innovations: (1) approximating full-precision weights \nwith the linear combination of multiple binary weight bases; (2) employing \nmultiple binary activations to alleviate information loss. The implementation \nof the resulting binary CNN, denoted as ABC-Net, is shown to achieve much \ncloser performance to its full-precision counterpart, and even reach the \ncomparable prediction accuracy on ImageNet and forest trail datasets, given \nadequate binary weight bases and activations. \n</p>"}, "author": "Xiaofan Lin, Cong Zhao, Wei Pan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936874", "id": "tag:google.com,2005:reader/item/000000033e2ca231", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "MR image reconstruction using the learned data distribution as prior. (arXiv:1711.11386v2 [cs.CV] UPDATED)", "published": 1512364309, "updated": 1512364319, "canonical": [{"href": "http://arxiv.org/abs/1711.11386"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11386", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a325058ad4b2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a325058ad4b2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>MR image reconstruction from undersampled data exploits priors to compensate \nfor missing k-space data. This has previously been achieved by using \nregularization methods, such as TV and wavelets, or data adaptive methods, such \nas dictionary learning. We propose to explicitly learn the probability \ndistribution of MR image patches and to constrain patches to have a high \nprobability according to this distribution in reconstruction, effectively \nemploying it as the prior. \n</p> \n<p>We use variational autoencoders (VAE) to learn the distribution of MR image \npatches. This high dimensional distribution is modelled by a latent parameter \nmodel of lower dimensions in a non-linear fashion. We develop a reconstruction \nalgorithm that uses the learned prior in a Maximum-A-Posteriori estimation \nformulation. We evaluate the proposed method with T1 weighted images and \ncompare it to existing alternatives. We also apply our method on images with \nwhite matter lesions. \n</p> \n<p>Visual evaluation of the samples drawn from the learned model showed that the \nVAE algorithm was able to approximate the distribution of MR image patches. \nFurthermore, the reconstruction algorithm using the approximate distribution \nproduced qualitatively better results. The proposed technique achieved RMSE, \nCNR and CN values of 2.77%, 0.43, 0.11 and 4.29%, 0.43, 0.11 for undersampling \nratios of 2 and 3, respectively. It outperformed other evaluated methods in \nterms of used metrics. In the experiments on images with white matter lesions, \nthe method faithfully reconstructed the lesions. \n</p> \n<p>We introduced a novel method for MR reconstruction, which takes a new \nperspective on regularization by learning priors. Results suggest the method \ncompares favorably against TV and dictionary based methods as well as the \nneural-network based ADMM-Net in terms of the RMSE, CNR and CN and perceptual \nimage quality and can reconstruct lesions as well. \n</p>"}, "author": "Kerem C. Tezcan, Christian F. Baumgartner, Ender Konukoglu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936873", "id": "tag:google.com,2005:reader/item/000000033e2ca240", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Who wins the Miss Contest for Imputation Methods? Our Vote for Miss BooPF. (arXiv:1711.11394v1 [stat.ML])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11394"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11394", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250590de46\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250590de46&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Missing data is an expected issue when large amounts of data is collected, \nand several imputation techniques have been proposed to tackle this problem. \nBeneath classical approaches such as MICE, the application of Machine Learning \ntechniques is tempting. Here, the recently proposed missForest imputation \nmethod has shown high imputation accuracy under the Missing (Completely) at \nRandom scheme with various missing rates. In its core, it is based on a random \nforest for classification and regression, respectively. In this paper we study \nwhether this approach can even be enhanced by other methods such as the \nstochastic gradient tree boosting method, the C5.0 algorithm or modified random \nforest procedures. In particular, other resampling strategies within the random \nforest protocol are suggested. In an extensive simulation study, we analyze \ntheir performances for continuous, categorical as well as mixed-type data. \nTherein, MissBooPF, a combination of the stochastic gradient tree boosting \nmethod together with the parametrically bootstrapped random forest method, \nappeared to be promising. Finally, an empirical analysis focusing on credit \ninformation and Facebook data is conducted. \n</p>"}, "author": "Burim Ramosaj, Markus Pauly", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936872", "id": "tag:google.com,2005:reader/item/000000033e2ca24d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The identity of information: how deterministic dependencies constrain information synergy and redundancy. (arXiv:1711.11408v1 [q-bio.NC])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11408"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11408", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Understanding how different information sources together transmit information \nis crucial in many domains. For example, understanding the neural code requires \ncharacterizing how different neurons contribute unique, redundant, or \nsynergistic pieces of information about sensory or behavioral variables. \nWilliams and Beer (2010) proposed a partial information decomposition (PID) \nwhich separates the mutual information that a set of sources contains about a \nset of targets into nonnegative terms interpretable as these pieces. \nQuantifying redundancy requires assigning an identity to different information \npieces, to assess when information is common across sources. Harder et al. \n(2013) proposed an identity axiom stating that there cannot be redundancy \nbetween two independent sources about a copy of themselves. However, \nBertschinger et al. (2012) showed that with a deterministically related \nsources-target copy this axiom is incompatible with ensuring PID nonnegativity. \nHere we study systematically the effect of deterministic target-sources \ndependencies. We introduce two synergy stochasticity axioms that generalize the \nidentity axiom, and we derive general expressions separating stochastic and \ndeterministic PID components. Our analysis identifies how negative terms can \noriginate from deterministic dependencies and shows how different assumptions \non information identity, implicit in the stochasticity and identity axioms, \ndetermine the PID structure. The implications for studying neural coding are \ndiscussed. \n</p>"}, "author": "Daniel Chicharro, Giuseppe Pica, Stefano Panzeri", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936871", "id": "tag:google.com,2005:reader/item/000000033e2ca25a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On reducing the communication cost of the diffusion LMS algorithm. (arXiv:1711.11423v1 [stat.ML])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11423"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11423", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The rise of digital and mobile communications has recently made the world \nmore connected and networked, resulting in an unprecedented volume of data \nflowing between sources, data centers, or processes. While these data may be \nprocessed in a centralized manner, it is often more suitable to consider \ndistributed strategies such as diffusion as they are scalable and can handle \nlarge amounts of data by distributing tasks over networked agents. Although it \nis relatively simple to implement diffusion strategies over a cluster, it \nappears to be challenging to deploy them in an ad-hoc network with limited \nenergy budget for communication. In this paper, we introduce a diffusion LMS \nstrategy that significantly reduces communication costs without compromising \nthe performance. Then, we analyze the proposed algorithm in the mean and \nmean-square sense. Next, we conduct numerical experiments to confirm the \ntheoretical findings. Finally, we perform large scale simulations to test the \nalgorithm efficiency in a scenario where energy is limited. \n</p>"}, "author": "Ibrahim El Khalil Harrane, R&#xe9;mi Flamary, C&#xe9;dric Richard", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936870", "id": "tag:google.com,2005:reader/item/000000033e2ca26d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Channel Multivariate Entropy Triangle and Balance Equation. (arXiv:1711.11510v1 [cs.IT])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11510"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11510", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we use information-theoretic measures to provide a theory and \ntools to analyze the flow of information from a discrete, multivariate source \nof information $\\overline X$ to a discrete, multivariate sink of information \n$\\overline Y$ joined by a distribution $P_{\\overline X \\overline Y}$. The first \ncontribution is a decomposition of the maximal potential entropy of $(\\overline \nX, \\overline Y)$ that we call a balance equation, that can also be split into \ndecompositions for the entropies of $\\overline X$ and $\\overline Y$ \nrespectively. Such balance equations accept normalizations that allow them to \nbe represented in de Finetti entropy diagrams, our second contribution. The \nmost important of these, the aggregate Channel Multivariate Entropy Triangle \nCMET is an exploratory tool to assess the efficiency of multivariate channels. \nWe also present a practical contribution in the application of these balance \nequations and diagrams to the assessment of information transfer efficiency for \nPCA and ICA as feature transformation and selection procedures in machine \nlearning applications. \n</p>"}, "author": "Francisco J. Valverde-Albacete, Carmen Pel&#xe1;ez-Moreno", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936869", "id": "tag:google.com,2005:reader/item/000000033e2ca279", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Thermostat-assisted Continuous-tempered Hamiltonian Monte Carlo for Multimodal Posterior Sampling on Large Datasets. (arXiv:1711.11511v1 [stat.ML])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11511"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11511", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a new sampling method named as the \nthermostat-assisted continuous-tempered Hamiltonian Monte Carlo for multimodal \nposterior sampling on large datasets. It simulates a noisy system, which is \naugmented by a coupling tempering variable as well as a set of Nos\\'e-Hoover \nthermostats. This augmentation is devised to address two main issues of \nconcern: the first is to effectively generate i.i.d. samples from complex \nmultimodal posterior distributions; the second is to adaptively control the \nsystem dynamics in the presence of unknown noise that arises from the use of \nmini-batches. The experiment on synthetic distributions has been performed; the \nresult demonstrates the effectiveness of the proposed method. \n</p>"}, "author": "Rui Luo, Yaodong Yang, Jun Wang, Yuanyuan Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936868", "id": "tag:google.com,2005:reader/item/000000033e2ca286", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improved Linear Embeddings via Lagrange Duality. (arXiv:1711.11527v1 [stat.ML])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11527"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11527", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Near isometric orthonormal embeddings to lower dimensions are a fundamental \ntool in data science and machine learning. We present a construction of such \nembeddings that minimizes the maximum distortion for a given set of points. We \nformulate the problem as a non convex constrained optimization problem. We \nfirst construct a primal relaxation and then using the theory of Lagrange \nduality, we construct dual relaxations. We also give a polynomial time \nalgorithm based on convex optimization to solve the dual relaxation provably. \nWe experimentally demonstrate the superiority of our algorithm compared to \nbaselines in terms scalability and the ability to achieve lower distortion. \n</p>"}, "author": "Kshiteej Sheth, Dinesh Garg, Anirban Dasgupta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936867", "id": "tag:google.com,2005:reader/item/000000033e2ca28e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning to Adapt by Minimizing Discrepancy. (arXiv:1711.11542v1 [cs.LG])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11542"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11542", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We explore whether useful temporal neural generative models can be learned \nfrom sequential data without back-propagation through time. We investigate the \nviability of a more neurocognitively-grounded approach in the context of \nunsupervised generative modeling of sequences. Specifically, we build on the \nconcept of predictive coding, which has gained influence in cognitive science, \nin a neural framework. To do so we develop a novel architecture, the Temporal \nNeural Coding Network, and its learning algorithm, Discrepancy Reduction. The \nunderlying directed generative model is fully recurrent, meaning that it \nemploys structural feedback connections and temporal feedback connections, \nyielding information propagation cycles that create local learning signals. \nThis facilitates a unified bottom-up and top-down approach for information \ntransfer inside the architecture. Our proposed algorithm shows promise on the \nbouncing balls generative modeling problem. Further experiments could be \nconducted to explore the strengths and weaknesses of our approach. \n</p>"}, "author": "Alexander G. Ororbia II, Patrick Haffner, David Reitter, C. Lee Giles", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936866", "id": "tag:google.com,2005:reader/item/000000033e2ca29c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Measuring the tendency of CNNs to Learn Surface Statistical Regularities. (arXiv:1711.11561v1 [cs.LG])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11561"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11561", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep CNNs are known to exhibit the following peculiarity: on the one hand \nthey generalize extremely well to a test set, while on the other hand they are \nextremely sensitive to so-called adversarial perturbations. The extreme \nsensitivity of high performance CNNs to adversarial examples casts serious \ndoubt that these networks are learning high level abstractions in the dataset. \nWe are concerned with the following question: How can a deep CNN that does not \nlearn any high level semantics of the dataset manage to generalize so well? The \ngoal of this article is to measure the tendency of CNNs to learn surface \nstatistical regularities of the dataset. To this end, we use Fourier filtering \nto construct datasets which share the exact same high level abstractions but \nexhibit qualitatively different surface statistical regularities. For the SVHN \nand CIFAR-10 datasets, we present two Fourier filtered variants: a low \nfrequency variant and a randomly filtered variant. Each of the Fourier \nfiltering schemes is tuned to preserve the recognizability of the objects. Our \nmain finding is that CNNs exhibit a tendency to latch onto the Fourier image \nstatistics of the training dataset, sometimes exhibiting up to a 28% \ngeneralization gap across the various test sets. Moreover, we observe that \nsignificantly increasing the depth of a network has a very marginal impact on \nclosing the aforementioned generalization gap. Thus we provide quantitative \nevidence supporting the hypothesis that deep CNNs tend to learn surface \nstatistical regularities in the dataset rather than higher-level abstract \nconcepts. \n</p>"}, "author": "Jason Jo, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936865", "id": "tag:google.com,2005:reader/item/000000033e2ca2a9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Outlier-robust moment-estimation via sum-of-squares. (arXiv:1711.11581v1 [cs.DS])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11581"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11581", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We develop efficient algorithms for estimating low-degree moments of unknown \ndistributions in the presence of adversarial outliers. The guarantees of our \nalgorithms improve in many cases significantly over the best previous ones, \nobtained in recent works of Diakonikolas et al, Lai et al, and Charikar et al. \nWe also show that the guarantees of our algorithms match information-theoretic \nlower-bounds for the class of distributions we consider. These improved \nguarantees allow us to give improved algorithms for independent component \nanalysis and learning mixtures of Gaussians in the presence of outliers. \n</p> \n<p>Our algorithms are based on a standard sum-of-squares relaxation of the \nfollowing conceptually-simple optimization problem: Among all distributions \nwhose moments are bounded in the same way as for the unknown distribution, find \nthe one that is closest in statistical distance to the empirical distribution \nof the adversarially-corrupted sample. \n</p>"}, "author": "Pravesh K. Kothari, David Steurer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936864", "id": "tag:google.com,2005:reader/item/000000033e2ca2b7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Toward Multimodal Image-to-Image Translation. (arXiv:1711.11586v1 [cs.CV])", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.11586"}], "alternate": [{"href": "http://arxiv.org/abs/1711.11586", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250590e09f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250590e09f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Many image-to-image translation problems are ambiguous, as a single input \nimage may correspond to multiple possible outputs. In this work, we aim to \nmodel a \\emph{distribution} of possible outputs in a conditional generative \nmodeling setting. The ambiguity of the mapping is distilled in a \nlow-dimensional latent vector, which can be randomly sampled at test time. A \ngenerator learns to map the given input, combined with this latent code, to the \noutput. We explicitly encourage the connection between output and the latent \ncode to be invertible. This helps prevent a many-to-one mapping from the latent \ncode to the output during training, also known as the problem of mode collapse, \nand produces more diverse results. We explore several variants of this approach \nby employing different training objectives, network architectures, and methods \nof injecting the latent code. Our proposed method encourages bijective \nconsistency between the latent encoding and output modes. We present a \nsystematic comparison of our method and other variants on both perceptual \nrealism and diversity. \n</p>"}, "author": "Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, Eli Shechtman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936863", "id": "tag:google.com,2005:reader/item/000000033e2ca2c3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Second-Order Stochastic Optimization for Machine Learning in Linear Time. (arXiv:1602.03943v5 [stat.ML] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1602.03943"}], "alternate": [{"href": "http://arxiv.org/abs/1602.03943", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>First-order stochastic methods are the state-of-the-art in large-scale \nmachine learning optimization owing to efficient per-iteration complexity. \nSecond-order methods, while able to provide faster convergence, have been much \nless explored due to the high cost of computing the second-order information. \nIn this paper we develop second-order stochastic methods for optimization \nproblems in machine learning that match the per-iteration cost of gradient \nbased methods, and in certain settings improve upon the overall running time \nover popular first-order methods. Furthermore, our algorithm has the desirable \nproperty of being implementable in time linear in the sparsity of the input \ndata. \n</p>"}, "author": "Naman Agarwal, Brian Bullins, Elad Hazan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936862", "id": "tag:google.com,2005:reader/item/000000033e2ca2ce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Convolutional Framelets: A General Deep Learning Framework for Inverse Problems. (arXiv:1707.00372v4 [stat.ML] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.00372"}], "alternate": [{"href": "http://arxiv.org/abs/1707.00372", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recently, deep learning approaches have achieved significant performance \nimprovement in various imaging problems. However, it is still unclear why these \ndeep learning architectures work. Moreover, the link between the deep learning \nand the classical signal processing approaches such as wavelet, non-local \nprocessing, compressed sensing, etc, is still not well understood. To address \nthese issues, here we show that the long-searched-for missing link is the \nconvolutional framelets for representing a signal by convolving local and \nnon-local bases. The convolutional framelets was originally developed to \ngeneralize the recent theory of low-rank Hankel matrix approaches, and this \npaper significantly extends the idea to derive a deep neural network using \nmulti-layer convolutional framelets with perfect reconstruction (PR) under \nrectified linear unit (ReLU). Our analysis also shows that the popular deep \nnetwork components such as residual block, redundant filter channels, and \nconcatenated ReLU (CReLU) indeed help to achieve the PR, while the pooling and \nunpooling layers should be augmented with multi-resolution convolutional \nframelets to achieve PR condition. This discovery reveals the limitations of \nmany existing deep learning architectures for inverse problems, and leads us to \npropose a novel deep convolutional framelets neural network. Using numerical \nexperiments with sparse view x-ray computed tomography (CT), we demonstrated \nthat our deep convolution framelets network shows consistent improvement. This \ndiscovery suggests that the success of deep learning is not from a magical \npower of a black-box, but rather comes from the power of a novel signal \nrepresentation using non-local basis combined with data-driven local basis, \nwhich is indeed a natural extension of classical signal processing theory. \n</p>"}, "author": "Jong Chul Ye, Yoseob Han, Eunju Cha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936861", "id": "tag:google.com,2005:reader/item/000000033e2ca2df", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Certifiable Distributional Robustness with Principled Adversarial Training. (arXiv:1710.10571v2 [stat.ML] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10571"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10571", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks are vulnerable to adversarial examples and researchers have \nproposed many heuristic attack and defense mechanisms. We take the principled \nview of distributionally robust optimization, which guarantees performance \nunder adversarial input perturbations. By considering a Lagrangian penalty \nformulation of perturbation of the underlying data distribution in a \nWasserstein ball, we provide a training procedure that augments model parameter \nupdates with worst-case perturbations of training data. For smooth losses, our \nprocedure provably achieves moderate levels of robustness with little \ncomputational or statistical cost relative to empirical risk minimization. \nFurthermore, our statistical guarantees allow us to efficiently certify \nrobustness for the population loss. We match or outperform heuristic approaches \non supervised and reinforcement learning tasks. \n</p>"}, "author": "Aman Sinha, Hongseok Namkoong, John Duchi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936860", "id": "tag:google.com,2005:reader/item/000000033e2ca2e8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Techniques for proving Asynchronous Convergence results for Markov Chain Monte Carlo methods. (arXiv:1711.06719v4 [stat.ML] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06719"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06719", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Markov Chain Monte Carlo (MCMC) methods such as Gibbs sampling are finding \nwidespread use in applied statistics and machine learning. These often lead to \ndifficult computational problems, which are increasingly being solved on \nparallel and distributed systems such as compute clusters. Recent work has \nproposed running iterative algorithms such as gradient descent and MCMC in \nparallel asynchronously for increased performance, with good empirical results \nin certain problems. Unfortunately, for MCMC this parallelization technique \nrequires new convergence theory, as it has been explicitly demonstrated to lead \nto divergence on some examples. Recent theory on Asynchronous Gibbs sampling \ndescribes why these algorithms can fail, and provides a way to alter them to \nmake them converge. In this article, we describe how to apply this theory in a \ngeneric setting, to understand the asynchronous behavior of any MCMC algorithm, \nincluding those implemented using parameter servers, and those not based on \nGibbs sampling. \n</p>"}, "author": "Alexander Terenin, Eric P. Xing", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936859", "id": "tag:google.com,2005:reader/item/000000033e2ca2f5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks. (arXiv:1711.06798v2 [cs.LG] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06798"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06798", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present MorphNet, an approach to automate the design of neural network \nstructures. MorphNet iteratively shrinks and expands a network, shrinking via a \nresource-weighted sparsifying regularizer on activations and expanding via a \nuniform multiplicative factor on all layers. In contrast to previous \napproaches, our method is scalable to large networks, adaptable to specific \nresource constraints (e.g. the number of floating-point operations per \ninference), and capable of increasing the network's performance. When applied \nto standard network architectures on a wide variety of datasets, our approach \ndiscovers novel structures in each domain, obtaining higher performance while \nrespecting the resource constraint. \n</p>"}, "author": "Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Tien-Ju Yang, Edward Choi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936858", "id": "tag:google.com,2005:reader/item/000000033e2ca30b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Disagreement-based combinatorial pure exploration: Efficient algorithms and an analysis with localization. (arXiv:1711.08018v2 [stat.ML] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08018"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08018", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We design new algorithms for the combinatorial pure exploration problem in \nthe multi-arm bandit framework. In this problem, we are given K distributions \nand a collection of subsets $\\mathcal{V} \\subset 2^K$ of these distributions, \nand we would like to find the subset $v \\in \\mathcal{V}$ that has largest \ncumulative mean, while collecting, in a sequential fashion, as few samples from \nthe distributions as possible. We study both the fixed budget and fixed \nconfidence settings, and our algorithms essentially achieve state-of-the-art \nperformance in all settings, improving on previous guarantees for structures \nlike matchings and submatrices that have large augmenting sets. Moreover, our \nalgorithms can be implemented efficiently whenever the decision set V admits \nlinear optimization. Our analysis involves precise concentration-of-measure \narguments and a new algorithm for linear programming with exponentially many \nconstraints. \n</p>"}, "author": "Tongyi Cao, Akshay Krishnamurthy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936857", "id": "tag:google.com,2005:reader/item/000000033e2ca317", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Proceedings of NIPS 2017 Symposium on Interpretable Machine Learning. (arXiv:1711.09889v3 [stat.ML] UPDATED)", "published": 1513141859, "updated": 1513141864, "canonical": [{"href": "http://arxiv.org/abs/1711.09889"}], "alternate": [{"href": "http://arxiv.org/abs/1711.09889", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This is the Proceedings of NIPS 2017 Symposium on Interpretable Machine \nLearning, held in Long Beach, California, USA on December 7, 2017 \n</p>"}, "author": "Andrew Gordon Wilson, Jason Yosinski, Patrice Simard, Rich Caruana, William Herlands", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936856", "id": "tag:google.com,2005:reader/item/000000033e2ca31f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hierarchical Policy Search via Return-Weighted Density Estimation. (arXiv:1711.10173v2 [cs.LG] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.10173"}], "alternate": [{"href": "http://arxiv.org/abs/1711.10173", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning an optimal policy from a multi-modal reward function is a \nchallenging problem in reinforcement learning (RL). Hierarchical RL (HRL) \ntackles this problem by learning a hierarchical policy, where multiple option \npolicies are in charge of different strategies corresponding to modes of a \nreward function and a gating policy selects the best option for a given \ncontext. Although HRL has been demonstrated to be promising, current \nstate-of-the-art methods cannot still perform well in complex real-world \nproblems due to the difficulty of identifying modes of the reward function. In \nthis paper, we propose a novel method called hierarchical policy search via \nreturn-weighted density estimation (HPSDE), which can efficiently identify the \nmodes through density estimation with return-weighted importance sampling. Our \nproposed method finds option policies corresponding to the modes of the return \nfunction and automatically determines the number and the location of option \npolicies, which significantly reduces the burden of hyper-parameters tuning. \nThrough experiments, we demonstrate that the proposed HPSDE successfully learns \noption policies corresponding to modes of the return function and that it can \nbe successfully applied to a challenging motion planning problem of a redundant \nrobotic manipulator. \n</p>"}, "author": "Takayuki Osa, Masashi Sugiyama", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936855", "id": "tag:google.com,2005:reader/item/000000033e2ca32f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Are GANs Created Equal? A Large-Scale Study. (arXiv:1711.10337v2 [stat.ML] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.10337"}], "alternate": [{"href": "http://arxiv.org/abs/1711.10337", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative adversarial networks (GAN) are a powerful subclass of generative \nmodels. Despite a very rich research activity leading to numerous interesting \nGAN algorithms, it is still very hard to assess which algorithm(s) perform \nbetter than others. We conduct a neutral, multi-faceted large-scale empirical \nstudy on state-of-the art models and evaluation measures. We find that most \nmodels can reach similar scores with enough hyperparameter optimization and \nrandom restarts. This suggests that improvements can arise from a higher \ncomputational budget and tuning more than fundamental algorithmic changes. To \novercome some limitations of the current metrics, we also propose several data \nsets on which precision and recall can be computed. Our experimental results \nsuggest that future GAN research should be based on more systematic and \nobjective evaluation procedures. Finally, we did not find evidence that any of \nthe tested algorithms consistently outperforms the original one. \n</p>"}, "author": "Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, Olivier Bousquet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1512136966937", "timestampUsec": "1512136966936854", "id": "tag:google.com,2005:reader/item/000000033e2ca33f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Valid Inference Corrected for Outlier Removal. (arXiv:1711.10635v2 [stat.ME] UPDATED)", "published": 1512136967, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.10635"}], "alternate": [{"href": "http://arxiv.org/abs/1711.10635", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a3250590e2e0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a3250590e2e0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Ordinary least square (OLS) estimation of a linear regression model is \nwell-known to be highly sensitive to outliers. It is common practice to first \nidentify and remove outliers by looking at the data then to fit OLS and form \nconfidence intervals and p-values on the remaining data as if this were the \noriginal data collected. We show in this paper that this \"detect-and-forget\" \napproach can lead to invalid inference, and we propose a framework that \nproperly accounts for outlier detection and removal to provide valid confidence \nintervals and hypothesis tests. Our inferential procedures apply to any outlier \nremoval procedure that can be characterized by a set of quadratic constraints \non the response vector, and we show that several of the most commonly used \noutlier detection procedures are of this form. Our methodology is built upon \nrecent advances in selective inference (Taylor &amp; Tibshirani 2015), which are \nfocused on inference corrected for variable selection. We conduct simulations \nto corroborate the theoretical results, and we apply our method to two classic \ndata sets considered in the outlier detection literature to illustrate how our \ninferential results can differ from the traditional detect-and-forget strategy. \nA companion R package, outference, implements these new procedures with an \ninterface that matches the functions commonly used for inference with lm in R. \n</p>"}, "author": "Shuxiao Chen, Jacob Bien", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}]}