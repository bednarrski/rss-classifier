{"all_articles": [{"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yan Ru Pei, Fabio L. Traversa, Massimiliano Di Ventra", "title": "On the Universality of Memcomputing Machines. (arXiv:1712.08702v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.08702", "type": "text/html"}], "timestampUsec": "1514352521979608", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050606144\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050606144&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Universal memcomputing machines (UMMs) [IEEE Trans. Neural Netw. Learn. Syst. \n26, 2702 (2015)] represent a novel computational model in which memory (time \nnon-locality) accomplishes both tasks of storing and processing of information. \nUMMs have been shown to be Turing-complete, namely they can simulate any Turing \nmachine. In this paper, using set theory and cardinality arguments, we compare \nthem with liquid-state machines (or \"reservoir computing\") and quantum machines \n(\"quantum computing\"). We show that UMMs can simulate both types of machines, \nhence they are both \"liquid-\" or \"reservoir-complete\" and \"quantum-complete\". \nOf course, these statements pertain only to the type of problems these machines \ncan solve, and not to the amount of resources required for such simulations. \nNonetheless, the method presented here provides a general framework in which to \ndescribe the relation between UMMs and any other type of computational model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a02b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08702"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tao Lei, Yu Zhang, Yoav Artzi", "title": "Training RNNs as Fast as CNNs. (arXiv:1709.02755v4 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.02755", "type": "text/html"}], "timestampUsec": "1514352521979607", "comments": [], "summary": {"content": "<p>Common recurrent neural network architectures scale poorly due to the \nintrinsic difficulty in parallelizing their state computations. In this work, \nwe propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that \nsimplifies the computation and exposes more parallelism. In SRU, the majority \nof computation for each step is independent of the recurrence and can be easily \nparallelized. SRU is as fast as a convolutional layer and 5-10x faster than an \noptimized LSTM implementation. We study SRUs on a wide range of applications, \nincluding classification, question answering, language modeling, translation \nand speech recognition. Our experiments demonstrate the effectiveness of SRU \nand the trade-off it enables between speed and performance. We open source our \nimplementation in PyTorch and CNTK. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a02f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.02755"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wei Wen, Yuxiong He, Samyam Rajbhandari, Wenhan Wang, Fang Liu, Bin Hu, Yiran Chen, Hai Li", "title": "Learning Intrinsic Sparse Structures within Long Short-Term Memory. (arXiv:1709.05027v4 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.05027", "type": "text/html"}], "timestampUsec": "1514352521979606", "comments": [], "summary": {"content": "<p>Model compression is significant for the wide adoption of Recurrent Neural \nNetworks (RNNs) in both user devices possessing limited resources and business \nclusters requiring quick responses to large-scale service requests. This work \naims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the \nsizes of basic structures within LSTM units, including input updates, gates, \nhidden states, cell states and outputs. Independently reducing the sizes of \nbasic structures can result in inconsistent dimensions among them, and \nconsequently, end up with invalid LSTM units. To overcome the problem, we \npropose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS \nwill simultaneously decrease the sizes of all basic structures by one and \nthereby always maintain the dimension consistency. By learning ISS within LSTM \nunits, the obtained LSTMs remain regular while having much smaller basic \nstructures. Based on group Lasso regularization, our method achieves 10.59x \nspeedup without losing any perplexity of a language modeling of Penn TreeBank \ndataset. It is also successfully evaluated through a compact model with only \n2.69M weights for machine Question Answering of SQuAD dataset. Our approach is \nsuccessfully extended to non- LSTM RNNs, like Recurrent Highway Networks \n(RHNs). Our source code is publicly available at \nhttps://github.com/wenwei202/iss-rnns \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a032", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.05027"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "A. Murat Ozbayoglu, Gokhan Kucukayan, Erdogan Dogdu", "title": "A Real-Time Autonomous Highway Accident Detection Model Based on Big Data Processing and Computational Intelligence. (arXiv:1712.09227v1 [cs.CE])", "alternate": [{"href": "http://arxiv.org/abs/1712.09227", "type": "text/html"}], "timestampUsec": "1514352521979605", "comments": [], "summary": {"content": "<p>Due to increasing urban population and growing number of motor vehicles, \ntraffic congestion is becoming a major problem of the 21st century. One of the \nmain reasons behind traffic congestion is accidents which can not only result \nin casualties and losses for the participants, but also in wasted and lost time \nfor the others that are stuck behind the wheels. Early detection of an accident \ncan save lives, provides quicker road openings, hence decreases wasted time and \nresources, and increases efficiency. In this study, we propose a preliminary \nreal-time autonomous accident-detection system based on computational \nintelligence techniques. Istanbul City traffic-flow data for the year 2015 from \nvarious sensor locations are populated using big data processing methodologies. \nThe extracted features are then fed into a nearest neighbor model, a regression \ntree, and a feed-forward neural network model. For the output, the possibility \nof an occurrence of an accident is predicted. The results indicate that even \nthough the number of false alarms dominates the real accident cases, the system \ncan still provide useful information that can be used for status verification \nand early reaction to possible accidents. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a037", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09227"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Arkar Min Aung, Yousef Fadila, Radian Gondokaryono, Luis Gonzalez", "title": "Building Robust Deep Neural Networks for Road Sign Detection. (arXiv:1712.09327v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.09327", "type": "text/html"}], "timestampUsec": "1514352521979604", "comments": [], "summary": {"content": "<p>Deep Neural Networks are built to generalize outside of training set in mind \nby using techniques such as regularization, early stopping and dropout. But \nconsiderations to make them more resilient to adversarial examples are rarely \ntaken. As deep neural networks become more prevalent in mission-critical and \nreal-time systems, miscreants start to attack them by intentionally making deep \nneural networks to misclassify an object of one type to be seen as another \ntype. This can be catastrophic in some scenarios where the classification of a \ndeep neural network can lead to a fatal decision by a machine. In this work, we \nused GTSRB dataset to craft adversarial samples by Fast Gradient Sign Method \nand Jacobian Saliency Method, used those crafted adversarial samples to attack \nanother Deep Convolutional Neural Network and built the attacked network to be \nmore resilient against adversarial attacks by making it more robust by \nDefensive Distillation and Adversarial Training \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a03b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09327"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Amit Dhurandhar, Vijay Iyengar, Ronny Luss, Karthikeyan Shanmugam", "title": "TIP: Typifying the Interpretability of Procedures. (arXiv:1706.02952v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.02952", "type": "text/html"}], "timestampUsec": "1514352521979603", "comments": [], "summary": {"content": "<p>We provide a novel notion of what it means to be interpretable, looking past \nthe usual association with human understanding. Our key insight is that \ninterpretability is not an absolute concept and so we define it relative to a \ntarget model, which may or may not be a human. We define a framework that \nallows for comparing interpretable procedures by linking it to important \npractical aspects such as accuracy and robustness. We characterize many of the \ncurrent state-of-the-art interpretable methods in our framework portraying its \ngeneral applicability. Finally, principled interpretable strategies are \nproposed and empirically evaluated on synthetic data, as well as on the largest \npublic olfaction dataset that was made recently available \\cite{olfs}. We also \nexperiment on MNIST with a simple target model and different oracle models of \nvarying complexity. This leads to the insight that the improvement in the \ntarget model is not only a function of the oracle models performance, but also \nits relative complexity with respect to the target model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a03e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.02952"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chun-Hao Chang, Ladislav Rampasek, Anna Goldenberg", "title": "Dropout Feature Ranking for Deep Learning Models. (arXiv:1712.08645v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08645", "type": "text/html"}], "timestampUsec": "1514352521979602", "comments": [], "summary": {"content": "<p>Deep neural networks are a promising technology achieving state-of-the-art \nresults in biological and healthcare domains. Unfortunately, DNNs are notorious \nfor their non-interpretability. Clinicians are averse to black boxes and thus \ninterpretability is paramount to broadly adopting this technology. We aim to \nclose this gap by proposing a new general feature ranking method for deep \nlearning. We show that our method outperforms LASSO, Elastic Net, Deep Feature \nSelection and various heuristics on a simulated dataset. We also compare our \nmethod in a multivariate clinical time-series dataset and demonstrate our \nranking rivals or outperforms other methods in Recurrent Neural Network \nsetting. Finally, we apply our feature ranking to the Variational Autoencoder \nrecently proposed to predict drug response in cell lines and show that it \nidentifies meaningful genes corresponding to the drug response. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a042", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08645"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Fnu Suya, Yuan Tian, David Evans, Paolo Papotti", "title": "Query-limited Black-box Attacks to Classifiers. (arXiv:1712.08713v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1712.08713", "type": "text/html"}], "timestampUsec": "1514352521979601", "comments": [], "summary": {"content": "<p>We study black-box attacks on machine learning classifiers where each query \nto the model incurs some cost or risk of detection to the adversary. We focus \nexplicitly on minimizing the number of queries as a major objective. \nSpecifically, we consider the problem of attacking machine learning classifiers \nsubject to a budget of feature modification cost while minimizing the number of \nqueries, where each query returns only a class and confidence score. We \ndescribe an approach that uses Bayesian optimization to minimize the number of \nqueries, and find that the number of queries can be reduced to approximately \none tenth of the number needed through a random strategy for scenarios where \nthe feature modification cost budget is low. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a049", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08713"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Qingjiu Zhang, Shiliang Sun", "title": "Weighted Data Normalization Based on Eigenvalues for Artificial Neural Network Classification. (arXiv:1712.08885v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08885", "type": "text/html"}], "timestampUsec": "1514352521979600", "comments": [], "summary": {"content": "<p>Artificial neural network (ANN) is a very useful tool in solving learning \nproblems. Boosting the performances of ANN can be mainly concluded from two \naspects: optimizing the architecture of ANN and normalizing the raw data for \nANN. In this paper, a novel method which improves the effects of ANN by \npreprocessing the raw data is proposed. It totally leverages the fact that \ndifferent features should play different roles. The raw data set is firstly \npreprocessed by principle component analysis (PCA), and then its principle \ncomponents are weighted by their corresponding eigenvalues. Several aspects of \nanalysis are carried out to analyze its theory and the applicable occasions. \nThree classification problems are launched by an active learning algorithm to \nverify the proposed method. From the empirical results, conclusion comes to the \nfact that the proposed method can significantly improve the performance of ANN. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a059", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08885"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Itay Safran, Ohad Shamir", "title": "Spurious Local Minima are Common in Two-Layer ReLU Neural Networks. (arXiv:1712.08968v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08968", "type": "text/html"}], "timestampUsec": "1514352521979599", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450506064c3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450506064c3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We consider the optimization problem associated with training simple ReLU \nneural networks of the form $\\mathbf{x}\\mapsto \n\\sum_{i=1}^{n}\\max\\{0,\\mathbf{w}_i^\\top \\mathbf{x}\\}$ with respect to the \nsquared loss. We provide a computer-assisted proof that even if the input \ndistribution is standard Gaussian, even if the dimension is unrestricted, and \neven if the target values are generated by such a network, with orthonormal \nparameter vectors, the problem can still have spurious local minima once $k\\geq \n6$. By a continuity argument, this implies that in high dimensions, \n\\emph{nearly all} target networks of the relevant sizes lead to spurious local \nminima. Moreover, we conduct experiments which show that the probability of \nhitting such local minima is quite high, and increasing with the network size. \nOn the positive side, mild over-parameterization appears to drastically reduce \nsuch local minima, indicating that an over-parameterization assumption is \nnecessary to get a positive result in this setting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a06a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08968"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Debdeep Pati, Anirban Bhattacharya, Yun Yang", "title": "On Statistical Optimality of Variational Bayes. (arXiv:1712.08983v1 [math.ST])", "alternate": [{"href": "http://arxiv.org/abs/1712.08983", "type": "text/html"}], "timestampUsec": "1514352521979598", "comments": [], "summary": {"content": "<p>The article addresses a long-standing open problem on the justification of \nusing variational Bayes methods for parameter estimation. We provide general \nconditions for obtaining optimal risk bounds for point estimates acquired from \nmean-field variational Bayesian inference. The conditions pertain to the \nexistence of certain test functions for the distance metric on the parameter \nspace and minimal assumptions on the prior. A general recipe for verification \nof the conditions is outlined which is broadly applicable to existing Bayesian \nmodels with or without latent variables. As illustrations, specific \napplications to Latent Dirichlet Allocation and Gaussian mixture models are \ndiscussed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a075", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08983"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rongqing Huang, Shiliang Sun", "title": "Kernel Regression with Sparse Metric Learning. (arXiv:1712.09001v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.09001", "type": "text/html"}], "timestampUsec": "1514352521979597", "comments": [], "summary": {"content": "<p>Kernel regression is a popular non-parametric fitting technique. It aims at \nlearning a function which estimates the targets for test inputs as precise as \npossible. Generally, the function value for a test input is estimated by a \nweighted average of the surrounding training examples. The weights are \ntypically computed by a distance-based kernel function and they strongly depend \non the distances between examples. In this paper, we first review the latest \ndevelopments of sparse metric learning and kernel regression. Then a novel \nkernel regression method involving sparse metric learning, which is called \nkernel regression with sparse metric learning (KR$\\_$SML), is proposed. The \nsparse kernel regression model is established by enforcing a mixed $(2,1)$-norm \nregularization over the metric matrix. It learns a Mahalanobis distance metric \nby a gradient descent procedure, which can simultaneously conduct \ndimensionality reduction and lead to good prediction results. Our work is the \nfirst to combine kernel regression with sparse metric learning. To verify the \neffectiveness of the proposed method, it is evaluated on 19 data sets for \nregression. Furthermore, the new method is also applied to solving practical \nproblems of forecasting short-term traffic flows. In the end, we compare the \nproposed method with other three related kernel regression methods on all test \ndata sets under two criterions. Experimental results show that the proposed \nmethod is much more competitive. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a07e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09001"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Qibing Li, Xiaolin Zheng", "title": "Deep Collaborative Autoencoder for Recommender Systems: A Unified Framework for Explicit and Implicit Feedback. (arXiv:1712.09043v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.09043", "type": "text/html"}], "timestampUsec": "1514352521979596", "comments": [], "summary": {"content": "<p>In recent years, deep neural networks have yielded state-of-the-art \nperformance on several tasks. Although some recent works have focused on \ncombining deep learning with recommendation, we highlight three issues of \nexisting works. First, most works perform deep content feature learning and \nresort to matrix factorization, which cannot effectively model the highly \ncomplex user-item interaction function. Second, due to the difficulty on \ntraining deep neural networks, existing models utilize a shallow architecture, \nand thus limit the expressiveness potential of deep learning. Third, neural \nnetwork models are easy to overfit on the implicit setting, because negative \ninteractions are not taken into account. To tackle these issues, we present a \nnovel recommender framework called Deep Collaborative Autoencoder (DCAE) for \nboth explicit feedback and implicit feedback, which can effectively capture the \nrelationship between interactions via its non-linear expressiveness. To \noptimize the deep architecture of DCAE, we develop a three-stage pre-training \nmechanism that combines supervised and unsupervised feature learning. Moreover, \nwe propose a popularity-based error reweighting module and a sparsity-aware \ndata-augmentation strategy for DCAE to prevent overfitting on the implicit \nsetting. Extensive experiments on three real-world datasets demonstrate that \nDCAE can significantly advance the state-of-the-art. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a07f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09043"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Bai Li, Changyou Chen, Hao Liu, Lawrence Carin", "title": "On Connecting Stochastic Gradient MCMC and Differential Privacy. (arXiv:1712.09097v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.09097", "type": "text/html"}], "timestampUsec": "1514352521979595", "comments": [], "summary": {"content": "<p>Significant success has been realized recently on applying machine learning \nto real-world applications. There have also been corresponding concerns on the \nprivacy of training data, which relates to data security and confidentiality \nissues. Differential privacy provides a principled and rigorous privacy \nguarantee on machine learning models. While it is common to design a model \nsatisfying a required differential-privacy property by injecting noise, it is \ngenerally hard to balance the trade-off between privacy and utility. We show \nthat stochastic gradient Markov chain Monte Carlo (SG-MCMC) -- a class of \nscalable Bayesian posterior sampling algorithms proposed recently -- satisfies \nstrong differential privacy with carefully chosen step sizes. We develop theory \non the performance of the proposed differentially-private SG-MCMC method. We \nconduct experiments to support our analysis and show that a standard SG-MCMC \nsampler without any modification (under a default setting) can reach \nstate-of-the-art performance in terms of both privacy and utility on Bayesian \nlearning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a081", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09097"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Romain Cosentino, Randall Balestriero, Richard Baraniuk, Ankit Patel", "title": "Overcomplete Frame Thresholding for Acoustic Scene Analysis. (arXiv:1712.09117v1 [eess.AS])", "alternate": [{"href": "http://arxiv.org/abs/1712.09117", "type": "text/html"}], "timestampUsec": "1514352521979594", "comments": [], "summary": {"content": "<p>In this work, we derive a generic overcomplete frame thresholding scheme \nbased on risk minimization. Overcomplete frames being favored for analysis \ntasks such as classification, regression or anomaly detection, we provide a way \nto leverage those optimal representations in real-world applications through \nthe use of thresholding. We validate the method on a large scale bird activity \ndetection task via the scattering network architecture performed by means of \ncontinuous wavelets, known for being an adequate dictionary in audio \nenvironments. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a086", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09117"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shameem A Puthiya Parambath, Nishant Vijayakumar, Sanjay Chawla", "title": "SAGA: A Submodular Greedy Algorithm For Group Recommendation. (arXiv:1712.09123v1 [cs.IR])", "alternate": [{"href": "http://arxiv.org/abs/1712.09123", "type": "text/html"}], "timestampUsec": "1514352521979593", "comments": [], "summary": {"content": "<p>In this paper, we propose a unified framework and an algorithm for the \nproblem of group recommendation where a fixed number of items or alternatives \ncan be recommended to a group of users. The problem of group recommendation \narises naturally in many real world contexts, and is closely related to the \nbudgeted social choice problem studied in economics. We frame the group \nrecommendation problem as choosing a subgraph with the largest group consensus \nscore in a completely connected graph defined over the item affinity matrix. We \npropose a fast greedy algorithm with strong theoretical guarantees, and show \nthat the proposed algorithm compares favorably to the state-of-the-art group \nrecommendation algorithms according to commonly used relevance and coverage \nperformance measures on benchmark dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a08c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09123"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville", "title": "Improved Training of Wasserstein GANs. (arXiv:1704.00028v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.00028", "type": "text/html"}], "timestampUsec": "1514352521979592", "comments": [], "summary": {"content": "<p>Generative Adversarial Networks (GANs) are powerful generative models, but \nsuffer from training instability. The recently proposed Wasserstein GAN (WGAN) \nmakes progress toward stable training of GANs, but sometimes can still generate \nonly low-quality samples or fail to converge. We find that these problems are \noften due to the use of weight clipping in WGAN to enforce a Lipschitz \nconstraint on the critic, which can lead to undesired behavior. We propose an \nalternative to clipping weights: penalize the norm of gradient of the critic \nwith respect to its input. Our proposed method performs better than standard \nWGAN and enables stable training of a wide variety of GAN architectures with \nalmost no hyperparameter tuning, including 101-layer ResNets and language \nmodels over discrete data. We also achieve high quality generations on CIFAR-10 \nand LSUN bedrooms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a08f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.00028"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Filipe Rodrigues, Francisco Pereira", "title": "Deep learning from crowds. (arXiv:1709.01779v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.01779", "type": "text/html"}], "timestampUsec": "1514352521979591", "comments": [], "summary": {"content": "<p>Over the last few years, deep learning has revolutionized the field of \nmachine learning by dramatically improving the state-of-the-art in various \ndomains. However, as the size of supervised artificial neural networks grows, \ntypically so does the need for larger labeled datasets. Recently, crowdsourcing \nhas established itself as an efficient and cost-effective solution for labeling \nlarge sets of data in a scalable manner, but it often requires aggregating \nlabels from multiple noisy contributors with different levels of expertise. In \nthis paper, we address the problem of learning deep neural networks from \ncrowds. We begin by describing an EM algorithm for jointly learning the \nparameters of the network and the reliabilities of the annotators. Then, a \nnovel general-purpose crowd layer is proposed, which allows us to train deep \nneural networks end-to-end, directly from the noisy labels of multiple \nannotators, using only backpropagation. We empirically show that the proposed \napproach is able to internally capture the reliability and biases of different \nannotators and achieve new state-of-the-art results for various crowdsourced \ndatasets across different settings, namely classification, regression and \nsequence labeling. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a091", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.01779"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ji Chen, Xiaodong Li", "title": "Memory-efficient Kernel PCA via Partial Matrix Sampling and Nonconvex Optimization: a Model-free Analysis of Local Minima. (arXiv:1711.01742v2 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.01742", "type": "text/html"}], "timestampUsec": "1514352521979590", "comments": [], "summary": {"content": "<p>Kernel PCA is a widely used nonlinear dimension reduction technique in \nmachine learning, but storing the kernel matrix is notoriously challenging when \nthe sample size is large. Inspired by Yi et al. [2016], where the idea of \npartial matrix sampling followed by nonconvex optimization is proposed for \nmatrix completion and robust PCA, we apply a similar approach to \nmemory-efficient Kernel PCA. In theory, with no assumptions on the kernel \nmatrix in terms of eigenvalues or eigenvectors, we established a model-free \ntheory for the low-rank approximation based on any local minimum of the \nproposed objective function. As interesting byproducts, when the underlying \npositive semidefinite matrix is assumed to be low-rank and highly structured, \ncorollaries of our main theorem improve the state-of-the-art results of Ge et \nal. [2016, 2017] for nonconvex matrix completion with no spurious local minima. \nNumerical experiments also show that our approach is competitive in terms of \napproximation accuracy compared to the well-known Nystr\\\"{o}m algorithm for \nKernel PCA. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a092", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.01742"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Katya Gonina, Navdeep Jaitly, Bo Li, Jan Chorowski, Michiel Bacchiani", "title": "State-of-the-art Speech Recognition With Sequence-to-Sequence Models. (arXiv:1712.01769v4 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.01769", "type": "text/html"}], "timestampUsec": "1514352521979589", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050606735\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050606735&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Attention-based encoder-decoder architectures such as Listen, Attend, and \nSpell (LAS), subsume the acoustic, pronunciation and language model components \nof a traditional automatic speech recognition (ASR) system into a single neural \nnetwork. In our previous work, we have shown that such architectures are \ncomparable to state-of-the-art ASR systems on dictation tasks, but it was not \nclear if such architectures would be practical for more challenging tasks such \nas voice search. In this work, we explore a variety of structural and \noptimization improvements to our LAS model which significantly improve \nperformance. On the structural side, we show that word piece models can be used \ninstead of graphemes. We introduce a multi-head attention architecture, which \noffers improvements over the commonly-used single-head attention. On the \noptimization side, we explore techniques such as synchronous training, \nscheduled sampling, label smoothing, and minimum word error rate optimization, \nwhich are all shown to improve accuracy. We present results with a \nunidirectional LSTM encoder for streaming recognition. On a 12,500 hour voice \nsearch task, we find that the proposed changes improve the WER of the LAS \nsystem from 9.2% to 5.6%, while the best conventional system achieve 6.7% WER. \nWe also test both models on a dictation dataset, and our model provide 4.1% WER \nwhile the conventional system provides 5% WER. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514352521980", "annotations": [], "published": 1514352522, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f41a096", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01769"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Greg Yang, Samuel S. Schoenholz", "title": "Mean Field Residual Networks: On the Edge of Chaos. (arXiv:1712.08969v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.08969", "type": "text/html"}], "timestampUsec": "1514351435275181", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450506560fb\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450506560fb&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We study randomly initialized residual networks using mean field theory and \nthe theory of difference equations. Classical feedforward neural networks, such \nas those with tanh activations, exhibit exponential behavior on the average \nwhen propagating inputs forward or gradients backward. The exponential forward \ndynamics causes rapid collapsing of the input space geometry, while the \nexponential backward dynamics causes drastic vanishing or exploding gradients. \nWe show, in contrast, that by adding skip connections, the network will, \ndepending on the nonlinearity, adopt subexponential forward and backward \ndynamics, and in many cases in fact polynomial. The exponents of these \npolynomials are obtained through analytic methods and proved and verified \nempirically to be correct. In terms of the \"edge of chaos\" hypothesis, these \nsubexponential and polynomial laws allow residual networks to \"hover over the \nboundary between stability and chaos,\" thus preserving the geometry of the \ninput space and the gradient information flow. In our experiments, for each \nactivation function we study here, we initialize residual networks with \ndifferent hyperparameters and train them on MNIST. Remarkably, our \ninitialization time theory can accurately predict test time performance of \nthese networks, by tracking either the expected amount of gradient explosion or \nthe expected squared distance between the images of two input vectors. \nImportantly, we show, theoretically as well as empirically, that common \ninitializations such as the Xavier or the He schemes are not optimal for \nresidual networks, because the optimal initialization variances depend on the \ndepth. Finally, we have made mathematical contributions by deriving several new \nidentities for the kernels of powers of ReLU functions by relating them to the \nzeroth Bessel function of the second kind. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda10", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08969"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "M. J. Gagen", "title": "Null Dynamical State Models of Human Cognitive Dysfunction. (arXiv:1712.09014v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.09014", "type": "text/html"}], "timestampUsec": "1514351435275180", "comments": [], "summary": {"content": "<p>The hard problem in artificial intelligence asks how the shuffling of \nsyntactical symbols in a program can lead to systems which experience semantics \nand qualia. We address this question in three stages. First, we introduce a new \nclass of human semantic symbols which appears when unexpected and drastic \nenvironmental change causes humans to become surprised, confused, uncertain, \nand in extreme cases, unresponsive, passive and dysfunctional. For this class \nof symbols, pre-learned programs become inoperative so these syntactical \nprograms cannot be the source of experienced qualia. Second, we model the \ndysfunctional human response to a radically changed environment as being the \nnatural response of any learning machine facing novel inputs from well outside \nits previous training set. In this situation, learning machines are unable to \nextract information from their input and will typically enter a dynamical state \ncharacterized by null outputs and a lack of response. This state immediately \npredicts and explains the characteristics of the semantic experiences of humans \nin similar circumstances. In the third stage, we consider learning machines \ntrained to implement multiple functions in simple sequential programs using \nenvironmental data to specify subroutine names, control flow instructions, \nmemory calls, and so on. Drastic change in any of these environmental inputs \ncan again lead to inoperative programs. By examining changes specific to people \nor locations we can model human cognitive symbols featuring these dependencies, \nsuch as attachment and grief. Our approach links known dynamical machines \nstates with human qualia and thus offers new insight into the hard problem of \nartificial intelligence. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda13", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09014"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Priyadarshini Panda, Kaushik Roy", "title": "Chaos-guided Input Structuring for Improved Learning in Recurrent Neural Networks. (arXiv:1712.09206v1 [q-bio.NC])", "alternate": [{"href": "http://arxiv.org/abs/1712.09206", "type": "text/html"}], "timestampUsec": "1514351435275179", "comments": [], "summary": {"content": "<p>Anatomical studies demonstrate that brain reformats input information to \ngenerate reliable responses for performing computations. However, it remains \nunclear how neural circuits encode complex spatio-temporal patterns. We show \nthat neural dynamics are strongly influenced by the phase alignment between the \ninput and the spontaneous chaotic activity. Input structuring along the \ndominant chaotic projections causes the chaotic trajectories to become stable \nchannels (or attractors), hence, improving the computational capability of a \nrecurrent network. Using mean field analysis, we derive the impact of input \nstructuring on the overall stability of attractors formed. Our results indicate \nthat input alignment determines the extent of intrinsic noise suppression and \nhence, alters the attractor state stability, thereby controlling the network's \ninference ability. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda17", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09206"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kenji Kawaguchi, Leslie Pack Kaelbling, Yoshua Bengio", "title": "Generalization in Deep Learning. (arXiv:1710.05468v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.05468", "type": "text/html"}], "timestampUsec": "1514351435275178", "comments": [], "summary": {"content": "<p>This paper explains why deep learning can generalize well, despite large \ncapacity and possible algorithmic instability, nonrobustness, and sharp minima, \neffectively addressing an open problem in the literature. Based on our \ntheoretical insight, this paper also proposes a family of new regularization \nmethods. Its simplest member was empirically shown to improve base models and \nachieve competitive performance on MNIST and CIFAR-10 benchmarks. Moreover, \nthis paper presents both data-dependent and data-independent generalization \nguarantees with improved convergence rates. Our results suggest several new \nopen areas of research. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.05468"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Wentao Zhu, Xiang Xiang, Trac D. Tran, Gregory D. Hager, Xiaohui Xie", "title": "Adversarial Deep Structured Nets for Mass Segmentation from Mammograms. (arXiv:1710.09288v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.09288", "type": "text/html"}], "timestampUsec": "1514351435275177", "comments": [], "summary": {"content": "<p>Mass segmentation provides effective morphological features which are \nimportant for mass diagnosis. In this work, we propose a novel end-to-end \nnetwork for mammographic mass segmentation which employs a fully convolutional \nnetwork (FCN) to model a potential function, followed by a CRF to perform \nstructured learning. Because the mass distribution varies greatly with pixel \nposition, the FCN is combined with a position priori. Further, we employ \nadversarial training to eliminate over-fitting due to the small sizes of \nmammogram datasets. Multi-scale FCN is employed to improve the segmentation \nperformance. Experimental results on two public datasets, INbreast and \nDDSM-BCRP, demonstrate that our end-to-end network achieves better performance \nthan state-of-the-art approaches. \n\\footnote{https://github.com/wentaozhu/adversarial-deep-structural-networks.git} \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda1f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.09288"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Fattaneh Jabbari, Mahdi Pakdaman Naeini, Gregory F. Cooper", "title": "Obtaining Accurate Probabilistic Causal Inference by Post-Processing Calibration. (arXiv:1712.08626v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08626", "type": "text/html"}], "timestampUsec": "1514351435275176", "comments": [], "summary": {"content": "<p>Discovery of an accurate causal Bayesian network structure from observational \ndata can be useful in many areas of science. Often the discoveries are made \nunder uncertainty, which can be expressed as probabilities. To guide the use of \nsuch discoveries, including directing further investigation, it is important \nthat those probabilities be well-calibrated. In this paper, we introduce a \nnovel framework to derive calibrated probabilities of causal relationships from \nobservational data. The framework consists of three components: (1) an \napproximate method for generating initial probability estimates of the edge \ntypes for each pair of variables, (2) the availability of a relatively small \nnumber of the causal relationships in the network for which the truth status is \nknown, which we call a calibration training set, and (3) a calibration method \nfor using the approximate probability estimates and the calibration training \nset to generate calibrated probabilities for the many remaining pairs of \nvariables. We also introduce a new calibration method based on a shallow neural \nnetwork. Our experiments on simulated data support that the proposed approach \nimproves the calibration of causal edge predictions. The results also support \nthat the approach often improves the precision and recall of predictions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda21", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08626"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alexander Trott, Caiming Xiong, Richard Socher", "title": "Interpretable Counting for Visual Question Answering. (arXiv:1712.08697v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08697", "type": "text/html"}], "timestampUsec": "1514351435275175", "comments": [], "summary": {"content": "<p>Questions that require counting a variety of objects in images remain a major \nchallenge in visual question answering (VQA). The most common approaches to VQA \ninvolve either classifying answers based on fixed length representations of \nboth the image and question or summing fractional counts estimated from each \nsection of the image. In contrast, we treat counting as a sequential decision \nprocess and force our model to make discrete choices of what to count. \nSpecifically, the model sequentially selects from detected objects and learns \ninteractions between objects that influence subsequent selections. A \ndistinction of our approach is its intuitive and interpretable output, as \ndiscrete counts are automatically grounded in the image. Furthermore, our \nmethod outperforms the state of the art architecture for VQA on multiple \nmetrics that evaluate counting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda27", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08697"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tom Hanika, Jens Zumbr&#xe4;gel", "title": "Towards Collaborative Conceptual Exploration. (arXiv:1712.08858v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08858", "type": "text/html"}], "timestampUsec": "1514351435275174", "comments": [], "summary": {"content": "<p>In domains with high knowledge distribution a natural objective is to create \nprinciple foundations for collaborative interactive learning environments. We \npresent a first mathematical characterization of a collaborative learning \ngroup, a consortium, based on closure systems of attribute sets and the \nwell-known attribute exploration algorithm from formal concept analysis. To \nthis end, we introduce (weak) local experts for subdomains of a given knowledge \ndomain. These entities are able to refute and potentially accept a given \n(implicational) query for some closure system that is a restriction of the \nwhole domain. On this we build up a consortial expert and show first insights \nabout the ability of such an expert to answer queries. Furthermore, we depict \ntechniques on how to cope with falsely accepted implications and on combining \ncounterexamples. Using notions from combinatorial design theory we further \nexpand those insights as far as providing first results on the decidability \nproblem if a given consortium is able to explore some target domain. \nApplications in conceptual knowledge acquisition as well as in collaborative \ninteractive ontology learning are at hand. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda29", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08858"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Meng Wang, Yihe Chen, Buyue Qian, Jun Liu, Sen Wang, Guodong Long, Fei Wang", "title": "Predicting Rich Drug-Drug Interactions via Biomedical Knowledge Graphs and Text Jointly Embedding. (arXiv:1712.08875v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08875", "type": "text/html"}], "timestampUsec": "1514351435275173", "comments": [], "summary": {"content": "<p>Minimizing adverse reactions caused by drug-drug interactions has always been \na momentous research topic in clinical pharmacology. Detecting all possible \ninteractions through clinical studies before a drug is released to the market \nis a demanding task. The power of big data is opening up new approaches to \ndiscover various drug-drug interactions. However, these discoveries contain a \nhuge amount of noise and provide knowledge bases far from complete and \ntrustworthy ones to be utilized. Most existing studies focus on predicting \nbinary drug-drug interactions between drug pairs but ignore other interactions. \nIn this paper, we propose a novel framework, called PRD, to predict drug-drug \ninteractions. The framework uses the graph embedding that can overcome data \nincompleteness and sparsity issues to achieve multiple DDI label prediction. \nFirst, a large-scale drug knowledge graph is generated from different sources. \nThen, the knowledge graph is embedded with comprehensive biomedical text into a \ncommon low dimensional space. Finally, the learned embeddings are used to \nefficiently compute rich DDI information through a link prediction process. To \nvalidate the effectiveness of the proposed framework, extensive experiments \nwere conducted on real-world datasets. The results demonstrate that our model \noutperforms several state-of-the-art baseline methods in terms of capability \nand accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda2d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08875"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alan F. T. Winfield", "title": "How Intelligent is your Intelligent Robot?. (arXiv:1712.08878v1 [cs.RO])", "alternate": [{"href": "http://arxiv.org/abs/1712.08878", "type": "text/html"}], "timestampUsec": "1514351435275172", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050656345\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050656345&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>How intelligent is robot A compared with robot B? And how intelligent are \nrobots A and B compared with animals (or plants) X and Y? These are both \ninteresting and deeply challenging questions. In this paper we address the \nquestion \"how intelligent is your intelligent robot?\" by proposing that \nembodied intelligence emerges from the interaction and integration of four \ndifferent and distinct kinds of intelligence. We then suggest a simple \ndiagrammatic representation on which these kinds of intelligence are shown as \nfour axes in a star diagram. A crude qualitative comparison of the intelligence \ngraphs of animals and robots both exposes and helps to explain the chronic \nintelligence deficit of intelligent robots. Finally we examine the options for \ndetermining numerical values for the four kinds of intelligence in an effort to \nmove toward a quantifiable intelligence vector. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda30", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08878"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Shiliang Sun, Changshui Zhang, Yi Zhang", "title": "Traffic Flow Forecasting Using a Spatio-Temporal Bayesian Network Predictor. (arXiv:1712.08883v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08883", "type": "text/html"}], "timestampUsec": "1514351435275171", "comments": [], "summary": {"content": "<p>A novel predictor for traffic flow forecasting, namely spatio-temporal \nBayesian network predictor, is proposed. Unlike existing methods, our approach \nincorporates all the spatial and temporal information available in a \ntransportation network to carry our traffic flow forecasting of the current \nsite. The Pearson correlation coefficient is adopted to rank the input \nvariables (traffic flows) for prediction, and the best-first strategy is \nemployed to select a subset as the cause nodes of a Bayesian network. Given the \nderived cause nodes and the corresponding effect node in the spatio-temporal \nBayesian network, a Gaussian Mixture Model is applied to describe the \nstatistical relationship between the input and output. Finally, traffic flow \nforecasting is performed under the criterion of Minimum Mean Square Error \n(M.M.S.E.). Experimental results with the urban vehicular flow data of Beijing \ndemonstrate the effectiveness of our presented spatio-temporal Bayesian network \npredictor. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda33", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08883"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "ElMouatez Billah Karbab, Mourad Debbabi, Abdelouahid Derhab, Djedjiga Mouheb", "title": "Android Malware Detection using Deep Learning on API Method Sequences. (arXiv:1712.08996v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1712.08996", "type": "text/html"}], "timestampUsec": "1514351435275170", "comments": [], "summary": {"content": "<p>Android OS experiences a blazing popularity since the last few years. This \npredominant platform has established itself not only in the mobile world but \nalso in the Internet of Things (IoT) devices. This popularity, however, comes \nat the expense of security, as it has become a tempting target of malicious \napps. Hence, there is an increasing need for sophisticated, automatic, and \nportable malware detection solutions. In this paper, we propose MalDozer, an \nautomatic Android malware detection and family attribution framework that \nrelies on sequences classification using deep learning techniques. Starting \nfrom the raw sequence of the app's API method calls, MalDozer automatically \nextracts and learns the malicious and the benign patterns from the actual \nsamples to detect Android malware. MalDozer can serve as a ubiquitous malware \ndetection system that is not only deployed on servers, but also on mobile and \neven IoT devices. We evaluate MalDozer on multiple Android malware datasets \nranging from 1K to 33K malware apps, and 38K benign apps. The results show that \nMalDozer can correctly detect malware and attribute them to their actual \nfamilies with an F1-Score of 96%-99% and a false positive rate of 0.06%-2%, \nunder all tested datasets and settings. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08996"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Luis M. Briceno-Arias, Giovanni Chierchia, Emilie Chouzenoux, Jean-Christophe Pesquet", "title": "A Random Block-Coordinate Douglas-Rachford Splitting Method with Low Computational Complexity for Binary Logistic Regression. (arXiv:1712.09131v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1712.09131", "type": "text/html"}], "timestampUsec": "1514351435275169", "comments": [], "summary": {"content": "<p>In this paper, we propose a new optimization algorithm for sparse logistic \nregression based on a stochastic version of the Douglas-Rachford splitting \nmethod. Our algorithm sweeps the training set by randomly selecting a \nmini-batch of data at each iteration, and it allows us to update the variables \nin a block coordinate manner. Our approach leverages the proximity operator of \nthe logistic loss, which is expressed with the generalized Lambert W function. \nExperiments carried out on standard datasets demonstrate the efficiency of our \napproach w.r.t. stochastic gradient-like methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda38", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09131"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Liat Cohen, Solomon Eyal Shimony, Gera Weiss", "title": "Estimating the Probability of Meeting a Deadline in Hierarchical Plans. (arXiv:1503.01327v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1503.01327", "type": "text/html"}], "timestampUsec": "1514351435275168", "comments": [], "summary": {"content": "<p>Given a hierarchical plan (or schedule) with uncertain task times, we propose \na deterministic polynomial (time and memory) algorithm for estimating the \nprobability that its meets a deadline, or, alternately, that its {\\em makespan} \nis less than a given duration. Approximation is needed as it is known that this \nproblem is NP-hard even for sequential plans (just, a sum of random variables). \nIn addition, we show two new complexity results: (1) Counting the number of \nevents that do not cross deadline is \\#P-hard; (2)~Computing the expected \nmakespan of a hierarchical plan is NP-hard. For the proposed approximation \nalgorithm, we establish formal approximation bounds and show that the time and \nmemory complexities grow polynomially with the required accuracy, the number of \nnodes in the plan, and with the size of the support of the random variables \nthat represent the durations of the primitive tasks. We examine these \napproximation bounds empirically and demonstrate, using task networks taken \nfrom the literature, how our scheme outperforms sampling techniques and exact \ncomputation in terms of accuracy and run-time. As the empirical data shows much \nbetter error bounds than guaranteed, we also suggest a method for tightening \nthe bounds in some cases. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda39", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1503.01327"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Martin C. Cooper, Stanislav &#x17d;ivn&#xfd;", "title": "The Power of Arc Consistency for CSPs Defined by Partially-Ordered Forbidden Patterns. (arXiv:1604.07981v4 [cs.CC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1604.07981", "type": "text/html"}], "timestampUsec": "1514351435275167", "comments": [], "summary": {"content": "<p>Characterising tractable fragments of the constraint satisfaction problem \n(CSP) is an important challenge in theoretical computer science and artificial \nintelligence. Forbidding patterns (generic sub-instances) provides a means of \ndefining CSP fragments which are neither exclusively language-based nor \nexclusively structure-based. It is known that the class of binary CSP instances \nin which the broken-triangle pattern (BTP) does not occur, a class which \nincludes all tree-structured instances, are decided by arc consistency (AC), a \nubiquitous reduction operation in constraint solvers. We provide a \ncharacterisation of simple partially-ordered forbidden patterns which have this \nAC-solvability property. It turns out that BTP is just one of five such \nAC-solvable patterns. The four other patterns allow us to exhibit new tractable \nclasses. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda3f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1604.07981"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ali Jazayeri, Hiroki Sayama", "title": "A Polynomial-Time Deterministic Approach to the Traveling Salesperson Problem. (arXiv:1608.01716v3 [cs.DS] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1608.01716", "type": "text/html"}], "timestampUsec": "1514351435275166", "comments": [], "summary": {"content": "<p>We propose a new polynomial-time deterministic algorithm that produces an \napproximated solution for the traveling salesperson problem. The proposed \nalgorithm ranks cities based on their priorities calculated using a power \nfunction of means and standard deviations of their distances from other cities \nand then connects the cities to their neighbors in the order of their \npriorities. When connecting a city, a neighbor is selected based on their \nneighbors' priorities calculated as another power function that additionally \nincludes their distance from the focal city to be connected. This repeats until \nall the cities are connected into a single loop. The time complexity of the \nproposed algorithm is $O(n^2)$, where $n$ is the number of cities. Numerical \nevaluation shows that, despite its simplicity, the proposed algorithm produces \nshorter tours with less time complexity than other conventional tour \nconstruction heuristics. The proposed algorithm can be used by itself or as an \ninitial tour generator for other more complex heuristic optimization \nalgorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda44", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1608.01716"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Raef Bassily, Shay Moran, Ido Nachum, Jonathan Shafer, Amir Yehudayoff", "title": "Learners that Leak Little Information. (arXiv:1710.05233v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.05233", "type": "text/html"}], "timestampUsec": "1514351435275165", "comments": [], "summary": {"content": "<p>We study learning algorithms that are restricted to using a small amount of \ninformation from their input sample. We introduce a category of learning \nalgorithms we term d-bit information learners, which are algorithms whose \noutput conveys at most d bits of information on their input. A central theme in \nthis work is that such algorithms generalize. \n</p> \n<p>We focus on the learning capacity of these algorithms, and prove sample \ncomplexity bounds with tight dependencies on the confidence and error \nparameters. We also observe connections with well studied notions such as \nsample compression schemes, Occam's razor, PAC-Bayes and differential privacy. \n</p> \n<p>We discuss an approach that allows us to prove upper bounds on the amount of \ninformation that algorithms reveal about their inputs, and also provide a lower \nbound by showing a simple concept class for which every (possibly randomized) \nempirical risk minimizer must reveal a lot of information. On the other hand, \nwe show that in the distribution-dependent setting every VC class has empirical \nrisk minimizers that do not reveal a lot of information. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda45", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.05233"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sandra Wachter, Brent Mittelstadt, Chris Russell", "title": "Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR. (arXiv:1711.00399v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.00399", "type": "text/html"}], "timestampUsec": "1514351435275164", "comments": [], "summary": {"content": "<p>There has been much discussion of the right to explanation in the EU General \nData Protection Regulation, and its existence, merits, and disadvantages. \nImplementing a right to explanation that opens the black box of algorithmic \ndecision-making faces major legal and technical barriers. Explaining the \nfunctionality of complex algorithmic decision-making systems and their \nrationale in specific cases is a technically challenging problem. Some \nexplanations may offer little meaningful information to data subjects, raising \nquestions around their value. Explanations of automated decisions need not \nhinge on the general public understanding how algorithmic systems function. \nEven though such interpretability is of great importance and should be pursued, \nexplanations can, in principle, be offered without opening the black box. \nLooking at explanations as a means to help a data subject act rather than \nmerely understand, one could gauge the scope and content of explanations \naccording to the specific goal or action they are intended to support. From the \nperspective of individuals affected by automated decision-making, we propose \nthree aims for explanations: (1) to inform and help the individual understand \nwhy a particular decision was reached, (2) to provide grounds to contest the \ndecision if the outcome is undesired, and (3) to understand what would need to \nchange in order to receive a desired result in the future, based on the current \ndecision-making model. We assess how each of these goals finds support in the \nGDPR. We suggest data controllers should offer a particular type of \nexplanation, unconditional counterfactual explanations, to support these three \naims. These counterfactual explanations describe the smallest change to the \nworld that can be made to obtain a desirable outcome, or to arrive at the \nclosest possible world, without needing to explain the internal logic of the \nsystem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda48", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.00399"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Stephen Tu, Benjamin Recht", "title": "Least-Squares Temporal Difference Learning for the Linear Quadratic Regulator. (arXiv:1712.08642v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08642", "type": "text/html"}], "timestampUsec": "1514351435275163", "comments": [], "summary": {"content": "<p>Reinforcement learning (RL) has been successfully used to solve many \ncontinuous control tasks. Despite its impressive results however, fundamental \nquestions regarding the sample complexity of RL on continuous problems remain \nopen. We study the performance of RL in this setting by considering the \nbehavior of the Least-Squares Temporal Difference (LSTD) estimator on the \nclassic Linear Quadratic Regulator (LQR) problem from optimal control. We give \nthe first finite-time analysis of the number of samples needed to estimate the \nvalue function for a fixed static state-feedback policy to within \n$\\varepsilon$-relative error. In the process of deriving our result, we give a \ngeneral characterization for when the minimum eigenvalue of the empirical \ncovariance matrix formed along the sample path of a fast-mixing stochastic \nprocess concentrates above zero, extending a result by Koltchinskii and \nMendelson in the independent covariates setting. Finally, we provide \nexperimental evidence indicating that our analysis correctly captures the \nqualitative behavior of LSTD on several LQR instances. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda4f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08642"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michael Bianco, Peter Gertoft", "title": "Sparse travel time tomography with adaptive dictionaries. (arXiv:1712.08655v1 [physics.geo-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.08655", "type": "text/html"}], "timestampUsec": "1514351435275162", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050656596\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050656596&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We develop a 2D travel time tomography method which regularizes the inversion \nby modeling groups of slowness pixels from discrete slowness maps, called \npatches, as sparse linear combinations of atoms from a dictionary. We further \npropose to learn optimal slowness dictionaries using dictionary learning, in \nparallel with the inversion. This patch regularization, which we call the local \nmodel, is integrated into the overall slowness map, called the global model. \nWhere the local model considers small-scale variations using a sparsity \nconstraint, the global model considers larger-scale features which are \nconstrained using $\\ell_2$-norm regularization. This local-global modeling \nstrategy with dictionary learning has been successful for image restoration \ntasks such as denoising and inpainting, where diverse image content is \nrecovered from noisy or incomplete measurements. We use this strategy in our \nlocally-sparse travel time tomography (LST) approach to model simultaneously \nsmooth and discontinuous slowness features. This is in contrast to conventional \ntomography methods, which constrain models to be exclusively smooth or \ndiscontinuous. We develop a $\\textit{maximum a posteriori}$ formulation for LST \nand exploit the sparsity of slowness patches using dictionary learning. We \ndemonstrate the LST approach on densely, but irregularly sampled synthetic \nslowness maps. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda53", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08655"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michael P.B. Gallaugher, Paul D. McNicholas", "title": "Mixtures of Matrix Variate Bilinear Factor Analyzers. (arXiv:1712.08664v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.08664", "type": "text/html"}], "timestampUsec": "1514351435275161", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450506a55c4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450506a55c4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Over the years data is becoming increasingly higher dimensional, which has \nprompted an increased need for dimension reduction techniques, in particular \nfor clustering and classification. Although dimension reduction in the area of \nclustering for multivariate data has been thoroughly discussed in the \nliterature there is relatively little work in the area of three way (matrix \nvariate) data. Herein, we develop a mixture of matrix variate bilinear factor \nanalyzers (MMVBFA) model for use in clustering high dimensional matrix variate \ndata. Parameter estimation is discussed, and the MMVBFA model is illustrated \nusing simulated data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda58", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08664"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Siddique Latif, Rajib Rana, Junaid Qadir, Julien Epps", "title": "Variational Autoencoders for Learning Latent Representations of Speech Emotion. (arXiv:1712.08708v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1712.08708", "type": "text/html"}], "timestampUsec": "1514351435275160", "comments": [], "summary": {"content": "<p>Latent representation of data in unsupervised fashion is a very interesting \nprocess. It provides more relevant features that can enhance the performance of \na classifier. For speech emotion recognition tasks generating effective \nfeatures is very crucial. Recently, deep generative models such as Variational \nAutoencoders (VAEs) have gained enormous success to model natural images. Being \ninspired by that in this paper, we use VAE for the modeling of emotions in \nhuman speech. We derive the latent representation of speech signal and use this \nfor classification of emotions. We demonstrate that features learned by VAEs \ncan achieve state-of-the-art emotion recognition results. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda59", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08708"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Hirofumi Ohta, Satoshi Hara", "title": "On Estimation of Conditional Modes Using Multiple Quantile Regressions. (arXiv:1712.08754v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08754", "type": "text/html"}], "timestampUsec": "1514351435275159", "comments": [], "summary": {"content": "<p>We propose an estimation method for the conditional mode when the \nconditioning variable is high-dimensional. In the proposed method, we first \nestimate the conditional density by solving quantile regressions multiple \ntimes. We then estimate the conditional mode by finding the maximum of the \nestimated conditional density. The proposed method has two advantages in that \nit is computationally stable because it has no initial parameter dependencies, \nand it is statistically efficient with a fast convergence rate. Synthetic and \nreal-world data experiments demonstrate the better performance of the proposed \nmethod compared to other existing ones. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda5c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08754"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Chao Chen, Xiao Lin, Gabriel Terejanu", "title": "An Approximate Bayesian Long Short-Term Memory Algorithm for Outlier Detection. (arXiv:1712.08773v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08773", "type": "text/html"}], "timestampUsec": "1514351435275158", "comments": [], "summary": {"content": "<p>Long Short-Term Memory networks trained with gradient descent and \nback-propagation have received great success in various applications. However, \npoint estimation of the weights of the networks is prone to over-fitting \nproblems and lacks important uncertainty information associated with the \nestimation. However, exact Bayesian neural network methods are intractable and \nnon-applicable for real-world applications. In this study, we propose an \napproximate estimation of the weights uncertainty using Ensemble Kalman Filter, \nwhich is easily scalable to a large number of weights. Furthermore, we optimize \nthe covariance of the noise distribution in the ensemble update step using \nmaximum likelihood estimation. To assess the proposed algorithm, we apply it to \noutlier detection in five real-world events retrieved from the Twitter \nplatform. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda62", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08773"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Anna D. Peterson, Arka P. Ghosh, Ranjan Maitra", "title": "Merging $K$-means with hierarchical clustering for identifying general-shaped groups. (arXiv:1712.08786v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08786", "type": "text/html"}], "timestampUsec": "1514351435275157", "comments": [], "summary": {"content": "<p>Clustering partitions a dataset such that observations placed together in a \ngroup are similar but different from those in other groups. Hierarchical and \n$K$-means clustering are two approaches but have different strengths and \nweaknesses. For instance, hierarchical clustering identifies groups in a \ntree-like structure but suffers from computational complexity in large datasets \nwhile $K$-means clustering is efficient but designed to identify homogeneous \nspherically-shaped clusters. We present a hybrid non-parametric clustering \napproach that amalgamates the two methods to identify general-shaped clusters \nand that can be applied to larger datasets. Specifically, we first partition \nthe dataset into spherical groups using $K$-means. We next merge these groups \nusing hierarchical methods with a data-driven distance measure as a stopping \ncriterion. Our proposal has the potential to reveal groups with general shapes \nand structure in a dataset. We demonstrate good performance on several \nsimulated and real datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda63", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08786"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Petros Drineas, Michael W. Mahoney", "title": "Lectures on Randomized Numerical Linear Algebra. (arXiv:1712.08880v1 [cs.DS])", "alternate": [{"href": "http://arxiv.org/abs/1712.08880", "type": "text/html"}], "timestampUsec": "1514351435275156", "comments": [], "summary": {"content": "<p>This chapter is based on lectures on Randomized Numerical Linear Algebra from \nthe 2016 Park City Mathematics Institute summer school on The Mathematics of \nData. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda67", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08880"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ahmed M. Alaa, Mihaela van der Schaar", "title": "Bayesian Nonparametric Causal Inference: Information Rates and Learning Algorithms. (arXiv:1712.08914v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.08914", "type": "text/html"}], "timestampUsec": "1514351435275155", "comments": [], "summary": {"content": "<p>We investigate the problem of estimating the causal effect of a treatment on \nindividual subjects from observational data, this is a central problem in \nvarious application domains, including healthcare, social sciences, and online \nadvertising. Within the Neyman Rubin potential outcomes model, we use the \nKullback Leibler (KL) divergence between the estimated and true distributions \nas a measure of accuracy of the estimate, and we define the information rate of \nthe Bayesian causal inference procedure as the (asymptotic equivalence class of \nthe) expected value of the KL divergence between the estimated and true \ndistributions as a function of the number of samples. Using Fano method, we \nestablish a fundamental limit on the information rate that can be achieved by \nany Bayesian estimator, and show that this fundamental limit is independent of \nthe selection bias in the observational data. We characterize the Bayesian \npriors on the potential (factual and counterfactual) outcomes that achieve the \noptimal information rate. As a consequence, we show that a particular class of \npriors that have been widely used in the causal inference literature cannot \nachieve the optimal information rate. On the other hand, a broader class of \npriors can achieve the optimal information rate. We go on to propose a prior \nadaptation procedure (which we call the information based empirical Bayes \nprocedure) that optimizes the Bayesian prior by maximizing an information \ntheoretic criterion on the recovered causal effects rather than maximizing the \nmarginal likelihood of the observed (factual) data. Building on our analysis, \nwe construct an information optimal Bayesian causal inference algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda6d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08914"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "George C. Linderman, Manas Rachh, Jeremy G. Hoskins, Stefan Steinerberger, Yuval Kluger", "title": "Efficient Algorithms for t-distributed Stochastic Neighborhood Embedding. (arXiv:1712.09005v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.09005", "type": "text/html"}], "timestampUsec": "1514351435275154", "comments": [], "summary": {"content": "<p>t-distributed Stochastic Neighborhood Embedding (t-SNE) is a method for \ndimensionality reduction and visualization that has become widely popular in \nrecent years. Efficient implementations of t-SNE are available, but they scale \npoorly to datasets with hundreds of thousands to millions of high dimensional \ndata-points. We present Fast Fourier Transform-accelerated Interpolation-based \nt-SNE (FIt-SNE), which dramatically accelerates the computation of t-SNE. The \nmost time-consuming step of t-SNE is a convolution that we accelerate by \ninterpolating onto an equispaced grid and subsequently using the fast Fourier \ntransform to perform the convolution. We also optimize the computation of input \nsimilarities in high dimensions using multi-threaded approximate nearest \nneighbors. We further present a modification to t-SNE called \"late \nexaggeration,\" which allows for easier identification of clusters in t-SNE \nembeddings. Finally, for datasets that cannot be loaded into the memory, we \npresent out-of-core randomized principal component analysis (oocPCA), so that \nthe top principal components of a dataset can be computed without ever fully \nloading the matrix, hence allowing for t-SNE of large datasets to be computed \non resource-limited machines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda74", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09005"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "David Liau, Eric Price, Zhao Song, Ger Yang", "title": "Stochastic Multi-armed Bandits in Constant Space. (arXiv:1712.09007v1 [cs.DS])", "alternate": [{"href": "http://arxiv.org/abs/1712.09007", "type": "text/html"}], "timestampUsec": "1514351435275153", "comments": [], "summary": {"content": "<p>We consider the stochastic bandit problem in the sublinear space setting, \nwhere one cannot record the win-loss record for all $K$ arms. We give an \nalgorithm using $O(1)$ words of space with regret \\[ \n</p> \n<p>\\sum_{i=1}^{K}\\frac{1}{\\Delta_i}\\log \\frac{\\Delta_i}{\\Delta}\\log T \\] where \n$\\Delta_i$ is the gap between the best arm and arm $i$ and $\\Delta$ is the gap \nbetween the best and the second-best arms. If the rewards are bounded away from \n$0$ and $1$, this is within an $O(\\log 1/\\Delta)$ factor of the optimum regret \npossible without space constraints. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda7a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09007"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ruben Loaiza-Maya, Michael Stanley Smith", "title": "Variational Bayes Estimation of Time Series Copulas for Multivariate Ordinal and Mixed Data. (arXiv:1712.09150v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.09150", "type": "text/html"}], "timestampUsec": "1514351435275152", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450506a5972\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450506a5972&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a new variational Bayes method for estimating high-dimensional \ncopulas with discrete, or discrete and continuous, margins. The method is based \non a variational approximation to a tractable augmented posterior, and is \nsubstantially faster than previous likelihood-based approaches. We use it to \nestimate drawable vine copulas for univariate and multivariate Markov ordinal \nand mixed time series. These have dimension $rT$, where $T$ is the number of \nobservations and $r$ is the number of series, and are difficult to estimate \nusing previous methods. The vine pair-copulas are carefully selected to allow \nfor heteroskedasticity, which is a common feature of ordinal time series data. \nWhen combined with flexible margins, the resulting time series models also \nallow for other common features of ordinal data, such as zero inflation, \nmultiple modes and under- or over-dispersion. Using data on homicides in New \nSouth Wales, and also U.S bankruptcies, we illustrate both the flexibility of \nthe time series copula models, and the efficacy of the variational Bayes \nestimator for copulas of up to 792 dimensions and 60 parameters. This far \nexceeds the size and complexity of copula models for discrete data that can be \nestimated using previous methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda7d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09150"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Andrew Ilyas, Ajil Jalal, Eirini Asteri, Constantinos Daskalakis, Alexandros G. Dimakis", "title": "The Robust Manifold Defense: Adversarial Training using Generative Models. (arXiv:1712.09196v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.09196", "type": "text/html"}], "timestampUsec": "1514351435275151", "comments": [], "summary": {"content": "<p>Deep neural networks are demonstrating excellent performance on several \nclassical vision problems. However, these networks are vulnerable to \nadversarial examples, minutely modified images that induce arbitrary \nattacker-chosen output from the network. We propose a mechanism to protect \nagainst these adversarial inputs based on a generative model of the data. We \nintroduce a pre-processing step that projects on the range of a generative \nmodel using gradient descent before feeding an input into a classifier. We show \nthat this step provides the classifier with robustness against first-order, \nsubstitute model, and combined adversarial attacks. Using a min-max \nformulation, we show that there may exist adversarial examples even in the \nrange of the generator, natural-looking images extremely close to the decision \nboundary for which the classifier has unjustifiedly high confidence. We show \nthat adversarial training on the generative manifold can be used to make a \nclassifier that is robust to these attacks. \n</p> \n<p>Finally, we show how our method can be applied even without a pre-trained \ngenerative model using a recent method called the deep image prior. We evaluate \nour method on MNIST, CelebA and Imagenet and show robustness against the \ncurrent state of the art attacks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda82", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09196"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yenisel Plasencia-Cala&#xf1;a, Mauricio Orozco-Alzate, Heydi M&#xe9;ndez-V&#xe1;zquez, Edel Garc&#xed;a-Reyes, Robert P.W. Duin", "title": "Scalable Prototype Selection by Genetic Algorithms and Hashing. (arXiv:1712.09277v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.09277", "type": "text/html"}], "timestampUsec": "1514351435275150", "comments": [], "summary": {"content": "<p>Classification in the dissimilarity space has become a very active research \narea since it provides a possibility to learn from data given in the form of \npairwise non-metric dissimilarities, which otherwise would be difficult to cope \nwith. The selection of prototypes is a key step for the further creation of the \nspace. However, despite previous efforts to find good prototypes, how to select \nthe best representation set remains an open issue. In this paper we proposed \nscalable methods to select the set of prototypes out of very large datasets. \nThe methods are based on genetic algorithms, dissimilarity-based hashing, and \ntwo different unsupervised and supervised scalable criteria. The unsupervised \ncriterion is based on the Minimum Spanning Tree of the graph created by the \nprototypes as nodes and the dissimilarities as edges. The supervised criterion \nis based on counting matching labels of objects and their closest prototypes. \nThe suitability of these type of algorithms is analyzed for the specific case \nof dissimilarity representations. The experimental results showed that the \nmethods select good prototypes taking advantage of the large datasets, and they \ndo so at low runtimes. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda85", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.09277"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Subhadeep Mukhopadhyay, Emanuel Parzen", "title": "Nonlinear Time Series Modeling: A Unified Perspective, Algorithm, and Application. (arXiv:1308.0642v4 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1308.0642", "type": "text/html"}], "timestampUsec": "1514351435275149", "comments": [], "summary": {"content": "<p>A new comprehensive approach to nonlinear time series analysis and modeling \nis developed in the present paper. We introduce novel data-specific \nmid-distribution based Legendre Polynomial (LP) like nonlinear transformations \nof the original time series Y(t) that enables us to adapt all the existing \nstationary linear Gaussian time series modeling strategy and made it applicable \nfor non-Gaussian and nonlinear processes in a robust fashion. The emphasis of \nthe present paper is on empirical time series modeling via the algorithm \nLPTime. We demonstrate the effectiveness of our theoretical framework using \ndaily S&amp;P 500 return data between Jan/2/1963 - Dec/31/2009. Our proposed LPTime \nalgorithm systematically discovers all the `stylized facts' of the financial \ntime series automatically all at once, which were previously noted by many \nresearchers one at a time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda8a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1308.0642"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xingguo Li, Raman Arora, Han Liu, Jarvis Haupt, Tuo Zhao", "title": "Nonconvex Sparse Learning via Stochastic Optimization with Progressive Variance Reduction. (arXiv:1605.02711v5 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1605.02711", "type": "text/html"}], "timestampUsec": "1514351435275148", "comments": [], "summary": {"content": "<p>We propose a stochastic variance reduced optimization algorithm for solving \nsparse learning problems with cardinality constraints. Sufficient conditions \nare provided, under which the proposed algorithm enjoys strong linear \nconvergence guarantees and optimal estimation accuracy in high dimensions. We \nfurther extend the proposed algorithm to an asynchronous parallel variant with \na near linear speedup. Numerical experiments demonstrate the efficiency of our \nalgorithm in terms of both parameter estimation and computational performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda8d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1605.02711"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "J&#xf6;rg L&#xfc;cke", "title": "Truncated Variational Expectation Maximization. (arXiv:1610.03113v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.03113", "type": "text/html"}], "timestampUsec": "1514351435275147", "comments": [], "summary": {"content": "<p>We derive a novel variational expectation maximization approach based on \ntruncated variational distributions. Truncated distributions are proportional \nto exact posteriors within a subset of a discrete state space and equal zero \notherwise. The novel variational approach is realized by first generalizing the \nstandard variational EM framework to include variational distributions with \nexact (`hard') zeros. A fully variational treatment of truncated distributions \nthen allows for deriving novel and mathematically grounded results, which in \nturn can be used to formulate novel efficient algorithms to optimize the \nparameters of probabilistic generative models. We find the free energies which \ncorrespond to truncated distributions to be given by concise and efficiently \ncomputable expressions, while update equations for model parameters (M-steps) \nremain in their standard form. Furthermore, we obtain generic expressions for \nexpectation values w.r.t. truncated distributions. Based on these observations, \nwe show how efficient and easily applicable meta-algorithms can be formulated \nthat guarantee a monotonic increase of the free energy. Example applications of \nthe here derived framework provide novel theoretical results and learning \nprocedures for latent variable models as well as mixture models including \nprocedures to tightly couple sampling and variational optimization approaches. \nFurthermore, by considering a special case of truncated variational \ndistributions, we can cleanly and fully embed the well-known `hard EM' \napproaches into the variational EM framework, and we show that `hard EM' (for \nmodels with discrete latents) provably optimizes a lower free energy bound of \nthe data log-likelihood. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fda99", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.03113"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Andrey Y. Lokhov, Marc Vuffray, Sidhant Misra, Michael Chertkov", "title": "Optimal structure and parameter learning of Ising models. (arXiv:1612.05024v2 [cond-mat.stat-mech] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1612.05024", "type": "text/html"}], "timestampUsec": "1514351435275146", "comments": [], "summary": {"content": "<p>Reconstruction of structure and parameters of an Ising model from binary \nsamples is a problem of practical importance in a variety of disciplines, \nranging from statistical physics and computational biology to image processing \nand machine learning. The focus of the research community shifted towards \ndeveloping universal reconstruction algorithms which are both computationally \nefficient and require the minimal amount of expensive data. We introduce a new \nmethod, Interaction Screening, which accurately estimates the model parameters \nusing local optimization problems. The algorithm provably achieves perfect \ngraph structure recovery with an information-theoretically optimal number of \nsamples, notably in the low-temperature regime which is known to be the hardest \nfor learning. The efficacy of Interaction Screening is assessed through \nextensive numerical tests on synthetic Ising models of various topologies with \ndifferent types of interactions, as well as on a real data produced by a D-Wave \nquantum computer. This study shows that the Interaction Screening method is an \nexact, tractable and optimal technique universally solving the inverse Ising \nproblem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdaa4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.05024"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pratik Jawanpuria, Bamdev Mishra", "title": "A unified framework for structured low-rank matrix learning. (arXiv:1704.07352v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.07352", "type": "text/html"}], "timestampUsec": "1514351435275145", "comments": [], "summary": {"content": "<p>We propose a novel optimization framework for learning a low-rank matrix \nwhich is also constrained to lie in a linear subspace. Exploiting the duality \ntheory, we present a factorization that decouples the low-rank and structural \nconstraints onto separate factors. The optimization problem is formulated on \nthe Riemannian spectrahedron manifold, where the Riemannian framework allows to \ndevelop computationally efficient conjugate gradient and trust-region \nalgorithms. Our approach easily accommodates popular non-smooth loss functions, \ne.g., L1-loss, and our algorithms are scalable to large-scale problem \ninstances. The numerical comparisons show that our algorithms outperform \nstate-of-the-art in standard, robust, and non-negative matrix completion, \nHankel matrix learning, and multi-task feature learning problems on various \nbenchmarks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdaaf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.07352"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michela Paganini, Luke de Oliveira, Benjamin Nachman", "title": "Accelerating Science with Generative Adversarial Networks: An Application to 3D Particle Showers in Multi-Layer Calorimeters. (arXiv:1705.02355v2 [hep-ex] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.02355", "type": "text/html"}], "timestampUsec": "1514351435275144", "comments": [], "summary": {"content": "<p>Physicists at the Large Hadron Collider (LHC) rely on detailed simulations of \nparticle collisions to build expectations of what experimental data may look \nlike under different theory modeling assumptions. Petabytes of simulated data \nare needed to develop analysis techniques, though they are expensive to \ngenerate using existing algorithms and computing resources. The modeling of \ndetectors and the precise description of particle cascades as they interact \nwith the material in the calorimeter are the most computationally demanding \nsteps in the simulation pipeline. We therefore introduce a deep neural \nnetwork-based generative model to enable high-fidelity, fast, electromagnetic \ncalorimeter simulation. There are still challenges for achieving precision \nacross the entire phase space, but our current solution can reproduce a variety \nof particle shower properties while achieving speed-up factors of up to \n100,000$\\times$. This opens the door to a new era of fast simulation that could \nsave significant computing time and disk space, while extending the reach of \nphysics searches and precision measurements at the LHC and beyond. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdab6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.02355"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Eduardo Pavez, Hilmi E. Egilmez, Antonio Ortega", "title": "Learning Graphs with Monotone Topology Properties and Multiple Connected Components. (arXiv:1705.10934v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.10934", "type": "text/html"}], "timestampUsec": "1514351435275143", "comments": [], "summary": {"content": "<p>Recent papers have formulated the problem of learning graphs from data as an \ninverse covariance estimation with graph Laplacian constraints. While such \nproblems are convex, existing methods cannot guarantee that solutions will have \nspecific graph topology properties (e.g., being a tree or k-partite), which are \ndesirable for some applications. In fact, the problem of learning a graph with \ngiven topology properties, e.g., finding the k-partite graph that best matches \nthe data, is in general non-convex. In this paper, we develop novel results \nthat provide theoretical guarantees for an approach to solve these problems by \ndecomposing them into two sub-problems, for which efficient solutions are \nknown. Specifically, a graph topology inference (GTI) step is employed to \nselect a feasible graph topology, i.e., one having the desired topology \nproperty. Then, a graph weight estimation (GWE) step is performed by solving a \ngeneralized graph Laplacian estimation problem, where edges are constrained by \nthe topology found in the GTI step. Our main result is a bound on the error of \nthe GWE step as a function of the error in the GTI step. This error bound \nindicates that the GTI step should be solved using an algorithm that \napproximates the similarity matrix (which in general corresponds to a complete \nweighted graph) by another matrix whose entries have been thresholded to zero \nto have the desired type of graph topology. The GTI stage can leverage existing \nmethods (e.g., state of the art approaches for graph coloring) which are \ntypically based on minimizing the total weight of removed edges. Since the GWE \nstage is formulated as an inverse covariance estimation problem with linear \nconstraints, it can be solved using existing convex optimization methods. We \ndemonstrate that our two step approach can achieve good results for both \nsynthetic and texture image data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdaba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.10934"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sumanta Basu, Karl Kumbier, James B. Brown, Bin Yu", "title": "Iterative Random Forests to detect predictive and stable high-order interactions. (arXiv:1706.08457v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.08457", "type": "text/html"}], "timestampUsec": "1514351435275142", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450506a5d79\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450506a5d79&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Genomics has revolutionized biology, enabling the interrogation of whole \ntranscriptomes, genome-wide binding sites for proteins, and many other \nmolecular processes. However, individual genomic assays measure elements that \ninteract in vivo as components of larger molecular machines. Understanding how \nthese high-order interactions drive gene expression presents a substantial \nstatistical challenge. Building on Random Forests (RF), Random Intersection \nTrees (RITs), and through extensive, biologically inspired simulations, we \ndeveloped the iterative Random Forest algorithm (iRF). iRF trains a \nfeature-weighted ensemble of decision trees to detect stable, high-order \ninteractions with same order of computational cost as RF. We demonstrate the \nutility of iRF for high-order interaction discovery in two prediction problems: \nenhancer activity in the early Drosophila embryo and alternative splicing of \nprimary transcripts in human derived cell lines. In Drosophila, among the 20 \npairwise transcription factor interactions iRF identifies as stable (returned \nin more than half of bootstrap replicates), 80% have been previously reported \nas physical interactions. Moreover, novel third-order interactions, e.g. \nbetween Zelda (Zld), Giant (Gt), and Twist (Twi), suggest high-order \nrelationships that are candidates for follow-up experiments. In human-derived \ncells, iRF re-discovered a central role of H3K36me3 in chromatin-mediated \nsplicing regulation, and identified novel 5th and 6th order interactions, \nindicative of multi-valent nucleosomes with specific roles in splicing \nregulation. By decoupling the order of interactions from the computational cost \nof identification, iRF opens new avenues of inquiry into the molecular \nmechanisms underlying genome biology. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdabe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.08457"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kun Yuan, Bicheng Ying, Jiageng Liu, Ali H. Sayed", "title": "Variance-Reduced Stochastic Learning by Networked Agents under Random Reshuffling. (arXiv:1708.01384v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.01384", "type": "text/html"}], "timestampUsec": "1514351435275141", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505070699e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505070699e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A new amortized variance-reduced gradient (AVRG) algorithm was developed in \n[1], which has constant storage requirement in comparison to SAGA and balanced \ngradient computations in comparison to SVRG. One key advantage of the AVRG \nstrategy is its amenability to decentralized implementations. In this work, we \nshow how AVRG can be extended to the network case where multiple learning \nagents are assumed to be connected by a graph topology. In this scenario, each \nagent observes data that is spatially distributed and all agents are only \nallowed to communicate with direct neighbors. Moreover, the amount of data \nobserved by the individual agents may differ drastically. For such situations, \nthe balanced gradient computation property of AVRG becomes a real advantage in \nreducing idle time caused by unbalanced local data storage requirements, which \nis characteristic of other reduced-variance gradient algorithms. The resulting \ndiffusion-AVRG algorithm is shown to have linear convergence to the exact \nsolution, and is much more memory efficient than other alternative algorithms. \nIn addition, by using a mini-batch strategy, it is shown that diffusion-AVRG is \nmore computationally efficient than exact diffusion or EXTRA while maintaining \nalmost the same amount of communications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdac2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.01384"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Heinrich Jiang", "title": "On the Consistency of Quick Shift. (arXiv:1710.10646v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.10646", "type": "text/html"}], "timestampUsec": "1514351435275140", "comments": [], "summary": {"content": "<p>Quick Shift is a popular mode-seeking and clustering algorithm. We present \nfinite sample statistical consistency guarantees for Quick Shift on mode and \ncluster recovery under mild distributional assumptions. We then apply our \nresults to construct a consistent modal regression algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdac7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.10646"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Nicolas Loizou, Peter Richt&#xe1;rik", "title": "Linearly convergent stochastic heavy ball method for minimizing generalization error. (arXiv:1710.10737v2 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.10737", "type": "text/html"}], "timestampUsec": "1514351435275139", "comments": [], "summary": {"content": "<p>In this work we establish the first linear convergence result for the \nstochastic heavy ball method. The method performs SGD steps with a fixed \nstepsize, amended by a heavy ball momentum term. In the analysis, we focus on \nminimizing the expected loss and not on finite-sum minimization, which is \ntypically a much harder problem. While in the analysis we constrain ourselves \nto quadratic loss, the overall objective is not necessarily strongly convex. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdacb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.10737"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, Matthew P. Lungren, Andrew Y. Ng", "title": "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning. (arXiv:1711.05225v3 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05225", "type": "text/html"}], "timestampUsec": "1514351435275138", "comments": [], "summary": {"content": "<p>We develop an algorithm that can detect pneumonia from chest X-rays at a \nlevel exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer \nconvolutional neural network trained on ChestX-ray14, currently the largest \npublicly available chest X-ray dataset, containing over 100,000 frontal-view \nX-ray images with 14 diseases. Four practicing academic radiologists annotate a \ntest set, on which we compare the performance of CheXNet to that of \nradiologists. We find that CheXNet exceeds average radiologist performance on \nthe F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and \nachieve state of the art results on all 14 diseases. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdad1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05225"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pravesh K. Kothari, David Steurer", "title": "Outlier-robust moment-estimation via sum-of-squares. (arXiv:1711.11581v2 [cs.DS] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.11581", "type": "text/html"}], "timestampUsec": "1514351435275137", "comments": [], "summary": {"content": "<p>We develop efficient algorithms for estimating low-degree moments of unknown \ndistributions in the presence of adversarial outliers. The guarantees of our \nalgorithms improve in many cases significantly over the best previous ones, \nobtained in recent works of Diakonikolas et al, Lai et al, and Charikar et al. \nWe also show that the guarantees of our algorithms match information-theoretic \nlower-bounds for the class of distributions we consider. These improved \nguarantees allow us to give improved algorithms for independent component \nanalysis and learning mixtures of Gaussians in the presence of outliers. \n</p> \n<p>Our algorithms are based on a standard sum-of-squares relaxation of the \nfollowing conceptually-simple optimization problem: Among all distributions \nwhose moments are bounded in the same way as for the unknown distribution, find \nthe one that is closest in statistical distance to the empirical distribution \nof the adversarially-corrupted sample. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdadc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11581"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Rui Gao, Xi Chen, Anton J. Kleywegt", "title": "Wasserstein Distributional Robustness and Regularization in Statistical Learning. (arXiv:1712.06050v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.06050", "type": "text/html"}], "timestampUsec": "1514351435275136", "comments": [], "summary": {"content": "<p>A central question in statistical learning is to design algorithms that not \nonly perform well on training data, but also generalize to new and unseen data. \nIn this paper, we tackle this question by formulating a distributionally robust \nstochastic optimization (DRSO) problem, which seeks a solution that minimizes \nthe worst-case expected loss over a family of distributions that are close to \nthe empirical distribution in Wasserstein distances. We establish a connection \nbetween such Wasserstein DRSO and regularization. More precisely, we identify a \nbroad class of loss functions, for which the Wasserstein DRSO is asymptotically \nequivalent to a regularization problem with a gradient-norm penalty. Such \nrelation provides new interpretations for problems involving regularization, \nincluding a great number of statistical learning problems and discrete choice \nmodels (e.g. multinomial logit). The connection suggests a principled way to \nregularize high-dimensional, non-convex problems. This is demonstrated through \nthe training of Wasserstein generative adversarial networks in deep learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514351435275", "annotations": [], "published": 1514351436, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034f3fdae5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06050"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kushagra Rastogi, Navreet Saini", "title": "Virtual Sensor Modelling using Neural Networks with Coefficient-based Adaptive Weights and Biases Search Algorithm for Diesel Engines. (arXiv:1712.08319v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.08319", "type": "text/html"}], "timestampUsec": "1514182860509778", "comments": [], "summary": {"content": "<p>With the explosion in the field of Big Data and introduction of more \nstringent emission norms every three to five years, automotive companies must \nnot only continue to enhance the fuel economy ratings of their products, but \nalso provide valued services to their customers such as delivering engine \nperformance and health reports at regular intervals. A reasonable solution to \nboth issues is installing a variety of sensors on the engine. Sensor data can \nbe used to develop fuel economy features and will directly indicate engine \nperformance. However, mounting a plethora of sensors is impractical in a very \ncost-sensitive industry. Thus, virtual sensors can replace physical sensors by \nreducing cost while capturing essential engine data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ecdc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08319"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Luiza Mici, German I. Parisi, Stefan Wermter", "title": "An Incremental Self-Organizing Architecture for Sensorimotor Learning and Prediction. (arXiv:1712.08521v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.08521", "type": "text/html"}], "timestampUsec": "1514182860509777", "comments": [], "summary": {"content": "<p>During visuomotor tasks, robots have to compensate for the temporal delays \ninherent in their sensorimotor processing systems. This capability becomes \ncrucial in a dynamic environment where the visual input is constantly changing, \ne.g. when interacting with humans. For this purpose, the robot should be \nequipped with a prediction mechanism able to use the acquired perceptual \nexperience in order to estimate possible future motor commands. In this paper, \nwe present a novel neural network architecture that learns prototypical \nvisuomotor representations and provides reliable predictions to compensate for \nthe delayed robot behavior in an online manner. We investigate the performance \nof our method in the context of a synchronization task, where a humanoid robot \nhas to generate visually perceived arm motion trajectories in synchrony with a \nhuman demonstrator. We evaluate the prediction accuracy in terms of mean \nprediction error and analyze the response of the network to novel movement \ndemonstrations. Additionally, we provide experiments with the system receiving \nincomplete data sequences, showing the robustness of the proposed architecture \nin the case of a noisy and faulty visual sensor. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ece5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08521"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jesse Engel, Matthew Hoffman, Adam Roberts", "title": "Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models. (arXiv:1711.05772v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05772", "type": "text/html"}], "timestampUsec": "1514182860509776", "comments": [], "summary": {"content": "<p>Deep generative neural networks have proven effective at both conditional and \nunconditional modeling of complex data distributions. Conditional generation \nenables interactive control, but creating new controls often requires expensive \nretraining. In this paper, we develop a method to condition generation without \nretraining the model. By post-hoc learning latent constraints, value functions \nthat identify regions in latent space that generate outputs with desired \nattributes, we can conditionally sample from these regions with gradient-based \noptimization or amortized actor functions. Combining attribute constraints with \na universal \"realism\" constraint, which enforces similarity to the data \ndistribution, we generate realistic conditional images from an unconditional \nvariational autoencoder. Further, using gradient-based optimization, we \ndemonstrate identity-preserving transformations that make the minimal \nadjustment in latent space to modify the attributes of an image. Finally, with \ndiscrete sequences of musical notes, we demonstrate zero-shot conditional \ngeneration, learning latent constraints in the absence of labeled data or a \ndifferentiable reward function. Code with dedicated cloud instance has been \nmade publicly available (https://goo.gl/STGMGx). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ecee", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05772"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Boris Chidlovskii", "title": "Multi-task learning of time series and its application to the travel demand. (arXiv:1712.08164v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08164", "type": "text/html"}], "timestampUsec": "1514182860509775", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050706c0a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050706c0a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We address the problem of modeling and prediction of a set of temporal events \nin the context of intelligent transportation systems. To leverage the \ninformation shared by different events, we propose a multi-task learning \nframework. We develop a support vector regression model for joint learning of \nmutually dependent time series. It is the regularization-based multi-task \nlearning previously developed for the classification case and extended to time \nseries. We discuss the relatedness of observed time series and first deploy the \ndynamic time warping distance measure to identify groups of similar series. \nThen we take into account both time and scale warping and propose to align \nmultiple time series by inferring their common latent representation. We test \nthe proposed models on the problem of travel demand prediction in Nancy \n(France) public transport system and analyze the benefits of multi-task \nlearning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ecf6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08164"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Saurabh Kumar, Pararth Shah, Dilek Hakkani-Tur, Larry Heck", "title": "Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning. (arXiv:1712.08266v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08266", "type": "text/html"}], "timestampUsec": "1514182860509774", "comments": [], "summary": {"content": "<p>We present a framework combining hierarchical and multi-agent deep \nreinforcement learning approaches to solve coordination problems among a \nmultitude of agents using a semi-decentralized model. The framework extends the \nmulti-agent learning setup by introducing a meta-controller that guides the \ncommunication between agent pairs, enabling agents to focus on communicating \nwith only one other agent at any step. This hierarchical decomposition of the \ntask allows for efficient exploration to learn policies that identify globally \noptimal solutions even as the number of collaborating agents increases. We show \npromising initial experimental results on a simulated distributed scheduling \nproblem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ecfd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08266"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, Marcin Detyniecki", "title": "Inverse Classification for Comparison-based Interpretability in Machine Learning. (arXiv:1712.08443v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08443", "type": "text/html"}], "timestampUsec": "1514182860509773", "comments": [], "summary": {"content": "<p>In the context of post-hoc interpretability, this paper addresses the task of \nexplaining the prediction of a classifier, considering the case where no \ninformation is available, neither on the classifier itself, nor on the \nprocessed data (neither the training nor the test data). It proposes an \ninstance-based approach whose principle consists in determining the minimal \nchanges needed to alter a prediction: given a data point whose classification \nmust be explained, the proposed method consists in identifying a close \nneighbour classified differently, where the closeness definition integrates a \nsparsity constraint. This principle is implemented using observation generation \nin the Growing Spheres algorithm. Experimental results on two datasets \nillustrate the relevance of the proposed approach that can be used to gain \nknowledge about the classifier. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed03", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08443"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kathryn Laing, Peter Adam Thwaites, John Paul Gosling", "title": "Rank Pruning for Dominance Queries in CP-Nets. (arXiv:1712.08588v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08588", "type": "text/html"}], "timestampUsec": "1514182860509772", "comments": [], "summary": {"content": "<p>Conditional preference networks (CP-nets) are a graphical representation of a \nperson's (conditional) preferences over a set of discrete variables. In this \npaper, we introduce a novel method of quantifying preference for any given \noutcome based on a CP-net representation of a user's preferences. We \ndemonstrate that these values are useful for reasoning about user preferences. \nIn particular, they allow us to order (any subset of) the possible outcomes in \naccordance with the user's preferences. Further, these values can be used to \nimprove the efficiency of outcome dominance testing. That is, given a pair of \noutcomes, we can determine which the user prefers more efficiently. We show \nthat these results also hold for CP-nets that express indifference between \nvariable values. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed13", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08588"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Clement Carbonnel, David A. Cohen, Martin C. Cooper, Stanislav Zivny", "title": "On Singleton Arc Consistency for CSPs Defined by Monotone Patterns. (arXiv:1704.06215v3 [cs.CC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.06215", "type": "text/html"}], "timestampUsec": "1514182860509771", "comments": [], "summary": {"content": "<p>Singleton arc consistency is an important type of local consistency which has \nbeen recently shown to solve all constraint satisfaction problems (CSPs) over \nconstraint languages of bounded width. We aim to characterise all classes of \nCSPs defined by a forbidden pattern that are solved by singleton arc \nconsistency and closed under removing constraints. We identify five new \npatterns whose absence ensures solvability by singleton arc consistency, four \nof which are provably maximal and three of which generalise 2-SAT. Combined \nwith simple counter-examples for other patterns, we make significant progress \ntowards a complete classification. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed27", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.06215"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Liron Cohen, Tansel Uras, Shiva Jahangiri, Aliyah Arunasalam, Sven Koenig, T.K. Satish Kumar", "title": "The FastMap Algorithm for Shortest Path Computations. (arXiv:1706.02792v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.02792", "type": "text/html"}], "timestampUsec": "1514182860509770", "comments": [], "summary": {"content": "<p>We present a new preprocessing algorithm for embedding the nodes of a given \nedge-weighted undirected graph into a Euclidean space. The Euclidean distance \nbetween any two nodes in this space approximates the length of the shortest \npath between them in the given graph. Later, at runtime, a shortest path \nbetween any two nodes can be computed with A* search using the Euclidean \ndistances as heuristic. Our preprocessing algorithm, called FastMap, is \ninspired by the data mining algorithm of the same name and runs in near-linear \ntime. Hence, FastMap is orders of magnitude faster than competing approaches \nthat produce a Euclidean embedding using Semidefinite Programming. FastMap also \nproduces admissible and consistent heuristics and therefore guarantees the \ngeneration of shortest paths. Moreover, FastMap applies to general undirected \ngraphs for which many traditional heuristics, such as the Manhattan Distance \nheuristic, are not well defined. Empirically, we demonstrate that A* search \nusing the FastMap heuristic is competitive with A* search using other \nstate-of-the-art heuristics, such as the Differential heuristic. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed34", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.02792"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sam Toyer, Felipe Trevizan, Sylvie Thi&#xe9;baux, Lexing Xie", "title": "Action Schema Networks: Generalised Policies with Deep Learning. (arXiv:1709.04271v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04271", "type": "text/html"}], "timestampUsec": "1514182860509769", "comments": [], "summary": {"content": "<p>In this paper, we introduce the Action Schema Network (ASNet): a neural \nnetwork architecture for learning generalised policies for probabilistic \nplanning problems. By mimicking the relational structure of planning problems, \nASNets are able to adopt a weight-sharing scheme which allows the network to be \napplied to any problem from a given planning domain. This allows the cost of \ntraining the network to be amortised over all problems in that domain. Further, \nwe propose a training method which balances exploration and supervised training \non small problems to produce a policy which remains robust when evaluated on \nlarger problems. In experiments, we show that ASNet's learning capability \nallows it to significantly outperform traditional non-learning planners in \nseveral challenging domains. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed47", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04271"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sahisnu Mazumder, Bing Liu", "title": "Context-aware Path Ranking for Knowledge Base Completion. (arXiv:1712.07745v1 [cs.CL] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1712.07745", "type": "text/html"}], "timestampUsec": "1514182860509768", "comments": [], "summary": {"content": "<p>Knowledge base (KB) completion aims to infer missing facts from existing ones \nin a KB. Among various approaches, path ranking (PR) algorithms have received \nincreasing attention in recent years. PR algorithms enumerate paths between \nentity pairs in a KB and use those paths as features to train a model for \nmissing fact prediction. Due to their good performances and high model \ninterpretability, several methods have been proposed. However, most existing \nmethods suffer from scalability (high RAM consumption) and feature explosion \n(trains on an exponentially large number of features) problems. This paper \nproposes a Context-aware Path Ranking (C-PR) algorithm to solve these problems \nby introducing a selective path exploration strategy. C-PR learns global \nsemantics of entities in the KB using word embedding and leverages the \nknowledge of entity semantics to enumerate contextually relevant paths using \nbidirectional random walk. Experimental results on three large KBs show that \nthe path features (fewer in number) discovered by C-PR not only improve \npredictive performance but also are more interpretable than existing baselines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed53", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07745"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Baptiste Goujaud, Eric W. Tramel, Pierre Courtiol, Mikhail Zaslavskiy, Gilles Wainrib", "title": "Robust Detection of Covariate-Treatment Interactions in Clinical Trials. (arXiv:1712.08211v1 [stat.AP])", "alternate": [{"href": "http://arxiv.org/abs/1712.08211", "type": "text/html"}], "timestampUsec": "1514182860509767", "comments": [], "summary": {"content": "<p>Detection of interactions between treatment effects and patient descriptors \nin clinical trials is critical for optimizing the drug development process. The \nincreasing volume of data accumulated in clinical trials provides a unique \nopportunity to discover new biomarkers and further the goal of personalized \nmedicine, but it also requires innovative robust biomarker detection methods \ncapable of detecting non-linear, and sometimes weak, signals. We propose a set \nof novel univariate statistical tests, based on the theory of random walks, \nwhich are able to capture non-linear and non-monotonic covariate-treatment \ninteractions. We also propose a novel combined test, which leverages the power \nof all of our proposed univariate tests into a single general-case tool. We \npresent results for both synthetic trials as well as real-world clinical \ntrials, where we compare our method with state-of-the-art techniques and \ndemonstrate the utility and robustness of our approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed5e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08211"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "N. Olspert, J. Pelt, M. J. K&#xe4;pyl&#xe4;, J. J. Lehtinen", "title": "Estimating activity cycles with probabilistic methods I. Bayesian Generalised Lomb-Scargle Periodogram with Trend. (arXiv:1712.08235v1 [astro-ph.SR])", "alternate": [{"href": "http://arxiv.org/abs/1712.08235", "type": "text/html"}], "timestampUsec": "1514182860509766", "comments": [], "summary": {"content": "<p>Period estimation is one of the central topics in astronomical time series \nanalysis, where data is often unevenly sampled. Especially challenging are \nstudies of stellar magnetic cycles, as there the periods looked for are of the \norder of the same length than the datasets themselves. The datasets often \ncontain trends, the origin of which is either a real long-term cycle or an \ninstrumental effect, but these effects cannot be reliably separated, while they \ncan lead to erroneous period determinations if not properly handled. In this \nstudy we aim at developing a method that can handle the trends properly, and by \nperforming extensive set of testing, we show that this is the optimal procedure \nwhen contrasted with methods that do not include the trend directly to the \nmodel. The effect of the noise model on the results is also investigated. We \nintroduce a Bayesian Generalised Lomb-Scargle Periodogram with Trend (BGLST), \nwhich is a probabilistic linear regression model using Gaussian priors for the \ncoefficients and uniform prior for the frequency parameter. We show, using \nsynthetic data, that when there is no prior information on whether and to what \nextent the true model of the data contains a linear trend, the introduced BGLST \nmethod is preferable to the methods which either detrend the data or leave the \ndata untrended before fitting the periodic model. Whether to use different from \nconstant noise model depends on the density of the data sampling as well as on \nthe true noise model of the process. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed69", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08235"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "N. Olspert, J. J. Lehtinen, M. J. K&#xe4;pyl&#xe4;, J. Pelt, A. Grigorievskiy", "title": "Estimating activity cycles with probabilistic methods II. The Mount Wilson Ca H&K data. (arXiv:1712.08240v1 [astro-ph.SR])", "alternate": [{"href": "http://arxiv.org/abs/1712.08240", "type": "text/html"}], "timestampUsec": "1514182860509765", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050706e7a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050706e7a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Debate over the existence versus nonexistence of trends in the stellar \nactivity-rotation diagrams continues. Application of modern time series \nanalysis tools to study the mean cycle periods in chromospheric activity index \nis lacking. We develop such models, based on Gaussian processes, for \none-dimensional time series and apply it to the extended Mount Wilson Ca H&amp;K \nsample. Our main aim is to study how the previously commonly used assumption of \nstrict harmonicity of the stellar cycles affects the results. We introduce \nthree methods of different complexity, starting with the simple harmonic model \nand followed by Gaussian Process models with periodic and quasi-periodic \ncovariance functions. We confirm the existence of two populations in the \nactivity-period diagram. We find only one significant trend in the inactive \npopulation, namely that the cycle periods get shorter with increasing rotation. \nThis is in contrast with earlier studies, that postulate the existence of \ntrends in both of the populations. In terms of rotation to cycle period ratio, \nour data is consistent with only two activity branches such that the active \nbranch merges together with the transitional one. The retrieved stellar cycles \nare uniformly distributed over the R'HK activity index, indicating that the \noperation of stellar large-scale dynamos carries smoothly over the \nVaughan-Preston gap. At around the solar activity index, however, indications \nof a disruption in the cyclic dynamo action are seen. Our study shows that \nstellar cycle estimates depend significantly on the model applied. Such \nmodel-dependent aspects include the improper treatment of linear trends and too \nsimple assumptions of the noise variance model. Assumption of strict \nharmonicity can result in the appearance of double cyclicities that seem more \nlikely to be explained by the quasi-periodicity of the cycles. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed79", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08240"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Maryan Morel, Emmanuel Bacry, St&#xe9;phane Ga&#xef;ffas, Agathe Guilloux, Fanny Leroy", "title": "ConvSCCS: convolutional self-controlled case series model for lagged adverse event detection. (arXiv:1712.08243v1 [stat.AP])", "alternate": [{"href": "http://arxiv.org/abs/1712.08243", "type": "text/html"}], "timestampUsec": "1514182860509764", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050750cb4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050750cb4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>With the increased availability of large databases of electronic health \nrecords (EHRs) comes the chance of enhancing health risks screening. Most \npost-marketing detections of adverse drug reaction (ADR) rely on physicians' \nspontaneous reports, leading to under reporting. To take up this challenge, we \ndevelop a scalable model to estimate the effect of multiple longitudinal \nfeatures (drug exposures) on a rare longitudinal outcome. Our procedure is \nbased on a conditional Poisson model also known as self-controlled case series \n(SCCS). We model the intensity of outcomes using a convolution between \nexposures and step functions, that are penalized using a combination of \ngroup-Lasso and total-variation. This approach does not require the \nspecification of precise risk periods, and allows to study in the same model \nseveral exposures at the same time. We illustrate the fact that this approach \nimproves the state-of-the-art for the estimation of the relative risks both on \nsimulations and on a cohort of diabetic patients, extracted from the large \nFrench national health insurance database (SNIIRAM). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ed8c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08243"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mohammad Reza Bonyadi, Viktor Vegh, David C. Reutens", "title": "Linear centralization classifier. (arXiv:1712.08259v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08259", "type": "text/html"}], "timestampUsec": "1514182860509763", "comments": [], "summary": {"content": "<p>A classification algorithm, called the Linear Centralization Classifier \n(LCC), is introduced. The algorithm seeks to find a transformation that best \nmaps instances from the feature space to a space where they concentrate towards \nthe center of their own classes, while maximimizing the distance between class \ncenters. We formulate the classifier as a quadratic program with quadratic \nconstraints. We then simplify this formulation to a linear program that can be \nsolved effectively using a linear programming solver (e.g., simplex-dual). We \nextend the formulation for LCC to enable the use of kernel functions for \nnon-linear classification applications. We compare our method with two standard \nclassification methods (support vector machine and linear discriminant \nanalysis) and four state-of-the-art classification methods when they are \napplied to eight standard classification datasets. Our experimental results \nshow that LCC is able to classify instances more accurately (based on the area \nunder the receiver operating characteristic) in comparison to other tested \nmethods on the chosen datasets. We also report the results for LCC with a \nparticular kernel to solve for synthetic non-linear classification problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20edab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08259"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kui Zhao, Yuechuan Li, Zhaoqian Shuai, Cheng Yang", "title": "Joint IDs Embedding and its Applications in E-commerce. (arXiv:1712.08289v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08289", "type": "text/html"}], "timestampUsec": "1514182860509762", "comments": [], "summary": {"content": "<p>E-commerce has become an important part of our daily lives and there are \ngreat challenges due to its dynamic and complex business environment. Many \nmachine intelligence techniques are developed to overcome these challenges. One \nof the essential elements in those techniques is the representation of data, \nespecially for ID-type data, e.g. item ID, product ID, store ID, brand ID, \ncategory ID etc. The classical one-hot encoding suffers sparsity problems due \nto its high dimension. Moreover, it cannot reflect the relationships among IDs, \neither homogeneous or heterogeneous ones. In this paper, we propose a novel \nhierarchical embedding model to jointly learn low-dimensional representations \nfor different types of IDs from the implicit feedback of users. Our approach \nincorporates the structural information among IDs and embeds all types of IDs \ninto a semantic space. The low-dimensional representations can be effectively \nextended to many applications including recommendation and forecast etc. We \nevaluate our approach in several scenarios of \"Hema App\" and the experimental \nresults validate the effectiveness of our approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20edbe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08289"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jan Chorowski, Ron J. Weiss, Rif A. Saurous, Samy Bengio", "title": "On Using Backpropagation for Speech Texture Generation and Voice Conversion. (arXiv:1712.08363v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1712.08363", "type": "text/html"}], "timestampUsec": "1514182860509761", "comments": [], "summary": {"content": "<p>Inspired by recent work on neural network image generation which rely on \nbackpropagation towards the network inputs, we present a proof-of-concept \nsystem for speech texture synthesis and voice conversion based on two \nmechanisms: approximate inversion of the representation learned by a speech \nrecognition neural network, and on matching statistics of neuron activations \nbetween different source and target utterances. Similar to image texture \nsynthesis and neural style transfer, the system works by optimizing a cost \nfunction with respect to the input waveform samples. To this end we use a \ndifferentiable mel-filterbank feature extraction pipeline and train a \nconvolutional CTC speech recognition network. Our system is able to extract \nspeaker characteristics from very limited amounts of target speaker data, as \nlittle as a few seconds, and can be used to generate realistic speech babble or \nreconstruct an utterance in a different voice. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20edcc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08363"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yann Ollivier", "title": "True Asymptotic Natural Gradient Optimization. (arXiv:1712.08449v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08449", "type": "text/html"}], "timestampUsec": "1514182860509760", "comments": [], "summary": {"content": "<p>We introduce a simple algorithm, True Asymptotic Natural Gradient \nOptimization (TANGO), that converges to a true natural gradient descent in the \nlimit of small learning rates, without explicit Fisher matrix estimation. \n</p> \n<p>For quadratic models the algorithm is also an instance of averaged stochastic \ngradient, where the parameter is a moving average of a \"fast\", constant-rate \ngradient descent. TANGO appears as a particular de-linearization of averaged \nSGD, and is sometimes quite different on non-quadratic models. This further \nconnects averaged SGD and natural gradient, both of which are arguably optimal \nasymptotically. \n</p> \n<p>In large dimension, small learning rates will be required to approximate the \nnatural gradient well. Still, this shows it is possible to get arbitrarily \nclose to exact natural gradient descent with a lightweight algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20edde", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08449"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Shounak Datta, Sayak Nag, Sankha Subhra Mullick, Swagatam Das", "title": "Diversifying Support Vector Machines for Boosting using Kernel Perturbation: Applications to Class Imbalance and Small Disjuncts. (arXiv:1712.08493v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08493", "type": "text/html"}], "timestampUsec": "1514182860509759", "comments": [], "summary": {"content": "<p>The diversification (generating slightly varying separating discriminators) \nof Support Vector Machines (SVMs) for boosting has proven to be a challenge due \nto the strong learning nature of SVMs. Based on the insight that perturbing the \nSVM kernel may help in diversifying SVMs, we propose two kernel perturbation \nbased boosting schemes where the kernel is modified in each round so as to \nincrease the resolution of the kernel-induced Reimannian metric in the vicinity \nof the datapoints misclassified in the previous round. We propose a method for \nidentifying the disjuncts in a dataset, dispelling the dependence on rule-based \nlearning methods for identifying the disjuncts. We also present a new \nperformance measure called Geometric Small Disjunct Index (GSDI) to quantify \nthe performance on small disjuncts for balanced as well as class imbalanced \ndatasets. Experimental comparison with a variety of state-of-the-art algorithms \nis carried out using the best classifiers of each type selected by a new \napproach inspired by multi-criteria decision making. The proposed method is \nfound to outperform the contending state-of-the-art methods on different \ndatasets (ranging from mildly imbalanced to highly imbalanced and characterized \nby varying number of disjuncts) in terms of three different performance indices \n(including the proposed GSDI). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20edee", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08493"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "R&#xe9;mi Le Priol, Ahmed Touati, Simon Lacoste-Julien", "title": "Adaptive Stochastic Dual Coordinate Ascent for Conditional Random Fields. (arXiv:1712.08577v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08577", "type": "text/html"}], "timestampUsec": "1514182860509758", "comments": [], "summary": {"content": "<p>This work investigates training Conditional Random Fields (CRF) by Stochastic \nDual Coordinate Ascent (SDCA). SDCA enjoys a linear convergence rate and a \nstrong empirical performance for independent classification problems. However, \nit has never been used to train CRF. Yet it benefits from an exact line search \nwith a single marginalization oracle call, unlike previous approaches. In this \npaper, we adapt SDCA to train CRF and we enhance it with an adaptive \nnon-uniform sampling strategy. Our preliminary experiments suggest that this \nmethod matches state-of-the-art CRF optimization techniques. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ee00", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08577"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jing Lei", "title": "Cross-Validation with Confidence. (arXiv:1703.07904v2 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.07904", "type": "text/html"}], "timestampUsec": "1514182860509757", "comments": [], "summary": {"content": "<p>Cross-validation is one of the most popular model selection methods in \nstatistics and machine learning. Despite its wide applicability, traditional \ncross validation methods tend to select overfitting models, due to the \nignorance of the uncertainty in the testing sample. We develop a new, \nstatistically principled inference tool based on cross-validation that takes \ninto account the uncertainty in the testing sample. This new method outputs a \nset of highly competitive candidate models containing the best one with \nguaranteed probability. As a consequence, our method can achieve consistent \nvariable selection in a classical linear regression setting, for which existing \ncross-validation methods require unconventional split ratios. When used for \nregularizing tuning parameter selection, the method can provide a further \ntrade-off between prediction accuracy and model interpretability. We \ndemonstrate the performance of the proposed method in several simulated and \nreal data examples. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ee0d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.07904"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Danielle Ensign, Sorelle A. Friedler, Scott Neville, Carlos Scheidegger, Suresh Venkatasubramanian", "title": "Runaway Feedback Loops in Predictive Policing. (arXiv:1706.09847v3 [cs.CY] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.09847", "type": "text/html"}], "timestampUsec": "1514182860509756", "comments": [], "summary": {"content": "<p>Predictive policing systems are increasingly used to determine how to \nallocate police across a city in order to best prevent crime. Discovered crime \ndata (e.g., arrest counts) are used to help update the model, and the process \nis repeated. Such systems have been empirically shown to be susceptible to \nrunaway feedback loops, where police are repeatedly sent back to the same \nneighborhoods regardless of the true crime rate. \n</p> \n<p>In response, we develop a mathematical model of predictive policing that \nproves why this feedback loop occurs, show empirically that this model exhibits \nsuch problems, and demonstrate how to change the inputs to a predictive \npolicing system (in a black-box manner) so the runaway feedback loop does not \noccur, allowing the true crime rate to be learned. Our results are \nquantitative: we can establish a link (in our model) between the degree to \nwhich runaway feedback causes problems and the disparity in crime rates between \nareas. Moreover, we can also demonstrate the way in which \\emph{reported} \nincidents of crime (those reported by residents) and \\emph{discovered} \nincidents of crime (i.e. those directly observed by police officers dispatched \nas a result of the predictive policing algorithm) interact: in brief, while \nreported incidents can attenuate the degree of runaway feedback, they cannot \nentirely remove it without the interventions we suggest. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ee18", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.09847"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Bita Darvish Rouhani, Mohammad Samragh, Tara Javidi, Farinaz Koushanfar", "title": "CuRTAIL: ChaRacterizing and Thwarting AdversarIal deep Learning. (arXiv:1709.02538v2 [cs.CR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.02538", "type": "text/html"}], "timestampUsec": "1514182860509755", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050750ef7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050750ef7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper proposes CuRTAIL, an end-to-end computing framework for \ncharacterizing and thwarting adversarial space in the context of Deep Learning \n(DL). The framework protects deep neural networks against adversarial samples, \nwhich are perturbed inputs carefully crafted by malicious entities to mislead \nthe underlying DL model. The precursor for the proposed methodology is a set of \nnew quantitative metrics to assess the vulnerability of various deep learning \narchitectures to adversarial samples. CuRTAIL formalizes the goal of preventing \nadversarial samples as a minimization of the space unexplored by the pertinent \nDL model that is characterized in CuRTAIL vulnerability analysis step. To \nthwart the adversarial machine learning attack, CuRTAIL introduces the concept \nof Modular Robust Redundancy (MRR) as a viable solution to achieve the \nformalized minimization objective. The MRR methodology explicitly characterizes \nthe geometry of the input data and the DL model parameters. It then learns a \nset of complementary but disjoint models which maximally cover the unexplored \nsubspaces of the target DL model, thus reducing the risk of integrity attacks. \nWe extensively evaluate CuRTAIL performance against the state-of-the-art attack \nmodels including fast-sign-gradient, Jacobian Saliency Map Attack, Deepfool, \nand Carlini&amp;WagnerL2. Proof-of-concept implementations for analyzing various \ndata collections including MNIST, CIFAR10, and ImageNet corroborate CuRTAIL \neffectiveness to detect adversarial samples in different settings. The \ncomputations in each MRR module can be performed independently. As such, \nCuRTAIL detection algorithm can be completely parallelized among multiple \nhardware settings to achieve maximum throughput. We further provide an \naccompanying API to facilitate the adoption of the proposed framework for \nvarious applications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ee1c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.02538"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xiaoyu Liu, Diyu Yang, Aly El Gamal", "title": "Deep Neural Network Architectures for Modulation Classification. (arXiv:1712.00443v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00443", "type": "text/html"}], "timestampUsec": "1514182860509754", "comments": [], "summary": {"content": "<p>In this work, we investigate the value of employing deep learning for the \ntask of wireless signal modulation recognition. Recently in [1], a framework \nhas been introduced by generating a dataset using GNU radio that mimics the \nimperfections in a real wireless channel, and uses 10 different modulation \ntypes. Further, a convolutional neural network (CNN) architecture was developed \nand shown to deliver performance that exceeds that of expert-based approaches. \nHere, we follow the framework of [1] and find deep neural network architectures \nthat deliver higher accuracy than the state of the art. We tested the \narchitecture of [1] and found it to achieve an accuracy of approximately 75% of \ncorrectly recognizing the modulation type. We first tune the CNN architecture \nof [1] and find a design with four convolutional layers and two dense layers \nthat gives an accuracy of approximately 83.8% at high SNR. We then develop \narchitectures based on the recently introduced ideas of Residual Networks \n(ResNet [2]) and Densely Connected Networks (DenseNet [3]) to achieve high SNR \naccuracies of approximately 83.5% and 86.6%, respectively. Finally, we \nintroduce a Convolutional Long Short-term Deep Neural Network (CLDNN [4]) to \nachieve an accuracy of approximately 88.5% at high SNR. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514182860510", "annotations": [], "published": 1514182861, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e20ee21", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00443"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ekaba Bisong", "title": "Benchmarking Decoupled Neural Interfaces with Synthetic Gradients. (arXiv:1712.08314v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08314", "type": "text/html"}], "timestampUsec": "1514178133841825", "comments": [], "summary": {"content": "<p>Artifical Neural Network are a particular class of learning system modeled \nafter biological neural functions with an interesting penchant for Hebbian \nlearning, that is \"neurons that wire together, fire together\". However, unlike \ntheir natural counterparts, artificial neural networks have a close and \nstringent coupling between the modules of neurons in the network. This coupling \nor locking imposes upon the network a strict and inflexible structure that \nprevent layers in the network from updating their weights until a full \nfeed-forward and backward pass has occurred. Such a constraint though may have \nsufficed for a while, is now no longer feasible in the era of very-large-scale \nmachine learning, coupled with the increased desire for parallelization of the \nlearning process across multiple computing infrastructures. To solve this \nproblem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are \nintroduced as a viable alternative to the backpropagation algorithm. This paper \nperforms a speed benchmark to compare the speed and accuracy capabilities of \nSG-DNI as over to a standard neural interface using multilayer perceptron MLP. \nSG-DNI shows good promise, in that it not only captures the learning problem, \nit is also over 3-fold faster due to it asynchronous learning capabilities. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a298f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08314"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pierre Baldi, Peter Sadowski, Zhiqin Lu", "title": "Learning in the Machine: the Symmetries of the Deep Learning Channel. (arXiv:1712.08608v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.08608", "type": "text/html"}], "timestampUsec": "1514178133841824", "comments": [], "summary": {"content": "<p>In a physical neural system, learning rules must be local both in space and \ntime. In order for learning to occur, non-local information must be \ncommunicated to the deep synapses through a communication channel, the deep \nlearning channel. We identify several possible architectures for this learning \nchannel (Bidirectional, Conjoined, Twin, Distinct) and six symmetry challenges: \n1) symmetry of architectures; 2) symmetry of weights; 3) symmetry of neurons; \n4) symmetry of derivatives; 5) symmetry of processing; and 6) symmetry of \nlearning rules. Random backpropagation (RBP) addresses the second and third \nsymmetry, and some of its variations, such as skipped RBP (SRBP) address the \nfirst and the fourth symmetry. Here we address the last two desirable \nsymmetries showing through simulations that they can be achieved and that the \nlearning channel is particularly robust to symmetry variations. Specifically, \nrandom backpropagation and its variations can be performed with the same \nnon-linear neurons used in the main input-output forward channel, and the \nconnections in the learning channel can be adapted using the same algorithm \nused in the forward channel, removing the need for any specialized hardware in \nthe learning channel. Finally, we provide mathematical results in simple cases \nshowing that the learning equations in the forward and backward channels \nconverge to fixed points, for almost any initial conditions. In symmetric \narchitectures, if the weights in both channels are small at initialization, \nadaptation in both channels leads to weights that are essentially symmetric \nduring and after learning. Biological connections are discussed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a2992", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08608"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pierre Baldi, Peter Sadowski, Zhiqin Lu", "title": "Learning in the Machine: Random Backpropagation and the Deep Learning Channel. (arXiv:1612.02734v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1612.02734", "type": "text/html"}], "timestampUsec": "1514178133841823", "comments": [], "summary": {"content": "<p>Random backpropagation (RBP) is a variant of the backpropagation algorithm \nfor training neural networks, where the transpose of the forward matrices are \nreplaced by fixed random matrices in the calculation of the weight updates. It \nis remarkable both because of its effectiveness, in spite of using random \nmatrices to communicate error information, and because it completely removes \nthe taxing requirement of maintaining symmetric weights in a physical neural \nsystem. To better understand random backpropagation, we first connect it to the \nnotions of local learning and learning channels. Through this connection, we \nderive several alternatives to RBP, including skipped RBP (SRPB), adaptive RBP \n(ARBP), sparse RBP, and their combinations (e.g. ASRBP) and analyze their \ncomputational complexity. We then study their behavior through simulations \nusing the MNIST and CIFAR-10 bechnmark datasets. These simulations show that \nmost of these variants work robustly, almost as well as backpropagation, and \nthat multiplication by the derivatives of the activation functions is \nimportant. As a follow-up, we study also the low-end of the number of bits \nrequired to communicate error information over the learning channel. We then \nprovide partial intuitive explanations for some of the remarkable properties of \nRBP and its variations. Finally, we prove several mathematical results, \nincluding the convergence to fixed points of linear chains of arbitrary length, \nthe convergence to fixed points of linear autoencoders with decorrelated data, \nthe long-term existence of solutions for linear systems with a single hidden \nlayer and convergence in special cases, and the convergence to fixed points of \nnon-linear chains, when the derivative of the activation functions is included. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a2997", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.02734"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Weiming Xiang, Hoang-Dung Tran, Taylor T. Johnson", "title": "Reachable Set Computation and Safety Verification for Neural Networks with ReLU Activations. (arXiv:1712.08163v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08163", "type": "text/html"}], "timestampUsec": "1514178133841822", "comments": [], "summary": {"content": "<p>Neural networks have been widely used to solve complex real-world problems. \nDue to the complicate, nonlinear, non-convex nature of neural networks, formal \nsafety guarantees for the output behaviors of neural networks will be crucial \nfor their applications in safety-critical systems.In this paper, the output \nreachable set computation and safety verification problems for a class of \nneural networks consisting of Rectified Linear Unit (ReLU) activation functions \nare addressed. A layer-by-layer approach is developed to compute output \nreachable set. The computation is formulated in the form of a set of \nmanipulations for a union of polyhedra, which can be efficiently applied with \nthe aid of polyhedron computation tools. Based on the output reachable set \ncomputation results, the safety verification for a ReLU neural network can be \nperformed by checking the intersections of unsafe regions and output reachable \nset described by a union of polyhedra. A numerical example of a randomly \ngenerated ReLU neural network is provided to show the effectiveness of the \napproach developed in this paper. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a299b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08163"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos Kalogerakis, Subhransu Maji", "title": "CSGNet: Neural Shape Parser for Constructive Solid Geometry. (arXiv:1712.08290v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.08290", "type": "text/html"}], "timestampUsec": "1514178133841821", "comments": [], "summary": {"content": "<p>We present a neural architecture that takes as input a 2D or 3D shape and \ninduces a program to generate it. The in- structions in our program are based \non constructive solid geometry principles, i.e., a set of boolean operations on \nshape primitives defined recursively. Bottom-up techniques for this task that \nrely on primitive detection are inherently slow since the search space over \npossible primitive combi- nations is large. In contrast, our model uses a \nrecurrent neural network conditioned on the input shape to produce a sequence \nof instructions in a top-down manner and is sig- nificantly faster. It is also \nmore effective as a shape detec- tor than existing state-of-the-art detection \ntechniques. We also demonstrate that our network can be trained on novel \ndatasets without ground-truth program annotations through policy gradient \ntechniques. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a29a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08290"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "James Sunthonlap, Phuoc Nguyen, Zilong Ye", "title": "Intelligent Device Discovery in the Internet of Things - Enabling the Robot Society. (arXiv:1712.08296v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08296", "type": "text/html"}], "timestampUsec": "1514178133841820", "comments": [], "summary": {"content": "<p>The Internet of Things (IoT) is continuously growing to connect billions of \nsmart devices anywhere and anytime in an Internet-like structure, which enables \na variety of applications, services and interactions between human and objects. \nIn the future, the smart devices are supposed to be able to autonomously \ndiscover a target device with desired features and generate a set of entirely \nnew services and applications that are not supervised or even imagined by human \nbeings. The pervasiveness of smart devices, as well as the heterogeneity of \ntheir design and functionalities, raise a major concern: How can a smart device \nefficiently discover a desired target device? In this paper, we propose a \nSocial-Aware and Distributed (SAND) scheme that achieves a fast, scalable and \nefficient device discovery in the IoT. The proposed SAND scheme adopts a novel \ndevice ranking criteria that measures the device's degree, social relationship \ndiversity, clustering coefficient and betweenness. Based on the device ranking \ncriteria, the discovery request can be guided to travel through critical \ndevices that stand at the major intersections of the network, and thus quickly \nreach the desired target device by contacting only a limited number of \nintermediate devices. With the help of such an intelligent device discovery as \nSAND, the IoT devices, as well as other computing facilities, software and data \non the Internet, can autonomously establish new social connections with each \nother as human being do. They can formulate self-organized computing groups to \nperform required computing tasks, facilitate a fusion of a variety of computing \nservice, network service and data to generate novel applications and services, \nevolve from the individual aritificial intelligence to the collaborative \nintelligence, and eventually enable the birth of a robot society. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a29a5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08296"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lajanugen Logeswaran, Honglak Lee, Dragomir Radev", "title": "Sentence Ordering and Coherence Modeling using Recurrent Neural Networks. (arXiv:1611.02654v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.02654", "type": "text/html"}], "timestampUsec": "1514178133841819", "comments": [], "summary": {"content": "<p>Modeling the structure of coherent texts is a key NLP problem. The task of \ncoherently organizing a given set of sentences has been commonly used to build \nand evaluate models that understand such structure. We propose an end-to-end \nunsupervised deep learning approach based on the set-to-sequence framework to \naddress this problem. Our model strongly outperforms prior methods in the order \ndiscrimination task and a novel task of ordering abstracts from scientific \narticles. Furthermore, our work shows that useful text representations can be \nobtained by learning to order sentences. Visualizing the learned sentence \nrepresentations shows that the model captures high-level logical structure in \nparagraphs. Our representations perform comparably to state-of-the-art \npre-training methods on sentence similarity and paraphrase detection tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a29a9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.02654"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aditya Grover, Stefano Ermon", "title": "Boosted Generative Models. (arXiv:1702.08484v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1702.08484", "type": "text/html"}], "timestampUsec": "1514178133841818", "comments": [], "summary": {"content": "<p>We propose a novel approach for using unsupervised boosting to create an \nensemble of generative models, where models are trained in sequence to correct \nearlier mistakes. Our meta-algorithmic framework can leverage any existing base \nlearner that permits likelihood evaluation, including recent deep expressive \nmodels. Further, our approach allows the ensemble to include discriminative \nmodels trained to distinguish real data from model-generated data. We show \ntheoretical conditions under which incorporating a new model in the ensemble \nwill improve the fit and empirically demonstrate the effectiveness of our \nblack-box boosting algorithms on density estimation, classification, and sample \ngeneration on benchmark datasets for a wide range of generative models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a29ad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1702.08484"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anna Leontjeva, Ilya Kuzovkin", "title": "Combining Static and Dynamic Features for Multivariate Sequence Classification. (arXiv:1712.08160v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08160", "type": "text/html"}], "timestampUsec": "1514178133841817", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505075112a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505075112a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Model precision in a classification task is highly dependent on the feature \nspace that is used to train the model. Moreover, whether the features are \nsequential or static will dictate which classification method can be applied as \nmost of the machine learning algorithms are designed to deal with either one or \nanother type of data. In real-life scenarios, however, it is often the case \nthat both static and dynamic features are present, or can be extracted from the \ndata. In this work, we demonstrate how generative models such as Hidden Markov \nModels (HMM) and Long Short-Term Memory (LSTM) artificial neural networks can \nbe used to extract temporal information from the dynamic data. We explore how \nthe extracted information can be combined with the static features in order to \nimprove the classification performance. We evaluate the existing techniques and \nsuggest a hybrid approach, which outperforms other methods on several public \ndatasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a29b3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08160"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Edward Raff, Jared Sylvester, Steven Mills", "title": "Fair Forests: Regularized Tree Induction to Minimize Model Bias. (arXiv:1712.08197v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08197", "type": "text/html"}], "timestampUsec": "1514178133841816", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450507a726a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450507a726a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The potential lack of fairness in the outputs of machine learning algorithms \nhas recently gained attention both within the research community as well as in \nsociety more broadly. Surprisingly, there is no prior work developing \ntree-induction algorithms for building fair decision trees or fair random \nforests. These methods have widespread popularity as they are one of the few to \nbe simultaneously interpretable, non-linear, and easy-to-use. In this paper we \ndevelop, to our knowledge, the first technique for the induction of fair \ndecision trees. We show that our \"Fair Forest\" retains the benefits of the \ntree-based approach, while providing both greater accuracy and fairness than \nother alternatives, for both \"group fairness\" and \"individual fairness.'\" We \nalso introduce new measures for fairness which are able to handle multinomial \nand continues attributes as well as regression problems, as opposed to binary \nattributes and labels only. Finally, we demonstrate a new, more robust \nevaluation procedure for algorithms that considers the dataset in its entirety \nrather than only a specific protected attribute. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a29b6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08197"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tengyuan Liang", "title": "How Well Can Generative Adversarial Networks (GAN) Learn Densities: A Nonparametric View. (arXiv:1712.08244v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08244", "type": "text/html"}], "timestampUsec": "1514178133841815", "comments": [], "summary": {"content": "<p>We study in this paper the rate of convergence for learning densities under \nthe Generative Adversarial Networks (GANs) framework, borrowing insights from \nnonparametric statistics. We introduce an improved GAN estimator that achieves \na faster rate, through leveraging the level of smoothness in the target density \nand the evaluation metric, which in theory remedies the mode collapse problem \nreported in the literature. A minimax lower bound is constructed to show that \nwhen the dimension is large, the exponent in the rate for the new GAN estimator \nis near optimal. One can view our results as answering in a quantitative way \nhow well GAN learns a wide range of densities with different smoothness \nproperties, under a hierarchy of evaluation metrics. As a byproduct, we also \nobtain improved bounds for GAN with deeper ReLU discriminator network. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a29b8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08244"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Soufiane Belharbi, Cl&#xe9;ment Chatelain, Romain H&#xe9;rault, S&#xe9;bastien Adam", "title": "Neural Networks Regularization Through Class-wise Invariant Representation Learning. (arXiv:1709.01867v4 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.01867", "type": "text/html"}], "timestampUsec": "1514178133841814", "comments": [], "summary": {"content": "<p>Training deep neural networks is known to require a large number of training \nsamples. However, in many applications only few training samples are available. \nIn this work, we tackle the issue of training neural networks for \nclassification task when few training samples are available. We attempt to \nsolve this issue by proposing a new regularization term that constrains the \nhidden layers of a network to learn class-wise invariant representations. In \nour regularization framework, learning invariant representations is generalized \nto the class membership where samples with the same class should have the same \nrepresentation. Numerical experiments over MNIST and its variants showed that \nour proposal helps improving the generalization of neural network particularly \nwhen trained with few samples. We provide the source code of our framework \nhttps://github.com/sbelharbi/learning-class-invariant-features . \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1514178133842", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034e1a29bc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.01867"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Benjamin Doerr", "title": "An Elementary Analysis of the Probability That a Binomial Random Variable Exceeds Its Expectation. (arXiv:1712.00519v3 [math.PR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00519", "type": "text/html"}], "timestampUsec": "1513924821253989", "comments": [], "summary": {"content": "<p>We give an elementary proof of the fact that a binomial random variable $X$ \nwith parameters $n$ and $0.29/n \\le p &lt; 1$ with probability at least $1/4$ \nstrictly exceeds its expectation. We also show that for $1/n \\le p &lt; 1 - 1/n$, \n$X$ exceeds its expectation by more than one with probability at least \n$0.0370$. Both probabilities approach $1/2$ when $np$ and $n(1-p)$ tend to \ninfinity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b2c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00519"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Vladimir Marochko, Leonard Johard, Manuel Mazzara, Luca Longo", "title": "Pseudorehearsal in actor-critic agents with neural network function approximation. (arXiv:1712.07686v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07686", "type": "text/html"}], "timestampUsec": "1513924821253988", "comments": [], "summary": {"content": "<p>Catastrophic forgetting has a significant negative impact in reinforcement \nlearning. The purpose of this study is to investigate how pseudorehearsal can \nchange performance of an actor-critic agent with neural-network function \napproximation. We tested agent in a pole balancing task and compared different \npseudorehearsal approaches. We have found that pseudorehearsal can assist \nlearning and decrease forgetting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b30", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07686"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Rajesh Chidambaram", "title": "Towards an unanimous international regulatory body for responsible use of Artificial Intelligence [UIRB-AI]. (arXiv:1712.07752v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07752", "type": "text/html"}], "timestampUsec": "1513924821253987", "comments": [], "summary": {"content": "<p>Artificial Intelligence (AI), is once again in the phase of drastic \nadvancements. Unarguably, the technology itself can revolutionize the way we \nlive our everyday life. But the exponential growth of technology poses a \ndaunting task for policy researchers and law makers in making amendments to the \nexisting norms. In addition, not everyone in the society is studying the \npotential socio-economic intricacies and cultural drifts that AI can bring \nabout. It is prudence to reflect from our historical past to propel the \ndevelopment of technology in the right direction. To benefit the society of the \npresent and future, I scientifically explore the societal impact of AI. While \nthere are many public and private partnerships working on similar aspects, here \nI describe the necessity for an Unanimous International Regulatory Body for all \napplications of AI (UIRB-AI). I also discuss the benefits and drawbacks of such \nan organization. To combat any drawbacks in the formation of an UIRB-AI, both \nidealistic and pragmatic perspectives are discussed alternatively. The paper \nfurther advances the discussion by proposing novel policies on how such \norganization should be structured and how it can bring about a win-win \nsituation for everyone in the society. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b38", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07752"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Seonmo Kim, Stephen McCamant", "title": "Bit-Vector Model Counting using Statistical Estimation. (arXiv:1712.07770v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1712.07770", "type": "text/html"}], "timestampUsec": "1513924821253986", "comments": [], "summary": {"content": "<p>Approximate model counting for bit-vector SMT formulas (generalizing \\#SAT) \nhas many applications such as probabilistic inference and quantitative \ninformation-flow security, but it is computationally difficult. Adding random \nparity constraints (XOR streamlining) and then checking satisfiability is an \neffective approximation technique, but it requires a prior hypothesis about the \nmodel count to produce useful results. We propose an approach inspired by \nstatistical estimation to continually refine a probabilistic estimate of the \nmodel count for a formula, so that each XOR-streamlined query yields as much \ninformation as possible. We implement this approach, with an approximate \nprobability model, as a wrapper around an off-the-shelf SMT solver or SAT \nsolver. Experimental results show that the implementation is faster than the \nmost similar previous approaches which used simpler refinement strategies. The \ntechnique also lets us model count formulas over floating-point constraints, \nwhich we demonstrate with an application to a vulnerability in differential \nprivacy mechanisms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b3e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07770"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Leon Bottou, Martin Arjovsky, David Lopez-Paz, Maxime Oquab", "title": "Geometrical Insights for Implicit Generative Modeling. (arXiv:1712.07822v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07822", "type": "text/html"}], "timestampUsec": "1513924821253985", "comments": [], "summary": {"content": "<p>Learning algorithms for implicit generative models can optimize a variety of \ncriteria that measure how the data distribution differs from the implicit model \ndistribution, including the Wasserstein distance, the Energy distance, and the \nMaximum Mean Discrepancy criterion. A careful look at the geometries induced by \nthese distances on the space of probability measures reveals interesting \ndifferences. In particular, we can establish surprising approximate global \nconvergence guarantees for the $1$-Wasserstein distance,even when the \nparametric generator has a nonconvex parametrization. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b45", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07822"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Soma Suzuki", "title": "Multiagent-based Participatory Urban Simulation through Inverse Reinforcement Learning. (arXiv:1712.07887v1 [cs.MA])", "alternate": [{"href": "http://arxiv.org/abs/1712.07887", "type": "text/html"}], "timestampUsec": "1513924821253984", "comments": [], "summary": {"content": "<p>The multiagent-based participatory simulation features prominently in urban \nplanning as the acquired model is considered as the hybrid system of the domain \nand the local knowledge. However, the key problem of generating realistic \nagents for particular social phenomena invariably remains. The existing models \nhave attempted to dictate the factors involving human behavior, which appeared \nto be intractable. In this paper, Inverse Reinforcement Learning (IRL) is \nintroduced to address this problem. IRL is developed for computational modeling \nof human behavior and has achieved great successes in robotics, psychology and \nmachine learning. The possibilities presented by this new style of modeling are \ndrawn out as conclusions, and the relative challenges with this modeling are \nhighlighted. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07887"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zhang-Wei Hong, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang Chang, Chun-Yi Lee", "title": "A Deep Policy Inference Q-Network for Multi-Agent Systems. (arXiv:1712.07893v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07893", "type": "text/html"}], "timestampUsec": "1513924821253983", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450507a7e04\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450507a7e04&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present DPIQN, a deep policy inference Q-network that targets multi-agent \nsystems composed of controllable agents, collaborators, and opponents that \ninteract with each other. We focus on one challenging issue in such \nsystems---modeling agents with varying strategies---and propose to employ \n\"policy features\" learned from raw observations (e.g., raw images) of \ncollaborators and opponents by inferring their policies. DPIQN incorporates the \nlearned policy features as a hidden vector into its own deep Q-network (DQN), \nsuch that it is able to predict better Q values for the controllable agents \nthan the state-of-the-art deep reinforcement learning models. We further \npropose an enhanced version of DPIQN, called deep recurrent policy inference \nQ-network (DRPIQN), for handling partial observability. Both DPIQN and DRPIQN \nare trained by an adaptive training procedure, which adjusts the network's \nattention to learn the policy features and its own Q-values at different phases \nof the training process. We present a comprehensive analysis of DPIQN and \nDRPIQN, and highlight their effectiveness and generalizability in various \nmulti-agent settings. Our models are evaluated in a classic soccer game \ninvolving both competitive and collaborative scenarios. Experimental results \nperformed on 1 vs. 1 and 2 vs. 2 games show that DPIQN and DRPIQN demonstrate \nsuperior performance to the baseline DQN and deep recurrent Q-network (DRQN) \nmodels. We also explore scenarios in which collaborators or opponents \ndynamically change their policies, and show that DPIQN and DRPIQN do lead to \nbetter overall performance in terms of stability and mean scores. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b52", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07893"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mario Lezcano Casado, Atilim Gunes Baydin, David Martinez Rubio, Tuan Anh Le, Frank Wood, Lukas Heinrich, Gilles Louppe, Kyle Cranmer, Karen Ng, Wahid Bhimji, Prabhat", "title": "Improvements to Inference Compilation for Probabilistic Programming in Large-Scale Scientific Simulators. (arXiv:1712.07901v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07901", "type": "text/html"}], "timestampUsec": "1513924821253982", "comments": [], "summary": {"content": "<p>We consider the problem of Bayesian inference in the family of probabilistic \nmodels implicitly defined by stochastic generative models of data. In \nscientific fields ranging from population biology to cosmology, low-level \nmechanistic components are composed to create complex generative models. These \nmodels lead to intractable likelihoods and are typically non-differentiable, \nwhich poses challenges for traditional approaches to inference. We extend \nprevious work in \"inference compilation\", which combines universal \nprobabilistic programming and deep learning methods, to large-scale scientific \nsimulators, and introduce a C++ based probabilistic programming library called \nCPProb. We successfully use CPProb to interface with SHERPA, a large code-base \nused in particle physics. Here we describe the technical innovations realized \nand planned for this library. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b56", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07901"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jordi de la Torre, Aida Valls, Domenec Puig", "title": "A Deep Learning Interpretable Classifier for Diabetic Retinopathy Disease Grading. (arXiv:1712.08107v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.08107", "type": "text/html"}], "timestampUsec": "1513924821253981", "comments": [], "summary": {"content": "<p>Deep neural network models have been proven to be very successful in image \nclassification tasks, also for medical diagnosis, but their main concern is its \nlack of interpretability. They use to work as intuition machines with high \nstatistical confidence but unable to give interpretable explanations about the \nreported results. The vast amount of parameters of these models make difficult \nto infer a rationale interpretation from them. In this paper we present a \ndiabetic retinopathy interpretable classifier able to classify retine images \ninto the different levels of disease severity and of explaining its results by \nassigning a score for every point in the hidden and input space, evaluating its \ncontribution to the final classification in a linear way. The generated visual \nmaps can be interpreted by an expert in order to compare its own knowledge with \nthe interpretation given by the model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b5d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08107"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tianyu Li, Guillaume Rabusseau, Doina Precup", "title": "Neural Network Based Nonlinear Weighted Finite Automata. (arXiv:1709.04380v2 [cs.FL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04380", "type": "text/html"}], "timestampUsec": "1513924821253980", "comments": [], "summary": {"content": "<p>Weighted finite automata (WFA) can expressively model functions defined over \nstrings but are inherently linear models. Given the recent successes of \nnonlinear models in machine learning, it is natural to wonder whether \nex-tending WFA to the nonlinear setting would be beneficial. In this paper, we \npropose a novel model of neural network based nonlinearWFA model (NL-WFA) along \nwith a learning algorithm. Our learning algorithm is inspired by the spectral \nlearning algorithm for WFAand relies on a nonlinear decomposition of the \nso-called Hankel matrix, by means of an auto-encoder network. The expressive \npower of NL-WFA and the proposed learning algorithm are assessed on both \nsynthetic and real-world data, showing that NL-WFA can lead to smaller model \nsizes and infer complex grammatical structures from data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b67", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04380"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mauro Annarumma, Giovanni Montana", "title": "Deep metric learning for multi-labelled radiographs. (arXiv:1712.07682v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07682", "type": "text/html"}], "timestampUsec": "1513924821253978", "comments": [], "summary": {"content": "<p>Many radiological studies can reveal the presence of several co-existing \nabnormalities, each one represented by a distinct visual pattern. In this \narticle we address the problem of learning a distance metric for plain \nradiographs that captures a notion of \"radiological similarity\": two chest \nradiographs are considered to be similar if they share similar abnormalities. \nDeep convolutional neural networks (DCNs) are used to learn a low-dimensional \nembedding for the radiographs that is equipped with the desired metric. Two \nloss functions are proposed to deal with multi-labelled images and potentially \nnoisy labels. We report on a large-scale study involving over 745,000 chest \nradiographs whose labels were automatically extracted from free-text \nradiological reports through a natural language processing system. Using 4,500 \nvalidated exams, we demonstrate that the methodology performs satisfactorily on \nclustering and image retrieval tasks. Remarkably, the learned metric separates \nnormal exams from those having radiological abnormalities. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b70", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07682"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Brooke E. Husic, Vijay S. Pande", "title": "Unsupervised learning of dynamical and molecular similarity using variance minimization. (arXiv:1712.07704v1 [physics.bio-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.07704", "type": "text/html"}], "timestampUsec": "1513924821253977", "comments": [], "summary": {"content": "<p>In this report, we present an unsupervised machine learning method for \ndetermining groups of molecular systems according to similarity in their \ndynamics or structures using Ward's minimum variance objective function. We \nfirst apply the minimum variance clustering to a set of simulated tripeptides \nusing the information theoretic Jensen-Shannon divergence between Markovian \ntransition matrices in order to gain insight into how point mutations affect \nprotein dynamics. Then, we extend the method to partition two chemoinformatic \ndatasets according to structural similarity to motivate a train/validation/test \nsplit for supervised learning that avoids overfitting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b79", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07704"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Dejiao Zhang, Yifan Sun, Brian Eriksson, Laura Balzano", "title": "Deep Unsupervised Clustering Using Mixture of Autoencoders. (arXiv:1712.07788v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.07788", "type": "text/html"}], "timestampUsec": "1513924821253976", "comments": [], "summary": {"content": "<p>Unsupervised clustering is one of the most fundamental challenges in machine \nlearning. A popular hypothesis is that data are generated from a union of \nlow-dimensional nonlinear manifolds; thus an approach to clustering is \nidentifying and separating these manifolds. In this paper, we present a novel \napproach to solve this problem by using a mixture of autoencoders. Our model \nconsists of two parts: 1) a collection of autoencoders where each autoencoder \nlearns the underlying manifold of a group of similar objects, and 2) a mixture \nassignment neural network, which takes the concatenated latent vectors from the \nautoencoders as input and infers the distribution over clusters. By jointly \noptimizing the two parts, we simultaneously assign data to clusters and learn \nthe underlying manifolds of each cluster. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b89", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07788"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Amal Agarwal, Lingzhou Xue", "title": "Model-Based Clustering of Nonparametric Weighted Networks. (arXiv:1712.07800v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.07800", "type": "text/html"}], "timestampUsec": "1513924821253975", "comments": [], "summary": {"content": "<p>Water pollution is a major global environmental problem, and it poses a great \nenvironmental risk to public health and biological diversity. This work is \nmotivated by assessing the potential environmental threat of coal mining \nthrough increased sulfate concentrations in river networks, which do not belong \nto any simple parametric distribution. However, existing network models mainly \nfocus on binary or discrete networks and weighted networks with known \nparametric weight distributions. We propose a principled nonparametric weighted \nnetwork model based on exponential-family random graph models and local \nlikelihood estimation and study its model-based clustering with application to \nlarge-scale water pollution network analysis. We do not require any parametric \ndistribution assumption on network weights. The proposed method greatly extends \nthe methodology and applicability of statistical network models. Furthermore, \nit is scalable to large and complex networks in large-scale environmental \nstudies and geoscientific research. The power of our proposed methods is \ndemonstrated in simulation studies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b93", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07800"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Takashi Kurokawa, Taihei Oki, Hiromichi Nagao", "title": "Multi-dimensional Graph Fourier Transform. (arXiv:1712.07811v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.07811", "type": "text/html"}], "timestampUsec": "1513924821253974", "comments": [], "summary": {"content": "<p>Many signals on Cartesian product graphs appear in the real world, such as \ndigital images, sensor observation time series, and movie ratings on Netflix. \nThese signals are \"multi-dimensional\" and have directional characteristics \nalong each factor graph. However, the existing graph Fourier transform does not \ndistinguish these directions, and assigns 1-D spectra to signals on product \ngraphs. Further, these spectra are often multi-valued at some frequencies. Our \nmain result is a multi-dimensional graph Fourier transform that solves such \nproblems associated with the conventional GFT. Using algebraic properties of \nCartesian products, the proposed transform rearranges 1-D spectra obtained by \nthe conventional GFT into the multi-dimensional frequency domain, of which each \ndimension represents a directional frequency along each factor graph. Thus, the \nmulti-dimensional graph Fourier transform enables directional frequency \nanalysis, in addition to frequency analysis with the conventional GFT. \nMoreover, this rearrangement resolves the multi-valuedness of spectra in some \ncases. The multi-dimensional graph Fourier transform is a foundation of novel \nfilterings and stationarities that utilize dimensional information of graph \nsignals, which are also discussed in this study. The proposed methods are \napplicable to a wide variety of data that can be regarded as signals on \nCartesian product graphs. This study also notes that multivariate graph signals \ncan be regarded as 2-D univariate graph signals. This correspondence provides \nnatural definitions of the multivariate graph Fourier transform and the \nmultivariate stationarity based on their 2-D univariate versions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b98", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07811"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Prateek Jain, Purushottam Kar", "title": "Non-convex Optimization for Machine Learning. (arXiv:1712.07897v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07897", "type": "text/html"}], "timestampUsec": "1513924821253973", "comments": [], "summary": {"content": "<p>A vast majority of machine learning algorithms train their models and perform \ninference by solving optimization problems. In order to capture the learning \nand prediction problems accurately, structural constraints such as sparsity or \nlow rank are frequently imposed or else the objective itself is designed to be \na non-convex function. This is especially true of algorithms that operate in \nhigh-dimensional spaces or that train non-linear models such as tensor models \nand deep networks. \n</p> \n<p>The freedom to express the learning problem as a non-convex optimization \nproblem gives immense modeling power to the algorithm designer, but often such \nproblems are NP-hard to solve. A popular workaround to this has been to relax \nnon-convex problems to convex ones and use traditional methods to solve the \n(convex) relaxed optimization problems. However this approach may be lossy and \nnevertheless presents significant challenges for large scale optimization. \n</p> \n<p>On the other hand, direct approaches to non-convex optimization have met with \nresounding success in several domains and remain the methods of choice for the \npractitioner, as they frequently outperform relaxation-based techniques - \npopular heuristics include projected gradient descent and alternating \nminimization. However, these are often poorly understood in terms of their \nconvergence and other properties. \n</p> \n<p>This monograph presents a selection of recent advances that bridge a \nlong-standing gap in our understanding of these heuristics. The monograph will \nlead the reader through several widely used non-convex optimization techniques, \nas well as applications thereof. The goal of this monograph is to both, \nintroduce the rich literature in this area, as well as equip the reader with \nthe tools and techniques needed to analyze these simple procedures for \nnon-convex problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704b9f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07897"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Philipp Hacker, Emil Wiedemann", "title": "A continuous framework for fairness. (arXiv:1712.07924v1 [cs.CY])", "alternate": [{"href": "http://arxiv.org/abs/1712.07924", "type": "text/html"}], "timestampUsec": "1513924821253972", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450507a8039\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450507a8039&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Increasingly, discrimination by algorithms is perceived as a societal and \nlegal problem. As a response, a number of criteria for implementing algorithmic \nfairness in machine learning have been developed in the literature. This paper \nproposes the Continuous Fairness Algorithm (CFA$\\theta$) which enables a \ncontinuous interpolation between different fairness definitions. More \nspecifically, we make three main contributions to the existing literature. \nFirst, our approach allows the decision maker to continuously vary between \nconcepts of individual and group fairness. As a consequence, the algorithm \nenables the decision maker to adopt intermediate \"worldviews\" on the degree of \ndiscrimination encoded in algorithmic processes, adding nuance to the extreme \ncases of \"we're all equal\" (WAE) and \"what you see is what you get\" (WYSIWYG) \nproposed so far in the literature. Second, we use optimal transport theory, and \nspecifically the concept of the barycenter, to maximize decision maker utility \nunder the chosen fairness constraints. Third, the algorithm is able to handle \ncases of intersectionality, i.e., of multi-dimensional discrimination of \ncertain groups on grounds of several criteria. We discuss three main examples \n(college admissions; credit application; insurance contracts) and map out the \npolicy implications of our approach. The explicit formalization of the \ntrade-off between individual and group fairness allows this post-processing \napproach to be tailored to different situational contexts in which one or the \nother fairness criterion may take precedence. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704ba4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07924"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ravi Tejwani, Adam Liska, Hongyuan You, Jenna Reinen, Payel Das", "title": "Autism Classification Using Brain Functional Connectivity Dynamics and Machine Learning. (arXiv:1712.08041v1 [q-bio.NC])", "alternate": [{"href": "http://arxiv.org/abs/1712.08041", "type": "text/html"}], "timestampUsec": "1513924821253971", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450507f2d8b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450507f2d8b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The goal of the present study is to identify autism using machine learning \ntechniques and resting-state brain imaging data, leveraging the temporal \nvariability of the functional connections (FC) as the only information. We \nestimated and compared the FC variability across brain regions between typical, \nhealthy subjects and autistic population by analyzing brain imaging data from a \nworld-wide multi-site database known as ABIDE (Autism Brain Imaging Data \nExchange). Our analysis revealed that patients diagnosed with autism spectrum \ndisorder (ASD) show increased FC variability in several brain regions that are \nassociated with low FC variability in the typical brain. We then used the \nenhanced FC variability of brain regions as features for training machine \nlearning models for ASD classification and achieved 65% accuracy in \nidentification of ASD versus control subjects within the dataset. We also used \nnode strength estimated from number of functional connections per node averaged \nover the whole scan as features for ASD classification.The results reveal that \nthe dynamic FC measures outperform or are comparable with the static FC \nmeasures in predicting ASD. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704ba9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08041"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Topi Paananen, Juho Piironen, Michael Riis Andersen, Aki Vehtari", "title": "Model selection for Gaussian processes utilizing sensitivity of posterior predictive distribution. (arXiv:1712.08048v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.08048", "type": "text/html"}], "timestampUsec": "1513924821253970", "comments": [], "summary": {"content": "<p>We propose two novel methods for simplifying Gaussian process (GP) models by \nexamining the predictions of a full model in the vicinity of the training \npoints and thereby ordering the covariates based on their predictive relevance. \nOur results on synthetic and real world data sets demonstrate improved variable \nselection compared to automatic relevance determination (ARD) in terms of \nconsistency and predictive performance. We expect our proposed methods to be \nuseful in interpreting and understanding complex Gaussian process models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704bb2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08048"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Christian Dansereau, Angela Tam, AmanPreet Badhwar, Sebastian Urchs, Pierre Orban, Pedro Rosa-Neto, Pierre Bellec", "title": "A brain signature highly predictive of future progression to Alzheimer's dementia. (arXiv:1712.08058v1 [q-bio.QM])", "alternate": [{"href": "http://arxiv.org/abs/1712.08058", "type": "text/html"}], "timestampUsec": "1513924821253969", "comments": [], "summary": {"content": "<p>Early prognosis of Alzheimer's dementia is hard. Mild cognitive impairment \n(MCI) typically precedes Alzheimer's dementia, yet only a fraction (30%-50%) of \nMCI individuals will progress to dementia. Even when a prognosis of dementia is \nestablished using machine learning models and biomarkers, the fraction of MCI \nprogressors remain limited (50%-75%). Instead of aiming at precise diagnosis in \nlarge clinical cohorts known for their heterogeneity, we propose here to \nidentify only a subset of individuals who share a common brain signature highly \npredictive of oncoming dementia. This signature was discovered using a machine \nlearning model in a reference public sample (ADNI), where the model was trained \nto identify patterns of brain atrophy and functional dysconnectivity commonly \nseen in patients suffering from dementia (N = 24), and not seen in cognitively \nnormal individuals (N = 49). The model then recognized the same brain signature \nin 10 MCI individuals, out of N = 56, 90% of which progressed to dementia \nwithin three years. This result is a marked improvement on the state-of-the-art \nin prognostic precision, while the brain signature still identified 47% of all \nMCI progressors (N = 19). We thus discovered a sizable MCI subpopulation with \nhomogeneous brain abnormalities and highly predictable clinical trajectories, \nwhich may represent an excellent recruitment target for clinical trials at the \nprodromal stage of Alzheimer's disease. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704bbd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08058"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sebastiaan H&#xf6;ppner, Eugen Stripling, Bart Baesens, Seppe vanden Broucke, Tim Verdonck", "title": "Profit Driven Decision Trees for Churn Prediction. (arXiv:1712.08101v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08101", "type": "text/html"}], "timestampUsec": "1513924821253968", "comments": [], "summary": {"content": "<p>Customer retention campaigns increasingly rely on predictive models to detect \npotential churners in a vast customer base. From the perspective of machine \nlearning, the task of predicting customer churn can be presented as a binary \nclassification problem. Using data on historic behavior, classification \nalgorithms are built with the purpose of accurately predicting the probability \nof a customer defecting. The predictive churn models are then commonly selected \nbased on accuracy related performance measures such as the area under the ROC \ncurve (AUC). However, these models are often not well aligned with the core \nbusiness requirement of profit maximization, in the sense that, the models fail \nto take into account not only misclassification costs, but also the benefits \noriginating from a correct classification. Therefore, the aim is to construct \nchurn prediction models that are profitable and preferably interpretable too. \nThe recently developed expected maximum profit measure for customer churn \n(EMPC) has been proposed in order to select the most profitable churn model. We \npresent a new classifier that integrates the EMPC metric directly into the \nmodel construction. Our technique, called ProfTree, uses an evolutionary \nalgorithm for learning profit driven decision trees. In a benchmark study with \nreal-life data sets from various telecommunication service providers, we show \nthat ProfTree achieves significant profit improvements compared to classic \naccuracy driven tree-based methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704bc4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08101"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Paul Hand, Babhru Joshi", "title": "A Convex Program for Mixed Linear Regression with a Recovery Guarantee for Well-Separated Data. (arXiv:1612.06067v2 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1612.06067", "type": "text/html"}], "timestampUsec": "1513924821253967", "comments": [], "summary": {"content": "<p>We introduce a convex approach for mixed linear regression over $d$ features. \nThis approach is a second-order cone program, based on L1 minimization, which \nassigns an estimate regression coefficient in $\\mathbb{R}^{d}$ for each data \npoint. These estimates can then be clustered using, for example, $k$-means. For \nproblems with two or more mixture classes, we prove that the convex program \nexactly recovers all of the mixture components in the noiseless setting under \ntechnical conditions that include a well-separation assumption on the data. \nUnder these assumptions, recovery is possible if each class has at least $d$ \nindependent measurements. We also explore an iteratively reweighted least \nsquares implementation of this method on real and synthetic data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704bca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.06067"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Johanna &#xc4;rje, Ville Tirronen, Salme K&#xe4;rkk&#xe4;inen, Kristian Meissner, Jenni Raitoharju, Alexandros Iosifidis, Moncef Gabbouj, Serkan Kiranyaz", "title": "Human experts vs. machines in taxa recognition. (arXiv:1708.06899v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.06899", "type": "text/html"}], "timestampUsec": "1513924821253966", "comments": [], "summary": {"content": "<p>Biomonitoring of waterbodies is vital as the number of anthropogenic \nstressors on aquatic ecosystems keeps growing. However, the continuous decrease \nin funding makes it impossible to meet monitoring goals or sustain traditional \nmanual sample processing. In this paper, we review what kind of statistical \ntools can be used to enhance the cost efficiency of biomonitoring: We explore \nautomated identification of freshwater macroinvertebrates which are used as one \nindicator group in biomonitoring of aquatic ecosystems. We present the first \nclassification results of a new imaging system producing multiple images per \nspecimen. Moreover, these results are compared with the results of human \nexperts. On a data set of 29 taxonomical groups, automated classification \nproduces a higher average accuracy than human experts. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704bdc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.06899"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Christian Donner, Manfred Opper", "title": "Inverse Ising problem in continuous time: A latent variable approach. (arXiv:1709.04495v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04495", "type": "text/html"}], "timestampUsec": "1513924821253965", "comments": [], "summary": {"content": "<p>We consider the inverse Ising problem, i.e. the inference of network \ncouplings from observed spin trajectories for a model with continuous time \nGlauber dynamics. By introducing two sets of auxiliary latent random variables \nwe render the likelihood into a form, which allows for simple iterative \ninference algorithms with analytical updates. The variables are: (1) Poisson \nvariables to linearise an exponential term which is typical for point process \nlikelihoods and (2) P\\'olya-Gamma variables, which make the likelihood \nquadratic in the coupling parameters. Using the augmented likelihood, we derive \nan expectation-maximization (EM) algorithm to obtain the maximum likelihood \nestimate of network parameters. Using a third set of latent variables we extend \nthe EM algorithm to sparse couplings via L1 regularization. Finally, we develop \nan efficient approximate Bayesian inference algorithm using a variational \napproach. We demonstrate the performance of our algorithms on data simulated \nfrom an Ising model. For data which are simulated from a more biologically \nplausible network with spiking neurons, we show that the Ising model captures \nwell the low order statistics of the data and how the Ising couplings are \nrelated to the underlying synaptic structure of the simulated network. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704be2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04495"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Lester Mackey, Vasilis Syrgkanis, Ilias Zadik", "title": "Orthogonal Machine Learning: Power and Limitations. (arXiv:1711.00342v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.00342", "type": "text/html"}], "timestampUsec": "1513924821253964", "comments": [], "summary": {"content": "<p>Double machine learning provides $\\sqrt{n}$-consistent estimates of \nparameters of interest even when high-dimensional or nonparametric nuisance \nparameters are estimated at an $n^{-1/4}$ rate. The key is to employ \nNeyman-orthogonal moment equations which are first-order insensitive to \nperturbations in the nuisance parameters. We show that the $n^{-1/4}$ \nrequirement can be improved to $n^{-1/(2k+2)}$ by employing a $k$-th order \nnotion of orthogonality that grants robustness to more complex or \nhigher-dimensional nuisance parameters. In the partially linear regression \nsetting popular in causal inference, we show that we can construct second-order \northogonal moments if and only if the treatment residual is not normally \ndistributed. Our proof relies on Stein's lemma and may be of independent \ninterest. We conclude by demonstrating the robustness benefits of an explicit \ndoubly-orthogonal estimation procedure for treatment effect. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513924821254", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c704bea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.00342"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Davide Venturelli, Minh Do, Eleanor Rieffel, Jeremy Frank", "title": "Compiling quantum circuits to realistic hardware architectures using temporal planners. (arXiv:1705.08927v2 [quant-ph] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08927", "type": "text/html"}], "timestampUsec": "1513920223111160", "comments": [], "summary": {"content": "<p>To run quantum algorithms on emerging gate-model quantum hardware, quantum \ncircuits must be compiled to take into account constraints on the hardware. For \nnear-term hardware, with only limited means to mitigate decoherence, it is \ncritical to minimize the duration of the circuit. We investigate the \napplication of temporal planners to the problem of compiling quantum circuits \nto newly emerging quantum hardware. While our approach is general, we focus on \ncompiling to superconducting hardware architectures with nearest neighbor \nconstraints. Our initial experiments focus on compiling Quantum Alternating \nOperator Ansatz (QAOA) circuits whose high number of commuting gates allow \ngreat flexibility in the order in which the gates can be applied. That freedom \nmakes it more challenging to find optimal compilations but also means there is \na greater potential win from more optimized compilation than for less flexible \ncircuits. We map this quantum circuit compilation problem to a temporal \nplanning problem, and generated a test suite of compilation problems for QAOA \ncircuits of various sizes to a realistic hardware architecture. We report \ncompilation results from several state-of-the-art temporal planners on this \ntest set. This early empirical evaluation demonstrates that temporal planning \nis a viable approach to quantum circuit compilation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513920223111", "annotations": [], "published": 1513920224, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c67bd79", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08927"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tien Huu Do, Duc Minh Nguyen, Evaggelia Tsiligianni, Bruno Cornelis, Nikos Deligiannis", "title": "Multiview Deep Learning for Predicting Twitter Users' Location. (arXiv:1712.08091v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.08091", "type": "text/html"}], "timestampUsec": "1513920223111158", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450507f3167\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450507f3167&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The problem of predicting the location of users on large social networks like \nTwitter has emerged from real-life applications such as social unrest detection \nand online marketing. Twitter user geolocation is a difficult and active \nresearch topic with a vast literature. Most of the proposed methods follow \neither a content-based or a network-based approach. The former exploits \nuser-generated content while the latter utilizes the connection or interaction \nbetween Twitter users. In this paper, we introduce a novel method combining the \nstrength of both approaches. Concretely, we propose a multi-entry neural \nnetwork architecture named MENET leveraging the advances in deep learning and \nmultiview learning. The generalizability of MENET enables the integration of \nmultiple data representations. In the context of Twitter user geolocation, we \nrealize MENET with textual, network, and metadata features. Considering the \nnatural distribution of Twitter users across the concerned geographical area, \nwe subdivide the surface of the earth into multi-scale cells and train MENET \nwith the labels of the cells. We show that our method outperforms the state of \nthe art by a large margin on three benchmark datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513920223111", "annotations": [], "published": 1513920224, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c67bd9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08091"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "J&#xf6;rg L&#xfc;cke, Zhenwen Dai, Georgios Exarchakis", "title": "Truncated Variational Sampling for \"Black Box\" Optimization of Generative Models. (arXiv:1712.08104v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.08104", "type": "text/html"}], "timestampUsec": "1513920223111157", "comments": [], "summary": {"content": "<p>We investigate the optimization of two generative models with binary hidden \nvariables using a novel variational EM approach. The novel approach \ndistinguishes itself from previous variational approaches by using hidden \nstates as variational parameters. Here we use efficient and general purpose \nsampling procedures to vary the hidden states, and investigate the \"black box\" \napplicability of the resulting optimization procedure. For general purpose \napplicability, samples are drawn from approximate marginal distributions of the \nconsidered generative model and from the prior distribution of a given \ngenerative model. As such, sampling is defined in a generic form with no \nadditional derivations required. As a proof of concept, we then apply the novel \nprocedure (A) to Binary Sparse Coding (a model with continuous observables), \nand (B) to basic Sigmoid Belief Networks (which are models with binary \nobservables). The approach is applicable without any further analytical steps \nand efficiently as well as effectively increases the variational free-energy \nobjective. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513920223111", "annotations": [], "published": 1513920224, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c67bda8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.08104"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David Venuto, Toby Dylan Hocking, Lakjaree Sphanurattana, Masashi Sugiyama", "title": "Support vector comparison machines. (arXiv:1401.8008v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1401.8008", "type": "text/html"}], "timestampUsec": "1513920223111156", "comments": [], "summary": {"content": "<p>In ranking problems, the goal is to learn a ranking function from labeled \npairs of input points. In this paper, we consider the related comparison \nproblem, where the label indicates which element of the pair is better, or if \nthere is no significant difference. We cast the learning problem as a margin \nmaximization, and show that it can be solved by converting it to a standard \nSVM. We use simulated nonlinear patterns, a real learning to rank sushi data \nset, and a chess data set to show that our proposed SVMcompare algorithm \noutperforms SVMrank when there are equality pairs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513920223111", "annotations": [], "published": 1513920224, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c67bdaf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1401.8008"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jamie Hayes, George Danezis", "title": "Learning Universal Adversarial Perturbations with Generative Models. (arXiv:1708.05207v2 [cs.CR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.05207", "type": "text/html"}], "timestampUsec": "1513920223111155", "comments": [], "summary": {"content": "<p>Neural networks are known to be vulnerable to adversarial examples, inputs \nthat have been intentionally perturbed to remain visually similar to the source \ninput, but cause a misclassification. It was recently shown that given a \ndataset and classifier, there exists so called universal adversarial \nperturbations, a single perturbation that causes a misclassification when \napplied to any input. In this work, we introduce universal adversarial \nnetworks, a generative network that is capable of fooling a target classifier \nwhen it's generated output is added to a clean sample from a dataset. We show \nthat this technique improves on known universal adversarial attacks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513920223111", "annotations": [], "published": 1513920224, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034c67bdb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.05207"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chelsea Finn, Sergey Levine", "title": "Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm. (arXiv:1710.11622v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.11622", "type": "text/html"}], "timestampUsec": "1513833379178756", "comments": [], "summary": {"content": "<p>Learning to learn is a powerful paradigm for enabling models to learn from \ndata more effectively and efficiently. A popular approach to meta-learning is \nto train a recurrent model to read in a training dataset as input and output \nthe parameters of a learned model, or output predictions for new test inputs. \nAlternatively, a more recent approach to meta-learning aims to acquire deep \nrepresentations that can be effectively fine-tuned, via standard gradient \ndescent, to new tasks. In this paper, we consider the meta-learning problem \nfrom the perspective of universality, formalizing the notion of learning \nalgorithm approximation and comparing the expressive power of the \naforementioned recurrent models to the more recent approaches that embed \ngradient descent into the meta-learner. In particular, we seek to answer the \nfollowing question: does deep representation combined with standard gradient \ndescent have sufficient capacity to approximate any learning algorithm? We find \nthat this is indeed true, and further find, in our experiments, that \ngradient-based meta-learning consistently leads to learning strategies that \ngeneralize more widely compared to those represented by recurrent models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833379, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa2815", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.11622"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Huishuai Zhang, Caiming Xiong, James Bradbury, Richard Socher", "title": "Block-diagonal Hessian-free Optimization for Training Neural Networks. (arXiv:1712.07296v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.07296", "type": "text/html"}], "timestampUsec": "1513833379178755", "comments": [], "summary": {"content": "<p>Second-order methods for neural network optimization have several advantages \nover methods based on first-order gradient descent, including better scaling to \nlarge mini-batch sizes and fewer updates needed for convergence. But they are \nrarely applied to deep learning in practice because of high computational cost \nand the need for model-dependent algorithmic variations. We introduce a variant \nof the Hessian-free method that leverages a block-diagonal approximation of the \ngeneralized Gauss-Newton matrix. Our method computes the curvature \napproximation matrix only for pairs of parameters from the same layer or block \nof the neural network and performs conjugate gradient updates independently for \neach block. Experiments on deep autoencoders, deep convolutional networks, and \nmultilayer LSTMs demonstrate better convergence and generalization compared to \nthe original Hessian-free approach and the Adam method. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833379, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa282e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07296"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiangyu Kong, Bo Xin, Fangchen Liu, Yizhou Wang", "title": "Revisiting the Master-Slave Architecture in Multi-Agent Deep Reinforcement Learning. (arXiv:1712.07305v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07305", "type": "text/html"}], "timestampUsec": "1513833379178754", "comments": [], "summary": {"content": "<p>Many tasks in artificial intelligence require the collaboration of multiple \nagents. We exam deep reinforcement learning for multi-agent domains. Recent \nresearch efforts often take the form of two seemingly conflicting perspectives, \nthe decentralized perspective, where each agent is supposed to have its own \ncontroller; and the centralized perspective, where one assumes there is a \nlarger model controlling all agents. In this regard, we revisit the idea of the \nmaster-slave architecture by incorporating both perspectives within one \nframework. Such a hierarchical structure naturally leverages advantages from \none another. The idea of combining both perspectives is intuitive and can be \nwell motivated from many real world systems, however, out of a variety of \npossible realizations, we highlights three key ingredients, i.e. composed \naction representation, learnable communication and independent reasoning. With \nnetwork designs to facilitate these explicitly, our proposal consistently \noutperforms latest competing methods both in synthetic experiments and when \napplied to challenging StarCraft micromanagement tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833379, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa2844", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07305"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Martin Schrimpf, Stephen Merity, James Bradbury, Richard Socher", "title": "A Flexible Approach to Automated RNN Architecture Generation. (arXiv:1712.07316v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.07316", "type": "text/html"}], "timestampUsec": "1513833379178753", "comments": [], "summary": {"content": "<p>The process of designing neural architectures requires expert knowledge and \nextensive trial and error. While automated architecture search may simplify \nthese requirements, the recurrent neural network (RNN) architectures generated \nby existing methods are limited in both flexibility and components. We propose \na domain-specific language (DSL) for use in automated architecture search which \ncan produce novel RNNs of arbitrary depth and width. The DSL is flexible enough \nto define standard architectures such as the Gated Recurrent Unit and Long \nShort Term Memory and allows the introduction of non-standard RNN components \nsuch as trigonometric curves and layer normalization. Using two different \ncandidate generation techniques, random search with a ranking function and \nreinforcement learning, we explore the novel architectures produced by the RNN \nDSL for language modeling and machine translation domains. The resulting \narchitectures do not follow human intuition yet perform well on their targeted \ntasks, suggesting the space of usable RNN architectures is far larger than \npreviously assumed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833379, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa284e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07316"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiaowei Jia, Ankush Khandelwal, Anuj Karpatne, Vipin Kumar", "title": "Discovery of Shifting Patterns in Sequence Classification. (arXiv:1712.07203v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.07203", "type": "text/html"}], "timestampUsec": "1513833379178752", "comments": [], "summary": {"content": "<p>In this paper, we investigate the multi-variate sequence classification \nproblem from a multi-instance learning perspective. Real-world sequential data \ncommonly show discriminative patterns only at specific time periods. For \ninstance, we can identify a cropland during its growing season, but it looks \nsimilar to a barren land after harvest or before planting. Besides, even within \nthe same class, the discriminative patterns can appear in different periods of \nsequential data. Due to such property, these discriminative patterns are also \nreferred to as shifting patterns. The shifting patterns in sequential data \nseverely degrade the performance of traditional classification methods without \nsufficient training data. \n</p> \n<p>We propose a novel sequence classification method by automatically mining \nshifting patterns from multi-variate sequence. The method employs a \nmulti-instance learning approach to detect shifting patterns while also \nmodeling temporal relationships within each multi-instance bag by an LSTM model \nto further improve the classification performance. We extensively evaluate our \nmethod on two real-world applications - cropland mapping and affective state \nrecognition. The experiments demonstrate the superiority of our proposed method \nin sequence classification performance and in detecting discriminative shifting \npatterns. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833379, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa2866", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07203"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "John E. Herr, Kun Yao, Ryker McIntyre, David Toth, John Parkhill", "title": "Metadynamics for Training Neural Network Model Chemistries: a Competitive Assessment. (arXiv:1712.07240v1 [physics.chem-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.07240", "type": "text/html"}], "timestampUsec": "1513833379178751", "comments": [], "summary": {"content": "<p>Neural network (NN) model chemistries (MCs) promise to facilitate the \naccurate exploration of chemical space and simulation of large reactive \nsystems. One important path to improving these models is to add layers of \nphysical detail, especially long-range forces. At short range, however, these \nmodels are data driven and data limited. Little is systematically known about \nhow data should be sampled, and `test data' chosen randomly from some sampling \ntechniques can provide poor information about generality. If the sampling \nmethod is narrow `test error' can appear encouragingly tiny while the model \nfails catastrophically elsewhere. In this manuscript we competitively evaluate \ntwo common sampling methods: molecular dynamics (MD), normal-mode sampling \n(NMS) and one uncommon alternative, Metadynamics (MetaMD), for preparing \ntraining geometries. We show that MD is an inefficient sampling method in the \nsense that additional samples do not improve generality. We also show MetaMD is \neasily implemented in any NNMC software package with cost that scales linearly \nwith the number of atoms in a sample molecule. MetaMD is a black-box way to \nensure samples always reach out to new regions of chemical space, while \nremaining relevant to chemistry near $k_bT$. It is one cheap tool to address \nthe issue of generalization. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833379, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa2879", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07240"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Martin Wistuba", "title": "Finding Competitive Network Architectures Within a Day Using UCT. (arXiv:1712.07420v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.07420", "type": "text/html"}], "timestampUsec": "1513833379178750", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450507f3544\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450507f3544&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The design of neural network architectures for a new data set is a laborious \ntask which requires human deep learning expertise. In order to make deep \nlearning available for a broader audience, automated methods for finding a \nneural network architecture are vital. Recently proposed methods can already \nachieve human expert level performances. However, these methods have run times \nof months or even years of GPU computing time, ignoring hardware constraints as \nfaced by many researchers and companies. We propose the use of Monte Carlo \nplanning in combination with two different UCT (upper confidence bound applied \nto trees) derivations to search for network architectures. We adapt the UCT \nalgorithm to the needs of network architecture search by proposing two ways of \nsharing information between different branches of the search tree. In an \nempirical study we are able to demonstrate that this method is able to find \ncompetitive networks for MNIST, SVHN and CIFAR-10 in just a single GPU day. \nExtending the search time to five GPU days, we are able to outperform human \narchitectures and our competitors which consider the same types of layers. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833379, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa288b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07420"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Vishwak Srinivasan, Adepu Ravi Sankar, Vineeth N Balasubramanian", "title": "ADINE: An Adaptive Momentum Method for Stochastic Gradient Descent. (arXiv:1712.07424v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07424", "type": "text/html"}], "timestampUsec": "1513833379178749", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505084f460\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505084f460&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Two major momentum-based techniques that have achieved tremendous success in \noptimization are Polyak's heavy ball method and Nesterov's accelerated \ngradient. A crucial step in all momentum-based methods is the choice of the \nmomentum parameter $m$ which is always suggested to be set to less than $1$. \nAlthough the choice of $m &lt; 1$ is justified only under very strong theoretical \nassumptions, it works well in practice even when the assumptions do not \nnecessarily hold. In this paper, we propose a new momentum based method \n$\\textit{ADINE}$, which relaxes the constraint of $m &lt; 1$ and allows the \nlearning algorithm to use adaptive higher momentum. We motivate our hypothesis \non $m$ by experimentally verifying that a higher momentum ($\\ge 1$) can help \nescape saddles much faster. Using this motivation, we propose our method \n$\\textit{ADINE}$ that helps weigh the previous updates more (by setting the \nmomentum parameter $&gt; 1$), evaluate our proposed algorithm on deep neural \nnetworks and show that $\\textit{ADINE}$ helps the learning algorithm to \nconverge much faster without compromising on the generalization error. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833379, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa28ba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07424"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Markus Wulfmeier, Alex Bewley, Ingmar Posner", "title": "Incremental Adversarial Domain Adaptation. (arXiv:1712.07436v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07436", "type": "text/html"}], "timestampUsec": "1513833379178748", "comments": [], "summary": {"content": "<p>Continuous appearance shifts such as changes in weather and lighting \nconditions can impact the performance of deployed machine learning models. \nUnsupervised domain adaptation aims to address this challenge, though current \napproaches do not utilise the continuity of the occurring shifts. Many robotic \napplications exhibit these conditions and thus facilitate the potential to \nincrementally adapt a learnt model over minor shifts which integrate to massive \ndifferences over time. Our work presents an adversarial approach for lifelong, \nincremental domain adaptation which benefits from unsupervised alignment to a \nseries of sub-domains which successively diverge from the labelled source \ndomain. We demonstrate on a drivable-path segmentation task that our \nincremental approach can better handle large appearance changes, e.g. day to \nnight, compared with a prior single alignment step approach. Furthermore, by \napproximating the marginal feature distribution for the source domain with a \ngenerative adversarial network, the deployment module can be rendered fully \nindependent of retaining potentially large amounts of the related source \ntraining data for only a minor reduction in performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833380, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa28d9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07436"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Robin C. Geyer, Tassilo Klein, Moin Nabi", "title": "Differentially Private Federated Learning: A Client Level Perspective. (arXiv:1712.07557v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1712.07557", "type": "text/html"}], "timestampUsec": "1513833379178747", "comments": [], "summary": {"content": "<p>Federated learning is a recent advance in privacy protection. In this \ncontext, a trusted curator aggregates parameters optimized in decentralized \nfashion by multiple clients. The resulting model is then distributed back to \nall clients, ultimately converging to a joint representative model without \nexplicitly having to share the data. However, the protocol is vulnerable to \ndifferential attacks, which could originate from any party contributing during \nfederated optimization. In such an attack, a client's contribution during \ntraining and information about their data set is revealed through analyzing the \ndistributed model. We tackle this problem and propose an algorithm for client \nsided differential privacy preserving federated optimization. The aim is to \nhide clients' contributions during training, balancing the trade-off between \nprivacy loss and model performance. Empirical studies suggest that given a \nsufficiently large number of participating clients, our proposed procedure can \nmaintain client-level differential privacy at only a minor cost in model \nperformance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833380, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa28eb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07557"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kristof T. Sch&#xfc;tt, Pieter-Jan Kindermans, Huziel E. Sauceda, Stefan Chmiela, Alexandre Tkatchenko, Klaus-Robert M&#xfc;ller", "title": "SchNet: A continuous-filter convolutional neural network for modeling quantum interactions. (arXiv:1706.08566v5 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.08566", "type": "text/html"}], "timestampUsec": "1513833379178746", "comments": [], "summary": {"content": "<p>Deep learning has the potential to revolutionize quantum chemistry as it is \nideally suited to learn representations for structured data and speed up the \nexploration of chemical space. While convolutional neural networks have proven \nto be the first choice for images, audio and video data, the atoms in molecules \nare not restricted to a grid. Instead, their precise locations contain \nessential physical information, that would get lost if discretized. Thus, we \npropose to use continuous-filter convolutional layers to be able to model local \ncorrelations without requiring the data to lie on a grid. We apply those layers \nin SchNet: a novel deep learning architecture modeling quantum interactions in \nmolecules. We obtain a joint model for the total energy and interatomic forces \nthat follows fundamental quantum-chemical principles. This includes \nrotationally invariant energy predictions and a smooth, differentiable \npotential energy surface. Our architecture achieves state-of-the-art \nperformance for benchmarks of equilibrium molecules and molecular dynamics \ntrajectories. Finally, we introduce a more challenging benchmark with chemical \nand structural variations that suggests the path for further work. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833380, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa28f8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.08566"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Joeseph P. Smith, Andrew D. Gronewold", "title": "Development and analysis of a Bayesian water balance model for large lake systems. (arXiv:1710.10161v2 [stat.AP] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.10161", "type": "text/html"}], "timestampUsec": "1513833379178745", "comments": [], "summary": {"content": "<p>Water balance models are often employed to improve understanding of drivers \nof change in regional hydrologic cycles. Most of these models, however, are \nphysically-based, and few employ state-of-the-art statistical methods to \nreconcile measurement uncertainty and bias. Here, we introduce a framework for \ndeveloping, analyzing, and selecting among alternative formulations of a \nstatistical water balance model for large lake systems that addresses this \nresearch gap. We demonstrate our new analytical framework using a model \ncustomized for Lakes Superior and Michigan-Huron, the two largest lakes on \nEarth by surface area. The selected model (from among 26 alternatives) closed \nthe water balance across both lakes with an order of magnitude less computation \ntime than prototype versions of the same model. We expect our new framework \nwill be used to improve computational efficiency and skill of water balance \nmodels for other lakes around the world. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513833379179", "annotations": [], "published": 1513833380, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034baa290d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.10161"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sidney Marlon Lopes de Lima, Abel Guilhermino da Silva Filho, Wellington Pinheiro dos Santos", "title": "Detection and classification of masses in mammographic images in a multi-kernel approach. (arXiv:1712.07116v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.07116", "type": "text/html"}], "timestampUsec": "1513832809776845", "comments": [], "summary": {"content": "<p>According to the World Health Organization, breast cancer is the main cause \nof cancer death among adult women in the world. Although breast cancer occurs \nindiscriminately in countries with several degrees of social and economic \ndevelopment, among developing and underdevelopment countries mortality rates \nare still high, due to low availability of early detection technologies. From \nthe clinical point of view, mammography is still the most effective diagnostic \ntechnology, given the wide diffusion of the use and interpretation of these \nimages. Herein this work we propose a method to detect and classify \nmammographic lesions using the regions of interest of images. Our proposal \nconsists in decomposing each image using multi-resolution wavelets. Zernike \nmoments are extracted from each wavelet component. Using this approach we can \ncombine both texture and shape features, which can be applied both to the \ndetection and classification of mammary lesions. We used 355 images of fatty \nbreast tissue of IRMA database, with 233 normal instances (no lesion), 72 \nbenign, and 83 malignant cases. Classification was performed by using SVM and \nELM networks with modified kernels, in order to optimize accuracy rates, \nreaching 94.11%. Considering both accuracy rates and training times, we defined \nthe ration between average percentage accuracy and average training time in a \nreverse order. Our proposal was 50 times higher than the ratio obtained using \nthe best method of the state-of-the-art. As our proposed model can combine high \naccuracy rate with low learning time, whenever a new data is received, our work \nwill be able to save a lot of time, hours, in learning process in relation to \nthe best method of the state-of-the-art. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91ee7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07116"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tyler A. Spears, Brandon G. Jacques, Marc W. Howard, Per B. Sederberg", "title": "Scale-invariant temporal history (SITH): optimal slicing of the past in an uncertain world. (arXiv:1712.07165v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07165", "type": "text/html"}], "timestampUsec": "1513832809776844", "comments": [], "summary": {"content": "<p>In both the human brain and any general artificial intelligence (AI), a \nrepresentation of the past is necessary to predict the future. However, perfect \nstorage of all experiences is not possible. One possibility, utilized in many \napplications, is to retain information about the past in a buffer. A limitation \nof this approach is that although events in the buffer are represented with \nperfect accuracy, the resources necessary to represent information at a \nparticular time scale go up rapidly. Here we present a neurally-plausible, \ncompressed, scale-free memory representation we call Scale-Invariant Temporal \nHistory (SITH). This representation covers an exponentially large period of \ntime in the past at the cost of sacrificing temporal accuracy for events \nfurther in the past. The form of this decay is scale-invariant and can be shown \nto be optimal in that it is able to respond to worlds with a wide range of time \nscales. We demonstrate the utility of this representation in learning to play a \nsimple video game. In this environment, SITH exhibits better learning \nperformance than a fixed-size buffer history representation. Whereas the buffer \nperforms well as long as the temporal dependencies can be represented within \nthe buffer, SITH performs well over a much larger range of time scales for the \nsame amount of resources. Finally, we discuss how the application of SITH, \nalong with other human-inspired models of cognition, could improve \nreinforcement and machine learning algorithms in general. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91ee8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07165"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Rajesh Bordawekar, Bortik Bandyopadhyay, Oded Shmueli", "title": "Cognitive Database: A Step towards Endowing Relational Databases with Artificial Intelligence Capabilities. (arXiv:1712.07199v1 [cs.DB])", "alternate": [{"href": "http://arxiv.org/abs/1712.07199", "type": "text/html"}], "timestampUsec": "1513832809776843", "comments": [], "summary": {"content": "<p>We propose Cognitive Databases, an approach for transparently enabling \nArtificial Intelligence (AI) capabilities in relational databases. A novel \naspect of our design is to first view the structured data source as meaningful \nunstructured text, and then use the text to build an unsupervised neural \nnetwork model using a Natural Language Processing (NLP) technique called word \nembedding. This model captures the hidden inter-/intra-column relationships \nbetween database tokens of different types. For each database token, the model \nincludes a vector that encodes contextual semantic relationships. We seamlessly \nintegrate the word embedding model into existing SQL query infrastructure and \nuse it to enable a new class of SQL-based analytics queries called cognitive \nintelligence (CI) queries. CI queries use the model vectors to enable complex \nqueries such as semantic matching, inductive reasoning queries such as \nanalogies, predictive queries using entities not present in a database, and, \nmore generally, using knowledge from external sources. We demonstrate unique \ncapabilities of Cognitive Databases using an Apache Spark based prototype to \nexecute inductive reasoning CI queries over a multi-modal database containing \ntext and images. We believe our first-of-a-kind system exemplifies using AI \nfunctionality to endow relational databases with capabilities that were \npreviously very hard to realize in practice. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91eef", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07199"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jianfu Zhang, Naiyan Wang, Liqing Zhang", "title": "Multi-shot Pedestrian Re-identification via Sequential Decision Making. (arXiv:1712.07257v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.07257", "type": "text/html"}], "timestampUsec": "1513832809776842", "comments": [], "summary": {"content": "<p>Multi-shot pedestrian re-identification problem is at the core of \nsurveillance video analysis. It matches two tracks of pedestrians from \ndifferent cameras. In contrary to existing works that aggregate single frames \nfeatures by time series model such as recurrent neural network, in this paper, \nwe propose an interpretable reinforcement learning based approach to this \nproblem. Particularly, we train an agent to verify a pair of images at each \ntime. The agent could choose to output the result (same or different) or \nrequest another pair of images to see (unsure). By this way, our model \nimplicitly learns the difficulty of image pairs, and postpone the decision when \nthe model does not accumulate enough evidence. Moreover, by adjusting the \nreward for unsure action, we can easily trade off between speed and accuracy. \nIn three open benchmarks, our method are competitive with the state-of-the-art \nmethods while only using 3% to 6% images. These promising results demonstrate \nthat our method is favorable in both efficiency and performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91ef4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07257"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Filipe Rolim Cordeiro, Wellington Pinheiro dos Santos, Abel Guilhermino da Silva Filho", "title": "Analysis of supervised and semi-supervised GrowCut applied to segmentation of masses in mammography images. (arXiv:1712.07312v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.07312", "type": "text/html"}], "timestampUsec": "1513832809776841", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505084f68f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505084f68f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Breast cancer is already one of the most common form of cancer worldwide. \nMammography image analysis is still the most effective diagnostic method to \npromote the early detection of breast cancer. Accurately segmenting tumors in \ndigital mammography images is important to improve diagnosis capabilities of \nhealth specialists and avoid misdiagnosis. In this work, we evaluate the \nfeasibility of applying GrowCut to segment regions of tumor and we propose two \nGrowCut semi-supervised versions. All the analysis was performed by evaluating \nthe application of segmentation techniques to a set of images obtained from the \nMini-MIAS mammography image database. GrowCut segmentation was compared to \nRegion Growing, Active Contours, Random Walks and Graph Cut techniques. \nExperiments showed that GrowCut, when compared to the other techniques, was \nable to acquire better results for the metrics analyzed. Moreover, the proposed \nsemi-supervised versions of GrowCut was proved to have a clinically \nsatisfactory quality of segmentation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07312"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michael Bukatin, Jon Anthony", "title": "Dataflow Matrix Machines and V-values: a Bridge between Programs and Neural Nets. (arXiv:1712.07447v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.07447", "type": "text/html"}], "timestampUsec": "1513832809776840", "comments": [], "summary": {"content": "<p>Dataflow matrix machines generalize neural nets by replacing streams of \nnumbers with streams of vectors (or other kinds of linear streams admitting a \nnotion of linear combination of several streams) and adding a few more changes \non top of that, namely arbitrary input and output arities for activation \nfunctions, countable-sized networks with finite dynamically changeable active \npart capable of unbounded growth, and a very expressive self-referential \nmechanism. \n</p> \n<p>While recurrent neural networks are Turing-complete, they form an esoteric \nprogramming platform, not conductive for practical general-purpose programming. \nDataflow matrix machines are more suitable as a general-purpose programming \nplatform, although it remains to be seen whether this platform can be made \nfully competitive with more traditional programming platforms currently in use. \nAt the same time, dataflow matrix machines retain the key property of recurrent \nneural networks: programs are expressed via matrices of real numbers, and \ncontinuous changes to those matrices produce arbitrarily small variations in \nthe programs associated with those matrices. \n</p> \n<p>Spaces of vector-like elements are of particular importance in this context. \nIn particular, we focus on the vector space $V$ of finite linear combinations \nof strings, which can be also understood as the vector space of finite prefix \ntrees with numerical leaves, the vector space of \"mixed rank tensors\", or the \nvector space of recurrent maps. \n</p> \n<p>This space, and a family of spaces of vector-like elements derived from it, \nare sufficiently expressive to cover all cases of interest we are currently \naware of, and allow a compact and streamlined version of dataflow matrix \nmachines based on a single space of vector-like elements and variadic neurons. \nWe call elements of these spaces V-values. Their role in our context is \nsomewhat similar to the role of S-expressions in Lisp. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f06", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07447"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Albert Gatt, Emiel Krahmer", "title": "Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation. (arXiv:1703.09902v3 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.09902", "type": "text/html"}], "timestampUsec": "1513832809776839", "comments": [], "summary": {"content": "<p>This paper surveys the current state of the art in Natural Language \nGeneration (NLG), defined as the task of generating text or speech from \nnon-linguistic input. A survey of NLG is timely in view of the changes that the \nfield has undergone over the past decade or so, especially in relation to new \n(usually data-driven) methods, as well as new applications of NLG technology. \nThis survey therefore aims to (a) give an up-to-date synthesis of research on \nthe core tasks in NLG and the architectures adopted in which such tasks are \norganised; (b) highlight a number of relatively recent research topics that \nhave arisen partly as a result of growing synergies between NLG and other areas \nof artificial intelligence; (c) draw attention to the challenges in NLG \nevaluation, relating them to similar challenges faced in other areas of Natural \nLanguage Processing, with an emphasis on different evaluation methods and the \nrelationships between them. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f12", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.09902"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tianmin Shu, Caiming Xiong, Richard Socher", "title": "Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning. (arXiv:1712.07294v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07294", "type": "text/html"}], "timestampUsec": "1513832809776838", "comments": [], "summary": {"content": "<p>Learning policies for complex tasks that require multiple different skills is \na major challenge in reinforcement learning (RL). It is also a requirement for \nits deployment in real-world scenarios. This paper proposes a novel framework \nfor efficient multi-task reinforcement learning. Our framework trains agents to \nemploy hierarchical policies that decide when to use a previously learned \npolicy and when to learn a new skill. This enables agents to continually \nacquire new skills during different stages of training. Each learned task \ncorresponds to a human language description. Because agents can only access \npreviously learned skills through these descriptions, the agent can always \nprovide a human-interpretable description of its choices. In order to help the \nagent learn the complex temporal dependencies necessary for the hierarchical \npolicy, we provide it with a stochastic temporal grammar that modulates when to \nrely on previously learned skills and when to execute new skills. We validate \nour approach on Minecraft games designed to explicitly test the ability to \nreuse previously learned skills while simultaneously learning new skills. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07294"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tobias Fromm", "title": "Self-Supervised Damage-Avoiding Manipulation Strategy Optimization via Mental Simulation. (arXiv:1712.07452v1 [cs.RO])", "alternate": [{"href": "http://arxiv.org/abs/1712.07452", "type": "text/html"}], "timestampUsec": "1513832809776837", "comments": [], "summary": {"content": "<p>Everyday robotics are challenged to deal with autonomous product handling in \napplications like logistics or retail, possibly causing damage on the items \nduring manipulation. Traditionally, most approaches try to minimize physical \ninteraction with goods. However, we propose to take into account any unintended \nmotion of objects in the scene and to learn manipulation strategies in a \nself-supervised way which minimize the potential damage. The presented approach \nconsists of a planning method that determines the optimal sequence to \nmanipulate a number of objects in a scene with respect to possible damage by \nsimulating interaction and hence anticipating scene dynamics. The planned \nmanipulation sequences are taken as input to a machine learning process which \ngeneralizes to new, unseen scenes in the same application scenario. This \nlearned manipulation strategy is continuously refined in a self-supervised \noptimization cycle dur- ing load-free times of the system. Such a \nsimulation-in-the-loop setup is commonly known as mental simulation and allows \nfor efficient, fully automatic generation of training data as opposed to \nclassical supervised learning approaches. In parallel, the generated \nmanipulation strategies can be deployed in near-real time in an anytime \nfashion. We evaluate our approach on one industrial scenario (autonomous \ncontainer unloading) and one retail scenario (autonomous shelf replenishment). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f1f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07452"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ayush Singhal, Pradeep Sinha, Rakesh Pant", "title": "Use of Deep Learning in Modern Recommendation System: A Summary of Recent Works. (arXiv:1712.07525v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07525", "type": "text/html"}], "timestampUsec": "1513832809776836", "comments": [], "summary": {"content": "<p>With the exponential increase in the amount of digital information over the \ninternet, online shops, online music, video and image libraries, search engines \nand recommendation system have become the most convenient ways to find relevant \ninformation within a short time. In the recent times, deep learning's advances \nhave gained significant attention in the field of speech recognition, image \nprocessing and natural language processing. Meanwhile, several recent studies \nhave shown the utility of deep learning in the area of recommendation systems \nand information retrieval as well. In this short review, we cover the recent \nadvances made in the field of recommendation using various variants of deep \nlearning technology. We organize the review in three parts: Collaborative \nsystem, Content based system and Hybrid system. The review also discusses the \ncontribution of deep learning integrated recommendation systems into several \napplication domains. The review concludes by discussion of the impact of deep \nlearning in recommendation system in various domain and whether deep learning \nhas shown any significant improvement over the conventional systems for \nrecommendation. Finally, we also provide future directions of research which \nare possible based on the current state of use of deep learning in \nrecommendation systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f22", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07525"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tom&#xe1;s Teijeiro, Paulo F&#xe9;lix", "title": "On the adoption of abductive reasoning for time series interpretation. (arXiv:1609.05632v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1609.05632", "type": "text/html"}], "timestampUsec": "1513832809776835", "comments": [], "summary": {"content": "<p>Time series interpretation aims to provide an explanation of what is observed \nin terms of its underlying processes. The present work is based on the \nassumption that common classification-based approaches to time series \ninterpretation suffer from a set of inherent weaknesses whose ultimate cause \nlies in the monotonic nature of the deductive reasoning paradigm. In this \ndocument we propose a new approach to this problem based on the initial \nhypothesis that abductive reasoning properly accounts for the human ability to \nidentify and characterize patterns appearing in a time series. The result of \nthe interpretation is a set of conjectures in the form of observations, \norganized into an abstraction hierarchy, and explaining what has been observed. \nA knowledge-based framework and a set of algorithms for the interpretation task \nare provided, implementing a hypothesize-and-test cycle guided by an \nattentional mechanism. As a representative application domain, the \ninterpretation of the electrocardiogram allows us to highlight the strengths of \nthe proposed approach in comparison with traditional classification-based \napproaches. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f27", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1609.05632"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Petar Veli&#x10d;kovi&#x107;, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li&#xf2;, Yoshua Bengio", "title": "Graph Attention Networks. (arXiv:1710.10903v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.10903", "type": "text/html"}], "timestampUsec": "1513832809776834", "comments": [], "summary": {"content": "<p>We present graph attention networks (GATs), novel neural network \narchitectures that operate on graph-structured data, leveraging masked \nself-attentional layers to address the shortcomings of prior methods based on \ngraph convolutions or their approximations. By stacking layers in which nodes \nare able to attend over their neighborhoods' features, we enable (implicitly) \nspecifying different weights to different nodes in a neighborhood, without \nrequiring any kind of costly matrix operation (such as inversion) or depending \non knowing the graph structure upfront. In this way, we address several key \nchallenges of spectral-based graph neural networks simultaneously, and make our \nmodel readily applicable to inductive as well as transductive problems. Our GAT \nmodels have achieved or matched state-of-the-art results across four \nestablished transductive and inductive graph benchmarks: the Cora, Citeseer and \nPubmed citation network datasets, as well as a protein-protein interaction \ndataset (wherein test graphs remain unseen during training). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f2b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.10903"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Dmitri S. Pavlichin, Jiantao Jiao, Tsachy Weissman", "title": "Approximate Profile Maximum Likelihood. (arXiv:1712.07177v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.07177", "type": "text/html"}], "timestampUsec": "1513832809776832", "comments": [], "summary": {"content": "<p>We propose an efficient algorithm for approximate computation of the profile \nmaximum likelihood (PML), a variant of maximum likelihood maximizing the \nprobability of observing a sufficient statistic rather than the empirical \nsample. The PML has appealing theoretical properties, but is difficult to \ncompute exactly. Inspired by observations gleaned from exactly solvable cases, \nwe look for an approximate PML solution, which, intuitively, clumps comparably \nfrequent symbols into one symbol. This amounts to lower-bounding a certain \nmatrix permanent by summing over a subgroup of the symmetric group rather than \nthe whole group during the computation. We extensively experiment with the \napproximate solution, and find the empirical performance of our approach is \ncompetitive and sometimes significantly better than state-of-the-art \nperformance for various estimation problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f2e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07177"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yehezkel S. Resheff, Moni Shahar", "title": "Fusing Multifaceted Transaction Data for User Modeling and Demographic Prediction. (arXiv:1712.07230v1 [cs.SI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07230", "type": "text/html"}], "timestampUsec": "1513832809776831", "comments": [], "summary": {"content": "<p>Inferring user characteristics such as demographic attributes is of the \nutmost importance in many user-centric applications. Demographic data is an \nenabler of personalization, identity security, and other applications. Despite \nthat, this data is sensitive and often hard to obtain. Previous work has shown \nthat purchase history can be used for multi-task prediction of many demographic \nfields such as gender and marital status. Here we present an embedding based \nmethod to integrate multifaceted sequences of transaction data, together with \nauxiliary relational tables, for better user modeling and demographic \nprediction. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f33", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07230"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Pushparaja Murugan", "title": "Hyperparameters Optimization in Deep Convolutional Neural Network / Bayesian Approach with Gaussian Process Prior. (arXiv:1712.07233v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.07233", "type": "text/html"}], "timestampUsec": "1513832809776830", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505084f8ae\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505084f8ae&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Convolutional Neural Network is known as ConvNet have been extensively used \nin many complex machine learning tasks. However, hyperparameters optimization \nis one of a crucial step in developing ConvNet architectures, since the \naccuracy and performance are reliant on the hyperparameters. This multilayered \narchitecture parameterized by a set of hyperparameters such as the number of \nconvolutional layers, number of fully connected dense layers &amp; neurons, the \nprobability of dropout implementation, learning rate. Hence the searching the \nhyperparameter over the hyperparameter space are highly difficult to build such \ncomplex hierarchical architecture. Many methods have been proposed over the \ndecade to explore the hyperparameter space and find the optimum set of \nhyperparameter values. Reportedly, Gird search and Random search are said to be \ninefficient and extremely expensive, due to a large number of hyperparameters \nof the architecture. Hence, Sequential model-based Bayesian Optimization is a \npromising alternative technique to address the extreme of the unknown cost \nfunction. The recent study on Bayesian Optimization by Snoek in nine \nconvolutional network parameters is achieved the lowerest error report in the \nCIFAR-10 benchmark. This article is intended to provide the overview of the \nmathematical concept behind the Bayesian Optimization over a Gaussian prior. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07233"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kevin H. Lee, Lingzhou Xue, David R. Hunter", "title": "Model-Based Clustering of Time-Evolving Networks through Temporal Exponential-Family Random Graph Models. (arXiv:1712.07325v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.07325", "type": "text/html"}], "timestampUsec": "1513832809776829", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450508a081a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450508a081a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Dynamic networks are a general language for describing time-evolving complex \nsystems, and discrete time network models provide an emerging statistical \ntechnique for various applications. It is a fundamental research question to \ndetect the community structure in time-evolving networks. However, due to \nsignificant computational challenges and difficulties in modeling communities \nof time-evolving networks, there is little progress in the current literature \nto effectively find communities in time-evolving networks. In this work, we \npropose a novel model-based clustering framework for time-evolving networks \nbased on discrete time exponential-family random graph models. To choose the \nnumber of communities, we use conditional likelihood to construct an effective \nmodel selection criterion. Furthermore, we propose an efficient variational \nexpectation-maximization (EM) algorithm to find approximate maximum likelihood \nestimates of network parameters and mixing proportions. By using variational \nmethods and minorization-maximization (MM) techniques, our method has appealing \nscalability for large-scale time-evolving networks. The power of our method is \ndemonstrated in simulation studies and empirical applications to international \ntrade networks and the collaboration networks of a large American research \nuniversity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f3c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07325"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sven Klaassen, Jannis Kueck, Martin Spindler", "title": "Transformation Models in High-Dimensions. (arXiv:1712.07364v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.07364", "type": "text/html"}], "timestampUsec": "1513832809776828", "comments": [], "summary": {"content": "<p>Transformation models are a very important tool for applied statisticians and \neconometricians. In many applications, the dependent variable is transformed so \nthat homogeneity or normal distribution of the error holds. In this paper, we \nanalyze transformation models in a high-dimensional setting, where the set of \npotential covariates is large. We propose an estimator for the transformation \nparameter and we show that it is asymptotically normally distributed using an \northogonalized moment condition where the nuisance functions depend on the \ntarget parameter. In a simulation study, we show that the proposed estimator \nworks well in small samples. A common practice in labor economics is to \ntransform wage with the log-function. In this study, we test if this \ntransformation holds in CPS data from the United States. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f41", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07364"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1513924823, "author": "Hong Wang, Ashkan Rezaei, Brian D. Ziebart", "title": "Adversarial Structured Prediction for Multivariate Measures. (arXiv:1712.07374v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.07374", "type": "text/html"}], "timestampUsec": "1513832809776827", "comments": [], "summary": {"content": "<p>Many predicted structured objects (e.g., sequences, matchings, trees) are \nevaluated using the F-score, alignment error rate (AER), or other multivariate \nperformance measures. Since inductively optimizing these measures using \ntraining data is typically computationally difficult, empirical risk \nminimization of surrogate losses is employed, using, e.g., the hinge loss for \n(structured) support vector machines. These approximations often introduce a \nmismatch between the learner's objective and the desired application \nperformance, leading to inconsistency. We take a different approach: \nadversarially approximate training data while optimizing the exact F-score or \nAER. Structured predictions under this formulation result from solving zero-sum \ngames between a predictor seeking the best performance and an adversary seeking \nthe worst while required to (approximately) match certain structured properties \nof the training data. We explore this approach for word alignment (AER \nevaluation) and named entity recognition (F-score evaluation) with linear-chain \nconstraints. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513924821, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f44", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07374"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Robert P.W. Duin, Sergey Verzakov", "title": "Fast kNN mode seeking clustering applied to active learning. (arXiv:1712.07454v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07454", "type": "text/html"}], "timestampUsec": "1513832809776826", "comments": [], "summary": {"content": "<p>A significantly faster algorithm is presented for the original kNN mode \nseeking procedure. It has the advantages over the well-known mean shift \nalgorithm that it is feasible in high-dimensional vector spaces and results in \nuniquely, well defined modes. Moreover, without any additional computational \neffort it may yield a multi-scale hierarchy of clusterings. The time complexity \nis just O(n^1.5). resulting computing times range from seconds for 10^4 objects \nto minutes for 10^5 objects and to less than an hour for 10^6 objects. The \nspace complexity is just O(n). The procedure is well suited for finding large \nsets of small clusters and is thereby a candidate to analyze thousands of \nclusters in millions of objects. \n</p> \n<p>The kNN mode seeking procedure can be used for active learning by assigning \nthe clusters to the class of the modal objects of the clusters. Its feasibility \nis shown by some examples with up to 1.5 million handwritten digits. The \nobtained classification results based on the clusterings are compared with \nthose obtained by the nearest neighbor rule and the support vector classifier \nbased on the same labeled objects for training. It can be concluded that using \nthe clustering structure for classification can be significantly better than \nusing the trained classifiers. A drawback of using the clustering for \nclassification, however, is that no classifier is obtained that may be used for \nout-of-sample objects. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07454"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Wenjie Zheng, Aur&#xe9;lien Bellet, Patrick Gallinari", "title": "A Distributed Frank-Wolfe Framework for Learning Low-Rank Matrices with the Trace Norm. (arXiv:1712.07495v1 [cs.DC])", "alternate": [{"href": "http://arxiv.org/abs/1712.07495", "type": "text/html"}], "timestampUsec": "1513832809776825", "comments": [], "summary": {"content": "<p>We consider the problem of learning a high-dimensional but low-rank matrix \nfrom a large-scale dataset distributed over several machines, where \nlow-rankness is enforced by a convex trace norm constraint. We propose \nDFW-Trace, a distributed Frank-Wolfe algorithm which leverages the low-rank \nstructure of its updates to achieve efficiency in time, memory and \ncommunication usage. The step at the heart of DFW-Trace is solved approximately \nusing a distributed version of the power method. We provide a theoretical \nanalysis of the convergence of DFW-Trace, showing that we can ensure sublinear \nconvergence in expectation to an optimal solution with few power iterations per \nepoch. We implement DFW-Trace in the Apache Spark distributed programming \nframework and validate the usefulness of our approach on synthetic and real \ndata, including the ImageNet dataset with high-dimensional features extracted \nfrom a deep neural network. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f53", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07495"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Tengyuan Liang, Weijie Su", "title": "Statistical Inference for the Population Landscape via Moment Adjusted Stochastic Gradients. (arXiv:1712.07519v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07519", "type": "text/html"}], "timestampUsec": "1513832809776824", "comments": [], "summary": {"content": "<p>Modern statistical inference tasks often require iterative optimization \nmethods to approximate the solution. Convergence analysis from optimization \nonly tells us how well we are approximating the solution deterministically, but \noverlooks the sampling nature of the data. However, due to the randomness in \nthe data, statisticians are keen to provide uncertainty quantification, or \nconfidence, for the answer obtained after certain steps of optimization. \nTherefore, it is important yet challenging to understand the sampling \ndistribution of the iterative optimization methods. \n</p> \n<p>This paper makes some progress along this direction by introducing a new \nstochastic optimization method for statistical inference, the moment adjusted \nstochastic gradient descent. We establish non-asymptotic theory that \ncharacterizes the statistical distribution of the iterative methods, with good \noptimization guarantee. On the statistical front, the theory allows for model \nmisspecification, with very mild conditions on the data. For optimization, the \ntheory is flexible for both the convex and non-convex cases. Remarkably, the \nmoment adjusting idea motivated from \"error standardization\" in statistics \nachieves similar effect as Nesterov's acceleration in optimization, for certain \nconvex problems as in fitting generalized linear models. We also demonstrate \nthis acceleration effect in the non-convex setting through experiments. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f59", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07519"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Daniel Krefl, Stefano Carrazza, Babak Haghighat, Jens Kahlen", "title": "Riemann-Theta Boltzmann Machine. (arXiv:1712.07581v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07581", "type": "text/html"}], "timestampUsec": "1513832809776823", "comments": [], "summary": {"content": "<p>A general Boltzmann machine with continuous visible and discrete integer \nvalued hidden states is introduced. Under mild assumptions about the connection \nmatrices, the probability density function of the visible units can be solved \nfor analytically, yielding a novel parametric density function involving a \nratio of Riemann-Theta functions. The conditional expectation of a hidden state \nfor given visible states can also be calculated analytically, yielding a \nderivative of the logarithmic Riemann-Theta function. The conditional \nexpectation can be used as activation function in a feedforward neural network, \nthereby increasing the modelling capacity of the network. Both the Boltzmann \nmachine and the derived feedforward neural network can be successfully trained \nvia standard gradient- and non-gradient-based optimization techniques. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f5c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07581"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "R. Lily Hu, Jeremy Karnowski, Ross Fadely, Jean-Patrick Pommier", "title": "Image Segmentation to Distinguish Between Overlapping Human Chromosomes. (arXiv:1712.07639v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.07639", "type": "text/html"}], "timestampUsec": "1513832809776822", "comments": [], "summary": {"content": "<p>In medicine, visualizing chromosomes is important for medical diagnostics, \ndrug development, and biomedical research. Unfortunately, chromosomes often \noverlap and it is necessary to identify and distinguish between the overlapping \nchromosomes. A segmentation solution that is fast and automated will enable \nscaling of cost effective medicine and biomedical research. We apply neural \nnetwork-based image segmentation to the problem of distinguishing between \npartially overlapping DNA chromosomes. A convolutional neural network is \ncustomized for this problem. The results achieved intersection over union (IOU) \nscores of 94.7% for the overlapping region and 88-94% on the non-overlapping \nchromosome regions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f63", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07639"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yining Wang, Adams Wei Yu, Aarti Singh", "title": "On Computationally Tractable Selection of Experiments in Measurement-Constrained Regression Models. (arXiv:1601.02068v6 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1601.02068", "type": "text/html"}], "timestampUsec": "1513832809776821", "comments": [], "summary": {"content": "<p>We derive computationally tractable methods to select a small subset of \nexperiment settings from a large pool of given design points. The primary focus \nis on linear regression models, while the technique extends to generalized \nlinear models and Delta's method (estimating functions of linear regression \nmodels) as well. The algorithms are based on a continuous relaxation of an \notherwise intractable combinatorial optimization problem, with sampling or \ngreedy procedures as post-processing steps. Formal approximation guarantees are \nestablished for both algorithms, and numerical results on both synthetic and \nreal-world data confirm the effectiveness of the proposed methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f6a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1601.02068"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "William B Levy, Toby Berger, Mustafa Sungkar", "title": "Neural computation from first principles: Using the maximum entropy method to obtain an optimal bits-per-joule neuron. (arXiv:1606.03063v2 [q-bio.NC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1606.03063", "type": "text/html"}], "timestampUsec": "1513832809776820", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450508a0a48\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450508a0a48&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Optimization results are one method for understanding neural computation from \nNature's perspective and for defining the physical limits on neuron-like \nengineering. Earlier work looks at individual properties or performance \ncriteria and occasionally a combination of two, such as energy and information. \nHere we make use of Jaynes' maximum entropy method and combine a larger set of \nconstraints, possibly dimensionally distinct, each expressible as an \nexpectation. The method identifies a likelihood-function and a sufficient \nstatistic arising from each such optimization. This likelihood is a \nfirst-hitting time distribution in the exponential class. Particular constraint \nsets are identified that, from an optimal inference perspective, justify \nearlier neurocomputational models. Interactions between constraints, mediated \nthrough the inferred likelihood, restrict constraint-set parameterizations, \ne.g., the energy-budget limits estimation performance which, in turn, matches \nan axonal communication constraint. Such linkages are, for biologists, \nexperimental predictions of the method. In addition to the related likelihood, \nat least one type of constraint set implies marginal distributions, and in this \ncase, a Shannon bits/joule statement arises. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f73", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1606.03063"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xiansheng Guo, Nirwan Ansari", "title": "Localization by Fusing a Group of Fingerprints via Multiple Antennas in Indoor Environment. (arXiv:1609.00661v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1609.00661", "type": "text/html"}], "timestampUsec": "1513832809776819", "comments": [], "summary": {"content": "<p>Most existing fingerprints-based indoor localization approaches are based on \nsome single fingerprints, such as received signal strength (RSS), channel \nimpulse response (CIR), and signal subspace. However, the localization accuracy \nobtained by the single fingerprint approach is rather susceptible to the \nchanging environment, multi-path, and non-line-of-sight (NLOS) propagation. \nFurthermore, building the fingerprints is a very time consuming process. In \nthis paper, we propose a novel localization framework by Fusing A Group Of \nfingerprinTs (FAGOT) via multiple antennas for the indoor environment. We first \nbuild a GrOup Of Fingerprints (GOOF), which includes five different \nfingerprints, namely, RSS, covariance matrix, signal subspace, fractional low \norder moment, and fourth-order cumulant, which are obtained by different \ntransformations of the received signals from multiple antennas in the offline \nstage. Then, we design a parallel GOOF multiple classifiers based on AdaBoost \n(GOOF-AdaBoost) to train each of these fingerprints in parallel as five strong \nmultiple classifiers. In the online stage, we input the corresponding \ntransformations of the real measurements into these strong classifiers to \nobtain independent decisions. Finally, we propose an efficient combination \nfusion algorithm, namely, MUltiple Classifiers mUltiple Samples (MUCUS) fusion \nalgorithm to improve the accuracy of localization by combining the predictions \nof multiple classifiers with different samples. As compared with the single \nfingerprint approaches, the prediction probability of our proposed approach is \nimproved significantly. The process for building fingerprints can also be \nreduced drastically. We demonstrate the feasibility and performance of the \nproposed algorithm through extensive simulations as well as via real \nexperimental data using a Universal Software Radio Peripheral (USRP) platform \nwith four antennas. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f78", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1609.00661"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Luca Martino, Victor Elvira, Gustau Camps-Valls", "title": "The Recycling Gibbs Sampler for Efficient Learning. (arXiv:1611.07056v2 [stat.CO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.07056", "type": "text/html"}], "timestampUsec": "1513832809776818", "comments": [], "summary": {"content": "<p>Monte Carlo methods are essential tools for Bayesian inference. Gibbs \nsampling is a well-known Markov chain Monte Carlo (MCMC) algorithm, extensively \nused in signal processing, machine learning, and statistics, employed to draw \nsamples from complicated high-dimensional posterior distributions. The key \npoint for the successful application of the Gibbs sampler is the ability to \ndraw efficiently samples from the full-conditional probability density \nfunctions. Since in the general case this is not possible, in order to speed up \nthe convergence of the chain, it is required to generate auxiliary samples \nwhose information is eventually disregarded. In this work, we show that these \nauxiliary samples can be recycled within the Gibbs estimators, improving their \nefficiency with no extra cost. This novel scheme arises naturally after \npointing out the relationship between the standard Gibbs sampler and the chain \nrule used for sampling purposes. Numerical simulations involving simple and \nreal inference problems confirm the excellent performance of the proposed \nscheme in terms of accuracy and computational efficiency. In particular we give \nempirical evidence of performance in a toy example, inference of Gaussian \nprocesses hyperparameters, and learning dependence graphs through regression. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f7f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.07056"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xiansheng Guo, Sihua Shao, Nirwan Ansari, Abdallah Khreishah", "title": "Indoor Localization Using Visible Light Via Fusion Of Multiple Classifiers. (arXiv:1703.02184v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.02184", "type": "text/html"}], "timestampUsec": "1513832809776817", "comments": [], "summary": {"content": "<p>A multiple classifiers fusion localization technique using received signal \nstrengths (RSSs) of visible light is proposed, in which the proposed system \ntransmits different intensity modulated sinusoidal signals by LEDs and the \nsignals received by a Photo Diode (PD) placed at various grid points. First, we \nobtain some {\\emph{approximate}} received signal strengths (RSSs) fingerprints \nby capturing the peaks of power spectral density (PSD) of the received signals \nat each given grid point. Unlike the existing RSSs based algorithms, several \nrepresentative machine learning approaches are adopted to train multiple \nclassifiers based on these RSSs fingerprints. The multiple classifiers \nlocalization estimators outperform the classical RSS-based LED localization \napproaches in accuracy and robustness. To further improve the localization \nperformance, two robust fusion localization algorithms, namely, grid \nindependent least square (GI-LS) and grid dependent least square (GD-LS), are \nproposed to combine the outputs of these classifiers. We also use a singular \nvalue decomposition (SVD) based LS (LS-SVD) method to mitigate the numerical \nstability problem when the prediction matrix is singular. Experiments conducted \non intensity modulated direct detection (IM/DD) systems have demonstrated the \neffectiveness of the proposed algorithms. The experimental results show that \nthe probability of having mean square positioning error (MSPE) of less than 5cm \nachieved by GD-LS is improved by 93.03\\% and 93.15\\%, respectively, as compared \nto those by the RSS ratio (RSSR) and RSS matching methods with the FFT length \nof 2000. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f84", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.02184"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Dave Zachariah, Petre Stoica", "title": "Model-Robust Counterfactual Prediction Method. (arXiv:1705.07019v3 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.07019", "type": "text/html"}], "timestampUsec": "1513832809776816", "comments": [], "summary": {"content": "<p>We develop a method for assessing counterfactual predictions with multiple \ngroups. It is tuning-free and operational in high-dimensional covariate \nscenarios, with a runtime that scales linearly in the number of datapoints. The \ncomputational efficiency is leveraged to produce valid confidence intervals \nusing the conformal prediction approach. The method is model-robust in that it \nenables inferences from observational data even when the data model is \nmisspecified. The approach is illustrated using both real and synthetic \ndatasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f8c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.07019"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yanbo Fan, Siwei Lyu, Yiming Ying, Bao-Gang Hu", "title": "Learning with Average Top-k Loss. (arXiv:1705.08826v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08826", "type": "text/html"}], "timestampUsec": "1513832809776815", "comments": [], "summary": {"content": "<p>In this work, we introduce the {\\em average top-$k$} (\\atk) loss as a new \naggregate loss for supervised learning, which is the average over the $k$ \nlargest individual losses over a training dataset. We show that the \\atk loss \nis a natural generalization of the two widely used aggregate losses, namely the \naverage loss and the maximum loss, but can combine their advantages and \nmitigate their drawbacks to better adapt to different data distributions. \nFurthermore, it remains a convex function over all individual losses, which can \nlead to convex optimization problems that can be solved effectively with \nconventional gradient-based methods. We provide an intuitive interpretation of \nthe \\atk loss based on its equivalent effect on the continuous individual loss \nfunctions, suggesting that it can reduce the penalty on correctly classified \ndata. We further give a learning theory analysis of \\matk learning on the \nclassification calibration of the \\atk loss and the error bounds of \\atk-SVM. \nWe demonstrate the applicability of minimum average top-$k$ learning for binary \nclassification and regression using synthetic and real datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f91", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08826"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Boris Hanin", "title": "Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations. (arXiv:1708.02691v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.02691", "type": "text/html"}], "timestampUsec": "1513832809776814", "comments": [], "summary": {"content": "<p>This article concerns the expressive power of depth in neural nets with ReLU \nactivations and bounded width. We are particularly interested in the following \nquestions: what is the minimal width $w_{\\text{min}}(d)$ so that ReLU nets of \nwidth $w_{\\text{min}}(d)$ (and arbitrary depth) can approximate any continuous \nfunction on the unit cube $[0,1]^d$ aribitrarily well? For ReLU nets near this \nminimal width, what can one say about the depth necessary to approximate a \ngiven function? Our approach to this paper is based on the observation that, \ndue to the convexity of the ReLU activation, ReLU nets are particularly \nwell-suited for representing convex functions. In particular, we prove that \nReLU nets with width $d+1$ can approximate any continuous convex function of \n$d$ variables arbitrarily well. These results then give quantitative depth \nestimates for the rate of approximation of any continuous scalar function on \nthe $d$-dimensional cube $[0,1]^d$ by ReLU nets with width $d+3.$ \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f95", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.02691"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Francois Petitjean, Wray Buntine, Geoffrey I. Webb, Nayyar Zaidi", "title": "Accurate parameter estimation for Bayesian Network Classifiers using Hierarchical Dirichlet Processes. (arXiv:1708.07581v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.07581", "type": "text/html"}], "timestampUsec": "1513832809776813", "comments": [], "summary": {"content": "<p>This paper introduces a novel parameter estimation method for the probability \ntables of Bayesian network classifiers (BNCs), using hierarchical Dirichlet \nprocesses (HDPs). The main result of this paper is to show that improved \nparameter estimation allows BNCs to outperform leading learning methods such as \nRandom Forest for both 0-1 loss and RMSE, albeit just on categorical datasets. \n</p> \n<p>As data assets become larger, entering the hyped world of \"big\", efficient \naccurate classification requires three main elements: (1) classifiers with \nlow-bias that can capture the fine-detail of large datasets (2) out-of-core \nlearners that can learn from data without having to hold it all in main memory \nand (3) models that can classify new data very efficiently. \n</p> \n<p>The latest Bayesian network classifiers (BNCs) satisfy these requirements. \nTheir bias can be controlled easily by increasing the number of parents of the \nnodes in the graph. Their structure can be learned out of core with a limited \nnumber of passes over the data. However, as the bias is made lower to \naccurately model classification tasks, so is the accuracy of their parameters' \nestimates, as each parameter is estimated from ever decreasing quantities of \ndata. In this paper, we introduce the use of Hierarchical Dirichlet Processes \nfor accurate BNC parameter estimation. \n</p> \n<p>We conduct an extensive set of experiments on 68 standard datasets and \ndemonstrate that our resulting classifiers perform very competitively with \nRandom Forest in terms of prediction, while keeping the out-of-core capability \nand superior classification time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91f9c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.07581"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Andreas Mardt, Luca Pasquali, Hao Wu, Frank No&#xe9;", "title": "VAMPnets: Deep learning of molecular kinetics. (arXiv:1710.06012v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.06012", "type": "text/html"}], "timestampUsec": "1513832809776812", "comments": [], "summary": {"content": "<p>There is an increasing demand for computing the relevant structures, \nequilibria and long-timescale kinetics of biomolecular processes, such as \nprotein-drug binding, from high-throughput molecular dynamics simulations. \nCurrent methods employ transformation of simulated coordinates into structural \nfeatures, dimension reduction, clustering the dimension-reduced data, and \nestimation of a Markov state model or related model of the interconversion \nrates between molecular structures. This handcrafted approach demands a \nsubstantial amount of modeling expertise, as poor decisions at any step will \nlead to large modeling errors. Here we employ the variational approach for \nMarkov processes (VAMP) to develop a deep learning framework for molecular \nkinetics using neural networks, dubbed VAMPnets. A VAMPnet encodes the entire \nmapping from molecular coordinates to Markov states, thus combining the whole \ndata processing pipeline in a single end-to-end framework. Our method performs \nequally or better than state-of-the art Markov modeling methods and provides \neasily interpretable few-state kinetic models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91fa2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.06012"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Erin Craig, Carlos Arias, David Gillman", "title": "Predicting readmission risk from doctors' notes. (arXiv:1711.10663v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10663", "type": "text/html"}], "timestampUsec": "1513832809776811", "comments": [], "summary": {"content": "<p>We develop a model using deep learning techniques and natural language \nprocessing on unstructured text from medical records to predict hospital-wide \n$30$-day unplanned readmission, with c-statistic $.70$. Our model is \nconstructed to allow physicians to interpret the significant features for \nprediction. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91faa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10663"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ruimin Sun, Xiaoyong Yuan, Pan He, Qile Zhu, Aokun Chen, Andre Gregio, Daniela Oliveira, Xiaolin Li", "title": "Learning Fast and Slow: PROPEDEUTICA for Real-time Malware Detection. (arXiv:1712.01145v1 [cs.CR] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1712.01145", "type": "text/html"}], "timestampUsec": "1513832809776808", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450508a0c7d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450508a0c7d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we introduce and evaluate PROPEDEUTICA, a novel methodology \nand framework for efficient and effective real-time malware detection, \nleveraging the best of conventional machine learning (ML) and deep learning \n(DL) algorithms. In PROPEDEUTICA, all software processes in the system start \nexecution subjected to a conventional ML detector for fast classification. If a \npiece of software receives a borderline classification, it is subjected to \nfurther analysis via more performance expensive and more accurate DL methods, \nvia our newly proposed DL algorithm DEEPMALWARE. Further, we introduce delays \nto the execution of software subjected to deep learning analysis as a way to \n\"buy time\" for DL analysis and to rate-limit the impact of possible malware in \nthe system. We evaluated PROPEDEUTICA with a set of 9,115 malware samples and \n877 commonly used benign software samples from various categories for the \nWindows OS. Our results show that the false positive rate for conventional ML \nmethods can reach 20%, and for modern DL methods it is usually below 6%. \nHowever, the classification time for DL can be 100X longer than conventional ML \nmethods. PROPEDEUTICA improved the detection F1-score from 77.54% (conventional \nML method) to 90.25%, and reduced the detection time by 54.86%. Further, the \npercentage of software subjected to DL analysis was approximately 40% on \naverage. Further, the application of delays in software subjected to ML reduced \nthe detection time by approximately 10%. Finally, we found and discussed a \ndiscrepancy between the detection accuracy offline (analysis after all traces \nare collected) and on-the-fly (analysis in tandem with trace collection). Our \ninsights show that conventional ML and modern DL-based malware detectors in \nisolation cannot meet the needs of efficient and effective malware detection: \nhigh accuracy, low false positive rate, and short classification time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513832809777", "annotations": [], "published": 1513832810, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034ba91fb4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01145"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Mansour Sheikhan, Ehsan Hemmati", "title": "PSO-Optimized Hopfield Neural Network-Based Multipath Routing for Mobile Ad-hoc Networks. (arXiv:1712.07019v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.07019", "type": "text/html"}], "timestampUsec": "1513747858834900", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050903e41\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050903e41&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Mobile ad-hoc network (MANET) is a dynamic collection of mobile computers \nwithout the need for any existing infrastructure. Nodes in a MANET act as hosts \nand routers. Designing of robust routing algorithms for MANETs is a challenging \ntask. Disjoint multipath routing protocols address this problem and increase \nthe reliability, security and lifetime of network. However, selecting an \noptimal multipath is an NP-complete problem. In this paper, Hopfield neural \nnetwork (HNN) which its parameters are optimized by particle swarm optimization \n(PSO) algorithm is proposed as multipath routing algorithm. Link expiration \ntime (LET) between each two nodes is used as the link reliability estimation \nmetric. This approach can find either node-disjoint or link-disjoint paths in \nsingle phase route discovery. Simulation results confirm that PSO-HNN routing \nalgorithm has better performance as compared to backup path set selection \nalgorithm (BPSA) in terms of the path set reliability and number of paths in \nthe set. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed904c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07019"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "A.V. Eremeev, Yu.V. Kovalenko", "title": "Genetic Algorithm with Optimal Recombination for the Asymmetric Travelling Salesman Problem. (arXiv:1706.06920v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.06920", "type": "text/html"}], "timestampUsec": "1513747858834899", "comments": [], "summary": {"content": "<p>We propose a new genetic algorithm with optimal recombination for the \nasymmetric instances of travelling salesman problem. The algorithm incorporates \nseveral new features that contribute to its effectiveness: (i) Optimal \nrecombination problem is solved within crossover operator. (ii) A new mutation \noperator performs a random jump within 3-opt or 4-opt neighborhood. (iii) \nGreedy constructive heuristic of W.Zhang and 3-opt local search heuristic are \nused to generate the initial population. A computational experiment on TSPLIB \ninstances shows that the proposed algorithm yields competitive results to other \nwell-known memetic algorithms for asymmetric travelling salesman problem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9050", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.06920"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zehao Huang, Naiyan Wang", "title": "Data-Driven Sparse Structure Selection for Deep Neural Networks. (arXiv:1707.01213v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.01213", "type": "text/html"}], "timestampUsec": "1513747858834898", "comments": [], "summary": {"content": "<p>Deep convolutional neural networks have liberated its extraordinary power on \nvarious tasks. However, it is still very challenging to deploy state-of-the-art \nmodels into real-world applications due to their high computational complexity. \nHow can we design a compact and effective network without massive experiments \nand expert knowledge? In this paper, we propose a simple and effective \nframework to learn and prune deep models in an end-to-end manner. In our \nframework, a new type of parameter -- scaling factor is first introduced to \nscale the outputs of specific structures, such as neurons, groups or residual \nblocks. Then we add sparsity regularizations on these factors, and solve this \noptimization problem by a modified stochastic Accelerated Proximal Gradient \n(APG) method. By forcing some of the factors to zero, we can safely remove the \ncorresponding structures, thus prune the unimportant parts of a CNN. Comparing \nwith other structure selection methods that may need thousands of trials or \niterative fine-tuning, our method is trained fully end-to-end in one training \npass without bells and whistles. We evaluate our method, Sparse Structure \nSelection with several state-of-the-art CNNs, and demonstrate very promising \nresults with adaptive depth and width selection. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9056", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.01213"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zehao Huang, Naiyan Wang", "title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer. (arXiv:1707.01219v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.01219", "type": "text/html"}], "timestampUsec": "1513747858834897", "comments": [], "summary": {"content": "<p>Despite deep neural networks have demonstrated extraordinary power in various \napplications, their superior performances are at expense of high storage and \ncomputational costs. Consequently, the acceleration and compression of neural \nnetworks have attracted much attention recently. Knowledge Transfer (KT), which \naims at training a smaller student network by transferring knowledge from a \nlarger teacher model, is one of the popular solutions. In this paper, we \npropose a novel knowledge transfer method by treating it as a distribution \nmatching problem. Particularly, we match the distributions of neuron \nselectivity patterns between teacher and student networks. To achieve this \ngoal, we devise a new KT loss function by minimizing the Maximum Mean \nDiscrepancy (MMD) metric between these distributions. Combined with the \noriginal loss function, our method can significantly improve the performance of \nstudent networks. We validate the effectiveness of our method across several \ndatasets, and further combine it with other KT methods to explore the best \npossible results. Last but not least, we fine-tune the model to other tasks \nsuch as object detection. The results are also encouraging, which confirm the \ntransferability of the learned features. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9058", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.01219"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang", "title": "DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer. (arXiv:1707.01220v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.01220", "type": "text/html"}], "timestampUsec": "1513747858834896", "comments": [], "summary": {"content": "<p>We have witnessed rapid evolution of deep neural network architecture design \nin the past years. These latest progresses greatly facilitate the developments \nin various areas such as computer vision and natural language processing. \nHowever, along with the extraordinary performance, these state-of-the-art \nmodels also bring in expensive computational cost. Directly deploying these \nmodels into applications with real-time requirement is still infeasible. \nRecently, Hinton etal. have shown that the dark knowledge within a powerful \nteacher model can significantly help the training of a smaller and faster \nstudent network. These knowledge are vastly beneficial to improve the \ngeneralization ability of the student model. Inspired by their work, we \nintroduce a new type of knowledge -- cross sample similarities for model \ncompression and acceleration. This knowledge can be naturally derived from deep \nmetric learning model. To transfer them, we bring the \"learning to rank\" \ntechnique into deep metric learning formulation. We test our proposed DarkRank \nmethod on various metric learning tasks including pedestrian re-identification, \nimage retrieval and image clustering. The results are quite encouraging. Our \nmethod can improve over the baseline method by a large margin. Moreover, it is \nfully compatible with other existing methods. When combined, the performance \ncan be further boosted. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed905f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.01220"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Andreas Holzinger, Bernd Malle, Peter Kieseberg, Peter M. Roth, Heimo M&#xfc;ller, Robert Reihs, Kurt Zatloukal", "title": "Towards the Augmented Pathologist: Challenges of Explainable-AI in Digital Pathology. (arXiv:1712.06657v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.06657", "type": "text/html"}], "timestampUsec": "1513747858834894", "comments": [], "summary": {"content": "<p>Digital pathology is not only one of the most promising fields of diagnostic \nmedicine, but at the same time a hot topic for fundamental research. Digital \npathology is not just the transfer of histopathological slides into digital \nrepresentations. The combination of different data sources (images, patient \nrecords, and *omics data) together with current advances in artificial \nintelligence/machine learning enable to make novel information accessible and \nquantifiable to a human expert, which is not yet available and not exploited in \ncurrent medical settings. The grand goal is to reach a level of usable \nintelligence to understand the data in the context of an application task, \nthereby making machine decisions transparent, interpretable and explainable. \nThe foundation of such an \"augmented pathologist\" needs an integrated approach: \nWhile machine learning algorithms require many thousands of training examples, \na human expert is often confronted with only a few data points. Interestingly, \nhumans can learn from such few examples and are able to instantly interpret \ncomplex patterns. Consequently, the grand goal is to combine the possibilities \nof artificial intelligence with human intelligence and to find a well-suited \nbalance between them to enable what neither of them could do on their own. This \ncan raise the quality of education, diagnosis, prognosis and prediction of \ncancer and other diseases. In this paper we describe some (incomplete) research \nissues which we believe should be addressed in an integrated and concerted \neffort for paving the way towards the augmented pathologist. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9063", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06657"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Saptarshi Pal, Soumya K Ghosh", "title": "Learning Representations from Road Network for End-to-End Urban Growth Simulation. (arXiv:1712.06778v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.06778", "type": "text/html"}], "timestampUsec": "1513747858834893", "comments": [], "summary": {"content": "<p>From our experiences in the past, we have seen that the growth of cities is \nvery much dependent on the transportation networks. In mega cities, \ntransportation networks determine to a significant extent as to where the \npeople will move and houses will be built. Hence, transportation network data \nis crucial to an urban growth prediction system. Existing works have used \nmanually derived distance based features based on the road networks to build \nmodels on urban growth. But due to the non-generic and laborious nature of the \nmanual feature engineering process, we can shift to End-to-End systems which do \nnot rely on manual feature engineering. In this paper, we propose a method to \nintegrate road network data to an existing Rule based End-to-End framework \nwithout manual feature engineering. Our method employs recurrent neural \nnetworks to represent road networks in a structured way such that it can be \nplugged into the previously proposed End-to-End framework. The proposed \napproach enhances the performance in terms of Figure of Merit, Producer's \naccuracy, User's accuracy and Overall accuracy of the existing Rule based \nEnd-to-End framework. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9066", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06778"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Christoph Wernhard", "title": "Heinrich Behmann's Contributions to Second-Order Quantifier Elimination from the View of Computational Logic. (arXiv:1712.06868v1 [cs.LO])", "alternate": [{"href": "http://arxiv.org/abs/1712.06868", "type": "text/html"}], "timestampUsec": "1513747858834892", "comments": [], "summary": {"content": "<p>For relational monadic formulas (the L\\\"owenheim class) second-order \nquantifier elimination, which is closely related to computation of uniform \ninterpolants, projection and forgetting - operations that currently receive \nmuch attention in knowledge processing - always succeeds. The decidability \nproof for this class by Heinrich Behmann from 1922 explicitly proceeds by \nelimination with equivalence preserving formula rewriting. Here we reconstruct \nthe results from Behmann's publication in detail and discuss related issues \nthat are relevant in the context of modern approaches to second-order \nquantifier elimination in computational logic. In addition, an extensive \ndocumentation of the letters and manuscripts in Behmann's bequest that concern \nsecond-order quantifier elimination is given, including a commented register \nand English abstracts of the German sources with focus on technical material. \nIn the late 1920s Behmann attempted to develop an elimination-based decision \nmethod for formulas with predicates whose arity is larger than one. His \nmanuscripts and the correspondence with Wilhelm Ackermann show technical \naspects that are still of interest today and give insight into the genesis of \nAckermann's landmark paper \"Untersuchungen \\\"uber das Eliminationsproblem der \nmathematischen Logik\" from 1935, which laid the foundation of the two \nprevailing modern approaches to second-order quantifier elimination. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed906a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06868"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Romain Laroche, Paul Trichelair, Layla El Asri", "title": "Safe Policy Improvement with Baseline Bootstrapping. (arXiv:1712.06924v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.06924", "type": "text/html"}], "timestampUsec": "1513747858834891", "comments": [], "summary": {"content": "<p>A common goal in Reinforcement Learning is to derive a good strategy given a \nlimited batch of data. In this paper, we adopt the safe policy improvement \n(SPI) approach: we compute a target policy guaranteed to perform at least as \nwell as a given baseline policy. Our SPI strategy, inspired by the \nknows-what-it-knows paradigms, consists in bootstrapping the target policy with \nthe baseline policy when it does not know. We develop two computationally \nefficient bootstrapping algorithms, a value-based and a policy-based, both \naccompanied with theoretical SPI bounds. Three algorithm variants are proposed. \nWe empirically show the literature algorithms limits on a small stochastic \ngridworld problem, and then demonstrate that our five algorithms not only \nimprove the worst case scenarios, but also the mean performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed906f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06924"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Serdar Kadioglu", "title": "Column Generation for Interaction Coverage in Combinatorial Software Testing. (arXiv:1712.07081v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.07081", "type": "text/html"}], "timestampUsec": "1513747858834890", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050904574\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050904574&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper proposes a novel column generation framework for combinatorial \nsoftware testing. In particular, it combines Mathematical Programming and \nConstraint Programming in a hybrid decomposition to generate covering arrays. \nThe approach allows generating parameterized test cases with coverage \nguarantees between parameter interactions of a given application. Compared to \nexhaustive testing, combinatorial test case generation reduces the number of \ntests to run significantly. Our column generation algorithm is generic and can \naccommodate mixed coverage arrays over heterogeneous alphabets. The algorithm \nis realized in practice as a cloud service and recognized as one of the five \nwinners of the company-wide cloud application challenge at Oracle. The service \nis currently helping software developers from a range of different product \nteams in their testing efforts while exposing declarative constraint models and \nhybrid optimization techniques to a broader audience. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9072", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07081"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Fangyi Zhang, J&#xfc;rgen Leitner, Michael Milford, Peter Corke", "title": "Modular Deep Q Networks for Sim-to-real Transfer of Visuo-motor Policies. (arXiv:1610.06781v4 [cs.RO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.06781", "type": "text/html"}], "timestampUsec": "1513747858834889", "comments": [], "summary": {"content": "<p>While deep learning has had significant successes in computer vision thanks \nto the abundance of visual data, collecting sufficiently large real-world \ndatasets for robot learning can be costly. To increase the practicality of \nthese techniques on real robots, we propose a modular deep reinforcement \nlearning method capable of transferring models trained in simulation to a \nreal-world robotic task. We introduce a bottleneck between perception and \ncontrol, enabling the networks to be trained independently, but then merged and \nfine-tuned in an end-to-end manner to further improve hand-eye coordination. On \na canonical, planar visually-guided robot reaching task a fine-tuned accuracy \nof 1.6 pixels is achieved, a significant improvement over naive transfer (17.5 \npixels), showing the potential for more complicated and broader applications. \nOur method provides a technique for more efficient learning and transfer of \nvisuo-motor policies for real robotic systems without relying entirely on large \nreal-world robot datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9075", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.06781"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Christoph Wernhard", "title": "The Boolean Solution Problem from the Perspective of Predicate Logic - Extended Version. (arXiv:1706.08329v3 [cs.LO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.08329", "type": "text/html"}], "timestampUsec": "1513747858834888", "comments": [], "summary": {"content": "<p>Finding solution values for unknowns in Boolean equations was a principal \nreasoning mode in the Algebra of Logic of the 19th century. Schr\\\"oder \ninvestigated it as \"Aufl\\\"osungsproblem\" (\"solution problem\"). It is closely \nrelated to the modern notion of Boolean unification. Today it is commonly \npresented in an algebraic setting, but seems potentially useful also in \nknowledge representation based on predicate logic. We show that it can be \nmodeled on the basis of first-order logic extended by second-order \nquantification. A wealth of classical results transfers, foundations for \nalgorithms unfold, and connections with second-order quantifier elimination and \nCraig interpolation show up. Although for first-order inputs the set of \nsolutions is recursively enumerable, the development of constructive methods \nremains a challenge. We identify some cases that allow constructions, most of \nthem based on Craig interpolation, and show a method to take vocabulary \nrestrictions on solution components into account. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed907a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.08329"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ethan Perez, Harm de Vries, Florian Strub, Vincent Dumoulin, Aaron Courville", "title": "Learning Visual Reasoning Without Strong Priors. (arXiv:1707.03017v5 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.03017", "type": "text/html"}], "timestampUsec": "1513747858834887", "comments": [], "summary": {"content": "<p>Achieving artificial visual reasoning - the ability to answer image-related \nquestions which require a multi-step, high-level process - is an important step \ntowards artificial general intelligence. This multi-modal task requires \nlearning a question-dependent, structured reasoning process over images from \nlanguage. Standard deep learning approaches tend to exploit biases in the data \nrather than learn this underlying structure, while leading methods learn to \nvisually reason successfully but are hand-crafted for reasoning. We show that a \ngeneral-purpose, Conditional Batch Normalization approach achieves \nstate-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4% \nerror rate. We outperform the next best end-to-end method (4.5%) and even \nmethods that use extra supervision (3.1%). We probe our model to shed light on \nhow it reasons, showing it has learned a question-dependent, multi-step \nprocess. Previous work has operated under the assumption that visual reasoning \ncalls for a specialized architecture, but we show that a general architecture \nwith proper conditioning can learn to visually reason effectively. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9080", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.03017"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, Aaron Courville", "title": "FiLM: Visual Reasoning with a General Conditioning Layer. (arXiv:1709.07871v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.07871", "type": "text/html"}], "timestampUsec": "1513747858834886", "comments": [], "summary": {"content": "<p>We introduce a general-purpose conditioning method for neural networks called \nFiLM: Feature-wise Linear Modulation. FiLM layers influence neural network \ncomputation via a simple, feature-wise affine transformation based on \nconditioning information. We show that FiLM layers are highly effective for \nvisual reasoning - answering image-related questions which require a \nmulti-step, high-level process - a task which has proven difficult for standard \ndeep learning methods that do not explicitly model reasoning. Specifically, we \nshow on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error \nfor the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are \nrobust to ablations and architectural modifications, and 4) generalize well to \nchallenging, new data from few examples or even zero-shot. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9087", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.07871"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Farshid Rayhan, Sajid Ahmed, Asif Mahbub, Md. Rafsan Jani, Swakkhar Shatabda, Dewan Md. Farid, Chowdhury Mofizur Rahman", "title": "MEBoost: Mixing Estimators with Boosting for Imbalanced Data Classification. (arXiv:1712.06658v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.06658", "type": "text/html"}], "timestampUsec": "1513747858834885", "comments": [], "summary": {"content": "<p>Class imbalance problem has been a challenging research problem in the fields \nof machine learning and data mining as most real life datasets are imbalanced. \nSeveral existing machine learning algorithms try to maximize the accuracy \nclassification by correctly identifying majority class samples while ignoring \nthe minority class. However, the concept of the minority class instances \nusually represents a higher interest than the majority class. Recently, several \ncost sensitive methods, ensemble models and sampling techniques have been used \nin literature in order to classify imbalance datasets. In this paper, we \npropose MEBoost, a new boosting algorithm for imbalanced datasets. MEBoost \nmixes two different weak learners with boosting to improve the performance on \nimbalanced datasets. MEBoost is an alternative to the existing techniques such \nas SMOTEBoost, RUSBoost, Adaboost, etc. The performance of MEBoost has been \nevaluated on 12 benchmark imbalanced datasets with state of the art ensemble \nmethods like SMOTEBoost, RUSBoost, Easy Ensemble, EUSBoost, DataBoost. \nExperimental results show significant improvement over the other methods and it \ncan be concluded that MEBoost is an effective and promising algorithm to deal \nwith imbalance datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed908c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06658"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yash Deshpande, Lester Mackey, Vasilis Syrgkanis, Matt Taddy", "title": "Accurate Inference for Adaptive Linear Models. (arXiv:1712.06695v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06695", "type": "text/html"}], "timestampUsec": "1513747858834884", "comments": [], "summary": {"content": "<p>Estimators computed from adaptively collected data do not behave like their \nnon-adaptive brethren. Rather, the sequential dependence of the collection \npolicy can lead to severe distributional biases that persist even in the \ninfinite data limit. We develop a general method decorrelation procedure -- \nW-decorrelation -- for transforming the bias of adaptive linear regression \nestimators into variance. The method uses only coarse-grained information about \nthe data collection policy and does not need access to propensity scores or \nexact knowledge of the policy. We bound the finite-sample bias and variance of \nthe W-estimator and develop asymptotically correct confidence intervals based \non a novel martingale central limit theorem. We then demonstrate the empirical \nbenefits of the generic W-decorrelation procedure in two different adaptive \ndata settings: the multi-armed bandits and autoregressive time series models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9091", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06695"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kriste Krstovski, Michael J. Kurtz, David A. Smith, Alberto Accomazzi", "title": "Multilingual Topic Models. (arXiv:1712.06704v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06704", "type": "text/html"}], "timestampUsec": "1513747858834883", "comments": [], "summary": {"content": "<p>Scientific publications have evolved several features for mitigating \nvocabulary mismatch when indexing, retrieving, and computing similarity between \narticles. These mitigation strategies range from simply focusing on high-value \narticle sections, such as titles and abstracts, to assigning keywords, often \nfrom controlled vocabularies, either manually or through automatic annotation. \nVarious document representation schemes possess different cost-benefit \ntradeoffs. In this paper, we propose to model different representations of the \nsame article as translations of each other, all generated from a common latent \nrepresentation in a multilingual topic model. We start with a methodological \noverview on latent variable models for parallel document representations that \ncould be used across many information science tasks. We then show how solving \nthe inference problem of mapping diverse representations into a shared topic \nspace allows us to evaluate representations based on how topically similar they \nare to the original article. In addition, our proposed approach provides means \nto discover where different concept vocabularies require improvement. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9094", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06704"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jiajun Shen, Yali Amit", "title": "Deformable Classifiers. (arXiv:1712.06715v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06715", "type": "text/html"}], "timestampUsec": "1513747858834882", "comments": [], "summary": {"content": "<p>Geometric variations of objects, which do not modify the object class, pose a \nmajor challenge for object recognition. These variations could be rigid as well \nas non-rigid transformations. In this paper, we design a framework for training \ndeformable classifiers, where latent transformation variables are introduced, \nand a transformation of the object image to a reference instantiation is \ncomputed in terms of the classifier output, separately for each class. The \nclassifier outputs for each class, after transformation, are compared to yield \nthe final decision. As a by-product of the classification this yields a \ntransformation of the input object to a reference pose, which can be used for \ndownstream tasks such as the computation of object support. We apply a two-step \ntraining mechanism for our framework, which alternates between optimizing over \nthe latent transformation variables and the classifier parameters to minimize \nthe loss function. We show that multilayer perceptrons, also known as deep \nnetworks, are well suited for this approach and achieve state of the art \nresults on the rotated MNIST and the Google Earth dataset, and produce \ncompetitive results on MNIST and CIFAR-10 when training on smaller subsets of \ntraining data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed9097", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06715"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jun Kitazono, Ryota Kanai, Masafumi Oizumi", "title": "Efficient Algorithms for Searching the Minimum Information Partition in Integrated Information Theory. (arXiv:1712.06745v1 [q-bio.NC])", "alternate": [{"href": "http://arxiv.org/abs/1712.06745", "type": "text/html"}], "timestampUsec": "1513747858834881", "comments": [], "summary": {"content": "<p>The ability to integrate information in the brain is considered to be an \nessential property for cognition and consciousness. Integrated Information \nTheory (IIT) hypothesizes that the amount of integrated information ($\\Phi$) in \nthe brain is related to the level of consciousness. IIT proposes that to \nquantify information integration in a system as a whole, integrated information \nshould be measured across the partition of the system at which information loss \ncaused by partitioning is minimized, called the Minimum Information Partition \n(MIP). The computational cost for exhaustively searching for the MIP grows \nexponentially with system size, making it difficult to apply IIT to real neural \ndata. It has been previously shown that if a measure of $\\Phi$ satisfies a \nmathematical property, submodularity, the MIP can be found in a polynomial \norder by an optimization algorithm. However, although the first version of \n$\\Phi$ is submodular, the later versions are not. In this study, we empirically \nexplore to what extent the algorithm can be applied to the non-submodular \nmeasures of $\\Phi$ by evaluating the accuracy of the algorithm in simulated \ndata and real neural data. We find that the algorithm identifies the MIP in a \nnearly perfect manner even for the non-submodular measures. Our results show \nthat the algorithm allows us to measure $\\Phi$ in large systems within a \npractical amount of time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed909d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06745"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Marzieh Haghighi, Simon K. Warfield, Sila Kurugol", "title": "Automatic Renal Segmentation in DCE-MRI using Convolutional Neural Networks. (arXiv:1712.07022v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.07022", "type": "text/html"}], "timestampUsec": "1513747858834880", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450509047a7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450509047a7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Kidney function evaluation using dynamic contrast-enhanced MRI (DCE-MRI) \nimages could help in diagnosis and treatment of kidney diseases of children. \nAutomatic segmentation of renal parenchyma is an important step in this \nprocess. In this paper, we propose a time and memory efficient fully automated \nsegmentation method which achieves high segmentation accuracy with running time \nin the order of seconds in both normal kidneys and kidneys with hydronephrosis. \nThe proposed method is based on a cascaded application of two 3D convolutional \nneural networks that employs spatial and temporal information at the same time \nin order to learn the tasks of localization and segmentation of kidneys, \nrespectively. Segmentation performance is evaluated on both normal and abnormal \nkidneys with varying levels of hydronephrosis. We achieved a mean dice \ncoefficient of 91.4 and 83.6 for normal and abnormal kidneys of pediatric \npatients, respectively. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed909f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07022"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Adil Salim, Pascal Bianchi, Walid Hachem", "title": "Snake: a Stochastic Proximal Gradient Algorithm for Regularized Problems over Large Graphs. (arXiv:1712.07027v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1712.07027", "type": "text/html"}], "timestampUsec": "1513747858834879", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050958e6b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050958e6b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A regularized optimization problem over a large unstructured graph is \nstudied, where the regularization term is tied to the graph geometry. Typical \nregularization examples include the total variation and the Laplacian \nregularizations over the graph. When applying the proximal gradient algorithm \nto solve this problem, there exist quite affordable methods to implement the \nproximity operator (backward step) in the special case where the graph is a \nsimple path without loops. In this paper, an algorithm, referred to as \"Snake\", \nis proposed to solve such regularized problems over general graphs, by taking \nbenefit of these fast methods. The algorithm consists in properly selecting \nrandom simple paths in the graph and performing the proximal gradient algorithm \nover these simple paths. This algorithm is an instance of a new general \nstochastic proximal gradient algorithm, whose convergence is proven. \nApplications to trend filtering and graph inpainting are provided among others. \nNumerical experiments are conducted over large graphs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07027"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Marta M. Stepniewska-Dziubinska, Piotr Zielenkiewicz, Pawel Siedlecki", "title": "Pafnucy -- A deep neural network for structure-based drug discovery. (arXiv:1712.07042v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07042", "type": "text/html"}], "timestampUsec": "1513747858834878", "comments": [], "summary": {"content": "<p>Virtual screening is one of the most successful approaches for augmenting the \ndrug discovery process. Currently, there is a notable shift towards machine \nlearning (ML) methodologies to aid this process. Deep learning has recently \ngained considerable attention as it allows the model to \"learn\" to extract \nfeatures that are relevant for the task at hand. We have developed a new deep \nneural network tailored to estimating the binding affinity of ligand-receptor \ncomplexes. The complex is represented with a 3D grid, and the model utilizes a \n3D convolution to produce a feature map of this representation, treating the \natoms of both proteins and ligands in the same manner. Our network was tested \non the CASF \"scoring power\" benchmark and Astex diverse set and outperformed \nclassical scoring functions. The model, together with usage instructions and \nexamples, is available as a git repository at \n<a href=\"http://gitlab.com/cheminfIBB/pafnucy\">this http URL</a> \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90a8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07042"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yingbo Zhou, Caiming Xiong, Richard Socher", "title": "Improving End-to-End Speech Recognition with Policy Learning. (arXiv:1712.07101v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.07101", "type": "text/html"}], "timestampUsec": "1513747858834877", "comments": [], "summary": {"content": "<p>Connectionist temporal classification (CTC) is widely used for maximum \nlikelihood learning in end-to-end speech recognition models. However, there is \nusually a disparity between the negative maximum likelihood and the performance \nmetric used in speech recognition, e.g., word error rate (WER). This results in \na mismatch between the objective function and metric during training. We show \nthat the above problem can be mitigated by jointly training with maximum \nlikelihood and policy gradient. In particular, with policy learning we are able \nto directly optimize on the (otherwise non-differentiable) performance metric. \nWe show that joint training improves relative performance by 4% to 13% for our \nend-to-end model as compared to the same model learned through maximum \nlikelihood. The model achieves 5.53% WER on Wall Street Journal dataset, and \n5.42% and 14.70% on Librispeech test-clean and test-other set, respectively. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90ad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07101"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Shahin Shahrampour, Ahmad Beirami, Vahid Tarokh", "title": "On Data-Dependent Random Features for Improved Generalization in Supervised Learning. (arXiv:1712.07102v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07102", "type": "text/html"}], "timestampUsec": "1513747858834876", "comments": [], "summary": {"content": "<p>The randomized-feature approach has been successfully employed in large-scale \nkernel approximation and supervised learning. The distribution from which the \nrandom features are drawn impacts the number of features required to \nefficiently perform a learning task. Recently, it has been shown that employing \ndata-dependent randomization improves the performance in terms of the required \nnumber of random features. In this paper, we are concerned with the \nrandomized-feature approach in supervised learning for good generalizability. \nWe propose the Energy-based Exploration of Random Features (EERF) algorithm \nbased on a data-dependent score function that explores the set of possible \nfeatures and exploits the promising regions. We prove that the proposed score \nfunction with high probability recovers the spectrum of the best fit within the \nmodel class. Our empirical results on several benchmark datasets further verify \nthat our method requires smaller number of random features to achieve a certain \ngeneralization error compared to the state-of-the-art while introducing \nnegligible pre-processing overhead. EERF can be implemented in a few lines of \ncode and requires no additional tuning parameters. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90b1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07102"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jayaraman J. Thiagarajan, Shusen Liu, Karthikeyan Natesan Ramamurthy, Peer-Timo Bremer", "title": "Exploring High-Dimensional Structure via Axis-Aligned Decomposition of Linear Projections. (arXiv:1712.07106v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.07106", "type": "text/html"}], "timestampUsec": "1513747858834875", "comments": [], "summary": {"content": "<p>Two-dimensional embeddings remain the dominant approach to visualize high \ndimensional data. The choice of embeddings ranges from highly non-linear ones, \nwhich can capture complex relationships but are difficult to interpret \nquantitatively, to axis-aligned projections, which are easy to interpret but \nare limited to bivariate relationships. Linear project can be considered as a \ncompromise between complexity and interpretability, as they allow explicit axes \nlabels, yet provide significantly more degrees of freedom compared to \naxis-aligned projections. Nevertheless, interpreting the axes directions, which \nare linear combinations often with many non-trivial components, remains \ndifficult. To address this problem we introduce a structure aware decomposition \nof (multiple) linear projections into sparse sets of axis aligned projections, \nwhich jointly capture all information of the original linear ones. In \nparticular, we use tools from Dempster-Shafer theory to formally define how \nrelevant a given axis aligned project is to explain the neighborhood relations \ndisplayed in some linear projection. Furthermore, we introduce a new approach \nto discover a diverse set of high quality linear projections and show that in \npractice the information of $k$ linear projections is often jointly encoded in \n$\\sim k$ axis aligned plots. We have integrated these ideas into an interactive \nvisualization system that allows users to jointly browse both linear \nprojections and their axis aligned representatives. Using a number of case \nstudies we show how the resulting plots lead to more intuitive visualizations \nand new insight. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90b4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07106"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yingbo Zhou, Caiming Xiong, Richard Socher", "title": "Improved Regularization Techniques for End-to-End Speech Recognition. (arXiv:1712.07108v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.07108", "type": "text/html"}], "timestampUsec": "1513747858834874", "comments": [], "summary": {"content": "<p>Regularization is important for end-to-end speech models, since the models \nare highly flexible and easy to overfit. Data augmentation and dropout has been \nimportant for improving end-to-end models in other domains. However, they are \nrelatively under explored for end-to-end speech models. Therefore, we \ninvestigate the effectiveness of both methods for end-to-end trainable, deep \nspeech recognition models. We augment audio data through random perturbations \nof tempo, pitch, volume, temporal alignment, and adding random noise.We further \ninvestigate the effect of dropout when applied to the inputs of all layers of \nthe network. We show that the combination of data augmentation and dropout give \na relative performance improvement on both Wall Street Journal (WSJ) and \nLibriSpeech dataset of over 20%. Our model performance is also competitive with \nother end-to-end speech models on both datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90b8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07108"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Per Mattsson, Dave Zachariah, Petre Stoica", "title": "Recursive nonlinear-system identification using latent variables. (arXiv:1606.04366v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1606.04366", "type": "text/html"}], "timestampUsec": "1513747858834873", "comments": [], "summary": {"content": "<p>In this paper we develop a method for learning nonlinear systems with \nmultiple outputs and inputs. We begin by modelling the errors of a nominal \npredictor of the system using a latent variable framework. Then using the \nmaximum likelihood principle we derive a criterion for learning the model. The \nresulting optimization problem is tackled using a majorization-minimization \napproach. Finally, we develop a convex majorization technique and show that it \nenables a recursive identification method. The method learns parsimonious \npredictive models and is tested on both synthetic and real nonlinear systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90bb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1606.04366"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jen-Cheng Hou, Syu-Siang Wang, Ying-Hui Lai, Yu Tsao, Hsiu-Wen Chang, Hsin-Min Wang", "title": "Audio-Visual Speech Enhancement based on Multimodal Deep Convolutional Neural Networks. (arXiv:1703.10893v5 [cs.SD] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.10893", "type": "text/html"}], "timestampUsec": "1513747858834872", "comments": [], "summary": {"content": "<p>Speech enhancement (SE) aims to reduce noise in speech signals. Most SE \ntechniques focus only on addressing audio information. In this work, inspired \nby multimodal learning, which utilizes data from different modalities, and the \nrecent success of convolutional neural networks (CNNs) in SE, we propose an \naudio-visual deep CNNs (AVDCNN) SE model, which incorporates audio and visual \nstreams into a unified network model. We also propose a multi-task learning \nframework for reconstructing audio and visual signals at the output layer. \nPrecisely speaking, the proposed AVDCNN model is structured as an audio-visual \nencoder-decoder network, in which audio and visual data are first processed \nusing individual CNNs, and then fused into a joint network to generate enhanced \nspeech (the primary task) and reconstructed images (the secondary task) at the \noutput layer. The model is trained in an end-to-end manner, and parameters are \njointly learned through back-propagation. We evaluate enhanced speech using \nfive instrumental criteria. Results show that the AVDCNN model yields a notably \nsuperior performance compared with an audio-only CNN-based SE model and two \nconventional SE approaches, confirming the effectiveness of integrating visual \ninformation into the SE process. In addition, the AVDCNN model also outperforms \nan existing audio-visual SE model, confirming its capability of effectively \ncombining audio and visual information in SE. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90c0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.10893"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Stephan Cl&#xe9;men&#xe7;on, Anna Korba, Eric Sibony", "title": "Ranking Median Regression: Learning to Order through Local Consensus. (arXiv:1711.00070v2 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.00070", "type": "text/html"}], "timestampUsec": "1513747858834871", "comments": [], "summary": {"content": "<p>This article is devoted to the problem of predicting the value taken by a \nrandom permutation $\\Sigma$, describing the preferences of an individual over a \nset of numbered items $\\{1,\\; \\ldots,\\; n\\}$ say, based on the observation of \nan input/explanatory r.v. $X$ e.g. characteristics of the individual), when \nerror is measured by the Kendall $\\tau$ distance. In the probabilistic \nformulation of the 'Learning to Order' problem we propose, which extends the \nframework for statistical Kemeny ranking aggregation developped in \n\\citet{CKS17}, this boils down to recovering conditional Kemeny medians of \n$\\Sigma$ given $X$ from i.i.d. training examples $(X_1, \\Sigma_1),\\; \\ldots,\\; \n(X_N, \\Sigma_N)$. For this reason, this statistical learning problem is \nreferred to as \\textit{ranking median regression} here. Our contribution is \ntwofold. We first propose a probabilistic theory of ranking median regression: \nthe set of optimal elements is characterized, the performance of empirical risk \nminimizers is investigated in this context and situations where fast learning \nrates can be achieved are also exhibited. Next we introduce the concept of \nlocal consensus/median, in order to derive efficient methods for ranking median \nregression. The major advantage of this local learning approach lies in its \nclose connection with the widely studied Kemeny aggregation problem. From an \nalgorithmic perspective, this permits to build predictive rules for ranking \nmedian regression by implementing efficient techniques for (approximate) Kemeny \nmedian computations at a local level in a tractable manner. In particular, \nversions of $k$-nearest neighbor and tree-based methods, tailored to ranking \nmedian regression, are investigated. Accuracy of piecewise constant ranking \nmedian regression rules is studied under a specific smoothness assumption for \n$\\Sigma$'s conditional distribution given $X$. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90c2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.00070"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Andreas Svensson, Dave Zachariah, Thomas B. Sch&#xf6;n", "title": "How consistent is my model with the data? Information-Theoretic Model Check. (arXiv:1712.02675v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02675", "type": "text/html"}], "timestampUsec": "1513747858834870", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450509590a8\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450509590a8&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The choice of model class is fundamental in statistical learning and system \nidentification, no matter whether the class is derived from physical principles \nor is a generic black-box. We develop a method to evaluate the specified model \nclass by assessing its capability of reproducing data that is similar to the \nobserved data record. This model check is based on the information-theoretic \nproperties of models viewed as data generators and is applicable to e.g. \nsequential data and nonlinear dynamical models. The method can be understood as \na specific two-sided posterior predictive test. We apply the \ninformation-theoretic model check to both synthetic and real data and compare \nit with a classical whiteness test. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02675"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Danyang Sun, Tongzheng Ren, Chongxun Li, Jun Zhu, Hang Su", "title": "Learning to Write Stylized Chinese Characters by Reading a Handful of Examples. (arXiv:1712.06424v1 [cs.CV] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1712.06424", "type": "text/html"}], "timestampUsec": "1513747858834869", "comments": [], "summary": {"content": "<p>Automatically writing stylized Chinese characters is an attractive yet \nchallenging task due to its wide applicabilities. In this paper, we propose a \nnovel framework named Style-Aware Variational Auto-Encoder (SA-VAE) to flexibly \ngenerate Chinese characters. Specifically, we propose to capture the different \ncharacteristics of a Chinese character by disentangling the latent features \ninto content-related and style-related components. Considering of the complex \nshapes and structures, we incorporate the structure information as prior \nknowledge into our framework to guide the generation. Our framework shows a \npowerful one-shot/low-shot generalization ability by inferring the style \ncomponent given a character with unseen style. To the best of our knowledge, \nthis is the first attempt to learn to write new-style Chinese characters by \nobserving only one or a few examples. Extensive experiments demonstrate its \neffectiveness in generating different stylized Chinese characters by fusing the \nfeature vectors corresponding to different contents and styles, which is of \nsignificant importance in real-world applications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513747858835", "annotations": [], "published": 1513747859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aed90ca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06424"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tom&#xe1;&#x161; Ko&#x10d;isk&#xfd;, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G&#xe1;bor Melis, Edward Grefenstette", "title": "The NarrativeQA Reading Comprehension Challenge. (arXiv:1712.07040v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.07040", "type": "text/html"}], "timestampUsec": "1513746930967327", "comments": [], "summary": {"content": "<p>Reading comprehension (RC)---in contrast to information retrieval---requires \nintegrating information and reasoning about events, entities, and their \nrelations across a full document. Question answering is conventionally used to \nassess RC ability, in both artificial agents and children learning to read. \nHowever, existing RC datasets and tasks are dominated by questions that can be \nsolved by selecting answers using superficial information (e.g., local context \nsimilarity or global term frequency); they thus fail to test for the essential \nintegrative aspect of RC. To encourage progress on deeper comprehension of \nlanguage, we present a new dataset and set of tasks in which the reader must \nanswer questions about stories by reading entire books or movie scripts. These \ntasks are designed so that successfully answering their questions requires \nunderstanding the underlying narrative rather than relying on shallow pattern \nmatching or salience. We show that although humans solve the tasks easily, \nstandard RC models struggle on the tasks presented here. We provide an analysis \nof the dataset and the challenges it presents. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda18", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07040"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lars Eidnes, Arild N&#xf8;kland", "title": "Shifting Mean Activation Towards Zero with Bipolar Activation Functions. (arXiv:1709.04054v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04054", "type": "text/html"}], "timestampUsec": "1513746930967326", "comments": [], "summary": {"content": "<p>We propose a simple extension to the ReLU-family of activation functions that \nallows them to shift the mean activation across a layer towards zero. Combined \nwith proper weight initialization, this alleviates the need for normalization \nlayers. We explore the training of deep vanilla recurrent neural networks \n(RNNs) with up to 144 layers, and show that bipolar activation functions help \nlearning in this setting. On the Penn Treebank and Text8 language modeling \ntasks we obtain competitive results, improving on the best reported results for \nnon-gated networks. In experiments with convolutional neural networks without \nbatch normalization, we find that bipolar activations produce a faster drop in \ntraining error, and results in a lower test error on the CIFAR-10 \nclassification task. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda26", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04054"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Boris Chidlovskii", "title": "Mining Smart Card Data for Travelers' Mini Activities. (arXiv:1712.06935v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.06935", "type": "text/html"}], "timestampUsec": "1513746930967325", "comments": [], "summary": {"content": "<p>In the context of public transport modeling and simulation, we address the \nproblem of mismatch between simulated transit trips and observed ones. We point \nto the weakness of the current travel demand modeling process; the trips it \ngenerates are over-optimistic and do not reflect the real passenger choices. We \nintroduce the notion of mini activities the travelers do during the trips; they \ncan explain the deviation of simulated trips from the observed trips. We \npropose to mine the smart card data to extract the mini activities. We develop \na technique to integrate them in the generated trips and learn such an \nintegration from two available sources, the trip history and trip planner \nrecommendations. For an input travel demand, we build a Markov chain over the \ntrip collection and apply the Monte Carlo Markov Chain algorithm to integrate \nmini activities in such a way that the selected characteristics converge to the \ndesired distributions. We test our method in different settings on the \npassenger trip collection of Nancy, France. We report experimental results \ndemonstrating a very important mismatch reduction. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda2e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06935"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rasoul Kaljahi, Jennifer Foster", "title": "Any-gram Kernels for Sentence Classification: A Sentiment Analysis Case Study. (arXiv:1712.07004v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.07004", "type": "text/html"}], "timestampUsec": "1513746930967324", "comments": [], "summary": {"content": "<p>Any-gram kernels are a flexible and efficient way to employ bag-of-n-gram \nfeatures when learning from textual data. They are also compatible with the use \nof word embeddings so that word similarities can be accounted for. While the \noriginal any-gram kernels are implemented on top of tree kernels, we propose a \nnew approach which is independent of tree kernels and is more efficient. We \nalso propose a more effective way to make use of word embeddings than the \noriginal any-gram formulation. When applied to the task of sentiment \nclassification, our new formulation achieves significantly better performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda34", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07004"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Upol Ehsan, Brent Harrison, Larry Chan, Mark O. Riedl", "title": "Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations. (arXiv:1702.07826v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1702.07826", "type": "text/html"}], "timestampUsec": "1513746930967323", "comments": [], "summary": {"content": "<p>We introduce AI rationalization, an approach for generating explanations of \nautonomous system behavior as if a human had performed the behavior. We \ndescribe a rationalization technique that uses neural machine translation to \ntranslate internal state-action representations of an autonomous agent into \nnatural language. We evaluate our technique in the Frogger game environment, \ntraining an autonomous game playing agent to rationalize its action choices \nusing natural language. A natural language training corpus is collected from \nhuman players thinking out loud as they play the game. We motivate the use of \nrationalization as an approach to explanation generation and show the results \nof two experiments evaluating the effectiveness of rationalization. Results of \nthese evaluations show that neural machine translation is able to accurately \ngenerate rationalizations that describe agent behavior, and that \nrationalizations are more satisfying to humans than other alternative methods \nof explanation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda39", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1702.07826"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ardhendu Tripathy, Ye Wang, Prakash Ishwar", "title": "Privacy-Preserving Adversarial Networks. (arXiv:1712.07008v1 [cs.IT])", "alternate": [{"href": "http://arxiv.org/abs/1712.07008", "type": "text/html"}], "timestampUsec": "1513746930967322", "comments": [], "summary": {"content": "<p>We propose a data-driven framework for optimizing privacy-preserving data \nrelease mechanisms toward the information-theoretically optimal tradeoff \nbetween minimizing distortion of useful data and concealing sensitive \ninformation. Our approach employs adversarially-trained neural networks to \nimplement randomized mechanisms and to perform a variational approximation of \nmutual information privacy. We empirically validate our Privacy-Preserving \nAdversarial Networks (PPAN) framework with experiments conducted on discrete \nand continuous synthetic data, as well as the MNIST handwritten digits dataset. \nWith the synthetic data, we find that our model-agnostic PPAN approach achieves \ntradeoff points very close to the optimal tradeoffs that are \nanalytically-derived from model knowledge. In experiments with the MNIST data, \nwe visually demonstrate a learned tradeoff between minimizing the pixel-level \ndistortion versus concealing the written digit. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda43", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07008"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiaoyong Yuan, Pan He, Qile Zhu, Rajendra Rana Bhat, Xiaolin Li", "title": "Adversarial Examples: Attacks and Defenses for Deep Learning. (arXiv:1712.07107v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.07107", "type": "text/html"}], "timestampUsec": "1513746930967321", "comments": [], "summary": {"content": "<p>With rapid progress and great successes in a wide spectrum of applications, \ndeep learning is being applied in many safety-critical environments. However, \ndeep neural networks have been recently found vulnerable to well-designed input \nsamples, called \\textit{adversarial examples}. Adversarial examples are \nimperceptible to human but can easily fool deep neural networks in the \ntesting/deploying stage. The vulnerability to adversarial examples becomes one \nof the major risks for applying deep neural networks in safety-critical \nscenarios. Therefore, the attacks and defenses on adversarial examples draw \ngreat attention. \n</p> \n<p>In this paper, we review recent findings on adversarial examples against deep \nneural networks, summarize the methods for generating adversarial examples, and \npropose a taxonomy of these methods. Under the taxonomy, applications and \ncountermeasures for adversarial examples are investigated. We further elaborate \non adversarial examples and explore the challenges and the potential solutions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07107"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin", "title": "Query-Efficient Black-box Adversarial Examples. (arXiv:1712.07113v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.07113", "type": "text/html"}], "timestampUsec": "1513746930967320", "comments": [], "summary": {"content": "<p>Current neural network-based image classifiers are susceptible to adversarial \nexamples, even in the black-box setting, where the attacker is limited to query \naccess without access to gradients. Previous methods --- substitute networks \nand coordinate-based finite-difference methods --- are either unreliable or \nquery-inefficient, making these methods impractical for certain problems. \n</p> \n<p>We introduce a new method for reliably generating adversarial examples under \nmore restricted, practical black-box threat models. First, we apply natural \nevolution strategies to perform black-box attacks using two to three orders of \nmagnitude fewer queries than previous methods. Second, we introduce a new \nalgorithm to perform targeted adversarial attacks in the partial-information \nsetting, where the attacker only has access to a limited number of target \nclasses. Using these techniques, we successfully perform the first targeted \nadversarial attack against a commercially deployed machine learning system, the \nGoogle Cloud Vision API, in the partial information setting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda53", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.07113"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Na Lei, Kehua Su, Li Cui, Shing-Tung Yau, David Xianfeng Gu", "title": "A Geometric View of Optimal Transportation and Generative Model. (arXiv:1710.05488v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.05488", "type": "text/html"}], "timestampUsec": "1513746930967319", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450509592d3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450509592d3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this work, we show the intrinsic relations between optimal transportation \nand convex geometry, especially the variational approach to solve Alexandrov \nproblem: constructing a convex polytope with prescribed face normals and \nvolumes. This leads to a geometric interpretation to generative models, and \nleads to a novel framework for generative models. By using the optimal \ntransportation view of GAN model, we show that the discriminator computes the \nKantorovich potential, the generator calculates the transportation map. For a \nlarge class of transportation costs, the Kantorovich potential can give the \noptimal transportation map by a close-form formula. Therefore, it is sufficient \nto solely optimize the discriminator. This shows the adversarial competition \ncan be avoided, and the computational architecture can be simplified. \nPreliminary experimental results show the geometric method outperforms WGAN for \napproximating probability measures with multiple clusters in low dimensional \nspace. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda60", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.05488"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, Bernhard Schoelkopf", "title": "Wasserstein Auto-Encoders. (arXiv:1711.01558v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.01558", "type": "text/html"}], "timestampUsec": "1513746930967318", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505099d552\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505099d552&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building \na generative model of the data distribution. WAE minimizes a penalized form of \nthe Wasserstein distance between the model distribution and the target \ndistribution, which leads to a different regularizer than the one used by the \nVariational Auto-Encoder (VAE). This regularizer encourages the encoded \ntraining distribution to match the prior. We compare our algorithm with several \nother techniques and show that it is a generalization of adversarial \nauto-encoders (AAE). Our experiments show that WAE shares many of the \nproperties of VAEs (stable training, encoder-decoder architecture, nice latent \nmanifold structure) while generating samples of better quality, as measured by \nthe FID score. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513746930967", "annotations": [], "published": 1513746931, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034aebda67", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.01558"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Chih-Cheng Chang, Pin-Chun Chen, Teyuh Chou, I-Ting Wang, Boris Hudec, Che-Chia Chang, Chia-Ming Tsai, Tian-Sheuan Chang, Tuo-Hung Hou", "title": "Mitigating Asymmetric Nonlinear Weight Update Effects in Hardware Neural Network based on Analog Resistive Synapse. (arXiv:1712.05895v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.05895", "type": "text/html"}], "timestampUsec": "1513676967346947", "comments": [], "summary": {"content": "<p>Asymmetric nonlinear weight update is considered as one of the major \nobstacles for realizing hardware neural networks based on analog resistive \nsynapses because it significantly compromises the online training capability. \nThis paper provides new solutions to this critical issue through \nco-optimization with the hardware-applicable deep-learning algorithms. New \ninsights on engineering activation functions and a threshold weight update \nscheme effectively suppress the undesirable training noise induced by \ninaccurate weight update. We successfully trained a two-layer perceptron \nnetwork online and improved the classification accuracy of MNIST handwritten \ndigit dataset to 87.8/94.8% by using 6-bit/8-bit analog synapses, respectively, \nwith extremely high asymmetric nonlinearity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e648", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05895"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Han Xiao", "title": "NDT: Neual Decision Tree Towards Fully Functioned Neural Graph. (arXiv:1712.05934v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.05934", "type": "text/html"}], "timestampUsec": "1513676967346946", "comments": [], "summary": {"content": "<p>Though traditional algorithms could be embedded into neural architectures \nwith the proposed principle of \\cite{xiao2017hungarian}, the variables that \nonly occur in the condition of branch could not be updated as a special case. \nTo tackle this issue, we multiply the conditioned branches with Dirac symbol \n(i.e. $\\mathbf{1}_{x&gt;0}$), then approximate Dirac symbol with the continuous \nfunctions (e.g. $1 - e^{-\\alpha|x|}$). In this way, the gradients of \ncondition-specific variables could be worked out in the back-propagation \nprocess, approximately, making a fully functioned neural graph. Within our \nnovel principle, we propose the neural decision tree \\textbf{(NDT)}, which \ntakes simplified neural networks as decision function in each branch and \nemploys complex neural networks to generate the output in each leaf. Extensive \nexperiments verify our theoretical analysis and demonstrate the effectiveness \nof our model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e659", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05934"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Assaf Shocher, Nadav Cohen, Michal Irani", "title": "\"Zero-Shot\" Super-Resolution using Deep Internal Learning. (arXiv:1712.06087v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.06087", "type": "text/html"}], "timestampUsec": "1513676967346945", "comments": [], "summary": {"content": "<p>Deep Learning has led to a dramatic leap in Super-Resolution (SR) performance \nin the past few years. However, being supervised, these SR methods are \nrestricted to specific training data, where the acquisition of the \nlow-resolution (LR) images from their high-resolution (HR) counterparts is \npredetermined (e.g., bicubic downscaling), without any distracting artifacts \n(e.g., sensor noise, image compression, non-ideal PSF, etc). Real LR images, \nhowever, rarely obey these restrictions, resulting in poor SR results by SotA \n(State of the Art) methods. In this paper we introduce \"Zero-Shot\" SR, which \nexploits the power of Deep Learning, but does not rely on prior training. We \nexploit the internal recurrence of information inside a single image, and train \na small image-specific CNN at test time, on examples extracted solely from the \ninput image itself. As such, it can adapt itself to different settings per \nimage. This allows to perform SR of real old photos, noisy images, biological \ndata, and other images where the acquisition process is unknown or non-ideal. \nOn such images, our method outperforms SotA CNN-based SR methods, as well as \nprevious unsupervised SR methods. To the best of our knowledge, this is the \nfirst unsupervised CNN-based SR method. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e66b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06087"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Rudy Raymond, Takayuki Osogami, Sakyasingha Dasgupta", "title": "Dynamic Boltzmann Machines for Second Order Moments and Generalized Gaussian Distributions. (arXiv:1712.06132v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06132", "type": "text/html"}], "timestampUsec": "1513676967346944", "comments": [], "summary": {"content": "<p>Dynamic Boltzmann Machine (DyBM) has been shown highly efficient to predict \ntime-series data. Gaussian DyBM is a DyBM that assumes the predicted data is \ngenerated by a Gaussian distribution whose first-order moment (mean) \ndynamically changes over time but its second-order moment (variance) is fixed. \nHowever, in many financial applications, the assumption is quite limiting in \ntwo aspects. First, even when the data follows a Gaussian distribution, its \nvariance may change over time. Such variance is also related to important \ntemporal economic indicators such as the market volatility. Second, financial \ntime-series data often requires learning datasets generated by the generalized \nGaussian distribution with an additional shape parameter that is important to \napproximate heavy-tailed distributions. Addressing those aspects, we show how \nto extend DyBM that results in significant performance improvement in \npredicting financial time-series data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e679", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06132"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sheng Xin Zhang, Wing Shing Chan, Zi Kang Peng, Shao Yong Zheng, Kit Sang Tang", "title": "Selective-Candidate Framework with Similarity Selection Rule for Evolutionary Optimization. (arXiv:1712.06338v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.06338", "type": "text/html"}], "timestampUsec": "1513676967346943", "comments": [], "summary": {"content": "<p>This paper proposes to resolve limitations of the traditional \none-reproduction (OR) framework which produces only one candidate in a single \nreproduction procedure. A selective-candidate framework with similarity \nselection rule (SCSS) is suggested to make possible, a selective direction of \nsearch. In the SCSS framework, M (M &gt; 1) candidates are generated from each \ncurrent solution by independently conducting the reproduction procedure M \ntimes. The winner is then determined by employing a similarity selection rule. \nTo maintain balanced exploitation and exploration capabilities, an efficient \nsimilarity selection rule based on the Euclidian distances between each of the \nM candidates and the corresponding current solution is proposed. The SCSS \nframework can be easily applied to any evolutionary algorithms or swarm \nintelligences. Experiments conducted with 60 benchmark functions show the \nsuperiority of SCSS over OR in three classic, four state-of-the-art and four \nup-to-date algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e68a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06338"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Brian Kenji Iwana, Seiichi Uchida", "title": "Dynamic Weight Alignment for Convolutional Neural Networks. (arXiv:1712.06530v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.06530", "type": "text/html"}], "timestampUsec": "1513676967346942", "comments": [], "summary": {"content": "<p>In this paper, we propose a method of improving Convolutional Neural Networks \n(CNN) by determining the optimal alignment of weights and inputs using dynamic \nprogramming. Conventional CNNs convolve learnable shared weights, or filters, \nacross the input data. The filters use a linear matching of weights to inputs \nusing an inner product between the filter and a window of the input. However, \nit is possible that there exists a more optimal alignment of weights. Thus, we \npropose the use of Dynamic Time Warping (DTW) to dynamically align the weights \nto optimized input elements. This dynamic alignment is useful for time series \nrecognition due to the complexities of temporal relations and temporal \ndistortions. We demonstrate the effectiveness of the proposed architecture on \nthe Unipen online handwritten digit and character datasets, the UCI Spoken \nArabic Digit dataset, and the UCI Activities of Daily Life dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e695", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06530"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Noah Golowich, Alexander Rakhlin, Ohad Shamir", "title": "Size-Independent Sample Complexity of Neural Networks. (arXiv:1712.06541v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.06541", "type": "text/html"}], "timestampUsec": "1513676967346941", "comments": [], "summary": {"content": "<p>We study the sample complexity of learning neural networks, by providing new \nbounds on their Rademacher complexity assuming norm constraints on the \nparameter matrix of each layer. Compared to previous work, these complexity \nbounds have improved dependence on the network depth, and under some additional \nassumptions, are fully independent of the network size (both depth and width). \nThese results are derived using some novel techniques, which may be of \nindependent interest. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e6a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06541"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Xingwen Zhang, Jeff Clune, Kenneth O. Stanley", "title": "On the Relationship Between the OpenAI Evolution Strategy and Stochastic Gradient Descent. (arXiv:1712.06564v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.06564", "type": "text/html"}], "timestampUsec": "1513676967346940", "comments": [], "summary": {"content": "<p>Because stochastic gradient descent (SGD) has shown promise optimizing neural \nnetworks with millions of parameters and few if any alternatives are known to \nexist, it has moved to the heart of leading approaches to reinforcement \nlearning (RL). For that reason, the recent result from OpenAI showing that a \nparticular kind of evolution strategy (ES) can rival the performance of \nSGD-based deep RL methods with large neural networks provoked surprise. This \nresult is difficult to interpret in part because of the lingering ambiguity on \nhow ES actually relates to SGD. The aim of this paper is to significantly \nreduce this ambiguity through a series of MNIST-based experiments designed to \nuncover their relationship. As a simple supervised problem without domain noise \n(unlike in most RL), MNIST makes it possible (1) to measure the correlation \nbetween gradients computed by ES and SGD and (2) then to develop an SGD-based \nproxy that accurately predicts the performance of different ES population \nsizes. These innovations give a new level of insight into the real capabilities \nof ES, and lead also to some unconventional means for applying ES to supervised \nproblems that shed further light on its differences from SGD. Incorporating \nthese lessons, the paper concludes by demonstrating that ES can achieve 99% \naccuracy on MNIST, a number higher than any previously published result for any \nevolutionary method. While not by any means suggesting that ES should \nsubstitute for SGD in supervised learning, the suite of experiments herein \nenables more informed decisions on the application of ES within RL and other \nparadigms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e6b3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06564"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Joel Lehman, Jay Chen, Jeff Clune, Kenneth O. Stanley", "title": "ES Is More Than Just a Traditional Finite-Difference Approximator. (arXiv:1712.06568v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.06568", "type": "text/html"}], "timestampUsec": "1513676967346939", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505099dba9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505099dba9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>An evolution strategy (ES) variant recently attracted significant attention \ndue to its surprisingly good performance at optimizing neural networks in \nchallenging deep reinforcement learning domains. It searches directly in the \nparameter space of neural networks by generating perturbations to the current \nset of parameters, checking their performance, and moving in the direction of \nhigher reward. The resemblance of this algorithm to a traditional \nfinite-difference approximation of the reward gradient in parameter space \nnaturally leads to the assumption that it is just that. However, this \nassumption is incorrect. The aim of this paper is to definitively demonstrate \nthis point empirically. ES is a gradient approximator, but optimizes for a \ndifferent gradient than just reward (especially when the magnitude of candidate \nperturbations is high). Instead, it optimizes for the average reward of the \nentire population, often also promoting parameters that are robust to \nperturbation. This difference can channel ES into significantly different areas \nof the search space than gradient descent in parameter space, and also \nconsequently to networks with significantly different properties. This unique \nrobustness-seeking property, and its consequences for optimization, are \ndemonstrated in several domains. They include humanoid locomotion, where \nnetworks from policy gradient-based reinforcement learning are far less robust \nto parameter perturbation than ES-based policies that solve the same task. \nWhile the implications of such robustness and robustness-seeking remain open to \nfurther study, the main contribution of this work is to highlight that such \ndifferences indeed exist and deserve attention. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e6bf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06568"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Aaron Tuor, Samuel Kaplan, Brian Hutchinson, Nicole Nichols, Sean Robinson", "title": "Deep Learning for Unsupervised Insider Threat Detection in Structured Cybersecurity Data Streams. (arXiv:1710.00811v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.00811", "type": "text/html"}], "timestampUsec": "1513676967346938", "comments": [], "summary": {"content": "<p>Analysis of an organization's computer network activity is a key component of \nearly detection and mitigation of insider threat, a growing concern for many \norganizations. Raw system logs are a prototypical example of streaming data \nthat can quickly scale beyond the cognitive power of a human analyst. As a \nprospective filter for the human analyst, we present an online unsupervised \ndeep learning approach to detect anomalous network activity from system logs in \nreal time. Our models decompose anomaly scores into the contributions of \nindividual user behavior features for increased interpretability to aid \nanalysts reviewing potential cases of insider threat. Using the CERT Insider \nThreat Dataset v6.2 and threat detection recall as our performance metric, our \nnovel deep and recurrent neural network models outperform Principal Component \nAnalysis, Support Vector Machine and Isolation Forest based anomaly detection \nbaselines. For our best model, the events labeled as insider threat activity in \nour dataset had an average anomaly score in the 95.53 percentile, demonstrating \nour approach's potential to greatly reduce analyst workloads. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e6d0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.00811"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Maithra Raghu, Alex Irpan, Jacob Andreas, Robert Kleinberg, Quoc V. Le, Jon Kleinberg", "title": "Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?. (arXiv:1711.02301v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.02301", "type": "text/html"}], "timestampUsec": "1513676967346937", "comments": [], "summary": {"content": "<p>Deep reinforcement learning has achieved many recent successes, but our \nunderstanding of its strengths and limitations is hampered by the lack of rich \nenvironments in which we can fully characterize optimal behavior, and \ncorrespondingly diagnose individual actions against such a characterization. \nHere we consider a family of combinatorial games, arising from work of Erdos, \nSelfridge, and Spencer, and we propose their use as environments for evaluating \nand comparing different approaches to reinforcement learning. These games have \na number of appealing features: they are challenging for current learning \napproaches, but they form (i) a low-dimensional, simply parametrized \nenvironment where (ii) there is a linear closed form solution for optimal \nbehavior from any state, and (iii) the difficulty of the game can be tuned by \nchanging environment parameters in an interpretable way. We use these \nErdos-Selfridge-Spencer games not only to compare different algorithms, but \nalso to compare approaches based on supervised and reinforcement learning, to \nanalyze the power of multi-agent approaches in improving performance, and to \nevaluate generalization to environments outside the training set. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e6dc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.02301"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Stuart Armstrong, S&#xf6;ren Mindermann", "title": "Impossibility of deducing preferences and rationality from human policy. (arXiv:1712.05812v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.05812", "type": "text/html"}], "timestampUsec": "1513676967346936", "comments": [], "summary": {"content": "<p>Inverse reinforcement learning (IRL) attempts to infer human rewards or \npreferences from observed behavior. However, human planning systematically \ndeviates from rationality. Though there has been some IRL work which assumes \nhumans are noisily rational, there has been little analysis of the general \nproblem of inferring the reward of a human of unknown rationality. The observed \nbehavior can, in principle, be decomposed into two composed into two \ncomponents: a reward function and a planning algorithm that maps reward \nfunction to policy. Both of these variables have to be inferred from behaviour. \nThis paper presents a \"No Free Lunch\" theorem in this area, showing that, \nwithout making `normative' assumptions beyond the data, nothing about the human \nreward function can be deduced from human behaviour. Unlike most No Free Lunch \ntheorems, this cannot be alleviated by regularising with simplicity \nassumptions. The simplest hypotheses are generally degenerate. The paper will \nthen sketch how one might begin to use normative assumptions to get around the \nproblem, without which solving the general IRL problem is impossible. The \nreward function-planning algorithm formalism can also be used to encode what it \nmeans for an agent to manipulate or override human preferences. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e6e3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05812"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Ion Stoica, Dawn Song, Raluca Ada Popa, David Patterson, Michael W. Mahoney, Randy Katz, Anthony D. Joseph, Michael Jordan, Joseph M. Hellerstein, Joseph E. Gonzalez, Ken Goldberg, Ali Ghodsi, David Culler, Pieter Abbeel", "title": "A Berkeley View of Systems Challenges for AI. (arXiv:1712.05855v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.05855", "type": "text/html"}], "timestampUsec": "1513676967346935", "comments": [], "summary": {"content": "<p>With the increasing commoditization of computer vision, speech recognition \nand machine translation systems and the widespread deployment of learning-based \nback-end technologies such as digital advertising and intelligent \ninfrastructures, AI (Artificial Intelligence) has moved from research labs to \nproduction. These changes have been made possible by unprecedented levels of \ndata and computation, by methodological advances in machine learning, by \ninnovations in systems software and architectures, and by the broad \naccessibility of these technologies. \n</p> \n<p>The next generation of AI systems promises to accelerate these developments \nand increasingly impact our lives via frequent interactions and making (often \nmission-critical) decisions on our behalf, often in highly personalized \ncontexts. Realizing this promise, however, raises daunting challenges. In \nparticular, we need AI systems that make timely and safe decisions in \nunpredictable environments, that are robust against sophisticated adversaries, \nand that can process ever increasing amounts of data across organizations and \nindividuals without compromising confidentiality. These challenges will be \nexacerbated by the end of the Moore's Law, which will constrain the amount of \ndata these technologies can store and process. In this paper, we propose \nseveral open research directions in systems, architectures, and security that \ncan address these challenges and help unlock AI's potential to improve lives \nand society. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e6f1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05855"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, William Paul, Michael I. Jordan, Ion Stoica", "title": "Ray: A Distributed Framework for Emerging AI Applications. (arXiv:1712.05889v1 [cs.DC])", "alternate": [{"href": "http://arxiv.org/abs/1712.05889", "type": "text/html"}], "timestampUsec": "1513676967346934", "comments": [], "summary": {"content": "<p>The next generation of AI applications will continuously interact with the \nenvironment and learn from these interactions. These applications impose new \nand demanding systems requirements, both in terms of performance and \nflexibility. In this paper, we consider these requirements and present Ray---a \ndistributed system to address them. Ray implements a dynamic task graph \ncomputation model that supports both the task-parallel and the actor \nprogramming models. To meet the performance requirements of AI applications, we \npropose an architecture that logically centralizes the system's control state \nusing a sharded storage system and a novel bottom-up distributed scheduler. In \nour experiments, we demonstrate sub-millisecond remote task latencies and \nlinear throughput scaling beyond 1.8 million tasks per second. We empirically \nvalidate that Ray speeds up challenging benchmarks and serves as both a natural \nand performant fit for an emerging class of reinforcement learning applications \nand algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e6f5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05889"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Bhavya Kailkhura, Jayaraman J. Thiagarajan, Charvi Rastogi, Pramod K. Varshney, Peer-Timo Bremer", "title": "A Spectral Approach for the Design of Experiments: Design, Analysis and Algorithms. (arXiv:1712.06028v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06028", "type": "text/html"}], "timestampUsec": "1513676967346933", "comments": [], "summary": {"content": "<p>This paper proposes a new approach to construct high quality space-filling \nsample designs. First, we propose a novel technique to quantify the \nspace-filling property and optimally trade-off uniformity and randomness in \nsample designs in arbitrary dimensions. Second, we connect the proposed metric \n(defined in the spatial domain) to the objective measure of the design \nperformance (defined in the spectral domain). This connection serves as an \nanalytic framework for evaluating the qualitative properties of space-filling \ndesigns in general. Using the theoretical insights provided by this \nspatial-spectral analysis, we derive the notion of optimal space-filling \ndesigns, which we refer to as space-filling spectral designs. Third, we propose \nan efficient estimator to evaluate the space-filling properties of sample \ndesigns in arbitrary dimensions and use it to develop an optimization framework \nto generate high quality space-filling designs. Finally, we carry out a \ndetailed performance comparison on two different applications in 2 to 6 \ndimensions: a) image reconstruction and b) surrogate modeling on several \nbenchmark optimization functions and an inertial confinement fusion (ICF) \nsimulation code. We demonstrate that the propose spectral designs significantly \noutperform existing approaches especially in high dimensions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e700", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06028"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yeo Hun Yoon, Shujaat Khan, Jaeyoung Huh, Jong Chul Ye", "title": "Deep Learning in RF Sub-sampled B-mode Ultrasound Imaging. (arXiv:1712.06096v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.06096", "type": "text/html"}], "timestampUsec": "1513676967346932", "comments": [], "summary": {"content": "<p>In portable, three dimensional, and ultra-fast ultrasound (US) imaging \nsystems, there is an increasing need to reconstruct high quality images from a \nlimited number of RF data from receiver (Rx) or scan-line (SC) sub-sampling. \nHowever, due to the severe side lobe artifacts from RF sub-sampling, the \nstandard beam-former often produces blurry images with less contrast that are \nnot suitable for diagnostic purpose. To address this problem, some researchers \nhave studied compressed sensing (CS) to exploit the sparsity of the image or RF \ndata in some domains. However, the existing CS approaches require either \nhardware changes or computationally expensive algorithms. To overcome these \nlimitations, here we propose a novel deep learning approach that directly \ninterpolates the missing RF data by utilizing redundancy in the Rx-SC plane. In \nparticular, the network design principle derives from a novel interpretation of \nthe deep neural network as a cascaded convolution framelets that learns the \ndata-driven bases for Hankel matrix decomposition. Our extensive experimental \nresults from sub-sampled RF data from a real US system confirmed that the \nproposed method can effectively reduce the data rate without sacrificing the \nimage quality. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e715", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06096"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jin-Hwa Kim, Byoung-Tak Zhang", "title": "Visual Explanations from Hadamard Product in Multimodal Deep Networks. (arXiv:1712.06228v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.06228", "type": "text/html"}], "timestampUsec": "1513676967346931", "comments": [], "summary": {"content": "<p>The visual explanation of learned representation of models helps to \nunderstand the fundamentals of learning. The attentional models of previous \nworks used to visualize the attended regions over an image or text using their \nlearned weights to confirm their intended mechanism. Kim et al. (2016) show \nthat the Hadamard product in multimodal deep networks, which is well-known for \nthe joint function of visual question answering tasks, implicitly performs an \nattentional mechanism for visual inputs. In this work, we extend their work to \nshow that the Hadamard product in multimodal deep networks performs not only \nfor visual inputs but also for textual inputs simultaneously using the proposed \ngradient-based visualization technique. The attentional effect of Hadamard \nproduct is visualized for both visual and textual inputs by analyzing the two \ninputs and an output of the Hadamard product with the proposed method and \ncompared with learned attentional weights of a visual question answering model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e726", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06228"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Jose Oramas, Kaili Wang, Tinne Tuytelaars", "title": "Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks. (arXiv:1712.06302v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.06302", "type": "text/html"}], "timestampUsec": "1513676967346930", "comments": [], "summary": {"content": "<p>Learning-based representations have become the defacto means to address \ncomputer vision tasks. Despite their massive adoption, the amount of work \naiming at understanding the internal representations learned by these models is \nrather limited. Existing methods aimed at model interpretation either require \nexhaustive manual inspection of visualizations, or link internal network \nactivations with external \"possibly useful\" annotated concepts. We propose an \nintermediate scheme in which, given a pretrained model, we automatically \nidentify internal features relevant for the set of classes considered by the \nmodel, without requiring additional annotations. We interpret the model through \naverage visualizations of these features. Then, at test time, we explain the \nnetwork prediction by accompanying the predicted class label with supporting \nheatmap visualizations derived from the identified relevant features. In \naddition, we propose a method to address the artifacts introduced by strided \noperations in deconvnet-based visualizations. Our evaluation on the MNIST, \nILSVRC 12 and Fashion 144k datasets quantitatively shows that the proposed \nmethod is able to identify relevant internal features for the classes of \ninterest while improving the quality of the produced visualizations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e736", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06302"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Stuart Armstrong", "title": "`Indifference' methods for managing agent rewards. (arXiv:1712.06365v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.06365", "type": "text/html"}], "timestampUsec": "1513676967346929", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505099ddf1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505099ddf1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Indifference is a class of methods that are used to control a reward based \nagent, by, for example, safely changing their reward or policy, or making the \nagent behave as if a certain outcome could never happen. These methods of \ncontrol work even if the implications of the agent's reward are otherwise not \nfully understood. Though they all come out of similar ideas, indifference \ntechniques can be classified as way of achieving one or more of three distinct \ngoals: rewards dependent on certain events (with no motivation for the agent to \nmanipulate the probability of those events), effective disbelief that an event \nwill ever occur, and seamless transition from one behaviour to another. There \nare five basic methods to achieve these three goals. This paper classifies and \nanalyses these methods on POMDPs (though the methods are highly portable to \nother agent designs), and establishes their uses, strengths, and limitations. \nIt aims to make the tools of indifference generally accessible and usable to \nagent designers. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e73e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06365"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Feng Liu, Yong Shi, Ying Liu", "title": "Three IQs of AI Systems and their Testing Methods. (arXiv:1712.06440v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.06440", "type": "text/html"}], "timestampUsec": "1513676967346928", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450509f0694\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450509f0694&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The rapid development of artificial intelligence has brought the artificial \nintelligence threat theory as well as the problem about how to evaluate the \nintelligence level of intelligent products. Both need to find a quantitative \nmethod to evaluate the intelligence level of intelligence systems, including \nhuman intelligence. Based on the standard intelligence system and the extended \nVon Neumann architecture, this paper proposes General IQ, Service IQ and Value \nIQ evaluation methods for intelligence systems, depending on different \nevaluation purposes. Among them, the General IQ of intelligence systems is to \nanswer the question of whether the artificial intelligence can surpass the \nhuman intelligence, which is reflected in putting the intelligence systems on \nan equal status and conducting the unified evaluation. The Service IQ and Value \nIQ of intelligence systems are used to answer the question of how the \nintelligent products can better serve the human, reflecting the intelligence \nand required cost of each intelligence system as a product in the process of \nserving human. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e74a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06440"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O. Stanley, Jeff Clune", "title": "Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents. (arXiv:1712.06560v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.06560", "type": "text/html"}], "timestampUsec": "1513676967346927", "comments": [], "summary": {"content": "<p>Evolution strategies (ES) are a family of black-box optimization algorithms \nable to train deep neural networks roughly as well as Q-learning and policy \ngradient methods on challenging deep reinforcement learning (RL) problems, but \nare much faster (e.g. hours vs. days) because they parallelize better. However, \nmany RL problems require directed exploration because they have reward \nfunctions that are sparse or deceptive (i.e. contain local optima), and it is \nnot known how to encourage such exploration with ES. Here we show that \nalgorithms that have been invented to promote directed exploration in \nsmall-scale evolved neural networks via populations of exploring agents, \nspecifically novelty search (NS) and quality diversity (QD) algorithms, can be \nhybridized with ES to improve its performance on sparse or deceptive deep RL \ntasks, while retaining scalability. Our experiments confirm that the resultant \nnew algorithms, NS-ES and a version of QD we call NSR-ES, avoid local optima \nencountered by ES to achieve higher performance on tasks ranging from playing \nAtari to simulated robots learning to walk around a deceptive trap. This paper \nthus introduces a family of fast, scalable algorithms for reinforcement \nlearning that are capable of directed exploration. It also adds this new family \nof exploration algorithms to the RL toolbox and raises the interesting \npossibility that analogous algorithms with multiple simultaneous paths of \nexploration might also combine well with existing RL algorithms outside ES. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e754", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06560"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Maxim Naumov", "title": "Parallel Complexity of Forward and Backward Propagation. (arXiv:1712.06577v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.06577", "type": "text/html"}], "timestampUsec": "1513676967346926", "comments": [], "summary": {"content": "<p>We show that the forward and backward propagation can be formulated as a \nsolution of lower and upper triangular systems of equations. For standard \nfeedforward (FNNs) and recurrent neural networks (RNNs) the triangular systems \nare always block bi-diagonal, while for a general computation graph (directed \nacyclic graph) they can have a more complex triangular sparsity pattern. We \ndiscuss direct and iterative parallel algorithms that can be used for their \nsolution and interpreted as different ways of performing model parallelism. \nAlso, we show that for FNNs and RNNs with $k$ layers and $\\tau$ time steps the \nbackward propagation can be performed in parallel in O($\\log k$) and O($\\log k \n\\log \\tau$) steps, respectively. Finally, we outline the generalization of this \ntechnique using Jacobians that potentially allows us to handle arbitrary \nlayers. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e761", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06577"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Supratik Paul, Konstantinos Chatzilygeroudis, Kamil Ciosek, Jean-Baptiste Mouret, Michael A. Osborne, Shimon Whiteson", "title": "Alternating Optimisation and Quadrature for Robust Control. (arXiv:1605.07496v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1605.07496", "type": "text/html"}], "timestampUsec": "1513676967346925", "comments": [], "summary": {"content": "<p>Bayesian optimisation has been successfully applied to a variety of \nreinforcement learning problems. However, the traditional approach for learning \noptimal policies in simulators does not utilise the opportunity to improve \nlearning by adjusting certain environment variables: state features that are \nunobservable and randomly determined by the environment in a physical setting \nbut are controllable in a simulator. This paper considers the problem of \nfinding a robust policy while taking into account the impact of environment \nvariables. We present Alternating Optimisation and Quadrature (ALOQ), which \nuses Bayesian optimisation and Bayesian quadrature to address such settings. \nALOQ is robust to the presence of significant rare events, which may not be \nobservable under random sampling, but play a substantial role in determining \nthe optimal policy. Experimental results across different domains show that \nALOQ can learn more efficiently and robustly than existing methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e775", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1605.07496"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Han Xiao", "title": "KSR: A Semantic Representation of Knowledge Graph within a Novel Unsupervised Paradigm. (arXiv:1608.07685v6 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1608.07685", "type": "text/html"}], "timestampUsec": "1513676967346924", "comments": [], "summary": {"content": "<p>Knowledge representation is a long-history topic in AI, which is very \nimportant. A variety of models have been proposed for knowledge graph \nembedding, which projects symbolic entities and relations into continuous \nvector space. However, most related methods merely focus on the data-fitting of \nknowledge graph, and ignore the interpretable semantic expression. Thus, \ntraditional embedding methods are not friendly for applications that require \nsemantic analysis, such as question answering and entity retrieval. To this \nend, this paper proposes a semantic representation method for knowledge graph \n\\textbf{(KSR)}, which imposes a two-level hierarchical generative process that \nglobally extracts many aspects and then locally assigns a specific category in \neach aspect for every triple. Since both aspects and categories are \nsemantics-relevant, the collection of categories in each aspect is treated as \nthe semantic representation of this triple. Extensive experiments show that our \nmodel outperforms other state-of-the-art baselines substantially. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e787", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1608.07685"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Shai Shalev-Shwartz, Shaked Shammah, Amnon Shashua", "title": "On a Formal Model of Safe and Scalable Self-driving Cars. (arXiv:1708.06374v4 [cs.RO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.06374", "type": "text/html"}], "timestampUsec": "1513676967346923", "comments": [], "summary": {"content": "<p>In recent years, car makers and tech companies have been racing towards self \ndriving cars. It seems that the main parameter in this race is who will have \nthe first car on the road. The goal of this paper is to add to the equation two \nadditional crucial parameters. The first is standardization of safety assurance \n--- what are the minimal requirements that every self-driving car must satisfy, \nand how can we verify these requirements. The second parameter is scalability \n--- engineering solutions that lead to unleashed costs will not scale to \nmillions of cars, which will push interest in this field into a niche academic \ncorner, and drive the entire field into a \"winter of autonomous driving\". In \nthe first part of the paper we propose a white-box, interpretable, mathematical \nmodel for safety assurance, which we call Responsibility-Sensitive Safety \n(RSS). In the second part we describe a design of a system that adheres to our \nsafety assurance requirements and is scalable to millions of cars. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e79e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.06374"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Yujian Li", "title": "Can Machines Think in Radio Language?. (arXiv:1710.02648v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.02648", "type": "text/html"}], "timestampUsec": "1513676967346922", "comments": [], "summary": {"content": "<p>People can think in auditory, visual and tactile forms of language, so can \nmachines principally. But is it possible for them to think in radio language? \nAccording to a first principle presented for general intelligence, i.e. the \nprinciple of language's relativity, the answer may give an exceptional solution \nfor robot astronauts to talk with each other in space exploration. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e7ac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.02648"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "An Bian, Kfir Y. Levy, Andreas Krause, Joachim M. Buhmann", "title": "Continuous DR-submodular Maximization: Structure and Algorithms. (arXiv:1711.02515v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.02515", "type": "text/html"}], "timestampUsec": "1513676967346921", "comments": [], "summary": {"content": "<p>DR-submodular continuous functions are important objectives with wide \nreal-world applications spanning MAP inference in determinantal point processes \n(DPPs), and mean-field inference for probabilistic submodular models, amongst \nothers. DR-submodularity captures a subclass of non-convex functions that \nenables both exact minimization and approximate maximization in polynomial \ntime. \n</p> \n<p>In this work we study the problem of maximizing non-monotone DR-submodular \ncontinuous functions under general down-closed convex constraints. We start by \ninvestigating geometric properties that underlie such objectives, e.g., a \nstrong relation between (approximately) stationary points and global optimum is \nproved. These properties are then used to devise two optimization algorithms \nwith provable guarantees. Concretely, we first devise a \"two-phase\" algorithm \nwith $1/4$ approximation guarantee. This algorithm allows the use of existing \nmethods for finding (approximately) stationary points as a subroutine, thus, \nharnessing recent progress in non-convex optimization. Then we present a \nnon-monotone Frank-Wolfe variant with $1/e$ approximation guarantee and \nsublinear convergence rate. Finally, we extend our approach to a broader class \nof generalized DR-submodular continuous functions, which captures a wider \nspectrum of applications. Our theoretical findings are validated on synthetic \nand real-world problem instances. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e7c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.02515"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Avgoustinos Vouros, Tiago V. Gehring, Kinga Szydlowska, Artur Janusz, Mike Croucher, Katarzyna Lukasiuk, Witold Konopka, Carmen Sandi, Zehai Tu, Eleni Vasilaki", "title": "A generalised framework for detailed classification of swimming paths inside the Morris Water Maze. (arXiv:1711.07446v2 [q-bio.QM] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07446", "type": "text/html"}], "timestampUsec": "1513676967346920", "comments": [], "summary": {"content": "<p>The Morris Water Maze is commonly used in behavioural neuroscience for the \nstudy of spatial learning with rodents. Over the years, various methods of \nanalysing rodent data collected in this task have been proposed. These methods \nspan from classical performance measurements (e.g. escape latency, rodent \nspeed, quadrant preference) to more sophisticated methods of categorisation \nwhich classify the animal swimming path into behavioural classes known as \nstrategies. Classification techniques provide additional insight in relation to \nthe actual animal behaviours but still only a limited amount of studies utilise \nthem mainly because they highly depend on machine learning knowledge. We have \npreviously demonstrated that the animals implement various strategies and by \nclassifying whole trajectories can lead to the loss of important information. \nIn this work, we developed a generalised and robust classification methodology \nwhich implements majority voting to boost the classification performance and \nsuccessfully nullify the need of manual tuning. Based on this framework, we \nbuilt a complete software, capable of performing the full analysis described in \nthis paper. The software provides an easy to use graphical user interface (GUI) \nthrough which users can enter their trajectory data, segment and label them and \nfinally generate reports and figures of the results. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e7d0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07446"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michael Gastegger, Ludwig Schwiedrzik, Marius Bittermann, Florian Berzsenyi, Philipp Marquetand", "title": "WACSF - Weighted Atom-Centered Symmetry Functions as Descriptors in Machine Learning Potentials. (arXiv:1712.05861v1 [physics.chem-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.05861", "type": "text/html"}], "timestampUsec": "1513676967346919", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450509f0956\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450509f0956&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We introduce weighted atom-centered symmetry functions (wACSFs) as \ndescriptors of a chemical system's geometry for use in the prediction of \nchemical properties such as enthalpies or potential energies via machine \nlearning. The wACSFs are based on conventional atom-centered symmetry functions \n(ACSFs) but overcome the undesirable scaling of the latter with increasing \nnumber of different elements in a chemical system. The performance of these two \ndescriptors is compared using them as inputs in high-dimensional neural network \npotentials (HDNNPs), employing the molecular structures and associated \nenthalpies of the 133855 molecules containing up to five different elements \nreported in the QM9 database as reference data. A substantially smaller number \nof wACSFs than ACSFs is needed to obtain a comparable spatial resolution of the \nmolecular structures. At the same time, this smaller set of wACSFs leads to \nsignificantly better generalization performance in the machine learning \npotential than the large set of conventional ACSFs. Furthermore, we show that \nthe intrinsic parameters of the descriptors can in principle be optimized with \na genetic algorithm in a highly automated manner. For the wACSFs employed here, \nwe find however that using a simple empirical parametrization scheme is \nsufficient in order to obtain HDNNPs with high accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e7e1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05861"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Amir Karami", "title": "Taming Wild High Dimensional Text Data with a Fuzzy Lash. (arXiv:1712.05997v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.05997", "type": "text/html"}], "timestampUsec": "1513676967346918", "comments": [], "summary": {"content": "<p>The bag of words (BOW) represents a corpus in a matrix whose elements are the \nfrequency of words. However, each row in the matrix is a very high-dimensional \nsparse vector. Dimension reduction (DR) is a popular method to address sparsity \nand high-dimensionality issues. Among different strategies to develop DR \nmethod, Unsupervised Feature Transformation (UFT) is a popular strategy to map \nall words on a new basis to represent BOW. The recent increase of text data and \nits challenges imply that DR area still needs new perspectives. Although a wide \nrange of methods based on the UFT strategy has been developed, the fuzzy \napproach has not been considered for DR based on this strategy. This research \ninvestigates the application of fuzzy clustering as a DR method based on the \nUFT strategy to collapse BOW matrix to provide a lower-dimensional \nrepresentation of documents instead of the words in a corpus. The quantitative \nevaluation shows that fuzzy clustering produces superior performance and \nfeatures to Principal Components Analysis (PCA) and Singular Value \nDecomposition (SVD), two popular DR methods based on the UFT strategy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e7ee", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05997"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Julio Amador, Axel Oehmichen, Miguel Molina-Solana", "title": "Characterizing Political Fake News in Twitter by its Meta-Data. (arXiv:1712.05999v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.05999", "type": "text/html"}], "timestampUsec": "1513676967346917", "comments": [], "summary": {"content": "<p>This article presents a preliminary approach towards characterizing political \nfake news on Twitter through the analysis of their meta-data. In particular, we \nfocus on more than 1.5M tweets collected on the day of the election of Donald \nTrump as 45th president of the United States of America. We use the meta-data \nembedded within those tweets in order to look for differences between tweets \ncontaining fake news and tweets not containing them. Specifically, we perform \nour analysis only on tweets that went viral, by studying proxies for users' \nexposure to the tweets, by characterizing accounts spreading fake news, and by \nlooking at their polarization. We found significant differences on the \ndistribution of followers, the number of URLs on tweets, and the verification \nof the users. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e7f9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05999"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Aditya Devarakonda, Kimon Fountoulakis, James Demmel, Michael W. Mahoney", "title": "Avoiding Synchronization in First-Order Methods for Sparse Convex Optimization. (arXiv:1712.06047v1 [cs.DC])", "alternate": [{"href": "http://arxiv.org/abs/1712.06047", "type": "text/html"}], "timestampUsec": "1513676967346916", "comments": [], "summary": {"content": "<p>Parallel computing has played an important role in speeding up convex \noptimization methods for big data analytics and large-scale machine learning \n(ML). However, the scalability of these optimization methods is inhibited by \nthe cost of communicating and synchronizing processors in a parallel setting. \nIterative ML methods are particularly sensitive to communication cost since \nthey often require communication every iteration. In this work, we extend \nwell-known techniques from Communication-Avoiding Krylov subspace methods to \nfirst-order, block coordinate descent methods for Support Vector Machines and \nProximal Least-Squares problems. Our Synchronization-Avoiding (SA) variants \nreduce the latency cost by a tunable factor of $s$ at the expense of a factor \nof $s$ increase in flops and bandwidth costs. We show that the SA-variants are \nnumerically stable and can attain large speedups of up to $5.1\\times$ on a Cray \nXC30 supercomputer. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e805", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06047"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Praneeth Narayanamurthy, Namrata Vaswani", "title": "MEDRoP: Memory-Efficient Dynamic Robust PCA. (arXiv:1712.06061v1 [cs.IT])", "alternate": [{"href": "http://arxiv.org/abs/1712.06061", "type": "text/html"}], "timestampUsec": "1513676967346915", "comments": [], "summary": {"content": "<p>Robust PCA (RPCA) is the problem of separating a given data matrix into the \nsum of a sparse matrix and a low-rank matrix. The column span of the low-rank \nmatrix gives the PCA solution. Dynamic RPCA is the time-varying extension of \nRPCA. It assumes that the true data vectors lie in a low-dimensional subspace \nthat can change with time, albeit slowly. The goal is to track this changing \nsubspace over time in the presence of sparse outliers. We propose an algorithm \nthat we call Memory-Efficient Dynamic Robust PCA (MEDRoP). This relies on the \nrecently studied recursive projected compressive sensing (ReProCS) framework \nfor solving dynamic RPCA problems, however, the actual algorithm is \nsignificantly different from, and simpler than, previous ReProCS-based methods. \nThe main contribution of this work is a theoretical guarantee that MEDRoP \nprovably solves dynamic RPCA under weakened versions of standard RPCA \nassumptions, a mild assumption on slow subspace change, and two simple \nassumptions (a lower bound on most outlier magnitudes and mutual independence \nof the true data vectors). Our result is important because (i) it removes the \nstrong assumptions needed by the three previous complete guarantees for \nReProCS-based algorithms; (ii) it shows that, it is possible to achieve \nsignificantly improved outlier tolerance compared to static RPCA solutions by \nexploiting slow subspace change and a lower bound on most outlier magnitudes; \n(iii) it is able to track a changed subspace within a delay that is more than \nthe subspace dimension by only logarithmic factors and thus is near-optimal; \nand (iv) it studies an algorithm that is online (after initialization), fast, \nand, memory-efficient (its memory complexity is within logarithmic factors of \nthe optimal). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e80b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06061"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Sivaraman Balakrishnan, Larry Wasserman", "title": "Hypothesis Testing for High-Dimensional Multinomials: A Selective Review. (arXiv:1712.06120v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06120", "type": "text/html"}], "timestampUsec": "1513676967346914", "comments": [], "summary": {"content": "<p>The statistical analysis of discrete data has been the subject of extensive \nstatistical research dating back to the work of Pearson. In this survey we \nreview some recently developed methods for testing hypotheses about \nhigh-dimensional multinomials. Traditional tests like the $\\chi^2$ test and the \nlikelihood ratio test can have poor power in the high-dimensional setting. Much \nof the research in this area has focused on finding tests with asymptotically \nNormal limits and developing (stringent) conditions under which tests have \nNormal limits. We argue that this perspective suffers from a significant \ndeficiency: it can exclude many high-dimensional cases when - despite having \nnon Normal null distributions - carefully designed tests can have high power. \nFinally, we illustrate that taking a minimax perspective and considering \nrefinements of this perspective can lead naturally to powerful and practical \ntests. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e816", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06120"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Nathan Killoran, Leo J. Lee, Andrew Delong, David Duvenaud, Brendan J. Frey", "title": "Generating and designing DNA with deep generative models. (arXiv:1712.06148v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.06148", "type": "text/html"}], "timestampUsec": "1513676967346913", "comments": [], "summary": {"content": "<p>We propose generative neural network methods to generate DNA sequences and \ntune them to have desired properties. We present three approaches: creating \nsynthetic DNA sequences using a generative adversarial network; a DNA-based \nvariant of the activation maximization (\"deep dream\") design method; and a \njoint procedure which combines these two approaches together. We show that \nthese tools capture important structures of the data and, when applied to \ndesigning probes for protein binding microarrays, allow us to generate new \nsequences whose properties are estimated to be superior to those found in the \ntraining data. We believe that these results open the door for applying deep \ngenerative models to advance genomics research. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e81d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06148"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Kuang Gong, Jaewon Yang, Kyungsang Kim, Georges El Fakhri, Youngho Seo, Quanzheng Li", "title": "Attenuation Correction for Brain PET imaging using Deep Neural Network based on Dixon and ZTE MR images. (arXiv:1712.06203v1 [physics.med-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.06203", "type": "text/html"}], "timestampUsec": "1513676967346912", "comments": [], "summary": {"content": "<p>Positron Emission Tomography (PET) is a functional imaging modality widely \nused in neuroscience studies. To obtain meaningful quantitative results from \nPET images, attenuation correction is necessary during image reconstruction. \nFor PET/MR hybrid systems, PET attenuation is challenging as Magnetic Resonance \n(MR) images do not reflect attenuation coefficients directly. To address this \nissue, we present deep neural network methods to derive the continuous \nattenuation coefficients for brain PET imaging from MR images. With only Dixon \nMR images as the network input, the existing U-net structure was adopted and \nanalysis using forty patient data sets shows it is superior than other Dixon \nbased methods. When both Dixon and zero echo time (ZTE) images are available, \napart from stacking multiple MR images along the U-net input channels, we have \nproposed a new network structure to extract the features from Dixon and ZTE \nimages independently at early layers and combine them together at later layers. \nQuantitative analysis based on fourteen real patient data sets demonstrates \nthat both network approaches can perform better than the standard methods, and \nthe proposed network structure can further reduce the PET quantification error \ncompared to the U-net structure with multiple inputs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e82b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06203"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Anna Little, Mauro Maggioni, James M. Murphy", "title": "Path-Based Spectral Clustering: Guarantees, Robustness to Outliers, and Fast Algorithms. (arXiv:1712.06206v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06206", "type": "text/html"}], "timestampUsec": "1513676967346911", "comments": [], "summary": {"content": "<p>We consider the problem of clustering with the longest leg path distance \n(LLPD) metric, which is informative for elongated and irregularly shaped \nclusters. We prove finite-sample guarantees on the performance of clustering \nwith respect to this metric when random samples are drawn from multiple \nintrinsically low-dimensional clusters in high-dimensional space, in the \npresence of a large number of high-dimensional outliers. By combining these \nresults with spectral clustering with respect to LLPD, we provide conditions \nunder which the eigengap statistic correctly determines the number of clusters \nfor a large class of data sets, and prove guarantees on the number of points \nmislabeled by the proposed algorithm. Our methods are quite general and provide \nperformance guarantees for spectral clustering with any ultrametric. We also \nintroduce an efficient approximation algorithm, easy to implement, for the \nLLPD, based on a multiscale analysis of adjacency graphs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e835", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06206"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Cameron Carlin, Long Van Ho, David Ledbetter, Melissa Aczon, Randall Wetzel", "title": "Predicting Individual Physiologically Acceptable States for Discharge from a Pediatric Intensive Care Unit. (arXiv:1712.06214v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06214", "type": "text/html"}], "timestampUsec": "1513676967346910", "comments": [], "summary": {"content": "<p>Objective: Predict patient-specific vitals deemed medically acceptable for \ndischarge from a pediatric intensive care unit (ICU). Design: The means of each \npatient's hr, sbp and dbp measurements between their medical and physical \ndischarge from the ICU were computed as a proxy for their physiologically \nacceptable state space (PASS) for successful ICU discharge. These individual \nPASS values were compared via root mean squared error (rMSE) to population \nage-normal vitals, a polynomial regression through the PASS values of a \nPediatric ICU (PICU) population and predictions from two recurrent neural \nnetwork models designed to predict personalized PASS within the first twelve \nhours following ICU admission. Setting: PICU at Children's Hospital Los Angeles \n(CHLA). Patients: 6,899 PICU episodes (5,464 patients) collected between 2009 \nand 2016. Interventions: None. Measurements: Each episode data contained 375 \nvariables representing vitals, labs, interventions, and drugs. They also \nincluded a time indicator for PICU medical discharge and physical discharge. \nMain Results: The rMSEs between individual PASS values and population \nage-normals (hr: 25.9 bpm, sbp: 13.4 mmHg, dbp: 13.0 mmHg) were larger than the \nrMSEs corresponding to the polynomial regression (hr: 19.1 bpm, sbp: 12.3 mmHg, \ndbp: 10.8 mmHg). The rMSEs from the best performing RNN model were the lowest \n(hr: 16.4 bpm; sbp: 9.9 mmHg, dbp: 9.0 mmHg). Conclusion: PICU patients are a \nunique subset of the general population, and general age-normal vitals may not \nbe suitable as target values indicating physiologic stability at discharge. \nAge-normal vitals that were specifically derived from the medical-to-physical \ndischarge window of ICU patients may be more appropriate targets for \n'acceptable' physiologic state for critical care patients. Going beyond simple \nage bins, an RNN model can provide more personalized target values. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e840", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06214"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Brian E. Moore, Chen Gao, Raj Rao Nadakuditi", "title": "Panoramic Robust PCA for Foreground-Background Separation on Noisy, Free-Motion Camera Video. (arXiv:1712.06229v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06229", "type": "text/html"}], "timestampUsec": "1513676967346909", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450509f0cd7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450509f0cd7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This work presents a novel approach for robust PCA with total variation \nregularization for foreground-background separation and denoising on noisy, \nmoving camera video. Our proposed algorithm registers the raw (possibly \ncorrupted) frames of a video and then jointly processes the registered frames \nto produce a decomposition of the scene into a low-rank background component \nthat captures the static components of the scene, a smooth foreground component \nthat captures the dynamic components of the scene, and a sparse component that \nisolates corruptions. Unlike existing methods, our proposed algorithm produces \na panoramic low-rank component that spans the entire field of view, \nautomatically stitching together corrupted data from partially overlapping \nscenes. The low-rank portion of our robust PCA model is based on a recently \ndiscovered optimal low-rank matrix estimator (OptShrink) that requires no \nparameter tuning. We demonstrate the performance of our algorithm on both \nstatic and moving camera videos corrupted by noise and outliers. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e846", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06229"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Zhuoran Yang, Lin F. Yang, Ethan X. Fang, Tuo Zhao, Zhaoran Wang, Matey Neykov", "title": "Misspecified Nonconvex Statistical Optimization for Phase Retrieval. (arXiv:1712.06245v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06245", "type": "text/html"}], "timestampUsec": "1513676967346908", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050a5009e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050a5009e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Existing nonconvex statistical optimization theory and methods crucially rely \non the correct specification of the underlying \"true\" statistical models. To \naddress this issue, we take a first step towards taming model misspecification \nby studying the high-dimensional sparse phase retrieval problem with \nmisspecified link functions. In particular, we propose a simple variant of the \nthresholded Wirtinger flow algorithm that, given a proper initialization, \nlinearly converges to an estimator with optimal statistical accuracy for a \nbroad family of unknown link functions. We further provide extensive numerical \nexperiments to support our theoretical findings. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e853", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06245"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Takashi Matsubara, Tetsuo Tashiro, Kuniaki Uehara", "title": "Deep Neural Generative Model of Functional MRI Images for Psychiatric Disorder Diagnosis. (arXiv:1712.06260v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06260", "type": "text/html"}], "timestampUsec": "1513676967346907", "comments": [], "summary": {"content": "<p>Accurate diagnosis of psychiatric disorders plays a critical role in \nimproving quality of life for patients and potentially supports the development \nof new treatments. Many studies have been conducted on machine learning \ntechniques that seek brain imaging data for specific biomarkers of disorders. \nThese studies have encountered the following dilemma: An end-to-end \nclassification overfits to a small number of high-dimensional samples but \nunsupervised feature-extraction has the risk of extracting a signal of no \ninterest. In addition, such studies often provided only diagnoses for patients \nwithout presenting the reasons for these diagnoses. This study proposed a deep \nneural generative model of resting-state functional magnetic resonance imaging \n(fMRI) data. The proposed model is conditioned by the assumption of the \nsubject's state and estimates the posterior probability of the subject's state \ngiven the imaging data, using Bayes' rule. This study applied the proposed \nmodel to diagnose schizophrenia and bipolar disorders. Diagnosis accuracy was \nimproved by a large margin over competitive approaches, namely a support vector \nmachine, logistic regression, and multilayer perceptron with or without \nunsupervised feature-extractors in addition to a Gaussian mixture model. The \nproposed model visualizes brain regions largely related to the disorders, thus \nmotivating further biological investigation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e868", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06260"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Adam J. Riesselman, John B. Ingraham, Debora S. Marks", "title": "Deep generative models of genetic variation capture mutation effects. (arXiv:1712.06527v1 [q-bio.QM])", "alternate": [{"href": "http://arxiv.org/abs/1712.06527", "type": "text/html"}], "timestampUsec": "1513676967346906", "comments": [], "summary": {"content": "<p>The functions of proteins and RNAs are determined by a myriad of interactions \nbetween their constituent residues, but most quantitative models of how \nmolecular phenotype depends on genotype must approximate this by simple \nadditive effects. While recent models have relaxed this constraint to also \naccount for pairwise interactions, these approaches do not provide a tractable \npath towards modeling higher-order dependencies. Here, we show how latent \nvariable models with nonlinear dependencies can be applied to capture \nbeyond-pairwise constraints in biomolecules. We present a new probabilistic \nmodel for sequence families, DeepSequence, that can predict the effects of \nmutations across a variety of deep mutational scanning experiments \nsignificantly better than site independent or pairwise models that are based on \nthe same evolutionary data. The model, learned in an unsupervised manner solely \nfrom sequence information, is grounded with biologically motivated priors, \nreveals latent organization of sequence families, and can be used to \nextrapolate to new parts of sequence space \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e871", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06527"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Gian-Andrea Thanei, Nicolai Meinshausen, Rajen D. Shah", "title": "The xyz algorithm for fast interaction search in high-dimensional data. (arXiv:1610.05108v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.05108", "type": "text/html"}], "timestampUsec": "1513676967346905", "comments": [], "summary": {"content": "<p>When performing regression on a dataset with $p$ variables, it is often of \ninterest to go beyond using main linear effects and include interactions as \nproducts between individual variables. For small-scale problems, these \ninteractions can be computed explicitly but this leads to a computational \ncomplexity of at least $\\mathcal{O}(p^2)$ if done naively. This cost can be \nprohibitive if $p$ is very large. We introduce a new randomised algorithm that \nis able to discover interactions with high probability and under mild \nconditions has a runtime that is subquadratic in $p$. We show that strong \ninteractions can be discovered in almost linear time, whilst finding weaker \ninteractions requires $\\mathcal{O}(p^\\alpha)$ operations for $1 &lt; \\alpha &lt; 2$ \ndepending on their strength. The underlying idea is to transform interaction \nsearch into a closestpair problem which can be solved efficiently in \nsubquadratic time. The algorithm is called $\\mathit{xyz}$ and is implemented in \nthe language R. We demonstrate its efficiency for application to genome-wide \nassociation studies, where more than $10^{11}$ interactions can be screened in \nunder $280$ seconds with a single-core $1.2$ GHz CPU. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e878", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.05108"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alexey Zaytsev, Evgeny Burnaev", "title": "Minimax Error of Interpolation and Optimal Design of Experiments for Variable Fidelity Data. (arXiv:1610.06731v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.06731", "type": "text/html"}], "timestampUsec": "1513676967346904", "comments": [], "summary": {"content": "<p>Engineering problems often involve data sources of variable fidelity with \ndifferent costs of obtaining an observation. In particular, one can use both a \ncheap low fidelity function (e.g. a computational experiment with a CFD code) \nand an expensive high fidelity function (e.g. a wind tunnel experiment) to \ngenerate a data sample in order to construct a regression model of a high \nfidelity function. The key question in this setting is how the sizes of the \nhigh and low fidelity data samples should be selected in order to stay within a \ngiven computational budget and maximize accuracy of the regression model prior \nto committing resources on data acquisition. \n</p> \n<p>In this paper we obtain minimax interpolation errors for single and variable \nfidelity scenarios for a multivariate Gaussian process regression. Evaluation \nof the minimax errors allows us to identify cases when the variable fidelity \ndata provides better interpolation accuracy than the exclusively high fidelity \ndata for the same computational budget. \n</p> \n<p>These results allow us to calculate the optimal shares of variable fidelity \ndata samples under the given computational budget constraint. Real and \nsynthetic data experiments suggest that using the obtained optimal shares often \noutperforms natural heuristics in terms of the regression accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e880", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.06731"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Alexander Jung, Nguyen Tran Quang, Alexandru Mara", "title": "When is Network Lasso Accurate?. (arXiv:1704.02107v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.02107", "type": "text/html"}], "timestampUsec": "1513676967346903", "comments": [], "summary": {"content": "<p>The \"least absolute shrinkage and selection operator\" (Lasso) method has been \nadapted recently for networkstructured datasets. In particular, this network \nLasso method allows to learn graph signals from a small number of noisy signal \nsamples by using the total variation of a graph signal for regularization. \nWhile efficient and scalable implementations of the network Lasso are \navailable, only little is known about the conditions on the underlying network \nstructure which ensure network Lasso to be accurate. By leveraging concepts of \ncompressed sensing, we address this gap and derive precise conditions on the \nunderlying network topology and sampling set which guarantee the network Lasso \nfor a particular loss function to deliver an accurate estimate of the entire \nunderlying graph signal. We also quantify the error incurred by network Lasso \nin terms of two constants which reflect the connectivity of the sampled nodes. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e88a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.02107"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Anubhav Ashok, Nicholas Rhinehart, Fares Beainy, Kris M. Kitani", "title": "N2N Learning: Network to Network Compression via Policy Gradient Reinforcement Learning. (arXiv:1709.06030v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.06030", "type": "text/html"}], "timestampUsec": "1513676967346902", "comments": [], "summary": {"content": "<p>While bigger and deeper neural network architectures continue to advance the \nstate-of-the-art for many computer vision tasks, real-world adoption of these \nnetworks is impeded by hardware and speed constraints. Conventional model \ncompression methods attempt to address this problem by modifying the \narchitecture manually or using pre-defined heuristics. Since the space of all \nreduced architectures is very large, modifying the architecture of a deep \nneural network in this way is a difficult task. In this paper, we tackle this \nissue by introducing a principled method for learning reduced network \narchitectures in a data-driven way using reinforcement learning. Our approach \ntakes a larger `teacher' network as input and outputs a compressed `student' \nnetwork derived from the `teacher' network. In the first stage of our method, a \nrecurrent policy network aggressively removes layers from the large `teacher' \nmodel. In the second stage, another recurrent policy network carefully reduces \nthe size of each remaining layer. The resulting network is then evaluated to \nobtain a reward -- a score based on the accuracy and compression of the \nnetwork. Our approach uses this reward signal with policy gradients to train \nthe policies to find a locally optimal student network. Our experiments show \nthat we can achieve compression rates of more than 10x for models such as \nResNet-34 while maintaining similar performance to the input `teacher' network. \nWe also present a valuable transfer learning result which shows that policies \nwhich are pre-trained on smaller `teacher' networks can be used to rapidly \nspeed up training on larger `teacher' networks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e892", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.06030"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Been Kim, Justin Gilmer, Fernanda Viegas, Ulfar Erlingsson, Martin Wattenberg", "title": "TCAV: Relative concept importance testing with Linear Concept Activation Vectors. (arXiv:1711.11279v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.11279", "type": "text/html"}], "timestampUsec": "1513676967346901", "comments": [], "summary": {"content": "<p>Neural networks commonly offer high utility but remain difficult to \ninterpret. Developing methods to explain their decisions is challenging due to \ntheir large size, complex structure, and inscrutable internal representations. \nThis work argues that the language of explanations should be expanded from that \nof input features (e.g., assigning importance weightings to pixels) to include \nthat of higher-level, human-friendly concepts. For example, an understandable \nexplanation of why an image classifier outputs the label \"zebra\" would ideally \nrelate to concepts such as \"stripes\" rather than a set of particular pixel \nvalues. This paper introduces the \"concept activation vector\" (CAV) which \nallows quantitative analysis of a concept's relative importance to \nclassification, with a user-provided set of input data examples defining the \nconcept. CAVs may be easily used by non-experts, who need only provide \nexamples, and with CAVs the high-dimensional structure of neural networks turns \ninto an aid to interpretation, rather than an obstacle. Using the domain of \nimage classification as a testing ground, we describe how CAVs may be used to \ntest hypotheses about classifiers and also generate insights into the \ndeficiencies and correlations in training data. CAVs also provide us a directed \napproach to choose the combinations of neurons to visualize with the DeepDream \ntechnique, which traditionally has chosen neurons or linear combinations of \nneurons at random to visualize. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e89e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11279"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Casey Chu, Andrey Zhmoginov, Mark Sandler", "title": "CycleGAN, a Master of Steganography. (arXiv:1712.02950v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02950", "type": "text/html"}], "timestampUsec": "1513676967346900", "comments": [], "summary": {"content": "<p>CycleGAN (Zhu et al. 2017) is one recent successful approach to learn a \ntransformation between two image distributions. In a series of experiments, we \ndemonstrate an intriguing property of the model: CycleGAN learns to \"hide\" \ninformation about a source image into the images it generates in a nearly \nimperceptible, high-frequency signal. This trick ensures that the generator can \nrecover the original sample and thus satisfy the cyclic consistency \nrequirement, while the generated image remains realistic. We connect this \nphenomenon with adversarial attacks by viewing CycleGAN's training procedure as \ntraining a generator of adversarial examples and demonstrate that the cyclic \nconsistency loss causes CycleGAN to be especially vulnerable to adversarial \nattacks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e8a7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02950"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Michael Gastegger, J&#xf6;rg Behler, Philipp Marquetand", "title": "Machine Learning Molecular Dynamics for the Simulation of Infrared Spectra. (arXiv:1705.05907v1 [physics.chem-ph] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1705.05907", "type": "text/html"}], "timestampUsec": "1513676967346899", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050a50424\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050a50424&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Machine learning has emerged as an invaluable tool in many research areas. In \nthe present work, we harness this power to predict highly accurate molecular \ninfrared spectra with unprecedented computational efficiency. To account for \nvibrational anharmonic and dynamical effects -- typically neglected by \nconventional quantum chemistry approaches -- we base our machine learning \nstrategy on ab initio molecular dynamics simulations. While these simulations \nare usually extremely time consuming even for small molecules, we overcome \nthese limitations by leveraging the power of a variety of machine learning \ntechniques, not only accelerating simulations by several orders of magnitude, \nbut also greatly extending the size of systems that can be treated. To this \nend, we develop a molecular dipole moment model based on environment dependent \nneural network charges and combine it with the neural network potentials of \nBehler and Parrinello. Contrary to the prevalent big data philosophy, we are \nable to obtain very accurate machine learning models for the prediction of \ninfrared spectra based on only a few hundreds of electronic structure reference \npoints. This is made possible through the introduction of a fully automated \nsampling scheme and the use of molecular forces during neural network potential \ntraining. We demonstrate the power of our machine learning approach by applying \nit to model the infrared spectra of a methanol molecule, n-alkanes containing \nup to 200 atoms and the protonated alanine tripeptide, which at the same time \nrepresents the first application of machine learning techniques to simulate the \ndynamics of a peptide. In all these case studies we find excellent agreement \nbetween the infrared spectra predicted via machine learning models and the \nrespective theoretical and experimental spectra. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513676967347", "annotations": [], "published": 1513676967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a50e8b1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.05907"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Vasily Morzhakov, Alexey Redozubov", "title": "An Artificial Neural Network Architecture Based on Context Transformations in Cortical Minicolumns. (arXiv:1712.05954v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.05954", "type": "text/html"}], "timestampUsec": "1513669229487545", "comments": [], "summary": {"content": "<p>Cortical minicolumns are considered a model of cortical organization. Their \nfunction is still a source of research and not reflected properly in modern \narchitecture of nets in algorithms of Artificial Intelligence. We assume its \nfunction and describe it in this article. Furthermore, we show how this \nproposal allows to construct a new architecture, that is not based on \nconvolutional neural networks, test it on MNIST data and receive close to \nConvolutional Neural Network accuracy. We also show that the proposed \narchitecture possesses an ability to train on a small quantity of samples. To \nachieve these results, we enable the minicolumns to remember context \ntransformations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac3a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05954"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andres Felipe Cruz Salinas, Jonatan Gomez Perdomo", "title": "Self-adaptation of Genetic Operators Through Genetic Programming Techniques. (arXiv:1712.06070v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.06070", "type": "text/html"}], "timestampUsec": "1513669229487544", "comments": [], "summary": {"content": "<p>Here we propose an evolutionary algorithm that self modifies its operators at \nthe same time that candidate solutions are evolved. This tackles convergence \nand lack of diversity issues, leading to better solutions. Operators are \nrepresented as trees and are evolved using genetic programming (GP) techniques. \nThe proposed approach is tested with real benchmark functions and an analysis \nof operator evolution is provided. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac44", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06070"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Farhan Shafiq, Takato Yamada, Antonio T. Vilchez, Sakyasingha Dasgupta", "title": "Automated flow for compressing convolution neural networks for efficient edge-computation with FPGA. (arXiv:1712.06272v1 [cs.AR])", "alternate": [{"href": "http://arxiv.org/abs/1712.06272", "type": "text/html"}], "timestampUsec": "1513669229487543", "comments": [], "summary": {"content": "<p>Deep convolutional neural networks (CNN) based solutions are the current \nstate- of-the-art for computer vision tasks. Due to the large size of these \nmodels, they are typically run on clusters of CPUs or GPUs. However, power \nrequirements and cost budgets can be a major hindrance in adoption of CNN for \nIoT applications. Recent research highlights that CNN contain significant \nredundancy in their structure and can be quantized to lower bit-width \nparameters and activations, while maintaining acceptable accuracy. Low \nbit-width and especially single bit-width (binary) CNN are particularly \nsuitable for mobile applications based on FPGA implementation, due to the \nbitwise logic operations involved in binarized CNN. Moreover, the transition to \nlower bit-widths opens new avenues for performance optimizations and model \nimprovement. In this paper, we present an automatic flow from trained \nTensorFlow models to FPGA system on chip implementation of binarized CNN. This \nflow involves quantization of model parameters and activations, generation of \nnetwork and model in embedded-C, followed by automatic generation of the FPGA \naccelerator for binary convolutions. The automated flow is demonstrated through \nimplementation of binarized \"YOLOV2\" on the low cost, low power Cyclone- V FPGA \ndevice. Experiments on object detection using binarized YOLOV2 demonstrate \nsignificant performance benefit in terms of model size and inference speed on \nFPGA as compared to CPU and mobile CPU platforms. Furthermore, the entire \nautomated flow from trained models to FPGA synthesis can be completed within \none hour. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac48", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06272"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Joel Lehman, Jay Chen, Jeff Clune, Kenneth O. Stanley", "title": "Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients. (arXiv:1712.06563v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.06563", "type": "text/html"}], "timestampUsec": "1513669229487542", "comments": [], "summary": {"content": "<p>While neuroevolution (evolving neural networks) has a successful track record \nacross a variety of domains from reinforcement learning to artificial life, it \nis rarely applied to large, deep neural networks. A central reason is that \nwhile random mutation generally works in low dimensions, a random perturbation \nof thousands or millions of weights is likely to break existing functionality, \nproviding no learning signal even if some individual weight changes were \nbeneficial. This paper proposes a solution by introducing a family of safe \nmutation (SM) operators that aim within the mutation operator itself to find a \ndegree of change that does not alter network behavior too much, but still \nfacilitates exploration. Importantly, these SM operators do not require any \nadditional interactions with the environment. The most effective SM variant \ncapitalizes on the intriguing opportunity to scale the degree of mutation of \neach individual weight according to the sensitivity of the network's outputs to \nthat weight, which requires computing the gradient of outputs with respect to \nthe weights (instead of the gradient of error, as in conventional deep \nlearning). This safe mutation through gradients (SM-G) operator dramatically \nincreases the ability of a simple genetic algorithm-based neuroevolution method \nto find solutions in high-dimensional domains that require deep and/or \nrecurrent neural networks (which tend to be particularly brittle to mutation), \nincluding domains that require processing raw pixels. By improving our ability \nto evolve deep neural networks, this new safer approach to mutation expands the \nscope of domains amenable to neuroevolution. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac4f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06563"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O. Stanley, Jeff Clune", "title": "Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning. (arXiv:1712.06567v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.06567", "type": "text/html"}], "timestampUsec": "1513669229487541", "comments": [], "summary": {"content": "<p>Deep artificial neural networks (DNNs) are typically trained via \ngradient-based learning algorithms, namely backpropagation. Evolution \nstrategies (ES) can rival backprop-based algorithms such as Q-learning and \npolicy gradients on challenging deep reinforcement learning (RL) problems. \nHowever, ES can be considered a gradient-based algorithm because it performs \nstochastic gradient descent via an operation similar to a finite-difference \napproximation of the gradient. That raises the question of whether \nnon-gradient-based evolutionary algorithms can work at DNN scales. Here we \ndemonstrate they can: we evolve the weights of a DNN with a simple, \ngradient-free, population-based genetic algorithm (GA) and it performs well on \nhard deep RL problems, including Atari and humanoid locomotion. The Deep GA \nsuccessfully evolves networks with over four million free parameters, the \nlargest neural networks ever evolved with a traditional evolutionary algorithm. \nThese results (1) expand our sense of the scale at which GAs can operate, (2) \nsuggest intriguingly that in some cases following the gradient is not the best \nchoice for optimizing performance, and (3) make immediately available the \nmultitude of techniques that have been developed in the neuroevolution \ncommunity to improve performance on RL problems. To demonstrate the latter, we \nshow that combining DNNs with novelty search, which was designed to encourage \nexploration on tasks with deceptive or sparse reward functions, can solve a \nhigh-dimensional problem on which reward-maximizing algorithms (e.g. DQN, A3C, \nES, and the GA) fail. Additionally, the Deep GA parallelizes better than ES, \nA3C, and DQN, and enables a state-of-the-art compact encoding technique that \ncan represent million-parameter DNNs in thousands of bytes. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac58", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06567"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Isabeau Pr&#xe9;mont-Schwarz, Alexander Ilin, Tele Hotloo Hao, Antti Rasmus, Rinu Boney, Harri Valpola", "title": "Recurrent Ladder Networks. (arXiv:1707.09219v4 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.09219", "type": "text/html"}], "timestampUsec": "1513669229487539", "comments": [], "summary": {"content": "<p>We propose a recurrent extension of the Ladder networks whose structure is \nmotivated by the inference required in hierarchical latent variable models. We \ndemonstrate that the recurrent Ladder is able to handle a wide variety of \ncomplex learning tasks that benefit from iterative inference and temporal \nmodeling. The architecture shows close-to-optimal results on temporal modeling \nof video data, competitive results on music modeling, and improved perceptual \ngrouping based on higher order abstractions, such as stochastic textures and \nmotion cues. We present results for fully supervised, semi-supervised, and \nunsupervised tasks. The results suggest that the proposed architecture and \nprinciples are powerful tools for learning a hierarchy of abstractions, \nlearning iterative inference and handling temporal information. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac6c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.09219"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513920224, "author": "Zahra Mahoor, Jack Felag, Josh Bongard", "title": "Morphology dictates a robot's ability to ground crowd-proposed language. (arXiv:1712.05881v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.05881", "type": "text/html"}], "timestampUsec": "1513669229487538", "comments": [], "summary": {"content": "<p>As more robots act in physical proximity to people, it is essential to ensure \nthey make decisions and execute actions that align with human values. To do so, \nrobots need to understand the true intentions behind human-issued commands. In \nthis paper, we define a safe robot as one that receives a natural-language \ncommand from humans, considers an action in response to that command, and \naccurately predicts how humans will judge that action if is executed in \nreality. Our contribution is two-fold: First, we introduce a web platform for \nhuman users to propose commands to simulated robots. The robots receive \ncommands and act based on those proposed commands, and then the users provide \npositive and/or negative reinforcement. Next, we train a critic for each robot \nto predict the crowd's responses to one of the crowd-proposed commands. Second, \nwe show that the morphology of a robot plays a role in the way it grounds \nlanguage: The critics show that two of the robots used in the experiment \nachieve a lower prediction error than the others. Thus, those two robots are \nsafer, according to our definition, since they ground the proposed command more \naccurately. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513920224, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac76", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05881"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mu Qiao, Luis Bathen, Simon-Pierre G&#xe9;not, Sunhwan Lee, Ramani Routray", "title": "StackInsights: Cognitive Learning for Hybrid Cloud Readiness. (arXiv:1712.06015v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.06015", "type": "text/html"}], "timestampUsec": "1513669229487537", "comments": [], "summary": {"content": "<p>Hybrid cloud is an integrated cloud computing environment utilizing a mix of \npublic cloud, private cloud, and on-premise traditional IT infrastructures. \nWorkload awareness, defined as a detailed full range understanding of each \nindividual workload, is essential in implementing the hybrid cloud. While it is \ncritical to perform an accurate analysis to determine which workloads are \nappropriate for on-premise deployment versus which workloads can be migrated to \na cloud off-premise, the assessment is mainly performed by rule or policy based \napproaches. In this paper, we introduce StackInsights, a novel cognitive system \nto automatically analyze and predict the cloud readiness of workloads for an \nenterprise. Our system harnesses the critical metrics across the entire stack: \n1) infrastructure metrics, 2) data relevance metrics, and 3) application \ntaxonomy, to identify workloads that have characteristics of a) low sensitivity \nwith respect to business security, criticality and compliance, and b) low \nresponse time requirements and access patterns. Since the capture of the data \nrelevance metrics involves an intrusive and in-depth scanning of the content of \nstorage objects, a machine learning model is applied to perform the business \nrelevance classification by learning from the meta level metrics harnessed \nacross stack. In contrast to traditional methods, StackInsights significantly \nreduces the total time for hybrid cloud readiness assessment by orders of \nmagnitude. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac84", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06015"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Per-Arne Andersen, Morten Goodwin, Ole-Christoffer Granmo", "title": "Towards a Deep Reinforcement Learning Approach for Tower Line Wars. (arXiv:1712.06180v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.06180", "type": "text/html"}], "timestampUsec": "1513669229487536", "comments": [], "summary": {"content": "<p>There have been numerous breakthroughs with reinforcement learning in the \nrecent years, perhaps most notably on Deep Reinforcement Learning successfully \nplaying and winning relatively advanced computer games. There is undoubtedly an \nanticipation that Deep Reinforcement Learning will play a major role when the \nfirst AI masters the complicated game plays needed to beat a professional \nReal-Time Strategy game player. For this to be possible, there needs to be a \ngame environment that targets and fosters AI research, and specifically Deep \nReinforcement Learning. Some game environments already exist, however, these \nare either overly simplistic such as Atari 2600 or complex such as Starcraft II \nfrom Blizzard Entertainment. We propose a game environment in between Atari \n2600 and Starcraft II, particularly targeting Deep Reinforcement Learning \nalgorithm research. The environment is a variant of Tower Line Wars from \nWarcraft III, Blizzard Entertainment. Further, as a proof of concept that the \nenvironment can harbor Deep Reinforcement algorithms, we propose and apply a \nDeep Q-Reinforcement architecture. The architecture simplifies the state space \nso that it is applicable to Q-learning, and in turn improves performance \ncompared to current state-of-the-art methods. Our experiments show that the \nproposed architecture can learn to play the environment well, and score 33% \nbetter than standard Deep Q-learning which in turn proves the usefulness of the \ngame environment. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eac97", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06180"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Erik Bodin, Iman Malik, Carl Henrik Ek, Neill D. F. Campbell", "title": "Nonparametric Inference for Auto-Encoding Variational Bayes. (arXiv:1712.06536v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06536", "type": "text/html"}], "timestampUsec": "1513669229487535", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050a5078d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050a5078d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We would like to learn latent representations that are low-dimensional and \nhighly interpretable. A model that has these characteristics is the Gaussian \nProcess Latent Variable Model. The benefits and negative of the GP-LVM are \ncomplementary to the Variational Autoencoder, the former provides interpretable \nlow-dimensional latent representations while the latter is able to handle large \namounts of data and can use non-Gaussian likelihoods. Our inspiration for this \npaper is to marry these two approaches and reap the benefits of both. In order \nto do so we will introduce a novel approximate inference scheme inspired by the \nGP-LVM and the VAE. We show experimentally that the approximation allows the \ncapacity of the generative bottle-neck (Z) of the VAE to be arbitrarily large \nwithout losing a highly interpretable representation, allowing reconstruction \nquality to be unlimited by Z at the same time as a low-dimensional space can be \nused to perform ancestral sampling from as well as a means to reason about the \nembedded data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eacb0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06536"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kyle Brown, Derek Doran", "title": "Realistic Traffic Generation for Web Robots. (arXiv:1712.05813v1 [cs.NI])", "alternate": [{"href": "http://arxiv.org/abs/1712.05813", "type": "text/html"}], "timestampUsec": "1513669229487534", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050aa0fc9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050aa0fc9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Critical to evaluating the capacity, scalability, and availability of web \nsystems are realistic web traffic generators. Web traffic generation is a \nclassic research problem, no generator accounts for the characteristics of web \nrobots or crawlers that are now the dominant source of traffic to a web server. \nAdministrators are thus unable to test, stress, and evaluate how their systems \nperform in the face of ever increasing levels of web robot traffic. To resolve \nthis problem, this paper introduces a novel approach to generate synthetic web \nrobot traffic with high fidelity. It generates traffic that accounts for both \nthe temporal and behavioral qualities of robot traffic by statistical and \nBayesian models that are fitted to the properties of robot traffic seen in web \nlogs from North America and Europe. We evaluate our traffic generator by \ncomparing the characteristics of generated traffic to those of the original \ndata. We look at session arrival rates, inter-arrival times and session \nlengths, comparing and contrasting them between generated and real traffic. \nFinally, we show that our generated traffic affects cache performance similarly \nto actual traffic, using the common LRU and LFU eviction policies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eacbc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05813"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Dustin Anderson, Jean-Roch Vlimant, Maria Spiropulu", "title": "An MPI-Based Python Framework for Distributed Training with Keras. (arXiv:1712.05878v1 [cs.DC])", "alternate": [{"href": "http://arxiv.org/abs/1712.05878", "type": "text/html"}], "timestampUsec": "1513669229487533", "comments": [], "summary": {"content": "<p>We present a lightweight Python framework for distributed training of neural \nnetworks on multiple GPUs or CPUs. The framework is built on the popular Keras \nmachine learning library. The Message Passing Interface (MPI) protocol is used \nto coordinate the training process, and the system is well suited for job \nsubmission at supercomputing sites. We detail the software's features, describe \nits use, and demonstrate its performance on systems of varying sizes on a \nbenchmark problem drawn from high-energy physics research. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eaccc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05878"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Junghoon Seo, Taegyun Jeon", "title": "On reproduction of On the regularization of Wasserstein GANs. (arXiv:1712.05882v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.05882", "type": "text/html"}], "timestampUsec": "1513669229487532", "comments": [], "summary": {"content": "<p>This report has several purposes. First, our report is written to investigate \nthe reproducibility of the submitted paper On the regularization of Wasserstein \nGANs (2018). Second, among the experiments performed in the submitted paper, \nfive aspects were emphasized and reproduced: learning speed, stability, \nrobustness against hyperparameter, estimating the Wasserstein distance, and \nvarious sampling method. Finally, we identify which parts of the contribution \ncan be reproduced, and at what cost in terms of resources. All source code for \nreproduction is open to the public. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eacda", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05882"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ryan Turner, Brady Neal", "title": "How well does your sampler really work?. (arXiv:1712.06006v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06006", "type": "text/html"}], "timestampUsec": "1513669229487531", "comments": [], "summary": {"content": "<p>We present a new data-driven benchmark system to evaluate the performance of \nnew MCMC samplers. Taking inspiration from the COCO benchmark in optimization, \nwe view this task as having critical importance to machine learning and \nstatistics given the rate at which new samplers are proposed. The common \nhand-crafted examples to test new samplers are unsatisfactory; we take a \nmeta-learning-like approach to generate benchmark examples from a large corpus \nof data sets and models. Surrogates of posteriors found in real problems are \ncreated using highly flexible density models including modern neural network \nbased approaches. We provide new insights into the real effective sample size \nof various samplers per unit time and the estimation efficiency of the samplers \nper sample. Additionally, we provide a meta-analysis to assess the predictive \nutility of various MCMC diagnostics and perform a nonparametric regression to \ncombine them. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eacde", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06006"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David Alvarez-Melis, Tommi S. Jaakkola, Stefanie Jegelka", "title": "Structured Optimal Transport. (arXiv:1712.06199v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06199", "type": "text/html"}], "timestampUsec": "1513669229487530", "comments": [], "summary": {"content": "<p>Optimal Transport has recently gained interest in machine learning for \napplications ranging from domain adaptation, sentence similarities to deep \nlearning. Yet, its ability to capture frequently occurring structure beyond the \n\"ground metric\" is limited. In this work, we develop a nonlinear generalization \nof (discrete) optimal transport that is able to reflect much additional \nstructure. We demonstrate how to leverage the geometry of this new model for \nfast algorithms, and explore connections and properties. Illustrative \nexperiments highlight the benefit of the induced structured couplings for tasks \nin domain adaptation and natural language processing. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eace6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06199"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Guoqing Chao, Shiliang Sun, Jinbo Bi", "title": "A Survey on Multi-View Clustering. (arXiv:1712.06246v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.06246", "type": "text/html"}], "timestampUsec": "1513669229487529", "comments": [], "summary": {"content": "<p>With the fast development of information technology, especially the \npopularization of internet, multi-view learning becomes more and more popular \nin machine learning and data mining fields. As we all know that, multi-view \nsemi-supervised learning, such as co-training, co-regularization has gained \nconsiderable attentions. Although recently, multi-view clustering (MVC) has \ndeveloped rapidly, there are not a survey or review to summarize and analyze \nthe current progress. Therefore, this paper sums up the common strategies of \ncombining multiple views and based on that we proposed a novel taxonomy of the \nMVC approaches. We also discussed the relationships between MVC and multi-view \nrepresentation, ensemble clustering, multi-task clustering, multi-view \nsupervised and multi-view semi-supervised learning. Several representative \nreal-world applications are elaborated. To promote the further development of \nMVC, we pointed out several open problems that are worth exploring in the \nfuture. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eaceb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06246"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Luca Franceschi (1 and 2), Michele Donini (2), Paolo Frasconi (3), Massimiliano Pontil (1 and 2) ((1) University College London, (2) Istituto Italiano di Teconologia, (3) Universita&#x27; degli Studi di Firenze)", "title": "A Bridge Between Hyperparameter Optimization and Larning-to-learn. (arXiv:1712.06283v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.06283", "type": "text/html"}], "timestampUsec": "1513669229487528", "comments": [], "summary": {"content": "<p>We consider a class of a nested optimization problems involving inner and \nouter objectives. We observe that by taking into explicit account the \noptimization dynamics for the inner objective it is possible to derive a \ngeneral framework that unifies gradient-based hyperparameter optimization and \nmeta-learning (or learning-to-learn). Depending on the specific setting, the \nvariables of the outer objective take either the meaning of hyperparameters in \na supervised learning problem or parameters of a meta-learner. We show that \nsome recently proposed methods in the latter setting can be instantiated in our \nframework and tackled with the same gradient-based algorithms. Finally, we \ndiscuss possible design patterns for learning-to-learn and present encouraging \npreliminary experiments for few-shot learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eacef", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06283"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Siyuan Ma, Raef Bassily, Mikhail Belkin", "title": "The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning. (arXiv:1712.06559v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.06559", "type": "text/html"}], "timestampUsec": "1513669229487527", "comments": [], "summary": {"content": "<p>Stochastic Gradient Descent (SGD) with small mini-batch is a key component in \nmodern large-scale machine learning. However, its efficiency has not been easy \nto analyze as most theoretical results require adaptive rates and show \nconvergence rates far slower than that for gradient descent, making \ncomputational comparisons difficult. \n</p> \n<p>In this paper we aim to clarify the issue of fast SGD convergence. The key \nobservation is that most modern architectures are over-parametrized and are \ntrained to interpolate the data by driving the empirical loss (classification \nand regression) close to zero. While it is still unclear why these interpolated \nsolutions perform well on test data, these regimes allow for very fast \nconvergence of SGD, comparable in the number of iterations to gradient descent. \n</p> \n<p>Specifically, consider the setting with quadratic objective function, or near \na minimum, where the quadratic term is dominant. We show that: (1) Mini-batch \nsize $1$ with constant step size is optimal in terms of computations to achieve \na given error. (2) There is a critical mini-batch size such that: (a. linear \nscaling) SGD iteration with mini-batch size $m$ smaller than the critical size \nis nearly equivalent to $m$ iterations of mini-batch size $1$. (b. saturation) \nSGD iteration with mini-batch larger than the critical size is nearly \nequivalent to a gradient descent step. \n</p> \n<p>The critical mini-batch size can be viewed as the limit for effective \nmini-batch parallelization. It is also nearly independent of the data size, \nimplying $O(n)$ acceleration over GD per unit of computation. \n</p> \n<p>We give experimental evidence on real data, with the results closely \nfollowing our theoretical analyses. \n</p> \n<p>Finally, we show how the interpolation perspective and our results fit with \nrecent developments in training deep neural networks and discuss connections to \nadaptive rates for SGD and variance reduction. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513669229488", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034a3eacf4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.06559"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Saptarshi Sengupta, Sanchita Basak, Richard Alan Peters II", "title": "Data Clustering using a Hybrid of Fuzzy C-Means and Quantum-behaved Particle Swarm Optimization. (arXiv:1712.05512v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.05512", "type": "text/html"}], "timestampUsec": "1513575516304230", "comments": [], "summary": {"content": "<p>Fuzzy clustering has become a widely used data mining technique and plays an \nimportant role in grouping, traversing and selectively using data for user \nspecified applications. The deterministic Fuzzy C-Means (FCM) algorithm may \nresult in suboptimal solutions when applied to multidimensional data in \nreal-world, time-constrained problems. In this paper the Quantum-behaved \nParticle Swarm Optimization (QPSO) with a fully connected topology is coupled \nwith the Fuzzy C-Means Clustering algorithm and is tested on a suite of \ndatasets from the UCI Machine Learning Repository. The global search ability of \nthe QPSO algorithm helps in avoiding stagnation in local optima while the soft \nclustering approach of FCM helps to partition data based on membership \nprobabilities. Clustering performance indices such as F-Measure, Accuracy, \nQuantization Error, Intercluster and Intracluster distances are reported for \ncompetitive techniques such as PSO K-Means, QPSO K-Means and QPSO FCM over all \ndatasets considered. Experimental results indicate that QPSO FCM provides \ncomparable and in most cases superior results when compared to the others. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d367", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05512"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Altaf H. Khan", "title": "Lightweight Neural Networks. (arXiv:1712.05695v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.05695", "type": "text/html"}], "timestampUsec": "1513575516304229", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050aa120d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050aa120d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Most of the weights in a Lightweight Neural Network have a value of zero, \nwhile the remaining ones are either +1 or -1. These universal approximators \nrequire approximately 1.1 bits/weight of storage, posses a quick forward pass \nand achieve classification accuracies similar to conventional continuous-weight \nnetworks. Their training regimen focuses on error reduction initially, but \nlater emphasizes discretization of weights. They ignore insignificant inputs, \nremove unnecessary weights, and drop unneeded hidden neurons. We have \nsuccessfully tested them on the MNIST, credit card fraud, and credit card \ndefaults data sets using networks having 2 to 16 hidden layers and up to 4.4 \nmillion weights. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d387", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05695"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Huda Hakami, Danushka Bollegala, Hayashi Kohei", "title": "Why PairDiff works? -- A Mathematical Analysis of Bilinear Relational Compositional Operators for Analogy Detection. (arXiv:1709.06673v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.06673", "type": "text/html"}], "timestampUsec": "1513575516304228", "comments": [], "summary": {"content": "<p>Representing the semantic relations that exist between two given words (or \nentities) is an important first step in a wide-range of NLP applications such \nas analogical reasoning, knowledge base completion and relational information \nretrieval. A simple, yet surprisingly accurate method for representing a \nrelation between two words is to compute the vector offset (\\PairDiff) between \ntheir corresponding word embeddings. Despite the empirical success, it remains \nunclear as to whether \\PairDiff is the best operator for obtaining a relational \nrepresentation from word embeddings. We conduct a theoretical analysis of \ngeneralised bilinear operators that can be used to measure the $\\ell_{2}$ \nrelational distance between two word-pairs. We show that, if the word \nembeddings are standardised and uncorrelated, such an operator will be \nindependent of bilinear terms, and can be simplified to a linear form, where \n\\PairDiff is a special case. For numerous word embedding types, we empirically \nverify the uncorrelation assumption, demonstrating the general applicability of \nour theoretical result. Moreover, we experimentally discover \\PairDiff from the \nbilinear relation composition operator on several benchmark analogy datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d399", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.06673"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Guillaume Bellec, David Kappel, Wolfgang Maass, Robert Legenstein", "title": "Deep Rewiring: Training very sparse deep networks. (arXiv:1711.05136v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05136", "type": "text/html"}], "timestampUsec": "1513575516304227", "comments": [], "summary": {"content": "<p>Neuromorphic hardware tends to pose limits on the connectivity of deep \nnetworks that one can run on them. But also generic hardware and software \nimplementations of deep learning run more efficiently on sparse networks. \nSeveral methods exist for pruning connections of a neural network after it was \ntrained without connectivity constraints. We present an algorithm, DEEP R, that \nenables us to train directly a sparsely connected neural network. DEEP R \nautomatically rewires the network during supervised training so that \nconnections are there where they are most needed for the task, while its total \nnumber is all the time strictly bounded. We demonstrate that DEEP R can be used \nto train very sparse feedforward and recurrent neural networks on standard \nbenchmark tasks with just a minor loss in performance. DEEP R is based on a \nrigorous theoretical foundation that views rewiring as stochastic sampling of \nnetwork configurations from a posterior. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d3ae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05136"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yi Tay, Anh Tuan Luu, Siu Cheung Hui", "title": "Learning to Attend via Word-Aspect Associative Fusion for Aspect-based Sentiment Analysis. (arXiv:1712.05403v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.05403", "type": "text/html"}], "timestampUsec": "1513575516304226", "comments": [], "summary": {"content": "<p>Aspect-based sentiment analysis (ABSA) tries to predict the polarity of a \ngiven document with respect to a given aspect entity. While neural network \narchitectures have been successful in predicting the overall polarity of \nsentences, aspect-specific sentiment analysis still remains as an open problem. \nIn this paper, we propose a novel method for integrating aspect information \ninto the neural model. More specifically, we incorporate aspect information \ninto the neural model by modeling word-aspect relationships. Our novel model, \n\\textit{Aspect Fusion LSTM} (AF-LSTM) learns to attend based on associative \nrelationships between sentence words and aspect which allows our model to \nadaptively focus on the correct words given an aspect term. This ameliorates \nthe flaws of other state-of-the-art models that utilize naive concatenations to \nmodel word-aspect similarity. Instead, our model adopts circular convolution \nand circular correlation to model the similarity between aspect and words and \nelegantly incorporates this within a differentiable neural attention framework. \nFinally, our model is end-to-end differentiable and highly related to \nconvolution-correlation (holographic like) memories. Our proposed neural model \nachieves state-of-the-art performance on benchmark datasets, outperforming \nATAE-LSTM by $4\\%-5\\%$ on average across multiple datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d3c9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05403"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi", "title": "AI2-THOR: An Interactive 3D Environment for Visual AI. (arXiv:1712.05474v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.05474", "type": "text/html"}], "timestampUsec": "1513575516304225", "comments": [], "summary": {"content": "<p>We introduce The House Of inteRactions (THOR), a framework for visual AI \nresearch, available at <a href=\"http://ai2thor.allenai.org.\">this http URL</a> AI2-THOR consists of near \nphoto-realistic 3D indoor scenes, where AI agents can navigate in the scenes \nand interact with objects to perform tasks. AI2-THOR enables research in many \ndifferent domains including but not limited to deep reinforcement learning, \nimitation learning, learning by interaction, planning, visual question \nanswering, unsupervised representation learning, object detection and \nsegmentation, and learning models of cognition. The goal of AI2-THOR is to \nfacilitate building visually intelligent models and push the research forward \nin this domain. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d3da", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05474"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ashwin Khadke, Manuela Veloso", "title": "What Can This Robot Do? Learning from Appearance and Experiments. (arXiv:1712.05497v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.05497", "type": "text/html"}], "timestampUsec": "1513575516304224", "comments": [], "summary": {"content": "<p>When presented with an unknown robot (subject) how can an autonomous agent \n(learner) figure out what this new robot can do? The subject's appearance can \nprovide cues to its physical as well as cognitive capabilities. Seeing a \nhumanoid can make one wonder if it can kick balls, climb stairs or recognize \nfaces. What if the learner can request the subject to perform these tasks? We \npresent an approach to make the learner build a model of the subject at a task \nbased on the latter's appearance and refine it by experimentation. Apart from \nthe subject's inherent capabilities, certain extrinsic factors may affect its \nperformance at a task. Based on the subject's appearance and prior knowledge \nabout the task a learner can identify a set of potential factors, a subset of \nwhich we assume are controllable. Our approach picks values of controllable \nfactors to generate the most informative experiments to test the subject at. \nAdditionally, we present a metric to determine if a factor should be \nincorporated in the model. We present results of our approach on modeling a \nhumanoid robot at the task of kicking a ball. Firstly, we show that actively \npicking values for controllable factors, even in noisy experiments, leads to \nfaster learning of the subject's model for the task. Secondly, starting from a \nminimal set of factors our metric identifies the set of relevant factors to \nincorporate in the model. Lastly, we show that the refined model better \nrepresents the subject's performance at the task. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d3e2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05497"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Siddharthan Rajasekaran, Jinwei Zhang, Jie Fu", "title": "Inverse Reinforce Learning with Nonparametric Behavior Clustering. (arXiv:1712.05514v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.05514", "type": "text/html"}], "timestampUsec": "1513575516304223", "comments": [], "summary": {"content": "<p>Inverse Reinforcement Learning (IRL) is the task of learning a single reward \nfunction given a Markov Decision Process (MDP) without defining the reward \nfunction, and a set of demonstrations generated by humans/experts. However, in \npractice, it may be unreasonable to assume that human behaviors can be \nexplained by one reward function since they may be inherently inconsistent. \nAlso, demonstrations may be collected from various users and aggregated to \ninfer and predict user's behaviors. In this paper, we introduce the \nNon-parametric Behavior Clustering IRL algorithm to simultaneously cluster \ndemonstrations and learn multiple reward functions from demonstrations that may \nbe generated from more than one behaviors. Our method is iterative: It \nalternates between clustering demonstrations into different behavior clusters \nand inverse learning the reward functions until convergence. It is built upon \nthe Expectation-Maximization formulation and non-parametric clustering in the \nIRL setting. Further, to improve the computation efficiency, we remove the need \nof completely solving multiple IRL problems for multiple clusters during the \niteration steps and introduce a resampling technique to avoid generating too \nmany unlikely clusters. We demonstrate the convergence and efficiency of the \nproposed method through learning multiple driver behaviors from demonstrations \ngenerated from a grid-world environment and continuous trajectories collected \nfrom autonomous robot cars using the Gazebo robot simulator. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d3e9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05514"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jin-Hwa Kim, Devi Parikh, Dhruv Batra, Byoung-Tak Zhang, Yuandong Tian", "title": "CoDraw: Visual Dialog for Collaborative Drawing. (arXiv:1712.05558v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.05558", "type": "text/html"}], "timestampUsec": "1513575516304222", "comments": [], "summary": {"content": "<p>In this work, we propose a goal-driven collaborative task that contains \nvision, language, and action in a virtual environment as its core components. \nSpecifically, we develop a collaborative `Image Drawing' game between two \nagents, called CoDraw. Our game is grounded in a virtual world that contains \nmovable clip art objects. Two players, Teller and Drawer, are involved. The \nTeller sees an abstract scene containing multiple clip arts in a semantically \nmeaningful configuration, while the Drawer tries to reconstruct the scene on an \nempty canvas using available clip arts. The two players communicate via two-way \ncommunication using natural language. We collect the CoDraw dataset of ~10K \ndialogs consisting of 138K messages exchanged between a Teller and a Drawer \nfrom Amazon Mechanical Turk (AMT). We analyze our dataset and present three \nmodels to model the players' behaviors, including an attention model to \ndescribe and draw multiple clip arts at each round. The attention models are \nquantitatively compared to the other models to show how the conventional \napproaches work for this new task. We also present qualitative visualizations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d3fb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05558"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "V.I. Yukalov, E.P. Yukalova, D. Sornette", "title": "Information Processing by Networks of Quantum Decision Makers. (arXiv:1712.05734v1 [physics.soc-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.05734", "type": "text/html"}], "timestampUsec": "1513575516304221", "comments": [], "summary": {"content": "<p>We suggest a model of a multi-agent society of decision makers taking \ndecisions being based on two criteria, one is the utility of the prospects and \nthe other is the attractiveness of the considered prospects. The model is the \ngeneralization of quantum decision theory, developed earlier for single \ndecision makers realizing one-step decisions, in two principal aspects. First, \nseveral decision makers are considered simultaneously, who interact with each \nother through information exchange. Second, a multistep procedure is treated, \nwhen the agents exchange information many times. Several decision makers \nexchanging information and forming their judgement, using quantum rules, form a \nkind of a quantum information network, where collective decisions develop in \ntime as a result of information exchange. In addition to characterizing \ncollective decisions that arise in human societies, such networks can describe \ndynamical processes occurring in artificial quantum intelligence composed of \nseveral parts or in a cluster of quantum computers. The practical usage of the \ntheory is illustrated on the dynamic disjunction effect for which three \nquantitative predictions are made: (i) the probabilistic behavior of decision \nmakers at the initial stage of the process is described; (ii) the decrease of \nthe difference between the initial prospect probabilities and the related \nutility factors is proved; (iii) the existence of a common consensus after \nmultiple exchange of information is predicted. The predicted numerical values \nare in very good agreement with empirical data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d410", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05734"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jordan Prosky, Xingyou Song, Andrew Tan, Michael Zhao", "title": "Sentiment Predictability for Stocks. (arXiv:1712.05785v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.05785", "type": "text/html"}], "timestampUsec": "1513575516304220", "comments": [], "summary": {"content": "<p>In this work, we present our findings and experiments for stock-market \nprediction using various textual sentiment analysis tools, such as mood \nanalysis and event extraction, as well as prediction models, such as LSTMs and \nspecific convolutional architectures. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d41c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05785"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Martin C. Cooper, Stanislav &#x17d;ivn&#xfd;", "title": "The Power of Arc Consistency for CSPs Defined by Partially-Ordered Forbidden Patterns. (arXiv:1604.07981v3 [cs.CC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1604.07981", "type": "text/html"}], "timestampUsec": "1513575516304219", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050aa1436\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050aa1436&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Characterising tractable fragments of the constraint satisfaction problem \n(CSP) is an important challenge in theoretical computer science and artificial \nintelligence. Forbidding patterns (generic sub-instances) provides a means of \ndefining CSP fragments which are neither exclusively language-based nor \nexclusively structure-based. It is known that the class of binary CSP instances \nin which the broken-triangle pattern (BTP) does not occur, a class which \nincludes all tree-structured instances, are decided by arc consistency (AC), a \nubiquitous reduction operation in constraint solvers. We provide a \ncharacterisation of simple partially-ordered forbidden patterns which have this \nAC-solvability property. It turns out that BTP is just one of five such \nAC-solvable patterns. The four other patterns allow us to exhibit new tractable \nclasses. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d430", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1604.07981"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sam Kriegman, Nick Cheney, Josh Bongard", "title": "How morphological development can guide evolution. (arXiv:1711.07387v2 [q-bio.PE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07387", "type": "text/html"}], "timestampUsec": "1513575516304218", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050b031c6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050b031c6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Organisms result from multiple adaptive processes occurring and interacting \nat different time scales. One such interaction is that between development and \nevolution. In modeling studies, it has been shown that development sweeps over \na series of traits in a single agent, and sometimes exposes promising static \ntraits. Subsequent evolution can then canalize these rare traits. Thus, \ndevelopment can, under the right conditions, increase evolvability. Here, we \nreport on a previously unknown phenomenon when embodied agents are allowed to \ndevelop and evolve: Evolution discovers body plans which are robust to control \nchanges, these body plans become genetically assimilated, yet controllers for \nthese agents are not assimilated. This allows evolution to continue climbing \nfitness gradients by tinkering with the developmental programs for controllers \nwithin these permissive body plans. This exposes a previously unknown detail \nabout the Baldwin effect: instead of all useful traits becoming genetically \nassimilated, only phenotypic traits that render the agent robust to changes in \nother traits become assimilated. We refer to this phenomenon as differential \ncanalization. This finding also has important implications for the evolutionary \ndesign of artificial and embodied agents such as robots: robots that are robust \nto internal changes in their controllers may also be robust to external changes \nin their environment, such as transferal from simulation to reality, or \ndeployment in novel environments. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d43c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07387"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Atsushi Nitanda, Taiji Suzuki", "title": "Stochastic Particle Gradient Descent for Infinite Ensembles. (arXiv:1712.05438v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.05438", "type": "text/html"}], "timestampUsec": "1513575516304216", "comments": [], "summary": {"content": "<p>The superior performance of ensemble methods with infinite models are well \nknown. Most of these methods are based on optimization problems in \ninfinite-dimensional spaces with some regularization, for instance, boosting \nmethods and convex neural networks use $L^1$-regularization with the \nnon-negative constraint. However, due to the difficulty of handling \n$L^1$-regularization, these problems require early stopping or a rough \napproximation to solve it inexactly. In this paper, we propose a new ensemble \nlearning method that performs in a space of probability measures, that is, our \nmethod can handle the $L^1$-constraint and the non-negative constraint in a \nrigorous way. Such an optimization is realized by proposing a general purpose \nstochastic optimization method for learning probability measures via \nparameterization using transport maps on base models. As a result of running \nthe method, a transport map to output an infinite ensemble is obtained, which \nforms a residual-type network. From the perspective of functional gradient \nmethods, we give a convergence rate as fast as that of a stochastic \noptimization method for finite dimensional nonconvex problems. Moreover, we \nshow an interior optimality property of a local optimality condition used in \nour analysis. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d449", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05438"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexander LeNail, Ludwig Schmidt, Johnathan Li, Tobias Ehrenberger, Karen Sachs, Stefanie Jegelka, Ernest Fraenkel", "title": "Graph-Sparse Logistic Regression. (arXiv:1712.05510v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.05510", "type": "text/html"}], "timestampUsec": "1513575516304215", "comments": [], "summary": {"content": "<p>We introduce Graph-Sparse Logistic Regression, a new algorithm for \nclassification for the case in which the support should be sparse but connected \non a graph. We val- idate this algorithm against synthetic data and benchmark \nit against L1-regularized Logistic Regression. We then explore our technique in \nthe bioinformatics context of proteomics data on the interactome graph. We make \nall our experimental code public and provide GSLR as an open source package. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d45a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05510"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kyriakos Polymenakos, Alessandro Abate, Stephen Roberts", "title": "Safe Policy Search with Gaussian Process Models. (arXiv:1712.05556v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.05556", "type": "text/html"}], "timestampUsec": "1513575516304214", "comments": [], "summary": {"content": "<p>We propose a method to optimise the parameters of a policy which will be used \nto safely perform a given task in a data-efficient manner. We train a Gaussian \nprocess model to capture the system dynamics, based on the PILCO framework. Our \nmodel has useful analytic properties, which allow closed form computation of \nerror gradients and estimating the probability of violating given state space \nconstraints. During training, as well as operation, only policies that are \ndeemed safe are implemented on the real system, minimising the risk of failure. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d467", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05556"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Milana Gataric, Tengyao Wang, Richard J. Samworth", "title": "Sparse principal component analysis via random projections. (arXiv:1712.05630v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.05630", "type": "text/html"}], "timestampUsec": "1513575516304213", "comments": [], "summary": {"content": "<p>We introduce a new method for sparse principal component analysis, based on \nthe aggregation of eigenvector information from carefully-selected random \nprojections of the sample covariance matrix. Unlike most alternative \napproaches, our algorithm is non-iterative, so is not vulnerable to a bad \nchoice of initialisation. Our theory provides great detail on the statistical \nand computational trade-off in our procedure, revealing a subtle interplay \nbetween the effective sample size and the number of random projections that are \nrequired to achieve the minimax optimal rate. Numerical studies provide further \ninsight into the procedure and confirm its highly competitive finite-sample \nperformance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d472", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05630"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hongzhou Lin, Julien Mairal, Zaid Harchaoui", "title": "Catalyst Acceleration for First-order Convex Optimization: from Theory to Practice. (arXiv:1712.05654v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.05654", "type": "text/html"}], "timestampUsec": "1513575516304212", "comments": [], "summary": {"content": "<p>We introduce a generic scheme for accelerating gradient-based optimization \nmethods in the sense of Nesterov. The approach, called Catalyst, builds upon \nthe inexact acceler- ated proximal point algorithm for minimizing a convex \nobjective function, and consists of approximately solving a sequence of \nwell-chosen auxiliary problems, leading to faster convergence. One of the key \nto achieve acceleration in theory and in practice is to solve these \nsub-problems with appropriate accuracy by using the right stopping criterion \nand the right warm-start strategy. In this paper, we give practical guidelines \nto use Catalyst and present a comprehensive theoretical analysis of its global \ncomplexity. We show that Catalyst applies to a large class of algorithms, \nincluding gradient descent, block coordinate descent, incremental algorithms \nsuch as SAG, SAGA, SDCA, SVRG, Finito/MISO, and their proximal variants. For \nall of these methods, we provide acceleration and explicit sup- port for \nnon-strongly convex objectives. We conclude with extensive experiments showing \nthat acceleration is useful in practice, especially for ill-conditioned \nproblems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d47a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05654"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Guangxi Li, Jinmian Ye, Haiqin Yang, Di Chen, Shuicheng Yan, Zenglin Xu", "title": "BT-Nets: Simplifying Deep Neural Networks via Block Term Decomposition. (arXiv:1712.05689v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.05689", "type": "text/html"}], "timestampUsec": "1513575516304211", "comments": [], "summary": {"content": "<p>Recently, deep neural networks (DNNs) have been regarded as the \nstate-of-the-art classification methods in a wide range of applications, \nespecially in image classification. Despite the success, the huge number of \nparameters blocks its deployment to situations with light computing resources. \nResearchers resort to the redundancy in the weights of DNNs and attempt to find \nhow fewer parameters can be chosen while preserving the accuracy at the same \ntime. Although several promising results have been shown along this research \nline, most existing methods either fail to significantly compress a \nwell-trained deep network or require a heavy fine-tuning process for the \ncompressed network to regain the original performance. In this paper, we \npropose the \\textit{Block Term} networks (BT-nets) in which the commonly used \nfully-connected layers (FC-layers) are replaced with block term layers \n(BT-layers). In BT-layers, the inputs and the outputs are reshaped into two \nlow-dimensional high-order tensors, then block-term decomposition is applied as \ntensor operators to connect them. We conduct extensive experiments on benchmark \ndatasets to demonstrate that BT-layers can achieve a very large compression \nratio on the number of parameters while preserving the representation power of \nthe original FC-layers as much as possible. Specifically, we can get a higher \nperformance while requiring fewer parameters compared with the tensor train \nmethod. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d482", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05689"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Felix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem Sokolov, Ann Clifton, Matt Post", "title": "Sockeye: A Toolkit for Neural Machine Translation. (arXiv:1712.05690v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.05690", "type": "text/html"}], "timestampUsec": "1513575516304210", "comments": [], "summary": {"content": "<p>We describe Sockeye (version 1.12), an open-source sequence-to-sequence \ntoolkit for Neural Machine Translation (NMT). Sockeye is a production-ready \nframework for training and applying models as well as an experimental platform \nfor researchers. Written in Python and built on MXNet, the toolkit offers \nscalable training and inference for the three most prominent encoder-decoder \narchitectures: attentional recurrent neural networks, self-attentional \ntransformers, and fully convolutional networks. Sockeye also supports a wide \nrange of optimizers, normalization and regularization techniques, and inference \nimprovements from current NMT literature. Users can easily run standard \ntraining recipes, explore different model settings, and incorporate new ideas. \nIn this paper, we highlight Sockeye's features and benchmark it against other \nNMT toolkits on two language arcs from the 2017 Conference on Machine \nTranslation (WMT): English-German and Latvian-English. We report competitive \nBLEU scores across all three architectures, including an overall best score for \nSockeye's transformer implementation. To facilitate further comparison, we \nrelease all system outputs and training scripts used in our experiments. The \nSockeye toolkit is free software released under the Apache 2.0 license. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d48e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05690"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Brian Bierig, Jonathan Hollenbeck, Alexander Stroud", "title": "Understanding Career Progression in Baseball Through Machine Learning. (arXiv:1712.05754v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.05754", "type": "text/html"}], "timestampUsec": "1513575516304209", "comments": [], "summary": {"content": "<p>Professional baseball players are increasingly guaranteed expensive long-term \ncontracts, with over 70 deals signed in excess of \\$90 million, mostly in the \nlast decade. These are substantial sums compared to a typical franchise \nvaluation of \\$1-2 billion. Hence, the players to whom a team chooses to give \nsuch a contract can have an enormous impact on both competitiveness and profit. \nDespite this, most published approaches examining career progression in \nbaseball are fairly simplistic. We applied four machine learning algorithms to \nthe problem and soundly improved upon existing approaches, particularly for \nbatting data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d49b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05754"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Cl&#xe9;ment Godard, Kevin Matzen, Matt Uyttendaele", "title": "Deep Burst Denoising. (arXiv:1712.05790v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.05790", "type": "text/html"}], "timestampUsec": "1513575516304208", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050b0363b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050b0363b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Noise is an inherent issue of low-light image capture, one which is \nexacerbated on mobile devices due to their narrow apertures and small sensors. \nOne strategy for mitigating noise in a low-light situation is to increase the \nshutter time of the camera, thus allowing each photosite to integrate more \nlight and decrease noise variance. However, there are two downsides of long \nexposures: (a) bright regions can exceed the sensor range, and (b) camera and \nscene motion will result in blurred images. Another way of gathering more light \nis to capture multiple short (thus noisy) frames in a \"burst\" and intelligently \nintegrate the content, thus avoiding the above downsides. In this paper, we use \nthe burst-capture strategy and implement the intelligent integration via a \nrecurrent fully convolutional deep neural net (CNN). We build our novel, \nmultiframe architecture to be a simple addition to any single frame denoising \nmodel, and design to handle an arbitrary number of noisy input frames. We show \nthat it achieves state of the art denoising results on our burst dataset, \nimproving on the best published multi-frame techniques, such as VBM4D and \nFlexISP. Finally, we explore other applications of image enhancement by \nintegrating content from multiple frames and demonstrate that our DNN \narchitecture generalizes well to image super-resolution. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d4ae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05790"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Simon Lee Dettmer, Johannes Berg", "title": "Inferring the parameters of a Markov process from snapshots of the steady state. (arXiv:1707.04114v3 [cond-mat.stat-mech] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.04114", "type": "text/html"}], "timestampUsec": "1513575516304207", "comments": [], "summary": {"content": "<p>We seek to infer the parameters of an ergodic Markov process from samples \ntaken independently from the steady state. Our focus is on non-equilibrium \nprocesses, where the steady state is not described by the Boltzmann measure, \nbut is generally unknown and hard to compute, which prevents the application of \nestablished equilibrium inference methods. We propose a quantity we call \npropagator likelihood, which takes on the role of the likelihood in equilibrium \nprocesses. This propagator likelihood is based on fictitious transitions \nbetween those configurations of the system which occur in the samples. The \npropagator likelihood can be derived by minimising the relative entropy between \nthe empirical distribution and a distribution generated by propagating the \nempirical distribution forward in time. Maximising the propagator likelihood \nleads to an efficient reconstruction of the parameters of the underlying model \nin different systems, both with discrete configurations and with continuous \nconfigurations. We apply the method to non-equilibrium models from statistical \nphysics and theoretical biology, including the asymmetric simple exclusion \nprocess (ASEP), the kinetic Ising model, and replicator dynamics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d4b5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.04114"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alejandro Cholaquidis, Ricardo Fraiman, Mariela Sued", "title": "Semi-supervised learning. (arXiv:1709.05673v2 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.05673", "type": "text/html"}], "timestampUsec": "1513575516304206", "comments": [], "summary": {"content": "<p>Semi-supervised learning deals with the problem of how, if possible, to take \nadvantage of a huge amount of not classified data, to perform classification, \nin situations when, typically, the labelled data are few. Even though this is \nnot always possible (it depends on how useful is to know the distribution of \nthe unlabelled data in the inference of the labels), several algorithm have \nbeen proposed recently. A new algorithm is proposed, that under almost \nneccesary conditions, attains asymptotically the performance of the best \ntheoretical rule, when the size of unlabeled data tends to infinity. The set of \nnecessary assumptions, although reasonables, show that semi-parametric \nclassification only works for very well conditioned problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d4bd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.05673"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Freweyni K. Teklehaymanot, Michael Muma, Abdelhak M. Zoubir", "title": "A Novel Bayesian Cluster Enumeration Criterion for Unsupervised Learning. (arXiv:1710.07954v2 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.07954", "type": "text/html"}], "timestampUsec": "1513575516304205", "comments": [], "summary": {"content": "<p>We derive a new Bayesian Information Criterion (BIC) from first principles by \nformulating the problem of estimating the number of clusters in an observed \ndata set as maximization of the posterior probability of the candidate models. \nGiven that some mild assumptions are satisfied, we provide a general BIC \nexpression for a broad class of data distributions. This serves as an important \nmilestone when deriving the BIC for specific data distributions. Along this \nline, we provide a closed-form BIC expression for multivariate Gaussian \ndistributed observations. We show that incorporating data structure of the \nclustering problem into the derivation of the BIC results in an expression \nwhose penalty term is different from that of the original BIC. We propose a \ntwo-step cluster enumeration algorithm. First, a model-based unsupervised \nlearning algorithm partitions the data according to a given set of candidate \nmodels. Subsequently, the optimal cluster number is determined as the one \nassociated to the model for which the proposed BIC is maximal. The performance \nof the proposed criterion is tested using synthetic and real data sets. Despite \nthe fact that the original BIC is a generic criterion which does not include \ninformation about the specific model selection problem at hand, it has been \nwidely used in the literature to estimate the number of clusters in an observed \ndata set. We, therefore, consider it as a benchmark comparison. Simulation \nresults show that our proposed criterion outperforms the existing cluster \nenumeration methods that are based on the original BIC. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d4d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.07954"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kay Gregor Hartmann, Robin Tibor Schirrmeister, Tonio Ball", "title": "Hierarchical internal representation of spectral features in deep convolutional networks trained for EEG decoding. (arXiv:1711.07792v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07792", "type": "text/html"}], "timestampUsec": "1513575516304204", "comments": [], "summary": {"content": "<p>Recently, there is increasing interest and research on the interpretability \nof machine learning models, for example how they transform and internally \nrepresent EEG signals in Brain-Computer Interface (BCI) applications. This can \nhelp to understand the limits of the model and how it may be improved, in \naddition to possibly provide insight about the data itself. Schirrmeister et \nal. (2017) have recently reported promising results for EEG decoding with deep \nconvolutional neural networks (ConvNets) trained in an end-to-end manner and, \nwith a causal visualization approach, showed that they learn to use spectral \namplitude changes in the input. In this study, we investigate how ConvNets \nrepresent spectral features through the sequence of intermediate stages of the \nnetwork. We show higher sensitivity to EEG phase features at earlier stages and \nhigher sensitivity to EEG amplitude features at later stages. Intriguingly, we \nobserved a specialization of individual stages of the network to the classical \nEEG frequency bands alpha, beta, and high gamma. Furthermore, we find first \nevidence that particularly in the last convolutional layer, the network learns \nto detect more complex oscillatory patterns beyond spectral phase and \namplitude, reminiscent of the representation of complex visual features in \nlater layers of ConvNets in computer vision tasks. Our findings thus provide \ninsights into how ConvNets hierarchically represent spectral EEG features in \ntheir intermediate layers and suggest that ConvNets can exploit and might help \nto better understand the compositional structure of EEG time series. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d4ed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07792"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yafeng Liu, Shimin Feng, Zhikai Zhao, Enjie Ding", "title": "Highly Efficient Human Action Recognition with Quantum Genetic Algorithm Optimized Support Vector Machine. (arXiv:1711.09511v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09511", "type": "text/html"}], "timestampUsec": "1513575516304203", "comments": [], "summary": {"content": "<p>In this paper we propose the use of quantum genetic algorithm to optimize the \nsupport vector machine (SVM) for human action recognition. The Microsoft Kinect \nsensor can be used for skeleton tracking, which provides the joints' position \ndata. However, how to extract the motion features for representing the dynamics \nof a human skeleton is still a challenge due to the complexity of human motion. \nWe present a highly efficient features extraction method for action \nclassification, that is, using the joint angles to represent a human skeleton \nand calculating the variance of each angle during an action time window. Using \nthe proposed representation, we compared the human action classification \naccuracy of two approaches, including the optimized SVM based on quantum \ngenetic algorithm and the conventional SVM with grid search. Experimental \nresults on the MSR-12 dataset show that the conventional SVM achieved an \naccuracy of $ 93.85\\% $. The proposed approach outperforms the conventional \nmethod with an accuracy of $ 96.15\\% $. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513575516304", "annotations": [], "published": 1513575517, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034974d4fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09511"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yanan Sun, Bing Xue, Mengjie Zhang", "title": "A Particle Swarm Optimization-based Flexible Convolutional Auto-Encoder for Image Classification. (arXiv:1712.05042v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.05042", "type": "text/html"}], "timestampUsec": "1513315287371381", "comments": [], "summary": {"content": "<p>Convolutional auto-encoders have shown their remarkable performance in \nstacking to deep convolutional neural networks for classifying image data \nduring past several years. However, they are unable to construct the \nstate-of-the-art convolutional neural networks due to their intrinsic \narchitectures. In this regard, we propose a flexible convolutional auto-encoder \nby eliminating the constraints on the numbers of convolutional layers and \npooling layers from the traditional convolutional auto-encoder. We also design \nan architecture discovery method by using particle swarm optimization, which is \ncapable of automatically searching for the optimal architectures of the \nproposed flexible convolutional auto-encoder with much less computational \nresource and without any manual intervention. We use the designed architecture \noptimization algorithm to test the proposed flexible convolutional auto-encoder \nthrough utilizing one graphic processing unit card on four extensively used \nimage classification datasets. Experimental results show that our work in this \npaper significantly outperform the peer competitors including the \nstate-of-the-art algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfe38", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05042"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yanan Sun, Gary G. Yen, Zhang Yi", "title": "Evolving Unsupervised Deep Neural Networks for Learning Meaningful Representations. (arXiv:1712.05043v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.05043", "type": "text/html"}], "timestampUsec": "1513315287371380", "comments": [], "summary": {"content": "<p>Deep Learning (DL) aims at learning the \\emph{meaningful representations}. A \nmeaningful representation refers to the one that gives rise to significant \nperformance improvement of associated Machine Learning (ML) tasks by replacing \nthe raw data as the input. However, optimal architecture design and model \nparameter estimation in DL algorithms are widely considered to be intractable. \nEvolutionary algorithms are much preferable for complex and non-convex problems \ndue to its inherent characteristics of gradient-free and insensitivity to local \noptimum. In this paper, we propose a computationally economical algorithm for \nevolving \\emph{unsupervised deep neural networks} to efficiently learn \n\\emph{meaningful representations}, which is very suitable in the current Big \nData era where sufficient labeled data for training is often expensive to \nacquire. In the proposed algorithm, finding an appropriate architecture and the \ninitialized parameter values for a ML task at hand is modeled by one \ncomputational efficient gene encoding approach, which is employed to \neffectively model the task with a large number of parameters. In addition, a \nlocal search strategy is incorporated to facilitate the exploitation search for \nfurther improving the performance. Furthermore, a small proportion labeled data \nis utilized during evolution search to guarantee the learnt representations to \nbe meaningful. The performance of the proposed algorithm has been thoroughly \ninvestigated over classification tasks. Specifically, error classification rate \non MNIST with $1.15\\%$ is reached by the proposed algorithm consistently, which \nis a very promising result against state-of-the-art unsupervised DL algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfe4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05043"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "V.I. Avrutskiy", "title": "Neural networks catching up with finite differences in solving partial differential equations in higher dimensions. (arXiv:1712.05067v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.05067", "type": "text/html"}], "timestampUsec": "1513315287371379", "comments": [], "summary": {"content": "<p>Fully connected multilayer perceptrons are used for obtaining numerical \nsolutions of partial differential equations in various dimensions. Independent \nvariables are fed into the input layer, and the output is considered as \nsolution's value. To train such a network one can use square of equation's \nresidual as a cost function and minimize it with respect to weights by gradient \ndescent. Following previously developed method, derivatives of the equation's \nresidual along random directions in space of independent variables are also \nadded to cost function. Similar procedure is known to produce nearly machine \nprecision results using less than 8 grid points per dimension for 2D case. The \nsame effect is observed here for higher dimensions: solutions are obtained on \nlow density grids, but maintain their precision in the entire region. Boundary \nvalue problems for linear and nonlinear Poisson equations are solved inside 2, \n3, 4, and 5 dimensional balls. Grids for linear cases have 40, 159, 512 and \n1536 points and for nonlinear 64, 350, 1536 and 6528 points respectively. In \nall cases maximum error is less than $8.8\\cdot10^{-6}$, and median error is \nless than $2.4\\cdot10^{-6}$. Very weak grid requirements enable neural networks \nto obtain solution of 5D linear problem within 22 minutes, whereas projected \nsolving time for finite differences on the same hardware is 50 minutes. Method \nis applied to second order equation, but requires little to none modifications \nto solve systems or higher order PDEs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfe5a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05067"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Freek Stulp, Pierre-Yves Oudeyer", "title": "Proximodistal Exploration in Motor Learning as an Emergent Property of Optimization. (arXiv:1712.05249v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.05249", "type": "text/html"}], "timestampUsec": "1513315287371378", "comments": [], "summary": {"content": "<p>To harness the complexity of their high-dimensional bodies during \nsensorimotor development, infants are guided by patterns of freezing and \nfreeing of degrees of freedom. For instance, when learning to reach, infants \nfree the degrees of freedom in their arm proximodistally, i.e. from joints that \nare closer to the body to those that are more distant. Here, we formulate and \nstudy computationally the hypothesis that such patterns can emerge \nspontaneously as the result of a family of stochastic optimization processes \n(evolution strategies with covariance-matrix adaptation), without an innate \nencoding of a maturational schedule. In particular, we present simulated \nexperiments with an arm where a computational learner progressively acquires \nreaching skills through adaptive exploration, and we show that a proximodistal \norganization appears spontaneously, which we denote PDFF (ProximoDistal \nFreezing and Freeing of degrees of freedom). We also compare this emergent \norganization between different arm morphologies -- from human-like to quite \nunnatural ones -- to study the effect of different kinematic structures on the \nemergence of PDFF. Keywords: human motor learning; proximo-distal exploration; \nstochastic optimization; modelling; evolution strategies; cross-entropy \nmethods; policy search; morphology.} \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfe69", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05249"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Miguel Aguilera, Manuel G. Bedia", "title": "Adaptation to criticality through organizational invariance in embodied agents. (arXiv:1712.05284v1 [nlin.AO])", "alternate": [{"href": "http://arxiv.org/abs/1712.05284", "type": "text/html"}], "timestampUsec": "1513315287371377", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050b039fb\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050b039fb&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Many biological and cognitive systems do not operate deep within one or other \nregime of activity. Instead, they are poised at critical points located at \ntransitions of their parameter space. The pervasiveness of criticality suggests \nthat there may be general principles inducing this behaviour, yet there is no \nwell-founded theory for understanding how criticality is found at a wide range \nof levels and contexts. In this paper we present a general adaptive mechanism \nthat maintains an internal organizational structure in order to drive a system \ntowards critical points while it interacts with different environments. We \nimplement the mechanism in artificial embodied agents controlled by a neural \nnetwork maintaining a correlation structure randomly sampled from an Ising \nmodel at critical temperature. Agents are evaluated in two classical \nreinforcement learning scenarios: the Mountain Car and the Acrobot double \npendulum. In both cases the neural controller reaches a point of criticality, \nwhich coincides with a transition point between two regimes of the agent's \nbehaviour. These results suggest that adaptation to criticality could be used \nas a general adaptive mechanism in some circumstances, providing an alternative \nexplanation for the pervasive presence of criticality in biological and \ncognitive systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfe76", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05284"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sam Kriegman, Marcin Szubert, Josh C. Bongard, Christian Skalka", "title": "Evolving Spatially Aggregated Features from Satellite Imagery for Regional Modeling. (arXiv:1706.07888v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.07888", "type": "text/html"}], "timestampUsec": "1513315287371376", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050b547fa\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050b547fa&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Satellite imagery and remote sensing provide explanatory variables at \nrelatively high resolutions for modeling geospatial phenomena, yet regional \nsummaries are often desirable for analysis and actionable insight. In this \npaper, we propose a novel method of inducing spatial aggregations as a \ncomponent of the machine learning process, yielding regional model features \nwhose construction is driven by model prediction performance rather than prior \nassumptions. Our results demonstrate that Genetic Programming is particularly \nwell suited to this type of feature construction because it can automatically \nsynthesize appropriate aggregations, as well as better incorporate them into \npredictive models compared to other regression methods we tested. In our \nexperiments we consider a specific problem instance and real-world dataset \nrelevant to predicting snow properties in high-mountain Asia. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfe8a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.07888"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jindong Wang, Yiqiang Chen, Shuji Hao, Xiaohui Peng, Lisha Hu", "title": "Deep Learning for Sensor-based Activity Recognition: A Survey. (arXiv:1707.03502v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.03502", "type": "text/html"}], "timestampUsec": "1513315287371375", "comments": [], "summary": {"content": "<p>Sensor-based activity recognition seeks the profound high-level knowledge \nabout human activities from multitudes of low-level sensor readings. \nConventional pattern recognition approaches have made tremendous progress in \nthe past years. However, those methods often heavily rely on heuristic \nhand-crafted feature extraction, which could hinder their generalization \nperformance. Additionally, existing methods are undermined for unsupervised and \nincremental learning tasks. Recently, the recent advancement of deep learning \nmakes it possible to perform automatic high-level feature extraction thus \nachieves promising performance in many areas. Since then, deep learning based \nmethods have been widely adopted for the sensor-based activity recognition \ntasks. This paper surveys the recent advance of deep learning based \nsensor-based activity recognition. We summarize existing literature from three \naspects: sensor modality, deep model, and application. We also present detailed \ninsights on existing work and propose grand challenges for future research. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfe8e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.03502"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ishai Rosenberg, Asaf Shabtai, Lior Rokach, Yuval Elovici", "title": "Generic Black-Box End-to-End Attack Against State of the Art API Call Based Malware Classifiers. (arXiv:1707.05970v3 [cs.CR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.05970", "type": "text/html"}], "timestampUsec": "1513315287371374", "comments": [], "summary": {"content": "<p>In this paper, we present a black-box attack against API call based machine \nlearning malware classifiers, focusing on generating adversarial API call \nsequences that would be misclassified by the classifier without affecting the \nmalware functionality. We show that this attack is effective against many \nclassifiers due to the transferability principle between RNN variants, feed \nforward DNNs, and traditional machine learning classifiers such as SVM. We \nfurther extend our attack against hybrid classifiers based on a combination of \nstatic and dynamic features, focusing on printable strings and API calls. \nFinally, we implement GADGET, a software framework to convert any malware \nbinary to a binary undetected by malware classifiers, using the proposed \nattack, without access to the malware source code. We conclude by discussing \npossible defense mechanisms against the attack. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfe9b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.05970"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yundong Zhang, Naveen Suda, Liangzhen Lai, Vikas Chandra", "title": "Hello Edge: Keyword Spotting on Microcontrollers. (arXiv:1711.07128v2 [cs.SD] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07128", "type": "text/html"}], "timestampUsec": "1513315287371373", "comments": [], "summary": {"content": "<p>Keyword spotting (KWS) is a critical component for enabling speech based user \ninteractions on smart devices. It requires real-time response and high accuracy \nfor good user experience. Recently, neural networks have become an attractive \nchoice for KWS architecture because of their superior accuracy compared to \ntraditional speech processing algorithms. Due to its always-on nature, KWS \napplication has highly constrained power budget and typically runs on tiny \nmicrocontrollers with limited memory and compute capability. The design of \nneural network architecture for KWS must consider these constraints. In this \nwork, we perform neural network architecture evaluation and exploration for \nrunning KWS on resource-constrained microcontrollers. We train various neural \nnetwork architectures for keyword spotting published in literature to compare \ntheir accuracy and memory/compute requirements. We show that it is possible to \noptimize these neural network architectures to fit within the memory and \ncompute constraints of microcontrollers without sacrificing accuracy. We \nfurther explore the depthwise separable convolutional neural network (DS-CNN) \nand compare it against other neural network architectures. DS-CNN achieves an \naccuracy of 95.4%, which is ~10% higher than the DNN model with similar number \nof parameters. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfeac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07128"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yi-Hsuan Tsai, Ming-Yu Liu, Deqing Sun, Ming-Hsuan Yang, Jan Kautz", "title": "Learning Binary Residual Representations for Domain-specific Video Streaming. (arXiv:1712.05087v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.05087", "type": "text/html"}], "timestampUsec": "1513315287371371", "comments": [], "summary": {"content": "<p>We study domain-specific video streaming. Specifically, we target a streaming \nsetting where the videos to be streamed from a server to a client are all in \nthe same domain and they have to be compressed to a small size for low-latency \ntransmission. Several popular video streaming services, such as the video game \nstreaming services of GeForce Now and Twitch, fall in this category. While \nconventional video compression standards such as H.264 are commonly used for \nthis task, we hypothesize that one can leverage the property that the videos \nare all in the same domain to achieve better video quality. Based on this \nhypothesis, we propose a novel video compression pipeline. Specifically, we \nfirst apply H.264 to compress domain-specific videos. We then train a novel \nbinary autoencoder to encode the leftover domain-specific residual information \nframe-by-frame into binary representations. These binary representations are \nthen compressed and sent to the client together with the H.264 stream. In our \nexperiments, we show that our pipeline yields consistent gains over standard \nH.264 compression across several benchmark datasets while using the same \nchannel bandwidth. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfeca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05087"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tom Bocklisch, Joey Faulker, Nick Pawlowski, Alan Nichol", "title": "Rasa: Open Source Language Understanding and Dialogue Management. (arXiv:1712.05181v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.05181", "type": "text/html"}], "timestampUsec": "1513315287371370", "comments": [], "summary": {"content": "<p>We introduce a pair of tools, Rasa NLU and Rasa Core, which are open source \npython libraries for building conversational software. Their purpose is to make \nmachine-learning based dialogue management and language understanding \naccessible to non-specialist software developers. In terms of design \nphilosophy, we aim for ease of use, and bootstrapping from minimal (or no) \ninitial training data. Both packages are extensively documented and ship with a \ncomprehensive suite of tests. The code is available at \nhttps://github.com/RasaHQ/ \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfedb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05181"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sachin Pawar, Girish K. Palshikar, Pushpak Bhattacharyya", "title": "Relation Extraction : A Survey. (arXiv:1712.05191v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.05191", "type": "text/html"}], "timestampUsec": "1513315287371369", "comments": [], "summary": {"content": "<p>With the advent of the Internet, large amount of digital text is generated \neveryday in the form of news articles, research publications, blogs, question \nanswering forums and social media. It is important to develop techniques for \nextracting information automatically from these documents, as lot of important \ninformation is hidden within them. This extracted information can be used to \nimprove access and management of knowledge hidden in large text corpora. \nSeveral applications such as Question Answering, Information Retrieval would \nbenefit from this information. Entities like persons and organizations, form \nthe most basic unit of the information. Occurrences of entities in a sentence \nare often linked through well-defined relations; e.g., occurrences of person \nand organization in a sentence may be linked through relations such as employed \nat. The task of Relation Extraction (RE) is to identify such relations \nautomatically. In this paper, we survey several important supervised, \nsemi-supervised and unsupervised RE techniques. We also cover the paradigms of \nOpen Information Extraction (OIE) and Distant Supervision. Finally, we describe \nsome of the recent trends in the RE techniques and possible future research \ndirections. This survey would be useful for three kinds of readers - i) \nNewcomers in the field who want to quickly learn about RE; ii) Researchers who \nwant to know how the various RE techniques evolved over time and what are \npossible future research directions and iii) Practitioners who just need to \nknow which RE technique works best in various settings. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfee3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05191"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Matthew Piekenbrock, Derek Doran", "title": "Intrinsic Point of Interest Discovery from Trajectory Data. (arXiv:1712.05247v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.05247", "type": "text/html"}], "timestampUsec": "1513315287371368", "comments": [], "summary": {"content": "<p>This paper presents a framework for intrinsic point of interest discovery \nfrom trajectory databases. Intrinsic points of interest are regions of a \ngeospatial area innately defined by the spatial and temporal aspects of \ntrajectory data, and can be of varying size, shape, and resolution. Any \ntrajectory database exhibits such points of interest, and hence are intrinsic, \nas compared to most other point of interest definitions which are said to be \nextrinsic, as they require trajectory metadata, external knowledge about the \nregion the trajectories are observed, or other application-specific \ninformation. Spatial and temporal aspects are qualities of any trajectory \ndatabase, making the framework applicable to data from any domain and of any \nresolution. The framework is developed under recent developments on the \nconsistency of nonparametric hierarchical density estimators and enables the \npossibility of formal statistical inference and evaluation over such intrinsic \npoints of interest. Comparisons of the POIs uncovered by the framework in \nsynthetic truth data to thousands of parameter settings for common POI \ndiscovery methods show a marked improvement in fidelity without the need to \ntune any parameters by hand. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfeed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05247"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gianluca Brero, S&#xe9;bastien Lahaie", "title": "A Bayesian Clearing Mechanism for Combinatorial Auctions. (arXiv:1712.05291v1 [cs.GT])", "alternate": [{"href": "http://arxiv.org/abs/1712.05291", "type": "text/html"}], "timestampUsec": "1513315287371367", "comments": [], "summary": {"content": "<p>We cast the problem of combinatorial auction design in a Bayesian framework \nin order to incorporate prior information into the auction process and minimize \nthe number of rounds to convergence. We first develop a generative model of \nagent valuations and market prices such that clearing prices become maximum a \nposteriori estimates given observed agent valuations. This generative model \nthen forms the basis of an auction process which alternates between refining \nestimates of agent valuations and computing candidate clearing prices. We \nprovide an implementation of the auction using assumed density filtering to \nestimate valuations and expectation maximization to compute prices. An \nempirical evaluation over a range of valuation domains demonstrates that our \nBayesian auction mechanism is highly competitive against the combinatorial \nclock auction in terms of rounds to convergence, even under the most favorable \nchoices of price increment for this baseline. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfef8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05291"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Damla Kizilay, Deniz T. Eliiyi, Pascal Van Hentenryck", "title": "Constraint and Mathematical Programming Models for Integrated Port Container Terminal Operations. (arXiv:1712.05302v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.05302", "type": "text/html"}], "timestampUsec": "1513315287371366", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050b551db\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050b551db&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper considers the integrated problem of quay crane assignment, quay \ncrane scheduling, yard location assignment, and vehicle dispatching operations \nat a container terminal. The main objective is to minimize vessel turnover \ntimes and maximize the terminal throughput, which are key economic drivers in \nterminal operations. Due to their computational complexities, these problems \nare not optimized jointly in existing work. This paper revisits this limitation \nand proposes Mixed Integer Programming (MIP) and Constraint Programming (CP) \nmodels for the integrated problem, under some realistic assumptions. \nExperimental results show that the MIP formulation can only solve small \ninstances, while the CP model finds optimal solutions in reasonable times for \nrealistic instances derived from actual container terminal operations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bfeff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05302"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Irene C&#xf3;rdoba S&#xe1;nchez, Concha Bielza, Pedro Larra&#xf1;aga", "title": "On Gaussian Markov models for conditional independence. (arXiv:1606.07282v3 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1606.07282", "type": "text/html"}], "timestampUsec": "1513315287371365", "comments": [], "summary": {"content": "<p>Markov models lie at the interface between statistical independence in a \nprobability distribution and graph separation properties. We review model \nselection and estimation in directed and undirected Markov models with Gaussian \nparametrization, emphasizing the main similarities and differences. These two \nmodels are similar but not equivalent, although they share a common \nintersection. We present the existing results from a historical perspective, \ntaking into account the amount of literature existing from both the artificial \nintelligence and statistics research communities, where these models were \noriginated. We also discuss how the Gaussian assumption can be relaxed. We \nfinally point out the main areas of application where these Markov models are \nnowadays used. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff07", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1606.07282"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, Shimon Whiteson", "title": "Counterfactual Multi-Agent Policy Gradients. (arXiv:1705.08926v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08926", "type": "text/html"}], "timestampUsec": "1513315287371364", "comments": [], "summary": {"content": "<p>Cooperative multi-agent systems can be naturally used to model many real \nworld problems, such as network packet routing and the coordination of \nautonomous vehicles. There is a great need for new reinforcement learning \nmethods that can efficiently learn decentralised policies for such systems. To \nthis end, we propose a new multi-agent actor-critic method called \ncounterfactual multi-agent (COMA) policy gradients. COMA uses a centralised \ncritic to estimate the Q-function and decentralised actors to optimise the \nagents' policies. In addition, to address the challenges of multi-agent credit \nassignment, it uses a counterfactual baseline that marginalises out a single \nagent's action, while keeping the other agents' actions fixed. COMA also uses a \ncritic representation that allows the counterfactual baseline to be computed \nefficiently in a single forward pass. We evaluate COMA in the testbed of \nStarCraft unit micromanagement, using a decentralised variant with significant \npartial observability. COMA significantly improves average performance over \nother multi-agent actor-critic methods in this setting, and the best performing \nagents are competitive with state-of-the-art centralised controllers that get \naccess to the full state. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff11", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08926"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sirui Yao, Bert Huang", "title": "New Fairness Metrics for Recommendation that Embrace Differences. (arXiv:1706.09838v2 [cs.CY] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.09838", "type": "text/html"}], "timestampUsec": "1513315287371363", "comments": [], "summary": {"content": "<p>We study fairness in collaborative-filtering recommender systems, which are \nsensitive to discrimination that exists in historical data. Biased data can \nlead collaborative filtering methods to make unfair predictions against \nminority groups of users. We identify the insufficiency of existing fairness \nmetrics and propose four new metrics that address different forms of \nunfairness. These fairness metrics can be optimized by adding fairness terms to \nthe learning objective. Experiments on synthetic and real data show that our \nnew metrics can better measure fairness than the baseline, and that the \nfairness objectives effectively help reduce unfairness. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff16", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.09838"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jose M. Pe&#xf1;a", "title": "Unifying DAGs and UGs. (arXiv:1708.08722v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.08722", "type": "text/html"}], "timestampUsec": "1513315287371361", "comments": [], "summary": {"content": "<p>We introduce a new class of graphical models that generalizes \nLauritzen-Wermuth-Frydenberg chain graphs by relaxing the semi-directed \nacyclity constraint so that only directed cycles are forbidden. Moreover, up to \ntwo edges are allowed between any pair of nodes. Specifically, we present \nlocal, pairwise and global Markov properties for the new graphical models and \nprove their equivalence. We also present an equivalent factorization property. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff23", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.08722"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lei Lin, Zhengbing He, Srinivas Peeta, Xuejin Wen", "title": "Predicting Station-level Hourly Demands in a Large-scale Bike-sharing Network: A Graph Convolutional Neural Network Approach. (arXiv:1712.04997v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04997", "type": "text/html"}], "timestampUsec": "1513315287371360", "comments": [], "summary": {"content": "<p>Bike sharing is a vital piece in a modern multi-modal transportation system. \nHowever, it suffers from the bike unbalancing problem due to fluctuating \nspatial and temporal demands. Accurate bike sharing demand predictions can help \noperators to make optimal routes and schedules for bike redistributions, and \ntherefore enhance the system efficiency. In this study, we propose a novel \nGraph Convolutional Neural Network with Data-driven Graph Filter (GCNN-DDGF) \nmodel to predict station-level hourly demands in a large-scale bike-sharing \nnetwork. With each station as a vertex in the network, the new proposed \nGCNN-DDGF model is able to automatically learn the hidden correlations between \nstations, and thus overcomes a common issue reported in the previous studies, \ni.e., the quality and performance of GCNN models rely on the predefinition of \nthe adjacency matrix. To show the performance of the proposed model, this study \ncompares the GCNN-DDGF model with four GCNNs models, whose adjacency matrices \nare from different bike sharing system matrices including the Spatial Distance \nmatrix (SD), the Demand matrix (DE), the Average Trip Duration matrix (ATD) and \nthe Demand Correlation matrix (DC), respectively. The five types of GCNN models \nand the classic Support Vector Regression model are built on a Citi Bike \ndataset from New York City which includes 272 stations and over 28 million \ntransactions from 2013 to 2016. Results show that the GCNN-DDGF model has the \nlowest Root Mean Square Error, followed by the GCNN-DC model, and the GCNN-ATD \nmodel has the worst performance. Through a further examination, we find the \nlearned DDGF captures some similar information embedded in the SD, DE and DC \nmatrices, and it also uncovers more hidden heterogeneous pairwise correlations \nbetween stations that are not revealed by any of those matrices. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff28", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04997"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexandre Lacoste, Thomas Boquet, Negar Rostamzadeh, Boris Oreshki, Wonchang Chung, David Krueger", "title": "Deep Prior. (arXiv:1712.05016v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.05016", "type": "text/html"}], "timestampUsec": "1513315287371359", "comments": [], "summary": {"content": "<p>The recent literature on deep learning offers new tools to learn a rich \nprobability distribution over high dimensional data such as images or sounds. \nIn this work we investigate the possibility of learning the prior distribution \nover neural network parameters using such tools. Our resulting variational \nBayes algorithm generalizes well to new tasks, even when very few training \nexamples are provided. Furthermore, this learned prior allows the model to \nextrapolate correctly far from a given task's training data on a meta-dataset \nof periodic signals. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff2b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05016"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jinmian Ye, Linnan Wang, Guangxi Li, Di Chen, Shandian Zhe, Xinqi Chu, Zenglin Xu", "title": "Learning Compact Recurrent Neural Networks with Block-Term Tensor Decomposition. (arXiv:1712.05134v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.05134", "type": "text/html"}], "timestampUsec": "1513315287371358", "comments": [], "summary": {"content": "<p>Recurrent Neural Networks (RNNs) are powerful sequence modeling tools. \nHowever, when dealing with high dimensional inputs, the training of RNNs \nbecomes computational expensive due to the large number of model parameters. \nThis hinders RNNs from solving many important computer vision tasks, such as \nAction Recognition in Videos and Image Captioning. To overcome this problem, we \npropose a compact and flexible structure, namely Block-Term tensor \ndecomposition, which greatly reduces the parameters of RNNs and improves their \ntraining efficiency. Compared with alternative low-rank approximations, such as \ntensor-train RNN (TT-RNN), our method, Block-Term RNN (BT-RNN), is not only \nmore concise (when using the same rank), but also able to attain a better \napproximation to the original RNNs with much fewer parameters. On three \nchallenging tasks, including Action Recognition in Videos, Image Captioning and \nImage Generation, BT-RNN outperforms TT-RNN and the standard RNN in terms of \nboth prediction accuracy and convergence rate. Specifically, BT-LSTM utilizes \n17,388 times fewer parameters than the standard LSTM to achieve an accuracy \nimprovement over 15.6\\% in the Action Recognition task on the UCF11 dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05134"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ingo Steinwart, Johanna F. Ziegel", "title": "Strictly proper kernel scores and characteristic kernels on compact spaces. (arXiv:1712.05279v1 [math.FA])", "alternate": [{"href": "http://arxiv.org/abs/1712.05279", "type": "text/html"}], "timestampUsec": "1513315287371357", "comments": [], "summary": {"content": "<p>Strictly proper kernel scores are well-known tool in probabilistic \nforecasting, while characteristic kernels have been extensively investigated in \nthe machine learning literature. We first show that both notions coincide, so \nthat insights from one part of the literature can be used in the other. We then \nshow that the metric induced by a characteristic kernel cannot reliably \ndistinguish between distributions that are far apart in the total variation \nnorm as soon as the underlying space of measures is infinite dimensional. In \naddition, we provide a characterization of characteristic kernels in terms of \neigenvalues and -functions and apply this characterization to the case of \ncontinuous kernels on (locally) compact spaces. In the compact case we further \nshow that characteristic kernels exist if and only if the space is metrizable. \nAs special cases of our general theory we investigate translation-invariant \nkernels on compact Abelian groups and isotropic kernels on spheres. The latter \nare of particular interest for forecast evaluation of probabilistic predictions \non spherical domains as frequently encountered in meteorology and climatology. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff37", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05279"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chung-Cheng Chiu, Colin Raffel", "title": "Monotonic Chunkwise Attention. (arXiv:1712.05382v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.05382", "type": "text/html"}], "timestampUsec": "1513315287371356", "comments": [], "summary": {"content": "<p>Sequence-to-sequence models with soft attention have been successfully \napplied to a wide variety of problems, but their decoding process incurs a \nquadratic time and space cost and is inapplicable to real-time sequence \ntransduction. To address these issues, we propose Monotonic Chunkwise Attention \n(MoChA), which adaptively splits the input sequence into small chunks over \nwhich soft attention is computed. We show that models utilizing MoChA can be \ntrained efficiently with standard backpropagation while allowing online and \nlinear-time decoding at test time. When applied to online speech recognition, \nwe obtain state-of-the-art results and match the performance of a model using \nan offline soft attention mechanism. In document summarization experiments \nwhere we do not expect monotonic alignments, we show significantly improved \nperformance compared to a baseline monotonic attention-based model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff3c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.05382"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "John Duchi, Hongseok Namkoong", "title": "Variance-based regularization with convex objectives. (arXiv:1610.02581v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.02581", "type": "text/html"}], "timestampUsec": "1513315287371355", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050b5543f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050b5543f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We develop an approach to risk minimization and stochastic optimization that \nprovides a convex surrogate for variance, allowing near-optimal and \ncomputationally efficient trading between approximation and estimation error. \nOur approach builds off of techniques for distributionally robust optimization \nand Owen's empirical likelihood, and we provide a number of finite-sample and \nasymptotic results characterizing the theoretical performance of the estimator. \nIn particular, we show that our procedure comes with certificates of \noptimality, achieving (in some scenarios) faster rates of convergence than \nempirical risk minimization by virtue of automatically balancing bias and \nvariance. We give corroborating empirical evidence showing that in practice, \nthe estimator indeed trades between variance and absolute performance on a \ntraining sample, improving out-of-sample (test) performance over standard \nempirical risk minimization for a number of classification problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff56", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.02581"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ricardo Pio Monti, Christoforos Anagnostopoulos, Giovanni Montana", "title": "Adaptive regularization for Lasso models in the context of non-stationary data streams. (arXiv:1610.09127v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.09127", "type": "text/html"}], "timestampUsec": "1513315287371354", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050ba5d8d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050ba5d8d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Large scale, streaming datasets are ubiquitous in modern machine learning. \nStreaming algorithms must be scalable, amenable to incremental training and \nrobust to the presence of non-stationarity. In this work consider the problem \nof learning $\\ell_1$ regularized linear models in the context of streaming \ndata. In particular, the focus of this work revolves around how to select the \nregularization parameter when data arrives sequentially and the underlying \ndistribution is non-stationary (implying the choice of optimal regularization \nparameter is itself time-varying). We propose a framework through which to \ninfer an adaptive regularization parameter. Our approach employs an $\\ell_1$ \npenalty constraint where the corresponding sparsity parameter is iteratively \nupdated via stochastic gradient descent. This serves to reformulate the choice \nof regularization parameter in a principled framework for online learning. The \nproposed method is derived for linear regression and subsequently extended to \ngeneralized linear models. We validate our approach using simulated and real \ndatasets and present an application to a neuroimaging dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff66", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.09127"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Elizabeth Hou, Kumar Sricharan, Alfred O. Hero", "title": "Latent Laplacian Maximum Entropy Discrimination for Detection of High-Utility Anomalies. (arXiv:1702.05148v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1702.05148", "type": "text/html"}], "timestampUsec": "1513315287371353", "comments": [], "summary": {"content": "<p>Data-driven anomaly detection methods suffer from the drawback of detecting \nall instances that are statistically rare, irrespective of whether the detected \ninstances have real-world significance or not. In this paper, we are interested \nin the problem of specifically detecting anomalous instances that are known to \nhave high real-world utility, while ignoring the low-utility statistically \nanomalous instances. To this end, we propose a novel method called Latent \nLaplacian Maximum Entropy Discrimination (LatLapMED) as a potential solution. \nThis method uses the EM algorithm to simultaneously incorporate the Geometric \nEntropy Minimization principle for identifying statistical anomalies, and the \nMaximum Entropy Discrimination principle to incorporate utility labels, in \norder to detect high-utility anomalies. We apply our method in both simulated \nand real datasets to demonstrate that it has superior performance over existing \nalternatives that independently pre-process with unsupervised anomaly detection \nalgorithms before classifying. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff70", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1702.05148"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, Alistair Stewart", "title": "Being Robust (in High Dimensions) Can Be Practical. (arXiv:1703.00893v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.00893", "type": "text/html"}], "timestampUsec": "1513315287371352", "comments": [], "summary": {"content": "<p>Robust estimation is much more challenging in high dimensions than it is in \none dimension: Most techniques either lead to intractable optimization problems \nor estimators that can tolerate only a tiny fraction of errors. Recent work in \ntheoretical computer science has shown that, in appropriate distributional \nmodels, it is possible to robustly estimate the mean and covariance with \npolynomial time algorithms that can tolerate a constant fraction of \ncorruptions, independent of the dimension. However, the sample and time \ncomplexity of these algorithms is prohibitively large for high-dimensional \napplications. In this work, we address both of these issues by establishing \nsample complexity bounds that are optimal, up to logarithmic factors, as well \nas giving various refinements that allow the algorithms to tolerate a much \nlarger fraction of corruptions. Finally, we show on both synthetic and real \ndata that our algorithms have state-of-the-art performance and suddenly make \nhigh-dimensional robust estimation a realistic possibility. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff79", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.00893"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jack Baker, Paul Fearnhead, Emily B. Fox, Christopher Nemeth", "title": "Control Variates for Stochastic Gradient MCMC. (arXiv:1706.05439v2 [stat.CO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.05439", "type": "text/html"}], "timestampUsec": "1513315287371351", "comments": [], "summary": {"content": "<p>It is well known that Markov chain Monte Carlo (MCMC) methods scale poorly \nwith dataset size. A popular class of methods for solving this issue is \nstochastic gradient MCMC. These methods use a noisy estimate of the gradient of \nthe log posterior, which reduces the per iteration computational cost of the \nalgorithm. Despite this, there are a number of results suggesting that \nstochastic gradient Langevin dynamics (SGLD), probably the most popular of \nthese methods, still has computational cost proportional to the dataset size. \nWe suggest an alternative log posterior gradient estimate for stochastic \ngradient MCMC, which uses control variates to reduce the variance. We analyse \nSGLD using this gradient estimate, and show that, under log-concavity \nassumptions on the target distribution, the computational cost required for a \ngiven level of accuracy is independent of the dataset size. Next we show that a \ndifferent control variate technique, known as zero variance control variates \ncan be applied to SGMCMC algorithms for free. This post-processing step \nimproves the inference of the algorithm by reducing the variance of the MCMC \noutput. Zero variance control variates rely on the gradient of the log \nposterior; we explore how the variance reduction is affected by replacing this \nwith the noisy gradient estimate calculated by SGMCMC. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff88", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.05439"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Torsten Hothorn", "title": "Top-down Transformation Choice. (arXiv:1706.08269v2 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.08269", "type": "text/html"}], "timestampUsec": "1513315287371350", "comments": [], "summary": {"content": "<p>Simple models are preferred over complex models, but over-simplistic models \ncould lead to erroneous interpretations. The classical approach is to start \nwith a simple model, whose shortcomings are assessed in residual-based model \ndiagnostics. Eventually, one increases the complexity of this initial overly \nsimple model and obtains a better-fitting model. I illustrate how \ntransformation analysis can be used as an alternative approach to model choice. \nInstead of adding complexity to simple models, step-wise complexity reduction \nis used to help identify simpler and better-interpretable models. As an \nexample, body mass index distributions in Switzerland are modelled by means of \ntransformation models to understand the impact of sex, age, smoking and other \nlifestyle factors on a person's body mass index. In this process, I searched \nfor a compromise between model fit and model interpretability. Special emphasis \nis given to the understanding of the connections between transformation models \nof increasing complexity. The models used in this analysis ranged from \nevergreens, such as the normal linear regression model with constant variance, \nto novel models with extremely flexible conditional distribution functions, \nsuch as transformation trees and transformation forests. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff8c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.08269"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Carolin Lawrence, Artem Sokolov, Stefan Riezler", "title": "Counterfactual Learning from Bandit Feedback under Deterministic Logging: A Case Study in Statistical Machine Translation. (arXiv:1707.09118v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.09118", "type": "text/html"}], "timestampUsec": "1513315287371349", "comments": [], "summary": {"content": "<p>The goal of counterfactual learning for statistical machine translation (SMT) \nis to optimize a target SMT system from logged data that consist of user \nfeedback to translations that were predicted by another, historic SMT system. A \nchallenge arises by the fact that risk-averse commercial SMT systems \ndeterministically log the most probable translation. The lack of sufficient \nexploration of the SMT output space seemingly contradicts the theoretical \nrequirements for counterfactual learning. We show that counterfactual learning \nfrom deterministic bandit logs is possible nevertheless by smoothing out \ndeterministic components in learning. This can be achieved by additive and \nmultiplicative control variates that avoid degenerate behavior in empirical \nrisk minimization. Our simulation experiments show improvements of up to 2 BLEU \npoints by counterfactual learning from deterministic bandit feedback. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff90", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.09118"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jack Baker, Paul Fearnhead, Emily B. Fox, Christopher Nemeth", "title": "sgmcmc: An R Package for Stochastic Gradient Markov Chain Monte Carlo. (arXiv:1710.00578v2 [stat.CO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.00578", "type": "text/html"}], "timestampUsec": "1513315287371348", "comments": [], "summary": {"content": "<p>This paper introduces the R package sgmcmc; which can be used for Bayesian \ninference on problems with large datasets using stochastic gradient Markov \nchain Monte Carlo (SGMCMC). Traditional Markov chain Monte Carlo (MCMC) \nmethods, such as Metropolis-Hastings, are known to run prohibitively slowly as \nthe dataset size increases. SGMCMC solves this issue by only using a subset of \ndata at each iteration. SGMCMC requires calculating gradients of the log \nlikelihood and log priors, which can be time consuming and error prone to \nperform by hand. The sgmcmc package calculates these gradients itself using \nautomatic differentiation, making the implementation of these methods much \neasier. To do this, the package uses the software library TensorFlow, which has \na variety of statistical distributions and mathematical operations as standard, \nmeaning a wide class of models can be built using this framework. SGMCMC has \nbecome widely adopted in the machine learning literature, but less so in the \nstatistics community. We believe this may be partly due to lack of software; \nthis package aims to bridge this gap. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff95", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.00578"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Adam N. Elmachtoub, Paul Grigas", "title": "Smart \"Predict, then Optimize\". (arXiv:1710.08005v2 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.08005", "type": "text/html"}], "timestampUsec": "1513315287371347", "comments": [], "summary": {"content": "<p>Many real-world analytics problems involve two significant challenges: \nprediction and optimization. Due to the typically complex nature of each \nchallenge, the standard paradigm is to predict, then optimize. By and large, \nmachine learning tools are intended to minimize prediction error and do not \naccount for how the predictions will be used in a downstream optimization \nproblem. In contrast, we propose a new and very general framework, called Smart \n\"Predict, then Optimize\" (SPO), which directly leverages the optimization \nproblem structure, i.e., its objective and constraints, for designing \nsuccessful analytics tools. A key component of our framework is the SPO loss \nfunction, which measures the quality of a prediction by comparing the objective \nvalues of the solutions generated using the predicted and observed parameters, \nrespectively. Training a model with respect to the SPO loss is computationally \nchallenging, and therefore we also develop a surrogate loss function, called \nthe SPO+ loss, which upper bounds the SPO loss, has desirable convexity \nproperties, and is statistically consistent under mild conditions. We also \npropose a stochastic gradient descent algorithm which allows for situations in \nwhich the number of training samples is large, model regularization is desired, \nand/or the optimization problem of interest is nonlinear or integer. Finally, \nwe perform computational experiments to empirically verify the success of our \nSPO framework in comparison to the standard predict-then-optimize approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bff9d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.08005"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Carolin Lawrence, Pratik Gajane, Stefan Riezler", "title": "Counterfactual Learning for Machine Translation: Degeneracies and Solutions. (arXiv:1711.08621v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08621", "type": "text/html"}], "timestampUsec": "1513315287371346", "comments": [], "summary": {"content": "<p>Counterfactual learning is a natural scenario to improve web-based machine \ntranslation services by offline learning from feedback logged during user \ninteractions. In order to avoid the risk of showing inferior translations to \nusers, in such scenarios mostly exploration-free deterministic logging policies \nare in place. We analyze possible degeneracies of inverse and reweighted \npropensity scoring estimators, in stochastic and deterministic settings, and \nrelate them to recently proposed techniques for counterfactual learning under \ndeterministic logging. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bffa5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08621"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Cong Ma, Kaizheng Wang, Yuejie Chi, Yuxin Chen", "title": "Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval, Matrix Completion and Blind Deconvolution. (arXiv:1711.10467v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10467", "type": "text/html"}], "timestampUsec": "1513315287371345", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050ba5fd9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050ba5fd9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Recent years have seen a flurry of activities in designing provably efficient \nnonconvex procedures for solving statistical estimation problems. Due to the \nhighly nonconvex nature of the empirical loss, state-of-the-art procedures \noften require proper regularization (e.g. trimming, regularized cost, \nprojection) in order to guarantee fast convergence. For vanilla procedures such \nas gradient descent, however, prior theory either recommends highly \nconservative learning rates to avoid overshooting, or completely lacks \nperformance guarantees. \n</p> \n<p>This paper uncovers a striking phenomenon in nonconvex optimization: even in \nthe absence of explicit regularization, gradient descent enforces proper \nregularization implicitly under various statistical models. In fact, gradient \ndescent follows a trajectory staying within a basin that enjoys nice geometry, \nconsisting of points incoherent with the sampling mechanism. This \"implicit \nregularization\" feature allows gradient descent to proceed in a far more \naggressive fashion without overshooting, which in turn results in substantial \ncomputational savings. Focusing on three fundamental statistical estimation \nproblems, i.e. phase retrieval, low-rank matrix completion, and blind \ndeconvolution, we establish that gradient descent achieves near-optimal \nstatistical and computational guarantees without explicit regularization. In \nparticular, by marrying statistical modeling with generic optimization theory, \nwe develop a general recipe for analyzing the trajectories of iterative \nalgorithms via a leave-one-out perturbation argument. As a byproduct, for noisy \nmatrix completion, we demonstrate that gradient descent achieves near-optimal \nerror control --- measured entrywise and by the spectral norm --- which might \nbe of independent interest. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bffb7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10467"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Khondkar Islam, Pouyan Ahmadi, Salman Yousaf", "title": "Assessment Formats and Student Learning Performance: What is the Relation?. (arXiv:1711.10396v1 [physics.ed-ph] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10396", "type": "text/html"}], "timestampUsec": "1513315287371342", "comments": [], "summary": {"content": "<p>Although compelling assessments have been examined in recent years, more \nstudies are required to yield a better understanding of the several methods \nwhere assessment techniques significantly affect student learning process. Most \nof the educational research in this area does not consider demographics data, \ndiffering methodologies, and notable sample size. To address these drawbacks, \nthe objective of our study is to analyse student learning outcomes of multiple \nassessment formats for a web-facilitated in-class section with an asynchronous \nonline class of a core data communications course in the Undergraduate IT \nprogram of the Information Sciences and Technology (IST) Department at George \nMason University (GMU). In this study, students were evaluated based on course \nassessments such as home and lab assignments, skill-based assessments, and \ntraditional midterm and final exams across all four sections of the course. All \nsections have equivalent content, assessments, and teaching methodologies. \nStudent demographics such as exam type and location preferences are considered \nin our study to determine whether they have any impact on their learning \napproach. Large amount of data from the learning management system (LMS), \nBlackboard (BB) Learn, had to be examined to compare the results of several \nassessment outcomes for all students within their respective section and \namongst students of other sections. To investigate the effect of dissimilar \nassessment formats on student performance, we had to correlate individual \nquestion formats with the overall course grade. The results show that \ncollective assessment formats allow students to be effective in demonstrating \ntheir knowledge. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513315287371", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003479bffdc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10396"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "V.I. Avrutskiy", "title": "Enhancing approximation abilities of neural networks by training derivatives. (arXiv:1712.04473v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.04473", "type": "text/html"}], "timestampUsec": "1513228697215439", "comments": [], "summary": {"content": "<p>Method for increasing precision of feedforward networks is presented. With \nthe aid of it they can serve as a better tool for describing smooth functions. \nNamely, it is shown that when training uses derivatives of target function up \nto the fourth order, approximation can be nearly machine precise. It is \ndemonstrated in a number of cases: 2D function approximation, training \nautoencoder to compress 3D spiral into 1D, and solving 2D boundary value \nproblem for Poisson equation with nonlinear source. In the first case cost \nfunction in addition to squared difference between output and target contains \nsquared differences between their derivatives with respect to input variables. \nTraining autoencoder is similar, but differentiation is done with respect to \nparameter that generates the spiral. Supplied with derivatives up to the fourth \nthe method is found to be 30-200 times more accurate than regular training \nprovided networks are of sufficient size and depth. Solving PDE is more \npractical since higher derivatives are not calculated beforehand, but \ninformation about them is extracted from the equation itself. Classical \napproach is to put perceptron in place of unknown function, choose the cost as \nsquared residual and to minimize it with respect to weights. This would ensure \nthat equation holds within some margin of error. Additional terms used in cost \nfunction are squared derivatives of the residual with respect to independent \nvariables. Supplied with terms up to the second order the method is found to be \n5 times more accurate. Efficient GPU version of algorithm is proposed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96aa0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04473"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David M. Schwartz, O. Ozan Koyluoglu", "title": "On the organization of grid and place cells: Neural de-noising via subspace learning. (arXiv:1712.04602v1 [q-bio.NC])", "alternate": [{"href": "http://arxiv.org/abs/1712.04602", "type": "text/html"}], "timestampUsec": "1513228697215438", "comments": [], "summary": {"content": "<p>Place cells in the hippocampus are active when an animal visits a certain \nlocations (referred to as place fields) within an environment and remain silent \notherwise. Grid cells in the medial entorhinal cortex (MEC) respond at multiple \nlocations, with firing fields that exhibit a hexagonally symmetric periodic \npattern. The joint activity of grid and place cell populations, as a function \nof location, forms a neural code for space. An ensemble of codes, for a given \nset of parameters, is generated by selecting grid and place cell population and \ntuning curve parameters. For each ensemble, codewords are generated by \nstimulating a network with a discrete set of locations. In this manuscript, we \ndevelop an understanding of the relationships between coding theoretic \nproperties of these combined populations and code construction parameters. \nThese observations are revisited by measuring the performances of biologically \nrealizable algorithms (e.g. neural bit-flipping) implemented by a network of \nplace and grid cell populations, as well as interneurons, which perform \nde-noising operations. Simulations demonstrate that de-noising mechanisms \nanalyzed here can significantly improve fidelity of this neural representation \nof space. Further, patterns observed in connectivity of each population of \nsimulated cells suggest the existence of heretofore unobserved neurobiological \nphenomena. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96aba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04602"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chase Gaudet, Anthony Maida", "title": "Deep Quaternion Networks. (arXiv:1712.04604v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.04604", "type": "text/html"}], "timestampUsec": "1513228697215437", "comments": [], "summary": {"content": "<p>The field of deep learning has seen significant advancement in recent years. \nHowever, much of the existing work has been focused on real-valued numbers. \nRecent work has shown that a deep learning system using the complex numbers can \nbe deeper for a set parameter budget compared to its real-valued counterpart. \nIn this work, we explore the benefits of generalizing one step further into the \nhyper-complex numbers, quaternions specifically, and provide the architecture \ncomponents needed to build deep quaternion networks. We go over quaternion \nconvolutions, present a quaternion weight initialization scheme, and present \nalgorithms for quaternion batch-normalization. These pieces are tested by \nend-to-end training on the CIFAR-10 and CIFAR-100 data sets to show the \nimproved convergence to a real-valued network. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96ad7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04604"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gabriele Scheler", "title": "Logarithmic distributions prove that intrinsic learning is Hebbian. (arXiv:1410.5610v3 [q-bio.NC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1410.5610", "type": "text/html"}], "timestampUsec": "1513228697215436", "comments": [], "summary": {"content": "<p>In this paper, we present data for the lognormal distributions of spike \nrates, synaptic weights and intrinsic excitability (gain) for neurons in \nvarious brain areas, such as auditory or visual cortex, hippocampus, \ncerebellum, striatum, midbrain nuclei. We find a remarkable consistency of \nheavy-tailed, specifically lognormal, distributions for rates, weights and \ngains in all brain areas examined. The difference between strongly recurrent \nand feed-forward connectivity (cortex vs. striatum and cerebellum), \nneurotransmitter (GABA (striatum) or glutamate (cortex)) or the level of \nactivation (low in cortex, high in Purkinje cells and midbrain nuclei) turns \nout to be irrelevant for this feature. Logarithmic scale distribution of \nweights and gains appears to be a general, functional property in all cases \nanalyzed. We then created a generic neural model to investigate adaptive \nlearning rules that create and maintain lognormal distributions. We \nconclusively demonstrate that not only weights, but also intrinsic gains, need \nto have strong Hebbian learning in order to produce and maintain the \nexperimentally attested distributions. This provides a solution to the \nlong-standing question about the type of plasticity exhibited by intrinsic \nexcitability. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96ae1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1410.5610"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Leslie N. Smith, Nicholay Topin", "title": "Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates. (arXiv:1708.07120v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.07120", "type": "text/html"}], "timestampUsec": "1513228697215435", "comments": [], "summary": {"content": "<p>In this paper, we show a phenomenon, which we named \"super-convergence\", \nwhere residual networks can be trained using an order of magnitude fewer \niterations than is used with standard training methods. The existence of \nsuper-convergence is relevant to understanding why deep networks generalize \nwell. One of the key elements of super-convergence is training with cyclical \nlearning rates and a large maximum learning rate. Furthermore, we present \nevidence that training with large learning rates improves performance by \nregularizing the network. In addition, we show that super-convergence provides \na greater boost in performance relative to standard training when the amount of \nlabeled training data is limited. We also derive a simplification of the \nHessian Free optimization method to compute an estimate of the optimal learning \nrate. The architectures and code to replicate the figures in this paper are \navailable at github.com/lnsmith54/super-convergence. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96aea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.07120"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Milad Makkie, Heng Huang, Yu Zhao, Athanasios V. Vasilakos, Tianming Liu", "title": "Fast and Scalable Distributed Deep Convolutional Autoencoder for fMRI Big Data Analytics. (arXiv:1710.08961v2 [cs.DC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.08961", "type": "text/html"}], "timestampUsec": "1513228697215434", "comments": [], "summary": {"content": "<p>In recent years, analyzing task-based fMRI (tfMRI) data has become an \nessential tool for understanding brain function and networks. However, due to \nthe sheer size of tfMRI data, its intrinsic complex structure, and lack of \nground truth of underlying neural activities, modeling tfMRI data is hard and \nchallenging. Previously proposed data-modeling methods including Independent \nComponent Analysis (ICA) and Sparse Dictionary Learning only provided a weakly \nestablished model based on blind source separation under the strong assumption \nthat original fMRI signals could be linearly decomposed into time series \ncomponents with corresponding spatial maps. Meanwhile, analyzing and learning a \nlarge amount of tfMRI data from a variety of subjects has been shown to be very \ndemanding but yet challenging even with technological advances in computational \nhardware. Given the Convolutional Neural Network (CNN), a robust method for \nlearning high-level abstractions from low-level data such as tfMRI time series, \nin this work we propose a fast and scalable novel framework for distributed \ndeep Convolutional Autoencoder model. This model aims to both learn the complex \nhierarchical structure of the tfMRI data and to leverage the processing power \nof multiple GPUs in a distributed fashion. To implement such a model, we have \ncreated an enhanced processing pipeline on the top of Apache Spark and \nTensorflow library, leveraging from a very large cluster of GPU machines. \nExperimental data from applying the model on the Human Connectome Project (HCP) \nshow that the proposed model is efficient and scalable toward tfMRI big data \nanalytics, thus enabling data-driven extraction of hierarchical neuroscientific \ninformation from massive fMRI big data in the future. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96aee", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.08961"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, David Barber", "title": "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning. (arXiv:1711.01577v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.01577", "type": "text/html"}], "timestampUsec": "1513228697215433", "comments": [], "summary": {"content": "<p>Long Short-Term Memory (LSTM) is a popular approach to boosting the ability \nof Recurrent Neural Networks to store longer term temporal information. The \ncapacity of an LSTM network can be increased by widening and adding layers. \nHowever, usually the former introduces additional parameters, while the latter \nincreases the runtime. As an alternative we propose the Tensorized LSTM in \nwhich the hidden states are represented by tensors and updated via a \ncross-layer convolution. By increasing the tensor size, the network can be \nwidened efficiently without additional parameters since the parameters are \nshared across different locations in the tensor; by delaying the output, the \nnetwork can be deepened implicitly with little additional runtime since deep \ncomputations for each timestep are merged into temporal computations of the \nsequence. Experiments conducted on five challenging sequence learning tasks \nshow the potential of the proposed model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96afc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.01577"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Bo Wu, Wen-Huang Cheng, Yongdong Zhang, Qiushi Huang, Jintao Li, Tao Mei", "title": "Sequential Prediction of Social Media Popularity with Deep Temporal Context Networks. (arXiv:1712.04443v1 [cs.SI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04443", "type": "text/html"}], "timestampUsec": "1513228697215432", "comments": [], "summary": {"content": "<p>Prediction of popularity has profound impact for social media, since it \noffers opportunities to reveal individual preference and public attention from \nevolutionary social systems. Previous research, although achieves promising \nresults, neglects one distinctive characteristic of social data, i.e., \nsequentiality. For example, the popularity of online content is generated over \ntime with sequential post streams of social media. To investigate the \nsequential prediction of popularity, we propose a novel prediction framework \ncalled Deep Temporal Context Networks (DTCN) by incorporating both temporal \ncontext and temporal attention into account. Our DTCN contains three main \ncomponents, from embedding, learning to predicting. With a joint embedding \nnetwork, we obtain a unified deep representation of multi-modal user-post data \nin a common embedding space. Then, based on the embedded data sequence over \ntime, temporal context learning attempts to recurrently learn two adaptive \ntemporal contexts for sequential popularity. Finally, a novel temporal \nattention is designed to predict new popularity (the popularity of a new \nuser-post pair) with temporal coherence across multiple time-scales. \nExperiments on our released image dataset with about 600K Flickr photos \ndemonstrate that DTCN outperforms state-of-the-art deep prediction algorithms, \nwith an average of 21.51% relative performance improvement in the popularity \nprediction (Spearman Ranking Correlation). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b03", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04443"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Son-Il Kwak, Oh-Chol Gwon, Chung-Jin Kwak", "title": "Consideration on Example 2 of \"An Algorithm of General Fuzzy InferenceWith The Reductive Property\". (arXiv:1712.04596v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04596", "type": "text/html"}], "timestampUsec": "1513228697215431", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050ba6220\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050ba6220&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we will show that (1) the results about the fuzzy reasoning \nalgoritm obtained in the paper \"Computer Sciences Vol. 34, No.4, pp.145-148, \n2007\" according to the paper \"IEEE Transactions On systems, Man and \ncybernetics, 18, pp.1049-1056, 1988\" are correct; (2) example 2 in the paper \n\"An Algorithm of General Fuzzy Inference With The Reductive Property\" presented \nby He Ying-Si, Quan Hai-Jin and Deng Hui-Wen according to the paper \"An \napproximate analogical reasoning approach based on similarity measures\" \npresented by Tursken I.B. and Zhong zhao is incorrect; (3) the mistakes in \ntheir paper are modified and then a calculation example of FMT is supplemented. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b0d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04596"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jinyoung Choi, Beom-Jin Lee, Byoung-Tak Zhang", "title": "Multi-focus Attention Network for Efficient Deep Reinforcement Learning. (arXiv:1712.04603v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04603", "type": "text/html"}], "timestampUsec": "1513228697215430", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050c02e43\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050c02e43&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep reinforcement learning (DRL) has shown incredible performance in \nlearning various tasks to the human level. However, unlike human perception, \ncurrent DRL models connect the entire low-level sensory input to the \nstate-action values rather than exploiting the relationship between and among \nentities that constitute the sensory input. Because of this difference, DRL \nneeds vast amount of experience samples to learn. In this paper, we propose a \nMulti-focus Attention Network (MANet) which mimics human ability to spatially \nabstract the low-level sensory input into multiple entities and attend to them \nsimultaneously. The proposed method first divides the low-level input into \nseveral segments which we refer to as partial states. After this segmentation, \nparallel attention layers attend to the partial states relevant to solving the \ntask. Our model estimates state-action values using these attended partial \nstates. In our experiments, MANet attains highest scores with significantly \nless experience samples. Additionally, the model shows higher performance \ncompared to the Deep Q-network and the single attention model as benchmarks. \nFurthermore, we extend our model to attentive communication model for \nperforming multi-agent cooperative tasks. In multi-agent cooperative task \nexperiments, our model shows 20% faster learning than existing state-of-the-art \nmodel. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b15", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04603"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Igor Halperin", "title": "Inverse Reinforcement Learning for Marketing. (arXiv:1712.04612v1 [q-fin.CP])", "alternate": [{"href": "http://arxiv.org/abs/1712.04612", "type": "text/html"}], "timestampUsec": "1513228697215429", "comments": [], "summary": {"content": "<p>Learning customer preferences from an observed behaviour is an important \ntopic in the marketing literature. Structural models typically model \nforward-looking customers or firms as utility-maximizing agents whose utility \nis estimated using methods of Stochastic Optimal Control. We suggest an \nalternative approach to study dynamic consumer demand, based on Inverse \nReinforcement Learning (IRL). We develop a version of the Maximum Entropy IRL \nthat leads to a highly tractable model formulation that amounts to \nlow-dimensional convex optimization in the search for optimal model parameters. \nUsing simulations of consumer demand, we show that observational noise for \nidentical customers can be easily confused with an apparent consumer \nheterogeneity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04612"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Subhash Kak", "title": "Reasoning in Systems with Elements that Randomly Switch Characteristics. (arXiv:1712.04909v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04909", "type": "text/html"}], "timestampUsec": "1513228697215428", "comments": [], "summary": {"content": "<p>We examine the issue of stability of probability in reasoning about complex \nsystems with uncertainty in structure. Normally, propositions are viewed as \nprobability functions on an abstract random graph where it is implicitly \nassumed that the nodes of the graph have stable properties. But what if some of \nthe nodes change their characteristics? This is a situation that cannot be \ncovered by abstractions of either static or dynamic sets when these changes \ntake place at regular intervals. We propose the use of sets with elements that \nchange, and modular forms are proposed to account for one type of such change. \nAn expression for the dependence of the mean on the probability of the \nswitching elements has been determined. The system is also analyzed from the \nperspective of decision between different hypotheses. Such sets are likely to \nbe of use in complex system queries and in analysis of surveys. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b2b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04909"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Scott Garrabrant, Tsvi Benson-Tilsen, Andrew Critch, Nate Soares, Jessica Taylor", "title": "Logical Induction. (arXiv:1609.03543v4 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1609.03543", "type": "text/html"}], "timestampUsec": "1513228697215427", "comments": [], "summary": {"content": "<p>We present a computable algorithm that assigns probabilities to every logical \nstatement in a given formal language, and refines those probabilities over \ntime. For instance, if the language is Peano arithmetic, it assigns \nprobabilities to all arithmetical statements, including claims about the twin \nprime conjecture, the outputs of long-running computations, and its own \nprobabilities. We show that our algorithm, an instance of what we call a \nlogical inductor, satisfies a number of intuitive desiderata, including: (1) it \nlearns to predict patterns of truth and falsehood in logical statements, often \nlong before having the resources to evaluate the statements, so long as the \npatterns can be written down in polynomial time; (2) it learns to use \nappropriate statistical summaries to predict sequences of statements whose \ntruth values appear pseudorandom; and (3) it learns to have accurate beliefs \nabout its own current beliefs, in a manner that avoids the standard paradoxes \nof self-reference. For example, if a given computer program only ever produces \noutputs in a certain range, a logical inductor learns this fact in a timely \nmanner; and if late digits in the decimal expansion of $\\pi$ are difficult to \npredict, then a logical inductor learns to assign $\\approx 10\\%$ probability to \n\"the $n$th digit of $\\pi$ is a 7\" for large $n$. Logical inductors also learn \nto trust their future beliefs more than their current beliefs, and their \nbeliefs are coherent in the limit (whenever $\\phi \\implies \\psi$, \n$\\mathbb{P}_\\infty(\\phi) \\le \\mathbb{P}_\\infty(\\psi)$, and so on); and logical \ninductors strictly dominate the universal semimeasure in the limit. \n</p> \n<p>These properties and many others all follow from a single logical induction \ncriterion, which is motivated by a series of stock trading analogies. Roughly \nspeaking, each logical sentence $\\phi$ is associated with a stock that is worth \n\\$1 per share if [...] \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b37", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1609.03543"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nick Cheney, Josh Bongard, Vytas SunSpiral, Hod Lipson", "title": "Scalable Co-Optimization of Morphology and Control in Embodied Machines. (arXiv:1706.06133v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.06133", "type": "text/html"}], "timestampUsec": "1513228697215426", "comments": [], "summary": {"content": "<p>Evolution sculpts both the body plans and nervous systems of agents together \nover time. In contrast, in AI and robotics, a robot's body plan is usually \ndesigned by hand, and control policies are then optimized for that fixed \ndesign. The task of simultaneously co-optimizing the morphology and controller \nof an embodied robot has remained a challenge. In psychology, the theory of \nembodied cognition posits that behavior arises from a close coupling between \nbody plan and sensorimotor control, which suggests why co-optimizing these two \nsubsystems is so difficult: most evolutionary changes to morphology tend to \nadversely impact sensorimotor control, leading to an overall decrease in \nbehavioral performance. Here, we further examine this hypothesis and \ndemonstrate a technique for \"morphological innovation protection\", which \ntemporarily reduces selection pressure on recently morphologically-changed \nindividuals, thus enabling evolution some time to \"readapt\" to the new \nmorphology with subsequent control policy mutations. We show the potential for \nthis method to avoid local optima and converge to similar highly fit \nmorphologies across widely varying initial conditions, while sustaining fitness \nimprovements further into optimization. While this technique is admittedly only \nthe first of many steps that must be taken to achieve scalable optimization of \nembodied machines, we hope that theoretical insight into the cause of \nevolutionary stagnation in current methods will help to enable the automation \nof robot design and behavioral training -- while simultaneously providing a \ntestbed to investigate the theory of embodied cognition. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b3b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.06133"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jeremy Morton, Tim A. Wheeler, Mykel J. Kochenderfer", "title": "Closed-Loop Policies for Operational Tests of Safety-Critical Systems. (arXiv:1707.08234v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.08234", "type": "text/html"}], "timestampUsec": "1513228697215425", "comments": [], "summary": {"content": "<p>Manufacturers of safety-critical systems must make the case that their \nproduct is sufficiently safe for public deployment. Much of this case often \nrelies upon critical event outcomes from real-world testing, requiring \nmanufacturers to be strategic about how they allocate testing resources in \norder to maximize their chances of demonstrating system safety. This work \nframes the partially observable and belief-dependent problem of test scheduling \nas a Markov decision process, which can be solved efficiently to yield \nclosed-loop manufacturer testing policies. By solving for policies over a wide \nrange of problem formulations, we are able to provide high-level guidance for \nmanufacturers and regulators on issues relating to the testing of \nsafety-critical systems. This guidance spans an array of topics, including \ncircumstances under which manufacturers should continue testing despite \nobserved incidents, when manufacturers should test aggressively, and when \nregulators should increase or reduce the real-world testing requirements for an \nautonomous vehicle. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b41", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.08234"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jakob N. Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, Igor Mordatch", "title": "Learning with Opponent-Learning Awareness. (arXiv:1709.04326v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04326", "type": "text/html"}], "timestampUsec": "1513228697215424", "comments": [], "summary": {"content": "<p>Multi-agent settings are quickly gathering importance in machine learning. \nBeyond a plethora of recent work on deep multi-agent reinforcement learning, \nhierarchical reinforcement learning, generative adversarial networks and \ndecentralized optimization can all be seen as instances of this setting. \nHowever, the presence of multiple learning agents in these settings renders the \ntraining problem non-stationary and often leads to unstable training or \nundesired final results. We present Learning with Opponent-Learning Awareness \n(LOLA), a method that reasons about the anticipated learning of the other \nagents. The LOLA learning rule includes an additional term that accounts for \nthe impact of the agent's policy on the anticipated parameter update of the \nother agents. We show that the LOLA update rule can be efficiently calculated \nusing an extension of the likelihood ratio policy gradient update, making the \nmethod suitable for model-free RL. This method thus scales to large parameter \nand input spaces and nonlinear function approximators. Preliminary results show \nthat the encounter of two LOLA agents leads to the emergence of tit-for-tat and \ntherefore cooperation in the iterated prisoners' dilemma (IPD), while \nindependent learning does not. In this domain, LOLA also receives higher \npayouts compared to a naive learner, and is robust against exploitation by \nhigher order gradient-based methods. Applied to infinitely repeated matching \npennies, LOLA agents converge to the Nash equilibrium. In a round robin \ntournament we show that LOLA agents can successfully shape the learning of a \nrange of multi-agent learning algorithms from literature, resulting in the \nhighest average returns on the IPD. We also apply LOLA to a grid world task \nwith an embedded social dilemma using deep recurrent policies. Again, by \nconsidering the learning of the other agent, LOLA agents learn to cooperate out \nof selfish interests. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04326"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Brendan Fong, David I. Spivak, R&#xe9;my Tuy&#xe9;ras", "title": "Backprop as Functor: A compositional perspective on supervised learning. (arXiv:1711.10455v2 [math.CT] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10455", "type": "text/html"}], "timestampUsec": "1513228697215422", "comments": [], "summary": {"content": "<p>A supervised learning algorithm searches over a set of functions $A \\to B$ \nparametrised by a space $P$ to find the best approximation to some ideal \nfunction $f\\colon A \\to B$. It does this by taking examples $(a,f(a)) \\in \nA\\times B$, and updating the parameter according to some rule. We define a \ncategory where these update rules may be composed, and show that gradient \ndescent---with respect to a fixed step size and an error function satisfying a \ncertain property---defines a monoidal functor from a category of parametrised \nfunctions to this category of update rules. This provides a structural \nperspective on backpropagation, as well as a broad generalisation of neural \nnetworks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b64", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10455"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Arun Venkitaraman, Dave Zachariah", "title": "Learning Sparse Graphs for Prediction and Filtering of Multivariate Data Processes. (arXiv:1712.04542v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04542", "type": "text/html"}], "timestampUsec": "1513228697215421", "comments": [], "summary": {"content": "<p>We address the problem of prediction and filtering of multivariate data \nprocess using an underlying graph model. We develop a method that learns a \nsparse partial correlation graph in a tuning-free and computationally efficient \nmanner. Specifically, the graph structure is learned recursively without the \nneed for cross-validation or parameter tuning by building upon a \nhyperparameter-free framework. Experiments using real-world datasets show that \nthe proposed method offers significant performance gains in prediction and \nfiltering tasks, in comparison with the graphs frequently associated with these \ndatasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b6a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04542"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Seokhyun Chung, Young Woong Park, Taesu Cheong", "title": "A Mathematical Programming Approach for Integrated Multiple Linear Regression Subset Selection and Validation. (arXiv:1712.04543v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04543", "type": "text/html"}], "timestampUsec": "1513228697215420", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050c032c6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050c032c6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Subset selection for multiple linear regression aims to construct a \nregression model that minimizes errors by selecting a small number of \nexplanatory variables. Once a model is built, various statistical tests and \ndiagnostics are conducted to validate the model and to determine whether \nregression assumptions are met. Most traditional approaches require human \ndecisions at this step, for example, the user adding or removing a variable \nuntil a satisfactory model is obtained. However, this trial-and-error strategy \ncannot guarantee that a subset that minimizes the errors while satisfying all \nregression assumptions will be found. In this paper, we propose a fully \nautomated model building procedure for multiple linear regression subset \nselection that integrates model building and validation based on mathematical \nprogramming. The proposed model minimizes mean squared errors while ensuring \nthat the majority of the important regression assumptions are met. When no \nsubset satisfies all of the considered regression assumptions, our model \nprovides an alternative subset that satisfies most of these assumptions. \nComputational results show that our model yields better solutions (i.e., \nsatisfying more regression assumptions) compared to benchmark models while \nmaintaining similar explanatory power. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b71", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04543"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ruben Martinez-Cantin, Kevin Tee, Michael McCourt", "title": "Practical Bayesian optimization in the presence of outliers. (arXiv:1712.04567v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04567", "type": "text/html"}], "timestampUsec": "1513228697215419", "comments": [], "summary": {"content": "<p>Inference in the presence of outliers is an important field of research as \noutliers are ubiquitous and may arise across a variety of problems and domains. \nBayesian optimization is method that heavily relies on probabilistic inference. \nThis allows outstanding sample efficiency because the probabilistic machinery \nprovides a memory of the whole optimization process. However, that virtue \nbecomes a disadvantage when the memory is populated with outliers, inducing \nbias in the estimation. In this paper, we present an empirical evaluation of \nBayesian optimization methods in the presence of outliers. The empirical \nevidence shows that Bayesian optimization with robust regression often produces \nsuboptimal results. We then propose a new algorithm which combines robust \nregression (a Gaussian process with Student-t likelihood) with outlier \ndiagnostics to classify data points as outliers or inliers. By using an \nscheduler for the classification of outliers, our method is more efficient and \nhas better convergence over the standard robust regression. Furthermore, we \nshow that even in controlled situations with no expected outliers, our method \nis able to produce better results. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b78", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04567"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Branislav Kveton, Csaba Szepesvari, Anup Rao, Zheng Wen, Yasin Abbasi-Yadkori, S. Muthukrishnan", "title": "Stochastic Low-Rank Bandits. (arXiv:1712.04644v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04644", "type": "text/html"}], "timestampUsec": "1513228697215418", "comments": [], "summary": {"content": "<p>Many problems in computer vision and recommender systems involve low-rank \nmatrices. In this work, we study the problem of finding the maximum entry of a \nstochastic low-rank matrix from sequential observations. At each step, a \nlearning agent chooses pairs of row and column arms, and receives the noisy \nproduct of their latent values as a reward. The main challenge is that the \nlatent values are unobserved. We identify a class of non-negative matrices \nwhose maximum entry can be found statistically efficiently and propose an \nalgorithm for finding them, which we call LowRankElim. We derive a \n$\\DeclareMathOperator{\\poly}{poly} O((K + L) \\poly(d) \\Delta^{-1} \\log n)$ \nupper bound on its $n$-step regret, where $K$ is the number of rows, $L$ is the \nnumber of columns, $d$ is the rank of the matrix, and $\\Delta$ is the minimum \ngap. The bound depends on other problem-specific constants that clearly do not \ndepend $K L$. To the best of our knowledge, this is the first such result in \nthe literature. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96b92", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04644"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "D. Belomestny, L. Iosipoi, N. Zhivotovskiy", "title": "Variance reduction via empirical variance minimization: convergence and complexity. (arXiv:1712.04667v1 [math.NA])", "alternate": [{"href": "http://arxiv.org/abs/1712.04667", "type": "text/html"}], "timestampUsec": "1513228697215417", "comments": [], "summary": {"content": "<p>In this paper we propose and study a generic variance reduction approach. The \nproposed method is based on minimization of the empirical variance over a \nsuitable class of zero mean control functionals. We present the corresponding \nconvergence analysis and analyze complexity. Finally some numerical results \nshowing efficiency of the proposed approach are presented. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96ba4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04667"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "George Philipp, Seunghak Lee, Eric P. Xing", "title": "Stability Selection for Structured Variable Selection. (arXiv:1712.04688v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04688", "type": "text/html"}], "timestampUsec": "1513228697215416", "comments": [], "summary": {"content": "<p>In variable or graph selection problems, finding a right-sized model or \ncontrolling the number of false positives is notoriously difficult. Recently, a \nmeta-algorithm called Stability Selection was proposed that can provide \nreliable finite-sample control of the number of false positives. Its benefits \nwere demonstrated when used in conjunction with the lasso and orthogonal \nmatching pursuit algorithms. \n</p> \n<p>In this paper, we investigate the applicability of stability selection to \nstructured selection algorithms: the group lasso and the structured \ninput-output lasso. We find that using stability selection often increases the \npower of both algorithms, but that the presence of complex structure reduces \nthe reliability of error control under stability selection. We give strategies \nfor setting tuning parameters to obtain a good model size under stability \nselection, and highlight its strengths and weaknesses compared to competing \nmethods screen and clean and cross-validation. We give guidelines about when to \nuse which error control method. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bb4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04688"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hideyuki Miyahara, Yuki Sughiyama", "title": "A Quantum Extension of Variational Bayes Inference. (arXiv:1712.04709v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04709", "type": "text/html"}], "timestampUsec": "1513228697215415", "comments": [], "summary": {"content": "<p>Variational Bayes (VB) inference is one of the most important algorithms in \nmachine learning and widely used in engineering and industry. However, VB is \nknown to suffer from the problem of local optima. In this Letter, we generalize \nVB by using quantum mechanics, and propose a new algorithm, which we call \nquantum annealing variational Bayes (QAVB) inference. We then show that QAVB \ndrastically improve the performance of VB by applying them to a clustering \nproblem described by a Gaussian mixture model. Finally, we discuss an intuitive \nunderstanding on how QAVB works well. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bbd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04709"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Loucas Pillaud-Vivien (SIERRA), Alessandro Rudi (SIERRA), Francis Bach (SIERRA)", "title": "Exponential convergence of testing error for stochastic gradient methods. (arXiv:1712.04755v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04755", "type": "text/html"}], "timestampUsec": "1513228697215414", "comments": [], "summary": {"content": "<p>We consider binary classification problems with positive definite kernels and \nsquare loss, and study the convergence rates of stochastic gradient methods. We \nshow that while the excess testing loss (squared loss) converges slowly to zero \nas the number of observations (and thus iterations) goes to infinity, the \ntesting error (classification error) converges exponentially fast if low-noise \nconditions are assumed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bc4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04755"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Cl&#xe9;mentine Barreyre, B&#xe9;atrice Laurent (IMT), Jean-Michel Loubes (IMT), Bertrand Cabon, Lo&#xef;c Boussouf", "title": "Multiple testing for outlier detection in functional data. (arXiv:1712.04775v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04775", "type": "text/html"}], "timestampUsec": "1513228697215413", "comments": [], "summary": {"content": "<p>We propose a novel procedure for outlier detection in functional data, in a \nsemi-supervised framework. As the data is functional, we consider the \ncoefficients obtained after projecting the observations onto orthonormal bases \n(wavelet, PCA). A multiple testing procedure based on the two-sample test is \ndefined in order to highlight the levels of the coefficients on which the \noutliers appear as significantly different to the normal data. The selected \ncoefficients are then called features for the outlier detection, on which we \ncompute the Local Outlier Factor to highlight the outliers. This procedure to \nselect the features is applied on simulated data that mimic the behaviour of \nspace telemetries, and compared with existing dimension reduction techniques. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bc9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04775"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Victor Chernozhukov, Mert Demirer, Esther Duflo, Ivan Fernandez-Val", "title": "Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments. (arXiv:1712.04802v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04802", "type": "text/html"}], "timestampUsec": "1513228697215412", "comments": [], "summary": {"content": "<p>We propose strategies to estimate and make inference on key features of \nheterogeneous effects in randomized experiments. These key features include \nbest linear predictors of the effects using machine learning proxies, average \neffects sorted by impact groups, and average characteristics of most and least \nimpacted units. The approach is valid in high dimensional settings, where the \neffects are proxied by machine learning methods. We post-process these proxies \ninto the estimates of the key features. Our approach is agnostic about the \nproperties of the machine learning estimators used to produce proxies, and it \ncompletely avoids making any strong assumption. Estimation and inference relies \non repeated data splitting to avoid overfitting and achieve validity. Our \nvariational inference method is shown to be uniformly valid and quantifies the \nuncertainty coming from both parameter estimation and data splitting. In \nessence, we take medians of p-values and medians of confidence intervals, \nresulting from many different data splits, and then adjust their nominal level \nto guarantee uniform validity. The inference method could be of substantial \nindependent interest in many machine learning applications. Empirical \napplications illustrate the use of the approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bd2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04802"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tom Hope, Dafna Shahaf", "title": "Ballpark Crowdsourcing: The Wisdom of Rough Group Comparisons. (arXiv:1712.04828v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04828", "type": "text/html"}], "timestampUsec": "1513228697215411", "comments": [], "summary": {"content": "<p>Crowdsourcing has become a popular method for collecting labeled training \ndata. However, in many practical scenarios traditional labeling can be \ndifficult for crowdworkers (for example, if the data is high-dimensional or \nunintuitive, or the labels are continuous). \n</p> \n<p>In this work, we develop a novel model for crowdsourcing that can complement \nstandard practices by exploiting people's intuitions about groups and relations \nbetween them. We employ a recent machine learning setting, called Ballpark \nLearning, that can estimate individual labels given only coarse, aggregated \nsignal over groups of data points. To address the important case of continuous \nlabels, we extend the Ballpark setting (which focused on classification) to \nregression problems. We formulate the problem as a convex optimization problem \nand propose fast, simple methods with an innate robustness to outliers. \n</p> \n<p>We evaluate our methods on real-world datasets, demonstrating how useful \nconstraints about groups can be harnessed from a crowd of non-experts. Our \nmethods can rival supervised models trained on many true labels, and can obtain \nconsiderably better results from the crowd than a standard label-collection \nprocess (for a lower price). By collecting rough guesses on groups of instances \nand using machine learning to infer the individual labels, our lightweight \nframework is able to address core crowdsourcing challenges and train machine \nlearning models in a cost-effective way. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bda", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04828"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sheng Lin, Ning Liu, Mahdi Nazemi, Hongjia Li, Caiwen Ding, Yanzhi Wang, Massoud Pedram", "title": "FFT-Based Deep Learning Deployment in Embedded Systems. (arXiv:1712.04910v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04910", "type": "text/html"}], "timestampUsec": "1513228697215410", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050c036b6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050c036b6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep learning has delivered its powerfulness in many application domains, \nespecially in image and speech recognition. As the backbone of deep learning, \ndeep neural networks (DNNs) consist of multiple layers of various types with \nhundreds to thousands of neurons. Embedded platforms are now becoming essential \nfor deep learning deployment due to their portability, versatility, and energy \nefficiency. The large model size of DNNs, while providing excellent accuracy, \nalso burdens the embedded platforms with intensive computation and storage. \nResearchers have investigated on reducing DNN model size with negligible \naccuracy loss. This work proposes a Fast Fourier Transform (FFT)-based DNN \ntraining and inference model suitable for embedded platforms with reduced \nasymptotic complexity of both computation and storage, making our approach \ndistinguished from existing approaches. We develop the training and inference \nalgorithms based on FFT as the computing kernel and deploy the FFT-based \ninference model on embedded platforms achieving extraordinary processing speed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96be6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04910"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xinkun Nie, Stefan Wager", "title": "Learning Objectives for Treatment Effect Estimation. (arXiv:1712.04912v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04912", "type": "text/html"}], "timestampUsec": "1513228697215409", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050c53b6b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050c53b6b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We develop a general class of two-step algorithms for heterogeneous treatment \neffect estimation in observational studies. We first estimate marginal effects \nand treatment propensities to form an objective function that isolates the \nheterogeneous treatment effects, and then optimize the learned objective. This \napproach has several advantages over existing methods. From a practical \nperspective, our method is very flexible and easy to use: In both steps, we can \nuse any method of our choice, e.g., penalized regression, a deep net, or \nboosting; moreover, these methods can be fine-tuned by cross-validating on the \nlearned objective. Meanwhile, in the case of penalized kernel regression, we \nshow that our method has a quasi-oracle property, whereby even if our pilot \nestimates for marginal effects and treatment propensities are not particularly \naccurate, we achieve the same regret bounds as an oracle who has a-priori \nknowledge of these nuisance components. We implement variants of our method \nbased on both penalized regression and convolutional neural networks, and find \npromising performance relative to existing baselines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96be8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04912"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, James Robins", "title": "Double/Debiased Machine Learning for Treatment and Causal Parameters. (arXiv:1608.00060v6 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1608.00060", "type": "text/html"}], "timestampUsec": "1513228697215408", "comments": [], "summary": {"content": "<p>Most modern supervised statistical/machine learning (ML) methods are \nexplicitly designed to solve prediction problems very well. Achieving this goal \ndoes not imply that these methods automatically deliver good estimators of \ncausal parameters. Examples of such parameters include individual regression \ncoefficients, average treatment effects, average lifts, and demand or supply \nelasticities. In fact, estimates of such causal parameters obtained via naively \nplugging ML estimators into estimating equations for such parameters can behave \nvery poorly due to the regularization bias. Fortunately, this regularization \nbias can be removed by solving auxiliary prediction problems via ML tools. \nSpecifically, we can form an orthogonal score for the target low-dimensional \nparameter by combining auxiliary and main ML predictions. The score is then \nused to build a de-biased estimator of the target parameter which typically \nwill converge at the fastest possible 1/root(n) rate and be approximately \nunbiased and normal, and from which valid confidence intervals for these \nparameters of interest may be constructed. The resulting method thus could be \ncalled a \"double ML\" method because it relies on estimating primary and \nauxiliary predictive models. In order to avoid overfitting, our construction \nalso makes use of the K-fold sample splitting, which we call cross-fitting. \nThis allows us to use a very broad set of ML predictive methods in solving the \nauxiliary and main prediction problems, such as random forest, lasso, ridge, \ndeep neural nets, boosted trees, as well as various hybrids and aggregators of \nthese methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1608.00060"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andreas Svensson, Thomas B. Sch&#xf6;n, Fredrik Lindsten", "title": "Learning of state-space models with highly informative observations: a tempered Sequential Monte Carlo solution. (arXiv:1702.01618v2 [stat.CO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1702.01618", "type": "text/html"}], "timestampUsec": "1513228697215407", "comments": [], "summary": {"content": "<p>Probabilistic (or Bayesian) modeling and learning offers interesting \npossibilities for systematic representation of uncertainty using probability \ntheory. However, probabilistic learning often leads to computationally \nchallenging problems. Some problems of this type that were previously \nintractable can now be solved on standard personal computers thanks to recent \nadvances in Monte Carlo methods. In particular, for learning of unknown \nparameters in nonlinear state-space models, methods based on the particle \nfilter (a Monte Carlo method) have proven very useful. A notoriously \nchallenging problem, however, still occurs when the observations in the \nstate-space model are highly informative, i.e. when there is very little or no \nmeasurement noise present, relative to the amount of process noise. The \nparticle filter will then struggle in estimating one of the basic components \nfor probabilistic learning, namely the likelihood $p($data$|$parameters$)$. To \nthis end we suggest an algorithm which initially assumes that there is \nsubstantial amount of artificial measurement noise present. The variance of \nthis noise is sequentially decreased in an adaptive fashion such that we, in \nthe end, recover the original problem or possibly a very close approximation of \nit. The main component in our algorithm is a sequential Monte Carlo (SMC) \nsampler, which gives our proposed method a clear resemblance to the SMC^2 \nmethod. Another natural link is also made to the ideas underlying the \napproximate Bayesian computation (ABC). We illustrate it with numerical \nexamples, and in particular show promising results for a challenging \nWiener-Hammerstein benchmark problem. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bf1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1702.01618"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xingguo Li, Lin F. Yang, Jason Ge, Jarvis Haupt, Tong Zhang, Tuo Zhao", "title": "On Quadratic Convergence of DC Proximal Newton Algorithm for Nonconvex Sparse Learning in High Dimensions. (arXiv:1706.06066v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.06066", "type": "text/html"}], "timestampUsec": "1513228697215406", "comments": [], "summary": {"content": "<p>We propose a DC proximal Newton algorithm for solving nonconvex regularized \nsparse learning problems in high dimensions. Our proposed algorithm integrates \nthe proximal Newton algorithm with multi-stage convex relaxation based on the \ndifference of convex (DC) programming, and enjoys both strong computational and \nstatistical guarantees. Specifically, by leveraging a sophisticated \ncharacterization of sparse modeling structures/assumptions (i.e., local \nrestricted strong convexity and Hessian smoothness), we prove that within each \nstage of convex relaxation, our proposed algorithm achieves (local) quadratic \nconvergence, and eventually obtains a sparse approximate local optimum with \noptimal statistical properties after only a few convex relaxations. Numerical \nexperiments are provided to support our theory. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96bf6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.06066"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Marie Chavent, Vanessa Kuentz-Simonet, Amaury Labenne, J&#xe9;r&#xf4;me Saracco", "title": "ClustGeo: an R package for hierarchical clustering with spatial constraints. (arXiv:1707.03897v2 [stat.CO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.03897", "type": "text/html"}], "timestampUsec": "1513228697215405", "comments": [], "summary": {"content": "<p>In this paper, we propose a Ward-like hierarchical clustering algorithm \nincluding spatial/geographical constraints. Two dissimilarity matrices $D_0$ \nand $D_1$ are inputted, along with a mixing parameter $\\alpha \\in [0,1]$. The \ndissimilarities can be non-Euclidean and the weights of the observations can be \nnon-uniform. The first matrix gives the dissimilarities in the \"feature space\" \nand the second matrix gives the dissimilarities in the \"constraint space\". The \ncriterion minimized at each stage is a convex combination of the homogeneity \ncriterion calculated with $D_0$ and the homogeneity criterion calculated with \n$D_1$. The idea is then to determine a value of $\\alpha$ which increases the \nspatial contiguity without deteriorating too much the quality of the solution \nbased on the variables of interest i.e. those of the feature space. This \nprocedure is illustrated on a real dataset using the R package ClustGeo. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96c23", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.03897"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Thanh V. Nguyen, Raymond K. W. Wong, Chinmay Hegde", "title": "Provably Accurate Double-Sparse Coding. (arXiv:1711.03638v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.03638", "type": "text/html"}], "timestampUsec": "1513228697215404", "comments": [], "summary": {"content": "<p>Sparse coding is a crucial subroutine in algorithms for various signal \nprocessing, deep learning, and other machine learning applications. The central \ngoal is to learn an overcomplete dictionary that can sparsely represent a given \ninput dataset. However, a key challenge is that storage, transmission, and \nprocessing of the learned dictionary can be untenably high if the data \ndimension is high. In this paper, we consider the double-sparsity model \nintroduced by Rubinstein et al. (2010b) where the dictionary itself is the \nproduct of a fixed, known basis and a data-adaptive sparse component. First, we \nintroduce a simple algorithm for double-sparse coding that can be amenable to \nefficient implementation via neural architectures. Second, we theoretically \nanalyze its performance and demonstrate asymptotic sample complexity and \nrunning time benefits over existing (provable) approaches for sparse coding. To \nour knowledge, our work introduces the first computationally efficient \nalgorithm for double-sparse coding that enjoys rigorous statistical guarantees. \nFinally, we support our analysis via several numerical experiments on simulated \ndata, confirming that our method can indeed be useful in problem sizes \nencountered in practical applications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96c42", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.03638"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jihun Hamm", "title": "Machine vs Machine: Minimax-Optimal Defense Against Adversarial Examples. (arXiv:1711.04368v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.04368", "type": "text/html"}], "timestampUsec": "1513228697215403", "comments": [], "summary": {"content": "<p>Recently, researchers have discovered that the state-of-the-art object \nclassifiers can be fooled easily by small perturbations in the input \nunnoticeable to human eyes. It is known that an attacker can generate strong \nadversarial examples if she knows the classifier parameters. Conversely, a \ndefender can robustify the classifier by retraining if she has the adversarial \nexamples. The cat-and-mouse game nature of attacks and defenses raises the \nquestion of the presence of equilibria in the dynamics. In this paper, we \npresent a neural-network based attack class to approximate a larger but \nintractable class of attacks, and formulate the attacker-defender interaction \nas a zero-sum leader-follower game. We present sensitivity-penalized \noptimization algorithms to find minimax solutions, which are the best \nworst-case defenses against whitebox attacks. Advantages of the learning-based \nattacks and defenses compared to gradient-based attacks and defenses are \ndemonstrated with MNIST and CIFAR-10. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96c52", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.04368"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Robert Geirhos, David H. J. Janssen, Heiko H. Sch&#xfc;tt, Jonas Rauber, Matthias Bethge, Felix A. Wichmann", "title": "Comparing deep neural networks against humans: object recognition when the signal gets weaker. (arXiv:1706.06969v1 [cs.CV] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1706.06969", "type": "text/html"}], "timestampUsec": "1513228697215401", "comments": [], "summary": {"content": "<p>Human visual object recognition is typically rapid and seemingly effortless, \nas well as largely independent of viewpoint and object orientation. Until very \nrecently, animate visual systems were the only ones capable of this remarkable \ncomputational feat. This has changed with the rise of a class of computer \nvision algorithms called deep neural networks (DNNs) that achieve human-level \nclassification performance on object recognition tasks. Furthermore, a growing \nnumber of studies report similarities in the way DNNs and the human visual \nsystem process objects, suggesting that current DNNs may be good models of \nhuman visual object recognition. Yet there clearly exist important \narchitectural and processing differences between state-of-the-art DNNs and the \nprimate visual system. The potential behavioural consequences of these \ndifferences are not well understood. We aim to address this issue by comparing \nhuman and DNN generalisation abilities towards image degradations. We find the \nhuman visual system to be more robust to image manipulations like contrast \nreduction, additive noise or novel eidolon-distortions. In addition, we find \nprogressively diverging classification error-patterns between man and DNNs when \nthe signal gets weaker, indicating that there may still be marked differences \nin the way humans and current DNNs perform visual object recognition. We \nenvision that our findings as well as our carefully measured and freely \navailable behavioural datasets provide a new useful benchmark for the computer \nvision community to improve the robustness of DNNs and a motivation for \nneuroscientists to search for mechanisms in the brain that could facilitate \nthis robustness. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513228697215", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346d96c57", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.06969"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Maryam Fazel-Zarandi, Shang-Wen Li, Jin Cao, Jared Casale, Peter Henderson, David Whitney, Alborz Geramifard", "title": "Learning Robust Dialog Policies in Noisy Environments. (arXiv:1712.04034v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.04034", "type": "text/html"}], "timestampUsec": "1513142281080184", "comments": [], "summary": {"content": "<p>Modern virtual personal assistants provide a convenient interface for \ncompleting daily tasks via voice commands. An important consideration for these \nassistants is the ability to recover from automatic speech recognition (ASR) \nand natural language understanding (NLU) errors. In this paper, we focus on \nlearning robust dialog policies to recover from these errors. To this end, we \ndevelop a user simulator which interacts with the assistant through voice \ncommands in realistic scenarios with noisy audio, and use it to learn dialog \npolicies through deep reinforcement learning. We show that dialogs generated by \nour simulator are indistinguishable from human generated dialogs, as determined \nby human evaluators. Furthermore, preliminary experimental results show that \nthe learned policies in noisy environments achieve the same execution success \nrate with fewer dialog turns compared to fixed rule-based policies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513142281080", "annotations": [], "published": 1513142281, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034613311b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04034"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Haitao Zhao", "title": "Neural Component Analysis for Fault Detection. (arXiv:1712.04118v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04118", "type": "text/html"}], "timestampUsec": "1513141858490941", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050c53dd2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050c53dd2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Principal component analysis (PCA) is largely adopted for chemical process \nmonitoring and numerous PCA-based systems have been developed to solve various \nfault detection and diagnosis problems. Since PCA-based methods assume that the \nmonitored process is linear, nonlinear PCA models, such as autoencoder models \nand kernel principal component analysis (KPCA), has been proposed and applied \nto nonlinear process monitoring. However, KPCA-based methods need to perform \neigen-decomposition (ED) on the kernel Gram matrix whose dimensions depend on \nthe number of training data. Moreover, prefixed kernel parameters cannot be \nmost effective for different faults which may need different parameters to \nmaximize their respective detection performances. Autoencoder models lack the \nconsideration of orthogonal constraints which is crucial for PCA-based \nalgorithms. To address these problems, this paper proposes a novel nonlinear \nmethod, called neural component analysis (NCA), which intends to train a \nfeedforward neural work with orthogonal constraints such as those used in PCA. \nNCA can adaptively learn its parameters through backpropagation and the \ndimensionality of the nonlinear features has no relationship with the number of \ntraining samples. Extensive experimental results on the Tennessee Eastman (TE) \nbenchmark process show the superiority of NCA in terms of missed detection rate \n(MDR) and false alarm rate (FAR). The source code of NCA can be found in \nhttps://github.com/haitaozhao/Neural-Component-Analysis.git. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127d8a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04118"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Daniel Hein, Steffen Udluft, Thomas A. Runkler", "title": "Interpretable Policies for Reinforcement Learning by Genetic Programming. (arXiv:1712.04170v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04170", "type": "text/html"}], "timestampUsec": "1513141858490940", "comments": [], "summary": {"content": "<p>The search for interpretable reinforcement learning policies is of high \nacademic and industrial interest. Especially for industrial systems, domain \nexperts are more likely to deploy autonomously learned controllers if they are \nunderstandable and convenient to evaluate. Basic algebraic equations are \nsupposed to meet these requirements, as long as they are restricted to an \nadequate complexity. Here we introduce the genetic programming for \nreinforcement learning (GPRL) approach based on model-based batch reinforcement \nlearning and genetic programming, which autonomously learns policy equations \nfrom pre-existing default state-action trajectory samples. GPRL is compared to \na straight-forward method which utilizes genetic programming for symbolic \nregression, yielding policies imitating an existing well-performing, but \nnon-interpretable policy. Experiments on three reinforcement learning \nbenchmarks, i.e., mountain car, cart-pole balancing, and industrial benchmark, \ndemonstrate the superiority of our GPRL approach compared to the symbolic \nregression method. GPRL is capable of producing well-performing interpretable \nreinforcement learning policies from pre-existing default trajectory data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127d8e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04170"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "V.I. Avrutskiy", "title": "Backpropagation generalized for output derivatives. (arXiv:1712.04185v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.04185", "type": "text/html"}], "timestampUsec": "1513141858490939", "comments": [], "summary": {"content": "<p>Backpropagation algorithm is the cornerstone for neural network analysis. \nPaper extends it for training any derivatives of neural network's output with \nrespect to its input. By the dint of it feedforward networks can be used to \nsolve or verify solutions of partial or simple, linear or nonlinear \ndifferential equations. This method vastly differs from traditional ones like \nfinite differences on a mesh. It contains no approximations, but rather an \nexact form of differential operators. Algorithm is built to train a feed \nforward network with any number of hidden layers and any kind of sufficiently \nsmooth activation functions. It's presented in a form of matrix-vector products \nso highly parallel implementation is readily possible. First part derives the \nmethod for 2D case with first and second order derivatives, second part extends \nit to N-dimensional case with any derivatives. All necessary expressions for \nusing this method to solve most applied PDE can be found in Appendix D. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127d98", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04185"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yoshihiro Nagano, Ryo Karakida, Masato Okada", "title": "Concept Formation and Dynamics of Repeated Inference in Deep Generative Models. (arXiv:1712.04195v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04195", "type": "text/html"}], "timestampUsec": "1513141858490938", "comments": [], "summary": {"content": "<p>Deep generative models are reported to be useful in broad applications \nincluding image generation. Repeated inference between data space and latent \nspace in these models can denoise cluttered images and improve the quality of \ninferred results. However, previous studies only qualitatively evaluated image \noutputs in data space, and the mechanism behind the inference has not been \ninvestigated. The purpose of the current study is to numerically analyze \nchanges in activity patterns of neurons in the latent space of a deep \ngenerative model called a \"variational auto-encoder\" (VAE). What kinds of \ninference dynamics the VAE demonstrates when noise is added to the input data \nare identified. The VAE embeds a dataset with clear cluster structures in the \nlatent space and the center of each cluster of multiple correlated data points \n(memories) is referred as the concept. Our study demonstrated that transient \ndynamics of inference first approaches a concept, and then moves close to a \nmemory. Moreover, the VAE revealed that the inference dynamics approaches a \nmore abstract concept to the extent that the uncertainty of input data \nincreases due to noise. It was demonstrated that by increasing the number of \nthe latent variables, the trend of the inference dynamics to approach a concept \ncan be enhanced, and the generalization ability of the VAE can be improved. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127d9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04195"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wieland Brendel, Jonas Rauber, Matthias Bethge", "title": "Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models. (arXiv:1712.04248v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04248", "type": "text/html"}], "timestampUsec": "1513141858490937", "comments": [], "summary": {"content": "<p>Many machine learning algorithms are vulnerable to almost imperceptible \nperturbations of their inputs. So far it was unclear how much risk adversarial \nperturbations carry for the safety of real-world machine learning applications \nbecause most methods used to generate such perturbations rely either on \ndetailed model information (gradient-based attacks) or on confidence scores \nsuch as class probabilities (score-based attacks), neither of which are \navailable in most real-world scenarios. In many such cases one currently needs \nto retreat to transfer-based attacks which rely on cumbersome substitute \nmodels, need access to the training data and can be defended against. Here we \nemphasise the importance of attacks which solely rely on the final model \ndecision. Such decision-based attacks are (1) applicable to real-world \nblack-box models such as autonomous cars, (2) need less knowledge and are \neasier to apply than transfer-based attacks and (3) are more robust to simple \ndefences than gradient- or score-based attacks. Previous attacks in this \ncategory were limited to simple models or simple datasets. Here we introduce \nthe Boundary Attack, a decision-based attack that starts from a large \nadversarial perturbation and then seeks to reduce the perturbation while \nstaying adversarial. The attack is conceptually simple, requires close to no \nhyperparameter tuning, does not rely on substitute models and is competitive \nwith the best gradient-based attacks in standard computer vision tasks like \nImageNet. We apply the attack on two black-box algorithms from Clarifai.com. \nThe Boundary Attack in particular and the class of decision-based attacks in \ngeneral open new avenues to study the robustness of machine learning models and \nraise new questions regarding the safety of deployed machine learning systems. \nAn implementation of the attack is available as part of Foolbox at \nhttps://github.com/bethgelab/foolbox . \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127da1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04248"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nicola Milano, Paolo Pagliuca, Stefano Nolfi", "title": "Robustness, Evolvability and Phenotypic Complexity: Insights from Evolving Digital Circuits. (arXiv:1712.04254v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.04254", "type": "text/html"}], "timestampUsec": "1513141858490936", "comments": [], "summary": {"content": "<p>We show how the characteristics of the evolutionary algorithm influence the \nevolvability of candidate solutions, i.e. the propensity of evolving \nindividuals to generate better solutions as a result of genetic variation. More \nspecifically, (1+{\\lambda}) evolutionary strategies largely outperform \n({\\mu}+1) evolutionary strategies in the context of the evolution of digital \ncircuits --- a domain characterized by a high level of neutrality. This \ndifference is due to the fact that the competition for robustness to mutations \namong the circuits evolved with ({\\mu}+1) evolutionary strategies leads to the \nselection of phenotypically simple but low evolvable circuits. These circuits \nachieve robustness by minimizing the number of functional genes rather than by \nrelying on redundancy or degeneracy to buffer the effects of mutations. The \nanalysis of these factors enabled us to design a new evolutionary algorithm, \nnamed Parallel Stochastic Hill Climber (PSHC), which outperforms the other two \nmethods considered. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127da4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04254"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Albert Gatt, Emiel Krahmer", "title": "Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation. (arXiv:1703.09902v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.09902", "type": "text/html"}], "timestampUsec": "1513141858490935", "comments": [], "summary": {"content": "<p>This paper surveys the current state of the art in Natural Language \nGeneration (NLG), defined as the task of generating text or speech from \nnon-linguistic input. A survey of NLG is timely in view of the changes that the \nfield has undergone over the past decade or so, especially in relation to new \n(usually data-driven) methods, as well as new applications of NLG technology. \nThis survey therefore aims to (a) give an up-to-date synthesis of research on \nthe core tasks in NLG and the architectures adopted in which such tasks are \norganised; (b) highlight a number of relatively recent research topics that \nhave arisen partly as a result of growing synergies between NLG and other areas \nof artificial intelligence; (c) draw attention to the challenges in NLG \nevaluation, relating them to similar challenges faced in other areas of Natural \nLanguage Processing, with an emphasis on different evaluation methods and the \nrelationships between them. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127dbb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.09902"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Vedran Dunjko, Yi-Kai Liu, Xingyao Wu, Jacob M. Taylor", "title": "Super-polynomial and exponential improvements for quantum-enhanced reinforcement learning. (arXiv:1710.11160v2 [quant-ph] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.11160", "type": "text/html"}], "timestampUsec": "1513141858490934", "comments": [], "summary": {"content": "<p>Recent work on quantum machine learning has demonstrated that quantum \ncomputers can offer dramatic improvements over classical devices for data \nmining, prediction and classification. However, less is known about the \nadvantages using quantum computers may bring in the more general setting of \nreinforcement learning, where learning is achieved via interaction with a task \nenvironment that provides occasional rewards. Reinforcement learning can \nincorporate data-analysis-oriented learning settings as special cases, but also \nincludes more complex situations where, e.g., reinforcing feedback is delayed. \nIn a few recent works, Grover-type amplification has been utilized to construct \nquantum agents that achieve up-to-quadratic improvements in learning \nefficiency. These encouraging results have left open the key question of \nwhether super-polynomial improvements in learning times are possible for \ngenuine reinforcement learning problems, that is problems that go beyond the \nother more restricted learning paradigms. In this work, we provide a family of \nsuch genuine reinforcement learning tasks. We construct quantum-enhanced \nlearners which learn super-polynomially, and even exponentially faster than any \nclassical reinforcement learning model, and we discuss the potential impact our \nresults may have on future technologies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127de5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.11160"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Daniel George, E. A. Huerta", "title": "Deep Learning for Real-time Gravitational Wave Detection and Parameter Estimation with LIGO Data. (arXiv:1711.07966v2 [gr-qc] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07966", "type": "text/html"}], "timestampUsec": "1513141858490933", "comments": [], "summary": {"content": "<p>The recent Nobel-prize-winning detections of gravitational waves from merging \nblack holes and the subsequent detection of the collision of two neutron stars \nin coincidence with electromagnetic observations have inaugurated a new era of \nmultimessenger astrophysics. To enhance the scope of this emergent science, we \nproposed the use of deep convolutional neural networks for the detection and \ncharacterization of gravitational wave signals in real-time. This method, Deep \nFiltering, was initially demonstrated using simulated LIGO noise. In this \narticle, we present the extension of Deep Filtering using real data from the \nfirst observing run of LIGO, for both detection and parameter estimation of \ngravitational waves from binary black hole mergers with continuous data streams \nfrom multiple LIGO detectors. We show for the first time that machine learning \ncan detect and estimate the true parameters of a real GW event observed by \nLIGO. Our comparisons show that Deep Filtering is far more computationally \nefficient than matched-filtering, while retaining similar sensitivity and lower \nerrors, allowing real-time processing of weak time-series signals in \nnon-stationary non-Gaussian noise, with minimal resources, and also enables the \ndetection of new classes of gravitational wave sources that may go unnoticed \nwith existing detection algorithms. This approach is uniquely suited to enable \ncoincident detection campaigns of gravitational waves and their multimessenger \ncounterparts in real-time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127df7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07966"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, Aleksander Madry", "title": "A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations. (arXiv:1712.02779v2 [cs.LG] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02779", "type": "text/html"}], "timestampUsec": "1513141858490931", "comments": [], "summary": {"content": "<p>Recent work has shown that neural network-based vision classifiers exhibit a \nsignificant vulnerability to misclassifications caused by imperceptible but \nadversarial perturbations of their inputs. These perturbations, however, are \npurely pixel-wise and built out of loss function gradients of either the \nattacked model or its surrogate. As a result, they tend to be contrived and \nlook pretty artificial. This might suggest that such vulnerability to slight \ninput perturbations can only arise in a truly adversarial setting and thus is \nunlikely to be an issue in more \"natural\" contexts. \n</p> \n<p>In this paper, we provide evidence that such belief might be incorrect. We \ndemonstrate that significantly simpler, and more likely to occur naturally, \ntransformations of the input - namely, rotations and translations alone, \nsuffice to significantly degrade the classification performance of neural \nnetwork-based vision models across a spectrum of datasets. This remains to be \nthe case even when these models are trained using appropriate data \naugmentation. Finding such \"fooling\" transformations does not require having \nany special access to the model - just trying out a small number of random \nrotation and translation combinations already has a significant effect. These \nfindings suggest that our current neural network-based vision models might not \nbe as reliable as we tend to assume. \n</p> \n<p>Finally, we consider a new class of perturbations that combines rotations and \ntranslations with the standard pixel-wise attacks. We observe that these two \ntypes of input transformations are, in a sense, orthogonal to each other. Their \neffect on the performance of the model seems to be additive, while robustness \nto one type does not seem to affect the robustness to the other type. This \nsuggests that this combined class of transformations is a more complete notion \nof similarity in the context of adversarial robustness of vision models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e17", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02779"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Michael Bernico, Yuntao Li, Dingchao Zhang", "title": "Investigation on How Data Volume Affects Transfer Learning Performances in Business Applications. (arXiv:1712.04008v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.04008", "type": "text/html"}], "timestampUsec": "1513141858490930", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050c53ff1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050c53ff1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Transfer Learning helps to build a system to recognize and apply knowledge \nand experience learned in previous tasks (source task) to new tasks or new \ndomains (target task), which share some commonality. The two important factors \nthat impact the performance of transfer learning models are: (a) the size of \nthe target dataset and (b) the similarity in distribution between source and \ntarget domains. Thus far there has been little investigation into just how \nimportant these factors are. In this paper, we investigated the impact of \ntarget dataset size and source/target domain similarity on model performance \nthrough a series of experiments. We found that more data is always beneficial, \nand that model performance improved linearly with the log of data size, until \nwe were out of data. As source/target domains differ, more data is required and \nfine tuning will render better performance than feature extraction. When \nsource/target domains are similar and data size is small, fine tuning and \nfeature extraction renders equivalent performance. We hope that our study \ninspires further work in transfer learning, which continues to be a very \nimportant technique for developing practical machine learning applications in \nbusiness domains. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04008"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Roman V. Yampolskiy", "title": "Detecting Qualia in Natural and Artificial Agents. (arXiv:1712.04020v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04020", "type": "text/html"}], "timestampUsec": "1513141858490929", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050ca8880\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050ca8880&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The Hard Problem of consciousness has been dismissed as an illusion. By \nshowing that computers are capable of experiencing, we show that they are at \nleast rudimentarily conscious with potential to eventually reach \nsuperconsciousness. The main contribution of the paper is a test for confirming \ncertain subjective experiences in a tested agent. We follow with analysis of \nbenefits and problems with conscious machines and implications of such \ncapability on future of computing, machine rights and artificial intelligence \nsafety. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04020"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Miao Liu, Marlos C. Machado, Gerald Tesauro, Murray Campbell", "title": "The Eigenoption-Critic Framework. (arXiv:1712.04065v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04065", "type": "text/html"}], "timestampUsec": "1513141858490928", "comments": [], "summary": {"content": "<p>Eigenoptions (EOs) have been recently introduced as a promising idea for \ngenerating a diverse set of options through the graph Laplacian, having been \nshown to allow efficient exploration. Despite its initial promising results, a \ncouple of issues in current algorithms limit its application, namely: (1) EO \nmethods require two separate steps (eigenoption discovery and reward \nmaximization) to learn a control policy, which can incur a significant amount \nof storage and computation; (2) EOs are only defined for problems with discrete \nstate-spaces and; (3) it is not easy to take the environment's reward function \ninto consideration when discovering EOs. To addresses these issues, we \nintroduce an algorithm termed eigenoption-critic (EOC) based on the \nOption-critic (OC) framework [Bacon17], a general hierarchical reinforcement \nlearning (RL) algorithm that allows learning the intra-option policies \nsimultaneously with the policy over options. We also propose a generalization \nof EOC to problems with continuous state-spaces through the Nystr\\\"om \napproximation. EOC can also be seen as extending OC to nonstationary settings, \nwhere the discovered options are not tailored for a single task. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e44", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04065"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Thomas Bartz-Beielstein, Lorenzo Gentile, Martin Zaefferer", "title": "In a Nutshell: Sequential Parameter Optimization. (arXiv:1712.04076v1 [cs.MS])", "alternate": [{"href": "http://arxiv.org/abs/1712.04076", "type": "text/html"}], "timestampUsec": "1513141858490927", "comments": [], "summary": {"content": "<p>The performance of optimization algorithms relies crucially on their \nparameterizations. Finding good parameter settings is called algorithm tuning. \nUsing a simple simulated annealing algorithm, we will demonstrate how \noptimization algorithms can be tuned using the sequential parameter \noptimization toolbox (SPOT). SPOT provides several tools for automated and \ninteractive tuning. The underling concepts of the SPOT approach are explained. \nThis includes key techniques such as exploratory fitness landscape analysis and \nresponse surface methodology. Many examples illustrate how SPOT can be used for \nunderstanding the performance of algorithms and gaining insight into \nalgorithm's behavior. Furthermore, we demonstrate how SPOT can be used as an \noptimizer and how a sophisticated ensemble approach is able to combine several \nmeta models via stacking. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e49", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04076"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, Zhangyang Wang", "title": "RESIDE: A Benchmark for Single Image Dehazing. (arXiv:1712.04143v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.04143", "type": "text/html"}], "timestampUsec": "1513141858490926", "comments": [], "summary": {"content": "<p>In this paper, we present a comprehensive study and evaluation of existing \nsingle image dehazing algorithms, using a new large-scale benchmark consisting \nof both synthetic and real-world hazy images, called REalistic Single Image \nDEhazing (RESIDE). RESIDE highlights diverse data sources and image contents, \nand is divided into five subsets, each serving different training or evaluation \npurposes. We further provide a rich variety of criteria for dehazing algorithm \nevaluation, ranging from full-reference metrics, to no-reference metrics, to \nsubjective evaluation and the novel task-driven evaluation. Experiments on \nRESIDE sheds light on the comparisons and limitations of state-of-the-art \ndehazing algorithms, and suggest promising future directions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04143"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jingyi Wang, Jun Sun, Yifan Jia", "title": "Toward `verifying' a Water Treatment System. (arXiv:1712.04155v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04155", "type": "text/html"}], "timestampUsec": "1513141858490925", "comments": [], "summary": {"content": "<p>Modeling and verifying real-world cyber-physical systems are challenging, \nespecially so for complex systems where manually modeling is infeasible. In \nthis work, we report our experience on combining model learning and abstraction \nrefinement to analyze a challenging system, i.e., a real-world Secure Water \nTreatment (SWaT) system. Given a set of safety requirements, the objective is \nto either show that the system is safe with a high probability (so that a \nsystem shutdown is rarely triggered due to safety violation) or otherwise. As \nthe system is too complicated to be manually modelled, we apply latest \nautomatic model learning techniques to construct a set of Markov chains through \nabstraction and refinement, based on two long system execution logs (one for \ntraining and the other for testing). For each probabilistic property, we either \nreport it does not hold with a certain level of probabilistic confidence, or \nreport that it holds by showing the evidence in the form of an abstract Markov \nchain. The Markov chains can subsequently be implemented as runtime monitors in \nSWaT. This is the first case study of applying model learning techniques to a \nreal-world system as far as we know. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e53", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04155"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Niek Tax, Marlon Dumas", "title": "Mining Non-Redundant Sets of Generalizing Patterns from Sequence Databases. (arXiv:1712.04159v1 [cs.DS])", "alternate": [{"href": "http://arxiv.org/abs/1712.04159", "type": "text/html"}], "timestampUsec": "1513141858490924", "comments": [], "summary": {"content": "<p>Sequential pattern mining techniques extract patterns corresponding to \nfrequent subsequences from a sequence database. A practical limitation of these \ntechniques is that they overload the user with too many patterns. Local Process \nModel (LPM) mining is an alternative approach coming from the field of process \nmining. While in traditional sequential pattern mining, a pattern describes one \nsubsequence, an LPM captures a set of subsequences. Also, while traditional \nsequential patterns only match subsequences that are observed in the sequence \ndatabase, an LPM may capture subsequences that are not explicitly observed, but \nthat are related to observed subsequences. In other words, LPMs generalize the \nbehavior observed in the sequence database. These properties make it possible \nfor a set of LPMs to cover the behavior of a much larger set of sequential \npatterns. Yet, existing LPM mining techniques still suffer from the pattern \nexplosion problem because they produce sets of redundant LPMs. In this paper, \nwe propose several heuristics to mine a set of non-redundant LPMs either from a \nset of redundant LPMs or from a set of sequential patterns. We empirically \ncompare the proposed heuristics between them and against existing (local) \nprocess mining techniques in terms of coverage, precision, and complexity of \nthe produced sets of LPMs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e5b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04159"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yueh-Hua Wu, Shou-De Lin", "title": "A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents. (arXiv:1712.04172v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04172", "type": "text/html"}], "timestampUsec": "1513141858490923", "comments": [], "summary": {"content": "<p>This paper proposes a low-cost, easily realizable strategy to equip a \nreinforcement learning (RL) agent the capability of behaving ethically. Our \nmodel allows the designers of RL agents to solely focus on the task to achieve, \nwithout having to worry about the implementation of multiple trivial ethical \npatterns to follow. Based on the assumption that the majority of human \nbehavior, regardless which goals they are achieving, is ethical, our design \nintegrates human policy with the RL policy to achieve the target objective with \nless chance of violating the ethical code that human beings normally obey. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e62", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04172"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wenpin Jiao", "title": "Contradiction-Centricity: A Uniform Model for Formation of Swarm Intelligence and its Simulations. (arXiv:1712.04182v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04182", "type": "text/html"}], "timestampUsec": "1513141858490922", "comments": [], "summary": {"content": "<p>It is a grand challenge to model the emergence of swarm intelligence and many \nprinciples or models had been proposed. However, existing models do not catch \nthe nature of swarm intelligence and they are not generic enough to describe \nvarious types of emergence phenomena. In this work, we propose a \ncontradiction-centric model for emergence of swarm intelligence, in which \nindividuals' contradictions dominate their appearances whilst they are \nassociated and interacting to update their contradictions. This model \nhypothesizes that 1) the emergence of swarm intelligence is rooted in the \ndevelopment of contradictions of individuals and the interactions among \nassociated individuals and 2) swarm intelligence is essentially a combinative \nreflection of the configurations of contradictions inside individuals and the \ndistributions of contradictions among individuals. To verify the feasibility of \nthe model, we simulate four types of swarm intelligence. As the simulations \nshow, our model is truly generic and can describe the emergence of a variety of \nswarm intelligence, and it is also very simple and can be easily applied to \ndemonstrate the emergence of swarm intelligence without needing complicated \ncomputations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e69", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04182"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mehdi Mohammadi, Ala Al-Fuqaha, Sameh Sorour, Mohsen Guizani", "title": "Deep Learning for IoT Big Data and Streaming Analytics: A Survey. (arXiv:1712.04301v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04301", "type": "text/html"}], "timestampUsec": "1513141858490921", "comments": [], "summary": {"content": "<p>In the era of the Internet of Things (IoT), an enormous amount of sensing \ndevices collect and/or generate various sensory data over time for a wide range \nof fields and applications. Based on the nature of the application, these \ndevices will result in big or fast/real-time data streams. Applying analytics \nover such data streams to discover new information, predict future insights, \nand make control decisions is a crucial process that makes IoT a worthy \nparadigm for businesses and a quality-of-life improving technology. In this \npaper, we provide a thorough overview on using a class of advanced machine \nlearning techniques, namely Deep Learning (DL), to facilitate the analytics and \nlearning in the IoT domain. We start by articulating IoT data characteristics \nand identifying two major treatments for IoT data from a machine learning \nperspective, namely IoT big data analytics and IoT streaming data analytics. We \nalso discuss why DL is a promising approach to achieve the desired analytics in \nthese types of data and applications. The potential of using emerging DL \ntechniques for IoT data analytics are then discussed, and its promises and \nchallenges are introduced. We present a comprehensive background on different \nDL architectures and algorithms. We also analyze and summarize major reported \nresearch attempts that leveraged DL in the IoT domain. The smart IoT devices \nthat have incorporated DL in their intelligence background are also discussed. \nDL implementation approaches on the fog and cloud centers in support of IoT \napplications are also surveyed. Finally, we shed light on some challenges and \npotential directions for future research. At the end of each section, we \nhighlight the lessons learned based on our experiments and review of the recent \nliterature. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e71", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04301"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mihai Nadin", "title": "In folly ripe. In reason rotten. Putting machine theology to rest. (arXiv:1712.04306v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04306", "type": "text/html"}], "timestampUsec": "1513141858490920", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050ca8ad0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050ca8ad0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Computation has changed the world more than any previous expressions of \nknowledge. In its particular algorithmic embodiment, it offers a perspective, \nwithin which the digital computer (one of many possible) exercises a role \nreminiscent of theology. Since it is closed to meaning, algorithmic digital \ncomputation can at most mimic the creative aspects of life. AI, in the \nperspective of time, proved to be less an acronym for artificial intelligence \nand more of automating tasks associated with intelligence. The entire \ndevelopment led to the hypostatized role of the machine: outputting nothing \nelse but reality, including that of the humanity that made the machine happen. \nThe convergence machine called deep learning is only the latest form through \nwhich the deterministic theology of the machine claims more than what extremely \neffective data processing actually is. A new understanding of complexity, as \nwell as the need to distinguish between the reactive nature of the artificial \nand the anticipatory nature of the living are suggested as practical responses \nto the challenges posed by machine theology. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e75", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04306"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gopal P. Sarma, Nick J. Hay, Adam Safron", "title": "AI Safety and Reproducibility: Establishing Robust Foundations for the Neuroscience of Human Values. (arXiv:1712.04307v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04307", "type": "text/html"}], "timestampUsec": "1513141858490919", "comments": [], "summary": {"content": "<p>We propose the creation of a systematic effort to identify and replicate key \nfindings in neuroscience and allied fields related to understanding human \nvalues. Our aim is to ensure that research underpinning the value alignment \nproblem of artificial intelligence has been sufficiently validated to play a \nrole in the design of AI systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e7d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04307"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Claudio Gallicchio, Alessio Micheli", "title": "Deep Echo State Network (DeepESN): A Brief Survey. (arXiv:1712.04323v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04323", "type": "text/html"}], "timestampUsec": "1513141858490918", "comments": [], "summary": {"content": "<p>The study of deep recurrent neural networks (RNNs) and, in particular, of \ndeep Reservoir Computing (RC) is gaining an increasing research attention in \nthe neural networks community. The recently introduced deep Echo State Network \n(deepESN) model opened the way to an extremely efficient approach for designing \ndeep neural networks for temporal data. At the same time, the study of deepESNs \nallowed to shed light on the intrinsic properties of state dynamics developed \nby hierarchical compositions of recurrent layers, i.e. on the bias of depth in \nRNNs architectural design. In this paper, we summarize the advancements in the \ndevelopment, analysis and applications of deepESNs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e84", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04323"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Patrick Klose, Rudolf Mester", "title": "Simulated Autonomous Driving on Realistic Road Networks using Deep Reinforcement Learning. (arXiv:1712.04363v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04363", "type": "text/html"}], "timestampUsec": "1513141858490917", "comments": [], "summary": {"content": "<p>Using Deep Reinforcement Learning (DRL) can be a promising approach to handle \ntasks in the field of (simulated) autonomous driving, whereby recent \npublications only consider learning in unusual driving environments. This paper \noutlines a developed software, which instead can be used for evaluating DRL \nalgorithms based on realistic road networks and therefore in more usual driving \nenvironments. Furthermore, we identify difficulties when DRL algorithms are \napplied to tasks, in which it is not only important to reach a goal, but also \nhow this goal is reached. We conclude this paper by presenting the results of \nan application of a new DRL algorithm, which can partly solve these problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e8a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04363"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Amrita Gupta, Mehrdad Farajtabar, Bistra Dilkina, Hongyuan Zha", "title": "Hawkes Processes for Invasive Species Modeling and Management. (arXiv:1712.04386v1 [q-bio.PE])", "alternate": [{"href": "http://arxiv.org/abs/1712.04386", "type": "text/html"}], "timestampUsec": "1513141858490916", "comments": [], "summary": {"content": "<p>The spread of invasive species to new areas threatens the stability of \necosystems and causes major economic losses in agriculture and forestry. We \npropose a novel approach to minimizing the spread of an invasive species given \na limited intervention budget. We first model invasive species propagation \nusing Hawkes processes, and then derive closed-form expressions for \ncharacterizing the effect of an intervention action on the invasion process. We \nuse this to obtain an optimal intervention plan based on an integer programming \nformulation, and compare the optimal plan against several \necologically-motivated heuristic strategies used in practice. We present an \nempirical study of two variants of the invasive control problem: minimizing the \nfinal rate of invasions, and minimizing the number of invasions at the end of a \ngiven time horizon. Our results show that the optimized intervention achieves \nnearly the same level of control that would be attained by completely \neradicating the species, with a 20% cost saving. Additionally, we design a \nheuristic intervention strategy based on a combination of the density and life \nstage of the invasive individuals, and find that it comes surprisingly close to \nthe optimized strategy, suggesting that this could serve as a good rule of \nthumb in invasive species management. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e95", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04386"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ignacio Mart&#xed;n, Jos&#xe9; Alberto Hern&#xe1;ndez, Alfonso Mu&#xf1;oz, Antonio Guzm&#xe1;n", "title": "Android Malware Characterization using Metadata and Machine Learning Techniques. (arXiv:1712.04402v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1712.04402", "type": "text/html"}], "timestampUsec": "1513141858490915", "comments": [], "summary": {"content": "<p>Android Malware has emerged as a consequence of the increasing popularity of \nsmartphones and tablets. While most previous work focuses on inherent \ncharacteristics of Android apps to detect malware, this study analyses indirect \nfeatures and meta-data to identify patterns in malware applications. Our \nexperiments show that: (1) the permissions used by an application offer only \nmoderate performance results; (2) other features publicly available at Android \nMarkets are more relevant in detecting malware, such as the application \ndeveloper and certificate issuer, and (3) compact and efficient classifiers can \nbe constructed for the early detection of malware applications prior to code \ninspection or sandboxing. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127e9b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04402"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhe Wu, Bharat Singh, Larry S. Davis, V. S. Subrahmanian", "title": "Deception Detection in Videos. (arXiv:1712.04415v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.04415", "type": "text/html"}], "timestampUsec": "1513141858490914", "comments": [], "summary": {"content": "<p>We present a system for covert automated deception detection in real-life \ncourtroom trial videos. We study the importance of different modalities like \nvision, audio and text for this task. On the vision side, our system uses \nclassifiers trained on low level video features which predict human \nmicro-expressions. We show that predictions of high-level micro-expressions can \nbe used as features for deception prediction. Surprisingly, IDT (Improved Dense \nTrajectory) features which have been widely used for action recognition, are \nalso very good at predicting deception in videos. We fuse the score of \nclassifiers trained on IDT features and high-level micro-expressions to improve \nperformance. MFCC (Mel-frequency Cepstral Coefficients) features from the audio \ndomain also provide a significant boost in performance, while information from \ntranscripts is not very beneficial for our system. Using various classifiers, \nour automated system obtains an AUC of 0.877 (10-fold cross-validation) when \nevaluated on subjects which were not part of the training set. Even though \nstate-of-the-art methods use human annotations of micro-expressions for \ndeception detection, our fully automated approach outperforms them by 5%. When \ncombined with human annotations of micro-expressions, our AUC improves to \n0.922. We also present results of a user-study to analyze how well do average \nhumans perform on this task, what modalities they use for deception detection \nand how they perform if only one modality is accessible. Our project page can \nbe found at \\url{https://doubaibai.github.io/DARE/}. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ea4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04415"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Konstantinos Chatzilygeroudis, Vassilis Vassiliades, Jean-Baptiste Mouret", "title": "Reset-free Trial-and-Error Learning for Robot Damage Recovery. (arXiv:1610.04213v4 [cs.RO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.04213", "type": "text/html"}], "timestampUsec": "1513141858490913", "comments": [], "summary": {"content": "<p>The high probability of hardware failures prevents many advanced robots \n(e.g., legged robots) from being confidently deployed in real-world situations \n(e.g., post-disaster rescue). Instead of attempting to diagnose the failures, \nrobots could adapt by trial-and-error in order to be able to complete their \ntasks. In this situation, damage recovery can be seen as a Reinforcement \nLearning (RL) problem. However, the best RL algorithms for robotics require the \nrobot and the environment to be reset to an initial state after each episode, \nthat is, the robot is not learning autonomously. In addition, most of the RL \nmethods for robotics do not scale well with complex robots (e.g., walking \nrobots) and either cannot be used at all or take too long to converge to a \nsolution (e.g., hours of learning). In this paper, we introduce a novel \nlearning algorithm called \"Reset-free Trial-and-Error\" (RTE) that (1) breaks \nthe complexity by pre-generating hundreds of possible behaviors with a dynamics \nsimulator of the intact robot, and (2) allows complex robots to quickly recover \nfrom damage while completing their tasks and taking the environment into \naccount. We evaluate our algorithm on a simulated wheeled robot, a simulated \nsix-legged robot, and a real six-legged walking robot that are damaged in \nseveral ways (e.g., a missing leg, a shortened leg, faulty motor, etc.) and \nwhose objective is to reach a sequence of targets in an arena. Our experiments \nshow that the robots can recover most of their locomotion abilities in an \nenvironment with obstacles, and without any human intervention. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127eab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.04213"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hanul Shin, Jung Kwon Lee, Jaehong Kim, Jiwon Kim", "title": "Continual Learning with Deep Generative Replay. (arXiv:1705.08690v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08690", "type": "text/html"}], "timestampUsec": "1513141858490912", "comments": [], "summary": {"content": "<p>Attempts to train a comprehensive artificial intelligence capable of solving \nmultiple tasks have been impeded by a chronic problem called catastrophic \nforgetting. Although simply replaying all previous data alleviates the problem, \nit requires large memory and even worse, often infeasible in real world \napplications where the access to past data is limited. Inspired by the \ngenerative nature of hippocampus as a short-term memory system in primate \nbrain, we propose the Deep Generative Replay, a novel framework with a \ncooperative dual model architecture consisting of a deep generative model \n(\"generator\") and a task solving model (\"solver\"). With only these two models, \ntraining data for previous tasks can easily be sampled and interleaved with \nthose for a new task. We test our methods in several sequential learning \nsettings involving image classification tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127eb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08690"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shai Shalev-Shwartz, Shaked Shammah, Amnon Shashua", "title": "On a Formal Model of Safe and Scalable Self-driving Cars. (arXiv:1708.06374v3 [cs.RO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.06374", "type": "text/html"}], "timestampUsec": "1513141858490911", "comments": [], "summary": {"content": "<p>In recent years, car makers and tech companies have been racing towards self \ndriving cars. It seems that the main parameter in this race is who will have \nthe first car on the road. The goal of this paper is to add to the equation two \nadditional crucial parameters. The first is standardization of safety assurance \n--- what are the minimal requirements that every self-driving car must satisfy, \nand how can we verify these requirements. The second parameter is scalability \n--- engineering solutions that lead to unleashed costs will not scale to \nmillions of cars, which will push interest in this field into a niche academic \ncorner, and drive the entire field into a \"winter of autonomous driving\". In \nthe first part of the paper we propose a white-box, interpretable, mathematical \nmodel for safety assurance, which we call Responsibility-Sensitive Safety \n(RSS). In the second part we describe a design of a system that adheres to our \nsafety assurance requirements and is scalable to millions of cars. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127eb9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.06374"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Seunghyun Yoon, Pablo Estrada, Kyomin Jung", "title": "Synonym Discovery with Etymology-based Word Embeddings. (arXiv:1709.10445v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.10445", "type": "text/html"}], "timestampUsec": "1513141858490910", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050ca8cf3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050ca8cf3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a novel approach to learn word embeddings based on an extended \nversion of the distributional hypothesis. Our model derives word embedding \nvectors using the etymological composition of words, rather than the context in \nwhich they appear. It has the strength of not requiring a large text corpus, \nbut instead it requires reliable access to etymological roots of words, making \nit specially fit for languages with logographic writing systems. The model \nconsists on three steps: (1) building an etymological graph, which is a \nbipartite network of words and etymological roots, (2) obtaining the \nbiadjacency matrix of the etymological graph and reducing its dimensionality, \n(3) using columns/rows of the resulting matrices as embedding vectors. We test \nour model in the Chinese and Sino-Korean vocabularies. Our graphs are formed by \na set of 117,000 Chinese words, and a set of 135,000 Sino-Korean words. In both \ncases we show that our model performs well in the task of synonym discovery. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ec1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.10445"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sima Behpour, Kris M. Kitani, Brian D. Ziebart", "title": "ADA: A Game-Theoretic Perspective on Data Augmentation for Object Detection. (arXiv:1710.07735v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.07735", "type": "text/html"}], "timestampUsec": "1513141858490909", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050d04edb\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050d04edb&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The use of random perturbations of ground truth data, such as random \ntranslation or scaling of bounding boxes, is a common heuristic used for data \naugmentation that has been shown to prevent overfitting and improve \ngeneralization. Since the design of data augmentation is largely guided by \nreported best practices, it is difficult to understand if those design choices \nare optimal. To provide a more principled perspective, we develop a \ngame-theoretic interpretation of data augmentation in the context of object \ndetection. We aim to find an optimal adversarial perturbations of the ground \ntruth data (i.e., the worst case perturbations) that forces the object bounding \nbox predictor to learn from the hardest distribution of perturbed examples for \nbetter test-time performance. We establish that the game theoretic solution, \nthe Nash equilibrium, provides both an optimal predictor and optimal data \naugmentation distribution. We show that our adversarial method of training a \npredictor can significantly improve test time performance for the task of \nobject detection. On the ImageNet object detection task, our adversarial \napproach improves performance by over 16\\% compared to the best performing data \naugmentation method \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ec7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.07735"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kevin T. Feigelis, Blue Sheffer, Daniel L. K. Yamins", "title": "Modular Continual Learning in a Unified Visual Environment. (arXiv:1711.07425v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07425", "type": "text/html"}], "timestampUsec": "1513141858490908", "comments": [], "summary": {"content": "<p>A core aspect of human intelligence is the ability to learn new tasks quickly \nand switch between them flexibly. Here, we describe a modular continual \nreinforcement learning paradigm inspired by these abilities. We first introduce \na visual interaction environment that allows many types of tasks to be unified \nin a single framework. We then describe a reward map prediction scheme that \nlearns new tasks robustly in the very large state and action spaces required by \nsuch an environment. We investigate how properties of module architecture \ninfluence efficiency of task learning, showing that a module motif \nincorporating specific design principles (e.g. early bottlenecks, low-order \npolynomial nonlinearities, and symmetry) significantly outperforms more \nstandard neural network motifs, needing fewer training examples and fewer \nneurons to achieve high levels of performance. Finally, we present a \nmeta-controller architecture for task switching based on a dynamic neural \nvoting scheme, which allows new modules to use information learned from \npreviously-seen tasks to substantially improve their own learning efficiency. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ecd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07425"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "John P Dickerson, Karthik A Sankararaman, Aravind Srinivasan, Pan Xu", "title": "Allocation Problems in Ride-Sharing Platforms: Online Matching with Offline Reusable Resources. (arXiv:1711.08345v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08345", "type": "text/html"}], "timestampUsec": "1513141858490907", "comments": [], "summary": {"content": "<p>Bipartite matching markets pair agents on one side of a market with agents, \nitems, or contracts on the opposing side. Prior work addresses online bipartite \nmatching markets, where agents arrive over time and are dynamically matched to \na known set of disposable resources. In this paper, we propose a new model, \nOnline Matching with (offline) Reusable Resources under Known Adversarial \nDistributions (OM-RR-KAD), in which resources on the offline side are reusable \ninstead of disposable; that is, once matched, resources become available again \nat some point in the future. We show that our model is tractable by presenting \nan LP-based adaptive algorithm that achieves an online competitive ratio of 1/2 \n- eps for any given eps greater than 0. We also show that no non-adaptive \nalgorithm can achieve a ratio of 1/2 + o(1) based on the same benchmark LP. \nThrough a data-driven analysis on a massive openly-available dataset, we show \nour model is robust enough to capture the application of taxi dispatching \nservices and ride-sharing systems. We also present heuristics that perform well \nin practice. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ed5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08345"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christopher Streiffer, Huan Chen, Theophilus Benson, Asim Kadav", "title": "DeepConfig: Automating Data Center Network Topologies Management with Machine Learning. (arXiv:1712.03890v1 [cs.NI] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1712.03890", "type": "text/html"}], "timestampUsec": "1513141858490906", "comments": [], "summary": {"content": "<p>In recent years, many techniques have been developed to improve the \nperformance and efficiency of data center networks. While these techniques \nprovide high accuracy, they are often designed using heuristics that leverage \ndomain-specific properties of the workload or hardware. \n</p> \n<p>In this vision paper, we argue that many data center networking techniques, \ne.g., routing, topology augmentation, energy savings, with diverse goals \nactually share design and architectural similarity. We present a design for \ndeveloping general intermediate representations of network topologies using \ndeep learning that is amenable to solving classes of data center problems. We \ndevelop a framework, DeepConfig, that simplifies the processing of configuring \nand training deep learning agents that use the intermediate representation to \nlearns different tasks. To illustrate the strength of our approach, we \nconfigured, implemented, and evaluated a DeepConfig-Agent that tackles the data \ncenter topology augmentation problem. Our initial results are promising --- \nDeepConfig performs comparably to the optimal. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127edb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03890"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Brian Dolhansky, Cristian Canton Ferrer", "title": "Eye In-Painting with Exemplar Generative Adversarial Networks. (arXiv:1712.03999v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.03999", "type": "text/html"}], "timestampUsec": "1513141858490905", "comments": [], "summary": {"content": "<p>This paper introduces a novel approach to in-painting where the identity of \nthe object to remove or change is preserved and accounted for at inference \ntime: Exemplar GANs (ExGANs). ExGANs are a type of conditional GAN that utilize \nexemplar information to produce high-quality, personalized in painting results. \nWe propose using exemplar information in the form of a reference image of the \nregion to in-paint, or a perceptual code describing that object. Unlike \nprevious conditional GAN formulations, this extra information can be inserted \nat multiple points within the adversarial network, thus increasing its \ndescriptive power. We show that ExGANs can produce photo-realistic personalized \nin-painting results that are both perceptually and semantically plausible by \napplying them to the task of closed to-open eye in-painting in natural \npictures. A new benchmark dataset is also introduced for the task of eye \nin-painting for future comparisons. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ee4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03999"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jason Poulos, Rafael Valle", "title": "Attention networks for image-to-text. (arXiv:1712.04046v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.04046", "type": "text/html"}], "timestampUsec": "1513141858490904", "comments": [], "summary": {"content": "<p>The paper approaches the problem of image-to-text with attention-based \nencoder-decoder networks that are trained to handle sequences of characters \nrather than words. We experiment on lines of text from a popular handwriting \ndatabase with different attention mechanisms for the decoder. The model trained \nwith softmax attention achieves the lowest test error, outperforming several \nother RNN-based models. Our results show that softmax attention is able to \nlearn a linear alignment whereas the alignment generated by sigmoid attention \nis linear but much less precise. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127eeb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04046"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zinan Lin, Ashish Khetan, Giulia Fanti, Sewoong Oh", "title": "PacGAN: The power of two samples in generative adversarial networks. (arXiv:1712.04086v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04086", "type": "text/html"}], "timestampUsec": "1513141858490903", "comments": [], "summary": {"content": "<p>Generative adversarial networks (GANs) are innovative techniques for learning \ngenerative models of complex data distributions from samples. Despite \nremarkable recent improvements in generating realistic images, one of their \nmajor shortcomings is the fact that in practice, they tend to produce samples \nwith little diversity, even when trained on diverse datasets. This phenomenon, \nknown as mode collapse, has been the main focus of several recent advances in \nGANs. Yet there is little understanding of why mode collapse happens and why \nexisting approaches are able to mitigate mode collapse. We propose a principled \napproach to handling mode collapse, which we call packing. The main idea is to \nmodify the discriminator to make decisions based on multiple samples from the \nsame class, either real or artificially generated. We borrow analysis tools \nfrom binary hypothesis testing---in particular the seminal result of Blackwell \n[Bla53]---to prove a fundamental connection between packing and mode collapse. \nWe show that packing naturally penalizes generators with mode collapse, thereby \nfavoring generator distributions with less mode collapse during the training \nprocess. Numerical experiments on benchmark datasets suggests that packing \nprovides significant improvements in practice as well. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ef6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04086"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alex Lamb, Devon Hjelm, Yaroslav Ganin, Joseph Paul Cohen, Aaron Courville, Yoshua Bengio", "title": "GibbsNet: Iterative Adversarial Inference for Deep Graphical Models. (arXiv:1712.04120v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.04120", "type": "text/html"}], "timestampUsec": "1513141858490902", "comments": [], "summary": {"content": "<p>Directed latent variable models that formulate the joint distribution as \n$p(x,z) = p(z) p(x \\mid z)$ have the advantage of fast and exact sampling. \nHowever, these models have the weakness of needing to specify $p(z)$, often \nwith a simple fixed prior that limits the expressiveness of the model. \nUndirected latent variable models discard the requirement that $p(z)$ be \nspecified with a prior, yet sampling from them generally requires an iterative \nprocedure such as blocked Gibbs-sampling that may require many steps to draw \nsamples from the joint distribution $p(x, z)$. We propose a novel approach to \nlearning the joint distribution between the data and a latent code which uses \nan adversarially learned iterative procedure to gradually refine the joint \ndistribution, $p(x, z)$, to better match with the data distribution on each \nstep. GibbsNet is the best of both worlds both in theory and in practice. \nAchieving the speed and simplicity of a directed latent variable model, it is \nguaranteed (assuming the adversarial game reaches the virtual training criteria \nglobal minimum) to produce samples from $p(x, z)$ with only a few sampling \niterations. Achieving the expressiveness and flexibility of an undirected \nlatent variable model, GibbsNet does away with the need for an explicit $p(z)$ \nand has the ability to do attribute prediction, class-conditional generation, \nand joint image-attribute modeling in a single model which is not trained for \nany of these specific tasks. We show empirically that GibbsNet is able to learn \na more complex $p(z)$ and show that this leads to improved inpainting and \niterative refinement of $p(x, z)$ for dozens of steps and stable generation \nwithout collapse for thousands of steps, despite being trained on only a few \nsteps. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f04", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04120"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Utkarsh Porwal, Smruthi Mukund", "title": "Outlier Detection by Consistent Data Selection Method. (arXiv:1712.04129v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04129", "type": "text/html"}], "timestampUsec": "1513141858490901", "comments": [], "summary": {"content": "<p>Often the challenge associated with tasks like fraud and spam detection[1] is \nthe lack of all likely patterns needed to train suitable supervised learning \nmodels. In order to overcome this limitation, such tasks are attempted as \noutlier or anomaly detection tasks. We also hypothesize that out- liers have \nbehavioral patterns that change over time. Limited data and continuously \nchanging patterns makes learning significantly difficult. In this work we are \nproposing an approach that detects outliers in large data sets by relying on \ndata points that are consistent. The primary contribution of this work is that \nit will quickly help retrieve samples for both consistent and non-outlier data \nsets and is also mindful of new outlier patterns. No prior knowledge of each \nset is required to extract the samples. The method consists of two phases, in \nthe first phase, consistent data points (non- outliers) are retrieved by an \nensemble method of unsupervised clustering techniques and in the second phase a \none class classifier trained on the consistent data point set is ap- plied on \nthe remaining sample set to identify the outliers. The approach is tested on \nthree publicly available data sets and the performance scores are competitive. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f0e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04129"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aidin Ferdowsi, Ursula Challita, Walid Saad", "title": "Deep Learning for Reliable Mobile Edge Analytics in Intelligent Transportation Systems. (arXiv:1712.04135v1 [cs.IT])", "alternate": [{"href": "http://arxiv.org/abs/1712.04135", "type": "text/html"}], "timestampUsec": "1513141858490900", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050d05121\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050d05121&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Intelligent transportation systems (ITSs) will be a major component of \ntomorrow's smart cities. However, realizing the true potential of ITSs requires \nultra-low latency and reliable data analytics solutions that can combine, in \nreal-time, a heterogeneous mix of data stemming from the ITS network and its \nenvironment. Such data analytics capabilities cannot be provided by \nconventional cloud-centric data processing techniques whose communication and \ncomputing latency can be high. Instead, edge-centric solutions that are \ntailored to the unique ITS environment must be developed. In this paper, an \nedge analytics architecture for ITSs is introduced in which data is processed \nat the vehicle or roadside smart sensor level in order to overcome the ITS \nlatency and reliability challenges. With a higher capability of passengers' \nmobile devices and intra-vehicle processors, such a distributed edge computing \narchitecture can leverage deep learning techniques for reliable mobile sensing \nin ITSs. In this context, the ITS mobile edge analytics challenges pertaining \nto heterogeneous data, autonomous control, vehicular platoon control, and \ncyber-physical security are investigated. Then, different deep learning \nsolutions for such challenges are proposed. The proposed deep learning \nsolutions will enable ITS edge analytics by endowing the ITS devices with \npowerful computer vision and signal processing functions. Preliminary results \nshow that the proposed edge analytics architecture, coupled with the power of \ndeep learning algorithms, can provide a reliable, secure, and truly smart \ntransportation environment. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04135"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Song Cheng, Jing Chen, Lei Wang", "title": "Information Perspective to Probabilistic Modeling: Boltzmann Machines versus Born Machines. (arXiv:1712.04144v1 [physics.data-an])", "alternate": [{"href": "http://arxiv.org/abs/1712.04144", "type": "text/html"}], "timestampUsec": "1513141858490899", "comments": [], "summary": {"content": "<p>We compare and contrast the statistical physics and quantum physics inspired \napproaches for unsupervised generative modeling of classical data. The two \napproaches represent probabilities of observed data using energy-based models \nand quantum states respectively.Classical and quantum information patterns of \nthe target datasets therefore provide principled guidelines for structural \ndesign and learning in these two approaches. Taking the restricted Boltzmann \nmachines (RBM) as an example, we analyze the information theoretical bounds of \nthe two approaches. We verify our reasonings by comparing the performance of \nRBMs of various architectures on the standard MNIST datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f31", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04144"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sho Sonoda, Noboru Murata", "title": "Transportation analysis of denoising autoencoders: a novel method for analyzing deep neural networks. (arXiv:1712.04145v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04145", "type": "text/html"}], "timestampUsec": "1513141858490898", "comments": [], "summary": {"content": "<p>The feature map obtained from the denoising autoencoder (DAE) is investigated \nby determining transportation dynamics of the DAE, which is a cornerstone for \ndeep learning. Despite the rapid development in its application, deep neural \nnetworks remain analytically unexplained, because the feature maps are nested \nand parameters are not faithful. In this paper, we address the problem of the \nformulation of nested complex of parameters by regarding the feature map as a \ntransport map. Even when a feature map has different dimensions between input \nand output, we can regard it as a transportation map by considering that both \nthe input and output spaces are embedded in a common high-dimensional space. In \naddition, the trajectory is a geometric object and thus, is independent of \nparameterization. In this manner, transportation can be regarded as a universal \ncharacter of deep neural networks. By determining and analyzing the \ntransportation dynamics, we can understand the behavior of a deep neural \nnetwork. In this paper, we investigate a fundamental case of deep neural \nnetworks: the DAE. We derive the transport map of the DAE, and reveal that the \ninfinitely deep DAE transports mass to decrease a certain quantity, such as \nentropy, of the data distribution. These results though analytically simple, \nshed light on the correspondence between deep neural networks and the \nWasserstein gradient flows. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f38", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04145"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Salman Salloum, Yulin He, Joshua Zhexue Huang, Xiaoliang Zhang, Tamer Z. Emara", "title": "A Random Sample Partition Data Model for Big Data Analysis. (arXiv:1712.04146v1 [cs.DC])", "alternate": [{"href": "http://arxiv.org/abs/1712.04146", "type": "text/html"}], "timestampUsec": "1513141858490897", "comments": [], "summary": {"content": "<p>Big data sets must be carefully partitioned into statistically similar data \nsubsets that can be used as representative samples for big data analysis tasks. \nIn this paper, we propose the random sample partition (RSP) to represent a big \ndata set as a set of non-overlapping data subsets, i.e. RSP data blocks, where \neach RSP data block has the same probability distribution with the whole big \ndata set. Then, the block-based sampling is used to directly select \nrepresentative samples for a variety of data analysis tasks. We show how RSP \ndata blocks can be employed to estimate statistics and build models which are \nequivalent (or approximate) to those from the whole big data set. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04146"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Irene Teinemaa, Marlon Dumas, Anna Leontjeva, Fabrizio Maria Maggi", "title": "Temporal Stability in Predictive Process Monitoring. (arXiv:1712.04165v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04165", "type": "text/html"}], "timestampUsec": "1513141858490896", "comments": [], "summary": {"content": "<p>Predictive business process monitoring is concerned with the analysis of \nevents produced during the execution of a business process in order to predict \nas early as possible the final outcome of an ongoing case. Traditionally, \npredictive process monitoring methods are optimized with respect to accuracy. \nHowever, in environments where users make decisions and take actions in \nresponse to the predictions they receive, it is equally important to optimize \nthe stability of the successive predictions made for each case. To this end, \nthis paper defines a notion of temporal stability for predictive process \nmonitoring and evaluates existing methods with respect to both temporal \nstability and accuracy. We find that methods based on XGBoost and LSTM neural \nnetworks exhibit the highest temporal stability. We then show that temporal \nstability can be enhanced by hyperparameter-optimizing random forests and \nXGBoost classifiers with respect to inter-run stability. Finally, we show that \ntime series smoothing techniques can further enhance temporal stability at the \nexpense of slightly lower accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f4f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04165"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hiroki Mori, Keisuke Kawano, Hiroki Yokoyama", "title": "Causal Patterns: Extraction of multiple causal relationships by Mixture of Probabilistic Partial Canonical Correlation Analysis. (arXiv:1712.04221v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.04221", "type": "text/html"}], "timestampUsec": "1513141858490895", "comments": [], "summary": {"content": "<p>In this paper, we propose a mixture of probabilistic partial canonical \ncorrelation analysis (MPPCCA) that extracts the Causal Patterns from two \nmultivariate time series. Causal patterns refer to the signal patterns within \ninteractions of two elements having multiple types of mutually causal \nrelationships, rather than a mixture of simultaneous correlations or the \nabsence of presence of a causal relationship between the elements. In \nmultivariate statistics, partial canonical correlation analysis (PCCA) \nevaluates the correlation between two multivariates after subtracting the \neffect of the third multivariate. PCCA can calculate the Granger Causal- ity \nIndex (which tests whether a time-series can be predicted from an- other \ntime-series), but is not applicable to data containing multiple partial \ncanonical correlations. After introducing the MPPCCA, we propose an \nexpectation-maxmization (EM) algorithm that estimates the parameters and latent \nvariables of the MPPCCA. The MPPCCA is expected to ex- tract multiple partial \ncanonical correlations from data series without any supervised signals to split \nthe data as clusters. The method was then eval- uated in synthetic data \nexperiments. In the synthetic dataset, our method estimated the multiple \npartial canonical correlations more accurately than the existing method. To \ndetermine the types of patterns detectable by the method, experiments were also \nconducted on real datasets. The method estimated the communication patterns In \nmotion-capture data. The MP- PCCA is applicable to various type of signals such \nas brain signals, human communication and nonlinear complex multibody systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f58", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04221"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Soumitro Chakrabarty, Emanu&#xeb;l A. P. Habets", "title": "Multi-Speaker Localization Using Convolutional Neural Network Trained with Noise. (arXiv:1712.04276v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1712.04276", "type": "text/html"}], "timestampUsec": "1513141858490894", "comments": [], "summary": {"content": "<p>The problem of multi-speaker localization is formulated as a multi-class \nmulti-label classification problem, which is solved using a convolutional \nneural network (CNN) based source localization method. Utilizing the common \nassumption of disjoint speaker activities, we propose a novel method to train \nthe CNN using synthesized noise signals. The proposed localization method is \nevaluated for two speakers and compared to a well-known steered response power \nmethod. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f5e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04276"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chuang Wang, Jonathan Mattingly, Yue M. Lu", "title": "Scaling Limit: Exact and Tractable Analysis of Online Learning Algorithms with Applications to Regularized Regression and PCA. (arXiv:1712.04332v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04332", "type": "text/html"}], "timestampUsec": "1513141858490893", "comments": [], "summary": {"content": "<p>We present a framework for analyzing the exact dynamics of a class of online \nlearning algorithms in the high-dimensional scaling limit. Our results are \napplied to two concrete examples: online regularized linear regression and \nprincipal component analysis. As the ambient dimension tends to infinity, and \nwith proper time scaling, we show that the time-varying joint empirical \nmeasures of the target feature vector and its estimates provided by the \nalgorithms will converge weakly to a deterministic measured-valued process that \ncan be characterized as the unique solution of a nonlinear PDE. Numerical \nsolutions of this PDE can be efficiently obtained. These solutions lead to \nprecise predictions of the performance of the algorithms, as many practical \nperformance metrics are linear functionals of the joint empirical measures. In \naddition to characterizing the dynamic performance of online learning \nalgorithms, our asymptotic analysis also provides useful insights. In \nparticular, in the high-dimensional limit, and due to exchangeability, the \noriginal coupled dynamics associated with the algorithms will be asymptotically \n\"decoupled\", with each coordinate independently solving a 1-D effective \nminimization problem via stochastic gradient descent. Exploiting this insight \nfor nonconvex optimization problems may prove an interesting line of future \nresearch. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f65", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04332"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Luis Perez", "title": "Predicting Yelp Star Reviews Based on Network Structure with Deep Learning. (arXiv:1712.04350v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04350", "type": "text/html"}], "timestampUsec": "1513141858490892", "comments": [], "summary": {"content": "<p>In this paper, we tackle the real-world problem of predicting Yelp \nstar-review rating based on business features (such as images, descriptions), \nuser features (average previous ratings), and, of particular interest, network \nproperties (which businesses has a user rated before). We compare multiple \nmodels on different sets of features -- from simple linear regression on \nnetwork features only to deep learning models on network and item features. \n</p> \n<p>In recent years, breakthroughs in deep learning have led to increased \naccuracy in common supervised learning tasks, such as image classification, \ncaptioning, and language understanding. However, the idea of combining deep \nlearning with network feature and structure appears to be novel. While the \nproblem of predicting future interactions in a network has been studied at \nlength, these approaches have often ignored either node-specific data or global \nstructure. \n</p> \n<p>We demonstrate that taking a mixed approach combining both node-level \nfeatures and network information can effectively be used to predict Yelp-review \nstar ratings. We evaluate on the Yelp dataset by splitting our data along the \ntime dimension (as would naturally occur in the real-world) and comparing our \nmodel against others which do no take advantage of the network structure and/or \ndeep learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f70", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04350"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Farshid Rayhan, Sajid Ahmed, Asif Mahbub, Md. Rafsan Jani, Swakkhar Shatabda, Dewan Md. Farid", "title": "CUSBoost: Cluster-based Under-sampling with Boosting for Imbalanced Classification. (arXiv:1712.04356v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04356", "type": "text/html"}], "timestampUsec": "1513141858490891", "comments": [], "summary": {"content": "<p>Class imbalance classification is a challenging research problem in data \nmining and machine learning, as most of the real-life datasets are often \nimbalanced in nature. Existing learning algorithms maximise the classification \naccuracy by correctly classifying the majority class, but misclassify the \nminority class. However, the minority class instances are representing the \nconcept with greater interest than the majority class instances in real-life \napplications. Recently, several techniques based on sampling methods \n(under-sampling of the majority class and over-sampling the minority class), \ncost-sensitive learning methods, and ensemble learning have been used in the \nliterature for classifying imbalanced datasets. In this paper, we introduce a \nnew clustering-based under-sampling approach with boosting (AdaBoost) \nalgorithm, called CUSBoost, for effective imbalanced classification. The \nproposed algorithm provides an alternative to RUSBoost (random under-sampling \nwith AdaBoost) and SMOTEBoost (synthetic minority over-sampling with AdaBoost) \nalgorithms. We evaluated the performance of CUSBoost algorithm with the \nstate-of-the-art methods based on ensemble learning like AdaBoost, RUSBoost, \nSMOTEBoost on 13 imbalance binary and multi-class datasets with various \nimbalance ratios. The experimental results show that the CUSBoost is a \npromising and effective approach for dealing with highly imbalanced datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f78", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04356"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexander Sage, Eirikur Agustsson, Radu Timofte, Luc Van Gool", "title": "Logo Synthesis and Manipulation with Clustered Generative Adversarial Networks. (arXiv:1712.04407v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.04407", "type": "text/html"}], "timestampUsec": "1513141858490890", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050d0533a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050d0533a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Designing a logo for a new brand is a lengthy and tedious back-and-forth \nprocess between a designer and a client. In this paper we explore to what \nextent machine learning can solve the creative task of the designer. For this, \nwe build a dataset -- LLD -- of 600k+ logos crawled from the world wide web. \nTraining Generative Adversarial Networks (GANs) for logo synthesis on such \nmulti-modal data is not straightforward and results in mode collapse for some \nstate-of-the-art methods. We propose the use of synthetic labels obtained \nthrough clustering to disentangle and stabilize GAN training. We are able to \ngenerate a high diversity of plausible logos and we demonstrate latent space \nexploration techniques to ease the logo design task in an interactive manner. \nMoreover, we validate the proposed clustered GAN training on CIFAR 10, \nachieving state-of-the-art Inception scores when using synthetic labels \nobtained via clustering the features of an ImageNet classifier. GANs can cope \nwith multi-modal data by means of synthetic labels achieved through clustering, \nand our results show the creative potential of such techniques for logo \nsynthesis and manipulation. Our dataset and models will be made publicly \navailable at https://data.vision.ee.ethz.ch/cvl/lld/. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f7e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04407"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Amir Gholami, Ariful Azad, Kurt Keutzer, Aydin Buluc", "title": "Integrated Model and Data Parallelism in Training Neural Networks. (arXiv:1712.04432v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.04432", "type": "text/html"}], "timestampUsec": "1513141858490889", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050d56dd3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050d56dd3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a new integrated method of exploiting both model and data \nparallelism for the training of deep neural networks (DNNs) on large \ndistributed-memory computers using mini-batch stochastic gradient descent \n(SGD). Our goal is to find an efficient parallelization strategy for a fixed \nbatch size using $P$ processes. Our method is inspired by the \ncommunication-avoiding algorithms in numerical linear algebra. We see $P$ \nprocesses as logically divided into a $P_r \\times P_c$ grid where the $P_r$ \ndimension is implicitly responsible for model parallelism and the $P_c$ \ndimension is implicitly responsible for data parallelism. In practice, the \nintegrated matrix-based parallel algorithm encapsulates both types of \nparallelism automatically. We analyze the communication complexity and \nanalytically demonstrate that the lowest communication costs are often achieved \nneither with pure model parallelism nor with pure data parallelism. We also \nshow the positive effect of our approach in the computational performance of \nSGD based DNN training where the reduced number of processes responsible for \ndata parallelism result in \"fatter\" matrices that enable higher-throughput \nmatrix multiplication. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f85", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.04432"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Matthieu Geist, Bilal Piot, Olivier Pietquin", "title": "Is the Bellman residual a bad proxy?. (arXiv:1606.07636v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1606.07636", "type": "text/html"}], "timestampUsec": "1513141858490888", "comments": [], "summary": {"content": "<p>This paper aims at theoretically and empirically comparing two standard \noptimization criteria for Reinforcement Learning: i) maximization of the mean \nvalue and ii) minimization of the Bellman residual. For that purpose, we place \nourselves in the framework of policy search algorithms, that are usually \ndesigned to maximize the mean value, and derive a method that minimizes the \nresidual $\\|T_* v_\\pi - v_\\pi\\|_{1,\\nu}$ over policies. A theoretical analysis \nshows how good this proxy is to policy optimization, and notably that it is \nbetter than its value-based counterpart. We also propose experiments on \nrandomly generated generic Markov decision processes, specifically designed for \nstudying the influence of the involved concentrability coefficient. They show \nthat the Bellman residual is generally a bad proxy to policy optimization and \nthat directly maximizing the mean value is much better, despite the current \nlack of deep theoretical analysis. This might seem obvious, as directly \naddressing the problem of interest is usually better, but given the prevalence \nof (projected) Bellman residual minimization in value-based reinforcement \nlearning, we believe that this question is worth to be considered. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f8c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1606.07636"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Luca Franceschi, Michele Donini, Paolo Frasconi, Massimiliano Pontil", "title": "Forward and Reverse Gradient-Based Hyperparameter Optimization. (arXiv:1703.01785v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.01785", "type": "text/html"}], "timestampUsec": "1513141858490887", "comments": [], "summary": {"content": "<p>We study two procedures (reverse-mode and forward-mode) for computing the \ngradient of the validation error with respect to the hyperparameters of any \niterative learning algorithm such as stochastic gradient descent. These \nprocedures mirror two methods of computing gradients for recurrent neural \nnetworks and have different trade-offs in terms of running time and space \nrequirements. Our formulation of the reverse-mode procedure is linked to \nprevious work by Maclaurin et al. [2015] but does not require reversible \ndynamics. The forward-mode procedure is suitable for real-time hyperparameter \nupdates, which may significantly speed up hyperparameter optimization on large \ndatasets. We present experiments on data cleaning and on learning task \ninteractions. We also present one large-scale experiment where the use of \nprevious gradient-based methods would be prohibitive. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127f9b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.01785"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Soumitro Chakrabarty, Emanu&#xeb;l. A. P. Habets", "title": "Broadband DOA estimation using Convolutional neural networks trained with noise signals. (arXiv:1705.00919v2 [cs.SD] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.00919", "type": "text/html"}], "timestampUsec": "1513141858490886", "comments": [], "summary": {"content": "<p>A convolution neural network (CNN) based classification method for broadband \nDOA estimation is proposed, where the phase component of the short-time Fourier \ntransform coefficients of the received microphone signals are directly fed into \nthe CNN and the features required for DOA estimation are learnt during \ntraining. Since only the phase component of the input is used, the CNN can be \ntrained with synthesized noise signals, thereby making the preparation of the \ntraining data set easier compared to using speech signals. Through experimental \nevaluation, the ability of the proposed noise trained CNN framework to \ngeneralize to speech sources is demonstrated. In addition, the robustness of \nthe system to noise, small perturbations in microphone positions, as well as \nits ability to adapt to different acoustic conditions is investigated using \nexperiments with simulated and real data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fa6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.00919"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexej Gossmann, Pascal Zille, Vince Calhoun, Yu-Ping Wang", "title": "FDR-Corrected Sparse Canonical Correlation Analysis with Applications to Imaging Genomics. (arXiv:1705.04312v3 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.04312", "type": "text/html"}], "timestampUsec": "1513141858490885", "comments": [], "summary": {"content": "<p>Reducing the number of false positive discoveries is presently one of the \nmost pressing issues in the life sciences. It is of especially great importance \nfor many applications in neuroimaging and genomics, where datasets are \ntypically high-dimensional, which means that the number of explanatory \nvariables exceeds the sample size. The false discovery rate (FDR) is a \ncriterion that can be employed to address that issue. Thus it has gained great \npopularity as a tool for testing multiple hypotheses. Canonical correlation \nanalysis (CCA) is a statistical technique that is used to make sense of the \ncross-correlation of two sets of measurements collected on the same set of \nsamples (e.g., brain imaging and genomic data for the same mental illness \npatients), and sparse CCA extends the classical method to high-dimensional \nsettings. Here we propose a way of applying the FDR concept to sparse CCA, and \na method to control the FDR. The proposed FDR correction directly influences \nthe sparsity of the solution, adapting it to the unknown true sparsity level. \nTheoretical derivation as well as simulation studies show that our procedure \nindeed keeps the FDR of the canonical vectors below a user-specified target \nlevel. We apply the proposed method to an imaging genomics dataset from the \nPhiladelphia Neurodevelopmental Cohort. Our results link the brain connectivity \nprofiles derived from brain activity during an emotion identification task, as \nmeasured by functional magnetic resonance imaging (fMRI), to the corresponding \nsubjects' genomic data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127faf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.04312"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lin F. Yang, Vladimir Braverman, Tuo Zhao, Mengdi Wang", "title": "Online Factorization and Partition of Complex Networks From Random Walks. (arXiv:1705.07881v4 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.07881", "type": "text/html"}], "timestampUsec": "1513141858490884", "comments": [], "summary": {"content": "<p>Finding the reduced-dimensional structure is critical to understanding \ncomplex networks. Existing approaches such as spectral clustering are \napplicable only when the full network is explicitly observed. In this paper, we \nfocus on the online factorization and partition of implicit large-scale \nnetworks based on observations from an associated random walk. We formulate \nthis into a nonconvex stochastic factorization problem and propose an efficient \nand scalable stochastic generalized Hebbian algorithm. The algorithm is able to \nprocess dependent state-transition data dynamically generated by the underlying \nnetwork and learn a low-dimensional representation for each vertex. By applying \na diffusion approximation analysis, we show that the continuous-time limiting \nprocess of the stochastic algorithm converges globally to the \"principal \ncomponents\" of the Markov chain and achieves a nearly optimal sample \ncomplexity. Once given the learned low-dimensional representations, we further \napply clustering techniques to recover the network partition. We show that when \nthe associated Markov process is lumpable, one can recover the partition \nexactly with high probability. We apply the proposed approach to model the \ntraffic flow of Manhattan as city-wide random walks. By using our algorithm to \nanalyze the taxi trip data, we discover a latent partition of the Manhattan \ncity that closely matches the traffic dynamics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fb9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.07881"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hao Wu, Frank No&#xe9;", "title": "Variational approach for learning Markov processes from time series data. (arXiv:1707.04659v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.04659", "type": "text/html"}], "timestampUsec": "1513141858490883", "comments": [], "summary": {"content": "<p>Inference, prediction and control of complex dynamical systems from time \nseries is important in many areas, including financial markets, power grid \nmanagement, climate and weather modeling, or molecular dynamics. The analysis \nof such highly nonlinear dynamical systems is facilitated by the fact that we \ncan often find a (generally nonlinear) transformation of the system coordinates \nto features in which the dynamics can be excellently approximated by a linear \nMarkovian model. Moreover, the large number of system variables often change \ncollectively on large time- and length-scales, facilitating a low-dimensional \nanalysis in feature space. In this paper, we introduce a variational approach \nfor Markov processes (VAMP) that allows us to find optimal feature mappings and \noptimal Markovian models of the dynamics from given time series data. The key \ninsight is that the best linear model can be obtained from the top singular \ncomponents of the Koopman operator. This leads to the definition of a family of \nscore functions called VAMP-r which can be calculated from data, and can be \nemployed to optimize a Markovian model. In addition, based on the relationship \nbetween the variational scores and approximation errors of Koopman operators, \nwe propose a new VAMP-E score, which can be applied to cross-validation for \nhyper-parameter optimization and model selection in VAMP. VAMP is valid for \nboth reversible and nonreversible processes and for stationary and \nnon-stationary processes or realizations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fbf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.04659"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tianyi Lin, Linbo Qiao, Teng Zhang, Jiashi Feng, Bofeng Zhang", "title": "Stochastic Primal-Dual Proximal ExtraGradient Descent for Compositely Regularized Optimization. (arXiv:1708.05978v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.05978", "type": "text/html"}], "timestampUsec": "1513141858490882", "comments": [], "summary": {"content": "<p>We consider a wide range of regularized stochastic minimization problems with \ntwo regularization terms, one of which is composed with a linear function. This \noptimization model abstracts a number of important applications in artificial \nintelligence and machine learning, such as fused Lasso, fused logistic \nregression, and a class of graph-guided regularized minimization. The \ncomputational challenges of this model are in two folds. On one hand, the \nclosed-form solution of the proximal mapping associated with the composed \nregularization term or the expected objective function is not available. On the \nother hand, the calculation of the full gradient of the expectation in the \nobjective is very expensive when the number of input data samples is \nconsiderably large. To address these issues, we propose a stochastic variant of \nextra-gradient type methods, namely \\textsf{Stochastic Primal-Dual Proximal \nExtraGradient descent (SPDPEG)}, and analyze its convergence property for both \nconvex and strongly convex objectives. For general convex objectives, the \nuniformly average iterates generated by \\textsf{SPDPEG} converge in expectation \nwith $O(1/\\sqrt{t})$ rate. While for strongly convex objectives, the uniformly \nand non-uniformly average iterates generated by \\textsf{SPDPEG} converge with \n$O(\\log(t)/t)$ and $O(1/t)$ rates, respectively. The order of the rate of the \nproposed algorithm is known to match the best convergence rate for first-order \nstochastic algorithms. Experiments on fused logistic regression and \ngraph-guided regularized logistic regression problems show that the proposed \nalgorithm performs very efficiently and consistently outperforms other \ncompeting algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fc8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.05978"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Agnieszka Sitko, Przemyslaw Biecek", "title": "The Merging Path Plot: adaptive fusing of k-groups with likelihood-based model selection. (arXiv:1709.04412v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04412", "type": "text/html"}], "timestampUsec": "1513141858490881", "comments": [], "summary": {"content": "<p>There are many statistical tests that verify the null hypothesis: the \nvariable of interest has the same distribution among k-groups. But once the \nnull hypothesis is rejected, how to present the structure of dissimilarity \nbetween groups? In this article, we introduce The Merging Path Plot - a \nmethodology, and factorMerger - an R package, for exploration and visualization \nof k-group dissimilarities. Comparison of k-groups is one of the most important \nissues in exploratory analyses and it has zillions of applications. The \nclassical solution is to test a~null hypothesis that observations from all \ngroups come from the same distribution. If the global null hypothesis is \nrejected, a~more detailed analysis of differences among pairs of groups is \nperformed. The traditional approach is to use pairwise post hoc tests in order \nto verify which groups differ significantly. However, this approach fails with \na large number of groups in both interpretation and visualization layer. \nThe~Merging Path Plot methodology solves this problem by using an \neasy-to-understand description of dissimilarity among groups based on \nLikelihood Ratio Test (LRT) statistic. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fd1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04412"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang", "title": "Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec. (arXiv:1710.02971v3 [cs.SI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.02971", "type": "text/html"}], "timestampUsec": "1513141858490880", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050d5702f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050d5702f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Since the invention of word2vec, the skip-gram model has significantly \nadvanced the research of network embedding, such as the recent emergence of the \nDeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of \nthe aforementioned models with negative sampling can be unified into the matrix \nfactorization framework with closed forms. Our analysis and proofs reveal that: \n(1) DeepWalk empirically produces a low-rank transformation of a network's \nnormalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk \nwhen the size of vertices' context is set to one; (3) As an extension of LINE, \nPTE can be viewed as the joint factorization of multiple networks' Laplacians; \n(4) node2vec is factorizing a matrix related to the stationary distribution and \ntransition probability tensor of a 2nd-order random walk. We further provide \nthe theoretical connections between skip-gram based network embedding \nalgorithms and the theory of graph Laplacian. Finally, we present the NetMF \nmethod as well as its approximation algorithm for computing network embedding. \nOur method offers significant improvements over DeepWalk and LINE for \nconventional network mining tasks. This work lays the theoretical foundation \nfor skip-gram based network embedding methods, leading to a better \nunderstanding of latent network representation learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fd8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.02971"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christina Heinze-Deml, Nicolai Meinshausen", "title": "Grouping-By-ID: Guarding Against Adversarial Domain Shifts. (arXiv:1710.11469v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.11469", "type": "text/html"}], "timestampUsec": "1513141858490879", "comments": [], "summary": {"content": "<p>When training a deep network for image classification, one can broadly \ndistinguish between two types of latent features that will drive the \nclassification. Following Gong et al. (2016), we can divide features into (i) \n\"core\" features $X^{ci}$ whose distribution $P(X^{ci} | Y)$ does not change \nsubstantially across domains and (ii) \"style\" or \"orthogonal\" features \n$X^\\perp$ whose distribution $P(X^\\perp | Y)$ can change substantially across \ndomains. These latter orthogonal features would generally include features such \nas position or brightness but also more complex ones like hair color or posture \nfor images of persons. We try to guard against future adversarial domain shifts \nby ideally just using the \"core\" features for classification. In contrast to \nprevious work, we assume that the domain itself is not observed and hence a \nlatent variable, i.e. we cannot directly see the distributional change of \nfeatures across different domains. We do assume, however, that we can sometimes \nobserve a so-called ID variable. E.g. we might know that two images show the \nsame person, with ID referring to the identity of the person. The method \nrequires only a small fraction of images to have an ID variable. We provide a \ncausal framework for the problem by adding the ID variable to the model of Gong \net al. (2016). If two or more samples share the same class and identifier, then \nwe treat those samples as counterfactuals under different interventions on the \northogonal features. Using this grouping-by-ID approach, we regularize the \nnetwork to provide near constant output across samples that share the same ID \nby penalizing with an appropriate graph Laplacian. This substantially improves \nperformance in settings where domains change in terms of image quality, \nbrightness, color or posture and movement. We show links to questions of \ninterpretability, fairness, transfer learning and adversarial examples. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fdf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.11469"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Felix Last, Georgios Douzas, Fernando Bacao", "title": "Oversampling for Imbalanced Learning Based on K-Means and SMOTE. (arXiv:1711.00837v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.00837", "type": "text/html"}], "timestampUsec": "1513141858490878", "comments": [], "summary": {"content": "<p>Learning from class-imbalanced data continues to be a common and challenging \nproblem in supervised learning as standard classification algorithms are \ndesigned to handle balanced class distributions. While different strategies \nexist to tackle this problem, methods which generate artificial data to achieve \na balanced class distribution are more versatile than modifications to the \nclassification algorithm. Such techniques, called oversamplers, modify the \ntraining data, allowing any classifier to be used with class-imbalanced \ndatasets. Many algorithms have been proposed for this task, but most are \ncomplex and tend to generate unnecessary noise. This work presents a simple and \neffective oversampling method based on k-means clustering and SMOTE \noversampling, which avoids the generation of noise and effectively overcomes \nimbalances between and within classes. Empirical results of extensive \nexperiments with 71 datasets show that training data oversampled with the \nproposed method improves classification results. Moreover, k-means SMOTE \nconsistently outperforms other popular oversampling methods. An implementation \nis made available in the python programming language. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fe3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.00837"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Saki Shinoda, Daniel E. Worrall, Gabriel J. Brostow", "title": "Virtual Adversarial Ladder Networks For Semi-supervised Learning. (arXiv:1711.07476v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07476", "type": "text/html"}], "timestampUsec": "1513141858490877", "comments": [], "summary": {"content": "<p>Semi-supervised learning (SSL) partially circumvents the high cost of \nlabeling data by augmenting a small labeled dataset with a large and relatively \ncheap unlabeled dataset drawn from the same distribution. This paper offers a \nnovel interpretation of two deep learning-based SSL approaches, ladder networks \nand virtual adversarial training (VAT), as applying distributional smoothing to \ntheir respective latent spaces. We propose a class of models that fuse these \napproaches. We achieve near-supervised accuracy with high consistency on the \nMNIST dataset using just 5 labels per class: our best model, ladder with \nlayer-wise virtual adversarial noise (LVAN-LW), achieves 1.42% +/- 0.12 average \nerror rate on the MNIST test set, in comparison with 1.62% +/- 0.65 reported \nfor the ladder network. On adversarial examples generated with L2-normalized \nfast gradient method, LVAN-LW trained with 5 examples per class achieves \naverage error rate 2.4% +/- 0.3 compared to 68.6% +/- 6.5 for the ladder \nnetwork and 9.9% +/- 7.5 for VAT. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127fe9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07476"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wataru Kumagai", "title": "Regret Analysis for Continuous Dueling Bandit. (arXiv:1711.07693v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07693", "type": "text/html"}], "timestampUsec": "1513141858490876", "comments": [], "summary": {"content": "<p>The dueling bandit is a learning framework wherein the feedback information \nin the learning process is restricted to a noisy comparison between a pair of \nactions. In this research, we address a dueling bandit problem based on a cost \nfunction over a continuous space. We propose a stochastic mirror descent \nalgorithm and show that the algorithm achieves an $O(\\sqrt{T\\log T})$-regret \nbound under strong convexity and smoothness assumptions for the cost function. \nSubsequently, we clarify the equivalence between regret minimization in dueling \nbandit and convex optimization for the cost function. Moreover, when \nconsidering a lower bound in convex optimization, our algorithm is shown to \nachieve the optimal convergence rate in convex optimization and the optimal \nregret in dueling bandit except for a logarithmic factor. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ff0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07693"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Maria De-Arteaga, William Herlands", "title": "Proceedings of NIPS 2017 Workshop on Machine Learning for the Developing World. (arXiv:1711.09522v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09522", "type": "text/html"}], "timestampUsec": "1513141858490875", "comments": [], "summary": {"content": "<p>This is the Proceedings of NIPS 2017 Workshop on Machine Learning for the \nDeveloping World, held in Long Beach, California, USA on December 8, 2017 \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513141858491", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000346127ff6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09522"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Boyang Deng, Junjie Yan, Dahua Lin", "title": "Peephole: Predicting Network Performance Before Training. (arXiv:1712.03351v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03351", "type": "text/html"}], "timestampUsec": "1513055249337172", "comments": [], "summary": {"content": "<p>The quest for performant networks has been a significant force that drives \nthe advancements of deep learning in recent years. While rewarding, improving \nnetwork design has never been an easy journey. The large design space combined \nwith the tremendous cost required for network training poses a major obstacle \nto this endeavor. In this work, we propose a new approach to this problem, \nnamely, predicting the performance of a network before training, based on its \narchitecture. Specifically, we develop a unified way to encode individual \nlayers into vectors and bring them together to form an integrated description \nvia LSTM. Taking advantage of the recurrent network's strong expressive power, \nthis method can reliably predict the performances of various network \narchitectures. Our empirical studies showed that it not only achieved accurate \npredictions but also produced consistent rankings across datasets -- a key \ndesideratum in performance prediction. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcee2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03351"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Abien Fred Agarap", "title": "An Architecture Combining Convolutional Neural Network (CNN) and Support Vector Machine (SVM) for Image Classification. (arXiv:1712.03541v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.03541", "type": "text/html"}], "timestampUsec": "1513055249337171", "comments": [], "summary": {"content": "<p>Convolutional neural networks (CNNs) are similar to \"ordinary\" neural \nnetworks in the sense that they are made up of hidden layers consisting of \nneurons with \"learnable\" parameters. These neurons receive inputs, performs a \ndot product, and then follows it with a non-linearity. The whole network \nexpresses the mapping between raw image pixels and their class scores. \nConventionally, the Softmax function is the classifier used at the last layer \nof this network. However, there have been studies (Alalshekmubarak and Smith, \n2013; Agarap, 2017; Tang, 2013) conducted to challenge this norm. The cited \nstudies introduce the usage of linear support vector machine (SVM) in an \nartificial neural network architecture. This project is yet another take on the \nsubject, and is inspired by (Tang, 2013). Empirical data has shown that the \nCNN-SVM model was able to achieve a test accuracy of ~99.04% using the MNIST \ndataset (LeCun, Cortes, and Burges, 2010). On the other hand, the CNN-Softmax \nwas able to achieve a test accuracy of ~99.23% using the same dataset. Both \nmodels were also tested on the recently-published Fashion-MNIST dataset (Xiao, \nRasul, and Vollgraf, 2017), which is suppose to be a more difficult image \nclassification dataset than MNIST (Zalandoresearch, 2017). This proved to be \nthe case as CNN-SVM reached a test accuracy of ~90.72%, while the CNN-Softmax \nreached a test accuracy of ~91.86%. The said results may be improved if data \npreprocessing techniques were employed on the datasets, and if the base CNN \nmodel was a relatively more sophisticated than the one used in this study. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcee5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03541"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Naveen Kodali, Jacob Abernethy, James Hays, Zsolt Kira", "title": "On Convergence and Stability of GANs. (arXiv:1705.07215v5 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.07215", "type": "text/html"}], "timestampUsec": "1513055249337170", "comments": [], "summary": {"content": "<p>We propose studying GAN training dynamics as regret minimization, which is in \ncontrast to the popular view that there is consistent minimization of a \ndivergence between real and generated distributions. We analyze the convergence \nof GAN training from this new point of view to understand why mode collapse \nhappens. We hypothesize the existence of undesirable local equilibria in this \nnon-convex game to be responsible for mode collapse. We observe that these \nlocal equilibria often exhibit sharp gradients of the discriminator function \naround some real data points. We demonstrate that these degenerate local \nequilibria can be avoided with a gradient penalty scheme called DRAGAN. We show \nthat DRAGAN enables faster training, achieves improved stability with fewer \nmode collapses, and leads to generator networks with better modeling \nperformance across a variety of architectures and objective functions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcee8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.07215"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhen Li, Zuoqiang Shi", "title": "A Flow Model of Neural Networks. (arXiv:1708.06257v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.06257", "type": "text/html"}], "timestampUsec": "1513055249337169", "comments": [], "summary": {"content": "<p>Based on a natural connection between ResNet and transport equation or its \ncharacteristic equation, we propose a continuous flow model for both ResNet and \nplain net. Through this continuous model, a ResNet can be explicitly \nconstructed as a refinement of a plain net. The flow model provides an \nalternative perspective to understand phenomena in deep neural networks, such \nas why it is necessary and sufficient to use 2-layer blocks in ResNets, why \ndeeper is better, and why ResNets are even deeper, and so on. It also opens a \ngate to bring in more tools from the huge area of differential equations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcef1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.06257"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Majdi Mafarja, Seyedali Mirjalili", "title": "S-Shaped vs. V-Shaped Transfer Functions for Antlion Optimization Algorithm in Feature Selection Problems. (arXiv:1712.03223v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.03223", "type": "text/html"}], "timestampUsec": "1513055249337168", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050d57257\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050d57257&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Feature selection is an important preprocessing step for classification \nproblems. It deals with selecting near optimal features in the original \ndataset. Feature selection is an NP-hard problem, so meta-heuristics can be \nmore efficient than exact methods. In this work, Ant Lion Optimizer (ALO), \nwhich is a recent metaheuristic algorithm, is employed as a wrapper feature \nselection method. Six variants of ALO are proposed where each employ a transfer \nfunction to map a continuous search space to a discrete search space. The \nperformance of the proposed approaches is tested on eighteen UCI datasets and \ncompared to a number of existing approaches in the literature: Particle Swarm \nOptimization, Gravitational Search Algorithm, and two existing ALO-based \napproaches. Computational experiments show that the proposed approaches \nefficiently explore the feature space and select the most informative features, \nwhich help to improve the classification accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcef6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03223"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Florian Krebs, Bruno Lubascher, Tobias Moers, Pieter Schaap, Gerasimos Spanakis", "title": "Social Emotion Mining Techniques for Facebook Posts Reaction Prediction. (arXiv:1712.03249v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.03249", "type": "text/html"}], "timestampUsec": "1513055249337167", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050da779b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050da779b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>As of February 2016 Facebook allows users to express their experienced \nemotions about a post by using five so-called `reactions'. This research paper \nproposes and evaluates alternative methods for predicting these reactions to \nuser posts on public pages of firms/companies (like supermarket chains). For \nthis purpose, we collected posts (and their reactions) from Facebook pages of \nlarge supermarket chains and constructed a dataset which is available for other \nresearches. In order to predict the distribution of reactions of a new post, \nneural network architectures (convolutional and recurrent neural networks) were \ntested using pretrained word embeddings. Results of the neural networks were \nimproved by introducing a bootstrapping approach for sentiment and emotion \nmining on the comments for each post. The final model (a combination of neural \nnetwork and a baseline emotion miner) is able to predict the reaction \ndistribution on Facebook posts with a mean squared error (or misclassification \nrate) of 0.135. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcefd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03249"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ben Parr, Deepak Dilipkumar, Yuan Liu", "title": "Nintendo Super Smash Bros. Melee: An \"Untouchable\" Agent. (arXiv:1712.03280v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.03280", "type": "text/html"}], "timestampUsec": "1513055249337166", "comments": [], "summary": {"content": "<p>Nintendo's Super Smash Bros. Melee fighting game can be emulated on modern \nhardware allowing us to inspect internal memory states, such as character \npositions. We created an AI that avoids being hit by training using these \ninternal memory states and outputting controller button presses. After training \non a month's worth of Melee matches, our best agent learned to avoid the \ntoughest AI built into the game for a full minute 74.6% of the time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03280"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Heejin Jeong, Daniel D. Lee", "title": "Bayesian Q-learning with Assumed Density Filtering. (arXiv:1712.03333v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03333", "type": "text/html"}], "timestampUsec": "1513055249337165", "comments": [], "summary": {"content": "<p>While off-policy temporal difference methods have been broadly used in \nreinforcement learning due to their efficiency and simple implementation, their \nBayesian counterparts have been relatively understudied. This is mainly because \nthe max operator in the Bellman optimality equation brings non-linearity and \ninconsistent distributions over value function. In this paper, we introduce a \nnew Bayesian approach to off-policy TD methods using Assumed Density Filtering, \ncalled ADFQ, which updates beliefs on action-values (Q) through an online \nBayesian inference method. Uncertainty measures in the beliefs not only are \nused in exploration but they provide a natural regularization in the belief \nupdates. We also present a connection between ADFQ and Q-learning. Our \nempirical results show the proposed ADFQ algorithms outperform comparing \nalgorithms in several task domains. Moreover, our algorithms improve general \ndrawbacks in BRL such as computational complexity, usage of uncertainty, and \nnonlinearity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf07", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03333"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Konda Reddy Mopuri, Utkarsh Ojha, Utsav Garg, R. Venkatesh Babu", "title": "NAG: Network for Adversary Generation. (arXiv:1712.03390v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.03390", "type": "text/html"}], "timestampUsec": "1513055249337164", "comments": [], "summary": {"content": "<p>Adversarial perturbations can pose a serious threat for deploying machine \nlearning systems. Recent works have shown existence of image-agnostic \nperturbations that can fool classifiers over most natural images. Existing \nmethods present optimization approaches that solve for a fooling objective with \nan imperceptibility constraint to craft the perturbations. However, for a given \nclassifier, they generate one perturbation at a time, which is a single \ninstance from the manifold of adversarial perturbations. Also, in order to \nbuild robust models, it is essential to explore the manifold of adversarial \nperturbations. In this paper, we propose for the first time, a generative \napproach to model the distribution of adversarial perturbations. The \narchitecture of the proposed model is inspired from that of GANs and is trained \nusing fooling and diversity objectives. Our trained generator network attempts \nto capture the distribution of adversarial perturbations for a given classifier \nand readily generates a wide variety of such perturbations. Our experimental \nevaluation demonstrates that perturbations crafted by our model (i) achieve \nstate-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver \nexcellent cross model generalizability. Our work can be deemed as an important \nstep in the process of inferring about the complex manifolds of adversarial \nperturbations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf0c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03390"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chanwoo Kim, Ehsan Variani, Arun Narayanan, Michiel Bacchiani", "title": "Efficient Implementation of the Room Simulator for Training Deep Neural Network Acoustic Models. (arXiv:1712.03439v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1712.03439", "type": "text/html"}], "timestampUsec": "1513055249337163", "comments": [], "summary": {"content": "<p>In this paper, we describe how to efficiently implement an acoustic room \nsimulator to generate large-scale simulated data for training deep neural \nnetworks. Even though Google Room Simulator in [1] was shown to be quite \neffective in reducing the Word Error Rates (WERs) for far-field applications by \ngenerating simulated far-field training sets, it requires a very large number \nof Fast Fourier Transforms (FFTs) of large size. Room Simulator in [1] used \napproximately 80 percent of Central Processing Unit (CPU) usage in our CPU + \nGraphics Processing Unit (GPU) training architecture [2]. In this work, we \nimplement an efficient OverLap Addition (OLA) based filtering using the \nopen-source FFTW3 library. Further, we investigate the effects of the Room \nImpulse Response (RIR) lengths. Experimentally, we conclude that we can cut the \ntail portions of RIRs whose power is less than 20 dB below the maximum power \nwithout sacrificing the speech recognition accuracy. However, we observe that \ncutting RIR tail more than this threshold harms the speech recognition accuracy \nfor rerecorded test sets. Using these approaches, we were able to reduce CPU \nusage for the room simulator portion down to 9.69 percent in CPU/GPU training \narchitecture. Profiling result shows that we obtain 22.4 times speed-up on a \nsingle machine and 37.3 times speed up on Google's distributed training \ninfrastructure. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf12", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03439"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexander Gruenstein, Raziel Alvarez, Chris Thornton, Mohammadali Ghodrat", "title": "A Cascade Architecture for Keyword Spotting on Mobile Devices. (arXiv:1712.03603v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1712.03603", "type": "text/html"}], "timestampUsec": "1513055249337162", "comments": [], "summary": {"content": "<p>We present a cascade architecture for keyword spotting with speaker \nverification on mobile devices. By pairing a small computational footprint with \nspecialized digital signal processing (DSP) chips, we are able to achieve low \npower consumption while continuously listening for a keyword. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf15", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03603"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Philipp Oettershagen, Florian Achermann, Benjamin M&#xfc;ller, Daniel Schneider, Roland Siegwart", "title": "Towards Fully Environment-Aware UAVs: Real-Time Path Planning with Online 3D Wind Field Prediction in Complex Terrain. (arXiv:1712.03608v1 [cs.RO])", "alternate": [{"href": "http://arxiv.org/abs/1712.03608", "type": "text/html"}], "timestampUsec": "1513055249337161", "comments": [], "summary": {"content": "<p>Today, low-altitude fixed-wing Unmanned Aerial Vehicles (UAVs) are largely \nlimited to primitively follow user-defined waypoints. To allow fully-autonomous \nremote missions in complex environments, real-time environment-aware navigation \nis required both with respect to terrain and strong wind drafts. This paper \npresents two relevant initial contributions: First, the literature's first-ever \n3D wind field prediction method which can run in real time onboard a UAV is \npresented. The approach retrieves low-resolution global weather data, and uses \npotential flow theory to adjust the wind field such that terrain boundaries, \nmass conservation, and the atmospheric stratification are observed. A \ncomparison with 1D LIDAR data shows an overall wind error reduction of 23% with \nrespect to the zero-wind assumption that is mostly used for UAV path planning \ntoday. However, given that the vertical winds are not resolved accurately \nenough further research is required and identified. Second, a sampling-based \npath planner that considers the aircraft dynamics in non-uniform wind \niteratively via Dubins airplane paths is presented. Performance optimizations, \ne.g. obstacle-aware sampling and fast 2.5D-map collision checks, render the \nplanner 50% faster than the Open Motion Planning Library (OMPL) implementation. \nTest cases in Alpine terrain show that the wind-aware planning performs up to \n50x less iterations than shortest-path planning and is thus slower in low \nwinds, but that it tends to deliver lower-cost paths in stronger winds. More \nimportantly, in contrast to the shortest-path planner, it always delivers \ncollision-free paths. Overall, our initial research demonstrates the \nfeasibility of 3D wind field prediction from a UAV and the advantages of \nwind-aware planning. This paves the way for follow-up research on \nfully-autonomous environment-aware navigation of UAVs in real-life missions and \ncomplex terrain. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03608"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, Girish Chowdhary", "title": "Robust Deep Reinforcement Learning with Adversarial Attacks. (arXiv:1712.03632v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03632", "type": "text/html"}], "timestampUsec": "1513055249337160", "comments": [], "summary": {"content": "<p>This paper proposes adversarial attacks for Reinforcement Learning (RL) and \nthen improves the robustness of Deep Reinforcement Learning algorithms (DRL) to \nparameter uncertainties with the help of these attacks. We show that even a \nnaively engineered attack successfully degrades the performance of DRL \nalgorithm. We further improve the attack using gradient information of an \nengineered loss function which leads to further degradation in performance. \nThese attacks are then leveraged during training to improve the robustness of \nRL within robust control framework. We show that this adversarial training of \nDRL algorithms like Deep Double Q learning and Deep Deterministic Policy \nGradients leads to significant increase in robustness to parameter variations \nfor RL benchmarks such as Cart-pole, Mountain Car, Hopper and Half Cheetah \nenvironment. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf28", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03632"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zlatan Ajanovic, Michael Stolz, Martin Horn", "title": "Novel model-based heuristics for energy optimal motion planning of an autonomous vehicle using A*. (arXiv:1712.03719v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1712.03719", "type": "text/html"}], "timestampUsec": "1513055249337159", "comments": [], "summary": {"content": "<p>Predictive motion planning is the key to achieve energy-efficient driving, \nwhich is one of the main benefits of automated driving. Researchers have been \nstudying the planning of velocity trajectories, a simpler form of motion \nplanning, for over a decade now and many different methods are available. \nDynamic programming has shown to be the most common choice due to its numerical \nbackground and ability to include nonlinear constraints and models. Although \nplanning of optimal trajectory is done in a systematic way, dynamic programming \ndoesn't use any knowledge about the considered problem to guide the exploration \nand therefore explores all possible trajectories. A* is an algorithm which \nenables using knowledge about the problem to guide the exploration to the most \npromising solutions first. Knowledge has to be represented in a form of a \nheuristic function, which gives an optimistic estimate of cost for \ntransitioning between two states, which is not a straightforward task. This \npaper presents a novel heuristics incorporating air drag and auxiliary power as \nwell as operational costs of the vehicle, besides kinetic and potential energy \nand rolling resistance known in the literature. Furthermore, optimal cruising \nvelocity, which depends on vehicle aerodynamic properties and auxiliary power, \nis derived. Results are compared for different variants of heuristic functions \nand dynamic programming as well. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf30", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03719"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rakesh R Pimplikar, Kushal Mukherjee, Gyana Parija, Harit Vishwakarma, Ramasuri Narayanam, Sarthak Ahuja, Rohith D Vallam, Ritwik Chaudhuri, Joydeep Mondal", "title": "Cogniculture: Towards a Better Human-Machine Co-evolution. (arXiv:1712.03724v1 [cs.CY])", "alternate": [{"href": "http://arxiv.org/abs/1712.03724", "type": "text/html"}], "timestampUsec": "1513055249337158", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050da79e0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050da79e0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Research in Artificial Intelligence is breaking technology barriers every \nday. New algorithms and high performance computing are making things possible \nwhich we could only have imagined earlier. Though the enhancements in AI are \nmaking life easier for human beings day by day, there is constant fear that AI \nbased systems will pose a threat to humanity. People in AI community have \ndiverse set of opinions regarding the pros and cons of AI mimicking human \nbehavior. Instead of worrying about AI advancements, we propose a novel idea of \ncognitive agents, including both human and machines, living together in a \ncomplex adaptive ecosystem, collaborating on human computation for producing \nessential social goods while promoting sustenance, survival and evolution of \nthe agents' life cycle. We highlight several research challenges and technology \nbarriers in achieving this goal. We propose a governance mechanism around this \necosystem to ensure ethical behaviors of all cognitive agents. Along with a \nnovel set of use-cases of Cogniculture, we discuss the road map ahead for this \njourney. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03724"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Bin Yu, Karl Kumbier", "title": "Artificial Intelligence and Statistics. (arXiv:1712.03779v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03779", "type": "text/html"}], "timestampUsec": "1513055249337157", "comments": [], "summary": {"content": "<p>Artificial intelligence (AI) is intrinsically data-driven. It calls for the \napplication of statistical concepts through human-machine collaboration during \ngeneration of data, development of algorithms, and evaluation of results. This \npaper discusses how such human-machine collaboration can be approached through \nthe statistical concepts of population, question of interest, \nrepresentativeness of training data, and scrutiny of results (PQRS). The PQRS \nworkflow provides a conceptual framework for integrating statistical ideas with \nhuman input into AI products and research. These ideas include experimental \ndesign principles of randomization and local control as well as the principle \nof stability to gain reproducibility and interpretability of algorithms and \ndata results. We discuss the use of these principles in the contexts of \nself-driving cars, automated medical diagnoses, and examples from the authors' \ncollaborative research. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf3b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03779"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Eunwoo Kim, Chanho Ahn, Songhwai Oh", "title": "Learning Nested Sparse Structures in Deep Neural Networks. (arXiv:1712.03781v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.03781", "type": "text/html"}], "timestampUsec": "1513055249337156", "comments": [], "summary": {"content": "<p>Recently, there have been increasing demands to construct compact deep \narchitectures to remove unnecessary redundancy and to improve the inference \nspeed. While many recent works focus on reducing the redundancy by eliminating \nunneeded weight parameters, it is not possible to apply a single deep \narchitecture for multiple devices with different resources. When a new device \nor circumstantial condition requires a new deep architecture, it is necessary \nto construct and train a new network from scratch. In this work, we propose a \nnovel deep learning framework, called a nested sparse network, which exploits \nan n-in-1-type nested structure in a neural network. A nested sparse network \nconsists of multiple levels of networks with a different sparsity ratio \nassociated with each level, and higher level networks share parameters with \nlower level networks to enable stable nested learning. The proposed framework \nrealizes a resource-aware versatile architecture as the same network can meet \ndiverse resource requirements. Moreover, the proposed nested network can learn \ndifferent forms of knowledge in its internal networks at different levels, \nenabling multiple tasks using a single network, such as coarse-to-fine \nhierarchical classification. In order to train the proposed nested sparse \nnetwork, we propose efficient weight connection learning and channel and layer \nscheduling strategies. We evaluate our network in multiple tasks, including \nadaptive deep compression, knowledge distillation, and learning class \nhierarchy, and demonstrate that nested sparse networks perform competitively, \nbut more efficiently, than existing methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf41", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03781"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Manolis Savva, Angel X. Chang, Alexey Dosovitskiy, Thomas Funkhouser, Vladlen Koltun", "title": "MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments. (arXiv:1712.03931v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03931", "type": "text/html"}], "timestampUsec": "1513055249337155", "comments": [], "summary": {"content": "<p>We present MINOS, a simulator designed to support the development of \nmultisensory models for goal-directed navigation in complex indoor \nenvironments. The simulator leverages large datasets of complex 3D environments \nand supports flexible configuration of multimodal sensor suites. We use MINOS \nto benchmark deep-learning-based navigation methods, to analyze the influence \nof environmental complexity on navigation performance, and to carry out a \ncontrolled study of multimodality in sensorimotor learning. The experiments \nshow that current deep reinforcement learning approaches fail in large \nrealistic environments. The experiments also indicate that multimodality is \nbeneficial in learning to navigate cluttered scenes. MINOS is released \nopen-source to the research community at <a href=\"http://minosworld.org\">this http URL</a> . A video that \nshows MINOS can be found at https://youtu.be/c0mL9K64q84 \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03931"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gaurav Bhatt, Aman Sharma, Shivam Sharma, Ankush Nagpal, Balasubramanian Raman, Ankush Mittal", "title": "On the Benefit of Combining Neural, Statistical and External Features for Fake News Identification. (arXiv:1712.03935v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.03935", "type": "text/html"}], "timestampUsec": "1513055249337154", "comments": [], "summary": {"content": "<p>Identifying the veracity of a news article is an interesting problem while \nautomating this process can be a challenging task. Detection of a news article \nas fake is still an open question as it is contingent on many factors which the \ncurrent state-of-the-art models fail to incorporate. In this paper, we explore \na subtask to fake news identification, and that is stance detection. Given a \nnews article, the task is to determine the relevance of the body and its claim. \nWe present a novel idea that combines the neural, statistical and external \nfeatures to provide an efficient solution to this problem. We compute the \nneural embedding from the deep recurrent model, statistical features from the \nweighted n-gram bag-of-words model and handcrafted external features with the \nhelp of feature engineering heuristics. Finally, using deep neural layer all \nthe features are combined, thereby classifying the headline-body news pair as \nagree, disagree, discuss, or unrelated. We compare our proposed technique with \nthe current state-of-the-art models on the fake news challenge dataset. Through \nextensive experiments, we find that the proposed model outperforms all the \nstate-of-the-art techniques including the submissions to the fake news \nchallenge. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf4f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03935"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gerhard Brewka, Stefan Ellmauthaler, Ricardo Gon&#xe7;alves, Matthias Knorr, Jo&#xe3;o Leite, J&#xf6;rg P&#xfc;hrer", "title": "Reactive Multi-Context Systems: Heterogeneous Reasoning in Dynamic Environments. (arXiv:1609.03438v3 [cs.LO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1609.03438", "type": "text/html"}], "timestampUsec": "1513055249337153", "comments": [], "summary": {"content": "<p>Managed multi-context systems (mMCSs) allow for the integration of \nheterogeneous knowledge sources in a modular and very general way. They were, \nhowever, mainly designed for static scenarios and are therefore not well-suited \nfor dynamic environments in which continuous reasoning over such heterogeneous \nknowledge with constantly arriving streams of data is necessary. In this paper, \nwe introduce reactive multi-context systems (rMCSs), a framework for reactive \nreasoning in the presence of heterogeneous knowledge sources and data streams. \nWe show that rMCSs are indeed well-suited for this purpose by illustrating how \nseveral typical problems arising in the context of stream reasoning can be \nhandled using them, by showing how inconsistencies possibly occurring in the \nintegration of multiple knowledge sources can be handled, and by arguing that \nthe potential non-determinism of rMCSs can be avoided if needed using an \nalternative, more skeptical well-founded semantics instead with beneficial \ncomputational properties. We also investigate the computational complexity of \nvarious reasoning problems related to rMCSs. Finally, we discuss related work, \nand show that rMCSs do not only generalize mMCSs to dynamic settings, but also \ncapture/extend relevant approaches w.r.t. dynamics in knowledge representation \nand stream reasoning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf54", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1609.03438"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Maxime Bucher (1), St&#xe9;phane Herbin (1), Fr&#xe9;d&#xe9;ric Jurie ((1) Palaiseau)", "title": "Generating Visual Representations for Zero-Shot Classification. (arXiv:1708.06975v3 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.06975", "type": "text/html"}], "timestampUsec": "1513055249337151", "comments": [], "summary": {"content": "<p>This paper addresses the task of learning an image clas-sifier when some \ncategories are defined by semantic descriptions only (e.g. visual attributes) \nwhile the others are defined by exemplar images as well. This task is often \nreferred to as the Zero-Shot classification task (ZSC). Most of the previous \nmethods rely on learning a common embedding space allowing to compare visual \nfeatures of unknown categories with semantic descriptions. This paper argues \nthat these approaches are limited as i) efficient discrimi-native classifiers \ncan't be used ii) classification tasks with seen and unseen categories \n(Generalized Zero-Shot Classification or GZSC) can't be addressed efficiently. \nIn contrast , this paper suggests to address ZSC and GZSC by i) learning a \nconditional generator using seen classes ii) generate artificial training \nexamples for the categories without exemplars. ZSC is then turned into a \nstandard supervised learning problem. Experiments with 4 generative models and \n5 datasets experimentally validate the approach, giving state-of-the-art \nresults on both ZSC and GZSC. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf61", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.06975"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Stephane Le Roux, Guillermo A. Perez", "title": "The Complexity of Graph-Based Reductions for Reachability in Markov Decision Processes. (arXiv:1710.07903v2 [cs.LO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.07903", "type": "text/html"}], "timestampUsec": "1513055249337150", "comments": [], "summary": {"content": "<p>We study the never-worse relation (NWR) for Markov decision processes with an \ninfinite-horizon reachability objective. A state q is never worse than a state \np if the maximal probability of reaching the target set of states from p is at \nmost the same value from q, regardless of the probabilities labelling the \ntransitions. Extremal-probability states, end components, and essential states \nare all special cases of the equivalence relation induced by the NWR. Using the \nNWR, states in the same equivalence class can be collapsed. Then, actions \nleading to sub-optimal states can be removed. We show the natural decision \nproblem associated to computing the NWR is coNP-complete. Finally, we extend a \nknown incomplete polynomial-time iterative algorithm to under-approximate the \nNWR. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf66", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.07903"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ting Chen, Martin Renqiang Min, Yizhou Sun", "title": "Learning K-way D-dimensional Discrete Code For Compact Embedding Representations. (arXiv:1711.03067v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.03067", "type": "text/html"}], "timestampUsec": "1513055249337149", "comments": [], "summary": {"content": "<p>Embedding methods such as word embedding have become pillars for many \napplications containing discrete structures. Conventional embedding methods \ndirectly associate each symbol with a continuous embedding vector, which is \nequivalent to applying linear transformation based on \"one-hot\" encoding of the \ndiscrete symbols. Despite its simplicity, such approach yields number of \nparameters that grows linearly with the vocabulary size and can lead to \noverfitting. In this work we propose a much more compact K-way D-dimensional \ndiscrete encoding scheme to replace the \"one-hot\" encoding. In \"KD encoding\", \neach symbol is represented by a $D$-dimensional code, and each of its dimension \nhas a cardinality of $K$. The final symbol embedding vector can be generated by \ncomposing the code embedding vectors. To learn the semantically meaningful \ncode, we derive a relaxed discrete optimization technique based on stochastic \ngradient descent. By adopting the new coding system, the efficiency of \nparameterization can be significantly improved (from linear to logarithmic), \nand this can also mitigate the over-fitting problem. In our experiments with \nlanguage modeling, the number of embedding parameters can be reduced by 97\\% \nwhile achieving similar or better performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf6e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.03067"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aleksandar Zeljic, Peter Backeman, Christoph M. Wintersteiger, Philipp Ruemmer", "title": "Exploring Approximations for Floating-Point Arithmetic using UppSAT. (arXiv:1711.08859v2 [cs.LO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08859", "type": "text/html"}], "timestampUsec": "1513055249337148", "comments": [], "summary": {"content": "<p>We consider the problem of solving floating-point constraints obtained from \nsoftware verification. We present UppSAT --- a new implementation of a \nsystematic approximation refinement framework [ZWR17] as an abstract SMT \nsolver. Provided with an approximation and a decision procedure (implemented in \nan off-the-shelf SMT solver), UppSAT yields an approximating SMT solver. \nAdditionally, UppSAT includes a library of predefined approximation components \nwhich can be combined and extended to define new encodings, orderings and \nsolving strategies. We propose that UppSAT can be used as a sandbox for easy \nand flexible exploration of new approximations. To substantiate this, we \nexplore several approximations of floating-point arithmetic. Approximations can \nbe viewed as a composition of an encoding into a target theory, a precision \nordering, and a number of strategies for model reconstruction and precision (or \napproximation) refinement. We present encodings of floating-point arithmetic \ninto reduced precision floating-point arithmetic, real-arithmetic, and \nfixed-point arithmetic (encoded in the theory of bit-vectors). In an \nexperimental evaluation, we compare the advantages and disadvantages of \napproximating solvers obtained by combining various encodings and decision \nprocedures (based on existing state-of-the-art SMT solvers for floating-point, \nreal, and bit-vector arithmetic). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf76", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08859"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mohammadreza Soltani, Chinmay Hegde", "title": "Fast Low-Rank Matrix Estimation without the Condition Number. (arXiv:1712.03281v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03281", "type": "text/html"}], "timestampUsec": "1513055249337145", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050da7c10\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050da7c10&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we study the general problem of optimizing a convex function \n$F(L)$ over the set of $p \\times p$ matrices, subject to rank constraints on \n$L$. However, existing first-order methods for solving such problems either are \ntoo slow to converge, or require multiple invocations of singular value \ndecompositions. On the other hand, factorization-based non-convex algorithms, \nwhile being much faster, require stringent assumptions on the \\emph{condition \nnumber} of the optimum. In this paper, we provide a novel algorithmic framework \nthat achieves the best of both worlds: asymptotically as fast as factorization \nmethods, while requiring no dependency on the condition number. \n</p> \n<p>We instantiate our general framework for three important matrix estimation \nproblems that impact several practical applications; (i) a \\emph{nonlinear} \nvariant of affine rank minimization, (ii) logistic PCA, and (iii) precision \nmatrix estimation in probabilistic graphical model learning. We then derive \nexplicit bounds on the sample complexity as well as the running time of our \napproach, and show that it achieves the best possible bounds for both cases. We \nalso provide an extensive range of experimental results, and demonstrate that \nour algorithm provides a very attractive tradeoff between estimation accuracy \nand running time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf83", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03281"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shankar Krishnan, Ying Xiao, Rif A. Saurous", "title": "Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks. (arXiv:1712.03298v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03298", "type": "text/html"}], "timestampUsec": "1513055249337144", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050e0ae41\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050e0ae41&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Progress in deep learning is slowed by the days or weeks it takes to train \nlarge models. The natural solution of using more hardware is limited by \ndiminishing returns, and leads to inefficient use of additional resources. In \nthis paper, we present a large batch, stochastic optimization algorithm that is \nboth faster than widely used algorithms for fixed amounts of computation, and \nalso scales up substantially better as more computational resources become \navailable. Our algorithm implicitly computes the inverse Hessian of each \nmini-batch to produce descent directions; we do so without either an explicit \napproximation to the Hessian or Hessian-vector products. We demonstrate the \neffectiveness of our algorithm by successfully training large ImageNet models \n(Inception-V3, Resnet-50, Resnet-101 and Inception-Resnet-V2) with mini-batch \nsizes of up to 32000 with no loss in validation error relative to current \nbaselines, and no increase in the total number of steps. At smaller mini-batch \nsizes, our optimizer improves the validation error in these models by 0.8-0.9%. \nAlternatively, we can trade off this accuracy to reduce the number of training \nsteps needed by roughly 10-30%. Our work is practical and easily usable by \nothers -- only one hyperparameter (learning rate) needs tuning, and \nfurthermore, the algorithm is as computationally cheap as the commonly used \nAdam optimizer. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf91", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03298"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chihao Zhang, Shihua Zhang", "title": "Bayesian Joint Matrix Decomposition for Data Integration with Heterogeneous Noise. (arXiv:1712.03337v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.03337", "type": "text/html"}], "timestampUsec": "1513055249337143", "comments": [], "summary": {"content": "<p>Matrix decomposition is a popular and fundamental approach in machine \nlearning and data mining. It has been successfully applied into various fields. \nMost matrix decomposition methods focus on decomposing a data matrix from one \nsingle source. However, it is common that data are from different sources with \nheterogeneous noise. A few of matrix decomposition methods have been extended \nfor such multi-view data integration and pattern discovery. While only few \nmethods were designed to consider the heterogeneity of noise in such multi-view \ndata for data integration explicitly. To this end, we propose a joint matrix \ndecomposition framework (BJMD), which models the heterogeneity of noise by \nGaussian distribution in a Bayesian framework. We develop two algorithms to \nsolve this model: one is a variational Bayesian inference algorithm, which \nmakes full use of the posterior distribution; and another is a maximum a \nposterior algorithm, which is more scalable and can be easily paralleled. \nExtensive experiments on synthetic and real-world datasets demonstrate that \nBJMD considering the heterogeneity of noise is superior or competitive to the \nstate-of-the-art methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf98", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03337"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Adam McCarthy, Blanca Rodriguez, Ana Minchole", "title": "Variational Inference over Non-differentiable Cardiac Simulators using Bayesian Optimization. (arXiv:1712.03353v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03353", "type": "text/html"}], "timestampUsec": "1513055249337142", "comments": [], "summary": {"content": "<p>Performing inference over simulators is generally intractable as their \nruntime means we cannot compute a marginal likelihood. We develop a \nlikelihood-free inference method to infer parameters for a cardiac simulator, \nwhich replicates electrical flow through the heart to the body surface. We \nimprove the fit of a state-of-the-art simulator to an electrocardiogram (ECG) \nrecorded from a real patient. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcf9f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03353"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Huiming Zhang, Jinzhu Jia", "title": "Elastic-net regularized High-dimensional Negative Binomial Regression: Consistency and Weak Signals Detection. (arXiv:1712.03412v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03412", "type": "text/html"}], "timestampUsec": "1513055249337141", "comments": [], "summary": {"content": "<p>We study sparse high-dimensional negative binomial regression problem for \ncount data regression by showing non-asymptotic merits of the Elastic-net \nregularized estimator. With the KKT conditions, we derive two types of \nnon-asymptotic oracle inequalities for the elastic net estimates of negative \nbinomial regression by utilizing Compatibility factor and Stabil Condition, \nrespectively. Based on oracle inequalities we proposed, we firstly show the \nsign consistency property of the Elastic-net estimators provided that the \nnon-zero components in sparse true vector are large than a proper choice of the \nweakest signal detection threshold, and the second application is that we give \nan oracle inequality for bounding the grouping effect with high probability, \nthirdly, under some assumptions of design matrix, we can recover the true \nvariable set with high probability if the weakest signal detection threshold is \nlarge than 3 times the value of turning parameter, at last, we briefly discuss \nthe de-biased Elastic-net estimator. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfa5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03412"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Matteo Pirotta, Marcello Restelli", "title": "Cost-Sensitive Approach to Batch Size Adaptation for Gradient Descent. (arXiv:1712.03428v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03428", "type": "text/html"}], "timestampUsec": "1513055249337140", "comments": [], "summary": {"content": "<p>In this paper, we propose a novel approach to automatically determine the \nbatch size in stochastic gradient descent methods. The choice of the batch size \ninduces a trade-off between the accuracy of the gradient estimate and the cost \nin terms of samples of each update. We propose to determine the batch size by \noptimizing the ratio between a lower bound to a linear or quadratic Taylor \napproximation of the expected improvement and the number of samples used to \nestimate the gradient. The performance of the proposed approach is empirically \ncompared with related methods on popular classification tasks. \n</p> \n<p>The work was presented at the NIPS workshop on Optimizing the Optimizers. \nBarcelona, Spain, 2016. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfa9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03428"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zahra Shakeri, Anand D. Sarwate, Waheed U. Bajwa", "title": "Identifiability of Kronecker-structured Dictionaries for Tensor Data. (arXiv:1712.03471v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03471", "type": "text/html"}], "timestampUsec": "1513055249337139", "comments": [], "summary": {"content": "<p>This paper derives sufficient conditions for reliable recovery of coordinate \ndictionaries comprising a Kronecker-structured dictionary that is used for \nrepresenting $K$th-order tensor data. Tensor observations are generated by a \nKronecker-structured dictionary and sparse coefficient tensors that follow the \nseparable sparsity model. This work provides sufficient conditions on the \nunderlying coordinate dictionaries, coefficient and noise distributions, and \nnumber of samples that guarantee recovery of the individual coordinate \ndictionaries up to a specified error with high probability. In particular, the \nsample complexity to recover $K$ coordinate dictionaries with dimensions \n$m_k\\times p_k$ up to estimation error $r_k$ is shown to be $\\max_{k \\in \n[K]}\\mathcal{O}(m_kp_k^3r_k^{-2})$. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfaf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03471"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Edgar Xi, Selina Bing, Yang Jin", "title": "Capsule Network Performance on Complex Data. (arXiv:1712.03480v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03480", "type": "text/html"}], "timestampUsec": "1513055249337138", "comments": [], "summary": {"content": "<p>In recent years, convolutional neural networks (CNN) have played an important \nrole in the field of deep learning. Variants of CNN's have proven to be very \nsuccessful in classification tasks across different domains. However, there are \ntwo big drawbacks to CNN's: their failure to take into account of important \nspatial hierarchies between features, and their lack of rotational invariance. \nAs long as certain key features of an object are present in the test data, \nCNN's classify the test data as the object, disregarding features' relative \nspatial orientation to each other. This causes false positives. The lack of \nrotational invariance in CNN's would cause the network to incorrectly assign \nthe object another label, causing false negatives. To address this concern, \nHinton et al. propose a novel type of neural network using the concept of \ncapsules in a recent paper. With the use of dynamic routing and reconstruction \nregularization, the capsule network model would be both rotation invariant and \nspatially aware. The capsule network has shown its potential by achieving a \nstate-of-the-art result of 0.25% test error on MNIST without data augmentation \nsuch as rotation and scaling, better than the previous baseline of 0.39%. To \nfurther test out the application of capsule networks on data with higher \ndimensionality, we attempt to find the best set of configurations that yield \nthe optimal test error on CIFAR10 dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03480"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pedro Silva, Sepehr Akhavan-Masouleh, Li Li", "title": "Improving Malware Detection Accuracy by Extracting Icon Information. (arXiv:1712.03483v1 [cs.CR])", "alternate": [{"href": "http://arxiv.org/abs/1712.03483", "type": "text/html"}], "timestampUsec": "1513055249337137", "comments": [], "summary": {"content": "<p>Detecting PE malware files is now commonly approached using statistical and \nmachine learning models. While these models commonly use features extracted \nfrom the structure of PE files, we propose that icons from these files can also \nhelp better predict malware. We propose an innovative machine learning approach \nto extract information from icons. Our proposed approach consists of two steps: \n1) extracting icon features using summary statics, histogram of gradients \n(HOG), and a convolutional autoencoder, 2) clustering icons based on the \nextracted icon features. Using publicly available data and by using machine \nlearning experiments, we show our proposed icon clusters significantly boost \nthe efficacy of malware prediction models. In particular, our experiments show \nan average accuracy increase of 10% when icon clusters are used in the \nprediction model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfb8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03483"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jason Poulos", "title": "Causal Inference for Observational Time-Series with Encoder-Decoder Networks. (arXiv:1712.03553v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03553", "type": "text/html"}], "timestampUsec": "1513055249337136", "comments": [], "summary": {"content": "<p>This paper proposes a method for estimating the causal effect of a discrete \nintervention in observational time-series data using encoder-decoder recurrent \nneural networks (RNNs). Encoder-decoder networks, which are special class of \nRNNs suitable for handling variable-length sequential data, are used to predict \na counterfactual time-series of treated unit outcomes. The proposed method does \nnot rely on pretreatment covariates and encoder-decoder networks are capable of \nlearning nonconvex combinations of control unit outcomes to construct a \ncounterfactual. To demonstrate the proposed method, I extend a field experiment \nstudying the effect of radio advertisements on electoral competition to \nobservational time-series. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfc1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03553"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Bo Wu, Yang Liu, Bo Lang, Lei Huang", "title": "DGCNN: Disordered Graph Convolutional Neural Network Based on the Gaussian Mixture Model. (arXiv:1712.03563v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03563", "type": "text/html"}], "timestampUsec": "1513055249337135", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050e0b145\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050e0b145&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Convolutional neural networks (CNNs) can be applied to graph similarity \nmatching, in which case they are called graph CNNs. Graph CNNs are attracting \nincreasing attention due to their effectiveness and efficiency. However, the \nexisting convolution approaches focus only on regular data forms and require \nthe transfer of the graph or key node neighborhoods of the graph into the same \nfixed form. During this transfer process, structural information of the graph \ncan be lost, and some redundant information can be incorporated. To overcome \nthis problem, we propose the disordered graph convolutional neural network \n(DGCNN) based on the mixed Gaussian model, which extends the CNN by adding a \npreprocessing layer called the disordered graph convolutional layer (DGCL). The \nDGCL uses a mixed Gaussian function to realize the mapping between the \nconvolution kernel and the nodes in the neighborhood of the graph. The output \nof the DGCL is the input of the CNN. We further implement a \nbackward-propagation optimization process of the convolutional layer by which \nwe incorporate the feature-learning model of the irregular node neighborhood \nstructure into the network. Thereafter, the optimization of the convolution \nkernel becomes part of the neural network learning process. The DGCNN can \naccept arbitrary scaled and disordered neighborhood graph structures as the \nreceptive fields of CNNs, which reduces information loss during graph \ntransformation. Finally, we perform experiments on multiple standard graph \ndatasets. The results show that the proposed method outperforms the \nstate-of-the-art methods in graph classification and retrieval. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfc5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03563"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Stefan Depeweg, Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato, Steffen Udluft, Thomas Runkler", "title": "Sensitivity Analysis for Predictive Uncertainty in Bayesian Neural Networks. (arXiv:1712.03605v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03605", "type": "text/html"}], "timestampUsec": "1513055249337134", "comments": [], "summary": {"content": "<p>We derive a novel sensitivity analysis of input variables for predictive \nepistemic and aleatoric uncertainty. We use Bayesian neural networks with \nlatent variables as a model class and illustrate the usefulness of our \nsensitivity analysis on real-world datasets. Our method increases the \ninterpretability of complex black-box probabilistic models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfcd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03605"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Robert Kwiatkowski, Oscar Chang", "title": "Gradient Normalization & Depth Based Decay For Deep Learning. (arXiv:1712.03607v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03607", "type": "text/html"}], "timestampUsec": "1513055249337133", "comments": [], "summary": {"content": "<p>In this paper we introduce a novel method of gradient normalization and decay \nwith respect to depth. Our method leverages the simple concept of normalizing \nall gradients in a deep neural network, and then decaying said gradients with \nrespect to their depth in the network. Our proposed normalization and decay \ntechniques can be used in conjunction with most current state of the art \noptimizers and are a very simple addition to any network. This method, although \nsimple, showed improvements in convergence time on state of the art networks \nsuch as DenseNet and ResNet on image classification tasks, as well as on an \nLSTM for natural language processing tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfd2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03607"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christos Thrampoulidis, Ankit Singh Rawat", "title": "The PhaseLift for Non-quadratic Gaussian Measurements. (arXiv:1712.03638v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03638", "type": "text/html"}], "timestampUsec": "1513055249337132", "comments": [], "summary": {"content": "<p>We study the problem of recovering a structured signal $\\mathbf{x}_0$ from \nhigh-dimensional measurements of the form $y=f(\\mathbf{a}^T\\mathbf{x}_0)$ for \nsome nonlinear function $f$. When the measurement vector $\\mathbf a$ is iid \nGaussian, Brillinger observed in his 1982 paper that $\\mu_\\ell\\cdot\\mathbf{x}_0 \n= \\min_{\\mathbf{x}}\\mathbb{E}(y - \\mathbf{a}^T\\mathbf{x})^2$, where \n$\\mu_\\ell=\\mathbb{E}_{\\gamma}[\\gamma f(\\gamma)]$ with $\\gamma$ being a standard \nGaussian random variable. Based on this simple observation, he showed that, in \nthe classical statistical setting, the least-squares method is consistent. More \nrecently, Plan \\&amp; Vershynin extended this result to the high-dimensional \nsetting and derived error bounds for the generalized Lasso. Unfortunately, both \nleast-squares and the Lasso fail to recover $\\mathbf{x}_0$ when $\\mu_\\ell=0$. \nFor example, this includes all even link functions. We resolve this issue by \nproposing and analyzing an appropriate generic semidefinite-optimization based \nmethod. In a nutshell, our idea is to treat such link functions as if they were \nlinear in a lifted space of higher-dimension. An appealing feature of our error \nanalysis is that it captures the effect of the nonlinearity in a few simple \nsummary parameters, which can be particularly useful in system design. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfda", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03638"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mustafa Hajij, Basem Assiri, Paul Rosen", "title": "Distributed Mapper. (arXiv:1712.03660v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.03660", "type": "text/html"}], "timestampUsec": "1513055249337131", "comments": [], "summary": {"content": "<p>The construction of Mapper has emerged in the last decade as a powerful and \neffective topological data analysis tool that approximates and generalizes \nother topological summaries, such as the Reeb graph, the contour tree, split, \nand joint trees. In this paper we study the parallel analysis of the \nconstruction of Mapper. We give a provably correct algorithm to distribute \nMapper on a set of processors and discuss the performance results that compare \nour approach to a reference sequential Mapper implementation. We report the \nperformance experiments that demonstrate the efficiency of our method. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcfe6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03660"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Luiz G A Alves, Haroldo V Ribeiro, Francisco A Rodrigues", "title": "Crime prediction through urban metrics and statistical learning. (arXiv:1712.03834v1 [physics.soc-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.03834", "type": "text/html"}], "timestampUsec": "1513055249337130", "comments": [], "summary": {"content": "<p>Understanding the causes of crime is a longstanding issue in researcher's \nagenda. While it is a hard task to extract causality from data, several linear \nmodels have been proposed to predict crime through the existing correlations \nbetween crime and urban metrics. However, because of non-Gaussian distributions \nand multicollinearity in urban indicators, it is common to find controversial \nconclusions about the influence of some urban indicators on crime. Machine \nlearning ensemble-based algorithms can handle well such problems. Here, we use \na random forest regressor to predict crime and quantify the influence of urban \nindicators on homicides. Our approach can have up to $97\\%$ of accuracy on \ncrime prediction and the importance of urban indicators is ranked and clustered \nin groups of equal influence, which are robust under slightly changes in the \ndata sample analyzed. Our results determine the rank of importance of urban \nindicators to predict crime, unveiling that unemployment and illiteracy are the \nmost important variables for describing homicides in Brazilian cities. We \nfurther believe that our approach helps in producing more robust conclusions \nregarding the effects of urban indicators on crime, having potential \napplications for guiding public policies for crime control. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dcffa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03834"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ferenc Husz&#xe1;r", "title": "On Quadratic Penalties in Elastic Weight Consolidation. (arXiv:1712.03847v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03847", "type": "text/html"}], "timestampUsec": "1513055249337129", "comments": [], "summary": {"content": "<p>Elastic weight consolidation (EWC, Kirkpatrick et al, 2017) is a novel \nalgorithm designed to safeguard against catastrophic forgetting in neural \nnetworks. EWC can be seen as an approximation to Laplace propagation (Eskin et \nal, 2004), and this view is consistent with the motivation given by Kirkpatrick \net al (2017). In this note, I present an extended derivation that covers the \ncase when there are more than two tasks. I show that the quadratic penalties in \nEWC are inconsistent with this derivation and might lead to double-counting \ndata from earlier tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd005", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03847"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gundeep Arora, Vinay Kumar Verma, Ashish Mishra, Piyush Rai", "title": "Generalized Zero-Shot Learning via Synthesized Examples. (arXiv:1712.03878v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03878", "type": "text/html"}], "timestampUsec": "1513055249337128", "comments": [], "summary": {"content": "<p>We present a generative framework for generalized zero-shot learning where \nthe training and test classes are not necessarily disjoint. Built upon a \nvariational autoencoder based architecture, consisting of a probabilistic \nencoder and a probabilistic conditional decoder, our model can generate novel \nexemplars from seen/unseen classes, given their respective class attributes. \nThese exemplars can subsequently be used to train any off-the-shelf \nclassification model. One of the key aspects of our encoder-decoder \narchitecture is a feedback-driven mechanism in which a discriminator (a \nmultivariate regressor) learns to map the generated exemplars to the \ncorresponding class attribute vectors, leading to an improved generator. Our \nmodel's ability to generate and leverage examples from unseen classes to train \nthe classification model naturally helps to mitigate the bias towards \npredicting seen classes in generalized zero-shot learning settings. Through a \ncomprehensive set of experiments, we show that our model outperforms several \nstate-of-the-art methods, on several benchmark datasets, for both standard as \nwell as generalized zero-shot learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd00a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03878"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Akinori Tanaka, Akio Tomiya", "title": "Towards reduction of autocorrelation in HMC by machine learning. (arXiv:1712.03893v1 [hep-lat])", "alternate": [{"href": "http://arxiv.org/abs/1712.03893", "type": "text/html"}], "timestampUsec": "1513055249337127", "comments": [], "summary": {"content": "<p>In this paper we propose new algorithm to reduce autocorrelation in Markov \nchain Monte-Carlo algorithms for euclidean field theories on the lattice. Our \nproposing algorithm is the Hybrid Monte-Carlo algorithm (HMC) with restricted \nBoltzmann machine. We examine the validity of the algorithm by employing the \nphi-fourth theory in three dimension. We observe reduction of the \nautocorrelation both in symmetric and broken phase as well. Our proposing \nalgorithm provides consistent central values of expectation values of the \naction density and one-point Green's function with ones from the original HMC \nin both the symmetric phase and broken phase within the statistical error. On \nthe other hand, two-point Green's functions have slight difference between one \ncalculated by the HMC and one by our proposing algorithm in the symmetric \nphase. Furthermore, near the criticality, the distribution of the one-point \nGreen's function differs from the one from HMC. We discuss the origin of \ndiscrepancies and its improvement. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd013", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03893"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Stefano Trac&#xe0;, Cynthia Rudin", "title": "Regulating Greed Over Time. (arXiv:1505.05629v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1505.05629", "type": "text/html"}], "timestampUsec": "1513055249337126", "comments": [], "summary": {"content": "<p>In retail, there are predictable yet dramatic time-dependent patterns in \ncustomer behavior, such as periodic changes in the number of visitors, or \nincreases in visitors just before major holidays. The current paradigm of \nmulti-armed bandit analysis does not take these known patterns into account. \nThis means that for applications in retail, where prices are fixed for periods \nof time, current bandit algorithms will not suffice. This work provides a \nremedy that takes the time-dependent patterns into account, and we show how \nthis remedy is implemented in the UCB and {\\epsilon}-greedy methods and we \nintroduce a new policy called the variable arm pool method. In the corrected \nmethods, exploitation (greed) is regulated over time, so that more exploitation \noccurs during higher reward periods, and more exploration occurs in periods of \nlow reward. In order to understand why regret is reduced with the corrected \nmethods, we present a set of bounds that provide insight into why we would want \nto exploit during periods of high reward, and discuss the impact on regret. Our \nproposed methods perform well in experiments, and were inspired by a \nhigh-scoring entry in the Exploration and Exploitation 3 contest using data \nfrom Yahoo! Front Page. That entry heavily used time-series methods to regulate \ngreed over time, which was substantially more effective than other contextual \nbandit methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd01e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1505.05629"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Matthias Bauer, Mateo Rojas-Carulla, Jakub Bart&#x142;omiej &#x15a;wi&#x105;tkowski, Bernhard Sch&#xf6;lkopf, Richard E. Turner", "title": "Discriminative k-shot learning using probabilistic models. (arXiv:1706.00326v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.00326", "type": "text/html"}], "timestampUsec": "1513055249337125", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050e0b3f7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050e0b3f7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper introduces a probabilistic framework for k-shot image \nclassification. The goal is to generalise from an initial large-scale \nclassification task to a separate task comprising new classes and small numbers \nof examples. The new approach not only leverages the feature-based \nrepresentation learned by a neural network from the initial task \n(representational transfer), but also information about the classes (concept \ntransfer). The concept information is encapsulated in a probabilistic model for \nthe final layer weights of the neural network which acts as a prior for \nprobabilistic k-shot learning. We show that even a simple probabilistic model \nachieves state-of-the-art on a standard k-shot learning dataset by a large \nmargin. Moreover, it is able to accurately model uncertainty, leading to well \ncalibrated classifiers, and is easily extensible and flexible, unlike many \nrecent approaches to k-shot learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd027", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.00326"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David W Dreisigmeyer", "title": "Tight Semi-Nonnegative Matrix Factorization. (arXiv:1709.04395v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04395", "type": "text/html"}], "timestampUsec": "1513055249337124", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050e6336b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050e6336b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The nonnegative matrix factorization is a widely used, flexible matrix \ndecomposition, finding applications in biology, image and signal processing and \ninformation retrieval, among other areas. Here we present a related matrix \nfactorization. A multi-objective optimization problem finds conical \ncombinations of templates that approximate a given data matrix. The templates \nare chosen so that as far as possible only the initial data set can be \nrepresented this way. However, the templates are not required to be nonnegative \nnor convex combinations of the original data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd031", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04395"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Arunselvan Ramaswamy, Shalabh Bhatnagar", "title": "Conditions for Stability and Convergence of Set-Valued Stochastic Approximations: Applications to Approximate Value and Fixed point Iterations. (arXiv:1709.04673v2 [cs.SY] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04673", "type": "text/html"}], "timestampUsec": "1513055249337123", "comments": [], "summary": {"content": "<p>The main aim of this paper is the development of easily verifiable sufficient \nconditions for stability (almost sure boundedness) and convergence of \nstochastic approximation algorithms (SAAs) with set-valued mean-fields, a class \nof model-free algorithms that have become important in recent times. In this \npaper we provide a complete analysis of such algorithms under three different, \nyet related sets of sufficient conditions, based on the existence of an \nassociated global/local Lyapunov function. Unlike previous Lyapunov function \nbased approaches, we provide a simple recipe for explicitly constructing the \nLyapunov function, needed for analysis. Our work builds on the works of \nAbounadi, Bertsekas and Borkar (2002), Munos (2005), and Ramaswamy and \nBhatnagar (2016). An important motivation for the flavor of our assumptions \ncomes from the need to understand dynamic programming and reinforcement \nlearning algorithms, that use deep neural networks (DNNs) for function \napproximations and parameterizations. These algorithms are popularly known as \ndeep learning algorithms. As an important application of our theory, we provide \na complete analysis of the stochastic approximation counterpart of approximate \nvalue iteration (AVI), an important dynamic programming method designed to \ntackle Bellman's curse of dimensionality. Further, the assumptions involved are \nsignificantly weaker, easily verifiable and truly model-free. The theory \npresented in this paper is also used to develop and analyze the first SAA for \nfinding fixed points of contractive set-valued maps. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd03c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04673"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anderson Y. Zhang, Harrison H. Zhou", "title": "Theoretical and Computational Guarantees of Mean Field Variational Inference for Community Detection. (arXiv:1710.11268v3 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.11268", "type": "text/html"}], "timestampUsec": "1513055249337122", "comments": [], "summary": {"content": "<p>The mean field variational Bayes method is becoming increasingly popular in \nstatistics and machine learning. Its iterative Coordinate Ascent Variational \nInference algorithm has been widely applied to large scale Bayesian inference. \nSee Blei et al. (2017) for a recent comprehensive review. Despite the \npopularity of the mean field method there exist remarkably little fundamental \ntheoretical justifications. To the best of our knowledge, the iterative \nalgorithm has never been investigated for any high dimensional and complex \nmodel. In this paper, we study the mean field method for community detection \nunder the Stochastic Block Model. For an iterative Batch Coordinate Ascent \nVariational Inference algorithm, we show that it has a linear convergence rate \nand converges to the minimax rate within $\\log n$ iterations. This complements \nthe results of Bickel et al. (2013) which studied the global minimum of the \nmean field variational Bayes and obtained asymptotic normal estimation of \nglobal model parameters. In addition, we obtain similar optimality results for \nGibbs sampling and an iterative procedure to calculate maximum likelihood \nestimation, which can be of independent interest. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd04a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.11268"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Therese Anders, Hong Xu, Cheng Cheng, T. K. Satish Kumar", "title": "Measuring Territorial Control in Civil Wars Using Hidden Markov Models: A Data Informatics-Based Approach. (arXiv:1711.06786v2 [stat.AP] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.06786", "type": "text/html"}], "timestampUsec": "1513055249337121", "comments": [], "summary": {"content": "<p>Territorial control is a key aspect shaping the dynamics of civil war. \nDespite its importance, we lack data on territorial control that are \nfine-grained enough to account for subnational spatio-temporal variation and \nthat cover a large set of conflicts. To resolve this issue, we propose a \ntheoretical model of the relationship between territorial control and tactical \nchoice in civil war and outline how Hidden Markov Models (HMMs) are suitable to \ncapture theoretical intuitions and estimate levels of territorial control. We \ndiscuss challenges of using HMMs in this application and mitigation strategies \nfor future work. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd04f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.06786"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Daniel George, Hongyu Shen, E. A. Huerta", "title": "Glitch Classification and Clustering for LIGO with Deep Transfer Learning. (arXiv:1711.07468v2 [astro-ph.IM] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07468", "type": "text/html"}], "timestampUsec": "1513055249337120", "comments": [], "summary": {"content": "<p>The detection of gravitational waves with LIGO and Virgo requires a detailed \nunderstanding of the response of these instruments in the presence of \nenvironmental and instrumental noise. Of particular interest is the study of \nanomalous non-Gaussian noise transients known as glitches, since their high \noccurrence rate in LIGO/Virgo data can obscure or even mimic true gravitational \nwave signals. Therefore, successfully identifying and excising glitches is of \nutmost importance to detect and characterize gravitational waves. In this \narticle, we present the first application of Deep Learning combined with \nTransfer Learning for glitch classification, using real data from LIGO's first \ndiscovery campaign labeled by Gravity Spy, showing that knowledge from \npre-trained models for real-world object recognition can be transferred for \nclassifying spectrograms of glitches. We demonstrate that this method enables \nthe optimal use of very deep convolutional neural networks for glitch \nclassification given small unbalanced training datasets, significantly reduces \nthe training time, and achieves state-of-the-art accuracy above 98.8%. Once \ntrained via transfer learning, we show that the networks can be truncated and \nused as feature extractors for unsupervised clustering to automatically group \ntogether new classes of glitches and anomalies. This novel capability is of \ncritical importance to identify and remove new types of glitches which will \noccur as the LIGO/Virgo detectors gradually attain design sensitivity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd060", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07468"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Subhadip Dey, Sawon Pratiher, Saon Banerjee, Chanchal Kumar Mukherjee", "title": "SolarisNet: A Deep Regression Network for Solar Radiation Prediction. (arXiv:1711.08413v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08413", "type": "text/html"}], "timestampUsec": "1513055249337119", "comments": [], "summary": {"content": "<p>Effective utilization of photovoltaic (PV) plants requires weather \nvariability robust global solar radiation (GSR) forecasting models. Random \nweather turbulence phenomena coupled with assumptions of clear sky model as \nsuggested by Hottel pose significant challenges to parametric &amp; non-parametric \nmodels in GSR conversion rate estimation. Also, a decent GSR estimate requires \ncostly high-tech radiometer and expert dependent instrument handling and \nmeasurements, which are subjective. As such, a computer aided monitoring (CAM) \nsystem to evaluate PV plant operation feasibility by employing smart grid past \ndata analytics and deep learning is developed. Our algorithm, SolarisNet is a \n6-layer deep neural network trained on data collected at two weather stations \nlocated near Kalyani metrological site, West Bengal, India. The daily GSR \nprediction performance using SolarisNet outperforms the existing state of art \nand its efficacy in inferring past GSR data insights to comprehend daily and \nseasonal GSR variability along with its competence for short term forecasting \nis discussed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1513055249337", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003454dd069", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08413"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kartik Audhkhasi, Brian Kingsbury, Bhuvana Ramabhadran, George Saon, Michael Picheny", "title": "Building competitive direct acoustics-to-word models for English conversational speech recognition. (arXiv:1712.03133v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.03133", "type": "text/html"}], "timestampUsec": "1512969981619087", "comments": [], "summary": {"content": "<p>Direct acoustics-to-word (A2W) models in the end-to-end paradigm have \nreceived increasing attention compared to conventional sub-word based automatic \nspeech recognition models using phones, characters, or context-dependent hidden \nMarkov model states. This is because A2W models recognize words from speech \nwithout any decoder, pronunciation lexicon, or externally-trained language \nmodel, making training and decoding with such models simple. Prior work has \nshown that A2W models require orders of magnitude more training data in order \nto perform comparably to conventional models. Our work also showed this \naccuracy gap when using the English Switchboard-Fisher data set. This paper \ndescribes a recipe to train an A2W model that closes this gap and is at-par \nwith state-of-the-art sub-word based models. We achieve a word error rate of \n8.8%/13.9% on the Hub5-2000 Switchboard/CallHome test sets without any decoder \nor language model. We find that model initialization, training data order, and \nregularization have the most impact on the A2W model performance. Next, we \npresent a joint word-character A2W model that learns to first spell the word \nand then recognize it. This model provides a rich output to the user instead of \nsimple word hypotheses, making it especially useful in the case of words unseen \nor rarely-seen during training. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902761", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03133"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wei Chen, YingYing Cao, Yifei Sun, Qunfeng Liu, Yun Li", "title": "Improving Brain Storm Optimization Algorithm via Simplex Search. (arXiv:1712.03166v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.03166", "type": "text/html"}], "timestampUsec": "1512969981619086", "comments": [], "summary": {"content": "<p>Through modeling human's brainstorming process, the brain storm optimization \n(BSO) algorithm has become a promising population based evolution algorithm. \nHowever, BSO is often good at global exploration but not good enough at local \nexploitation, just like most global optimization algorithms. In this paper, the \nNelder-Mead's Simplex (NMS) method is adopted in a simple version of BSO. Our \ngoal is to combine BSO's exploration ability and NMS's exploitation ability \ntogether, and develop an enhanced BSO via a better balance between global \nexploration and local exploitation. Large number of experimental results are \nreported, and the proposed algorithm is shown to perform better than both BSO \nand NMS. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902782", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03166"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Basant Agarwal, Heri Ramampiaro, Helge Langseth, Massimiliano Ruocco", "title": "A Deep Network Model for Paraphrase Detection in Short Text Messages. (arXiv:1712.02820v1 [cs.IR])", "alternate": [{"href": "http://arxiv.org/abs/1712.02820", "type": "text/html"}], "timestampUsec": "1512969981619084", "comments": [], "summary": {"content": "<p>This paper is concerned with paraphrase detection. The ability to detect \nsimilar sentences written in natural language is crucial for several \napplications, such as text mining, text summarization, plagiarism detection, \nauthorship authentication and question answering. Given two sentences, the \nobjective is to detect whether they are semantically identical. An important \ninsight from this work is that existing paraphrase systems perform well when \napplied on clean texts, but they do not necessarily deliver good performance \nagainst noisy texts. Challenges with paraphrase detection on user generated \nshort texts, such as Twitter, include language irregularity and noise. To cope \nwith these challenges, we propose a novel deep neural network-based approach \nthat relies on coarse-grained sentence modeling using a convolutional neural \nnetwork and a long short-term memory model, combined with a specific \nfine-grained word-level similarity matching model. Our experimental results \nshow that the proposed approach outperforms existing state-of-the-art \napproaches on user-generated noisy social media data, such as Twitter texts, \nand achieves highly competitive performance on a cleaner corpus. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449027ab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02820"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Li Zhou, Kevin Small, Oleg Rokhlenko, Charles Elkan", "title": "End-to-End Offline Goal-Oriented Dialog Policy Learning via Policy Gradient. (arXiv:1712.02838v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.02838", "type": "text/html"}], "timestampUsec": "1512969981619083", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050e6368d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050e6368d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Learning a goal-oriented dialog policy is generally performed offline with \nsupervised learning algorithms or online with reinforcement learning (RL). \nAdditionally, as companies accumulate massive quantities of dialog transcripts \nbetween customers and trained human agents, encoder-decoder methods have gained \npopularity as agent utterances can be directly treated as supervision without \nthe need for utterance-level annotations. However, one potential drawback of \nsuch approaches is that they myopically generate the next agent utterance \nwithout regard for dialog-level considerations. To resolve this concern, this \npaper describes an offline RL method for learning from unannotated corpora that \ncan optimize a goal-oriented policy at both the utterance and dialog level. We \nintroduce a novel reward function and use both on-policy and off-policy policy \ngradient to learn a policy offline without requiring online user interaction or \nan explicit state space definition. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449027b4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02838"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aditya Ganeshan", "title": "Per-Pixel Feedback for improving Semantic Segmentation. (arXiv:1712.02861v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.02861", "type": "text/html"}], "timestampUsec": "1512969981619082", "comments": [], "summary": {"content": "<p>Semantic segmentation is the task of assigning a label to each pixel in the \nimage.In recent years, deep convolutional neural networks have been driving \nadvances in multiple tasks related to cognition. Although, DCNNs have resulted \nin unprecedented visual recognition performances, they offer little \ntransparency. To understand how DCNN based models work at the task of semantic \nsegmentation, we try to analyze the DCNN models in semantic segmentation. We \ntry to find the importance of global image information for labeling pixels. \n</p> \n<p>Based on the experiments on discriminative regions, and modeling of \nfixations, we propose a set of new training loss functions for fine-tuning DCNN \nbased models. The proposed training regime has shown improvement in performance \nof DeepLab Large FOV(VGG-16) Segmentation model for PASCAL VOC 2012 dataset. \nHowever, further test remains to conclusively evaluate the benefits due to the \nproposed loss functions across models, and data-sets. \n</p> \n<p>Submitted in part fulfillment of the requirements for the degree of \nIntegrated Masters of Science in Applied Mathematics. \n</p> \n<p>Update: Further Experiment showed minimal benefits. \n</p> \n<p>Code Available [here](https://github.com/BardOfCodes/Seg-Unravel). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449027c8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02861"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Harold Boley, Gen Zou", "title": "Perspectival Knowledge in PSOA RuleML: Representation, Model Theory, and Translation. (arXiv:1712.02869v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.02869", "type": "text/html"}], "timestampUsec": "1512969981619081", "comments": [], "summary": {"content": "<p>In Positional-Slotted Object-Applicative (PSOA) RuleML, a predicate \napplication (atom) can have an Object IDentifier (OID) and descriptors that may \nbe positional arguments (tuples) or attribute-value pairs (slots). PSOA RuleML \n1.0 specifies for each descriptor whether it is to be interpreted under the \nperspective of the predicate in whose scope it occurs. This perspectivity \ndimension refines the space between oidless, positional atoms (relationships) \nand oidful, slotted atoms (frames): While relationships use only a \npredicate-scope-sensitive (predicate-dependent) tuple and frames use only \npredicate-scope-insensitive (predicate-independent) slots, PSOA RuleML 1.0 uses \na systematics of orthogonal constructs also permitting atoms with \n(predicate-)independent tuples and atoms with (predicate-)dependent slots. This \nsupports data and knowledge representation where a slot attribute can have \ndifferent values depending on the predicate. PSOA thus extends object-oriented \nmulti-membership and multiple inheritance. Based on objectification, PSOA laws \nare given: Besides unscoping and centralization, the semantic restriction and \ntransformation of describution permits rescoping of one atom's independent \ndescriptors to another atom with the same OID but a different predicate. For \ninheritance, default descriptors are realized by rules. On top of a metamodel \nand a Grailog visualization, PSOA's atom systematics for facts, queries, and \nrules is explained. The presentation and (XML-)serialization syntaxes of PSOA \nRuleML 1.0 are introduced. Its model-theoretic semantics is formalized by \nextending the earlier interpretation functions for dependent descriptors. The \nopen-source PSOATransRun 1.3 system realizes PSOA RuleML 1.0 by a translator to \nruntime predicates, including for dependent tuples (prdtupterm) and slots \n(prdsloterm). Our tests show efficiency advantages of dependent and tupled \nmodeling. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449027ea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02869"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chen Zhu, Hengshu Zhu, Hui Xiong, Pengliang Ding, Fang Xie", "title": "Recruitment Market Trend Analysis with Sequential Latent Variable Models. (arXiv:1712.02975v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.02975", "type": "text/html"}], "timestampUsec": "1512969981619080", "comments": [], "summary": {"content": "<p>Recruitment market analysis provides valuable understanding of \nindustry-specific economic growth and plays an important role for both \nemployers and job seekers. With the rapid development of online recruitment \nservices, massive recruitment data have been accumulated and enable a new \nparadigm for recruitment market analysis. However, traditional methods for \nrecruitment market analysis largely rely on the knowledge of domain experts and \nclassic statistical models, which are usually too general to model large-scale \ndynamic recruitment data, and have difficulties to capture the fine-grained \nmarket trends. To this end, in this paper, we propose a new research paradigm \nfor recruitment market analysis by leveraging unsupervised learning techniques \nfor automatically discovering recruitment market trends based on large-scale \nrecruitment data. Specifically, we develop a novel sequential latent variable \nmodel, named MTLVM, which is designed for capturing the sequential dependencies \nof corporate recruitment states and is able to automatically learn the latent \nrecruitment topics within a Bayesian generative framework. In particular, to \ncapture the variability of recruitment topics over time, we design hierarchical \ndirichlet processes for MTLVM. These processes allow to dynamically generate \nthe evolving recruitment topics. Finally, we implement a prototype system to \nempirically evaluate our approach based on real-world recruitment data in \nChina. Indeed, by visualizing the results from MTLVM, we can successfully \nreveal many interesting findings, such as the popularity of LBS related jobs \nreached the peak in the 2nd half of 2014, and decreased in 2015. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449027ff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02975"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Farnood Salehi, Patrick Thiran, L. Elisa Celis", "title": "Stochastic Dual Coordinate Descent with Bandit Sampling. (arXiv:1712.03010v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03010", "type": "text/html"}], "timestampUsec": "1512969981619079", "comments": [], "summary": {"content": "<p>Coordinate descent methods minimize a cost function by updating a single \ndecision variable (corresponding to one coordinate) at a time. Ideally, one \nwould update the decision variable that yields the largest marginal decrease in \nthe cost function. However, finding this coordinate would require checking all \nof them, which is not computationally practical. We instead propose a new \nadaptive method for coordinate descent. First, we define a lower bound on the \ndecrease of the cost function when a coordinate is updated and, instead of \ncalculating this lower bound for all coordinates, we use a multi-armed bandit \nalgorithm to learn which coordinates result in the largest marginal decrease \nwhile simultaneously performing coordinate descent. We show that our approach \nimproves the convergence of the coordinate methods (including parallel \nversions) both theoretically and experimentally. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902809", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03010"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zengkun Li", "title": "A Heuristic Search Algorithm Using the Stability of Learning Algorithms as the Fitness Function in Certain Scenarios: An Artificial General Intelligence Engineering Approach. (arXiv:1712.03043v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.03043", "type": "text/html"}], "timestampUsec": "1512969981619078", "comments": [], "summary": {"content": "<p>This paper presents a non-manual design engineering method based on heuristic \nsearch algorithm to search for candidate agents in the solution space which \nformed by artificial intelligence agents modeled on the base of \nbionics.Compared with the artificial design method represented by meta-learning \nand the bionics method represented by the neural architecture chip,this method \nis more feasible for realizing artificial general intelligence,and it has a \nmuch better interaction with cognitive neuroscience;at the same time,the \nengineering method is based on the theoretical hypothesis that the final \nlearning algorithm is stable in certain scenarios,and has generalization \nability in various scenarios.The paper discusses the theory preliminarily and \nproposes the possible correlation between the theory and the fixed-point \ntheorem in the field of mathematics.Limited by the author's knowledge \nlevel,this correlation is proposed only as a kind of conjecture. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902814", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03043"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mayank Kejriwal, Jiayuan Ding, Runqi Shao, Anoop Kumar, Pedro Szekely", "title": "FlagIt: A System for Minimally Supervised Human Trafficking Indicator Mining. (arXiv:1712.03086v1 [cs.CY])", "alternate": [{"href": "http://arxiv.org/abs/1712.03086", "type": "text/html"}], "timestampUsec": "1512969981619077", "comments": [], "summary": {"content": "<p>In this paper, we describe and study the indicator mining problem in the \nonline sex advertising domain. We present an in-development system, FlagIt \n(Flexible and adaptive generation of Indicators from text), which combines the \nbenefits of both a lightweight expert system and classical semi-supervision \n(heuristic re-labeling) with recently released state-of-the-art unsupervised \ntext embeddings to tag millions of sentences with indicators that are highly \ncorrelated with human trafficking. The FlagIt technology stack is open source. \nOn preliminary evaluations involving five indicators, FlagIt illustrates \npromising performance compared to several alternatives. The system is being \nactively developed, refined and integrated into a domain-specific search system \nused by over 200 law enforcement agencies to combat human trafficking, and is \nbeing aggressively extended to mine at least six more indicators with minimal \nprogramming effort. FlagIt is a good example of a system that operates in \nlimited label settings, and that requires creative combinations of established \nmachine learning techniques to produce outputs that could be used by real-world \nnon-technical analysts. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902818", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03086"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Charles A. Johnson, Enoch Yeung", "title": "A Class of Logistic Functions for Approximating State-Inclusive Koopman Operators. (arXiv:1712.03132v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.03132", "type": "text/html"}], "timestampUsec": "1512969981619076", "comments": [], "summary": {"content": "<p>An outstanding challenge in nonlinear systems theory is identification or \nlearning of a given nonlinear system's Koopman operator directly from data or \nmodels. Advances in extended dynamic mode decomposition approaches and machine \nlearning methods have enabled data-driven discovery of Koopman operators, for \nboth continuous and discrete-time systems. Since Koopman operators are often \ninfinite-dimensional, they are approximated in practice using \nfinite-dimensional systems. The fidelity and convergence of a given \nfinite-dimensional Koopman approximation is a subject of ongoing research. In \nthis paper we introduce a class of Koopman observable functions that confer an \napproximate closure property on their corresponding finite-dimensional \napproximations of the Koopman operator. We derive error bounds for the fidelity \nof this class of observable functions, as well as identify two key learning \nparameters which can be used to tune performance. We illustrate our approach on \ntwo classical nonlinear system models: the Van Der Pol oscillator and the \nbistable toggle switch. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902822", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03132"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sylwia Polberg, Anthony Hunter", "title": "Empirical Evaluation of Abstract Argumentation: Supporting the Need for Bipolar and Probabilistic Approaches. (arXiv:1707.09324v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.09324", "type": "text/html"}], "timestampUsec": "1512969981619075", "comments": [], "summary": {"content": "<p>In dialogical argumentation it is often assumed that the involved parties \nalways correctly identify the intended statements posited by each other, \nrealize all of the associated relations, conform to the three acceptability \nstates (accepted, rejected, undecided), adjust their views when new and correct \ninformation comes in, and that a framework handling only attack relations is \nsufficient to represent their opinions. Although it is natural to make these \nassumptions as a starting point for further research, removing them or even \nacknowledging that such removal should happen is more challenging for some of \nthese concepts than for others. Probabilistic argumentation is one of the \napproaches that can be harnessed for more accurate user modelling. The \nepistemic approach allows us to represent how much a given argument is believed \nby a given person, offering us the possibility to express more than just three \nagreement states. It is equipped with a wide range of postulates, including \nthose that do not make any restrictions concerning how initial arguments should \nbe viewed, thus potentially being more adequate for handling beliefs of the \npeople that have not fully disclosed their opinions in comparison to Dung's \nsemantics. The constellation approach can be used to represent the views of \ndifferent people concerning the structure of the framework we are dealing with, \nincluding cases in which not all relations are acknowledged or when they are \nseen differently than intended. Finally, bipolar argumentation frameworks can \nbe used to express both positive and negative relations between arguments. In \nthis paper we describe the results of an experiment in which participants \njudged dialogues in terms of agreement and structure. We compare our findings \nwith the aforementioned assumptions as well as with the constellation and \nepistemic approaches to probabilistic argumentation and bipolar argumentation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902832", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.09324"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexander Peysakhovich, Adam Lerer", "title": "Prosocial learning agents solve generalized Stag Hunts better than selfish ones. (arXiv:1709.02865v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.02865", "type": "text/html"}], "timestampUsec": "1512969981619074", "comments": [], "summary": {"content": "<p>Deep reinforcement learning has become an important paradigm for constructing \nagents that can enter complex multi-agent situations and improve their policies \nthrough experience. One commonly used technique is reactive training - applying \nstandard RL methods while treating other agents as a part of the learner's \nenvironment. It is known that in general-sum games reactive training can lead \ngroups of agents to converge to inefficient outcomes. We focus on one such \nclass of environments: Stag Hunt games. Here agents either choose a risky \ncooperative policy (which leads to high payoffs if both choose it but low \npayoffs to an agent who attempts it alone) or a safe one (which leads to a safe \npayoff no matter what). We ask how we can change the learning rule of a single \nagent to improve its outcomes in Stag Hunts that include other reactive \nlearners. We extend existing work on reward-shaping in multi-agent \nreinforcement learning and show that that making a single agent prosocial, that \nis, making them care about the rewards of their partners can increase the \nprobability that groups converge to good outcomes. Thus, even if we control a \nsingle agent in a group making that agent prosocial can increase our agent's \nlong-run payoff. We show experimentally that this result carries over to a \nvariety of more complex environments with Stag Hunt-like dynamics including \nones where agents must learn from raw input pixels. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902846", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.02865"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, Jun Wang", "title": "Long Text Generation via Adversarial Training with Leaked Information. (arXiv:1709.08624v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.08624", "type": "text/html"}], "timestampUsec": "1512969981619073", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050e63983\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050e63983&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Automatically generating coherent and semantically meaningful text has many \napplications in machine translation, dialogue systems, image captioning, etc. \nRecently, by combining with policy gradient, Generative Adversarial Nets (GAN) \nthat use a discriminative model to guide the training of the generative model \nas a reinforcement learning policy has shown promising results in text \ngeneration. However, the scalar guiding signal is only available after the \nentire text has been generated and lacks intermediate information about text \nstructure during the generative process. As such, it limits its success when \nthe length of the generated text samples is long (more than 20 words). In this \npaper, we propose a new framework, called LeakGAN, to address the problem for \nlong text generation. We allow the discriminative net to leak its own \nhigh-level extracted features to the generative net to further help the \nguidance. The generator incorporates such informative signals into all \ngeneration steps through an additional Manager module, which takes the \nextracted features of current generated words and outputs a latent vector to \nguide the Worker module for next-word generation. Our extensive experiments on \nsynthetic data and various real-world tasks with Turing test demonstrate that \nLeakGAN is highly effective in long text generation and also improves the \nperformance in short text generation scenarios. More importantly, without any \nsupervision, LeakGAN would be able to implicitly learn sentence structures only \nthrough the interaction between Manager and Worker. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902858", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.08624"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Enoch Yeung, Soumya Kundu, Nathan Hodas", "title": "Learning Deep Neural Network Representations for Koopman Operators of Nonlinear Dynamical Systems. (arXiv:1708.06850v2 [cs.LG] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1708.06850", "type": "text/html"}], "timestampUsec": "1512969981619072", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050eb6396\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050eb6396&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The Koopman operator has recently garnered much attention for its value in \ndynamical systems analysis and data-driven model discovery. However, its \napplication has been hindered by the computational complexity of extended \ndynamic mode decomposition; this requires a combinatorially large basis set to \nadequately describe many nonlinear systems of interest, e.g. cyber-physical \ninfrastructure systems, biological networks, social systems, and fluid \ndynamics. Often the dictionaries generated for these problems are manually \ncurated, requiring domain-specific knowledge and painstaking tuning. In this \npaper we introduce a deep learning framework for learning Koopman operators of \nnonlinear dynamical systems. We show that this novel method automatically \nselects efficient deep dictionaries, outperforming state-of-the-art methods. We \nbenchmark this method on partially observed nonlinear systems, including the \nglycolytic oscillator and show it is able to predict quantitatively 100 steps \ninto the future, using only a single timepoint, and qualitative oscillatory \nbehavior 400 steps into the future. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902869", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.06850"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Seyed Mehran Kazemi, David Poole", "title": "RelNN: A Deep Neural Model for Relational Learning. (arXiv:1712.02831v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02831", "type": "text/html"}], "timestampUsec": "1512969981619071", "comments": [], "summary": {"content": "<p>Statistical relational AI (StarAI) aims at reasoning and learning in noisy \ndomains described in terms of objects and relationships by combining \nprobability with first-order logic. With huge advances in deep learning in the \ncurrent years, combining deep networks with first-order logic has been the \nfocus of several recent studies. Many of the existing attempts, however, only \nfocus on relations and ignore object properties. The attempts that do consider \nobject properties are limited in terms of modelling power or scalability. In \nthis paper, we develop relational neural networks (RelNNs) by adding hidden \nlayers to relational logistic regression (the relational counterpart of \nlogistic regression). We learn latent properties for objects both directly and \nthrough general rules. Back-propagation is used for training these models. A \nmodular, layer-wise architecture facilitates utilizing the techniques developed \nwithin deep learning community to our architecture. Initial experiments on \neight tasks over three real-world datasets show that RelNNs are promising \nmodels for relational learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902883", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02831"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lukas Mosser, Olivier Dubrule, Martin J. Blunt", "title": "Stochastic reconstruction of an oolitic limestone by generative adversarial networks. (arXiv:1712.02854v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.02854", "type": "text/html"}], "timestampUsec": "1512969981619070", "comments": [], "summary": {"content": "<p>Stochastic image reconstruction is a key part of modern digital rock physics \nand materials analysis that aims to create numerous representative samples of \nmaterial micro-structures for upscaling, numerical computation of effective \nproperties and uncertainty quantification. We present a method of \nthree-dimensional stochastic image reconstruction based on generative \nadversarial neural networks (GANs). GANs represent a framework of unsupervised \nlearning methods that require no a priori inference of the probability \ndistribution associated with the training data. Using a fully convolutional \nneural network allows fast sampling of large volumetric images.We apply a GAN \nbased workflow of network training and image generation to an oolitic Ketton \nlimestone micro-CT dataset. Minkowski functionals, effective permeability as \nwell as velocity distributions of simulated flow within the acquired images are \ncompared with the synthetic reconstructions generated by the deep neural \nnetwork. While our results show that GANs allow a fast and accurate \nreconstruction of the evaluated image dataset, we address a number of open \nquestions and challenges involved in the evaluation of generative network-based \nmethods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902893", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02854"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Valerio Perrone, Rodolphe Jenatton, Matthias Seeger, Cedric Archambeau", "title": "Multiple Adaptive Bayesian Linear Regression for Scalable Bayesian Optimization with Warm Start. (arXiv:1712.02902v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02902", "type": "text/html"}], "timestampUsec": "1512969981619069", "comments": [], "summary": {"content": "<p>Bayesian optimization (BO) is a model-based approach for gradient-free \nblack-box function optimization. Typically, BO is powered by a Gaussian process \n(GP), whose algorithmic complexity is cubic in the number of evaluations. \nHence, GP-based BO cannot leverage large amounts of past or related function \nevaluations, for example, to warm start the BO procedure. We develop a multiple \nadaptive Bayesian linear regression model as a scalable alternative whose \ncomplexity is linear in the number of observations. The multiple Bayesian \nlinear regression models are coupled through a shared feedforward neural \nnetwork, which learns a joint representation and transfers knowledge across \nmachine learning problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449028b6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02902"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Panagiotis A. Traganitis, Alba Pag&#xe8;s-Zamora, Georgios B. Giannakis", "title": "Blind Multi-class Ensemble Learning with Unequally Reliable Classifiers. (arXiv:1712.02903v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02903", "type": "text/html"}], "timestampUsec": "1512969981619068", "comments": [], "summary": {"content": "<p>The rising interest in pattern recognition and data analytics has spurred the \ndevelopment of innovative machine learning algorithms and tools. However, as \neach algorithm has its strengths and limitations, one is motivated to \njudiciously fuse multiple algorithms in order to find the \"best\" performing \none, for a given dataset. Ensemble learning aims at such high-performance \nmeta-algorithm, by combining the outputs from multiple algorithms. The present \nwork introduces a blind scheme for learning from ensembles of classifiers, \nusing a moment matching method that leverages joint tensor and matrix \nfactorization. Blind refers to the combiner who has no knowledge of the \nground-truth labels that each classifier has been trained on. A rigorous \nperformance analysis is derived and the proposed scheme is evaluated on \nsynthetic and real datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449028cb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02903"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Casey Chu, Andrey Zhmoginov, Mark Sandler", "title": "CycleGAN: a Master of Steganography. (arXiv:1712.02950v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.02950", "type": "text/html"}], "timestampUsec": "1512969981619067", "comments": [], "summary": {"content": "<p>CycleGAN is one of the latest successful approaches to learn a correspondence \nbetween two image distributions. In a series of experiments, we demonstrate an \nintriguing property of the model: CycleGAN learns to \"hide\" information about a \nsource image inside the generated image in nearly imperceptible, high-frequency \nnoise. This trick ensures that the complementary generator can recover the \noriginal sample and thus satisfy the cyclic consistency requirement, but the \ngenerated image remains realistic. We connect this phenomenon with adversarial \nattacks by viewing CycleGAN's training procedure as training a generator of \nadversarial examples, thereby showing that adversarial attacks are not limited \nto classifiers but also may target generative models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449028d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02950"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xue Lu, Niall Adams, Nikolas Kantas", "title": "On Adaptive Estimation for Dynamic Bernoulli Bandits. (arXiv:1712.03134v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.03134", "type": "text/html"}], "timestampUsec": "1512969981619066", "comments": [], "summary": {"content": "<p>The multi-armed bandit (MAB) problem is a classic example of the \nexploration-exploitation dilemma. It is concerned with maximising the total \nrewards for a gambler by sequentially pulling an arm from a multi-armed slot \nmachine where each arm is associated with a reward distribution. In static \nMABs, the reward distributions do not change over time, while in dynamic MABs, \neach arm's reward distribution can change, and the optimal arm can switch over \ntime. Motivated by many real applications where rewards are binary counts, we \nfocus on dynamic Bernoulli bandits. Standard methods like $\\epsilon$-Greedy and \nUpper Confidence Bound (UCB), which rely on the sample mean estimator, often \nfail to track the changes in underlying reward for dynamic problems. In this \npaper, we overcome the shortcoming of slow response to change by deploying \nadaptive estimation in the standard methods and propose a new family of \nalgorithms, which are adaptive versions of $\\epsilon$-Greedy, UCB, and Thompson \nsampling. These new methods are simple and easy to implement. Moreover, they do \nnot require any prior knowledge about the data, which is important for real \napplications. We examine the new algorithms numerically in different scenarios \nand find out that the results show solid improvements of our algorithms in \ndynamic environments. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449028dc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.03134"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexander Novikov, Mikhail Trofimov, Ivan Oseledets", "title": "Exponential Machines. (arXiv:1605.03795v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1605.03795", "type": "text/html"}], "timestampUsec": "1512969981619065", "comments": [], "summary": {"content": "<p>Modeling interactions between features improves the performance of machine \nlearning solutions in many domains (e.g. recommender systems or sentiment \nanalysis). In this paper, we introduce Exponential Machines (ExM), a predictor \nthat models all interactions of every order. The key idea is to represent an \nexponentially large tensor of parameters in a factorized format called Tensor \nTrain (TT). The Tensor Train format regularizes the model and lets you control \nthe number of underlying parameters. To train the model, we develop a \nstochastic Riemannian optimization procedure, which allows us to fit tensors \nwith 2^160 entries. We show that the model achieves state-of-the-art \nperformance on synthetic data with high-order interactions and that it works on \npar with high-order factorization machines on a recommender system dataset \nMovieLens 100K. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449028e9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1605.03795"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yuxin Chen, Emmanuel Candes", "title": "The Projected Power Method: An Efficient Algorithm for Joint Alignment from Pairwise Differences. (arXiv:1609.05820v3 [cs.IT] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1609.05820", "type": "text/html"}], "timestampUsec": "1512969981619064", "comments": [], "summary": {"content": "<p>Various applications involve assigning discrete label values to a collection \nof objects based on some pairwise noisy data. Due to the discrete---and hence \nnonconvex---structure of the problem, computing the optimal assignment \n(e.g.~maximum likelihood assignment) becomes intractable at first sight. This \npaper makes progress towards efficient computation by focusing on a concrete \njoint alignment problem---that is, the problem of recovering $n$ discrete \nvariables $x_i \\in \\{1,\\cdots, m\\}$, $1\\leq i\\leq n$ given noisy observations \nof their modulo differences $\\{x_i - x_j~\\mathsf{mod}~m\\}$. We propose a \nlow-complexity and model-free procedure, which operates in a lifted space by \nrepresenting distinct label values in orthogonal directions, and which attempts \nto optimize quadratic functions over hypercubes. Starting with a first guess \ncomputed via a spectral method, the algorithm successively refines the iterates \nvia projected power iterations. We prove that for a broad class of statistical \nmodels, the proposed projected power method makes no error---and hence \nconverges to the maximum likelihood estimate---in a suitable regime. Numerical \nexperiments have been carried out on both synthetic and real data to \ndemonstrate the practicality of our algorithm. We expect this algorithmic \nframework to be effective for a broad range of discrete assignment problems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449028f1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1609.05820"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lanlan Liu, Jia Deng", "title": "Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs by Selective Execution. (arXiv:1701.00299v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1701.00299", "type": "text/html"}], "timestampUsec": "1512969981619063", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050eb67a0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050eb67a0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We introduce Dynamic Deep Neural Networks (D2NN), a new type of feed-forward \ndeep neural network that allows selective execution. Given an input, only a \nsubset of D2NN neurons are executed, and the particular subset is determined by \nthe D2NN itself. By pruning unnecessary computation depending on input, D2NNs \nprovide a way to improve computational efficiency. To achieve dynamic selective \nexecution, a D2NN augments a feed-forward deep neural network (directed acyclic \ngraph of differentiable modules) with controller modules. Each controller \nmodule is a sub-network whose output is a decision that controls whether other \nmodules can execute. A D2NN is trained end to end. Both regular and controller \nmodules in a D2NN are learnable and are jointly trained to optimize both \naccuracy and efficiency. Such training is achieved by integrating \nbackpropagation with reinforcement learning. With extensive experiments of \nvarious D2NN architectures on image classification tasks, we demonstrate that \nD2NNs are general and flexible, and can effectively optimize \naccuracy-efficiency trade-offs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003449028f9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1701.00299"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yizhen Wang, Somesh Jha, Kamalika Chaudhuri", "title": "Analyzing the Robustness of Nearest Neighbors to Adversarial Examples. (arXiv:1706.03922v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.03922", "type": "text/html"}], "timestampUsec": "1512969981619062", "comments": [], "summary": {"content": "<p>Motivated by applications such as autonomous vehicles, test-time attacks via \nadversarial examples have received a great deal of recent attention. In this \nsetting, an adversary is capable of making queries to a classifier, and \nperturbs a test example by a small amount in order to force the classifier to \nreport an incorrect label. While a long line of work has explored a number of \nattacks, not many reliable defenses are known, and there is an overall lack of \ngeneral understanding about the foundations of designing machine learning \nalgorithms robust to adversarial examples. \n</p> \n<p>In this paper, we take a step towards addressing this challenging question by \nintroducing a new theoretical framework, analogous to bias-variance theory, \nwhich we can use to tease out the causes of vulnerability. We apply our \nframework to a simple classification algorithm: nearest neighbors, and analyze \nits robustness to adversarial examples. Motivated by our analysis, we propose a \nmodified version of the nearest neighbor algorithm, and demonstrate both \ntheoretically and empirically that it has superior robustness to standard \nnearest neighbors. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902907", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.03922"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Long Chen, Fajie Yuan, Joemon M. Jose, Weinan Zhang", "title": "Improving Negative Sampling for Word Representation using Self-embedded Features. (arXiv:1710.09805v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.09805", "type": "text/html"}], "timestampUsec": "1512969981619061", "comments": [], "summary": {"content": "<p>Although the word-popularity based negative sampler has shown superb \nperformance in the skip-gram model, the theoretical motivation behind \noversampling popular (non-observed) words as negative samples is still not well \nunderstood. In this paper, we start from an investigation of the gradient \nvanishing issue in the skip-gram model without a proper negative sampler. By \nperforming an insightful analysis from the stochastic gradient descent (SGD) \nlearning perspective, we demonstrate that, both theoretically and intuitively, \nnegative samples with larger inner product scores are more informative than \nthose with lower scores for the SGD learner in terms of both convergence rate \nand accuracy. Understanding this, we propose an alternative sampling algorithm \nthat dynamically selects informative negative samples during each SGD update. \nMore importantly, the proposed sampler accounts for multi-dimensional \nself-embedded features during the sampling process, which essentially makes it \nmore effective than the original popularity-based (one-dimensional) sampler. \nEmpirical experiments further verify our observations, and show that our \nfine-grained samplers gain significant improvement over the existing ones \nwithout increasing computational complexity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034490290c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.09805"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Geng Ji, Robert Bamler, Erik B. Sudderth, Stephan Mandt", "title": "Bayesian Paragraph Vectors. (arXiv:1711.03946v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.03946", "type": "text/html"}], "timestampUsec": "1512969981619060", "comments": [], "summary": {"content": "<p>Word2vec (Mikolov et al., 2013) has proven to be successful in natural \nlanguage processing by capturing the semantic relationships between different \nwords. Built on top of single-word embeddings, paragraph vectors (Le and \nMikolov, 2014) find fixed-length representations for pieces of text with \narbitrary lengths, such as documents, paragraphs, and sentences. In this work, \nwe propose a novel interpretation for neural-network-based paragraph vectors by \ndeveloping an unsupervised generative model whose maximum likelihood solution \ncorresponds to traditional paragraph vectors. This probabilistic formulation \nallows us to go beyond point estimates of parameters and to perform Bayesian \nposterior inference. We find that the entropy of paragraph vectors decreases \nwith the length of documents, and that information about posterior uncertainty \nimproves performance in supervised learning tasks such as sentiment analysis \nand paraphrase detection. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902913", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.03946"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rhicheek Patra, Egor Samosvat, Michael Roizner, Andrei Mishchenko", "title": "BoostJet: Towards Combining Statistical Aggregates with Neural Embeddings for Recommendations. (arXiv:1711.05828v2 [cs.IR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05828", "type": "text/html"}], "timestampUsec": "1512969981619059", "comments": [], "summary": {"content": "<p>Recommenders have become widely popular in recent years because of their \nbroader applicability in many e-commerce applications. These applications rely \non recommenders for generating advertisements for various offers or providing \ncontent recommendations. However, the quality of the generated recommendations \ndepends on user features (like demography, temporality), offer features (like \npopularity, price), and user-offer features (like implicit or explicit \nfeedback). Current state-of-the-art recommenders do not explore such diverse \nfeatures concurrently while generating the recommendations. \n</p> \n<p>In this paper, we first introduce the notion of Trackers which enables us to \ncapture the above-mentioned features and thus incorporate users' online \nbehaviour through statistical aggregates of different features (demography, \ntemporality, popularity, price). We also show how to capture offer-to-offer \nrelations, based on their consumption sequence, leveraging neural embeddings \nfor offers in our Offer2Vec algorithm. We then introduce BoostJet, a novel \nrecommender which integrates the Trackers along with the neural embeddings \nusing MatrixNet, an efficient distributed implementation of gradient boosted \ndecision tree, to improve the recommendation quality significantly. We provide \nan in-depth evaluation of BoostJet on Yandex's dataset, collecting online \nbehaviour from tens of millions of online users, to demonstrate the \npracticality of BoostJet in terms of recommendation quality as well as \nscalability. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902917", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05828"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rachid Guerraoui, Erwan Le Merrer, Rhicheek Patra, Jean-Ronan Vigouroux", "title": "Sequences, Items And Latent Links: Recommendation With Consumed Item Packs. (arXiv:1711.06100v2 [cs.IR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.06100", "type": "text/html"}], "timestampUsec": "1512969981619058", "comments": [], "summary": {"content": "<p>Recommenders personalize the web content by typically using collaborative \nfiltering to relate users (or items) based on explicit feedback, e.g., ratings. \nThe difficulty of collecting this feedback has recently motivated to consider \nimplicit feedback (e.g., item consumption along with the corresponding time). \n</p> \n<p>In this paper, we introduce the notion of consumed item pack (CIP) which \nenables to link users (or items) based on their implicit analogous consumption \nbehavior. Our proposal is generic, and we show that it captures three novel \nimplicit recommenders: a user-based (CIP-U), an item-based (CIP-I), and a word \nembedding-based (DEEPCIP), as well as a state-of-the-art technique using \nimplicit feedback (FISM). We show that our recommenders handle incremental \nupdates incorporating freshly consumed items. We demonstrate that all three \nrecommenders provide a recommendation quality that is competitive with \nstate-of-the-art ones, including one incorporating both explicit and implicit \nfeedback. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902938", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.06100"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Haoye Cai, Chunyan Bai, Yu-Wing Tai, Chi-Keung Tang", "title": "Deep Video Generation, Prediction and Completion of Human Action Sequences. (arXiv:1711.08682v3 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08682", "type": "text/html"}], "timestampUsec": "1512969981619057", "comments": [], "summary": {"content": "<p>Current deep learning results on video generation are limited while there are \nonly a few first results on video prediction and no relevant significant \nresults on video completion. This is due to the severe ill-posedness inherent \nin these three problems. In this paper, we focus on human action videos, and \npropose a general, two-stage deep framework to generate human action videos \nwith no constraints or arbitrary number of constraints, which uniformly address \nthe three problems: video generation given no input frames, video prediction \ngiven the first few frames, and video completion given the first and last \nframes. To make the problem tractable, in the first stage we train a deep \ngenerative model that generates a human pose sequence from random noise. In the \nsecond stage, a skeleton-to-image network is trained, which is used to generate \na human action video given the complete human pose sequence generated in the \nfirst stage. By introducing the two-stage strategy, we sidestep the original \nill-posed problems while producing for the first time high-quality video \ngeneration/prediction/completion results of much longer duration. We present \nquantitative and qualitative evaluation to show that our two-stage approach \noutperforms state-of-the-art methods in video generation, prediction and video \ncompletion. Our video result demonstration can be viewed at \nhttps://iamacewhite.github.io/supp/index.html \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512969981619", "annotations": [], "published": 1512969982, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000344902944", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08682"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513315289, "author": "Chen Huang, Chen Kong, Simon Lucey", "title": "CNNs are Globally Optimal Given Multi-Layer Support. (arXiv:1712.02501v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02501", "type": "text/html"}], "timestampUsec": "1512709730983976", "comments": [], "summary": {"content": "<p>Stochastic Gradient Descent (SGD) is the central workhorse for training \nmodern CNNs. Although giving impressive empirical performance it can be slow to \nconverge. In this paper we explore a novel strategy for training a CNN using an \nalternation strategy that offers substantial speedups during training. We make \nthe following contributions: (i) replace the ReLU non-linearity within a CNN \nwith positive hard-thresholding, (ii) reinterpret this non-linearity as a \nbinary state vector making the entire CNN linear if the multi-layer support is \nknown, and (iii) demonstrate that under certain conditions a global optima to \nthe CNN can be found through local descent. We then employ a novel alternation \nstrategy (between weights and support) for CNN training that leads to \nsubstantially faster convergence rates, nice theoretical properties, and \nachieving state of the art results across large scale datasets (e.g. ImageNet) \nas well as other standard benchmarks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7c7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02501"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Federico Raue, Andreas Dengel, Thomas M. Breuel, Marcus Liwicki", "title": "Symbol Grounding Association in Multimodal Sequences with Missing Elements. (arXiv:1511.04401v5 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1511.04401", "type": "text/html"}], "timestampUsec": "1512709730983975", "comments": [], "summary": {"content": "<p>In this paper, we extend a symbolic association framework for being able to \nhandle missing elements in multimodal sequences. The general scope of the work \nis the symbolic associations of object-word mappings as it happens in language \ndevelopment in infants. In other words, two different representations of the \nsame abstract concepts can associate in both directions. This scenario has been \nlong interested in Artificial Intelligence, Psychology, and Neuroscience. In \nthis work, we extend a recent approach for multimodal sequences (visual and \naudio) to also cope with missing elements in one or both modalities. Our method \nuses two parallel Long Short-Term Memories (LSTMs) with a learning rule based \non EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We \npropose to include an extra step for the combination with the max operation for \nexploiting the common elements between both sequences. The motivation behind is \nthat the combination acts as a condition selector for choosing the best \nrepresentation from both LSTMs. We evaluated the proposed extension in the \nfollowing scenarios: missing elements in one modality (visual or audio) and \nmissing elements in both modalities (visual and sound). The performance of our \nextension reaches better results than the original model and similar results to \nindividual LSTM trained in each modality. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7cb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1511.04401"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kieran Greer", "title": "New Ideas for Brain Modelling 3. (arXiv:1612.00369v6 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1612.00369", "type": "text/html"}], "timestampUsec": "1512709730983974", "comments": [], "summary": {"content": "<p>This paper considers a process for the creation and subsequent firing of \nsequences of neuronal patterns, as might be found in the human brain. The scale \nis one of larger patterns emerging from an ensemble mass, possibly through some \ntype of energy equation and a reduction procedure. The links between the \npatterns can be formed naturally, as a residual effect of the pattern creation \nitself. This paper follows-on closely from the earlier research, including two \nearlier papers in the series and uses the ideas of entropy and cohesion. With a \nsmall addition, it is possible to show how the inter-pattern links can be \ndetermined. A new compact Grid form of an earlier Counting Mechanism is also \ndemonstrated. It is possible to explain how a very basic repeating structure \ncan form the arbitrary patterns and activation sequences between them, and a \nkey question of how nodes synchronise may even be answerable. The paper \nfinishes with an implementation architecture, for the realisation and storage \nof knowledge and memory, as part of a general design, based on distributed \nneural components. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7d1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.00369"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andrea Soltoggio, Kenneth O. Stanley, Sebastian Risi", "title": "Born to Learn: the Inspiration, Progress, and Future of Evolved Plastic Artificial Neural Networks. (arXiv:1703.10371v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.10371", "type": "text/html"}], "timestampUsec": "1512709730983973", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050eb6a8b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050eb6a8b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Biological plastic neural networks are systems of extraordinary computational \ncapabilities shaped by evolution, development, and lifetime learning. The \ninterplay of these elements leads to the emergence of adaptive behavior and \nintelligence. Inspired by such intricate natural phenomena, Evolved Plastic \nArtificial Neural Networks (EPANNs) use simulated evolution in-silico to breed \nplastic neural networks with a large variety of dynamics, architectures, and \nplasticity rules: these artificial systems are composed of inputs, outputs, and \nplastic components that change in response to experiences in an environment. \nThese systems may autonomously discover novel adaptive algorithms, and lead to \nhypotheses on the emergence of biological adaptation. EPANNs have seen \nconsiderable progress over the last two decades. Current scientific and \ntechnological advances in artificial neural networks are now setting the \nconditions for radically new approaches and results. In particular, the \nlimitations of hand-designed networks could be overcome by more flexible and \ninnovative solutions. This paper brings together a variety of inspiring ideas \nthat define the field of EPANNs. The main methods and results are reviewed. \nFinally, new opportunities and developments are presented. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7d6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.10371"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jesus Lago, Fjo De Ridder, Peter Vrancx, Bart De Schutter", "title": "Forecasting day-ahead electricity prices in Europe: the importance of considering market integration. (arXiv:1708.07061v3 [q-fin.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.07061", "type": "text/html"}], "timestampUsec": "1512709730983972", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050fb5378\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050fb5378&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Motivated by the increasing integration among electricity markets, in this \npaper we propose two different methods to incorporate market integration in \nelectricity price forecasting and to improve the predictive performance. First, \nwe propose a deep neural network that considers features from connected markets \nto improve the predictive accuracy in a local market. To measure the importance \nof these features, we propose a novel feature selection algorithm that, by \nusing Bayesian optimization and functional analysis of variance, evaluates the \neffect of the features on the algorithm performance. In addition, using market \nintegration, we propose a second model that, by simultaneously predicting \nprices from two markets, improves the forecasting accuracy even further. As a \ncase study, we consider the electricity market in Belgium and the improvements \nin forecasting accuracy when using various French electricity features. We show \nthat the two proposed models lead to improvements that are statistically \nsignificant. Particularly, due to market integration, the predictive accuracy \nis improved from 15.7% to 12.5% sMAPE (symmetric mean absolute percentage \nerror). In addition, we show that the proposed feature selection algorithm is \nable to perform a correct assessment, i.e. to discard the irrelevant features. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7db", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.07061"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Robin Tibor Schirrmeister, Lukas Gemein, Katharina Eggensperger, Frank Hutter, Tonio Ball", "title": "Deep learning with convolutional neural networks for decoding and visualization of EEG pathology. (arXiv:1708.08012v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.08012", "type": "text/html"}], "timestampUsec": "1512709730983971", "comments": [], "summary": {"content": "<p>We apply convolutional neural networks (ConvNets) to the task of \ndistinguishing pathological from normal EEG recordings in the Temple University \nHospital EEG Abnormal Corpus. We use two basic, shallow and deep ConvNet \narchitectures recently shown to decode task-related information from EEG at \nleast as well as established algorithms designed for this purpose. In decoding \nEEG pathology, both ConvNets reached substantially better accuracies (about 6% \nbetter, ~85% vs. ~79%) than the only published result for this dataset, and \nwere still better when using only 1 minute of each recording for training and \nonly six seconds of each recording for testing. We used automated methods to \noptimize architectural hyperparameters and found intriguingly different ConvNet \narchitectures, e.g., with max pooling as the only nonlinearity. Visualizations \nof the ConvNet decoding behavior showed that they used spectral power changes \nin the delta (0-4 Hz) and theta (4-8 Hz) frequency range, possibly alongside \nother features, consistent with expectations derived from spectral analysis of \nthe EEG data and from the textual medical reports. Analysis of the textual \nmedical reports also highlighted the potential for accuracy increases by \nintegrating contextual information, such as the age of subjects. In summary, \nthe ConvNets and visualization techniques used in this study constitute a next \nstep towards clinically useful automated EEG diagnosis and establish a new \nbaseline for future work on this topic. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7e1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.08012"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Juncai Xu, Zhenzhong Shen, Qingwen Ren, Xin Xie, Zhengyu Yang", "title": "Geometric Semantic Genetic Programming Algorithm and Slump Prediction. (arXiv:1709.06114v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.06114", "type": "text/html"}], "timestampUsec": "1512709730983970", "comments": [], "summary": {"content": "<p>Research on the performance of recycled concrete as building material in the \ncurrent world is an important subject. Given the complex composition of \nrecycled concrete, conventional methods for forecasting slump scarcely obtain \nsatisfactory results. Based on theory of nonlinear prediction method, we \npropose a recycled concrete slump prediction model based on geometric semantic \ngenetic programming (GSGP) and combined it with recycled concrete features. \nTests show that the model can accurately predict the recycled concrete slump by \nusing the established prediction model to calculate the recycled concrete slump \nwith different mixing ratios in practical projects and by comparing the \npredicted values with the experimental values. By comparing the model with \nseveral other nonlinear prediction models, we can conclude that GSGP has higher \naccuracy and reliability than conventional methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7e5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.06114"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, Jaap Kamps", "title": "Avoiding Your Teacher's Mistakes: Training Neural Networks with Controlled Weak Supervision. (arXiv:1711.00313v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.00313", "type": "text/html"}], "timestampUsec": "1512709730983969", "comments": [], "summary": {"content": "<p>Training deep neural networks requires massive amounts of training data, but \nfor many tasks only limited labeled data is available. This makes weak \nsupervision attractive, using weak or noisy signals like the output of \nheuristic methods or user click-through data for training. In a semi-supervised \nsetting, we can use a large set of data with weak labels to pretrain a neural \nnetwork and then fine-tune the parameters with a small amount of data with true \nlabels. This feels intuitively sub-optimal as these two independent stages \nleave the model unaware about the varying label quality. What if we could \nsomehow inform the model about the label quality? In this paper, we propose a \nsemi-supervised learning method where we train two neural networks in a \nmulti-task fashion: a \"target network\" and a \"confidence network\". The target \nnetwork is optimized to perform a given task and is trained using a large set \nof unlabeled data that are weakly annotated. We propose to weight the gradient \nupdates to the target network using the scores provided by the second \nconfidence network, which is trained on a small amount of supervised data. Thus \nwe avoid that the weight updates computed from noisy labels harm the quality of \nthe target network model. We evaluate our learning strategy on two different \ntasks: document ranking and sentiment classification. The results demonstrate \nthat our approach not only enhances the performance compared to the baselines \nbut also speeds up the learning process from weak labels. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7eb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.00313"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jiajun Lu, Hussein Sibai, Evan Fabry", "title": "Adversarial Examples that Fool Detectors. (arXiv:1712.02494v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.02494", "type": "text/html"}], "timestampUsec": "1512709730983967", "comments": [], "summary": {"content": "<p>An adversarial example is an example that has been adjusted to produce a \nwrong label when presented to a system at test time. To date, adversarial \nexample constructions have been demonstrated for classifiers, but not for \ndetectors. If adversarial examples that could fool a detector exist, they could \nbe used to (for example) maliciously create security hazards on roads populated \nwith smart vehicles. In this paper, we demonstrate a construction that \nsuccessfully fools two standard detectors, Faster RCNN and YOLO. The existence \nof such examples is surprising, as attacking a classifier is very different \nfrom attacking a detector, and that the structure of detectors - which must \nsearch for their own bounding box, and which cannot estimate that box very \naccurately - makes it quite likely that adversarial patterns are strongly \ndisrupted. We show that our construction produces adversarial examples that \ngeneralize well across sequences digitally, even though large perturbations are \nneeded. We also show that our construction yields physical objects that are \nadversarial. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7f2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02494"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas", "title": "ChemNet: A Transferable and Generalizable Deep Neural Network for Small-Molecule Property Prediction. (arXiv:1712.02734v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02734", "type": "text/html"}], "timestampUsec": "1512709730983966", "comments": [], "summary": {"content": "<p>With access to large datasets, deep neural networks (DNN) have achieved \nhuman-level accuracy in image and speech recognition tasks. However, in \nchemistry, availability of large standardized and labelled datasets is scarce, \nand many chemical properties of research interest, chemical data is inherently \nsmall and fragmented. In this work, we explore transfer learning techniques in \nconjunction with the existing Chemception CNN model, to create a transferable \nand generalizable deep neural network for small-molecule property prediction. \nOur latest model, ChemNet learns in a semi-supervised manner from inexpensive \nlabels computed from the ChEMBL database. When fine-tuned to the Tox21, HIV and \nFreeSolv dataset, which are 3 separate chemical properties that ChemNet was not \noriginally trained on, we demonstrate that ChemNet exceeds the performance of \nexisting Chemception models and other contemporary DNN models. Furthermore, as \nChemNet has been pre-trained on a large diverse chemical database, it can be \nused as a general-purpose plug-and-play deep neural network for the prediction \nof novel small-molecule chemical properties. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c7fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02734"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lucas Bechberger, Kai-Uwe K&#xfc;hnberger", "title": "Measuring Relations Between Concepts In Conceptual Spaces. (arXiv:1707.02292v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.02292", "type": "text/html"}], "timestampUsec": "1512709730983965", "comments": [], "summary": {"content": "<p>The highly influential framework of conceptual spaces provides a geometric \nway of representing knowledge. Instances are represented by points in a \nhigh-dimensional space and concepts are represented by regions in this space. \nOur recent mathematical formalization of this framework is capable of \nrepresenting correlations between different domains in a geometric way. In this \npaper, we extend our formalization by providing quantitative mathematical \ndefinitions for the notions of concept size, subsethood, implication, \nsimilarity, and betweenness. This considerably increases the representational \npower of our formalization by introducing measurable ways of describing \nrelations between concepts. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c802", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.02292"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Dianmu Zhang, Blake Hannaford", "title": "IKBT: solving closed-form Inverse Kinematics with Behavior Tree. (arXiv:1711.05412v3 [cs.RO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05412", "type": "text/html"}], "timestampUsec": "1512709730983964", "comments": [], "summary": {"content": "<p>Serial robot arms have complicated kinematic equations which must be solved \nto write effective arm planning and control software (the Inverse Kinematics \nProblem). Existing software packages for inverse kinematics often rely on \nnumerical methods which have significant shortcomings. Here we report a new \nsymbolic inverse kinematics solver which overcomes the limitations of numerical \nmethods, and the shortcomings of previous symbolic software packages. We \nintegrate Behavior Trees, an execution planning framework previously used for \ncontrolling intelligent robot behavior, to organize the equation solving \nprocess, and a modular architecture for each solution technique. The system \nsuccessfully solved, generated a LaTex report, and generated a Python code \ntemplate for 18 out of 19 example robots of 4-6 DOF. The system is readily \nextensible, maintainable, and multi-platform with few dependencies. The \ncomplete package is available with a Modified BSD license on Github. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c807", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05412"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Georgios Theocharous, Zheng Wen, Yasin Abbasi-Yadkori, Nikos Vlassis", "title": "Posterior Sampling for Large Scale Reinforcement Learning. (arXiv:1711.07979v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.07979", "type": "text/html"}], "timestampUsec": "1512709730983963", "comments": [], "summary": {"content": "<p>Posterior sampling for reinforcement learning (PSRL) is a popular algorithm \nfor learning to control an unknown Markov decision process (MDP). PSRL \nmaintains a distribution over MDP parameters and in an episodic fashion samples \nMDP parameters, computes the optimal policy for them and executes it. A special \ncase of PSRL is where at the end of each episode the MDP resets to the initial \nstate distribution. Extensions of this idea to general MDPs without state \nresetting has so far produced non-practical algorithms and in some cases buggy \ntheoretical analysis. This is due to the difficulty of analyzing regret under \nepisode switching schedules that depend on random variables of the true \nunderlying model. We propose a solution to this problem that involves using a \ndeterministic, model-independent episode switching schedule, and establish a \nBayes regret bound under mild assumptions. Our algorithm termed deterministic \nschedule PSRL (DS-PSRL) is efficient in terms of time, sample, and space \ncomplexity. Our result is more generally applicable to continuous state action \nproblems. We demonstrate how this algorithm is well suited for sequential \nrecommendation problems such as points of interest (POI). We derive a general \nprocedure for parameterizing the underlying MDPs, to create action condition \ndynamics from passive data, that do not contain actions. We prove that such \nparameterization satisfies the assumptions of our analysis. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c812", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.07979"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1514178135, "author": "Lirong Xue, Samory Kpotufe", "title": "Achieving the time of $1$-NN, but the accuracy of $k$-NN. (arXiv:1712.02369v2 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02369", "type": "text/html"}], "timestampUsec": "1512709730983961", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050fb5736\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050fb5736&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a simple approach which, given distributed computing resources, \ncan nearly achieve the accuracy of $k$-NN prediction, while matching (or \nimproving) the faster prediction time of $1$-NN. The approach consists of \naggregating denoised $1$-NN predictors over a small number of distributed \nsubsamples. We show, both theoretically and experimentally, that small \nsubsample sizes suffice to attain similar performance as $k$-NN, without \nsacrificing the computational efficiency of $1$-NN. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1514178134, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c815", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02369"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Guodong Zhang, Shengyang Sun, David Duvenaud, Roger Grosse", "title": "Noisy Natural Gradient as Variational Inference. (arXiv:1712.02390v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.02390", "type": "text/html"}], "timestampUsec": "1512709730983960", "comments": [], "summary": {"content": "<p>Combining the flexibility of deep learning with Bayesian uncertainty \nestimation has long been a goal in our field, and many modern approaches are \nbased on variational Bayes. Unfortunately, one is forced to choose between \noverly simplistic variational families (e.g. fully factorized) or expensive and \ncomplicated inference procedures. We show that natural gradient ascent with \nadaptive weight noise can be interpreted as fitting a variational posterior to \nmaximize the evidence lower bound (ELBO). This insight allows us to train full \ncovariance, fully factorized, and matrix variate Gaussian variational \nposteriors using noisy versions of natural gradient, Adam, and K-FAC, \nrespectively. On standard regression benchmarks, our noisy K-FAC algorithm \nmakes better predictions and matches HMC's predictive variances better than \nexisting methods. Its improved uncertainty estimates lead to more efficient \nexploration in the settings of active learning and intrinsic motivation for \nreinforcement learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c81e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02390"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Guo Yu, Jacob Bien", "title": "Estimating the error variance in a high-dimensional linear model. (arXiv:1712.02412v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.02412", "type": "text/html"}], "timestampUsec": "1512709730983959", "comments": [], "summary": {"content": "<p>The lasso has been studied extensively as a tool for estimating the \ncoefficient vector in the high-dimensional linear model; however, considerably \nless is known about estimating the error variance. Indeed, most well-known \ntheoretical properties of the lasso, including recent advances in selective \ninference with the lasso, are established under the assumption that the \nunderlying error variance is known. Yet the error variance in practice is, of \ncourse, unknown. In this paper, we propose the natural lasso estimator for the \nerror variance, which maximizes a penalized likelihood objective. A key aspect \nof the natural lasso is that the likelihood is expressed in terms of the \nnatural parameterization of the multiparameter exponential family of a Gaussian \nwith unknown mean and variance. The result is a remarkably simple estimator \nwith provably good performance in terms of mean squared error. These \ntheoretical results do not require placing any assumptions on the design matrix \nor the true regression coefficients. We also propose a companion estimator, \ncalled the organic lasso, which theoretically does not require tuning of the \nregularization parameter. Both estimators do well compared to preexisting \nmethods, especially in settings where successful recovery of the true support \nof the coefficient vector is hard. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c824", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02412"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lorenzo Boninsegna, Feliks N&#xfc;ske, Cecilia Clementi", "title": "Sparse learning of stochastic dynamic equations. (arXiv:1712.02432v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02432", "type": "text/html"}], "timestampUsec": "1512709730983958", "comments": [], "summary": {"content": "<p>With the rapid increase of available data for complex systems, there is great \ninterest in the extraction of physically relevant information from massive \ndatasets. Recently, a framework called Sparse Identification of Nonlinear \nDynamics (SINDy) has been introduced to identify the governing equations of \ndynamical systems from simulation data. In this study, we extend SINDy to \nstochastic dynamical systems, which are frequently used to model biophysical \nprocesses. We prove the asymptotic correctness of stochastics SINDy in the \ninfinite data limit, both in the original and projected variables. We discuss \nalgorithms to solve the sparse regression problem arising from the practical \nimplementation of SINDy, and show that cross validation is an essential tool to \ndetermine the right level of sparsity. We demonstrate the proposed methodology \non two test systems, namely, the diffusion in a one-dimensional potential, and \nthe projected dynamics of a two-dimensional diffusion process. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c829", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02432"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Farzaneh S. Fard, Thomas P. Trappenberg", "title": "A Novel Model for Arbitration between Planning and Habitual Control Systems. (arXiv:1712.02441v1 [cs.SY])", "alternate": [{"href": "http://arxiv.org/abs/1712.02441", "type": "text/html"}], "timestampUsec": "1512709730983957", "comments": [], "summary": {"content": "<p>It is well established that humans decision making and instrumental control \nuses multiple systems, some which use habitual action selection and some which \nrequire deliberate planning. Deliberate planning systems use predictions of \naction-outcomes using an internal model of the agent's environment, while \nhabitual action selection systems learn to automate by repeating previously \nrewarded actions. Habitual control is computationally efficient but may be \ninflexible in changing environments. Conversely, deliberate planning may be \ncomputationally expensive, but flexible in dynamic environments. This paper \nproposes a general architecture comprising both control paradigms by \nintroducing an arbitrator that controls which subsystem is used at any time. \nThis system is implemented for a target-reaching task with a simulated \ntwo-joint robotic arm that comprises a supervised internal model and deep \nreinforcement learning. Through permutation of target-reaching conditions, we \ndemonstrate that the proposed is capable of rapidly learning kinematics of the \nsystem without a priori knowledge, and is robust to (A) changing environmental \nreward and kinematics, and (B) occluded vision. The arbitrator model is \ncompared to exclusive deliberate planning with the internal model and exclusive \nhabitual control instances of the model. The results show how such a model can \nharness the benefits of both systems, using fast decisions in reliable \ncircumstances while optimizing performance in changing environments. In \naddition, the proposed model learns very fast. Finally, the system which \nincludes internal models is able to reach the target under the visual \nocclusion, while the pure habitual system is unable to operate sufficiently \nunder such conditions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c82e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02441"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yunpeng Li, Ivan Kiskin, Davide Zilli, Marianne Sinka, Henry Chan, Kathy Willis, Stephen Roberts", "title": "Cost-sensitive detection with variational autoencoders for environmental acoustic sensing. (arXiv:1712.02488v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02488", "type": "text/html"}], "timestampUsec": "1512709730983956", "comments": [], "summary": {"content": "<p>Environmental acoustic sensing involves the retrieval and processing of audio \nsignals to better understand our surroundings. While large-scale acoustic data \nmake manual analysis infeasible, they provide a suitable playground for machine \nlearning approaches. Most existing machine learning techniques developed for \nenvironmental acoustic sensing do not provide flexible control of the trade-off \nbetween the false positive rate and the false negative rate. This paper \npresents a cost-sensitive classification paradigm, in which the \nhyper-parameters of classifiers and the structure of variational autoencoders \nare selected in a principled Neyman-Pearson framework. We examine the \nperformance of the proposed approach using a dataset from the HumBug project \nwhich aims to detect the presence of mosquitoes using sound collected by simple \nembedded devices. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c834", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02488"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lucas Roberts, Leo Razoumov, Lin Su, Yuyang Wang", "title": "Gini-regularized Optimal Transport with an Application to Spatio-Temporal Forecasting. (arXiv:1712.02512v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02512", "type": "text/html"}], "timestampUsec": "1512709730983955", "comments": [], "summary": {"content": "<p>Rapidly growing product lines and services require a finer-granularity \nforecast that considers geographic locales. However the open question remains, \nhow to assess the quality of a spatio-temporal forecast? In this manuscript we \nintroduce a metric to evaluate spatio-temporal forecasts. This metric is based \non an Opti- mal Transport (OT) problem. The metric we propose is a constrained \nOT objec- tive function using the Gini impurity function as a regularizer. We \ndemonstrate through computer experiments both the qualitative and the \nquantitative charac- teristics of the Gini regularized OT problem. Moreover, we \nshow that the Gini regularized OT problem converges to the classical OT \nproblem, when the Gini regularized problem is considered as a function of \n{\\lambda}, the regularization parame-ter. The convergence to the classical OT \nsolution is faster than the state-of-the-art Entropic-regularized OT[Cuturi, \n2013] and results in a numerically more stable algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c83c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02512"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513315291, "author": "Fengshuo Zhang, Chao Gao", "title": "Convergence Rates of Variational Posterior Distributions. (arXiv:1712.02519v2 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02519", "type": "text/html"}], "timestampUsec": "1512709730983954", "comments": [], "summary": {"content": "<p>We study convergence rates of variational posterior distributions for \nnonparametric and high-dimensional inference. We formulate general conditions \non prior, likelihood, and variational class that characterize the convergence \nrates. Under similar \"prior mass and testing\" conditions considered in the \nliterature, the rate is found to be the sum of two terms. The first term stands \nfor the convergence rate of the true posterior distribution, and the second \nterm is contributed by the variational approximation error. For a class of \npriors that admit the structure of a mixture of product measures, we propose a \nnovel prior mass condition, under which the variational approximation error of \nthe generalized mean-field class is dominated by convergence rate of the true \nposterior. We demonstrate the applicability of our general results for various \nmodels, prior distributions and variational classes by deriving convergence \nrates of the corresponding variational posteriors. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c845", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02519"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jianqiao Wangni, Jingwei Zhuo, Jun Zhu", "title": "Learning Random Fourier Features by Hybrid Constrained Optimization. (arXiv:1712.02527v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02527", "type": "text/html"}], "timestampUsec": "1512709730983953", "comments": [], "summary": {"content": "<p>The kernel embedding algorithm is an important component for adapting kernel \nmethods to large datasets. Since the algorithm consumes a major computation \ncost in the testing phase, we propose a novel teacher-learner framework of \nlearning computation-efficient kernel embeddings from specific data. In the \nframework, the high-precision embeddings (teacher) transfer the data \ninformation to the computation-efficient kernel embeddings (learner). We \njointly select informative embedding functions and pursue an orthogonal \ntransformation between two embeddings. We propose a novel approach of \nconstrained variational expectation maximization (CVEM), where the alternate \ndirection method of multiplier (ADMM) is applied over a nonconvex domain in the \nmaximization step. We also propose two specific formulations based on the \nprevalent Random Fourier Feature (RFF), the masked and blocked version of \nComputation-Efficient RFF (CERF), by imposing a random binary mask or a block \nstructure on the transformation matrix. By empirical studies of several \napplications on different real-world datasets, we demonstrate that the CERF \nsignificantly improves the performance of kernel methods upon the RFF, under \ncertain arithmetic operation requirements, and suitable for structured matrix \nmultiplication in Fastfood type algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c84b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02527"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Carles Roger Riera Molina, Oriol Pujol Vila", "title": "Solving internal covariate shift in deep learning with linked neurons. (arXiv:1712.02609v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02609", "type": "text/html"}], "timestampUsec": "1512709730983952", "comments": [], "summary": {"content": "<p>This work proposes a novel solution to the problem of internal covariate \nshift and dying neurons using the concept of linked neurons. We define the \nneuron linkage in terms of two constraints: first, all neuron activations in \nthe linkage must have the same operating point. That is to say, all of them \nshare input weights. Secondly, a set of neurons is linked if and only if there \nis at least one member of the linkage that has a non-zero gradient in regard to \nthe input of the activation function. This means that for any input in the \nactivation function, there is at least one member of the linkage that operates \nin a non-flat and non-zero area. This simple change has profound implications \nin the network learning dynamics. In this article we explore the consequences \nof this proposal and show that by using this kind of units, internal covariate \nshift is implicitly solved. As a result of this, the use of linked neurons \nallows to train arbitrarily large networks without any architectural or \nalgorithmic trick, effectively removing the need of using re-normalization \nschemes such as Batch Normalization, which leads to halving the required \ntraining time. It also solves the problem of the need for standarized input \ndata. Results show that the units using the linkage not only do effectively \nsolve the aforementioned problems, but are also a competitive alternative with \nrespect to state-of-the-art with very promising results. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c853", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02609"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513247526, "author": "Beyza Ermis, Ali Taylan Cemgil", "title": "Differentially Private Variational Dropout. (arXiv:1712.02629v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02629", "type": "text/html"}], "timestampUsec": "1512709730983951", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45050fb5af8\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45050fb5af8&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep neural networks with their large number of parameters are highly \nflexible learning systems. The high flexibility in such networks brings with \nsome serious problems such as overfitting, and regularization is used to \naddress this problem. A currently popular and effective regularization \ntechnique for controlling the overfitting is dropout. Often, large data \ncollections required for neural networks contain sensitive information such as \nthe medical histories of patients, and the privacy of the training data should \nbe protected. In this paper, we modify the recently proposed variational \ndropout technique which provided an elegant Bayesian interpretation to dropout, \nand show that the intrinsic noise in the variational dropout can be exploited \nto obtain a degree of differential privacy. The iterative nature of training \nneural networks presents a challenge for privacy-preserving estimation since \nmultiple iterations increase the amount of noise added. We overcome this by \nusing a relaxed notion of differential privacy, called concentrated \ndifferential privacy, which provides tighter estimates on the overall privacy \nloss. We demonstrate the accuracy of our privacy-preserving variational dropout \nalgorithm on benchmark datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1513247526, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c85a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02629"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alain Virouleau, Agathe Guilloux, St&#xe9;phane Ga&#xef;ffas, Malgorzata Bogdan", "title": "High-dimensional robust regression and outliers detection with SLOPE. (arXiv:1712.02640v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02640", "type": "text/html"}], "timestampUsec": "1512709730983950", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450510c117c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450510c117c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The problems of outliers detection and robust regression in a \nhigh-dimensional setting are fundamental in statistics, and have numerous \napplications. Following a recent set of works providing methods for \nsimultaneous robust regression and outliers detection, we consider in this \npaper a model of linear regression with individual intercepts, in a \nhigh-dimensional setting. We introduce a new procedure for simultaneous \nestimation of the linear regression coefficients and intercepts, using two \ndedicated sorted-$\\ell_1$ penalizations, also called SLOPE. We develop a \ncomplete theory for this problem: first, we provide sharp upper bounds on the \nstatistical estimation error of both the vector of individual intercepts and \nregression coefficients. Second, we give an asymptotic control on the False \nDiscovery Rate (FDR) and statistical power for support selection of the \nindividual intercepts. As a consequence, this paper is the first to introduce a \nprocedure with guaranteed FDR and statistical power control for outliers \ndetection under the mean-shift model. Numerical illustrations, with a \ncomparison to recent alternative approaches, are provided on both simulated and \nseveral real-world datasets. Experiments are conducted using an open-source \nsoftware written in Python and C++. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c85f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02640"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ga&#xeb;lle Loosli, Hattoibe Aboubacar", "title": "Using SVDD in SimpleMKL for 3D-Shapes Filtering. (arXiv:1712.02658v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02658", "type": "text/html"}], "timestampUsec": "1512709730983949", "comments": [], "summary": {"content": "<p>This paper proposes the adaptation of Support Vector Data Description (SVDD) \nto the multiple kernel case (MK-SVDD), based on SimpleMKL. It also introduces a \nvariant called Slim-MK-SVDD that is able to produce a tighter frontier around \nthe data. For the sake of comparison, the equivalent methods are also developed \nfor One-Class SVM, known to be very similar to SVDD for certain shapes of \nkernels. \n</p> \n<p>Those algorithms are illustrated in the context of 3D-shapes filtering and \noutliers detection. For the 3D-shapes problem, the objective is to be able to \nselect a sub-category of 3D-shapes, each sub-category being learned with our \nalgorithm in order to create a filter. For outliers detection, we apply the \nproposed algorithms for unsupervised outliers detection as well as for the \nsupervised case. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c864", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02658"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andreas Svensson, Dave Zachariah, Thomas B. Sch&#xf6;n", "title": "Is My Model Flexible Enough? Information-Theoretic Model Check. (arXiv:1712.02675v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02675", "type": "text/html"}], "timestampUsec": "1512709730983948", "comments": [], "summary": {"content": "<p>The choice of model class is fundamental in statistical learning and system \nidentification, no matter whether the class is derived from physical principles \nor is a generic black-box. We develop a method to evaluate the specified model \nclass by assessing its capability of reproducing data that is similar to the \nobserved data record. This model check is based on the information-theoretic \nproperties of models viewed as data generators and is applicable to e.g. \nsequential data and nonlinear dynamical models. The method can be understood as \na specific two-sided posterior predictive test. We apply the \ninformation-theoretic model check to both synthetic and real data and compare \nit with a classical whiteness test. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c86d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02675"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chia-Yu Chen, Jungwook Choi, Daniel Brand, Ankur Agrawal, Wei Zhang, Kailash Gopalakrishnan", "title": "AdaComp : Adaptive Residual Gradient Compression for Data-Parallel Distributed Training. (arXiv:1712.02679v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.02679", "type": "text/html"}], "timestampUsec": "1512709730983947", "comments": [], "summary": {"content": "<p>Highly distributed training of Deep Neural Networks (DNNs) on future compute \nplatforms (offering 100 of TeraOps/s of computational capacity) is expected to \nbe severely communication constrained. To overcome this limitation, new \ngradient compression techniques are needed that are computationally friendly, \napplicable to a wide variety of layers seen in Deep Neural Networks and \nadaptable to variations in network architectures as well as their \nhyper-parameters. In this paper we introduce a novel technique - the Adaptive \nResidual Gradient Compression (AdaComp) scheme. AdaComp is based on localized \nselection of gradient residues and automatically tunes the compression rate \ndepending on local activity. We show excellent results on a wide spectrum of \nstate of the art Deep Learning models in multiple domains (vision, speech, \nlanguage), datasets (MNIST, CIFAR10, ImageNet, BN50, Shakespeare), optimizers \n(SGD with momentum, Adam) and network parameters (number of learners, \nminibatch-size etc.). Exploiting both sparsity and quantization, we demonstrate \nend-to-end compression rates of ~200X for fully-connected and recurrent layers, \nand ~40X for convolutional layers, without any noticeable degradation in model \naccuracies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c873", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02679"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Thomas Hehn, Fred A. Hamprecht", "title": "End-to-end Learning of Deterministic Decision Trees. (arXiv:1712.02743v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02743", "type": "text/html"}], "timestampUsec": "1512709730983946", "comments": [], "summary": {"content": "<p>Conventional decision trees have a number of favorable properties, including \ninterpretability, a small computational footprint and the ability to learn from \nlittle training data. However, they lack a key quality that has helped fuel the \ndeep learning revolution: that of being end-to-end trainable, and to learn from \nscratch those features that best allow to solve a given supervised learning \nproblem. Recent work (Kontschieder 2015) has addressed this deficit, but at the \ncost of losing a main attractive trait of decision trees: the fact that each \nsample is routed along a small subset of tree nodes only. We here propose a \nmodel and Expectation-Maximization training scheme for decision trees that are \nfully probabilistic at train time, but after a deterministic annealing process \nbecome deterministic at test time. We also analyze the learned oblique split \nparameters on image datasets and show that Neural Networks can be trained at \neach split node. In summary, we present the first end-to-end learning scheme \nfor deterministic decision trees and present results on par with or superior to \npublished standard oblique decision tree algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c879", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02743"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Abdolreza Mohammadi, Ernst C. Wit", "title": "BDgraph: An R Package for Bayesian Structure Learning in Graphical Models. (arXiv:1501.05108v5 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1501.05108", "type": "text/html"}], "timestampUsec": "1512709730983945", "comments": [], "summary": {"content": "<p>Graphical models provide powerful tools to uncover complicated patterns in \nmultivariate data and are commonly used in Bayesian statistics and machine \nlearning. In this paper, we introduce an R package BDgraph which performs \nBayesian structure learning for general undirected graphical models with either \ncontinuous or discrete variables. The package efficiently implements recent \nimprovements in the Bayesian literature. To speed up computations, the \ncomputationally intensive tasks have been implemented in C++ and interfaced \nwith R. In addition, the package contains several functions for simulation and \nvisualization, as well as two multivariate datasets taken from the literature \nand are used to describe the package capabilities. The paper includes a brief \noverview of the statistical methods which have been implemented in the package. \nThe main body of the paper explains how to use the package. Furthermore, we \nillustrate the package's functionality in both real and artificial examples, as \nwell as in an extensive simulation study. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c881", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1501.05108"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Peter D. Gr&#xfc;nwald, Nishant A. Mehta", "title": "Fast Rates for General Unbounded Loss Functions: from ERM to Generalized Bayes. (arXiv:1605.00252v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1605.00252", "type": "text/html"}], "timestampUsec": "1512709730983944", "comments": [], "summary": {"content": "<p>We present new excess risk bounds for general unbounded loss functions \nincluding log loss and squared loss, where the distribution of the losses may \nbe heavy-tailed. The bounds hold for general estimators, but they are optimized \nwhen applied to $\\eta$-generalized Bayesian, MDL, and ERM estimators. When \napplied with log loss, the bounds imply convergence rates for generalized \nBayesian inference under misspecification in terms of a generalization of the \nHellinger metric as long as the learning rate $\\eta$ is set correctly. For \ngeneral loss functions, our bounds rely on two separate conditions: the \n$v$-GRIP (generalized reversed information projection) conditions, which \ncontrol the lower tail of the excess loss; and the newly introduced witness \ncondition, which controls the upper tail. The parameter $v$ in the $v$-GRIP \nconditions determines the achievable rate and is akin to the exponent in the \nwell-known Tsybakov margin condition and the Bernstein condition for bounded \nlosses, which the $v$-GRIP conditions generalize; favorable $v$ in combination \nwith small model complexity leads to $\\tilde{O}(1/n)$ rates. The witness \ncondition allows us to connect the excess risk to an 'annealed' version \nthereof, by which we generalize several previous results connecting Hellinger \nand R\\'enyi divergence to KL divergence. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c887", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1605.00252"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Luca Ambrogioni, Eric Maris", "title": "Complex-valued Gaussian Process Regression for Time Series Analysis. (arXiv:1611.10073v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.10073", "type": "text/html"}], "timestampUsec": "1512709730983943", "comments": [], "summary": {"content": "<p>The construction of synthetic complex-valued signals from real-valued \nobservations is an important step in many time series analysis techniques. The \nmost widely used approach is based on the Hilbert transform, which maps the \nreal-valued signal into its quadrature component. In this paper, we define a \nprobabilistic generalization of this approach. We model the observable \nreal-valued signal as the real part of a latent complex-valued Gaussian \nprocess. In order to obtain the appropriate statistical relationship between \nits real and imaginary parts, we define two new classes of complex-valued \ncovariance functions. Through an analysis of simulated chirplets and stochastic \noscillations, we show that the resulting Gaussian process complex-valued signal \nprovides a better estimate of the instantaneous amplitude and frequency than \nthe established approaches. Furthermore, the complex-valued Gaussian process \nregression allows to incorporate prior information about the structure in \nsignal and noise and thereby to tailor the analysis to the features of the \nsignal. As a example, we analyze the non-stationary dynamics of brain \noscillations in the alpha band, as measured using magneto-encephalography. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c88b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.10073"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andee Kaplan, Daniel Nordman, Stephen Vardeman", "title": "Properties and Bayesian fitting of restricted Boltzmann machines. (arXiv:1612.01158v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1612.01158", "type": "text/html"}], "timestampUsec": "1512709730983942", "comments": [], "summary": {"content": "<p>A restricted Boltzmann machine (RBM) is an undirected graphical model \nconstructed for discrete or continuous random variables, with two layers, one \nhidden and one visible, and no conditional dependency within a layer. In recent \nyears, RBMs have risen to prominence due to their connection to deep learning. \nBy treating a hidden layer of one RBM as the visible layer in a second RBM, a \ndeep architecture can be created. RBMs are thought to thereby have the ability \nto encode very complex and rich structures in data, making them attractive for \nsupervised learning. However, the generative behavior of RBMs is largely \nunexplored. In this paper, we discuss the relationship between RBM parameter \nspecification in the binary case and model properties such as degeneracy, \ninstability and uninterpretability. We also describe the difficulties that \narise in likelihood-based and Bayes fitting of such (highly flexible) models, \nespecially as Gibbs sampling (quasi-Bayes) methods are often advocated for the \nRBM model structure. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c896", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.01158"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Martin Arjovsky, Soumith Chintala, L&#xe9;on Bottou", "title": "Wasserstein GAN. (arXiv:1701.07875v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1701.07875", "type": "text/html"}], "timestampUsec": "1512709730983941", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450510c1462\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450510c1462&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We introduce a new algorithm named WGAN, an alternative to traditional GAN \ntraining. In this new model, we show that we can improve the stability of \nlearning, get rid of problems like mode collapse, and provide meaningful \nlearning curves useful for debugging and hyperparameter searches. Furthermore, \nwe show that the corresponding optimization problem is sound, and provide \nextensive theoretical work highlighting the deep connections to other distances \nbetween distributions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c8a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1701.07875"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Luca Ambrogioni, Eric Maris", "title": "Integral Transforms from Finite Data: An Application of Gaussian Process Regression to Fourier Analysis. (arXiv:1704.02828v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.02828", "type": "text/html"}], "timestampUsec": "1512709730983940", "comments": [], "summary": {"content": "<p>Computing accurate estimates of the Fourier transform of analog signals from \ndiscrete data points is important in many fields of science and engineering. \nThe conventional approach of performing the discrete Fourier transform of the \ndata implicitly assumes periodicity and bandlimitedness of the signal. In this \npaper, we use Gaussian process regression to estimate the Fourier transform (or \nany other integral transform) without making these assumptions. This is \npossible because the posterior expectation of Gaussian process regression maps \na finite set of samples to a function defined on the whole real line, expressed \nas a linear combination of covariance functions. We estimate the covariance \nfunction from the data using an appropriately designed gradient ascent method \nthat constrains the solution to a linear combination of tractable kernel \nfunctions. This procedure results in a posterior expectation of the analog \nsignal whose Fourier transform can be obtained analytically by exploiting \nlinearity. Our simulations show that the new method leads to sharper and more \nprecise estimation of the spectral density both in noise-free and \nnoise-corrupted signals. We further validate the method in two real-world \napplications: the analysis of the yearly fluctuation in atmospheric CO2 level \nand the analysis of the spectral content of brain signals. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c8b8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.02828"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yossi Arjevani", "title": "Limitations on Variance-Reduction and Acceleration Schemes for Finite Sum Optimization. (arXiv:1706.01686v2 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.01686", "type": "text/html"}], "timestampUsec": "1512709730983939", "comments": [], "summary": {"content": "<p>We study the conditions under which one is able to efficiently apply \nvariance-reduction and acceleration schemes on finite sum optimization \nproblems. First, we show that, perhaps surprisingly, the finite sum structure \nby itself, is not sufficient for obtaining a complexity bound of \n$\\tilde{\\cO}((n+L/\\mu)\\ln(1/\\epsilon))$ for $L$-smooth and $\\mu$-strongly \nconvex individual functions - one must also know which individual function is \nbeing referred to by the oracle at each iteration. Next, we show that for a \nbroad class of first-order and coordinate-descent finite sum algorithms \n(including, e.g., SDCA, SVRG, SAG), it is not possible to get an `accelerated' \ncomplexity bound of $\\tilde{\\cO}((n+\\sqrt{n L/\\mu})\\ln(1/\\epsilon))$, unless \nthe strong convexity parameter is given explicitly. Lastly, we show that when \nthis class of algorithms is used for minimizing $L$-smooth and convex finite \nsums, the optimal complexity bound is $\\tilde{\\cO}(n+L/\\epsilon)$, assuming \nthat (on average) the same update rule is used in every iteration, and \n$\\tilde{\\cO}(n+\\sqrt{nL/\\epsilon})$, otherwise. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c8ca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.01686"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "R&#xe9;mi Gribonval (PANAMA), Gilles Blanchard, Nicolas Keriven (PANAMA, UR1), Yann Traonmilin (PANAMA)", "title": "Compressive Statistical Learning with Random Feature Moments. (arXiv:1706.07180v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.07180", "type": "text/html"}], "timestampUsec": "1512709730983938", "comments": [], "summary": {"content": "<p>We describe a general framework --compressive statistical learning-- for \nresource-efficient large-scale learning: the training collection is compressed \nin one pass into a low-dimensional sketch (a vector of random empirical \ngeneralized moments) that captures the information relevant to the considered \nlearning task. A near-minimizer of the risk is computed from the sketch through \nthe solution of a nonlinear least squares problem. We investigate sufficient \nsketch sizes to control the generalization error of this procedure. The \nframework is illustrated on compressive clustering, compressive Gaussian \nmixture Modeling with fixed known variance, and compressive PCA. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c8d9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.07180"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "George Barmpalias, Frank Stephan", "title": "Algorithmic learning of probability distributions from random data in the limit. (arXiv:1710.11303v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.11303", "type": "text/html"}], "timestampUsec": "1512709730983937", "comments": [], "summary": {"content": "<p>We study the problem of identifying a probability distribution for some given \nrandomly sampled data in the limit, in the context of algorithmic learning \ntheory as proposed recently by Vinanyi and Chater. We show that there exists a \ncomputable partial learner for the computable probability measures, while by \nBienvenu, Monin and Shen it is known that there is no computable learner for \nthe computable probability measures. Our main result is the characterization of \nthe oracles that compute explanatory learners for the computable (continuous) \nprobability measures as the high oracles. This provides an analogue of a \nwell-known result of Adleman and Blum in the context of learning computable \nprobability distributions. We also discuss related learning notions such as \nbehaviorally correct learning and orther variations of explanatory learning, in \nthe context of learning probability distributions from data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c8e4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.11303"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Marco Federici, Karen Ullrich, Max Welling", "title": "Improved Bayesian Compression. (arXiv:1711.06494v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.06494", "type": "text/html"}], "timestampUsec": "1512709730983936", "comments": [], "summary": {"content": "<p>Compression of Neural Networks (NN) has become a highly studied topic in \nrecent years. The main reason for this is the demand for industrial scale usage \nof NNs such as deploying them on mobile devices, storing them efficiently, \ntransmitting them via band-limited channels and most importantly doing \ninference at scale. In this work, we propose to join the Soft-Weight Sharing \nand Variational Dropout approaches that show strong results to define a new \nstate-of-the-art in terms of model compression. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c8f3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.06494"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Fabian Offert", "title": "\"I know it when I see it\". Visualization and Intuitive Interpretability. (arXiv:1711.08042v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08042", "type": "text/html"}], "timestampUsec": "1512709730983935", "comments": [], "summary": {"content": "<p>Most research on the interpretability of machine learning systems focuses on \nthe development of a more rigorous notion of interpretability. I suggest that a \nbetter understanding of the deficiencies of the intuitive notion of \ninterpretability is needed as well. I show that visualization enables but also \nimpedes intuitive interpretability, as it presupposes two levels of technical \npre-interpretation: dimensionality reduction and regularization. Furthermore, I \nargue that the use of positive concepts to emulate the distributed semantic \nstructure of machine learning models introduces a significant human bias into \nthe model. As a consequence, I suggest that, if intuitive interpretability is \nneeded, singular representations of internal model states should be avoided. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512709730984", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000342b7c8f7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08042"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nand Sharma", "title": "Single-trial P300 Classification using PCA with LDA, QDA and Neural Networks. (arXiv:1712.01977v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.01977", "type": "text/html"}], "timestampUsec": "1512624568381662", "comments": [], "summary": {"content": "<p>The P300 event-related potential (ERP), evoked in scalp-recorded \nelectroencephalography (EEG) by external stimuli, has proven to be a reliable \nresponse for controlling a BCI. The P300 component of an event related \npotential is thus widely used in brain-computer interfaces to translate the \nsubjects' intent by mere thoughts into commands to control artificial devices. \nThe main challenge in the classification of P300 trials in \nelectroencephalographic (EEG) data is the low signal-to-noise ratio (SNR) of \nthe P300 response. To overcome the low SNR of individual trials, it is common \npractice to average together many consecutive trials, which effectively \ndiminishes the random noise. Unfortunately, when more repeated trials are \nrequired for applications such as the P300 speller, the communication rate is \ngreatly reduced. This has resulted in a need for better methods to improve \nsingle-trial classification accuracy of P300 response. In this work, we use \nPrincipal Component Analysis (PCA) as a preprocessing method and use Linear \nDiscriminant Analysis (LDA)and neural networks for classification. The results \nshow that a combination of PCA with these methods provided as high as 13\\% \naccuracy gain for single-trial classification while using only 3 to 4 principal \ncomponents. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c37", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01977"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kyeong Soo Kim, Sanghyuk Lee, Kaizhu Huang", "title": "A Scalable Deep Neural Network Architecture for Multi-Building and Multi-Floor Indoor Localization Based on Wi-Fi Fingerprinting. (arXiv:1712.01990v1 [cs.NI])", "alternate": [{"href": "http://arxiv.org/abs/1712.01990", "type": "text/html"}], "timestampUsec": "1512624568381661", "comments": [], "summary": {"content": "<p>One of the key technologies for future large-scale location-aware services \ncovering a complex of multi-story buildings --- e.g., a big shopping mall and a \nuniversity campus --- is a scalable indoor localization technique. In this \npaper, we report the current status of our investigation on the use of deep \nneural networks (DNNs) for scalable building/floor classification and \nfloor-level position estimation based on Wi-Fi fingerprinting. Exploiting the \nhierarchical nature of the building/floor estimation and floor-level \ncoordinates estimation of a location, we propose a new DNN architecture \nconsisting of a stacked autoencoder for the reduction of feature space \ndimension and a feed-forward classifier for multi-label classification of \nbuilding/floor/location, on which the multi-building and multi-floor indoor \nlocalization system based on Wi-Fi fingerprinting is built. Experimental \nresults for the performance of building/floor estimation and floor-level \ncoordinates estimation of a given location demonstrate the feasibility of the \nproposed DNN-based indoor localization system, which can provide near \nstate-of-the-art performance using a single DNN, for the implementation with \nlower complexity and energy consumption at mobile devices. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c43", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01990"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mahdi Namazifar", "title": "Named Entity Sequence Classification. (arXiv:1712.02316v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.02316", "type": "text/html"}], "timestampUsec": "1512624568381660", "comments": [], "summary": {"content": "<p>Named Entity Recognition (NER) aims at locating and classifying named \nentities in text. In some use cases of NER, including cases where detected \nnamed entities are used in creating content recommendations, it is crucial to \nhave a reliable confidence level for the detected named entities. In this work \nwe study the problem of finding confidence levels for detected named entities. \nWe refer to this problem as Named Entity Sequence Classification (NESC). We \nframe NESC as a binary classification problem and we use NER as well as \nrecurrent neural networks to find the probability of candidate named entity is \na real named entity. We apply this approach to Tweet texts and we show how we \ncould find named entities with high confidence levels from Tweets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c49", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02316"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Omid Poursaeed, Isay Katsman, Bicheng Gao, Serge Belongie", "title": "Generative Adversarial Perturbations. (arXiv:1712.02328v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.02328", "type": "text/html"}], "timestampUsec": "1512624568381659", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450510c1805\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450510c1805&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we propose novel generative models for creating adversarial \nexamples, slightly perturbed images resembling natural images but maliciously \ncrafted to fool pre-trained models. We present trainable deep neural networks \nfor transforming images to adversarial perturbations. Our proposed models can \nproduce image-agnostic and image-dependent perturbations for both targeted and \nnon-targeted attacks. We also demonstrate that similar architectures can \nachieve impressive results in fooling classification and semantic segmentation \nmodels, obviating the need for hand-crafting attack methods for each task. \nUsing extensive experiments on challenging high-resolution datasets such as \nImageNet and Cityscapes, we show that our perturbations achieve high fooling \nrates with small perturbation norms. Moreover, our attacks are considerably \nfaster than current iterative methods at inference time. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02328"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kyriaki Kalimeri, Mariano G. Beiro, Matteo Delfino, Robert Raleigh, Ciro Cattuto", "title": "Predicting Demographics, Moral Foundations, and Human Values from Digital Behaviors. (arXiv:1712.01930v1 [cs.CY])", "alternate": [{"href": "http://arxiv.org/abs/1712.01930", "type": "text/html"}], "timestampUsec": "1512624568381658", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505113dc01\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505113dc01&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Personal electronic devices such as smartphones give access to a broad range \nof behavioral signals that can be used to learn about the characteristics and \npreferences of individuals. In this study we explore the connection between \ndemographic and psychological attributes and digital records for a cohort of \n7,633 people, closely representative of the US population with respect to \ngender, age, geographical distribution, education, and income. We collected \nself-reported assessments on validated psychometric questionnaires based on \nboth the Moral Foundations and Basic Human Values theories, and combined this \ninformation with passively-collected multi-modal digital data from web browsing \nbehavior, smartphone usage and demographic data. Then, we designed a machine \nlearning framework to infer both the demographic and psychological attributes \nfrom the behavioral data. In a cross-validated setting, our model is found to \npredict demographic attributes with good accuracy (weighted AUC scores of 0.90 \nfor gender, 0.71 for age, 0.74 for ethnicity). Our weighted AUC scores for \nMoral Foundation attributes (0.66) and Human Values attributes (0.60) suggest \nthat accurate prediction of complex psychometric attributes is more challenging \nbut feasible. This connection might prove useful for designing personalized \nservices, communication strategies, and interventions, and can be used to \nsketch a portrait of people with similar worldviews. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c55", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01930"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yantian Zha, Yikang Li, Sriram Gopalakrishnan, Baoxin Li, Subbarao Kambhampati", "title": "Recognizing Plans by Learning Embeddings from Observed Action Distributions. (arXiv:1712.01949v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.01949", "type": "text/html"}], "timestampUsec": "1512624568381657", "comments": [], "summary": {"content": "<p>Recent advances in visual activity recognition have raised the possibility of \napplications such as automated video surveillance. Effective approaches for \nsuch problems however require the ability to recognize the plans of the agents \nfrom video information. Although traditional plan recognition algorithms depend \non access to sophisticated domain models, one recent promising direction \ninvolves learning shallow models directly from the observed activity sequences, \nand using them to recognize/predict plans. One limitation of such approaches is \nthat they expect observed action sequences as training data. In many cases \ninvolving vision or sensing from raw data, there is considerably uncertainty \nabout the specific action at any given time point. The most we can expect in \nsuch cases is probabilistic information about the action at that point. The \ntraining data will then be sequences of such observed action distributions. In \nthis paper, we focus on doing effective plan recognition with such uncertain \nobservations. Our contribution is a novel extension of word vector embedding \ntechniques to directly handle such observation distributions as input. This \ninvolves computing embeddings by minimizing the distance between distributions \n(measured as KL-divergence). We will show that our approach has superior \nperformance when the perception error rate (PER) is higher, and competitive \nperformance when the PER is lower. We will also explore the possibility of \nusing importance sampling techniques to handle observed action distributions \nwith traditional word vector embeddings. We will show that although such \napproaches can give good recognition accuracy, they take significantly longer \ntraining time and their performance will degrade significantly at higher \nperception error rate. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c5c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01949"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N. Sainath, Zhifeng Chen, Rohit Prabhavalkar", "title": "An analysis of incorporating an external language model into a sequence-to-sequence model. (arXiv:1712.01996v1 [eess.AS])", "alternate": [{"href": "http://arxiv.org/abs/1712.01996", "type": "text/html"}], "timestampUsec": "1512624568381656", "comments": [], "summary": {"content": "<p>Attention-based sequence-to-sequence models for automatic speech recognition \njointly train an acoustic model, language model, and alignment mechanism. Thus, \nthe language model component is only trained on transcribed audio-text pairs. \nThis leads to the use of shallow fusion with an external language model at \ninference time. Shallow fusion refers to log-linear interpolation with a \nseparately trained language model at each step of the beam search. In this \nwork, we investigate the behavior of shallow fusion across a range of \nconditions: different types of language models, different decoding units, and \ndifferent tasks. On Google Voice Search, we demonstrate that the use of shallow \nfusion with a neural LM with wordpieces yields a 9.1% relative word error rate \nreduction (WERR) over our competitive attention-based sequence-to-sequence \nmodel, obviating the need for second-pass rescoring. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c65", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01996"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Garrett B. Goh, Nathan O. Hodas, Charles Siegel, Abhinav Vishnu", "title": "SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for Predicting Chemical Properties. (arXiv:1712.02034v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02034", "type": "text/html"}], "timestampUsec": "1512624568381655", "comments": [], "summary": {"content": "<p>Chemical databases store information in text representations, and the SMILES \nformat is a universal standard used in many cheminformatics software. Encoded \nin each SMILES string is structural information that can be used to predict \ncomplex chemical properties. In this work, we develop SMILES2Vec, a deep RNN \nthat automatically learns features from SMILES strings to predict chemical \nproperties, without the need for additional explicit chemical information, or \nthe \"grammar\" of how SMILES encode structural data. Using Bayesian optimization \nmethods to tune the network architecture, we show that an optimized SMILES2Vec \nmodel can serve as a general-purpose neural network for learning a range of \ndistinct chemical properties including toxicity, activity, solubility and \nsolvation energy, while outperforming contemporary MLP networks that uses \nengineered features. Furthermore, we demonstrate proof-of-concept of \ninterpretability by developing an explanation mask that localizes on the most \nimportant characters used in making a prediction. When tested on the solubility \ndataset, this localization identifies specific parts of a chemical that is \nconsistent with established first-principles knowledge of solubility with an \naccuracy of 88%, demonstrating that neural networks can learn technically \naccurate chemical concepts. The fact that SMILES2Vec validates established \nchemical facts, while providing state-of-the-art accuracy, makes it a potential \ntool for widespread adoption of interpretable deep learning by the chemistry \ncommunity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c6f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02034"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Borui Wang, Geoffrey Gordon", "title": "Learning General Latent-Variable Graphical Models with Predictive Belief Propagation and Hilbert Space Embeddings. (arXiv:1712.02046v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.02046", "type": "text/html"}], "timestampUsec": "1512624568381654", "comments": [], "summary": {"content": "<p>In this paper, we propose a new algorithm for learning general \nlatent-variable probabilistic graphical models using the techniques of \npredictive state representation, instrumental variable regression, and \nreproducing-kernel Hilbert space embeddings of distributions. Under this new \nlearning framework, we first convert latent-variable graphical models into \ncorresponding latent-variable junction trees, and then reduce the hard \nparameter learning problem into a pipeline of supervised learning problems, \nwhose results will then be used to perform predictive belief propagation over \nthe latent junction tree during the actual inference procedure. We then give \nproofs of our algorithm's correctness, and demonstrate its good performance in \nexperiments on one synthetic dataset and two real-world tasks from \ncomputational biology and computer vision - classifying DNA splice junctions \nand recognizing human actions in videos. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c7b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02046"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jinbae Im, Sungzoon Cho", "title": "Distance-based Self-Attention Network for Natural Language Inference. (arXiv:1712.02047v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.02047", "type": "text/html"}], "timestampUsec": "1512624568381653", "comments": [], "summary": {"content": "<p>Attention mechanism has been used as an ancillary means to help RNN or CNN. \nHowever, the Transformer (Vaswani et al., 2017) recently recorded the \nstate-of-the-art performance in machine translation with a dramatic reduction \nin training time by solely using attention. Motivated by the Transformer, \nDirectional Self Attention Network (Shen et al., 2017), a fully attention-based \nsentence encoder, was proposed. It showed good performance with various data by \nusing forward and backward directional information in a sentence. But in their \nstudy, not considered at all was the distance between words, an important \nfeature when learning the local dependency to help understand the context of \ninput text. We propose Distance-based Self-Attention Network, which considers \nthe word distance by using a simple distance mask in order to model the local \ndependency without losing the ability of modeling global dependency which \nattention has inherent. Our model shows good performance with NLI data, and it \nrecords the new state-of-the-art result with SNLI data. Additionally, we show \nthat our model has a strength in long sentences or documents. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c82", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02047"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Luca Pappalardo, Paolo Cintia, Dino Pedreschi, Fosca Giannotti, Albert-Laszlo Barabasi", "title": "Human Perception of Performance. (arXiv:1712.02224v1 [physics.soc-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.02224", "type": "text/html"}], "timestampUsec": "1512624568381652", "comments": [], "summary": {"content": "<p>Humans are routinely asked to evaluate the performance of other individuals, \nseparating success from failure and affecting outcomes from science to \neducation and sports. Yet, in many contexts, the metrics driving the human \nevaluation process remain unclear. Here we analyse a massive dataset capturing \nplayers' evaluations by human judges to explore human perception of performance \nin soccer, the world's most popular sport. We use machine learning to design an \nartificial judge which accurately reproduces human evaluation, allowing us to \ndemonstrate how human observers are biased towards diverse contextual features. \nBy investigating the structure of the artificial judge, we uncover the aspects \nof the players' behavior which attract the attention of human judges, \ndemonstrating that human evaluation is based on a noticeability heuristic where \nonly feature values far from the norm are considered to rate an individual's \nperformance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c89", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02224"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xuelin Qian, Yanwei Fu, Wenxuan Wang, Tao Xiang, Yang Wu, Yu-Gang Jiang, Xiangyang Xue", "title": "Pose-Normalized Image Generation for Person Re-identification. (arXiv:1712.02225v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.02225", "type": "text/html"}], "timestampUsec": "1512624568381651", "comments": [], "summary": {"content": "<p>Person Re-identification (re-id) faces two major challenges: the lack of \ncross-view paired training data and learning discriminative identity-sensitive \nand view-invariant features in the presence of large pose variations. In this \nwork, we address both problems by proposing a novel deep person image \ngeneration model for synthesizing realistic person images conditional on pose. \nThe model is based on a generative adversarial network (GAN) and used \nspecifically for pose normalization in re-id, thus termed pose-normalization \nGAN (PN-GAN). With the synthesized images, we can learn a new type of deep \nre-id feature free of the influence of pose variations. We show that this \nfeature is strong on its own and highly complementary to features learned with \nthe original images. Importantly, we now have a model that generalizes to any \nnew re-id dataset without the need for collecting any training data for model \nfine-tuning, thus making a deep re-id model truly scalable. Extensive \nexperiments on five benchmarks show that our model outperforms the \nstate-of-the-art models, often significantly. In particular, the features \nlearned on Market-1501 can achieve a Rank-1 accuracy of 68.67% on VIPeR without \nany model fine-tuning, beating almost all existing models fine-tuned on the \ndataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c94", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02225"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Daniel Golovin, Andreas Krause", "title": "Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization. (arXiv:1003.3967v5 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1003.3967", "type": "text/html"}], "timestampUsec": "1512624568381650", "comments": [], "summary": {"content": "<p>Solving stochastic optimization problems under partial observability, where \none needs to adaptively make decisions with uncertain outcomes, is a \nfundamental but notoriously difficult challenge. In this paper, we introduce \nthe concept of adaptive submodularity, generalizing submodular set functions to \nadaptive policies. We prove that if a problem satisfies this property, a simple \nadaptive greedy algorithm is guaranteed to be competitive with the optimal \npolicy. In addition to providing performance guarantees for both stochastic \nmaximization and coverage, adaptive submodularity can be exploited to \ndrastically speed up the greedy algorithm by using lazy evaluations. We \nillustrate the usefulness of the concept by giving several examples of adaptive \nsubmodular objectives arising in diverse applications including sensor \nplacement, viral marketing and active learning. Proving adaptive submodularity \nfor these problems allows us to recover existing results in these applications \nas special cases, improve approximation guarantees and handle natural \ngeneralizations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7c9c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1003.3967"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhaohan Daniel Guo, Philip S. Thomas, Emma Brunskill", "title": "Using Options and Covariance Testing for Long Horizon Off-Policy Policy Evaluation. (arXiv:1703.03453v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.03453", "type": "text/html"}], "timestampUsec": "1512624568381649", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505113e02a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505113e02a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Evaluating a policy by deploying it in the real world can be risky and \ncostly. Off-policy policy evaluation (OPE) algorithms use historical data \ncollected from running a previous policy to evaluate a new policy, which \nprovides a means for evaluating a policy without requiring it to ever be \ndeployed. Importance sampling is a popular OPE method because it is robust to \npartial observability and works with continuous states and actions. However, \nthe amount of historical data required by importance sampling can scale \nexponentially with the horizon of the problem: the number of sequential \ndecisions that are made. We propose using policies over temporally extended \nactions, called options, and show that combining these policies with importance \nsampling can significantly improve performance for long-horizon problems. In \naddition, we can take advantage of special cases that arise due to \noptions-based policies to further improve the performance of importance \nsampling. We further generalize these special cases to a general covariance \ntesting rule that can be used to decide which weights to drop in an IS \nestimate, and derive a new IS algorithm called Incremental Importance Sampling \nthat can provide significantly more accurate estimates for a broad class of \ndomains. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7ca7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.03453"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ting-Hao &#x27;Kenneth&#x27; Huang, Yun-Nung Chen, Jeffrey P. Bigham", "title": "Real-time On-Demand Crowd-powered Entity Extraction. (arXiv:1704.03627v2 [cs.HC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.03627", "type": "text/html"}], "timestampUsec": "1512624568381648", "comments": [], "summary": {"content": "<p>Output-agreement mechanisms such as ESP Game have been widely used in human \ncomputation to obtain reliable human-generated labels. In this paper, we argue \nthat a \"time-limited\" output-agreement mechanism can be used to create a fast \nand robust crowd-powered component in interactive systems, particularly \ndialogue systems, to extract key information from user utterances on the fly. \nOur experiments on Amazon Mechanical Turk using the Airline Travel Information \nSystem (ATIS) dataset showed that the proposed approach achieves high-quality \nresults with an average response time shorter than 9 seconds. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.03627"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations. (arXiv:1704.04683v5 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.04683", "type": "text/html"}], "timestampUsec": "1512624568381647", "comments": [], "summary": {"content": "<p>We present RACE, a new dataset for benchmark evaluation of methods in the \nreading comprehension task. Collected from the English exams for middle and \nhigh school Chinese students in the age range between 12 to 18, RACE consists \nof near 28,000 passages and near 100,000 questions generated by human experts \n(English instructors), and covers a variety of topics which are carefully \ndesigned for evaluating the students' ability in understanding and reasoning. \nIn particular, the proportion of questions that requires reasoning is much \nlarger in RACE than that in other benchmark datasets for reading comprehension, \nand there is a significant gap between the performance of the state-of-the-art \nmodels (43%) and the ceiling human performance (95%). We hope this new dataset \ncan serve as a valuable resource for research and evaluation in machine \ncomprehension. The dataset is freely available at \n<a href=\"http://www.cs.cmu.edu/~glai1/data/race/\">this http URL</a> and the code is available at \nhttps://github.com/qizhex/RACE_AR_baselines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.04683"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Riccardo Polvara, Massimiliano Patacchiola, Sanjay Sharma, Jian Wan, Andrew Manning, Robert Sutton, Angelo Cangelosi", "title": "Autonomous Quadrotor Landing using Deep Reinforcement Learning. (arXiv:1709.03339v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.03339", "type": "text/html"}], "timestampUsec": "1512624568381646", "comments": [], "summary": {"content": "<p>Landing an unmanned aerial vehicle (UAV) on a ground marker is an open \nproblem despite the effort of the research community. Previous attempts mostly \nfocused on the analysis of hand-crafted geometric features and the use of \nexternal sensors in order to allow the vehicle to approach the land-pad. In \nthis article, we propose a method based on deep reinforcement learning that \nonly requires low-resolution images taken from a down-looking camera in order \nto identify the position of the marker and land the UAV on it. The proposed \napproach is based on a hierarchy of Deep Q-Networks (DQNs) used as high-level \ncontrol policy for the navigation toward the marker. We implemented different \ntechnical solutions, such as the combination of vanilla and double DQNs trained \nusing a partitioned buffer replay.The results show that policies trained on \nuniform textures can accomplish autonomous landing on a large variety of \nsimulated environments. The overall performance is comparable with a \nstate-of-the-art algorithm and human pilots. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cbb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.03339"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Behzad Tabibian, Utkarsh Upadhyay, Abir De, Ali Zarezade, Bernhard Schoelkopf, Manuel Gomez-Rodriguez", "title": "Optimizing Human Learning. (arXiv:1712.01856v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01856", "type": "text/html"}], "timestampUsec": "1512624568381644", "comments": [], "summary": {"content": "<p>Spaced repetition is a technique for efficient memorization which uses \nrepeated, spaced review of content to improve long-term retention. Can we find \nthe optimal reviewing schedule to maximize the benefits of spaced repetition? \nIn this paper, we introduce a novel, flexible representation of spaced \nrepetition using the framework of marked temporal point processes and then \naddress the above question as an optimal control problem for stochastic \ndifferential equations with jumps. For two well-known human memory models, we \nshow that the optimal reviewing schedule is given by the recall probability of \nthe content to be learned. As a result, we can then develop a simple, scalable \nonline algorithm, Memorize, to sample the optimal reviewing times. Experiments \non both synthetic and real data gathered from Duolingo, a popular \nlanguage-learning online platform, show that our algorithm may be able to help \nlearners memorize more effectively than alternatives. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cbf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01856"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tara N. Sainath, Rohit Prabhavalkar, Shankar Kumar, Seungji Lee, Anjuli Kannan, David Rybach, Vlad Schogol, Patrick Nguyen, Bo Li, Yonghui Wu, Zhifeng Chen, Chung-Cheng Chiu", "title": "No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models. (arXiv:1712.01864v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.01864", "type": "text/html"}], "timestampUsec": "1512624568381643", "comments": [], "summary": {"content": "<p>For decades, context-dependent phonemes have been the dominant sub-word unit \nfor conventional acoustic modeling systems. This status quo has begun to be \nchallenged recently by end-to-end models which seek to combine acoustic, \npronunciation, and language model components into a single neural network. Such \nsystems, which typically predict graphemes or words, simplify the recognition \nprocess since they remove the need for a separate expert-curated pronunciation \nlexicon to map from phoneme-based units to words. However, there has been \nlittle previous work comparing phoneme-based versus grapheme-based sub-word \nunits in the end-to-end modeling framework, to determine whether the gains from \nsuch approaches are primarily due to the new probabilistic model, or from the \njoint learning of the various components with grapheme-based units. \n</p> \n<p>In this work, we conduct detailed experiments which are aimed at quantifying \nthe value of phoneme-based pronunciation lexica in the context of end-to-end \nmodels. We examine phoneme-based end-to-end models, which are contrasted \nagainst grapheme-based ones on a large vocabulary English Voice-search task, \nwhere we find that graphemes do indeed outperform phonemes. We also compare \ngrapheme and phoneme-based approaches on a multi-dialect English task, which \nonce again confirm the superiority of graphemes, greatly simplifying the system \nfor recognizing multiple dialects. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cc7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01864"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally", "title": "Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training. (arXiv:1712.01887v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01887", "type": "text/html"}], "timestampUsec": "1512624568381642", "comments": [], "summary": {"content": "<p>Large-scale distributed training requires significant communication bandwidth \nfor gradient exchange that limits the scalability of multi-node training, and \nrequires expensive high-bandwidth network infrastructure. The situation gets \neven worse with distributed training on mobile devices (federated learning), \nwhich suffers from higher latency, lower throughput, and intermittent poor \nconnections. In this paper, we find 99.9% of the gradient exchange in \ndistributed SGD is redundant, and propose Deep Gradient Compression (DGC) to \ngreatly reduce the communication bandwidth. To preserve accuracy during \ncompression, DGC employs four methods: momentum correction, local gradient \nclipping, momentum factor masking, and warm-up training. We have applied Deep \nGradient Compression to image classification, speech recognition, and language \nmodeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and \nLibrispeech Corpus. On these scenarios, Deep Gradient Compression achieves a \ngradient compression ratio from 270x to 600x without losing accuracy, cutting \nthe gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from \n488MB to 0.74MB. Deep gradient compression enables large-scale distributed \ntraining on inexpensive commodity 1Gbps Ethernet and facilitates distributed \ntraining on mobile. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cd1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01887"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gilles Blanchard, Oleksandr Zadorozhnyi", "title": "Concentration of weakly dependent Banach-valued sums and applications to kernel learning methods. (arXiv:1712.01934v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01934", "type": "text/html"}], "timestampUsec": "1512624568381641", "comments": [], "summary": {"content": "<p>We obtain a new Bernstein-type inequality for sums of Banach-valued random \nvariables satisfying a weak dependence assumption of general type and under \ncertain smoothness assumptions of the underlying Banach norm. We use this \ninequality in order to investigate in asymptotical regime the error upper \nbounds for the broad family of spectral regularization methods for reproducing \nkernel decision rules, when trained on a sample coming from a $\\tau-$mixing \nprocess. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cdd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01934"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sujayam Saha, Adityanand Guntuboyina", "title": "On the nonparametric maximum likelihood estimator for Gaussian location mixture densities with application to Gaussian denoising. (arXiv:1712.02009v1 [math.ST])", "alternate": [{"href": "http://arxiv.org/abs/1712.02009", "type": "text/html"}], "timestampUsec": "1512624568381640", "comments": [], "summary": {"content": "<p>We study the Nonparametric Maximum Likelihood Estimator (NPMLE) for \nestimating Gaussian location mixture densities in $d$-dimensions from \nindependent observations. Unlike usual likelihood-based methods for fitting \nmixtures, NPMLEs are based on convex optimization. We prove finite sample \nresults on the Hellinger accuracy of every NPMLE. Our results imply, in \nparticular, that every NPMLE achieves near parametric risk (up to logarithmic \nmultiplicative factors) when the true density is a discrete Gaussian mixture \nwithout any prior information on the number of mixture components. NPMLEs can \nnaturally be used to yield empirical Bayes estimates of the Oracle Bayes \nestimator in the Gaussian denoising problem. We prove bounds for the accuracy \nof the empirical Bayes estimate as an approximation to the Oracle Bayes \nestimator. Here our results imply that the empirical Bayes estimator performs \nat nearly the optimal level (up to logarithmic multiplicative factors) for \ndenoising in clustering situations without any prior knowledge of the number of \nclusters. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cf1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02009"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aditya Devarakonda, Maxim Naumov, Michael Garland", "title": "AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks. (arXiv:1712.02029v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.02029", "type": "text/html"}], "timestampUsec": "1512624568381639", "comments": [], "summary": {"content": "<p>Training deep neural networks with Stochastic Gradient Descent, or its \nvariants, requires careful choice of both learning rate and batch size. While \nsmaller batch sizes generally converge in fewer training epochs, larger batch \nsizes offer more parallelism and hence better computational efficiency. We have \ndeveloped a new training approach that, rather than statically choosing a \nsingle batch size for all epochs, adaptively increases the batch size during \nthe training process. Our method delivers the convergence rate of small batch \nsizes while achieving performance similar to large batch sizes. We analyse our \napproach using the standard AlexNet, ResNet, and VGG networks operating on the \npopular CIFAR-10, CIFAR-100, and ImageNet datasets. Our results demonstrate \nthat learning with adaptive batch sizes can improve performance by factors of \nup to 6.25 on 4 NVIDIA Tesla P100 GPUs while changing accuracy by less than 1% \nrelative to training with fixed batch sizes. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7cf7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02029"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David Barmherzig, Ju Sun", "title": "A Local Analysis of Block Coordinate Descent for Gaussian Phase Retrieval. (arXiv:1712.02083v1 [cs.IT])", "alternate": [{"href": "http://arxiv.org/abs/1712.02083", "type": "text/html"}], "timestampUsec": "1512624568381638", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505113e40a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505113e40a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>While convergence of the Alternating Direction Method of Multipliers (ADMM) \non convex problems is well studied, convergence on nonconvex problems is only \npartially understood. In this paper, we consider the Gaussian phase retrieval \nproblem, formulated as a linear constrained optimization problem with a \nbiconvex objective. The particular structure allows for a novel application of \nthe ADMM. It can be shown that the dual variable is zero at the global \nminimizer. This motivates the analysis of a block coordinate descent algorithm, \nwhich is equivalent to the ADMM with the dual variable fixed to be zero. We \nshow that the block coordinate descent algorithm converges to the global \nminimizer at a linear rate, when starting from a deterministically achievable \ninitialization point. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02083"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sebastian Stabinger, Antonio Rodriguez-Sanchez", "title": "Guided Labeling using Convolutional Neural Networks. (arXiv:1712.02154v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.02154", "type": "text/html"}], "timestampUsec": "1512624568381637", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450511ab6e3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450511ab6e3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Over the last couple of years, deep learning and especially convolutional \nneural networks have become one of the work horses of computer vision. One \nlimiting factor for the applicability of supervised deep learning to more areas \nis the need for large, manually labeled datasets. In this paper we propose an \neasy to implement method we call guided labeling, which automatically \ndetermines which samples from an unlabeled dataset should be labeled. We show \nthat using this procedure, the amount of samples that need to be labeled is \nreduced considerably in comparison to labeling images arbitrarily. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d06", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02154"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512709735, "author": "Chaopeng Shen", "title": "A trans-disciplinary review of deep learning research for water resources scientists. (arXiv:1712.02162v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.02162", "type": "text/html"}], "timestampUsec": "1512624568381636", "comments": [], "summary": {"content": "<p>Deep learning (DL), a new-generation artificial neural network research, has \nmade profound strides in recent years. This review paper is intended to provide \nwater resources scientists with a simple technical overview, trans-disciplinary \nprogress update, and potentially inspirations about DL. Effective \narchitectures, more accessible data, advances in regularization, and new \ncomputing power enabled the success of DL. A trans-disciplinary review reveals \nthat DL is rapidly transforming myriad scientific disciplines including \nhigh-energy physics, astronomy, chemistry, genomics and remote sensing, where \nsystematic DL toolkits, innovative customizations, and sub-disciplines have \nemerged. However, with a few exceptions, its adoption in hydrology has so far \nbeen gradual. The literature suggests that novel regularization techniques can \neffectively prevent high-capacity deep networks from overfitting. As a result, \nin most scientific disciplines, DL models demonstrated superior predictive and \ngeneralization performance to conventional methods. Meanwhile, less noticed is \nthat DL may also serve as a scientific exploratory tool. A new area termed \"AI \nneuroscience\", has been born. This budding sub-discipline is accumulating a \nsignificant body of work, e.g., distilling knowledge obtained in DL networks to \ninterpretable models, attributing decisions to inputs via back-propagation of \nrelevance, or visualization of activations. These methods are designed to \ninterpret the decision process of deep networks and derive insights. While \nscientists so far have mostly been using customized, ad-hoc methods for \ninterpretation, vast opportunities await for DL to propel advancement in water \nscience. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512709731, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d14", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02162"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alejandro Murua, Ranjan Maitra", "title": "Fast spatial inference in the homogeneous Ising model. (arXiv:1712.02195v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1712.02195", "type": "text/html"}], "timestampUsec": "1512624568381635", "comments": [], "summary": {"content": "<p>The Ising model is important in statistical modeling and inference in many \napplications, however its normalizing constant, mean number of active vertices \nand mean spin interaction are intractable. We provide accurate approximations \nthat make it possible to calculate these quantities numerically. Simulation \nstudies indicate good performance when compared to Markov Chain Monte Carlo \nmethods and at a tiny fraction of the time. The methodology is also used to \nperform Bayesian inference in a functional Magnetic Resonance Imaging \nactivation detection experiment. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d1f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02195"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nicolas Thiebaut, Antoine Simoulin, Karl Neuberger, Issam Ibnoushein, Nicolas Bousquet, Nathalie Reix, S&#xe9;bastien Moli&#xe8;re, Carole Mathelin", "title": "An innovative solution for breast cancer textual big data analysis. (arXiv:1712.02259v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02259", "type": "text/html"}], "timestampUsec": "1512624568381634", "comments": [], "summary": {"content": "<p>The digitalization of stored information in hospitals now allows for the \nexploitation of medical data in text format, as electronic health records \n(EHRs), initially gathered for other purposes than epidemiology. Manual search \nand analysis operations on such data become tedious. In recent years, the use \nof natural language processing (NLP) tools was highlighted to automatize the \nextraction of information contained in EHRs, structure it and perform \nstatistical analysis on this structured information. The main difficulties with \nthe existing approaches is the requirement of synonyms or ontology \ndictionaries, that are mostly available in English only and do not include \nlocal or custom notations. In this work, a team composed of oncologists as \ndomain experts and data scientists develop a custom NLP-based system to process \nand structure textual clinical reports of patients suffering from breast \ncancer. The tool relies on the combination of standard text mining techniques \nand an advanced synonym detection method. It allows for a global analysis by \nretrieval of indicators such as medical history, tumor characteristics, \ntherapeutic responses, recurrences and prognosis. The versatility of the method \nallows to obtain easily new indicators, thus opening up the way for \nretrospective studies with a substantial reduction of the amount of manual \nwork. With no need for biomedical annotators or pre-defined ontologies, this \nlanguage-agnostic method reached an good extraction accuracy for several \nconcepts of interest, according to a comparison with a manually structured \nfile, without requiring any existing corpus with local or new notations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d2d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02259"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiaoyong Pan, Junchi Yan", "title": "Attention based convolutional neural network for predicting RNA-protein binding sites. (arXiv:1712.02270v1 [q-bio.GN])", "alternate": [{"href": "http://arxiv.org/abs/1712.02270", "type": "text/html"}], "timestampUsec": "1512624568381633", "comments": [], "summary": {"content": "<p>RNA-binding proteins (RBPs) play crucial roles in many biological processes, \ne.g. gene regulation. Computational identification of RBP binding sites on RNAs \nare urgently needed. In particular, RBPs bind to RNAs by recognizing sequence \nmotifs. Thus, fast locating those motifs on RNA sequences is crucial and \ntime-efficient for determining whether the RNAs interact with the RBPs or not. \nIn this study, we present an attention based convolutional neural network, \niDeepA, to predict RNA-protein binding sites from raw RNA sequences. We first \nencode RNA sequences into one-hot encoding. Next, we design a deep learning \nmodel with a convolutional neural network (CNN) and an attention mechanism, \nwhich automatically search for important positions, e.g. binding motifs, to \nlearn discriminant high-level features for predicting RBP binding sites. We \nevaluate iDeepA on publicly gold-standard RBP binding sites derived from \nCLIP-seq data. The results demonstrate iDeepA achieves comparable performance \nwith other state-of-the-art methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d3a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02270"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Victor Veitch, Ekansh Sharma, Zacharie Naulet, Daniel M. Roy", "title": "Exchangeable modelling of relational data: checking sparsity, train-test splitting, and sparse exchangeable Poisson matrix factorization. (arXiv:1712.02311v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02311", "type": "text/html"}], "timestampUsec": "1512624568381632", "comments": [], "summary": {"content": "<p>A variety of machine learning tasks---e.g., matrix factorization, topic \nmodelling, and feature allocation---can be viewed as learning the parameters of \na probability distribution over bipartite graphs. Recently, a new class of \nmodels for networks, the sparse exchangeable graphs, have been introduced to \nresolve some important pathologies of traditional approaches to statistical \nnetwork modelling; most notably, the inability to model sparsity (in the \nasymptotic sense). The present paper explains some practical insights arising \nfrom this work. We first show how to check if sparsity is relevant for \nmodelling a given (fixed size) dataset by using network subsampling to identify \na simple signature of sparsity. We discuss the implications of the (sparse) \nexchangeable subsampling theory for test-train dataset splitting; we argue \ncommon approaches can lead to biased results, and we propose a principled \nalternative. Finally, we study sparse exchangeable Poisson matrix factorization \nas a worked example. In particular, we show how to adapt mean field variational \ninference to the sparse exchangeable setting, allowing us to scale inference to \nhuge datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d42", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02311"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tatjana Chavdarova, Fran&#xe7;ois Fleuret", "title": "SGAN: An Alternative Training of Generative Adversarial Networks. (arXiv:1712.02330v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.02330", "type": "text/html"}], "timestampUsec": "1512624568381631", "comments": [], "summary": {"content": "<p>The Generative Adversarial Networks (GANs) have demonstrated impressive \nperformance for data synthesis, and are now used in a wide range of computer \nvision tasks. In spite of this success, they gained a reputation for being \ndifficult to train, what results in a time-consuming and human-involved \ndevelopment process to use them. \n</p> \n<p>We consider an alternative training process, named SGAN, in which several \nadversarial \"local\" pairs of networks are trained independently so that a \n\"global\" supervising pair of networks can be trained against them. The goal is \nto train the global pair with the corresponding ensemble opponent for improved \nperformances in terms of mode coverage. This approach aims at increasing the \nchances that learning will not stop for the global pair, preventing both to be \ntrapped in an unsatisfactory local minimum, or to face oscillations often \nobserved in practice. To guarantee the latter, the global pair never affects \nthe local ones. \n</p> \n<p>The rules of SGAN training are thus as follows: the global generator and \ndiscriminator are trained using the local discriminators and generators, \nrespectively, whereas the local networks are trained with their fixed local \nopponent. \n</p> \n<p>Experimental results on both toy and real-world problems demonstrate that \nthis approach outperforms standard training in terms of better mitigating mode \ncollapse, stability while converging and that it surprisingly, increases the \nconvergence speed as well. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d4c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.02330"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Dmitrii Marin, Meng Tang, Ismail Ben Ayed, Yuri Boykov", "title": "Kernel clustering: density biases and solutions. (arXiv:1705.05950v5 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.05950", "type": "text/html"}], "timestampUsec": "1512624568381630", "comments": [], "summary": {"content": "<p>Kernel methods are popular in clustering due to their generality and \ndiscriminating power. However, we show that many kernel clustering criteria \nhave density biases theoretically explaining some practically significant \nartifacts empirically observed in the past. For example, we provide conditions \nand formally prove the density mode isolation bias in kernel K-means for a \ncommon class of kernels. We call it Breiman's bias due to its similarity to the \nhistogram mode isolation previously discovered by Breiman in decision tree \nlearning with Gini impurity. We also extend our analysis to other popular \nkernel clustering methods, e.g. average/normalized cut or dominant sets, where \ndensity biases can take different forms. For example, splitting isolated points \nby cut-based criteria is essentially the sparsest subset bias, which is the \nopposite of the density mode bias. Our findings suggest that a principled \nsolution for density biases in kernel clustering should directly address data \ninhomogeneity. We show that density equalization can be implicitly achieved \nusing either locally adaptive weights or locally adaptive kernels. Moreover, \ndensity equalization makes many popular kernel clustering objectives \nequivalent. Our synthetic and real data experiments illustrate density biases \nand proposed solutions. We anticipate that theoretical understanding of kernel \nclustering limitations and their principled solutions will be important for a \nbroad spectrum of data analysis applications across the disciplines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d52", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.05950"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andrew J. Wagenmaker, Brian E. Moore, Raj Rao Nadakuditi", "title": "Robust Photometric Stereo via Dictionary Learning. (arXiv:1710.08873v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.08873", "type": "text/html"}], "timestampUsec": "1512624568381629", "comments": [], "summary": {"content": "<p>Photometric stereo is a method that seeks to reconstruct the normal vectors \nof an object from a set of images of the object illuminated under different \nlight sources. While effective in some situations, classical photometric stereo \nrelies on a diffuse surface model that cannot handle objects with complex \nreflectance patterns, and it is sensitive to non-idealities in the images. In \nthis work, we propose a novel approach to photometric stereo that relies on \ndictionary learning to produce robust normal vector reconstructions. \nSpecifically, we develop three formulations for applying dictionary learning to \nphotometric stereo. We propose a preprocessing step that utilizes dictionary \nlearning to denoise the images. We also present a model that applies dictionary \nlearning to regularize and reconstruct the normal vectors from the images under \nthe classic Lambertian reflectance model. Finally, we generalize the latter \nmodel to explicitly model non-Lambertian objects. We investigate all three \napproaches through extensive experimentation on synthetic and real benchmark \ndatasets and observe state-of-the-art performance compared to existing robust \nphotometric stereo methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d5c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.08873"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, Michael I. Jordan", "title": "Stochastic Cubic Regularization for Fast Nonconvex Optimization. (arXiv:1711.02838v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.02838", "type": "text/html"}], "timestampUsec": "1512624568381628", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450511ab9a7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450511ab9a7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper proposes a stochastic variant of a classic algorithm---the \ncubic-regularized Newton method [Nesterov and Polyak 2006]. The proposed \nalgorithm efficiently escapes saddle points and finds approximate local minima \nfor general smooth, nonconvex functions in only \n$\\mathcal{\\tilde{O}}(\\epsilon^{-3.5})$ stochastic gradient and stochastic \nHessian-vector product evaluations. The latter can be computed as efficiently \nas stochastic gradients. This improves upon the \n$\\mathcal{\\tilde{O}}(\\epsilon^{-4})$ rate of stochastic gradient descent. Our \nrate matches the best-known result for finding local minima without requiring \nany delicate acceleration or variance-reduction techniques. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d64", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.02838"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Vince Lyzinski, Keith Levin, Carey E. Priebe", "title": "On consistent vertex nomination schemes. (arXiv:1711.05610v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.05610", "type": "text/html"}], "timestampUsec": "1512624568381627", "comments": [], "summary": {"content": "<p>Given a vertex of interest in a network $G_1$, the vertex nomination problem \nseeks to find the corresponding vertex of interest (if it exists) in a second \nnetwork $G_2$. Although the vertex nomination problem and related tasks have \nattracted much attention in the machine learning literature, with applications \nto social and biological networks, the framework has so far been confined to a \ncomparatively small class of network models, and the concept of statistically \nconsistent vertex nomination schemes has been only shallowly explored. In this \npaper, we extend the vertex nomination problem to a very general statistical \nmodel of graphs. Further, drawing inspiration from the long-established \nclassification framework in the pattern recognition literature, we provide \ndefinitions for the key notions of Bayes optimality and consistency in our \nextended vertex nomination framework, including a derivation of the Bayes \noptimal vertex nomination scheme. In addition, we prove that no universally \nconsistent vertex nomination schemes exist. Illustrative examples are provided \nthroughout. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d6e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.05610"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yunpeng Li, Davide Zilli, Henry Chan, Ivan Kiskin, Marianne Sinka, Stephen Roberts, Kathy Willis", "title": "Mosquito detection with low-cost smartphones: data acquisition for malaria research. (arXiv:1711.06346v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.06346", "type": "text/html"}], "timestampUsec": "1512624568381626", "comments": [], "summary": {"content": "<p>Mosquitoes are a major vector for malaria, causing hundreds of thousands of \ndeaths in the developing world each year. Not only is the prevention of \nmosquito bites of paramount importance to the reduction of malaria transmission \ncases, but understanding in more forensic detail the interplay between malaria, \nmosquito vectors, vegetation, standing water and human populations is crucial \nto the deployment of more effective interventions. Typically the presence and \ndetection of malaria-vectoring mosquitoes is only quantified by hand-operated \ninsect traps or signified by the diagnosis of malaria. If we are to gather \ntimely, large-scale data to improve this situation, we need to automate the \nprocess of mosquito detection and classification as much as possible. In this \npaper, we present a candidate mobile sensing system that acts as both a \nportable early warning device and an automatic acoustic data acquisition \npipeline to help fuel scientific inquiry and policy. The machine learning \nalgorithm that powers the mobile system achieves excellent off-line \nmulti-species detection performance while remaining computationally efficient. \nFurther, we have conducted preliminary live mosquito detection tests using \nlow-cost mobile phones and achieved promising results. The deployment of this \nsystem for field usage in Southeast Asia and Africa is planned in the near \nfuture. In order to accelerate processing of field recordings and labelling of \ncollected data, we employ a citizen science platform in conjunction with \nautomated methods, the former implemented using the Zooniverse platform, \nallowing crowdsourcing on a grand scale. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d78", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.06346"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan, Xilin Chen", "title": "Arbitrary Facial Attribute Editing: Only Change What You Want. (arXiv:1711.10678v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10678", "type": "text/html"}], "timestampUsec": "1512624568381625", "comments": [], "summary": {"content": "<p>Facial attribute editing aims to modify either single or multiple attributes \non a face image. Since it is practically infeasible to collect images with \narbitrarily specified attributes for each person, the generative adversarial \nnet (GAN) and the encoder-decoder architecture are usually incorporated to \nhandle this task. With the encoder-decoder architecture, arbitrary attribute \nediting can then be conducted by decoding the latent representation of the face \nimage conditioned on the specified attributes. A few existing methods attempt \nto establish attribute-independent latent representation for arbitrarily \nchanging the attributes. However, since the attributes portray the \ncharacteristics of the face image, the attribute-independent constraint on the \nlatent representation is excessive. Such constraint may result in information \nloss and unexpected distortion on the generated images (e.g. over-smoothing), \nespecially for those identifiable attributes such as gender, race etc. Instead \nof imposing the attribute-independent constraint on the latent representation, \nwe introduce an attribute classification constraint on the generated image, \njust requiring the correct change of the attributes. Meanwhile, reconstruction \nlearning is introduced in order to guarantee the preservation of all other \nattribute-excluding details on the generated image, and adversarial learning is \nemployed for visually realistic generation. Moreover, our method can be \nnaturally extended to attribute intensity manipulation. Experiments on the \nCelebA dataset show that our method outperforms the state-of-the-arts on \ngenerating realistic attribute editing results with facial details well \npreserved. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512624568382", "annotations": [], "published": 1512624569, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341fb7d83", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10678"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Joon Kyung Kim, Vikas Chandra, Hadi Esmaeilzadeh", "title": "Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks. (arXiv:1712.01507v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.01507", "type": "text/html"}], "timestampUsec": "1512537801473962", "comments": [], "summary": {"content": "<p>Hardware acceleration of Deep Neural Networks (DNNs) aims to tame their \nenormous compute intensity. Fully realizing the potential of acceleration in \nthis domain requires understanding and leveraging algorithmic properties of \nDNNs. This paper builds upon the algorithmic insight that bitwidth of \noperations in DNNs can be reduced without compromising their accuracy. However, \nto prevent accuracy loss, the bitwidth varies significantly across DNNs and it \nmay even be adjusted for each layer individually. Thus, a fixed-bitwidth \naccelerator would either offer limited benefits to accommodate the worst-case \nbitwidth, or inevitably lead to a degradation in final accuracy. To alleviate \nthese deficiencies, this work introduces dynamic bit-level fusion/decomposition \nas a new dimension in the design of DNN accelerators. We explore this dimension \nby designing Bit Fusion, a bit-flexible accelerator, that constitutes an array \nof bit-level processing elements that dynamically fuse to match the bitwidth of \nindividual DNN layers. This flexibility in the architecture minimizes the \ncomputation and the communication at the finest granularity possible with no \nloss in accuracy. We evaluate the benefits of Bit Fusion using eight real-world \nfeed-forward and recurrent DNNs. The proposed microarchitecture is implemented \nin Verilog and synthesized in 45 nm technology. Using the synthesis results and \ncycle accurate simulation, we compare the benefits of Bit Fusion to two \nstate-of-the-art DNN accelerators, Eyeriss and Stripes. In the same area, \nfrequency, and technology node, Bit Fusion offers 4.3x speedup and 9.6x energy \nsavings over Eyeriss. Bit Fusion provides 2.4x speedup and 4.1x energy \nreduction over Stripes at 45 nm node when Bit Fusion area and frequency are set \nto those of Stripes. Compared to Jetson-TX2, Bit Fusion offers 4.3x speedup and \nalmost matches the performance of TitanX, which is 4.6x faster than TX2. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394e91", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01507"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pierre-Yves Oudeyer (Flowers)", "title": "Autonomous development and learning in artificial intelligence and robotics: Scaling up deep learning to human--like learning. (arXiv:1712.01626v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.01626", "type": "text/html"}], "timestampUsec": "1512537801473961", "comments": [], "summary": {"content": "<p>Autonomous lifelong development and learning is a fundamental capability of \nhumans, differentiating them from current deep learning systems. However, other \nbranches of artificial intelligence have designed crucial ingredients towards \nautonomous learning: curiosity and intrinsic motivation, social learning and \nnatural interaction with peers, and embodiment. These mechanisms guide \nexploration and autonomous choice of goals, and integrating them with deep \nlearning opens stimulating perspectives. Deep learning (DL) approaches made \ngreat advances in artificial intelligence, but are still far away from human \nlearning. As argued convincingly by Lake et al., differences include human \ncapabilities to learn causal models of the world from very little data, \nleveraging compositional representations and priors like intuitive physics and \npsychology. However, there are other fundamental differences between current DL \nsystems and human learning, as well as technical ingredients to fill this gap, \nthat are either superficially, or not adequately, discussed by Lake et al. \nThese fundamental mechanisms relate to autonomous development and learning. \nThey are bound to play a central role in artificial intelligence in the future. \nCurrent DL systems require engineers to manually specify a task-specific \nobjective function for every new task, and learn through off-line processing of \nlarge training databases. On the contrary, humans learn autonomously open-ended \nrepertoires of skills, deciding for themselves which goals to pursue or value, \nand which skills to explore, driven by intrinsic motivation/curiosity and \nsocial learning through natural interaction with peers. Such learning processes \nare incremental, online, and progressive. Human child development involves a \nprogressive increase of complexity in a curriculum of learning where skills are \nexplored, acquired, and built on each other, through particular ordering and \ntiming. Finally, human learning happens in the physical world, and through \nbodily and physical experimentation, under severe constraints on energy, time, \nand computational resources. In the two last decades, the field of \nDevelopmental and Cognitive Robotics (Cangelosi and Schlesinger, 2015, Asada et \nal., 2009), in strong interaction with developmental psychology and \nneuroscience, has achieved significant advances in computational \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394e9d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01626"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hojjat Salehinejad, Shahrokh Valaee, Tim Dowdell, Errol Colak, Joseph Barfett", "title": "Generalization of Deep Neural Networks for Chest Pathology Classification in X-Rays Using Generative Adversarial Networks. (arXiv:1712.01636v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01636", "type": "text/html"}], "timestampUsec": "1512537801473960", "comments": [], "summary": {"content": "<p>Medical datasets are often highly imbalanced, over representing common \nmedical problems, and sparsely representing rare problems. We propose \nsimulation of pathology in images to overcome the above limitations. Using \nchest Xrays as a model medical image, we implement a generative adversarial \nnetwork (GAN) to create artificial images based upon a modest sized labeled \ndataset. We employ a combination of real and artificial images to train a deep \nconvolutional neural network (DCNN) to detect pathology across five classes of \ndisease. We furthermore demonstrate that augmenting the original imbalanced \ndataset with GAN generated images improves performance of chest pathology \nclassification using the proposed DCNN in comparison to the same DCNN trained \nwith the original dataset alone. This improved performance is largely \nattributed to balancing of the dataset using GAN generated images, where image \nclasses that are lacking in example images are preferentially augmented. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394ea3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01636"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Maciej J. Mrowinski, Piotr Fronczak, Agata Fronczak, Marcel Ausloos, Olgica Nedic", "title": "Artificial intelligence in peer review: How can evolutionary computation support journal editors?. (arXiv:1712.01682v1 [cs.DL])", "alternate": [{"href": "http://arxiv.org/abs/1712.01682", "type": "text/html"}], "timestampUsec": "1512537801473959", "comments": [], "summary": {"content": "<p>With the volume of manuscripts submitted for publication growing every year, \nthe deficiencies of peer review (e.g. long review times) are becoming more \napparent. Editorial strategies, sets of guidelines designed to speed up the \nprocess and reduce editors workloads, are treated as trade secrets by \npublishing houses and are not shared publicly. To improve the effectiveness of \ntheir strategies, editors in small publishing groups are faced with undertaking \nan iterative trial-and-error approach. We show that Cartesian Genetic \nProgramming, a nature-inspired evolutionary algorithm, can dramatically improve \neditorial strategies. The artificially evolved strategy reduced the duration of \nthe peer review process by 30%, without increasing the pool of reviewers (in \ncomparison to a typical human-developed strategy). Evolutionary computation has \ntypically been used in technological processes or biological ecosystems. Our \nresults demonstrate that genetic programs can improve real-world social systems \nthat are usually much harder to understand and control than physical systems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394eab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01682"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wellington Pinheiro dos Santos, Francisco Marcos de Assis, Ricardo Emmanuel de Souza, Priscilla B. Mendes, Henrique S. S. Monteiro, Havana Diogo Alves", "title": "Fuzzy-Based Dialectical Non-Supervised Image Classification and Clustering. (arXiv:1712.01694v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01694", "type": "text/html"}], "timestampUsec": "1512537801473958", "comments": [], "summary": {"content": "<p>The materialist dialectical method is a philosophical investigative method to \nanalyze aspects of reality. These aspects are viewed as complex processes \ncomposed by basic units named poles, which interact with each other. Dialectics \nhas experienced considerable progress in the 19th century, with Hegel's \ndialectics and, in the 20th century, with the works of Marx, Engels, and \nGramsci, in Philosophy and Economics. The movement of poles through their \ncontradictions is viewed as a dynamic process with intertwined phases of \nevolution and revolutionary crisis. In order to build a computational process \nbased on dialectics, the interaction between poles can be modeled using fuzzy \nmembership functions. Based on this assumption, we introduce the Objective \nDialectical Classifier (ODC), a non-supervised map for classification based on \nmaterialist dialectics and designed as an extension of fuzzy c-means \nclassifier. As a case study, we used ODC to classify 181 magnetic resonance \nsynthetic multispectral images composed by proton density, $T_1$- and \n$T_2$-weighted synthetic brain images. Comparing ODC to k-means, fuzzy c-means, \nand Kohonen's self-organized maps, concerning with image fidelity indexes as \nestimatives of quantization distortion, we proved that ODC can reach almost the \nsame quantization performance as optimal non-supervised classifiers like \nKohonen's self-organized maps. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394eae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01694"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Higor Neto Lima, Wellington Pinheiro dos Santos, M&#xea;user Jorge Silva Valen&#xe7;a", "title": "Triagem virtual de imagens de imuno-histoqu\\'imica usando redes neurais artificiais e espectro de padr\\~oes. (arXiv:1712.01695v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01695", "type": "text/html"}], "timestampUsec": "1512537801473957", "comments": [], "summary": {"content": "<p>The importance of organizing medical images according to their nature, \napplication and relevance is increasing. Furhermore, a previous selection of \nmedical images can be useful to accelerate the task of analysis by \npathologists. Herein this work we propose an image classifier to integrate a \nCBIR (Content-Based Image Retrieval) selection system. This classifier is based \non pattern spectra and neural networks. Feature selection is performed using \npattern spectra and principal component analysis, whilst image classification \nis based on multilayer perceptrons and a composition of self-organizing maps \nand learning vector quantization. These methods were applied for content \nselection of immunohistochemical images of placenta and newdeads lungs. Results \ndemonstrated that this approach can reach reasonable classification \nperformance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394ebb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01695"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wellington Pinheiro dos Santos, Francisco Marcos de Assis, Ricardo Emmanuel de Souza, Pl&#xed;nio Batista dos Santos Filho, Fernando Buarque de Lima Neto", "title": "Dialectical Multispectral Classification of Diffusion-Weighted Magnetic Resonance Images as an Alternative to Apparent Diffusion Coefficients Maps to Perform Anatomical Analysis. (arXiv:1712.01697v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01697", "type": "text/html"}], "timestampUsec": "1512537801473956", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450511abbf4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450511abbf4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Multispectral image analysis is a relatively promising field of research with \napplications in several areas, such as medical imaging and satellite \nmonitoring. A considerable number of current methods of analysis are based on \nparametric statistics. Alternatively, some methods in Computational \nIntelligence are inspired by biology and other sciences. Here we claim that \nPhilosophy can be also considered as a source of inspiration. This work \nproposes the Objective Dialectical Method (ODM): a method for classification \nbased on the Philosophy of Praxis. ODM is instrumental in assembling evolvable \nmathematical tools to analyze multispectral images. In the case study described \nin this paper, multispectral images are composed of diffusion-weighted (DW) \nmagnetic resonance (MR) images. The results are compared to ground-truth images \nproduced by polynomial networks using a morphological similarity index. The \nclassification results are used to improve the usual analysis of the apparent \ndiffusion coefficient map. Such results proved that gray and white matter can \nbe distinguished in DW-MR multispectral analysis and, consequently, DW-MR \nimages can also be used to furnish anatomical information. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394ec8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01697"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Manuele Rusci, Lukas Cavigelli, Luca Benini", "title": "Design Automation for Binarized Neural Networks: A Quantum Leap Opportunity?. (arXiv:1712.01743v1 [cs.OH])", "alternate": [{"href": "http://arxiv.org/abs/1712.01743", "type": "text/html"}], "timestampUsec": "1512537801473955", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505120e59f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505120e59f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Design automation in general, and in particular logic synthesis, can play a \nkey role in enabling the design of application-specific Binarized Neural \nNetworks (BNN). This paper presents the hardware design and synthesis of a \npurely combinational BNN for ultra-low power near-sensor processing. We \nleverage the major opportunities raised by BNN models, which consist mostly of \nlogical bit-wise operations and integer counting and comparisons, for pushing \nultra-low power deep learning circuits close to the sensor and coupling it with \nbinarized mixed-signal image sensor data. We analyze area, power and energy \nmetrics of BNNs synthesized as combinational networks. Our synthesis results in \nGlobalFoundries 22nm SOI technology shows a silicon area of 2.61mm2 for \nimplementing a combinational BNN with 32x32 binary input sensor receptive field \nand weight parameters fixed at design time. This is 2.2x smaller than a \nsynthesized network with re-configurable parameters. With respect to other \ncomparable techniques for deep learning near-sensor processing, our approach \nfeatures a 10x higher energy efficiency. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394ecd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01743"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Patrick McClure, Nikolaus Kriegeskorte", "title": "Robustly representing inferential uncertainty in deep neural networks through sampling. (arXiv:1611.01639v5 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.01639", "type": "text/html"}], "timestampUsec": "1512537801473954", "comments": [], "summary": {"content": "<p>As deep neural networks (DNNs) are applied to increasingly challenging \nproblems, they will need to be able to represent their own uncertainty. \nModeling uncertainty is one of the key features of Bayesian methods. Using \nBernoulli dropout with sampling at prediction time has recently been proposed \nas an efficient and well performing variational inference method for DNNs. \nHowever, sampling from other multiplicative noise based variational \ndistributions has not been investigated in depth. We evaluated Bayesian DNNs \ntrained with Bernoulli or Gaussian multiplicative masking of either the units \n(dropout) or the weights (dropconnect). We tested the calibration of the \nprobabilistic predictions of Bayesian convolutional neural networks (CNNs) on \nMNIST and CIFAR-10. Sampling at prediction time increased the calibration of \nthe DNNs' probabalistic predictions. Sampling weights, whether Gaussian or \nBernoulli, led to more robust representation of uncertainty compared to \nsampling of units. However, using either Gaussian or Bernoulli dropout led to \nincreased test set classification accuracy. Based on these findings we used \nboth Bernoulli dropout and Gaussian dropconnect concurrently, which we show \napproximates the use of a spike-and-slab variational distribution without \nincreasing the number of learned parameters. We found that spike-and-slab \nsampling had higher test set performance than Gaussian dropconnect and more \nrobustly represented its uncertainty compared to Bernoulli dropout. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394edb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.01639"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yan Duan, Marcin Andrychowicz, Bradly C. Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, Wojciech Zaremba", "title": "One-Shot Imitation Learning. (arXiv:1703.07326v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.07326", "type": "text/html"}], "timestampUsec": "1512537801473953", "comments": [], "summary": {"content": "<p>Imitation learning has been commonly applied to solve different tasks in \nisolation. This usually requires either careful feature engineering, or a \nsignificant number of samples. This is far from what we desire: ideally, robots \nshould be able to learn from very few demonstrations of any given task, and \ninstantly generalize to new situations of the same task, without requiring \ntask-specific engineering. In this paper, we propose a meta-learning framework \nfor achieving such capability, which we call one-shot imitation learning. \n</p> \n<p>Specifically, we consider the setting where there is a very large set of \ntasks, and each task has many instantiations. For example, a task could be to \nstack all blocks on a table into a single tower, another task could be to place \nall blocks on a table into two-block towers, etc. In each case, different \ninstances of the task would consist of different sets of blocks with different \ninitial states. At training time, our algorithm is presented with pairs of \ndemonstrations for a subset of all tasks. A neural net is trained that takes as \ninput one demonstration and the current state (which initially is the initial \nstate of the other demonstration of the pair), and outputs an action with the \ngoal that the resulting sequence of states and actions matches as closely as \npossible with the second demonstration. At test time, a demonstration of a \nsingle instance of a new task is presented, and the neural net is expected to \nperform well on new instances of this new task. The use of soft attention \nallows the model to generalize to conditions and tasks unseen in the training \ndata. We anticipate that by training this model on a much greater variety of \ntasks and settings, we will obtain a general system that can turn any \ndemonstrations into robust policies that can accomplish an overwhelming variety \nof tasks. \n</p> \n<p>Videos available at https://bit.ly/nips2017-oneshot . \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394ee4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.07326"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Peter Bartlett, Dylan J. Foster, Matus Telgarsky", "title": "Spectrally-normalized margin bounds for neural networks. (arXiv:1706.08498v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.08498", "type": "text/html"}], "timestampUsec": "1512537801473952", "comments": [], "summary": {"content": "<p>This paper presents a margin-based multiclass generalization bound for neural \nnetworks that scales with their margin-normalized \"spectral complexity\": their \nLipschitz constant, meaning the product of the spectral norms of the weight \nmatrices, times a certain correction factor. This bound is empirically \ninvestigated for a standard AlexNet network trained with SGD on the mnist and \ncifar10 datasets, with both original and random labels; the bound, the \nLipschitz constants, and the excess risks are all in direct correlation, \nsuggesting both that SGD selects predictors whose complexity scales with the \ndifficulty of the learning task, and secondly that the presented bound is \nsensitive to this complexity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394ef0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.08498"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Juncai Xu, Zhenzhong Shen, Qingwen Ren, Xin Xie, Zhengyu Yang", "title": "Slope Stability Analysis with Geometric Semantic Genetic Programming. (arXiv:1708.09116v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.09116", "type": "text/html"}], "timestampUsec": "1512537801473951", "comments": [], "summary": {"content": "<p>Genetic programming has been widely used in the engineering field. Compared \nwith the conventional genetic programming and artificial neural network, \ngeometric semantic genetic programming (GSGP) is superior in astringency and \ncomputing efficiency. In this paper, GSGP is adopted for the classification and \nregression analysis of a sample dataset. Furthermore, a model for slope \nstability analysis is established on the basis of geometric semantics. \nAccording to the results of the study based on GSGP, the method can analyze \nslope stability objectively and is highly precise in predicting slope stability \nand safety factors. Hence, the predicted results can be used as a reference for \nslope safety design. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394efb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.09116"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yong-Siang Shih, Kai-Yueh Chang, Hsuan-Tien Lin, Min Sun", "title": "Compatibility Family Learning for Item Recommendation and Generation. (arXiv:1712.01262v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01262", "type": "text/html"}], "timestampUsec": "1512537801473950", "comments": [], "summary": {"content": "<p>Compatibility between items, such as clothes and shoes, is a major factor \namong customer's purchasing decisions. However, learning \"compatibility\" is \nchallenging due to (1) broader notions of compatibility than those of \nsimilarity, (2) the asymmetric nature of compatibility, and (3) only a small \nset of compatible and incompatible items are observed. We propose an end-to-end \ntrainable system to embed each item into a latent vector and project a query \nitem into K compatible prototypes in the same space. These prototypes reflect \nthe broad notions of compatibility. We refer to both the embedding and \nprototypes as \"Compatibility Family\". In our learned space, we introduce a \nnovel Projected Compatibility Distance (PCD) function which is differentiable \nand ensures diversity by aiming for at least one prototype to be close to a \ncompatible item, whereas none of the prototypes are close to an incompatible \nitem. We evaluate our system on a toy dataset, two Amazon product datasets, and \nPolyvore outfit dataset. Our method consistently achieves state-of-the-art \nperformance. Finally, we show that we can visualize the candidate compatible \nprototypes using a Metric-regularized Conditional Generative Adversarial \nNetwork (MrCGAN), where the input is a projected prototype and the output is a \ngenerated image of a compatible item. We ask human evaluators to judge the \nrelative compatibility between our generated images and images generated by \nCGANs conditioned directly on query items. Our generated images are \nsignificantly preferred, with roughly twice the number of votes as others. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f14", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01262"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shangtong Zhang, Richard S. Sutton", "title": "A Deeper Look at Experience Replay. (arXiv:1712.01275v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01275", "type": "text/html"}], "timestampUsec": "1512537801473949", "comments": [], "summary": {"content": "<p>Experience replay plays an important role in the success of deep \nreinforcement learning (RL) by helping stabilize the neural networks. It has \nbecome a new norm in deep RL algorithms. In this paper, however, we showcase \nthat varying the size of the experience replay buffer can hurt the performance \neven in very simple tasks. The size of the replay buffer is actually a \nhyper-parameter which needs careful tuning. Moreover, our study of experience \nreplay leads to the formulation of the Combined DQN algorithm, which can \nsignificantly outperform primitive DQN in some tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f21", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01275"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rakshit Agrawal, Anwar Habeeb, Chih-Hsin Hsueh", "title": "Learning User Intent from Action Sequences on Interactive Systems. (arXiv:1712.01328v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.01328", "type": "text/html"}], "timestampUsec": "1512537801473948", "comments": [], "summary": {"content": "<p>Interactive systems have taken over the web and mobile space with increasing \nparticipation from users. Applications across every marketing domain can now be \naccessed through mobile or web where users can directly perform certain actions \nand reach a desired outcome. Actions of user on a system, though, can be \nrepresentative of a certain intent. Ability to learn this intent through user's \nactions can help draw certain insight into the behavior of users on a system. \n</p> \n<p>In this paper, we present models to optimize interactive systems by learning \nand analyzing user intent through their actions on the system. We present a \nfour phased model that uses time-series of interaction actions sequentially \nusing a Long Short-Term Memory (LSTM) based sequence learning system that helps \nbuild a model for intent recognition. Our system then provides an objective \nspecific maximization followed by analysis and contrasting methods in order to \nidentify spaces of improvement in the interaction system. We discuss deployment \nscenarios for such a system and present results from evaluation on an online \nmarketplace using user clickstream data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f28", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01328"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mircea Mironenco, Dana Kianfar, Ke Tran, Evangelos Kanoulas, Efstratios Gavves", "title": "Examining Cooperation in Visual Dialog Models. (arXiv:1712.01329v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01329", "type": "text/html"}], "timestampUsec": "1512537801473947", "comments": [], "summary": {"content": "<p>In this work we propose a blackbox intervention method for visual dialog \nmodels, with the aim of assessing the contribution of individual linguistic or \nvisual components. Concretely, we conduct structured or randomized \ninterventions that aim to impair an individual component of the model, and \nobserve changes in task performance. We reproduce a state-of-the-art visual \ndialog model and demonstrate that our methodology yields surprising insights, \nnamely that both dialog and image information have minimal contributions to \ntask performance. The intervention method presented here can be applied as a \nsanity check for the strength and robustness of each component in visual dialog \nsystems. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f33", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01329"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhiqian Chen, Xuchao Zhang, Arnold P. Boedihardjo, Jing Dai, Chang-Tien Lu", "title": "Multimodal Storytelling via Generative Adversarial Imitation Learning. (arXiv:1712.01455v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.01455", "type": "text/html"}], "timestampUsec": "1512537801473946", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505120e83f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505120e83f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deriving event storylines is an effective summarization method to succinctly \norganize extensive information, which can significantly alleviate the pain of \ninformation overload. The critical challenge is the lack of widely recognized \ndefinition of storyline metric. Prior studies have developed various approaches \nbased on different assumptions about users' interests. These works can extract \ninteresting patterns, but their assumptions do not guarantee that the derived \npatterns will match users' preference. On the other hand, their exclusiveness \nof single modality source misses cross-modality information. This paper \nproposes a method, multimodal imitation learning via generative adversarial \nnetworks(MIL-GAN), to directly model users' interests as reflected by various \ndata. In particular, the proposed model addresses the critical challenge by \nimitating users' demonstrated storylines. Our proposed model is designed to \nlearn the reward patterns given user-provided storylines and then applies the \nlearned policy to unseen data. The proposed approach is demonstrated to be \ncapable of acquiring the user's implicit intent and outperforming competing \nmethods by a substantial margin with a user study. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f3f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01455"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tomer Libal (Inria, Paris), Xaviera Steele (American University of Paris)", "title": "Determinism in the Certification of UNSAT Proofs. (arXiv:1712.01488v1 [cs.LO])", "alternate": [{"href": "http://arxiv.org/abs/1712.01488", "type": "text/html"}], "timestampUsec": "1512537801473945", "comments": [], "summary": {"content": "<p>The search for increased trustworthiness of SAT solvers is very active and \nuses various methods. Some of these methods obtain a proof from the provers \nthen check it, normally by replicating the search based on the proof's \ninformation. Because the certification process involves another nontrivial \nproof search, the trust we can place in it is decreased. Some attempts to amend \nthis use certifiers which have been verified by proofs assistants such as \nIsabelle/HOL and Coq. Our approach is different because it is based on an \nextremely simplified certifier. This certifier enjoys a very high level of \ntrust but is very inefficient. In this paper, we experiment with this approach \nand conclude that by placing some restrictions on the formats, one can mostly \neliminate the need for search and in principle, can certify proofs of arbitrary \nsize. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01488"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Qingxiang Feng, Yicong Zhou", "title": "Discriminant Projection Representation-based Classification for Vision Recognition. (arXiv:1712.01643v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01643", "type": "text/html"}], "timestampUsec": "1512537801473944", "comments": [], "summary": {"content": "<p>Representation-based classification methods such as sparse \nrepresentation-based classification (SRC) and linear regression classification \n(LRC) have attracted a lot of attentions. In order to obtain the better \nrepresentation, a novel method called projection representation-based \nclassification (PRC) is proposed for image recognition in this paper. PRC is \nbased on a new mathematical model. This model denotes that the 'ideal \nprojection' of a sample point $x$ on the hyper-space $H$ may be gained by \niteratively computing the projection of $x$ on a line of hyper-space $H$ with \nthe proper strategy. Therefore, PRC is able to iteratively approximate the \n'ideal representation' of each subject for classification. Moreover, the \ndiscriminant PRC (DPRC) is further proposed, which obtains the discriminant \ninformation by maximizing the ratio of the between-class reconstruction error \nover the within-class reconstruction error. Experimental results on five \ntypical databases show that the proposed PRC and DPRC are effective and \noutperform other state-of-the-art methods on several vision recognition tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f50", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01643"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shun Miao, Sebastien Piat, Peter Fischer, Ahmet Tuysuzoglu, Philip Mewes, Tommaso Mansi, Rui Liao", "title": "Dilated FCN for Multi-Agent 2D/3D Medical Image Registration. (arXiv:1712.01651v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01651", "type": "text/html"}], "timestampUsec": "1512537801473943", "comments": [], "summary": {"content": "<p>2D/3D image registration to align a 3D volume and 2D X-ray images is a \nchallenging problem due to its ill-posed nature and various artifacts presented \nin 2D X-ray images. In this paper, we propose a multi-agent system with an auto \nattention mechanism for robust and efficient 2D/3D image registration. \nSpecifically, an individual agent is trained with dilated Fully Convolutional \nNetwork (FCN) to perform registration in a Markov Decision Process (MDP) by \nobserving a local region, and the final action is then taken based on the \nproposals from multiple agents and weighted by their corresponding confidence \nlevels. The contributions of this paper are threefold. First, we formulate \n2D/3D registration as a MDP with observations, actions, and rewards properly \ndefined with respect to X-ray imaging systems. Second, to handle various \nartifacts in 2D X-ray images, multiple local agents are employed efficiently \nvia FCN-based structures, and an auto attention mechanism is proposed to favor \nthe proposals from regions with more reliable visual cues. Third, a dilated \nFCN-based training mechanism is proposed to significantly reduce the Degree of \nFreedom in the simulation of registration environment, and drastically improve \ntraining efficiency by an order of magnitude compared to standard CNN-based \ntraining method. We demonstrate that the proposed method achieves high \nrobustness on both spine cone beam Computed Tomography data with a low \nsignal-to-noise ratio and data from minimally invasive spine surgery where \nsevere image artifacts and occlusions are presented due to metal screws and \nguide wires, outperforming other state-of-the-art methods (single agent-based \nand optimization-based) by a large margin. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f54", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01651"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Siyu Yu, Nanning Zheng, Yongqiang Ma, Hao Wu, Badong Chen", "title": "A Novel Brain Decoding Method: a Correlation Network Framework for Revealing Brain Connections. (arXiv:1712.01668v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01668", "type": "text/html"}], "timestampUsec": "1512537801473942", "comments": [], "summary": {"content": "<p>Brain decoding is a hot spot in cognitive science, which focuses on \nreconstructing perceptual images from brain activities. Analyzing the \ncorrelations of collected data from human brain activities and representing \nactivity patterns are two problems in brain decoding based on functional \nmagnetic resonance imaging (fMRI) signals. However, existing correlation \nanalysis methods mainly focus on the strength information of voxel, which \nreveals functional connectivity in the cerebral cortex. They tend to neglect \nthe structural information that implies the intracortical or intrinsic \nconnections; that is, structural connectivity. Hence, the effective \nconnectivity inferred by these methods is relatively unilateral. Therefore, we \nproposed a correlation network (CorrNet) framework that could be flexibly \ncombined with diverse pattern representation models. In the CorrNet framework, \nthe topological correlation was introduced to reveal structural information. \nRich correlations were obtained, which contributed to specifying the underlying \neffective connectivity. We also combined the CorrNet framework with a linear \nsupport vector machine (SVM) and a dynamic evolving spike neuron network (SNN) \nfor pattern representation separately, thus providing a novel method for \ndecoding cognitive activity patterns. Experimental results verified the \nreliability and robustness of our CorrNet framework and demonstrated that the \nnew method achieved significant improvement in brain decoding over comparable \nmethods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f5b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01668"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis", "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm. (arXiv:1712.01815v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.01815", "type": "text/html"}], "timestampUsec": "1512537801473941", "comments": [], "summary": {"content": "<p>The game of chess is the most widely-studied domain in the history of \nartificial intelligence. The strongest programs are based on a combination of \nsophisticated search techniques, domain-specific adaptations, and handcrafted \nevaluation functions that have been refined by human experts over several \ndecades. In contrast, the AlphaGo Zero program recently achieved superhuman \nperformance in the game of Go, by tabula rasa reinforcement learning from games \nof self-play. In this paper, we generalise this approach into a single \nAlphaZero algorithm that can achieve, tabula rasa, superhuman performance in \nmany challenging domains. Starting from random play, and given no domain \nknowledge except the game rules, AlphaZero achieved within 24 hours a \nsuperhuman level of play in the games of chess and shogi (Japanese chess) as \nwell as Go, and convincingly defeated a world-champion program in each case. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f5f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01815"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Marko Horvat, Anton Grbin, Gordan Gledec", "title": "WNtags: A Web-Based Tool For Image Labeling And Retrieval With Lexical Ontologies. (arXiv:1302.2223v2 [cs.IR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1302.2223", "type": "text/html"}], "timestampUsec": "1512537801473940", "comments": [], "summary": {"content": "<p>Ever growing number of image documents available on the Internet continuously \nmotivates research in better annotation models and more efficient retrieval \nmethods. Formal knowledge representation of objects and events in pictures, \ntheir interaction as well as context complexity becomes no longer an option for \na quality image repository, but a necessity. We present an ontology-based \nonline image annotation tool WNtags and demonstrate its usefulness in several \ntypical multimedia retrieval tasks using International Affective Picture System \nemotionally annotated image database. WNtags is built around WordNet lexical \nontology but considers Suggested Upper Merged Ontology as the preferred \nlabeling formalism. WNtags uses sets of weighted WordNet synsets as high-level \nimage semantic descriptors and query matching is performed with word stemming \nand node distance metrics. We also elaborate our near future plans to expand \nimage content description with induced affect as in stimuli for research of \nhuman emotion and attention. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f6b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1302.2223"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ivan Brugere, Brian Gallagher, Tanya Y. Berger-Wolf", "title": "Network Structure Inference, A Survey: Motivations, Methods, and Applications. (arXiv:1610.00782v3 [cs.SI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1610.00782", "type": "text/html"}], "timestampUsec": "1512537801473939", "comments": [], "summary": {"content": "<p>Networks represent relationships between entities in many complex systems, \nspanning from online social interactions to biological cell development and \nbrain connectivity. In many cases, relationships between entities are \nunambiguously known: are two users 'friends' in a social network? Do two \nresearchers collaborate on a published paper? Do two road segments in a \ntransportation system intersect? These are directly observable in the system in \nquestion. In most cases, relationship between nodes are not directly observable \nand must be inferred: does one gene regulate the expression of another? Do two \nanimals who physically co-locate have a social bond? Who infected whom in a \ndisease outbreak in a population? Existing approaches for inferring networks \nfrom data are found across many application domains, and use specialized \nknowledge to infer and measure the quality of inferred network for a specific \ntask or hypothesis. However, current research lacks a rigorous methodology \nwhich employs standard statistical validation on inferred models. In this \nsurvey, we examine (1) how network representations are constructed from \nunderlying data, (2) the variety of questions and tasks on these \nrepresentations over several domains, and (3) validation strategies for \nmeasuring the inferred network's capability of answering questions on the \nsystem of interest. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f7b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1610.00782"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman, Filip De Turck, Pieter Abbeel", "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning. (arXiv:1611.04717v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.04717", "type": "text/html"}], "timestampUsec": "1512537801473938", "comments": [], "summary": {"content": "<p>Count-based exploration algorithms are known to perform near-optimally when \nused in conjunction with tabular reinforcement learning (RL) methods for \nsolving small discrete Markov decision processes (MDPs). It is generally \nthought that count-based methods cannot be applied in high-dimensional state \nspaces, since most states will only occur once. Recent deep RL exploration \nstrategies are able to deal with high-dimensional continuous state spaces \nthrough complex heuristics, often relying on optimism in the face of \nuncertainty or intrinsic motivation. In this work, we describe a surprising \nfinding: a simple generalization of the classic count-based approach can reach \nnear state-of-the-art performance on various high-dimensional and/or continuous \ndeep RL benchmarks. States are mapped to hash codes, which allows to count \ntheir occurrences with a hash table. These counts are then used to compute a \nreward bonus according to the classic count-based exploration theory. We find \nthat simple hash functions can achieve surprisingly good results on many \nchallenging tasks. Furthermore, we show that a domain-dependent learned hash \ncode may further improve these results. Detailed analysis reveals important \naspects of a good hash function: 1) having appropriate granularity and 2) \nencoding information relevant to solving the MDP. This exploration strategy \nachieves near state-of-the-art performance on both continuous control tasks and \nAtari 2600 games, hence providing a simple yet powerful baseline for solving \nMDPs that require considerable exploration. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f83", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.04717"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anagha Kulkarni, Yantian Zha, Tathagata Chakraborti, Satya Gautam Vadlamudi, Yu Zhang, Subbarao Kambhampati", "title": "Explicablility as Minimizing Distance from Expected Behavior. (arXiv:1611.05497v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.05497", "type": "text/html"}], "timestampUsec": "1512537801473937", "comments": [], "summary": {"content": "<p>In order to have effective human AI collaboration, it is not simply enough to \naddress the question of autonomy; an equally important question is, how the \nAI's behavior is being perceived by their human counterparts. When AI agent's \ntask plans are generated without such considerations, they may often \ndemonstrate inexplicable behavior from the human's point of view. This problem \narises due to the human's partial or inaccurate understanding of the agent's \nplanning process and/or the model. This may have serious implications on \nhuman-AI collaboration, from increased cognitive load and reduced trust in the \nagent, to more serious concerns of safety in interactions with physical agent. \nIn this paper, we address this issue by modeling the notion of plan \nexplicability as a function of the distance between a plan that agent makes and \nthe plan that human expects it to make. To this end, we learn a distance \nfunction based on different plan distance measures that can accurately model \nthis notion of plan explicability, and develop an anytime search algorithm that \ncan use this distance as a heuristic to come up with progressively explicable \nplans. We evaluate the effectiveness of our approach in a simulated autonomous \ncar domain and a physical service robot domain. We provide empirical \nevaluations that demonstrate the usefulness of our approach in making the \nplanning process of an autonomous agent conform to human expectations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394f92", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.05497"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "TonTon Hsien-De Huang, Hung-Yu Kao", "title": "R2-D2: ColoR-inspired Convolutional NeuRal Network (CNN)-based AndroiD Malware Detections. (arXiv:1705.04448v4 [cs.CR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.04448", "type": "text/html"}], "timestampUsec": "1512537801473936", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505120ea92\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505120ea92&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Machine Learning (ML) has found it particularly useful in malware detection. \nHowever, as the malware evolves very fast, the stability of the feature \nextracted from malware serves as a critical issue in malware detection. Recent \nsuccess of deep learning in image recognition, natural language processing, and \nmachine translation indicate a potential solution for stabilizing the malware \ndetection effectiveness. We present a coloR-inspired convolutional neuRal \nnetwork-based AndroiD malware Detection (R2-D2), which can detect malware \nwithout extracting pre-selected features (e.g., the control-flow of op-code, \nclasses, methods of functions and the timing they are invoked etc.) from \nAndroid apps. In particular, we develop a color representation for translating \nAndroid apps into RGB color code and transform them to a fixed-sized encoded \nimage. After that, the encoded image is fed to convolutional neural network for \nautomatic feature extraction and learning, reducing the expert's intervention. \nWe have collected over 1 million malware samples and 1 million benign samples \naccording to the data provided by Leopard Mobile Inc. from its core product \nSecurity Master (which has 623 million monthly active users and 10k new malware \nsamples per day). It is shown that R2-D2 can effectively detect the malware. \nFurthermore, we keep our research results and release experiment material on \n<a href=\"http://R2D2.TWMAN.ORG\">this http URL</a> if there is any update. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fa3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.04448"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yihui He, Xiaobo Ma, Xiapu Luo, Jianfeng Li, Mengchen Zhao, Bo An, Xiaohong Guan", "title": "Vehicle Traffic Driven Camera Placement for Better Metropolis Security Surveillance. (arXiv:1705.08508v3 [cs.CY] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08508", "type": "text/html"}], "timestampUsec": "1512537801473935", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450512617c4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450512617c4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Security surveillance is one of the most important issues in smart cities, \nespecially in an era of terrorism. Deploying a number of (video) cameras is a \ncommon surveillance approach. Given the never-ending power offered by vehicles \nto metropolises, exploiting vehicle traffic to design camera placement \nstrategies could potentially facilitate security surveillance. This article \nconstitutes the first effort toward building the linkage between vehicle \ntraffic and security surveillance, which is a critical problem for smart \ncities. We expect our study could influence the decision making of surveillance \ncamera placement, and foster more research of principled ways of security \nsurveillance beneficial to our physical-world life. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08508"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Abhishek Kumar, Prasanna Sattigeri, P. Thomas Fletcher", "title": "Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference. (arXiv:1705.08850v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08850", "type": "text/html"}], "timestampUsec": "1512537801473934", "comments": [], "summary": {"content": "<p>Semi-supervised learning methods using Generative Adversarial Networks (GANs) \nhave shown promising empirical success recently. Most of these methods use a \nshared discriminator/classifier which discriminates real examples from fake \nwhile also predicting the class label. Motivated by the ability of the GANs \ngenerator to capture the data manifold well, we propose to estimate the tangent \nspace to the data manifold using GANs and employ it to inject invariances into \nthe classifier. In the process, we propose enhancements over existing methods \nfor learning the inverse mapping (i.e., the encoder) which greatly improves in \nterms of semantic similarity of the reconstructed sample with the input sample. \nWe observe considerable empirical gains in semi-supervised learning over \nbaselines, particularly in the cases when the number of labeled examples is \nlow. We also provide insights into how fake examples influence the \nsemi-supervised learning procedure. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fb5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08850"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513228699, "author": "Haoshu Fang, Yuanlu Xu, Wenguan Wang, Xiaobai Liu, Song-Chun Zhu", "title": "Learning Pose Grammar to Encode Human Body Configuration for 3D Pose Estimation. (arXiv:1710.06513v5 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.06513", "type": "text/html"}], "timestampUsec": "1512537801473933", "comments": [], "summary": {"content": "<p>In this paper, we propose a pose grammar to tackle the problem of 3D human \npose estimation. Our model directly takes 2D pose as input and learns a \ngeneralized 2D-3D mapping function. The proposed model consists of a base \nnetwork which efficiently captures pose-aligned features and a hierarchy of \nBi-directional RNNs (BRNN) on the top to explicitly incorporate a set of \nknowledge regarding human body configuration (i.e., kinematics, symmetry, motor \ncoordination). The proposed model thus enforces high-level constraints over \nhuman poses. In learning, we develop a pose sample simulator to augment \ntraining samples in virtual camera views, which further improves our model \ngeneralizability. We validate our method on public 3D human pose benchmarks and \npropose a new evaluation protocol working on cross-view setting to verify the \ngeneralization capability of different methods. We empirically observe that \nmost state-of-the-art methods encounter difficulty under such setting while our \nmethod can well handle such challenges. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1513228697, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fb8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.06513"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Cyprien Gilet, Marie Deprez, Jean-Baptiste Caillau, Michel Barlaud", "title": "Clustering with feature selection using alternating minimization, Application to computational biology. (arXiv:1711.02974v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.02974", "type": "text/html"}], "timestampUsec": "1512537801473932", "comments": [], "summary": {"content": "<p>This paper deals with unsupervised clustering with feature selection. The \nproblem is to estimate both labels and a sparse projection matrix of weights. \nTo address this combinatorial non-convex problem maintaining a strict control \non the sparsity of the matrix of weights, we propose an alternating \nminimization of the Frobenius norm criterion. We provide a new efficient \nalgorithm named K-sparse which alternates k-means with projection-gradient \nminimization. The projection-gradient step is a method of splitting type, with \nexact projection on the $\\ell^1$ ball to promote sparsity. The convergence of \nthe gradient-projection step is addressed, and a preliminary analysis of the \nalternating minimization is made. The Frobenius norm criterion converges as the \nnumber of iterates in Algorithm K-sparse goes to infinity. Experiments on \nSingle Cell RNA sequencing datasets show that our method significantly improves \nthe results of PCA k-means, spectral clustering, SIMLR, and Sparcl methods, and \nachieves a relevant selection of genes. The complexity of K-sparse is linear in \nthe number of samples (cells), so that the method scales up to large datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fc1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.02974"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Igor Melnyk, Cicero Nogueira dos Santos, Kahini Wadhawan, Inkit Padhi, Abhishek Kumar", "title": "Improved Neural Text Attribute Transfer with Non-parallel Data. (arXiv:1711.09395v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09395", "type": "text/html"}], "timestampUsec": "1512537801473931", "comments": [], "summary": {"content": "<p>Text attribute transfer using non-parallel data requires methods that can \nperform disentanglement of content and linguistic attributes. In this work, we \npropose multiple improvements over the existing approaches that enable the \nencoder-decoder framework to cope with the text attribute transfer from \nnon-parallel data. We perform experiments on the sentiment transfer task using \ntwo datasets. For both datasets, our proposed method outperforms a strong \nbaseline in two of the three employed evaluation metrics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fc6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09395"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christos Louizos, Max Welling, Diederik P. Kingma", "title": "Learning Sparse Neural Networks through $L_0$ Regularization. (arXiv:1712.01312v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01312", "type": "text/html"}], "timestampUsec": "1512537801473928", "comments": [], "summary": {"content": "<p>We propose a practical method for $L_0$ norm regularization for neural \nnetworks: pruning the network during training by encouraging weights to become \nexactly zero. Such regularization is interesting since (1) it can greatly speed \nup training and inference, and (2) it can improve generalization. AIC and BIC, \nwell-known model selection criteria, are special cases of $L_0$ regularization. \nHowever, since the $L_0$ norm of weights is non-differentiable, we cannot \nincorporate it directly as a regularization term in the objective function. We \npropose a solution through the inclusion of a collection of non-negative \nstochastic gates, which collectively determine which weights to set to zero. We \nshow that, somewhat surprisingly, for certain distributions over the gates, the \nexpected $L_0$ norm of the resulting gated weights is differentiable with \nrespect to the distribution parameters. We further propose the \\emph{hard \nconcrete} distribution for the gates, which is obtained by \"stretching\" a \nbinary concrete distribution and then transforming its samples with a \nhard-sigmoid. The parameters of the distribution over the gates can then be \njointly optimized with the original network parameters. As a result our method \nallows for straightforward and efficient learning of model structures with \nstochastic gradient descent and allows for conditional computation in a \nprincipled way. We perform various experiments to demonstrate the effectiveness \nof the resulting approach and regularizer. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fd3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01312"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Katja Ried, Thomas M&#xfc;ller, Hans J. Briegel", "title": "Modelling collective motion based on the principle of agency. (arXiv:1712.01334v1 [q-bio.PE])", "alternate": [{"href": "http://arxiv.org/abs/1712.01334", "type": "text/html"}], "timestampUsec": "1512537801473927", "comments": [], "summary": {"content": "<p>Collective motion is an intriguing phenomenon, especially considering that it \narises from a set of simple rules governing local interactions between \nindividuals. In theoretical models, these rules are normally \\emph{assumed} to \ntake a particular form, possibly constrained by heuristic arguments. We propose \na new class of models, which describe the individuals as \\emph{agents}, capable \nof deciding for themselves how to act and learning from their experiences. The \nlocal interaction rules do not need to be postulated in this model, since they \n\\emph{emerge} from the learning process. We apply this ansatz to a concrete \nscenario involving marching locusts, in order to model the phenomenon of \ndensity-dependent alignment. We show that our learning agent-based model can \naccount for a Fokker-Planck equation that describes the collective motion and, \nmost notably, that the agents can learn the appropriate local interactions, \nrequiring no strong previous assumptions on their form. These results suggest \nthat learning agent-based models are a powerful tool for studying a broader \nclass of problems involving collective motion and animal agency in general. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fd7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01334"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Samuel E. Otto, Clarence W. Rowley", "title": "Linearly-Recurrent Autoencoder Networks for Learning Dynamics. (arXiv:1712.01378v1 [math.DS])", "alternate": [{"href": "http://arxiv.org/abs/1712.01378", "type": "text/html"}], "timestampUsec": "1512537801473926", "comments": [], "summary": {"content": "<p>This paper describes a method for learning low-dimensional approximations of \nnonlinear dynamical systems, based on neural-network approximations of the \nunderlying Koopman operator. Extended Dynamic Mode Decomposition (EDMD) \nprovides a useful data-driven approximation of the Koopman operator for \nanalyzing dynamical systems. This paper addresses a fundamental problem \nassociated with EDMD: a trade-off between representational capacity of the \ndictionary and over-fitting due to insufficient data. A new neural network \narchitecture combining an autoencoder with linear recurrent dynamics in the \nencoded state is used to learn a low-dimensional and highly informative \nKoopman-invariant subspace of observables. A method is also presented for \nbalanced model reduction of over-specified EDMD systems in feature space. \nNonlinear reconstruction using partially linear multi-kernel regression aims to \nimprove reconstruction accuracy from the low-dimensional state when the data \nhas complex but intrinsically low-dimensional structure. The techniques \ndemonstrate the ability to identify Koopman eigenfunctions of the unforced \nDuffing equation, create accurate low-dimensional models of an unstable \ncylinder wake flow, and make short-time predictions of the chaotic \nKuramoto-Sivashinsky equation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fdd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01378"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shubhanshu Shekhar, Tara Javidi", "title": "Gaussian Process bandits with adaptive discretization. (arXiv:1712.01447v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01447", "type": "text/html"}], "timestampUsec": "1512537801473925", "comments": [], "summary": {"content": "<p>In this paper, the problem of maximizing a black-box function $f:\\mathcal{X} \n\\to \\mathbb{R}$ is studied in the Bayesian framework with a Gaussian Process \n(GP) prior. In particular, a new algorithm for this problem is proposed, and \nhigh probability bounds on its simple and cumulative regret are established. \nThe query point selection rule in most existing methods involves an exhaustive \nsearch over an increasingly fine sequence of uniform discretizations of \n$\\mathcal{X}$. The proposed algorithm, in contrast, adaptively refines \n$\\mathcal{X}$ which leads to a lower computational complexity, particularly \nwhen $\\mathcal{X}$ is a subset of a high dimensional Euclidean space. In \naddition to the computational gains, sufficient conditions are identified under \nwhich the regret bounds of the new algorithm improve upon the known results. \nFinally an extension of the algorithm to the case of contextual bandits is \nproposed, and high probability bounds on the contextual regret are presented. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394fe4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01447"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Thomas Laurent, James von Brecht", "title": "Deep linear neural networks with arbitrary loss: All local minima are global. (arXiv:1712.01473v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01473", "type": "text/html"}], "timestampUsec": "1512537801473924", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45051261a31\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45051261a31&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We consider deep linear networks with arbitrary differentiable loss. We \nprovide a short and elementary proof of the following fact: all local minima \nare global minima if each hidden layer is wider than either the input or output \nlayer. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394feb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01473"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhanli Chen, Rashid Ansari, Diana J. Wilkie", "title": "Learning Pain from Action Unit Combinations: A Weakly Supervised Approach via Multiple Instance Learning. (arXiv:1712.01496v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01496", "type": "text/html"}], "timestampUsec": "1512537801473923", "comments": [], "summary": {"content": "<p>Facial pain expression is an important modality for assessing pain, \nespecially when a patient's verbal ability to communicate is impaired. A set of \neight facial muscle based action units (AUs), which are defined by the Facial \nAction Coding System (FACS), have been widely studied and are highly reliable \nfor pain detection through facial expressions. However, using FACS is a very \ntime consuming task that makes its clinical use prohibitive. An automated \nfacial expression recognition system (AFER) reliably detecting pain-related AUs \nwould be highly beneficial for efficient and practical pain monitoring. \nAutomated pain detection under clinical settings is viewed as a weakly \nsupervised problem, which is not suitable general AFER system that trained on \nwell labeled data. Existing pain oriented AFER research either focus on the \nindividual pain-related AU recognition or bypassing the AU detection procedure \nby training a binary pain classifier from pain intensity data. In this paper, \nwe decouple pain detection into two consecutive tasks: the AFER based AU \nlabeling at video frame level and a probabilistic measure of pain at sequence \nlevel from AU combination scores. Our work is distinguished in the following \naspects, 1) State of the art AFER tools Emotient is applied on pain oriented \ndata sets for single AU labeling. 2) Two different data structures are proposed \nto encode AU combinations from single AU scores, which forms low-dimensional \nfeature vectors for the learning framework. 3) Two weakly supervised learning \nframeworks namely multiple instance learning and multiple clustered instance \nlearning are employed corresponding to each feature structure to learn pain \nfrom video sequences. The results shows 87% pain recognition accuracy with 0.94 \nAUC on UNBC-McMaster dataset. Tests on Wilkie's dataset suggests the potential \nvalue of the proposed system for pain monitoring task under clinical settings. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341394ff0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01496"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wei Xiao", "title": "An Online Algorithm for Nonparametric Correlations. (arXiv:1712.01521v1 [stat.AP])", "alternate": [{"href": "http://arxiv.org/abs/1712.01521", "type": "text/html"}], "timestampUsec": "1512537801473922", "comments": [], "summary": {"content": "<p>Nonparametric correlations such as Spearman's rank correlation and Kendall's \ntau correlation are widely applied in scientific and engineering fields. This \npaper investigates the problem of computing nonparametric correlations on the \nfly for streaming data. Standard batch algorithms are generally too slow to \nhandle real-world big data applications. They also require too much memory \nbecause all the data need to be stored in the memory before processing. This \npaper proposes a novel online algorithm for computing nonparametric \ncorrelations. The algorithm has O(1) time complexity and O(1) memory cost and \nis quite suitable for edge devices, where only limited memory and processing \npower are available. You can seek a balance between speed and accuracy by \nchanging the number of cutpoints specified in the algorithm. The online \nalgorithm can compute the nonparametric correlations 10 to 1,000 times faster \nthan the corresponding batch algorithm, and it can compute them based either on \nall past observations or on fixed-size sliding windows. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034139500b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01521"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zhiwu Huang, Jiqing Wu, Luc Van Gool", "title": "Manifold-valued Image Generation with Wasserstein Adversarial Networks. (arXiv:1712.01551v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01551", "type": "text/html"}], "timestampUsec": "1512537801473921", "comments": [], "summary": {"content": "<p>Unsupervised image generation has recently received an increasing amount of \nattention thanks to the great success of generative adversarial networks \n(GANs), particularly Wasserstein GANs. Inspired by the paradigm of real-valued \nimage generation, this paper makes the first attempt to formulate the problem \nof generating manifold-valued images, which are frequently encountered in \nreal-world applications. For the study, we specially exploit three typical \nmanifold-valued image generation tasks: hue-saturation-value (HSV) color image \ngeneration, chromaticity-brightness (CB) color image generation, and \ndiffusion-tensor (DT) image generation. In order to produce such kinds of \nimages as realistic as possible, we generalize the state-of-the-art technique \nof Wasserstein GANs to the manifold context with exploiting Riemannian \ngeometry. For the proposed manifold-valued image generation problem, we \nrecommend three benchmark datasets that are CIFAR-10 HSV/CB color images, \nImageNet HSV/CB color images, UCL DT image datasets. On the three datasets, we \nexperimentally demonstrate the proposed manifold-aware Wasserestein GAN can \ngenerate high quality manifold-valued images. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034139506d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01551"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Stefan Klus, Ingmar Schuster, Krikamol Muandet", "title": "Eigendecompositions of Transfer Operators in Reproducing Kernel Hilbert Spaces. (arXiv:1712.01572v1 [math.DS])", "alternate": [{"href": "http://arxiv.org/abs/1712.01572", "type": "text/html"}], "timestampUsec": "1512537801473920", "comments": [], "summary": {"content": "<p>Transfer operators such as the Perron-Frobenius or Koopman operator play an \nimportant role in the global analysis of complex dynamical systems. The \neigenfunctions of these operators can be used to detect metastable sets, to \nproject the dynamics onto the dominant slow processes, or to separate \nsuperimposed signals. We extend transfer operator theory to reproducing kernel \nHilbert spaces and show that these operators are related to Hilbert space \nrepresentations of conditional distributions, known as conditional mean \nembeddings in the machine learning community. Moreover, numerical methods to \ncompute empirical estimates of these embeddings are akin to data-driven methods \nfor the approximation of transfer operators such as extended dynamic mode \ndecomposition and its variants. In fact, most of the existing methods can be \nderived from our framework, providing a unifying view on the approximation of \ntransfer operators. One main benefit of the presented kernel-based approaches \nis that these methods can be applied to any domain where a similarity measure \ngiven by a kernel is available. We illustrate the results with the aid of \nguiding examples and highlight potential applications in molecular dynamics as \nwell as video and text data analysis. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341395083", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01572"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David Janz, Jos van der Westhuizen, Brooks Paige, Matt J. Kusner, Jose Miguel Hernandez-Labato", "title": "Learning a Generative Model for Validity in Complex Discrete Structures. (arXiv:1712.01664v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01664", "type": "text/html"}], "timestampUsec": "1512537801473919", "comments": [], "summary": {"content": "<p>Deep generative models have been successfully used to learn representations \nfor high-dimensional discrete spaces by representing discrete objects as \nsequences, for which powerful sequence-based deep models can be employed. \nUnfortunately, these techniques are significantly hindered by the fact that \nthese generative models often produce invalid sequences: sequences which do not \nrepresent any underlying discrete structure. As a step towards solving this \nproblem, we propose to learn a deep recurrent validator model, which can \nestimate whether a partial sequence can function as the beginning of a full, \nvalid sequence. This model not only discriminates between valid and invalid \nsequences, but also provides insight as to how individual sequence elements \ninfluence the validity of the overall sequence, and the existence of a \ncorresponding discrete object. To learn this model we propose a reinforcement \nlearning approach, where an oracle which can evaluate validity of complete \nsequences provides a sparse reward signal. We believe this is a key step toward \nlearning generative models that faithfully produce valid sequences which \nrepresent discrete objects. We demonstrate its effectiveness in evaluating the \nvalidity of Python 3 source code for mathematical expressions, and improving \nthe ability of a variational autoencoder trained on SMILES strings to decode \nvalid molecular structures. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034139509e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01664"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Beyza Ermis, Ali Taylan Cemgil", "title": "Differentially Private Dropout. (arXiv:1712.01665v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01665", "type": "text/html"}], "timestampUsec": "1512537801473918", "comments": [], "summary": {"content": "<p>Large data collections required for the training of neural networks often \ncontain sensitive information such as the medical histories of patients, and \nthe privacy of the training data must be preserved. In this paper, we introduce \na dropout technique that provides an elegant Bayesian interpretation to \ndropout, and show that the intrinsic noise added, with the primary goal of \nregularization, can be exploited to obtain a degree of differential privacy. \nThe iterative nature of training neural networks presents a challenge for \nprivacy-preserving estimation since multiple iterations increase the amount of \nnoise added. We overcome this by using a relaxed notion of differential \nprivacy, called concentrated differential privacy, which provides tighter \nestimates on the overall privacy loss. We demonstrate the accuracy of our \nprivacy-preserving dropout algorithm on benchmark datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003413950b2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01665"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jos&#xe9; Lezama, Qiang Qiu, Pablo Mus&#xe9;, Guillermo Sapiro", "title": "OL\\'E: Orthogonal Low-rank Embedding, A Plug and Play Geometric Loss for Deep Learning. (arXiv:1712.01727v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.01727", "type": "text/html"}], "timestampUsec": "1512537801473917", "comments": [], "summary": {"content": "<p>Deep neural networks trained using a softmax layer at the top and the \ncross-entropy loss are ubiquitous tools for image classification. Yet, this \ndoes not naturally enforce intra-class similarity nor inter-class margin of the \nlearned deep representations. To simultaneously achieve these two goals, \ndifferent solutions have been proposed in the literature, such as the pairwise \nor triplet losses. However, such solutions carry the extra task of selecting \npairs or triplets, and the extra computational burden of computing and learning \nfor many combinations of them. In this paper, we propose a plug-and-play loss \nterm for deep networks that explicitly reduces intra-class variance and \nenforces inter-class margin simultaneously, in a simple and elegant geometric \nmanner. For each class, the deep features are collapsed into a learned linear \nsubspace, or union of them, and inter-class subspaces are pushed to be as \northogonal as possible. Our proposed Orthogonal Low-rank Embedding (OL\\'E) does \nnot require carefully crafting pairs or triplets of samples for training, and \nworks standalone as a classification loss, being the first reported deep metric \nlearning framework of its kind. Because of the improved margin between features \nof different classes, the resulting deep networks generalize better, are more \ndiscriminative, and more robust. We demonstrate improved classification \nperformance in general object recognition, plugging the proposed loss term into \nexisting off-the-shelf architectures. In particular, we show the advantage of \nthe proposed loss in the small data/model scenario, and we significantly \nadvance the state-of-the-art on the Stanford STL-10 benchmark. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003413950bc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01727"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tara N. Sainath, Chung-Cheng Chiu, Rohit Prabhavalkar, Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Zhifeng Chen", "title": "Improving the Performance of Online Neural Transducer Models. (arXiv:1712.01807v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.01807", "type": "text/html"}], "timestampUsec": "1512537801473916", "comments": [], "summary": {"content": "<p>Having a sequence-to-sequence model which can operate in an online fashion is \nimportant for streaming applications such as Voice Search. Neural transducer is \na streaming sequence-to-sequence model, but has shown a significant degradation \nin performance compared to non-streaming models such as Listen, Attend and \nSpell (LAS). In this paper, we present various improvements to NT. \nSpecifically, we look at increasing the window over which NT computes \nattention, mainly by looking backwards in time so the model still remains \nonline. In addition, we explore initializing a NT model from a LAS-trained \nmodel so that it is guided with a better alignment. Finally, we explore \nincluding stronger language models such as using wordpiece models, and applying \nan external LM during the beam search. On a Voice Search task, we find with \nthese improvements we can get NT to match the performance of LAS. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003413950cd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01807"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David M. Blei, Alp Kucukelbir, Jon D. McAuliffe", "title": "Variational Inference: A Review for Statisticians. (arXiv:1601.00670v7 [stat.CO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1601.00670", "type": "text/html"}], "timestampUsec": "1512537801473915", "comments": [], "summary": {"content": "<p>One of the core problems of modern statistics is to approximate \ndifficult-to-compute probability densities. This problem is especially \nimportant in Bayesian statistics, which frames all inference about unknown \nquantities as a calculation involving the posterior density. In this paper, we \nreview variational inference (VI), a method from machine learning that \napproximates probability densities through optimization. VI has been used in \nmany applications and tends to be faster than classical methods, such as Markov \nchain Monte Carlo sampling. The idea behind VI is to first posit a family of \ndensities and then to find the member of that family which is close to the \ntarget. Closeness is measured by Kullback-Leibler divergence. We review the \nideas behind mean-field variational inference, discuss the special case of VI \napplied to exponential family models, present a full example with a Bayesian \nmixture of Gaussians, and derive a variant that uses stochastic optimization to \nscale up to massive data. We discuss modern research in VI and highlight \nimportant open problems. VI is powerful, but it is not yet well understood. Our \nhope in writing this paper is to catalyze statistical research on this class of \nalgorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003413950da", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1601.00670"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513669231, "author": "Justin Sirignano, Konstantinos Spiliopoulos", "title": "DGM: A deep learning algorithm for solving partial differential equations. (arXiv:1708.07469v3 [q-fin.MF] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.07469", "type": "text/html"}], "timestampUsec": "1512537801473914", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45051261c5a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45051261c5a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>High-dimensional PDEs have been a longstanding computational challenge. We \npropose to solve high-dimensional PDEs by approximating the solution with a \ndeep neural network which is trained to satisfy the differential operator, \ninitial condition, and boundary conditions. We prove that the neural network \nconverges to the solution of the partial differential equation as the number of \nhidden units increases. Our algorithm is meshfree, which is key since meshes \nbecome infeasible in higher dimensions. Instead of forming a mesh, the neural \nnetwork is trained on batches of randomly sampled time and space points. We \nimplement the approach for American options (a type of free-boundary PDE which \nis widely used in finance) in up to $200$ dimensions. We call the algorithm a \n\"Deep Galerkin Method (DGM)\" since it is similar in spirit to Galerkin methods, \nwith the solution approximated by a neural network instead of a linear \ncombination of basis functions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003413950ff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.07469"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Xiaolin Hu, Jianguo Li, Jun Zhu", "title": "Boosting Adversarial Attacks with Momentum. (arXiv:1710.06081v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.06081", "type": "text/html"}], "timestampUsec": "1512537801473913", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450512b6cb2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450512b6cb2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep neural networks are vulnerable to adversarial examples, which poses \nsecurity concerns on these algorithms due to the potentially severe \nconsequences. Adversarial attacks serve as an important surrogate to evaluate \nthe robustness of deep learning models before they are deployed. However, most \nof the existing adversarial attacks can only fool a black-box model with a low \nsuccess rate because of the coupling of the attack ability and the \ntransferability. To address this issue, we propose a broad class of \nmomentum-based iterative algorithms to boost adversarial attacks. By \nintegrating the momentum term into the iterative process for attacks, our \nmethods can stabilize update directions and escape from poor local maxima \nduring the iterations, resulting in more transferable adversarial examples. To \nfurther improve the success rates for black-box attacks, we apply momentum \niterative algorithms to an ensemble of models, and show that the adversarially \ntrained models with a strong defense ability are also vulnerable to our \nblack-box attacks. We hope that the proposed methods will serve as a benchmark \nfor evaluating the robustness of various deep models and defense methods. We \nwon the first places in NIPS 2017 Non-targeted Adversarial Attack and Targeted \nAdversarial Attack competitions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341395110", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.06081"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jungtaek Kim, Saehoon Kim, Seungjin Choi", "title": "Learning to Warm-Start Bayesian Hyperparameter Optimization. (arXiv:1710.06219v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.06219", "type": "text/html"}], "timestampUsec": "1512537801473912", "comments": [], "summary": {"content": "<p>Hyperparameter optimization undergoes extensive evaluations of validation \nerrors in order to find its best configuration. Bayesian optimization is now \npopular for hyperparameter optimization, since it reduces the number of \nvalidation error evaluations required. Suppose that we are given a collection \nof datasets on which hyperparameters are already tuned by either humans with \ndomain expertise or extensive trials of cross-validation. When a model is \napplied to a new dataset, it is desirable to let Bayesian optimization start \nfrom configurations that were successful on similar datasets. To this end, we \nconstruct a Siamese network with convolutional layers followed by \nbi-directional LSTM layers, to learn meta-features over image datasets. Learned \nmeta-features are used to select a few datasets that are similar to the new \ndataset, so that a set of configurations in similar datasets is adopted as \ninitialization to warm-start Bayesian hyperparameter optimization. Experiments \non image datasets demonstrate that our learned meta-features are useful in \noptimizing hyperparameters in deep residual networks for image classification. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341395124", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.06219"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anqi Wu, Oluwasanmi Koyejo, Jonathan W. Pillow", "title": "Dependent relevance determination for smooth and structured sparse regression. (arXiv:1711.10058v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10058", "type": "text/html"}], "timestampUsec": "1512537801473911", "comments": [], "summary": {"content": "<p>In many problem settings, parameter vectors are not merely sparse, but \ndependent in such a way that non-zero coefficients tend to cluster together. We \nrefer to this form of dependency as \"region sparsity\". Classical sparse \nregression methods, such as the lasso and automatic relevance determination \n(ARD), which model parameters as independent a priori, and therefore do not \nexploit such dependencies. Here we introduce a hierarchical model for smooth, \nregion-sparse weight vectors and tensors in a linear regression setting. Our \napproach represents a hierarchical extension of the relevance determination \nframework, where we add a transformed Gaussian process to model the \ndependencies between the prior variances of regression weights. We combine this \nwith a structured model of the prior variances of Fourier coefficients, which \neliminates unnecessary high frequencies. The resulting prior encourages weights \nto be region-sparse in two different bases simultaneously. We develop Laplace \napproximation and Monte Carlo Markov Chain (MCMC) sampling to provide efficient \ninference for the posterior. Furthermore, a two-stage convex relaxation of the \nLaplace approximation approach is also provided to relax the inevitable \nnon-convexity during the optimization. We finally show substantial improvements \nover comparable methods for both simulated and real datasets from brain \nimaging. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512537801474", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034139513b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10058"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 1513318279, "author": "Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Katya Gonina, Navdeep Jaitly, Bo Li, Jan Chorowski, Michiel Bacchiani", "title": "State-of-the-art Speech Recognition With Sequence-to-Sequence Models. (arXiv:1712.01769v2 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.01769", "type": "text/html"}], "timestampUsec": "1512536858997297", "comments": [], "summary": {"content": "<p>Attention-based encoder-decoder architectures such as Listen, Attend, and \nSpell (LAS), subsume the acoustic, pronunciation and language model components \nof a traditional automatic speech recognition (ASR) system into a single neural \nnetwork. In our previous work, we have shown that such architectures are \ncomparable to state-of-the-art ASR systems on dictation tasks, but it was not \nclear if such architectures would be practical for more challenging tasks such \nas voice search. In this work, we explore a variety of structural and \noptimization improvements to our LAS model which significantly improve \nperformance. On the structural side, we show that word piece models can be used \ninstead of graphemes. We introduce a novel multi-head attention architecture, \nwhich offers improvements over the commonly-used single-head attention. On the \noptimization side, we explore techniques such as synchronous training, \nscheduled sampling, label smoothing, and applying minimum word error rate \noptimization, which are all shown to improve accuracy. We present results with \na unidirectional LSTM encoder for streaming recognition. On a 12,500~hour voice \nsearch task, we find that the proposed changes improve the WER of the LAS \nsystem from 9.2% to 5.6%, which corresponds to a 16% relative improvement over \nthe best conventional system which achieves 6.7% WER. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512536858997", "annotations": [], "published": 1513318279, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034137931c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01769"}]}, {"commentsNum": -1, "origin": {"title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml"}, "updated": 0, "author": "Rohit Prabhavalkar, Tara N. Sainath, Yonghui Wu, Patrick Nguyen, Zhifeng Chen, Chung-Cheng Chiu, Anjuli Kannan", "title": "Minimum Word Error Rate Training for Attention-based Sequence-to-Sequence Models. (arXiv:1712.01818v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.01818", "type": "text/html"}], "timestampUsec": "1512536858997296", "comments": [], "summary": {"content": "<p>Sequence-to-sequence models, such as attention-based models in automatic \nspeech recognition (ASR), are typically trained to optimize the cross-entropy \ncriterion which corresponds to improving the log-likelihood of the data. \nHowever, system performance is usually measured in terms of word error rate \n(WER), not log-likelihood. Traditional ASR systems benefit from discriminative \nsequence training which optimizes criteria such as the state-level minimum \nBayes risk (sMBR) which are more closely related to WER. In the present work, \nwe explore techniques to train attention-based models to directly minimize \nexpected word error rate. We consider two loss functions which approximate the \nexpected number of word errors: either by sampling from the model, or by using \nN-best lists of decoded hypotheses, which we find to be more effective than the \nsampling-based method. In experimental evaluations, we find that the proposed \ntraining procedure improves performance by up to 8.2% relative to the baseline \nsystem. This allows us to train grapheme-based, uni-directional attention-based \nmodels which match the performance of a traditional, state-of-the-art, \ndiscriminative sequence-trained system on a mobile voice-search task. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512536858997", "annotations": [], "published": 1512536859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000341379320", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01818"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512767377, "author": "Benjamin Doerr", "title": "An Elementary Analysis of the Probability That a Binomial Random Variable Exceeds Its Expectation. (arXiv:1712.00519v2 [math.PR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00519", "type": "text/html"}], "timestampUsec": "1512450843841025", "comments": [], "summary": {"content": "<p>We give an elementary proof of the fact that a binomial random variable $X$ \nwith parameters $n$ and $0.29/n \\le p &lt; 1$ with probability at least $1/4$ \nstrictly exceeds its expectation. We also show that for $1/n \\le p &lt; 1 - 1/n$, \n$X$ exceeds its expectation by more than one with probability at least \n$0.0370$. Both probabilities approach $1/2$ when $np$ and $n(1-p)$ tend to \ninfinity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512767377, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763912", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00519"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aaron Tuor, Ryan Baerwolf, Nicolas Knowles, Brian Hutchinson, Nicole Nichols, Rob Jasper", "title": "Recurrent Neural Network Language Models for Open Vocabulary Event-Level Cyber Anomaly Detection. (arXiv:1712.00557v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.00557", "type": "text/html"}], "timestampUsec": "1512450843841024", "comments": [], "summary": {"content": "<p>Automated analysis methods are crucial aids for monitoring and defending a \nnetwork to protect the sensitive or confidential data it hosts. This work \nintroduces a flexible, powerful, and unsupervised approach to detecting \nanomalous behavior in computer and network logs, one that largely eliminates \ndomain-dependent feature engineering employed by existing methods. By treating \nsystem logs as threads of interleaved \"sentences\" (event log lines) to train \nonline unsupervised neural network language models, our approach provides an \nadaptive model of normal network behavior. We compare the effectiveness of both \nstandard and bidirectional recurrent neural network language models at \ndetecting malicious activity within network log data. Extending these models, \nwe introduce a tiered recurrent architecture, which provides context by \nmodeling sequences of users' actions over time. Compared to Isolation Forest \nand Principal Components Analysis, two popular anomaly detection algorithms, we \nobserve superior performance on the Los Alamos National Laboratory Cyber \nSecurity dataset. For log-line-level red team detection, our best performing \ncharacter-based model provides test set area under the receiver operator \ncharacteristic curve of 0.98, demonstrating the strong fine-grained anomaly \ndetection performance of this approach on open vocabulary logging sources. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763916", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00557"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wellington Pinheiro dos Santos, Ricardo Emmanuel de Souza, Pl&#xed;nio B. dos Santos Filho", "title": "Evaluation of Alzheimer's Disease by Analysis of MR Images using Multilayer Perceptrons and Kohonen SOM Classifiers as an Alternative to the ADC Maps. (arXiv:1712.00712v1 [eess.IV])", "alternate": [{"href": "http://arxiv.org/abs/1712.00712", "type": "text/html"}], "timestampUsec": "1512450843841023", "comments": [], "summary": {"content": "<p>Alzheimer's disease is the most common cause of dementia, yet hard to \ndiagnose precisely without invasive techniques, particularly at the onset of \nthe disease. This work approaches image analysis and classification of \nsynthetic multispectral images composed by diffusion-weighted magnetic \nresonance (MR) cerebral images for the evaluation of cerebrospinal fluid area \nand measuring the advance of Alzheimer's disease. A clinical 1.5 T MR imaging \nsystem was used to acquire all images presented. The classification methods are \nbased on multilayer perceptrons and Kohonen Self-Organized Map classifiers. We \nassume the classes of interest can be separated by hyperquadrics. Therefore, a \n2-degree polynomial network is used to classify the original image, generating \nthe ground truth image. The classification results are used to improve the \nusual analysis of the apparent diffusion coefficient map. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034076391b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00712"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lyudmila Grigoryeva, Juan-Pablo Ortega", "title": "Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems. (arXiv:1712.00754v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.00754", "type": "text/html"}], "timestampUsec": "1512450843841022", "comments": [], "summary": {"content": "<p>A new class of non-homogeneous state-affine systems is introduced. Sufficient \nconditions are identified that guarantee first, that the associated reservoir \ncomputers with linear readouts are causal, time-invariant, and satisfy the \nfading memory property and second, that a subset of this class is universal in \nthe category of fading memory filters with stochastic almost surely bounded \ninputs. This means that any discrete-time filter that satisfies the fading \nmemory property with random inputs of that type can be uniformly approximated \nby elements in the non-homogeneous state-affine family. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763922", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00754"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Valter Augusto de Freitas Barbosa, Reiga Ramalho Ribeiro, Allan Rivalles Souza Feitosa, Victor Luiz Bezerra Ara&#xfa;jo da Silva, Arthur Diego Dias Rocha, Rafaela Covello de Freitas, Ricardo Emmanuel de Souza, Wellington Pinheiro dos Santos", "title": "Reconstruction of Electrical Impedance Tomography Using Fish School Search, Non-Blind Search, and Genetic Algorithm. (arXiv:1712.00789v1 [physics.med-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.00789", "type": "text/html"}], "timestampUsec": "1512450843841021", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450512b70bb\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450512b70bb&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Electrical Impedance Tomography (EIT) is a noninvasive imaging technique that \ndoes not use ionizing radiation, with application both in environmental \nsciences and in health. Image reconstruction is performed by solving an inverse \nproblem and ill-posed. Evolutionary Computation and Swarm Intelligence have \nbecome a source of methods for solving inverse problems. Fish School Search \n(FSS) is a promising search and optimization method, based on the dynamics of \nschools of fish. In this article the authors present a method for \nreconstruction of EIT images based on FSS and Non-Blind Search (NBS). The \nmethod was evaluated using numerical phantoms consisting of electrical \nconductivity images with subjects in the center, between the center and the \nedge and on the edge of a circular section, with meshes of 415 finite elements. \nThe authors performed 20 simulations for each configuration. Results showed \nthat both FSS and FSS-NBS were able to converge faster than genetic algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763928", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00789"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Andrew Levy, Robert Platt, Kate Saenko", "title": "Hierarchical Actor-Critic. (arXiv:1712.00948v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00948", "type": "text/html"}], "timestampUsec": "1512450843841020", "comments": [], "summary": {"content": "<p>We present a novel approach to hierarchical reinforcement learning called \nHierarchical Actor-Critic (HAC). HAC aims to make learning tasks with sparse \nbinary rewards more efficient by enabling agents to learn how to break down \ntasks from scratch. The technique uses of a set of actor-critic networks that \nlearn to decompose tasks into a hierarchy of subgoals. We demonstrate that HAC \nsignificantly improves sample efficiency in a series of tasks that involve \nsparse binary rewards and require behavior over a long time horizon. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763933", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00948"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Johannes Lengler", "title": "Drift Analysis. (arXiv:1712.00964v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.00964", "type": "text/html"}], "timestampUsec": "1512450843841019", "comments": [], "summary": {"content": "<p>Drift analysis is one of the major tools for analysing evolutionary \nalgorithms and nature-inspired search heuristics. In this chapter we give an \nintroduction to drift analysis and give some examples of how to use it for the \nanalysis of evolutionary algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763941", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00964"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Paolo Meloni, Alessandro Capotondi, Gianfranco Deriu, Michele Brian, Francesco Conti, Davide Rossi, Luigi Raffo, Luca Benini", "title": "NEURAghe: Exploiting CPU-FPGA Synergies for Efficient and Flexible CNN Inference Acceleration on Zynq SoCs. (arXiv:1712.00994v1 [cs.NE])", "alternate": [{"href": "http://arxiv.org/abs/1712.00994", "type": "text/html"}], "timestampUsec": "1512450843841018", "comments": [], "summary": {"content": "<p>Deep convolutional neural networks (CNNs) obtain outstanding results in tasks \nthat require human-level understanding of data, like image or speech \nrecognition. However, their computational load is significant, motivating the \ndevelopment of CNN-specialized accelerators. This work presents NEURAghe, a \nflexible and efficient hardware/software solution for the acceleration of CNNs \non Zynq SoCs. NEURAghe leverages the synergistic usage of Zynq ARM cores and of \na powerful and flexible Convolution-Specific Processor deployed on the \nreconfigurable logic. The Convolution-Specific Processor embeds both a \nconvolution engine and a programmable soft core, releasing the ARM processors \nfrom most of the supervision duties and allowing the accelerator to be \ncontrolled by software at an ultra-fine granularity. This methodology opens the \nway for cooperative heterogeneous computing: while the accelerator takes care \nof the bulk of the CNN workload, the ARM cores can seamlessly execute \nhard-to-accelerate parts of the computational graph, taking advantage of the \nNEON vector engines to further speed up computation. Through the companion \nNeuDNN SW stack, NEURAghe supports end-to-end CNN-based classification with a \npeak performance of 169 Gops/s, and an energy efficiency of 17 Gops/W. Thanks \nto our heterogeneous computing model, our platform improves upon the \nstate-of-the-art, achieving a frame rate of 5.5 fps on the end-to-end execution \nof VGG-16, and 6.6 fps on ResNet-18. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034076394a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00994"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ga&#xe9;tan Marceau-Caron, Yann Ollivier", "title": "Natural Langevin Dynamics for Neural Networks. (arXiv:1712.01076v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01076", "type": "text/html"}], "timestampUsec": "1512450843841017", "comments": [], "summary": {"content": "<p>One way to avoid overfitting in machine learning is to use model parameters \ndistributed according to a Bayesian posterior given the data, rather than the \nmaximum likelihood estimator. Stochastic gradient Langevin dynamics (SGLD) is \none algorithm to approximate such Bayesian posteriors for large models and \ndatasets. SGLD is a standard stochastic gradient descent to which is added a \ncontrolled amount of noise, specifically scaled so that the parameter converges \nin law to the posterior distribution [WT11, TTV16]. The posterior predictive \ndistribution can be approximated by an ensemble of samples from the trajectory. \n</p> \n<p>Choice of the variance of the noise is known to impact the practical behavior \nof SGLD: for instance, noise should be smaller for sensitive parameter \ndirections. Theoretically, it has been suggested to use the inverse Fisher \ninformation matrix of the model as the variance of the noise, since it is also \nthe variance of the Bayesian posterior [PT13, AKW12, GC11]. But the Fisher \nmatrix is costly to compute for large- dimensional models. \n</p> \n<p>Here we use the easily computed Fisher matrix approximations for deep neural \nnetworks from [MO16, Oll15]. The resulting natural Langevin dynamics combines \nthe advantages of Amari's natural gradient descent and Fisher-preconditioned \nLangevin dynamics for large neural networks. \n</p> \n<p>Small-scale experiments on MNIST show that Fisher matrix preconditioning \nbrings SGLD close to dropout as a regularizing technique. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763952", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01076"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513141860, "author": "Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, Neoklis Polyzotis", "title": "The Case for Learned Index Structures. (arXiv:1712.01208v2 [cs.DB] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.01208", "type": "text/html"}], "timestampUsec": "1512450843841016", "comments": [], "summary": {"content": "<p>Indexes are models: a B-Tree-Index can be seen as a model to map a key to the \nposition of a record within a sorted array, a Hash-Index as a model to map a \nkey to a position of a record within an unsorted array, and a BitMap-Index as a \nmodel to indicate if a data record exists or not. In this exploratory research \npaper, we start from this premise and posit that all existing index structures \ncan be replaced with other types of models, including deep-learning models, \nwhich we term learned indexes. The key idea is that a model can learn the sort \norder or structure of lookup keys and use this signal to effectively predict \nthe position or existence of records. We theoretically analyze under which \nconditions learned indexes outperform traditional index structures and describe \nthe main challenges in designing learned index structures. Our initial results \nshow, that by using neural nets we are able to outperform cache-optimized \nB-Trees by up to 70% in speed while saving an order-of-magnitude in memory over \nseveral real-world data sets. More importantly though, we believe that the idea \nof replacing core components of a data management system through learned models \nhas far reaching implications for future systems designs and that this work \njust provides a glimpse of what might be possible. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1513141859, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763964", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01208"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tim Rockt&#xe4;schel, Sebastian Riedel", "title": "End-to-End Differentiable Proving. (arXiv:1705.11040v2 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.11040", "type": "text/html"}], "timestampUsec": "1512450843841015", "comments": [], "summary": {"content": "<p>We introduce neural networks for end-to-end differentiable proving of queries \nto knowledge bases by operating on dense vector representations of symbols. \nThese neural networks are constructed recursively by taking inspiration from \nthe backward chaining algorithm as used in Prolog. Specifically, we replace \nsymbolic unification with a differentiable computation on vector \nrepresentations of symbols using a radial basis function kernel, thereby \ncombining symbolic reasoning with learning subsymbolic vector representations. \nBy using gradient descent, the resulting neural network can be trained to infer \nfacts from a given incomplete knowledge base. It learns to (i) place \nrepresentations of similar symbols in close proximity in a vector space, (ii) \nmake use of such similarities to prove queries, (iii) induce logical rules, and \n(iv) use provided and induced logical rules for multi-hop reasoning. We \ndemonstrate that this architecture outperforms ComplEx, a state-of-the-art \nneural link prediction model, on three out of four benchmark knowledge bases \nwhile at the same time inducing interpretable function-free first-order logic \nrules. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763969", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.11040"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Abhinav Gupta, Yajie Miao, Leonardo Neves, Florian Metze", "title": "Visual Features for Context-Aware Speech Recognition. (arXiv:1712.00489v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.00489", "type": "text/html"}], "timestampUsec": "1512450843841014", "comments": [], "summary": {"content": "<p>Automatic transcriptions of consumer-generated multi-media content such as \n\"Youtube\" videos still exhibit high word error rates. Such data typically \noccupies a very broad domain, has been recorded in challenging conditions, with \ncheap hardware and a focus on the visual modality, and may have been \npost-processed or edited. In this paper, we extend our earlier work on adapting \nthe acoustic model of a DNN-based speech recognition system to an RNN language \nmodel and show how both can be adapted to the objects and scenes that can be \nautomatically detected in the video. We are working on a corpus of \"how-to\" \nvideos from the web, and the idea is that an object that can be seen (\"car\"), \nor a scene that is being detected (\"kitchen\") can be used to condition both \nmodels on the \"context\" of the recording, thereby reducing perplexity and \nimproving transcription. We achieve good improvements in both cases and compare \nand analyze the respective reductions in word error rate. We expect that our \nresults can be used for any type of speech processing in which \"context\" \ninformation is available, for example in robotics, man-machine interaction, or \nwhen indexing large audio-visual archives, and should ultimately help to bring \ntogether the \"video-to-text\" and \"speech-to-text\" communities. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763970", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00489"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512537804, "author": "Tim Miller, Piers Howe, Liz Sonenberg", "title": "Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences. (arXiv:1712.00547v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00547", "type": "text/html"}], "timestampUsec": "1512450843841013", "comments": [], "summary": {"content": "<p>In his seminal book `The Inmates are Running the Asylum: Why High-Tech \nProducts Drive Us Crazy And How To Restore The Sanity' [2004, Sams \nIndianapolis, IN, USA], Alan Cooper argues that a major reason why software is \noften poorly designed (from a user perspective) is that programmers are in \ncharge of design decisions, rather than interaction designers. As a result, \nprogrammers design software for themselves, rather than for their target \naudience, a phenomenon he refers to as the `inmates running the asylum'. This \npaper argues that explainable AI risks a similar fate. While the re-emergence \nof explainable AI is positive, this paper argues most of us as AI researchers \nare building explanatory agents for ourselves, rather than for the intended \nusers. But explainable AI is more likely to succeed if researchers and \npractitioners understand, adopt, implement, and improve models from the vast \nand valuable bodies of research in philosophy, psychology, and cognitive \nscience, and if evaluation of these models is focused more on people than on \ntechnology. From a light scan of literature, we demonstrate that there is \nconsiderable scope to infuse more results from the social and behavioural \nsciences into explainable AI, and present some key results from these fields \nthat are relevant to explainable AI. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763978", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00547"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yan Zhu, Shaoting Zhang, Dimitris Metaxas", "title": "Interactive Reinforcement Learning for Object Grounding via Self-Talking. (arXiv:1712.00576v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00576", "type": "text/html"}], "timestampUsec": "1512450843841012", "comments": [], "summary": {"content": "<p>Humans are able to identify a referred visual object in a complex scene via a \nfew rounds of natural language communications. Success communication requires \nboth parties to engage and learn to adapt for each other. In this paper, we \nintroduce an interactive training method to improve the natural language \nconversation system for a visual grounding task. During interactive training, \nboth agents are reinforced by the guidance from a common reward function. The \nparametrized reward function also cooperatively updates itself via \ninteractions, and contribute to accomplishing the task. We evaluate the method \non GuessWhat?! visual grounding task, and significantly improve the task \nsuccess rate. However, we observe language drifting problem during training and \npropose to use reward engineering to improve the interpretability for the \ngenerated conversations. Our result also indicates evaluating goal-ended visual \nconversation tasks require semantic relevant metrics beyond task success rate. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034076397e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00576"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lianmin Zheng, Jiacheng Yang, Han Cai, Weinan Zhang, Jun Wang, Yong Yu", "title": "MAgent: A Many-Agent Reinforcement Learning Platform for Artificial Collective Intelligence. (arXiv:1712.00600v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00600", "type": "text/html"}], "timestampUsec": "1512450843841011", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450512b746a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450512b746a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We introduce MAgent, a platform to support research and development of \nmany-agent reinforcement learning. Unlike previous research platforms on single \nor multi-agent reinforcement learning, MAgent focuses on supporting the tasks \nand the applications that require hundreds to millions of agents. Within the \ninteractions among a population of agents, it enables not only the study of \nlearning algorithms for agents' optimal polices, but more importantly, the \nobservation and understanding of individual agent's behaviors and social \nphenomena emerging from the AI society, including communication languages, \nleaderships, altruism. MAgent is highly scalable and can host up to one million \nagents on a single GPU server. MAgent also provides flexible configurations for \nAI researchers to design their customized environments and agents. In this \ndemo, we present three environments designed on MAgent and show emerged \ncollective intelligence by learning from scratch. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000034076399a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00600"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Stefan Richthofer, Laurenz Wiskott", "title": "PFAx: Predictable Feature Analysis to Perform Control. (arXiv:1712.00634v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00634", "type": "text/html"}], "timestampUsec": "1512450843841010", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45051316407\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45051316407&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Predictable Feature Analysis (PFA) (Richthofer, Wiskott, ICMLA 2015) is an \nalgorithm that performs dimensionality reduction on high dimensional input \nsignal. It extracts those subsignals that are most predictable according to a \ncertain prediction model. We refer to these extracted signals as predictable \nfeatures. \n</p> \n<p>In this work we extend the notion of PFA to take supplementary information \ninto account for improving its predictions. Such information can be a \nmultidimensional signal like the main input to PFA, but is regarded external. \nThat means it won't participate in the feature extraction - no features get \nextracted or composed of it. Features will be exclusively extracted from the \nmain input such that they are most predictable based on themselves and the \nsupplementary information. We refer to this enhanced PFA as PFAx (PFA \nextended). \n</p> \n<p>Even more important than improving prediction quality is to observe the \neffect of supplementary information on feature selection. PFAx transparently \nprovides insight how the supplementary information adds to prediction quality \nand whether it is valuable at all. Finally we show how to invert that relation \nand can generate the supplementary information such that it would yield a \ncertain desired outcome of the main signal. \n</p> \n<p>We apply this to a setting inspired by reinforcement learning and let the \nalgorithm learn how to control an agent in an environment. With this method it \nis feasible to locally optimize the agent's state, i.e. reach a certain goal \nthat is near enough. We are preparing a follow-up paper that extends this \nmethod such that also global optimization is feasible. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003407639bc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00634"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Eyke H&#xfc;llermeier", "title": "From knowledge-based to data-driven modeling of fuzzy rule-based systems: A critical reflection. (arXiv:1712.00646v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00646", "type": "text/html"}], "timestampUsec": "1512450843841009", "comments": [], "summary": {"content": "<p>This paper briefly elaborates on a development in (applied) fuzzy logic that \nhas taken place in the last couple of decades, namely, the complementation or \neven replacement of the traditional knowledge-based approach to fuzzy \nrule-based systems design by a data-driven one. It is argued that the classical \nrule-based modeling paradigm is actually more amenable to the knowledge-based \napproach, for which it has originally been conceived, while being less apt to \ndata-driven model design. An important reason that prevents fuzzy (rule-based) \nsystems from being leveraged in large-scale applications is the flat structure \nof rule bases, along with the local nature of fuzzy rules and their limited \nability to express complex dependencies between variables. This motivates \nalternative approaches to fuzzy systems modeling, in which functional \ndependencies can be represented more flexibly and more compactly in terms of \nhierarchical structures. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003407639d1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00646"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alper Kose, Berke Aral Sonmez, Metin Balaban", "title": "Simulated Annealing Algorithm for Graph Coloring. (arXiv:1712.00709v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00709", "type": "text/html"}], "timestampUsec": "1512450843841008", "comments": [], "summary": {"content": "<p>The goal of this Random Walks project is to code and experiment the Markov \nChain Monte Carlo (MCMC) method for the problem of graph coloring. In this \nreport, we present the plots of cost function \\(\\mathbf{H}\\) by varying the \nparameters like \\(\\mathbf{q}\\) (Number of colors that can be used in coloring) \nand \\(\\mathbf{c}\\) (Average node degree). The results are obtained by using \nsimulated annealing scheme, where the temperature (inverse of \n\\(\\mathbf{\\beta}\\)) parameter in the MCMC is lowered progressively. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003407639e6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00709"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512767377, "author": "Laura Graesser, Abhinav Gupta, Lakshay Sharma, Evelina Bakhturina", "title": "Sentiment Classification using Images and Label Embeddings. (arXiv:1712.00725v1 [cs.CL] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00725", "type": "text/html"}], "timestampUsec": "1512450843841007", "comments": [], "summary": {"content": "<p>In this project we analysed how much semantic information images carry, and \nhow much value image data can add to sentiment analysis of the text associated \nwith the images. To better understand the contribution from images, we compared \nmodels which only made use of image data, models which only made use of text \ndata, and models which combined both data types. We also analysed if this \napproach could help sentiment classifiers generalize to unknown sentiments. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512767377, "likingUsers": [], "id": "tag:google.com,2005:reader/item/00000003407639fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00725"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Simon S. Du, Jason D. Lee, Yuandong Tian, Barnabas Poczos, Aarti Singh", "title": "Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of Spurious Local Minima. (arXiv:1712.00779v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00779", "type": "text/html"}], "timestampUsec": "1512450843841006", "comments": [], "summary": {"content": "<p>We consider the problem of learning a one-hidden-layer neural network with \nnon-overlapping convolutional layer and ReLU activation function, i.e., \n$f(\\mathbf{Z}; \\mathbf{w}, \\mathbf{a}) = \\sum_j \na_j\\sigma(\\mathbf{w}^\\top\\mathbf{Z}_j)$, in which both the convolutional \nweights $\\mathbf{w}$ and the output weights $\\mathbf{a}$ are parameters to be \nlearned. We prove that with Gaussian input $\\mathbf{Z}$, there is a spurious \nlocal minimum that is not a global mininum. Surprisingly, in the presence of \nlocal minimum, starting from randomly initialized weights, gradient descent \nwith weight normalization can still be proven to recover the true parameters \nwith constant probability (which can be boosted to arbitrarily high accuracy \nwith multiple restarts). We also show that with constant probability, the same \nprocedure could also converge to the spurious local minimum, showing that the \nlocal minimum plays a non-trivial role in the dynamics of gradient descent. \nFurthermore, a quantitative analysis shows that the gradient descent dynamics \nhas two phases: it starts off slow, but converges much faster after several \niterations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a13", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00779"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jakob Suchan, Mehul Bhatt, Przemys&#x142;aw Wa&#x142;&#x119;ga, Carl Schultz", "title": "Visual Explanation by High-Level Abduction: On Answer-Set Programming Driven Reasoning about Moving Objects. (arXiv:1712.00840v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00840", "type": "text/html"}], "timestampUsec": "1512450843841005", "comments": [], "summary": {"content": "<p>We propose a hybrid architecture for systematically computing robust visual \nexplanation(s) encompassing hypothesis formation, belief revision, and default \nreasoning with video data. The architecture consists of two tightly integrated \nsynergistic components: (1) (functional) answer set programming based abductive \nreasoning with space-time tracklets as native entities; and (2) a visual \nprocessing pipeline for detection based object tracking and motion analysis. \n</p> \n<p>We present the formal framework, its general implementation as a \n(declarative) method in answer set programming, and an example application and \nevaluation based on two diverse video datasets: the MOTChallenge benchmark \ndeveloped by the vision community, and a recently developed Movie Dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a24", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00840"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kyle Hundman, Thamme Gowda, Mayank Kejriwal, Benedikt Boecking", "title": "Always Lurking: Understanding and Mitigating Bias in Online Human Trafficking Detection. (arXiv:1712.00846v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00846", "type": "text/html"}], "timestampUsec": "1512450843841004", "comments": [], "summary": {"content": "<p>Web-based human trafficking activity has increased in recent years but it \nremains sparsely dispersed among escort advertisements and difficult to \nidentify due to its often-latent nature. The use of intelligent systems to \ndetect trafficking can thus have a direct impact on investigative resource \nallocation and decision-making, and, more broadly, help curb a widespread \nsocial problem. Trafficking detection involves assigning a normalized score to \na set of escort advertisements crawled from the Web -- a higher score indicates \na greater risk of trafficking-related (involuntary) activities. In this paper, \nwe define and study the problem of trafficking detection and present a \ntrafficking detection pipeline architecture developed over three years of \nresearch within the DARPA Memex program. Drawing on multi-institutional data, \nsystems, and experiences collected during this time, we also conduct post hoc \nbias analyses and present a bias mitigation plan. Our findings show that, while \nautomatic trafficking detection is an important application of AI for social \ngood, it also provides cautionary lessons for deploying predictive machine \nlearning algorithms without appropriate de-biasing. This ultimately led to \nintegration of an interpretable solution into a search system that contains \nover 100 million advertisements and is used by over 200 law enforcement \nagencies to investigate leads. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a2a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00846"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Catherine Dubois, Bruno Woltzenlogel Paleo", "title": "Proceedings of the Fifth Workshop on Proof eXchange for Theorem Proving. (arXiv:1712.00898v1 [cs.LO])", "alternate": [{"href": "http://arxiv.org/abs/1712.00898", "type": "text/html"}], "timestampUsec": "1512450843841003", "comments": [], "summary": {"content": "<p>This volume of EPTCS contains the proceedings of the Fifth Workshop on Proof \nExchange for Theorem Proving (PxTP 2017), held on September 23-24, 2017 as part \nof the Tableaux, FroCoS and ITP conferences in Brasilia, Brazil. The PxTP \nworkshop series brings together researchers working on various aspects of \ncommunication, integration, and cooperation between reasoning systems and \nformalisms, with a special focus on proofs. The progress in computer-aided \nreasoning, both automated and interactive, during the past decades, made it \npossible to build deduction tools that are increasingly more applicable to a \nwider range of problems and are able to tackle larger problems progressively \nfaster. In recent years, cooperation between such tools in larger systems has \ndemonstrated the potential to reduce the amount of manual intervention. \nCooperation between reasoning systems relies on availability of theoretical \nformalisms and practical tools to exchange problems, proofs, and models. The \nPxTP workshop series strives to encourage such cooperation by inviting \ncontributions on all aspects of cooperation between reasoning tools, whether \nautomatic or interactive. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a30", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00898"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jaejun Yoo, Sohail Sabir, Duchang Heo, Kee Hyun Kim, Abdul Wahab, Yoonseok Choi, Seul-I Lee, Eun Young Chae, Hak Hee Kim, Young Min Bae, Young-wook Choi, Seungryong Cho, Jong Chul Ye", "title": "Deep Learning Can Reverse Photon Migration for Diffuse Optical Tomography. (arXiv:1712.00912v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.00912", "type": "text/html"}], "timestampUsec": "1512450843841002", "comments": [], "summary": {"content": "<p>Can artificial intelligence (AI) learn complicated non-linear physics? Here \nwe propose a novel deep learning approach that learns non-linear photon \nscattering physics and obtains accurate 3D distribution of optical anomalies. \nIn contrast to the traditional black-box deep learning approaches to inverse \nproblems, our deep network learns to invert the Lippmann-Schwinger integral \nequation which describes the essential physics of photon migration of diffuse \nnear-infrared (NIR) photons in turbid media. As an example for clinical \nrelevance, we applied the method to our prototype diffuse optical tomography \n(DOT). We show that our deep neural network, trained with only simulation data, \ncan accurately recover the location of anomalies within biomimetic phantoms and \nlive animals without the use of an exogenous contrast agent. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a37", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00912"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tomoaki Nakamura, Takayuki Nagai, Tadahiro Taniguchi", "title": "SERKET: An Architecture For Connecting Stochastic Models to Realize a Large-Scale Cognitive Model. (arXiv:1712.00929v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00929", "type": "text/html"}], "timestampUsec": "1512450843841001", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45051316681\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45051316681&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>To realize human-like robot intelligence, a large-scale cognitive \narchitecture is required for robots to understand the environment through a \nvariety of sensors with which they are equipped. In this paper, we propose a \nnovel framework named Serket that enables the construction of a large-scale \ngenerative model and its inference easily by connecting sub-modules to allow \nthe robots to acquire various capabilities through interaction with their \nenvironments and others. We consider that large-scale cognitive models can be \nconstructed by connecting smaller fundamental models hierarchically while \nmaintaining their programmatic independence. Moreover, connected modules are \ndependent on each other, and parameters are required to be optimized as a \nwhole. Conventionally, the equations for parameter estimation have to be \nderived and implemented depending on the models. However, it becomes harder to \nderive and implement those of a larger scale model. To solve these problems, in \nthis paper, we propose a method for parameter estimation by communicating the \nminimal parameters between various modules while maintaining their programmatic \nindependence. Therefore, Serket makes it easy to construct large-scale models \nand estimate their parameters via the connection of modules. Experimental \nresults demonstrated that the model can be constructed by connecting modules, \nthe parameters can be optimized as a whole, and they are comparable with the \noriginal models that we have proposed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a46", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00929"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sachin Pawar, Pushpak Bhattacharya, Girish K. Palshikar", "title": "End-to-End Relation Extraction using Markov Logic Networks. (arXiv:1712.00988v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00988", "type": "text/html"}], "timestampUsec": "1512450843841000", "comments": [], "summary": {"content": "<p>The task of end-to-end relation extraction consists of two sub-tasks: i) \nidentifying entity mentions along with their types and ii) recognizing semantic \nrelations among the entity mention pairs. %Identifying entity mentions along \nwith their types and recognizing semantic relations among the entity mentions, \nare two very important problems in Information Extraction. It has been shown \nthat for better performance, it is necessary to address these two sub-tasks \njointly. We propose an approach for simultaneous extraction of entity mentions \nand relations in a sentence, by using inference in Markov Logic Networks (MLN). \nWe learn three different classifiers : i) local entity classifier, ii) local \nrelation classifier and iii) \"pipeline\" relation classifier which uses \npredictions of the local entity classifier. Predictions of these classifiers \nmay be inconsistent with each other. We represent these predictions along with \nsome domain knowledge using weighted first-order logic rules in an MLN and \nperform joint inference over the MLN to obtain a global output with minimum \ninconsistencies. Experiments on the ACE (Automatic Content Extraction) 2004 \ndataset demonstrate that our approach of joint extraction using MLNs \noutperforms the baselines of individual classifiers. Our end-to-end relation \nextraction performance is better than 2 out of 3 previous results reported on \nthe ACE 2004 dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a58", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00988"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Girish Keshav Palshikar, Sachin Pawar, Saheb Chourasia, Nitin Ramrakhiyani", "title": "Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals. (arXiv:1712.00991v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1712.00991", "type": "text/html"}], "timestampUsec": "1512450843840999", "comments": [], "summary": {"content": "<p>Performance appraisal (PA) is an important HR process to periodically measure \nand evaluate every employee's performance vis-a-vis the goals established by \nthe organization. A PA process involves purposeful multi-step multi-modal \ncommunication between employees, their supervisors and their peers, such as \nself-appraisal, supervisor assessment and peer feedback. Analysis of the \nstructured data and text produced in PA is crucial for measuring the quality of \nappraisals and tracking actual improvements. In this paper, we apply text \nmining techniques to produce insights from PA text. First, we perform sentence \nclassification to identify strengths, weaknesses and suggestions of \nimprovements found in the supervisor assessments and then use clustering to \ndiscover broad categories among them. Next we use multi-class multi-label \nclassification techniques to match supervisor assessments to predefined broad \nperspectives on performance. Finally, we propose a short-text summarization \ntechnique to produce a summary of peer feedback comments for a given employee \nand compare it with manual summaries. All techniques are illustrated using a \nreal-life dataset of supervisor assessment and peer feedback text produced \nduring the PA of 4528 employees in a large multi-national IT company. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a65", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00991"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Leopoldo Bertossi", "title": "Characterizing and Computing Causes for Query Answers in Databases from Database Repairs and Repair Programs. (arXiv:1712.01001v1 [cs.DB])", "alternate": [{"href": "http://arxiv.org/abs/1712.01001", "type": "text/html"}], "timestampUsec": "1512450843840998", "comments": [], "summary": {"content": "<p>A correspondence between database tuples as causes for query answers in \ndatabases and tuple-based repairs of inconsistent databases with respect to \ndenial constraints has already been established. In this work, answer-set \nprograms that specify repairs of databases are used as a basis for solving \ncomputational and reasoning problems about causes. Here, causes are also \nintroduced at the attribute level by appealing to a both null-based and \nattribute-based repair semantics. The corresponding repair programs are \npresented, and they are used as a basis for computation and reasoning about \nattribute-level causes. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a6c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01001"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christoph Adami", "title": "The mind as a computational system. (arXiv:1712.01093v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.01093", "type": "text/html"}], "timestampUsec": "1512450843840997", "comments": [], "summary": {"content": "<p>The present document is an excerpt of an essay that I wrote as part of my \napplication material to graduate school in Computer Science (with a focus on \nArtificial Intelligence), in 1986. I was not invited by any of the schools that \nreceived it, so I became a theoretical physicist instead. The essay's full \ntitle was \"Some Topics in Philosophy and Computer Science\". I am making this \ntext (unchanged from 1985, preserving the typesetting as much as possible) \navailable now in memory of Jerry Fodor, whose writings had influenced me \nsignificantly at the time (even though I did not always agree). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a74", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01093"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David Isele, Akansel Cosgun", "title": "Transferring Autonomous Driving Knowledge on Simulated and Real Intersections. (arXiv:1712.01106v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01106", "type": "text/html"}], "timestampUsec": "1512450843840996", "comments": [], "summary": {"content": "<p>We view intersection handling on autonomous vehicles as a reinforcement \nlearning problem, and study its behavior in a transfer learning setting. We \nshow that a network trained on one type of intersection generally is not able \nto generalize to other intersections. However, a network that is pre-trained on \none intersection and fine-tuned on another performs better on the new task \ncompared to training in isolation. This network also retains knowledge of the \nprior task, even though some forgetting occurs. Finally, we show that the \nbenefits of fine-tuning hold when transferring simulated intersection handling \nknowledge to a real autonomous vehicle. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a7c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01106"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Abhinav Jauhri, Carlee Joe-Wong, John Paul Shen", "title": "On the Real-time Vehicle Placement Problem. (arXiv:1712.01235v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.01235", "type": "text/html"}], "timestampUsec": "1512450843840995", "comments": [], "summary": {"content": "<p>Motivated by ride-sharing platforms' efforts to reduce their riders' wait \ntimes for a vehicle, this paper introduces a novel problem of placing vehicles \nto fulfill real-time pickup requests in a spatially and temporally changing \nenvironment. The real-time nature of this problem makes it fundamentally \ndifferent from other placement and scheduling problems, as it requires not only \nreal-time placement decisions but also handling real-time request dynamics, \nwhich are influenced by human mobility patterns. We use a dataset of ten \nmillion ride requests from four major U.S. cities to show that the requests \nexhibit significant self-similarity. We then propose distributed online \nlearning algorithms for the real-time vehicle placement problem and bound their \nexpected performance under this observed self-similarity. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a86", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01235"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wei Ma, Jun Lu", "title": "An Equivalence of Fully Connected Layer and Convolutional Layer. (arXiv:1712.01252v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01252", "type": "text/html"}], "timestampUsec": "1512450843840994", "comments": [], "summary": {"content": "<p>This article demonstrates that convolutional operation can be converted to \nmatrix multiplication, which has the same calculation way with fully connected \nlayer. The article is helpful for the beginners of the neural network to \nunderstand how fully connected layer and the convolutional layer work in the \nbackend. To be concise and to make the article more readable, we only consider \nthe linear case. It can be extended to the non-linear case easily through \nplugging in a non-linear encapsulation to the values like this $\\sigma(x)$ \ndenoted as $x^{\\prime}$. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a8e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01252"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Masataro Asai, Alex Fukunaga", "title": "Classical Planning in Deep Latent Space: Bridging the Subsymbolic-Symbolic Boundary. (arXiv:1705.00154v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.00154", "type": "text/html"}], "timestampUsec": "1512450843840993", "comments": [], "summary": {"content": "<p>Current domain-independent, classical planners require symbolic models of the \nproblem domain and instance as input, resulting in a knowledge acquisition \nbottleneck. Meanwhile, although deep learning has achieved significant success \nin many fields, the knowledge is encoded in a subsymbolic representation which \nis incompatible with symbolic systems such as planners. We propose LatPlan, an \nunsupervised architecture combining deep learning and classical planning. Given \nonly an unlabeled set of image pairs showing a subset of transitions allowed in \nthe environment (training inputs), and a pair of images representing the \ninitial and the goal states (planning inputs), LatPlan finds a plan to the goal \nstate in a symbolic latent space and returns a visualized plan execution. The \ncontribution of this paper is twofold: (1) State Autoencoder, which finds a \npropositional state representation of the environment using a Variational \nAutoencoder. It generates a discrete latent vector from the images, based on \nwhich a PDDL model can be constructed and then solved by an off-the-shelf \nplanner. (2) Action Autoencoder / Discriminator, a neural architecture which \njointly finds the action symbols and the implicit action models \n(preconditions/effects), and provides a successor function for the implicit \ngraph search. We evaluate LatPlan using image-based versions of 3 planning \ndomains: 8-puzzle, Towers of Hanoi and LightsOut. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763a96", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.00154"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Thomas Anthony, Zheng Tian, David Barber", "title": "Thinking Fast and Slow with Deep Learning and Tree Search. (arXiv:1705.08439v4 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08439", "type": "text/html"}], "timestampUsec": "1512450843840992", "comments": [], "summary": {"content": "<p>Sequential decision making problems, such as structured prediction, robotic \ncontrol, and game playing, require a combination of planning policies and \ngeneralisation of those plans. In this paper, we present Expert Iteration \n(ExIt), a novel reinforcement learning algorithm which decomposes the problem \ninto separate planning and generalisation tasks. Planning new policies is \nperformed by tree search, while a deep neural network generalises those plans. \nSubsequently, tree search is improved by using the neural network policy to \nguide search, increasing the strength of new plans. In contrast, standard deep \nReinforcement Learning algorithms rely on a neural network not only to \ngeneralise plans, but to discover them too. We show that ExIt outperforms \nREINFORCE for training a neural network to play the board game Hex, and our \nfinal tree search agent, trained tabula rasa, defeats MoHex 1.0, the most \nrecent Olympiad Champion player to be publicly released. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763aa0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08439"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, Sergey Levine", "title": "Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning. (arXiv:1708.02596v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.02596", "type": "text/html"}], "timestampUsec": "1512450843840991", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450513169ce\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450513169ce&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Model-free deep reinforcement learning algorithms have been shown to be \ncapable of learning a wide range of robotic skills, but typically require a \nvery large number of samples to achieve good performance. Model-based \nalgorithms, in principle, can provide for much more efficient learning, but \nhave proven difficult to extend to expressive, high-capacity models such as \ndeep neural networks. In this work, we demonstrate that medium-sized neural \nnetwork models can in fact be combined with model predictive control (MPC) to \nachieve excellent sample complexity in a model-based reinforcement learning \nalgorithm, producing stable and plausible gaits to accomplish various complex \nlocomotion tasks. We also propose using deep neural network dynamics models to \ninitialize a model-free learner, in order to combine the sample efficiency of \nmodel-based approaches with the high task-specific performance of model-free \nmethods. We empirically demonstrate on MuJoCo locomotion tasks that our pure \nmodel-based approach trained on just random action data can follow arbitrary \ntrajectories with excellent sample efficiency, and that our hybrid algorithm \ncan accelerate model-free learning on high-speed benchmark tasks, achieving \nsample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents. \nVideos can be found at https://sites.google.com/view/mbmf \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763aa7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.02596"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513315289, "author": "Tongzhou Wang, Yi Wu, David A. Moore, Stuart J. Russell", "title": "Neural Block Sampling. (arXiv:1708.06040v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.06040", "type": "text/html"}], "timestampUsec": "1512450843840990", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505136e908\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505136e908&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Efficient Monte Carlo inference often requires manual construction of \nmodel-specific proposals. We propose an approach to automated proposal \nconstruction by training neural networks to provide fast approximations to \nblock Gibbs conditionals. The learned proposals generalize to occurrences of \ncommon structural motifs both within a given model and across models, allowing \nfor the construction of a library of learned inference primitives that can \naccelerate inference on unseen models with no model-specific training required. \nWe explore several applications including open-universe Gaussian mixture \nmodels, in which our learned proposals outperform a hand-tuned sampler, and a \nreal-world named entity recognition task, in which our sampler's ability to \nescape local modes yields higher final F1 scores than single-site Gibbs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763aaf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.06040"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xinchen Yan, Jasmine Hsu, Mohi Khansari, Yunfei Bai, Arkanath Pathak, Abhinav Gupta, James Davidson, Honglak Lee", "title": "Learning 6-DOF Grasping Interaction with Deep Geometry-aware 3D Representations. (arXiv:1708.07303v3 [cs.RO] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.07303", "type": "text/html"}], "timestampUsec": "1512450843840989", "comments": [], "summary": {"content": "<p>This paper focuses on the problem of learning 6-DOF grasping with a parallel \njaw gripper in simulation. We propose the notion of a geometry-aware \nrepresentation in grasping based on the assumption that knowledge of 3D \ngeometry is at the heart of interaction. Our key idea is constraining and \nregularizing grasping interaction learning through 3D geometry prediction. \nSpecifically, we formulate the learning of deep geometry-aware grasping model \nin two steps: First, we learn to build mental geometry-aware representation by \nreconstructing the scene (i.e., 3D occupancy grid) from RGBD input via \ngenerative 3D shape modeling. Second, we learn to predict grasping outcome with \nits internal geometry-aware representation. The learned outcome prediction \nmodel is used to sequentially propose grasping solutions via \nanalysis-by-synthesis optimization. Our contributions are fourfold: (1) To best \nof our knowledge, we are presenting for the first time a method to learn a \n6-DOF grasping net from RGBD input; (2) We build a grasping dataset from \ndemonstrations in virtual reality with rich sensory and interaction \nannotations. This dataset includes 101 everyday objects spread across 7 \ncategories, additionally, we propose a data augmentation strategy for effective \nlearning; (3) We demonstrate that the learned geometry-aware representation \nleads to about 10 percent relative performance improvement over the baseline \nCNN on grasping objects from our dataset. (4) We further demonstrate that the \nmodel generalizes to novel viewpoints and object instances. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763ab8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.07303"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wengong Jin, Connor W. Coley, Regina Barzilay, Tommi Jaakkola", "title": "Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network. (arXiv:1709.04555v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04555", "type": "text/html"}], "timestampUsec": "1512450843840988", "comments": [], "summary": {"content": "<p>The prediction of organic reaction outcomes is a fundamental problem in \ncomputational chemistry. Since a reaction may involve hundreds of atoms, fully \nexploring the space of possible transformations is intractable. The current \nsolution utilizes reaction templates to limit the space, but it suffers from \ncoverage and efficiency issues. In this paper, we propose a template-free \napproach to efficiently explore the space of product molecules by first \npinpointing the reaction center -- the set of nodes and edges where graph edits \noccur. Since only a small number of atoms contribute to reaction center, we can \ndirectly enumerate candidate products. The generated candidates are scored by a \nWeisfeiler-Lehman Difference Network that models high-order interactions \nbetween changes occurring at nodes across the molecule. Our framework \noutperforms the top-performing template-based approach with a 10\\% margin, \nwhile running orders of magnitude faster. Finally, we demonstrate that the \nmodel accuracy rivals the performance of domain experts. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763abc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04555"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yuanduo He, Xu Chu, Juguang Peng, Jingyue Gao, Yasha Wang", "title": "Motif-based Rule Discovery for Predicting Real-valued Time Series. (arXiv:1709.04763v4 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04763", "type": "text/html"}], "timestampUsec": "1512450843840987", "comments": [], "summary": {"content": "<p>Time series prediction is of great significance in many applications and has \nattracted extensive attention from the data mining community. Existing work \nsuggests that for many problems, the shape in the current time series may \ncorrelate an upcoming shape in the same or another series. Therefore, it is a \npromising strategy to associate two recurring patterns as a rule's antecedent \nand consequent: the occurrence of the antecedent can foretell the occurrence of \nthe consequent, and the learned shape of consequent will give accurate \npredictions. Earlier work employs symbolization methods, but the symbolized \nrepresentation maintains too little information of the original series to mine \nvalid rules. The state-of-the-art work, though directly manipulating the \nseries, fails to segment the series precisely for seeking \nantecedents/consequents, resulting in inaccurate rules in common scenarios. In \nthis paper, we propose a novel motif-based rule discovery method, which \nutilizes motif discovery to accurately extract frequently occurring consecutive \nsubsequences, i.e. motifs, as antecedents/consequents. It then investigates the \nunderlying relationships between motifs by matching motifs as rule candidates \nand ranking them based on the similarities. Experimental results on real open \ndatasets show that the proposed approach outperforms the baseline method by \n23.9%. Furthermore, it extends the applicability from single time series to \nmultiple ones. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763ac2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04763"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Fang Zhang, Xiaochen Wang, Jingfei Han, Jie Tang, Shiyin Wang, Marie-Francine Moens", "title": "Fast Top-k Area Topics Extraction with Knowledge Base. (arXiv:1710.04822v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.04822", "type": "text/html"}], "timestampUsec": "1512450843840986", "comments": [], "summary": {"content": "<p>What are the most popular research topics in Artificial Intelligence (AI)? We \nformulate the problem as extracting top-$k$ topics that can best represent a \ngiven area with the help of knowledge base. We theoretically prove that the \nproblem is NP-hard and propose an optimization model, FastKATE, to address this \nproblem by combining both explicit and latent representations for each topic. \nWe leverage a large-scale knowledge base (Wikipedia) to generate topic \nembeddings using neural networks and use this kind of representations to help \ncapture the representativeness of topics for given areas. We develop a fast \nheuristic algorithm to efficiently solve the problem with a provable error \nbound. We evaluate the proposed model on three real-world datasets. \nExperimental results demonstrate our model's effectiveness, robustness, \nreal-timeness (return results in $&lt;1$s), and its superiority over several \nalternative methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763ace", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.04822"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aixiang Chen, Bingchuan Chen, Xiaolong Chai, Rui Bian, Hengguang Li", "title": "A Novel Stochastic Stratified Average Gradient Method: Convergence Rate and Its Complexity. (arXiv:1710.07783v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.07783", "type": "text/html"}], "timestampUsec": "1512450843840985", "comments": [], "summary": {"content": "<p>SGD (Stochastic Gradient Descent) is a popular algorithm for large scale \noptimization problems due to its low iterative cost. However, SGD can not \nachieve linear convergence rate as FGD (Full Gradient Descent) because of the \ninherent gradient variance. To attack the problem, mini-batch SGD was proposed \nto get a trade-off in terms of convergence rate and iteration cost. In this \npaper, a general CVI (Convergence-Variance Inequality) equation is presented to \nstate formally the interaction of convergence rate and gradient variance. Then \na novel algorithm named SSAG (Stochastic Stratified Average Gradient) is \nintroduced to reduce gradient variance based on two techniques, stratified \nsampling and averaging over iterations that is a key idea in SAG (Stochastic \nAverage Gradient). Furthermore, SSAG can achieve linear convergence rate of \n$\\mathcal {O}((1-\\frac{\\mu}{8CL})^k)$ at smaller storage and iterative costs, \nwhere $C\\geq 2$ is the category number of training data. This convergence rate \ndepends mainly on the variance between classes, but not on the variance within \nthe classes. In the case of $C\\ll N$ ($N$ is the training data size), SSAG's \nconvergence rate is much better than SAG's convergence rate of $\\mathcal \n{O}((1-\\frac{\\mu}{8NL})^k)$. Our experimental results show SSAG outperforms SAG \nand many other algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763ad2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.07783"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Weichao Zhou, Wenchao Li", "title": "Safety-Aware Apprenticeship Learning. (arXiv:1710.07983v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.07983", "type": "text/html"}], "timestampUsec": "1512450843840984", "comments": [], "summary": {"content": "<p>Apprenticeship learning (AL) is a class of \"learning from demonstrations\" \ntechniques where the reward function of a Markov Decision Process (MDP) is \nunknown to the learning agent and the agent has to derive a good policy by \nobserving an expert's demonstrations. In this paper, we study the problem of \nhow to make AL algorithms inherently safe while still meeting its learning \nobjective. We consider a setting where the unknown reward function is assumed \nto be a linear combination of a set of state features, and the safety property \nis specified in Probabilistic Computation Tree Logic (PCTL). By embedding \nprobabilistic model checking inside AL, we propose a novel \ncounterexample-guided approach that can ensure both safety and performance of \nthe learned policy. We demonstrate the effectiveness of our approach on several \nchallenging AL scenarios where safety is essential. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763adb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.07983"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chong Huang, Peter Kairouz, Xiao Chen, Lalitha Sankar, Ram Rajagopal", "title": "Context-Aware Generative Adversarial Privacy. (arXiv:1710.09549v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.09549", "type": "text/html"}], "timestampUsec": "1512450843840983", "comments": [], "summary": {"content": "<p>Preserving the utility of published datasets while simultaneously providing \nprovable privacy guarantees is a well-known challenge. On the one hand, \ncontext-free privacy solutions, such as differential privacy, provide strong \nprivacy guarantees, but often lead to a significant reduction in utility. On \nthe other hand, context-aware privacy solutions, such as information theoretic \nprivacy, achieve an improved privacy-utility tradeoff, but assume that the data \nholder has access to dataset statistics. We circumvent these limitations by \nintroducing a novel context-aware privacy framework called generative \nadversarial privacy (GAP). GAP leverages recent advancements in generative \nadversarial networks (GANs) to allow the data holder to learn privatization \nschemes from the dataset itself. Under GAP, learning the privacy mechanism is \nformulated as a constrained minimax game between two players: a privatizer that \nsanitizes the dataset in a way that limits the risk of inference attacks on the \nindividuals' private variables, and an adversary that tries to infer the \nprivate variables from the sanitized dataset. To evaluate GAP's performance, we \ninvestigate two simple (yet canonical) statistical dataset models: (a) the \nbinary data model, and (b) the binary Gaussian mixture model. For both models, \nwe derive game-theoretically optimal minimax privacy mechanisms, and show that \nthe privacy mechanisms learned from data (in a generative adversarial fashion) \nmatch the theoretically optimal ones. This demonstrates that our framework can \nbe easily applied in practice, even in the absence of dataset statistics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763ae3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.09549"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Anna Harutyunyan, Peter Vrancx, Pierre-Luc Bacon, Doina Precup, Ann Nowe", "title": "Learning with Options that Terminate Off-Policy. (arXiv:1711.03817v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.03817", "type": "text/html"}], "timestampUsec": "1512450843840982", "comments": [], "summary": {"content": "<p>A temporally abstract action, or an option, is specified by a policy and a \ntermination condition: the policy guides option behavior, and the termination \ncondition roughly determines its length. Generally, learning with longer \noptions (like learning with multi-step returns) is known to be more efficient. \nHowever, if the option set for the task is not ideal, and cannot express the \nprimitive optimal policy exactly, shorter options offer more flexibility and \ncan yield a better solution. Thus, the termination condition puts learning \nefficiency at odds with solution quality. We propose to resolve this dilemma by \ndecoupling the behavior and target terminations, just like it is done with \npolicies in off-policy learning. To this end, we give a new algorithm, \nQ(\\beta), that learns the solution with respect to any termination condition, \nregardless of how the options actually terminate. We derive Q(\\beta) by casting \nlearning with options into a common framework with well-studied multi-step \noff-policy learning. We validate our algorithm empirically, and show that it \nholds up to its motivating claims. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763ae9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.03817"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kory Wallace Mathewson, Piotr Mirowski", "title": "Improvised Comedy as a Turing Test. (arXiv:1711.08819v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08819", "type": "text/html"}], "timestampUsec": "1512450843840981", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505136eb6d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505136eb6d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The best improvisational theatre actors can make any scene partner, of any \nskill level or ability, appear talented and proficient in the art form, and \nthus \"make them shine\". To challenge this improvisational paradigm, we built an \nartificial intelligence (AI) trained to perform live shows alongside human \nactors for human audiences. Over the course of 30 performances to a combined \naudience of almost 3000 people, we have refined theatrical games which involve \ncombinations of human and (at times, adversarial) AI actors. We have developed \nspecific scene structures to include audience participants in interesting ways. \nFinally, we developed a complete show structure that submitted the audience to \na Turing test and observed their suspension of disbelief, which we believe is \nkey for human/non-human theatre co-creation. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763af2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08819"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "YoungJoon Yoo, Seonguk Park, Junyoung Choi, Sangdoo Yun, Nojun Kwak", "title": "Butterfly Effect: Bidirectional Control of Classification Performance by Small Additive Perturbation. (arXiv:1711.09681v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09681", "type": "text/html"}], "timestampUsec": "1512450843840980", "comments": [], "summary": {"content": "<p>This paper proposes a new algorithm for controlling classification results by \ngenerating a small additive perturbation without changing the classifier \nnetwork. Our work is inspired by existing works generating adversarial \nperturbation that worsens classification performance. In contrast to the \nexisting methods, our work aims to generate perturbations that can enhance \noverall classification performance. To solve this performance enhancement \nproblem, we newly propose a perturbation generation network (PGN) influenced by \nthe adversarial learning strategy. In our problem, the information in a large \nexternal dataset is summarized by a small additive perturbation, which helps to \nimprove the performance of the classifier trained with the target dataset. In \naddition to this performance enhancement problem, we show that the proposed PGN \ncan be adopted to solve the classical adversarial problem without utilizing the \ninformation on the target classifier. The mentioned characteristics of our \nmethod are verified through extensive experiments on publicly available visual \ndatasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763af9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09681"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Clemente Rubio-Manzano, Tomas Lermanda Senoceain", "title": "How linguistic descriptions of data can help to the teaching-learning process in higher education, case of study: artificial intelligence. (arXiv:1711.09744v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09744", "type": "text/html"}], "timestampUsec": "1512450843840979", "comments": [], "summary": {"content": "<p>Artificial Intelligence is a central topic in the computer science \ncurriculum. From the year 2011 a project-based learning methodology based on \ncomputer games has been designed and implemented into the intelligence \nartificial course at the University of the Bio-Bio. The project aims to develop \nsoftware-controlled agents (bots) which are programmed by using heuristic \nalgorithms seen during the course. This methodology allows us to obtain good \nlearning results, however several challenges have been founded during its \nimplementation. \n</p> \n<p>In this paper we show how linguistic descriptions of data can help to provide \nstudents and teachers with technical and personalized feedback about the \nlearned algorithms. Algorithm behavior profile and a new Turing test for \ncomputer games bots based on linguistic modelling of complex phenomena are also \nproposed in order to deal with such challenges. \n</p> \n<p>In order to show and explore the possibilities of this new technology, a web \nplatform has been designed and implemented by one of authors and its \nincorporation in the process of assessment allows us to improve the teaching \nlearning process. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b00", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09744"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mohsen Ahmadi Fahandar, Eyke H&#xfc;llermeier", "title": "Learning to Rank based on Analogical Reasoning. (arXiv:1711.10207v1 [stat.ML] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10207", "type": "text/html"}], "timestampUsec": "1512450843840977", "comments": [], "summary": {"content": "<p>Object ranking or \"learning to rank\" is an important problem in the realm of \npreference learning. On the basis of training data in the form of a set of \nrankings of objects represented as feature vectors, the goal is to learn a \nranking function that predicts a linear order of any new set of objects. In \nthis paper, we propose a new approach to object ranking based on principles of \nanalogical reasoning. More specifically, our inference pattern is formalized in \nterms of so-called analogical proportions and can be summarized as follows: \nGiven objects $A,B,C,D$, if object $A$ is known to be preferred to $B$, and $C$ \nrelates to $D$ as $A$ relates to $B$, then $C$ is (supposedly) preferred to \n$D$. Our method applies this pattern as a main building block and combines it \nwith ideas and techniques from instance-based learning and rank aggregation. \nBased on first experimental results for data sets from various domains (sports, \neducation, tourism, etc.), we conclude that our approach is highly competitive. \nIt appears to be specifically interesting in situations in which the objects \nare coming from different subdomains, and which hence require a kind of \nknowledge transfer. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b08", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10207"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Samaneh Nasiri Ghosheh Bolagh, Gari. D. Clifford", "title": "Subject Selection on a Riemannian Manifold for Unsupervised Cross-subject Seizure Detection. (arXiv:1712.00465v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00465", "type": "text/html"}], "timestampUsec": "1512450843840976", "comments": [], "summary": {"content": "<p>Inter-subject variability between individuals poses a challenge in \ninter-subject brain signal analysis problems. A new algorithm for \nsubject-selection based on clustering covariance matrices on a Riemannian \nmanifold is proposed. After unsupervised selection of the subsets of relevant \nsubjects, data in a cluster is mapped to a tangent space at the mean point of \ncovariance matrices in that cluster and an SVM classifier on labeled data from \nrelevant subjects is trained. Experiment on an EEG seizure database shows that \nthe proposed method increases the accuracy over state-of-the-art from 86.83% to \n89.84% and specificity from 87.38% to 89.64% while reducing the false positive \nrate/hour from 0.8/hour to 0.77/hour. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b11", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00465"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hasham Ul Haq, Rameel Ahmad, Sibt Ul Hussain", "title": "Intelligent EHRs: Predicting Procedure Codes From Diagnosis Codes. (arXiv:1712.00481v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00481", "type": "text/html"}], "timestampUsec": "1512450843840975", "comments": [], "summary": {"content": "<p>In order to submit a claim to insurance companies, a doctor needs to code a \npatient encounter with both the diagnosis (ICDs) and procedures performed \n(CPTs) in an Electronic Health Record (EHR). Identifying and applying relevant \nprocedures code is a cumbersome and time-consuming task as a doctor has to \nchoose from around 13,000 procedure codes with no predefined one-to-one \nmapping. In this paper, we propose a state-of-the-art deep learning method for \nautomatic and intelligent coding of procedures (CPTs) from the diagnosis codes \n(ICDs) entered by the doctor. Precisely, we cast the learning problem as a \nmulti-label classification problem and use distributed representation to learn \nthe input mapping of high-dimensional sparse ICDs codes. Our final model \ntrained on 2.3 million claims is able to outperform existing rule-based \nprobabilistic and association-rule mining based methods and has a recall of \n90@3. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b1f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00481"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Michael C. Hughes, Gabriel Hope, Leah Weiner, Thomas H. McCoy, Roy H. Perlis, Erik B. Sudderth, Finale Doshi-Velez", "title": "Prediction-Constrained Topic Models for Antidepressant Recommendation. (arXiv:1712.00499v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00499", "type": "text/html"}], "timestampUsec": "1512450843840974", "comments": [], "summary": {"content": "<p>Supervisory signals can help topic models discover low-dimensional data \nrepresentations that are more interpretable for clinical tasks. We propose a \nframework for training supervised latent Dirichlet allocation that balances two \ngoals: faithful generative explanations of high-dimensional data and accurate \nprediction of associated class labels. Existing approaches fail to balance \nthese goals by not properly handling a fundamental asymmetry: the intended task \nis always predicting labels from data, not data from labels. Our new \nprediction-constrained objective trains models that predict labels from heldout \ndata well while also producing good generative likelihoods and interpretable \ntopic-word parameters. In a case study on predicting depression medications \nfrom electronic health records, we demonstrate improved recommendations \ncompared to previous supervised topic models and high- dimensional logistic \nregression from words alone. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b29", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00499"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rui Luo, Weinan Zhang, Xiaojun Xu, Jun Wang", "title": "A Neural Stochastic Volatility Model. (arXiv:1712.00504v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00504", "type": "text/html"}], "timestampUsec": "1512450843840973", "comments": [], "summary": {"content": "<p>In this paper, we show that the recent integration of statistical models with \ndeep recurrent neural networks provides a new way of formulating volatility \n(the degree of variation of time series) models that have been widely used in \ntime series analysis and prediction in finance. The model comprises a pair of \ncomplementary stochastic recurrent neural networks: the generative network \nmodels the joint distribution of the stochastic volatility process; the \ninference network approximates the conditional distribution of the latent \nvariables given the observables. Our focus here is on the formulation of \ntemporal dynamics of volatility over time under a stochastic recurrent neural \nnetwork framework. Experiments on real-world stock price datasets demonstrate \nthat the proposed model generates a better volatility estimation and prediction \nthat outperforms stronge baseline methods, including the deterministic models, \nsuch as GARCH and its variants, and the stochastic MCMC-based models, and the \nGaussian-process-based, on the average negative log-likelihood measure. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00504"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sunho Park, Tae Hyun Hwang", "title": "Bayesian Semi-nonnegative Tri-matrix Factorization to Identify Pathways Associated with Cancer Types. (arXiv:1712.00520v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00520", "type": "text/html"}], "timestampUsec": "1512450843840972", "comments": [], "summary": {"content": "<p>Identifying altered pathways that are associated with specific cancer types \ncan potentially bring a significant impact on cancer patient treatment. \nAccurate identification of such key altered pathways information can be used to \ndevelop novel therapeutic agents as well as to understand the molecular \nmechanisms of various types of cancers better. Tri-matrix factorization is an \nefficient tool to learn associations between two different entities (e.g., \ncancer types and pathways in our case) from data. To successfully apply \ntri-matrix factorization methods to biomedical problems, biological prior \nknowledge such as pathway databases or protein-protein interaction (PPI) \nnetworks, should be taken into account in the factorization model. However, it \nis not straightforward in the Bayesian setting even though Bayesian methods are \nmore appealing than point estimate methods, such as a maximum likelihood or a \nmaximum posterior method, in the sense that they calculate distributions over \nvariables and are robust against overfitting. We propose a Bayesian \n(semi-)nonnegative matrix factorization model for human cancer genomic data, \nwhere the biological prior knowledge represented by a pathway database and a \nPPI network is taken into account in the factorization model through a finite \ndependent Beta-Bernoulli prior. We tested our method on The Cancer Genome Atlas \n(TCGA) dataset and found that the pathways identified by our method can be used \nas a prognostic biomarkers for patient subgroup identification. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b38", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00520"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "George H. Chen, Jeremy C. Weiss", "title": "Survival-Supervised Topic Modeling with Anchor Words: Characterizing Pancreatitis Outcomes. (arXiv:1712.00535v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00535", "type": "text/html"}], "timestampUsec": "1512450843840971", "comments": [], "summary": {"content": "<p>We introduce a new approach for topic modeling that is supervised by survival \nanalysis. Specifically, we build on recent work on unsupervised topic modeling \nwith so-called anchor words by providing supervision through an elastic-net \nregularized Cox proportional hazards model. In short, an anchor word being \npresent in a document provides strong indication that the document is partially \nabout a specific topic. For example, by seeing \"gallstones\" in a document, we \nare fairly certain that the document is partially about medicine. Our proposed \nmethod alternates between learning a topic model and learning a survival model \nto find a local minimum of a block convex optimization problem. We apply our \nproposed approach to predicting how long patients with pancreatitis admitted to \nan intensive care unit (ICU) will stay in the ICU. Our approach is as accurate \nas the best of a variety of baselines while being more interpretable than any \nof the baselines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b41", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00535"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chen Fang, Panuwat Janwattanapong, Chunfei Li, Malek Adjouadi", "title": "A global feature extraction model for the effective computer aided diagnosis of mild cognitive impairment using structural MRI images. (arXiv:1712.00556v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00556", "type": "text/html"}], "timestampUsec": "1512450843840970", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505136ed79\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505136ed79&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Multiple modalities of biomarkers have been proved to be very sensitive in \nassessing the progression of Alzheimer's disease (AD), and using these \nmodalities and machine learning algorithms, several approaches have been \nproposed to assist in the early diagnosis of AD. Among the recent investigated \nstate-of-the-art approaches, Gaussian discriminant analysis (GDA)-based \napproaches have been demonstrated to be more effective and accurate in the \nclassification of AD, especially for delineating its prodromal stage of mild \ncognitive impairment (MCI). Moreover, among those binary classification \ninvestigations, the local feature extraction methods were mostly used, which \nmade them hardly be applied to a practical computer aided diagnosis system. \nTherefore, this study presents a novel global feature extraction model taking \nadvantage of the recent proposed GDA-based dual high-dimensional decision \nspaces, which can significantly improve the early diagnosis performance \ncomparing to those local feature extraction methods. In the true test using 20% \nheld-out data, for discriminating the most challenging MCI group from the \ncognitively normal control (CN) group, an F1 score of 91.06%, an accuracy of \n88.78%, a sensitivity of 91.80%, and a specificity of 83.78% were achieved that \ncan be considered as the best performance obtained so far. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b53", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00556"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chanh Nguyen, Georgi Georgiev, Yujie Ji, Ting Wang", "title": "Where Classification Fails, Interpretation Rises. (arXiv:1712.00558v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00558", "type": "text/html"}], "timestampUsec": "1512450843840969", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450513c65c3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450513c65c3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>An intriguing property of deep neural networks is their inherent \nvulnerability to adversarial inputs, which significantly hinders their \napplication in security-critical domains. Most existing detection methods \nattempt to use carefully engineered patterns to distinguish adversarial inputs \nfrom their genuine counterparts, which however can often be circumvented by \nadaptive adversaries. In this work, we take a completely different route by \nleveraging the definition of adversarial inputs: while deceiving for deep \nneural networks, they are barely discernible for human visions. Building upon \nrecent advances in interpretable models, we construct a new detection framework \nthat contrasts an input's interpretation against its classification. We \nvalidate the efficacy of this framework through extensive experiments using \nbenchmark datasets and attacks. We believe that this work opens a new direction \nfor designing adversarial input detection methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b5b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00558"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, Kevin Murphy", "title": "Progressive Neural Architecture Search. (arXiv:1712.00559v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.00559", "type": "text/html"}], "timestampUsec": "1512450843840968", "comments": [], "summary": {"content": "<p>We propose a method for learning CNN structures that is more efficient than \nprevious approaches: instead of using reinforcement learning (RL) or genetic \nalgorithms (GA), we use a sequential model-based optimization (SMBO) strategy, \nin which we search for architectures in order of increasing complexity, while \nsimultaneously learning a surrogate function to guide the search, similar to A* \nsearch. On the CIFAR-10 dataset, our method finds a CNN structure with the same \nclassification accuracy (3.41% error rate) as the RL method of Zoph et al. \n(2017), but 2 times faster (in terms of number of models evaluated). It also \noutperforms the GA method of Liu et al. (2017), which finds a model with worse \nperformance (3.63% error rate), and takes 5 times longer. Finally we show that \nthe model we learned on CIFAR also works well at the task of ImageNet \nclassification. In particular, we match the state-of-the-art performance of \n82.9% top-1 and 96.1% top-5 accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b64", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00559"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gabriel Erion, Hugh Chen, Scott M. Lundberg, Su-In Lee", "title": "Anesthesiologist-level forecasting of hypoxemia with only SpO2 data using deep learning. (arXiv:1712.00563v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00563", "type": "text/html"}], "timestampUsec": "1512450843840967", "comments": [], "summary": {"content": "<p>We use a deep learning model trained only on a patient's blood oxygenation \ndata (measurable with an inexpensive fingertip sensor) to predict impending \nhypoxemia (low blood oxygen) more accurately than trained anesthesiologists \nwith access to all the data recorded in a modern operating room. We also \nprovide a simple way to visualize the reason why a patient's risk is low or \nhigh by assigning weight to the patient's past blood oxygen values. This work \nhas the potential to provide cutting-edge clinical decision support in \nlow-resource settings, where rates of surgical complication and death are \nsubstantially greater than in high-resource areas. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b7d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00563"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zihao Hu, Xiyi Luo, Hongtao Lu, Yong Yu", "title": "Supervised Hashing based on Energy Minimization. (arXiv:1712.00573v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00573", "type": "text/html"}], "timestampUsec": "1512450843840966", "comments": [], "summary": {"content": "<p>Recently, supervised hashing methods have attracted much attention since they \ncan optimize retrieval speed and storage cost while preserving semantic \ninformation. Because hashing codes learning is NP-hard, many methods resort to \nsome form of relaxation technique. But the performance of these methods can \neasily deteriorate due to the relaxation. Luckily, many supervised hashing \nformulations can be viewed as energy functions, hence solving hashing codes is \nequivalent to learning marginals in the corresponding conditional random field \n(CRF). By minimizing the KL divergence between a fully factorized distribution \nand the Gibbs distribution of this CRF, a set of consistency equations can be \nobtained, but updating them in parallel may not yield a local optimum since the \nvariational lower bound is not guaranteed to increase. In this paper, we use a \nlinear approximation of the sigmoid function to convert these consistency \nequations to linear systems, which have a closed-form solution. By applying \nthis novel technique to two classical hashing formulations KSH and SPLH, we \nobtain two new methods called EM (energy minimizing based)-KSH and EM-SPLH. \nExperimental results on three datasets show the superiority of our methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b8c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00573"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Maggie Makar, Marzyeh Ghassemi, David Cutler, Ziad Obermeyer", "title": "Short-term Mortality Prediction for Elderly Patients Using Medicare Claims Data. (arXiv:1712.00644v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00644", "type": "text/html"}], "timestampUsec": "1512450843840965", "comments": [], "summary": {"content": "<p>Risk prediction is central to both clinical medicine and public health. While \nmany machine learning models have been developed to predict mortality, they are \nrarely applied in the clinical literature, where classification tasks typically \nrely on logistic regression. One reason for this is that existing machine \nlearning models often seek to optimize predictions by incorporating features \nthat are not present in the databases readily available to providers and policy \nmakers, limiting generalizability and implementation. Here we tested a number \nof machine learning classifiers for prediction of six-month mortality in a \npopulation of elderly Medicare beneficiaries, using an administrative claims \ndatabase of the kind available to the majority of health care payers and \nproviders. We show that machine learning classifiers substantially outperform \ncurrent widely-used methods of risk prediction but only when used with an \nimproved feature set incorporating insights from clinical medicine, developed \nfor this study. Our work has applications to supporting patient and provider \ndecision making at the end of life, as well as population health-oriented \nefforts to identify patients at high risk of poor outcomes. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b96", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00644"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xuanqing Liu, Minhao Cheng, Huan Zhang, Cho-Jui Hsieh", "title": "Towards Robust Neural Networks via Random Self-ensemble. (arXiv:1712.00673v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00673", "type": "text/html"}], "timestampUsec": "1512450843840964", "comments": [], "summary": {"content": "<p>Recent studies have revealed the vulnerability of deep neural networks - A \nsmall adversarial perturbation that is imperceptible to human can easily make a \nwell-trained deep neural network mis-classify. This makes it unsafe to apply \nneural networks in security-critical applications. In this paper, we propose a \nnew defensive algorithm called Random Self-Ensemble (RSE) by combining two \nimportant concepts: ${\\bf randomness}$ and ${\\bf ensemble}$. To protect a \ntargeted model, RSE adds random noise layers to the neural network to prevent \nfrom state-of-the-art gradient-based attacks, and ensembles the prediction over \nrandom noises to stabilize the performance. We show that our algorithm is \nequivalent to ensemble an infinite number of noisy models $f_\\epsilon$ without \nany additional memory overhead, and the proposed training procedure based on \nnoisy stochastic gradient descent can ensure the ensemble model has good \npredictive capability. Our algorithm significantly outperforms previous defense \ntechniques on real datasets. For instance, on CIFAR-10 with VGG network (which \nhas $92\\%$ accuracy without any attack), under the state-of-the-art C&amp;W attack \nwithin a certain distortion tolerance, the accuracy of unprotected model drops \nto less than $10\\%$, the best previous defense technique has $48\\%$ accuracy, \nwhile our method still has $86\\%$ prediction accuracy under the same level of \nattack. Finally, our method is simple and easy to integrate into any neural \nnetwork. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763b9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00673"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513669231, "author": "Frans A. Oliehoek, Rahul Savani, Jose Gallego-Posada, Elise van der Pol, Edwin D. de Jong, Roderich Gross", "title": "GANGs: Generative Adversarial Network Games. (arXiv:1712.00679v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00679", "type": "text/html"}], "timestampUsec": "1512450843840963", "comments": [], "summary": {"content": "<p>Generative Adversarial Networks (GAN) have become one of the most successful \nframeworks for unsupervised generative modeling. As GANs are difficult to train \nmuch research has focused on this. However, very little of this research has \ndirectly exploited game-theoretic techniques. We introduce Generative \nAdversarial Network Games (GANGs), which explicitly model a finite zero-sum \ngame between a generator ($G$) and classifier ($C$) that use mixed strategies. \nThe size of these games precludes exact solution methods, therefore we define \nresource-bounded best responses (RBBRs), and a resource-bounded Nash \nEquilibrium (RB-NE) as a pair of mixed strategies such that neither $G$ or $C$ \ncan find a better RBBR. The RB-NE solution concept is richer than the notion of \n`local Nash equilibria' in that it captures not only failures of escaping local \noptima of gradient descent, but applies to any approximate best response \ncomputations, including methods with random restarts. To validate our approach, \nwe solve GANGs with the Parallel Nash Memory algorithm, which provably \nmonotonically converges to an RB-NE. We compare our results to standard GAN \nsetups, and demonstrate that our method deals well with typical GAN problems \nsuch as mode collapse, partial mode coverage and forgetting. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763baa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00679"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rajeev Ranjan, Swami Sankaranarayanan, Carlos D. Castillo, Rama Chellappa", "title": "Improving Network Robustness against Adversarial Attacks with Compact Convolution. (arXiv:1712.00699v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00699", "type": "text/html"}], "timestampUsec": "1512450843840962", "comments": [], "summary": {"content": "<p>Though Convolutional Neural Networks (CNNs) have surpassed human-level \nperformance on tasks such as object classification and face verification, they \ncan easily be fooled by adversarial attacks. These attacks add a small \nperturbation to the input image that causes the network to mis-classify the \nsample. In this paper, we focus on neutralizing adversarial attacks by \nexploring the effect of different loss functions such as CenterLoss and \nL2-Softmax Loss for enhanced robustness to adversarial perturbations. \nAdditionally, we propose power convolution, a novel method of convolution that \nwhen incorporated in conventional CNNs improve their robustness. Power \nconvolution ensures that features at every layer are bounded and close to each \nother. Extensive experiments show that Power Convolutional Networks (PCNs) \nneutralize multiple types of attacks, and perform better than existing methods \nfor defending adversarial attacks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763bc5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00699"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Qing Qu, Yuqian Zhang, Yonina C. Eldar, John Wright", "title": "Convolutional Phase Retrieval via Gradient Descent. (arXiv:1712.00716v1 [stat.CO])", "alternate": [{"href": "http://arxiv.org/abs/1712.00716", "type": "text/html"}], "timestampUsec": "1512450843840961", "comments": [], "summary": {"content": "<p>We study the convolutional phase retrieval problem, which considers \nrecovering an unknown signal $\\mathbf x \\in \\mathbb C^n $ from $m$ measurements \nconsisting of the magnitude of its cyclic convolution with a known kernel \n$\\mathbf a \\in \\mathbb C^m $. This model is motivated by applications such as \nchannel estimation, optics, and underwater acoustic communication, where the \nsignal of interest is acted on by a given channel/filter, and phase information \nis difficult or impossible to acquire. We show that when $\\mathbf a$ is random \nand the sample number $m$ is sufficiently large, with high probability $\\mathbf \nx$ can be efficiently recovered up to a global phase using a combination of \nspectral initialization and generalized gradient descent. The main challenge is \ncoping with dependencies in the measurement operator. We overcome this \nchallenge by using ideas from decoupling theory, suprema of chaos processes and \nthe restricted isometry property of random circulant matrices, and recent \nanalysis for alternating minimization methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763bd3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00716"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hongwei Wang, Jia Wang, Miao Zhao, Jiannong Cao, Minyi Guo", "title": "Joint Topic-Semantic-aware Social Recommendation for Online Voting. (arXiv:1712.00731v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00731", "type": "text/html"}], "timestampUsec": "1512450843840960", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450513c6840\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450513c6840&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Online voting is an emerging feature in social networks, in which users can \nexpress their attitudes toward various issues and show their unique interest. \nOnline voting imposes new challenges on recommendation, because the propagation \nof votings heavily depends on the structure of social networks as well as the \ncontent of votings. In this paper, we investigate how to utilize these two \nfactors in a comprehensive manner when doing voting recommendation. First, due \nto the fact that existing text mining methods such as topic model and semantic \nmodel cannot well process the content of votings that is typically short and \nambiguous, we propose a novel Topic-Enhanced Word Embedding (TEWE) method to \nlearn word and document representation by jointly considering their topics and \nsemantics. Then we propose our Joint Topic-Semantic-aware social Matrix \nFactorization (JTS-MF) model for voting recommendation. JTS-MF model calculates \nsimilarity among users and votings by combining their TEWE representation and \nstructural information of social networks, and preserves this \ntopic-semantic-social similarity during matrix factorization. To evaluate the \nperformance of TEWE representation and JTS-MF model, we conduct extensive \nexperiments on real online voting dataset. The results prove the efficacy of \nour approach against several state-of-the-art baselines. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763bd8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00731"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hongwei Wang, Fuzheng Zhang, Min Hou, Xing Xie, Minyi Guo, Qi Liu", "title": "SHINE: Signed Heterogeneous Information Network Embedding for Sentiment Link Prediction. (arXiv:1712.00732v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00732", "type": "text/html"}], "timestampUsec": "1512450843840959", "comments": [], "summary": {"content": "<p>In online social networks people often express attitudes towards others, \nwhich forms massive sentiment links among users. Predicting the sign of \nsentiment links is a fundamental task in many areas such as personal \nadvertising and public opinion analysis. Previous works mainly focus on textual \nsentiment classification, however, text information can only disclose the \"tip \nof the iceberg\" about users' true opinions, of which the most are unobserved \nbut implied by other sources of information such as social relation and users' \nprofile. To address this problem, in this paper we investigate how to predict \npossibly existing sentiment links in the presence of heterogeneous information. \nFirst, due to the lack of explicit sentiment links in mainstream social \nnetworks, we establish a labeled heterogeneous sentiment dataset which consists \nof users' sentiment relation, social relation and profile knowledge by \nentity-level sentiment extraction method. Then we propose a novel and flexible \nend-to-end Signed Heterogeneous Information Network Embedding (SHINE) framework \nto extract users' latent representations from heterogeneous networks and \npredict the sign of unobserved sentiment links. SHINE utilizes multiple deep \nautoencoders to map each user into a low-dimension feature space while \npreserving the network structure. We demonstrate the superiority of SHINE over \nstate-of-the-art baselines on link prediction and node recommendation in two \nreal-world datasets. The experimental results also prove the efficacy of SHINE \nin cold start scenario. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763be1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00732"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wenqi Wang, Vaneet Aggarwal, Shuchin Aeron", "title": "Tensor Train Neighborhood Preserving Embedding. (arXiv:1712.00828v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00828", "type": "text/html"}], "timestampUsec": "1512450843840958", "comments": [], "summary": {"content": "<p>In this paper, we propose a Tensor Train Neighborhood Preserving Embedding \n(TTNPE) to embed multi-dimensional tensor data into low dimensional tensor \nsubspace. Novel approaches to solve the optimization problem in TTNPE are \nproposed. For this embedding, we evaluate novel trade-off gain among \nclassification, computation, and dimensionality reduction (storage) for \nsupervised learning. It is shown that compared to the state-of-the-arts tensor \nembedding methods, TTNPE achieves superior trade-off in classification, \ncomputation, and dimensionality reduction in MNIST handwritten digits and \nWeizmann face datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763bf6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00828"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Matthew Kupilik, Frank Witmer, Euan-Angus MacLeod, Caixia Wang, Tom Ravens", "title": "Gaussian Process Regression for Arctic Coastal Erosion Forecasting. (arXiv:1712.00867v1 [physics.geo-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.00867", "type": "text/html"}], "timestampUsec": "1512450843840957", "comments": [], "summary": {"content": "<p>Arctic coastal morphology is governed by multiple factors, many of which are \naffected by climatological changes. As the season length for shorefast ice \ndecreases and temperatures warm permafrost soils, coastlines are more \nsusceptible to erosion from storm waves. Such coastal erosion is a concern, \nsince the majority of the population centers and infrastructure in the Arctic \nare located near the coasts. Stakeholders and decision makers increasingly need \nmodels capable of scenario-based predictions to assess and mitigate the effects \nof coastal morphology on infrastructure and land use. Our research uses \nGaussian process models to forecast Arctic coastal erosion along the Beaufort \nSea near Drew Point, AK. Gaussian process regression is a data-driven modeling \nmethodology capable of extracting patterns and trends from data-sparse \nenvironments such as remote Arctic coastlines. To train our model, we use \nannual coastline positions and near-shore summer temperature averages from \nexisting datasets and extend these data by extracting additional coastlines \nfrom satellite imagery. We combine our calibrated models with future climate \nmodels to generate a range of plausible future erosion scenarios. Our results \nshow that the Gaussian process methodology substantially improves yearly \npredictions compared to linear and nonlinear least squares methods, and is \ncapable of generating detailed forecasts suitable for use by decision makers. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763c11", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00867"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512537806, "author": "Mostafa Rahmani, George Atia", "title": "Data Dropout in Arbitrary Basis for Deep Network Regularization. (arXiv:1712.00891v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00891", "type": "text/html"}], "timestampUsec": "1512450843840956", "comments": [], "summary": {"content": "<p>An important problem in training deep networks with high capacity is to \nensure that the trained network works well when presented with new inputs \noutside the training dataset. Dropout is an effective regularization technique \nto boost the network generalization in which a random subset of the elements of \nthe given data and the extracted features are set to zero during the training \nprocess. In this paper, a new randomized regularization technique in which we \nwithhold a random part of the data without necessarily turning off the \nneurons/data-elements is proposed. In the proposed method, of which the \nconventional dropout is shown to be a special case, random data dropout is \nperformed in an arbitrary basis, hence the designation Generalized Dropout. We \nalso present a framework whereby the proposed technique can be applied \nefficiently to convolutional neural networks. The presented numerical \nexperiments demonstrate that the proposed technique yields notable performance \ngain. Generalized Dropout provides new insight into the idea of dropout, shows \nthat we can achieve different performance gains by using different bases \nmatrices, and opens up a new research question as of how to choose optimal \nbases matrices that achieve maximal performance gain. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763c29", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00891"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513098583, "author": "Giambattista Parascandolo, Mateo Rojas-Carulla, Niki Kilbertus, Bernhard Sch&#xf6;lkopf", "title": "Learning Independent Causal Mechanisms. (arXiv:1712.00961v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00961", "type": "text/html"}], "timestampUsec": "1512450843840955", "comments": [], "summary": {"content": "<p>Independent causal mechanisms are a central concept in the study of causality \nwith implications for machine learning tasks. In this work we develop an \nalgorithm to recover a set of (inverse) independent mechanisms relating a \ndistribution transformed by the mechanisms to a reference distribution. The \napproach is fully unsupervised and based on a set of experts that compete for \ndata to specialize and extract the mechanisms. We test and analyze the proposed \nmethod on a series of experiments based on image transformations. Each expert \nsuccessfully maps a subset of the transformed data to the original domain, and \nthe learned mechanisms generalize to other domains. We discuss implications for \ndomain transfer and links to recent trends in generative modeling. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1513098583, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763c3f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00961"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Emanuele Pesce, Petros-Pavlos Ypsilantis, Samuel Withey, Robert Bakewell, Vicky Goh, Giovanni Montana", "title": "Learning to detect chest radiographs containing lung nodules using visual attention networks. (arXiv:1712.00996v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00996", "type": "text/html"}], "timestampUsec": "1512450843840954", "comments": [], "summary": {"content": "<p>Machine learning approaches hold great potential for the automated detection \nof lung nodules in chest radiographs, but training the algorithms requires vary \nlarge amounts of manually annotated images, which are difficult to obtain. Weak \nlabels indicating whether a radiograph is likely to contain pulmonary nodules \nare typically easier to obtain at scale by parsing historical free-text \nradiological reports associated to the radiographs. Using a repositotory of \nover 700,000 chest radiographs, in this study we demonstrate that promising \nnodule detection performance can be achieved using weak labels through \nconvolutional neural networks for radiograph classification. We propose two \nnetwork architectures for the classification of images likely to contain \npulmonary nodules using both weak labels and manually-delineated bounding \nboxes, when these are available. Annotated nodules are used at training time to \ndeliver a visual attention mechanism informing the model about its localisation \nperformance. The first architecture extracts saliency maps from high-level \nconvolutional layers and compares the estimated position of a nodule against \nthe ground truth, when this is available. A corresponding localisation error is \nthen back-propagated along with the softmax classification error. The second \napproach consists of a recurrent attention model that learns to observe a short \nsequence of smaller image portions through reinforcement learning. When a \nnodule annotation is available at training time, the reward function is \nmodified accordingly so that exploring portions of the radiographs away from a \nnodule incurs a larger penalty. Our empirical results demonstrate the potential \nadvantages of these architectures in comparison to competing methodologies. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763c4b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00996"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yi Xu, Rong Jin, Tianbao Yang", "title": "NEON+: Accelerated Gradient Methods for Extracting Negative Curvature for Non-Convex Optimization. (arXiv:1712.01033v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1712.01033", "type": "text/html"}], "timestampUsec": "1512450843840953", "comments": [], "summary": {"content": "<p>Accelerated gradient (AG) methods are breakthroughs in convex optimization, \nimproving the convergence rate of the gradient descent method for optimization \nwith smooth functions. However, the analysis of AG methods for non-convex \noptimization is still limited. It remains an open question whether AG methods \nfrom convex optimization can accelerate the convergence of the gradient descent \nmethod for finding local minimum of non-convex optimization problems. This \npaper provides an affirmative answer to this question. In particular, we \nanalyze two renowned variants of AG methods (namely Polyak's Heavy Ball method \nand Nesterov's Accelerated Gradient method) for extracting the negative \ncurvature from random noise, which is central to escaping from saddle points. \nBy leveraging the proposed AG methods for extracting the negative curvature, we \npresent a new AG algorithm with double loops for non-convex \noptimization~\\footnote{this is in contrast to a single-loop AG algorithm \nproposed in a recent manuscript~\\citep{AGNON}, which directly analyzed the \nNesterov's AG method for non-convex optimization and appeared online on \nNovember 29, 2017. However, we emphasize that our work is an independent work, \nwhich is inspired by our earlier work~\\citep{NEON17} and is based on a \ndifferent novel analysis.}, which converges to second-order stationary point \n$\\x$ such that $\\|\\nabla f(\\x)\\|\\leq \\epsilon$ and $\\nabla^2 f(\\x)\\geq \n-\\sqrt{\\epsilon} I$ with $\\widetilde O(1/\\epsilon^{1.75})$ iteration \ncomplexity, improving that of gradient descent method by a factor of \n$\\epsilon^{-0.25}$ and matching the best iteration complexity of second-order \nHessian-free methods for non-convex optimization. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763c74", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01033"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mohammad Emtiyaz Khan, Zuozhu Liu, Voot Tangkaratt, Yarin Gal", "title": "Vprop: Variational Inference using RMSprop. (arXiv:1712.01038v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01038", "type": "text/html"}], "timestampUsec": "1512450843840952", "comments": [], "summary": {"content": "<p>Many computationally-efficient methods for Bayesian deep learning rely on \ncontinuous optimization algorithms, but the implementation of these methods \nrequires significant changes to existing code-bases. In this paper, we propose \nVprop, a method for Gaussian variational inference that can be implemented with \ntwo minor changes to the off-the-shelf RMSprop optimizer. Vprop also reduces \nthe memory requirements of Black-Box Variational Inference by half. We derive \nVprop using the conjugate-computation variational inference method, and \nestablish its connections to Newton's method, natural-gradient methods, and \nextended Kalman filters. Overall, this paper presents Vprop as a principled, \ncomputationally-efficient, and easy-to-implement method for Bayesian deep \nlearning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763c94", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01038"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, Pascal Frossard", "title": "Adaptive Quantization for Deep Neural Network. (arXiv:1712.01048v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01048", "type": "text/html"}], "timestampUsec": "1512450843840951", "comments": [], "summary": {"content": "<p>In recent years Deep Neural Networks (DNNs) have been rapidly developed in \nvarious applications, together with increasingly complex architectures. The \nperformance gain of these DNNs generally comes with high computational costs \nand large memory consumption, which may not be affordable for mobile platforms. \nDeep model quantization can be used for reducing the computation and memory \ncosts of DNNs, and deploying complex DNNs on mobile equipment. In this work, we \npropose an optimization framework for deep model quantization. First, we \npropose a measurement to estimate the effect of parameter quantization errors \nin individual layers on the overall model prediction accuracy. Then, we propose \nan optimization process based on this measurement for finding optimal \nquantization bit-width for each layer. This is the first work that \ntheoretically analyse the relationship between parameter quantization errors of \nindividual layers and model accuracy. Our new quantization algorithm \noutperforms previous quantization optimization methods, and achieves 20-40% \nhigher compression rate compared to equal bit-width quantization at the same \nmodel prediction accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763ca9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01048"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Muhammad Raza Khan, Joshua Blumenstock", "title": "Determinants of Mobile Money Adoption in Pakistan. (arXiv:1712.01081v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01081", "type": "text/html"}], "timestampUsec": "1512450843840950", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450513c6a77\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450513c6a77&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this work, we analyze the problem of adoption of mobile money in Pakistan \nby using the call detail records of a major telecom company as our input. Our \nresults highlight the fact that different sections of the society have \ndifferent patterns of adoption of digital financial services but user mobility \nrelated features are the most important one when it comes to adopting and using \nmobile money services. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763cbe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01081"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Dieter Hendricks, Adam Cobb, Richard Everett, Jonathan Downing, Stephen J. Roberts", "title": "Inferring agent objectives at different scales of a complex adaptive system. (arXiv:1712.01137v1 [q-fin.TR])", "alternate": [{"href": "http://arxiv.org/abs/1712.01137", "type": "text/html"}], "timestampUsec": "1512450843840949", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45051429875\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45051429875&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We introduce a framework to study the effective objectives at different time \nscales of financial market microstructure. The financial market can be regarded \nas a complex adaptive system, where purposeful agents collectively and \nsimultaneously create and perceive their environment as they interact with it. \nIt has been suggested that multiple agent classes operate in this system, with \na non-trivial hierarchy of top-down and bottom-up causation classes with \ndifferent effective models governing each level. We conjecture that agent \nclasses may in fact operate at different time scales and thus act differently \nin response to the same perceived market state. Given scale-specific temporal \nstate trajectories and action sequences estimated from aggregate market \nbehaviour, we use Inverse Reinforcement Learning to compute the effective \nreward function for the aggregate agent class at each scale, allowing us to \nassess the relative attractiveness of feature vectors across different scales. \nDifferences in reward functions for feature vectors may indicate different \nobjectives of market participants, which could assist in finding the scale \nboundary for agent classes. This has implications for learning algorithms \noperating in this domain. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763cd5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01137"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Abdul-Saboor Sheikh, Kashif Rasul, Andreas Merentitis, Urs Bergmann", "title": "Stochastic Maximum Likelihood Optimization via Hypernetworks. (arXiv:1712.01141v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01141", "type": "text/html"}], "timestampUsec": "1512450843840948", "comments": [], "summary": {"content": "<p>This work explores maximum likelihood optimization of neural networks through \nhypernetworks. A hypernetwork initializes the weights of another network, which \nin turn can be employed for typical functional tasks such as regression and \nclassification. We optimize hypernetworks to directly maximize the conditional \nlikelihood of target variables given input. Using this approach we obtain \ncompetitive empirical results on regression and classification benchmarks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763cdf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01141"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mohsen Ahmadi Fahandar, Eyke H&#xfc;llermeier, In&#xe9;s Couso", "title": "Statistical Inference for Incomplete Ranking Data: The Case of Rank-Dependent Coarsening. (arXiv:1712.01158v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.01158", "type": "text/html"}], "timestampUsec": "1512450843840947", "comments": [], "summary": {"content": "<p>We consider the problem of statistical inference for ranking data, \nspecifically rank aggregation, under the assumption that samples are incomplete \nin the sense of not comprising all choice alternatives. In contrast to most \nexisting methods, we explicitly model the process of turning a full ranking \ninto an incomplete one, which we call the coarsening process. To this end, we \npropose the concept of rank-dependent coarsening, which assumes that incomplete \nrankings are produced by projecting a full ranking to a random subset of ranks. \nFor a concrete instantiation of our model, in which full rankings are drawn \nfrom a Plackett-Luce distribution and observations take the form of pairwise \npreferences, we study the performance of various rank aggregation methods. In \naddition to predictive accuracy in the finite sample setting, we address the \ntheoretical question of consistency, by which we mean the ability to recover a \ntarget ranking when the sample size goes to infinity, despite a potential bias \nin the observations caused by the (unknown) coarsening. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763cf6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01158"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "David G. Nagy, Gerg&#x151; Orb&#xe1;n", "title": "Episodic memory for continual model learning. (arXiv:1712.01169v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01169", "type": "text/html"}], "timestampUsec": "1512450843840946", "comments": [], "summary": {"content": "<p>Both the human brain and artificial learning agents operating in real-world \nor comparably complex environments are faced with the challenge of online model \nselection. In principle this challenge can be overcome: hierarchical Bayesian \ninference provides a principled method for model selection and it converges on \nthe same posterior for both off-line (i.e. batch) and online learning. However, \nmaintaining a parameter posterior for each model in parallel has in general an \neven higher memory cost than storing the entire data set and is consequently \nclearly unfeasible. Alternatively, maintaining only a limited set of models in \nmemory could limit memory requirements. However, sufficient statistics for one \nmodel will usually be insufficient for fitting a different kind of model, \nmeaning that the agent loses information with each model change. We propose \nthat episodic memory can circumvent the challenge of limited memory-capacity \nonline model selection by retaining a selected subset of data points. We design \na method to compute the quantities necessary for model selection even when the \ndata is discarded and only statistics of one (or few) learnt models are \navailable. We demonstrate on a simple model that a limited-sized episodic \nmemory buffer, when the content is optimised to retain data with statistics not \nmatching the current representation, can resolve the fundamental challenge of \nonline model selection. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763cfd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01169"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Madhav Nimishakavi, Pratik Jawanpuria, Bamdev Mishra", "title": "A Dual Framework for Low-rank Tensor Completion. (arXiv:1712.01193v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.01193", "type": "text/html"}], "timestampUsec": "1512450843840945", "comments": [], "summary": {"content": "<p>We propose a novel formulation of the low-rank tensor completion problem that \nis based on the duality theory and a particular choice of low-rank regularizer. \nThis low-rank regularizer along with the dual perspective provides a simple \ncharacterization of the solution to the tensor completion problem. Motivated by \nlarge-scale setting, we next derive a rank-constrained reformulation of the \nproposed optimization problem, which is shown to lie on the Riemannian \nspectrahedron manifold. We exploit the versatile Riemannian optimization \nframework to develop computationally efficient conjugate gradient and \ntrust-region algorithms. The experiments confirm the benefits of our choice of \nregularization and the proposed algorithms outperform state-of-the-art \nalgorithms on several real-world data sets in different applications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.01193"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christopher Nowzohour, Marloes H. Maathuis, Robin J. Evans, Peter B&#xfc;hlmann", "title": "Distributional Equivalence and Structure Learning for Bow-free Acyclic Path Diagrams. (arXiv:1508.01717v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1508.01717", "type": "text/html"}], "timestampUsec": "1512450843840944", "comments": [], "summary": {"content": "<p>We consider the problem of structure learning for bow-free acyclic path \ndiagrams (BAPs). BAPs can be viewed as a generalization of linear Gaussian DAG \nmodels that allow for certain hidden variables. We present a first method for \nthis problem using a greedy score-based search algorithm. We also prove some \nnecessary and some sufficient conditions for distributional equivalence of BAPs \nwhich are used in an algorithmic ap- proach to compute (nearly) equivalent \nmodel structures. This allows us to infer lower bounds of causal effects. We \nalso present applications to real and simulated datasets using our publicly \navailable R-package. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d27", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1508.01717"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sandrine Dallaporta, Yohann De Castro", "title": "Sparse Recovery Guarantees from Extreme Eigenvalues Small Deviations. (arXiv:1604.01171v3 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1604.01171", "type": "text/html"}], "timestampUsec": "1512450843840943", "comments": [], "summary": {"content": "<p>This article provides a new toolbox to derive sparse recovery guarantees from \nsmall deviations on extreme singular values or extreme eigenvalues obtained in \nRandom Matrix Theory. This work is based on Restricted Isometry Constants \n(RICs) which are a pivotal notion in Compressed Sensing and High-Dimensional \nStatistics as these constants finely assess how a linear operator is \nconditioned on the set of sparse vectors and hence how it performs in SRSR. \nWhile it is an open problem to construct deterministic matrices with apposite \nRICs, one can prove that such matrices exist using random matrices models. In \nthis paper, we show upper bounds on RICs for Gaussian and Rademacher matrices \nusing state-of-the-art small deviation estimates on their extreme eigenvalues. \nThis allows us to derive a lower bound on the probability of getting SRSR. One \nbenefit of this paper is a direct and explicit derivation of upper bounds on \nRICs and lower bounds on SRSR from small deviations on the extreme eigenvalues \ngiven by Random Matrix theory. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1604.01171"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nicholas Boyd, Trevor Hastie, Stephen Boyd, Benjamin Recht, Michael Jordan", "title": "Saturating Splines and Feature Selection. (arXiv:1609.06764v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1609.06764", "type": "text/html"}], "timestampUsec": "1512450843840942", "comments": [], "summary": {"content": "<p>We extend the adaptive regression spline model by incorporating saturation, \nthe natural requirement that a function extend as a constant outside a certain \nrange. We fit saturating splines to data using a convex optimization problem \nover a space of measures, which we solve using an efficient algorithm based on \nthe conditional gradient method. Unlike many existing approaches, our algorithm \nsolves the original infinite-dimensional (for splines of degree at least two) \noptimization problem without pre-specified knot locations. We then adapt our \nalgorithm to fit generalized additive models with saturating splines as \ncoordinate functions and show that the saturation requirement allows our model \nto simultaneously perform feature selection and nonlinear function fitting. \nFinally, we briefly sketch how the method can be extended to higher order \nsplines and to different requirements on the extension outside the data range. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d42", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1609.06764"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ryan J. Gallagher, Kyle Reing, David Kale, Greg Ver Steeg", "title": "Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge. (arXiv:1611.10277v3 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1611.10277", "type": "text/html"}], "timestampUsec": "1512450843840941", "comments": [], "summary": {"content": "<p>While generative models such as Latent Dirichlet Allocation (LDA) have proven \nfruitful in topic modeling, they often require detailed assumptions and careful \nspecification of hyperparameters. Such model complexity issues only compound \nwhen trying to generalize generative models to incorporate human input. We \nintroduce Correlation Explanation (CorEx), an alternative approach to topic \nmodeling that does not assume an underlying generative model, and instead \nlearns maximally informative topics through an information-theoretic framework. \nThis framework naturally generalizes to hierarchical and semi-supervised \nextensions with no additional modeling assumptions. In particular, word-level \ndomain knowledge can be flexibly incorporated within CorEx through anchor \nwords, allowing topic separability and representation to be promoted with \nminimal human intervention. Across a variety of datasets, metrics, and \nexperiments, we demonstrate that CorEx produces topics that are comparable in \nquality to those produced by unsupervised and semi-supervised variants of LDA. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d46", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1611.10277"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yuheng Bu, Shaofeng Zou, Venugopal V. Veeravalli", "title": "Linear-Complexity Exponentially-Consistent Tests for Universal Outlying Sequence Detection. (arXiv:1701.06084v4 [cs.IT] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1701.06084", "type": "text/html"}], "timestampUsec": "1512450843840940", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45051429af8\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45051429af8&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The problem of universal outlying sequence detection is studied, where the \ngoal is to detect outlying sequences among $M$ sequences of samples. A sequence \nis considered as outlying if the observations therein are generated by a \ndistribution different from those generating the observations in the majority \nof the sequences. In the universal setting, we are interested in identifying \nall the outlying sequences without knowing the underlying generating \ndistributions. In this paper, a class of tests based on distribution clustering \nis proposed. These tests are shown to be exponentially consistent with linear \ntime complexity in $M$. Numerical results demonstrate that our clustering-based \ntests achieve similar performance to existing tests, while being considerably \nmore computationally efficient. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1701.06084"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sitao Xiang, Hao Li", "title": "On the Effects of Batch and Weight Normalization in Generative Adversarial Networks. (arXiv:1704.03971v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.03971", "type": "text/html"}], "timestampUsec": "1512450843840939", "comments": [], "summary": {"content": "<p>Generative adversarial networks (GANs) are highly effective unsupervised \nlearning frameworks that can generate very sharp data, even for data such as \nimages with complex, highly multimodal distributions. However GANs are known to \nbe very hard to train, suffering from problems such as mode collapse and \ndisturbing visual artifacts. Batch normalization (BN) techniques have been \nintroduced to address the training. Though BN accelerates the training in the \nbeginning, our experiments show that the use of BN can be unstable and \nnegatively impact the quality of the trained model. The evaluation of BN and \nnumerous other recent schemes for improving GAN training is hindered by the \nlack of an effective objective quality measure for GAN models. To address these \nissues, we first introduce a weight normalization (WN) approach for GAN \ntraining that significantly improves the stability, efficiency and the quality \nof the generated samples. To allow a methodical evaluation, we introduce \nsquared Euclidean reconstruction error on a test set as a new objective \nmeasure, to assess training performance in terms of speed, stability, and \nquality of generated samples. Our experiments with a standard DCGAN \narchitecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10) \nindicate that training using WN is generally superior to BN for GANs, achieving \n10% lower mean squared loss for reconstruction and significantly better \nqualitative results than BN. We further demonstrate the stability of WN on a \n21-layer ResNet trained with the CelebA data set. The code for this paper is \navailable at https://github.com/stormraiser/gan-weightnorm-resnet \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d51", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.03971"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Cem M. Deniz, Siyuan Xiang, Spencer Hallyburton, Arakua Welbeck, Stephen Honig, Kyunghyun Cho, Gregory Chang", "title": "Segmentation of the Proximal Femur from MR Images using Deep Convolutional Neural Networks. (arXiv:1704.06176v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.06176", "type": "text/html"}], "timestampUsec": "1512450843840938", "comments": [], "summary": {"content": "<p>Magnetic resonance imaging (MRI) has been proposed as a complimentary method \nto measure bone quality and assess fracture risk. However, manual segmentation \nof MR images of bone is time-consuming, limiting the use of MRI measurements in \nthe clinical practice. The purpose of this paper is to present an automatic \nproximal femur segmentation method that is based on deep convolutional neural \nnetworks (CNNs). This study had institutional review board approval and written \ninformed consent was obtained from all subjects. A dataset of volumetric \nstructural MR images of the proximal femur from 86 subject were \nmanually-segmented by an expert. We performed experiments by training two \ndifferent CNN architectures with multiple number of initial feature maps and \nlayers, and tested their segmentation performance against the gold standard of \nmanual segmentations using four-fold cross-validation. Automatic segmentation \nof the proximal femur achieved a high dice similarity score of 0.94$\\pm$0.05 \nwith precision = 0.95$\\pm$0.02, and recall = 0.94$\\pm$0.08 using a CNN \narchitecture based on 3D convolution exceeding the performance of 2D CNNs. The \nhigh segmentation accuracy provided by CNNs has the potential to help bring the \nuse of structural MRI measurements of bone quality into clinical practice for \nmanagement of osteoporosis. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d63", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.06176"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Crist&#xf3;bal Esteban, Stephanie L. Hyland, Gunnar R&#xe4;tsch", "title": "Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs. (arXiv:1706.02633v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.02633", "type": "text/html"}], "timestampUsec": "1512450843840937", "comments": [], "summary": {"content": "<p>Generative Adversarial Networks (GANs) have shown remarkable success as a \nframework for training models to produce realistic-looking data. In this work, \nwe propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to \nproduce realistic real-valued multi-dimensional time series, with an emphasis \non their application to medical data. RGANs make use of recurrent neural \nnetworks in the generator and the discriminator. In the case of RCGANs, both of \nthese RNNs are conditioned on auxiliary information. We demonstrate our models \nin a set of toy datasets, where we show visually and quantitatively (using \nsample likelihood and maximum mean discrepancy) that they can successfully \ngenerate realistic time-series. We also describe novel evaluation methods for \nGANs, where we generate a synthetic labelled training dataset, and evaluate on \na real test set the performance of a model trained on the synthetic data, and \nvice-versa. We illustrate with these metrics that RCGANs can generate \ntime-series data useful for supervised training, with only minor degradation in \nperformance on real test data. This is demonstrated on digit classification \nfrom 'serialised' MNIST and by training an early warning system on a medical \ndataset of 17,000 patients from an intensive care unit. We further discuss and \nanalyse the privacy concerns that may arise when using RCGANs to generate \nrealistic synthetic medical time series data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d70", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.02633"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shiv Shankar, Sunita Sarawagi", "title": "Labeled Memory Networks for Online Model Adaptation. (arXiv:1707.01461v3 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.01461", "type": "text/html"}], "timestampUsec": "1512450843840936", "comments": [], "summary": {"content": "<p>Augmenting a neural network with memory that can grow without growing the \nnumber of trained parameters is a recent powerful concept with many exciting \napplications. We propose a design of memory augmented neural networks (MANNs) \ncalled Labeled Memory Networks (LMNs) suited for tasks requiring online \nadaptation in classification models. LMNs organize the memory with classes as \nthe primary key.The memory acts as a second boosted stage following a regular \nneural network thereby allowing the memory and the primary network to play \ncomplementary roles. Unlike existing MANNs that write to memory for every \ninstance and use LRU based memory replacement, LMNs write only for instances \nwith non-zero loss and use label-based memory replacement. We demonstrate \nsignificant accuracy gains on various tasks including word-modelling and \nfew-shot learning. In this paper, we establish their potential in online \nadapting a batch trained neural network to domain-relevant labeled data at \ndeployment time. We show that LMNs are better than other MANNs designed for \nmeta-learning. We also found them to be more accurate and faster than \nstate-of-the-art methods of retuning model parameters for adapting to \ndomain-specific labeled data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d79", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.01461"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Peter Mills", "title": "Accelerating Kernel Classifiers Through Borders Mapping. (arXiv:1708.05917v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1708.05917", "type": "text/html"}], "timestampUsec": "1512450843840935", "comments": [], "summary": {"content": "<p>Support vector machines (SVM) and other kernel techniques represent a family \nof powerful statistical classification methods with high accuracy and broad \napplicability. Because they use all or a significant portion of the training \ndata, however, they can be slow, especially for large problems. Piecewise \nlinear classifiers are similarly versatile, yet have the additional advantages \nof simplicity, ease of interpretation and, if the number of component linear \nclassifiers is not too large, speed. Here we show how a simple, piecewise \nlinear classifier can be trained from a kernel-based classifier in order to \nimprove the classification speed. The method works by finding the root of the \ndifference in conditional probabilities between pairs of opposite classes to \nbuild up a representation of the decision boundary. When tested on 17 different \ndatasets, it succeeded in improving the classification speed of a SVM for 9 of \nthem by factors as high as 88 times or more. The method is best suited to \nproblems with continuum features data and smooth probability functions. Because \nthe component linear classifiers are built up individually from an existing \nclassifier, rather than through a simultaneous optimization procedure, the \nclassifier is also fast to train. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d81", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1708.05917"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Naoki Hayashi, Sumio Watanabe", "title": "Asymptotic Bayesian Generalization Error in a General Stochastic Matrix Factorization for Markov Chain and Bayesian Network. (arXiv:1709.04212v3 [math.ST] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04212", "type": "text/html"}], "timestampUsec": "1512450843840934", "comments": [], "summary": {"content": "<p>Stochastic matrix factorization (SMF) can be regarded as a restriction of \nnon-negative matrix factorization (NMF). SMF is useful for inference of topic \nmodels, NMF for binary matrices data, Markov chains, and Bayesian networks. \nHowever, SMF needs strong assumptions to reach a unique factorization and its \ntheoretical prediction accuracy has not yet been clarified. In this paper, we \nstudy the maximum the pole of zeta function (real log canonical threshold) of a \ngeneral SMF and derive an upper bound of the generalization error in Bayesian \ninference. The results give a foundation for a widely applicable and rigorous \nfactorization method of SMF and mean that the generalization error in SMF \nbecomes smaller than regular statistical models by Bayesian inference. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d8e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04212"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tsz Kit Lau, Yuan Yao", "title": "Accelerated Block Coordinate Proximal Gradients with Applications in High Dimensional Statistics. (arXiv:1710.05338v7 [math.OC] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.05338", "type": "text/html"}], "timestampUsec": "1512450843840933", "comments": [], "summary": {"content": "<p>Nonconvex optimization problems arise in different research fields and arouse \nlots of attention in signal processing, statistics and machine learning. In \nthis work, we explore the accelerated proximal gradient method and some of its \nvariants which have been shown to converge under nonconvex context recently. We \nshow that a novel variant proposed here, which exploits adaptive momentum and \nblock coordinate update with specific update rules, further improves the \nperformance of a broad class of nonconvex problems. In applications to sparse \nlinear regression with regularizations like Lasso, grouped Lasso, capped \n$\\ell_1$ and SCAP, the proposed scheme enjoys provable local linear \nconvergence, with experimental justification. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763d9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.05338"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Urs K&#xf6;ster, Tristan J. Webb, Xin Wang, Marcel Nassar, Arjun K. Bansal, William H. Constable, O&#x11f;uz H. Elibol, Scott Gray, Stewart Hall, Luke Hornof, Amir Khosrowshahi, Carey Kloss, Ruby J. Pai, Naveen Rao", "title": "Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks. (arXiv:1711.02213v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.02213", "type": "text/html"}], "timestampUsec": "1512450843840932", "comments": [], "summary": {"content": "<p>Deep neural networks are commonly developed and trained in 32-bit floating \npoint format. Significant gains in performance and energy efficiency could be \nrealized by training and inference in numerical formats optimized for deep \nlearning. Despite advances in limited precision inference in recent years, \ntraining of neural networks in low bit-width remains a challenging problem. \nHere we present the Flexpoint data format, aiming at a complete replacement of \n32-bit floating point format training and inference, designed to support modern \ndeep network topologies without modifications. Flexpoint tensors have a shared \nexponent that is dynamically adjusted to minimize overflows and maximize \navailable dynamic range. We validate Flexpoint by training AlexNet, a deep \nresidual network and a generative adversarial network, using a simulator \nimplemented with the neon deep learning framework. We demonstrate that 16-bit \nFlexpoint closely matches 32-bit floating point in training all three models, \nwithout any need for tuning of model hyperparameters. Our results suggest \nFlexpoint as a promising numerical format for future hardware for training and \ninference. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763db0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.02213"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Bogdan Kulynych, Carmela Troncoso", "title": "Feature importance scores and lossless feature pruning using Banzhaf power indices. (arXiv:1711.04992v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.04992", "type": "text/html"}], "timestampUsec": "1512450843840931", "comments": [], "summary": {"content": "<p>Understanding the influence of features in machine learning is crucial to \ninterpreting models and selecting the best features for classification. In this \nwork we propose the use of principles from coalitional game theory to reason \nabout importance of features. In particular, we propose the use of the Banzhaf \npower index as a measure of influence of features on the outcome of a \nclassifier. We show that features having Banzhaf power index of zero can be \nlosslessly pruned without damage to classifier accuracy. Computing the power \nindices does not require having access to data samples. However, if samples are \navailable, the indices can be empirically estimated. We compute Banzhaf power \nindices for a neural network classifier on real-life data, and compare the \nresults with gradient-based feature saliency, and coefficients of a logistic \nregression model with $L_1$ regularization. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763dc5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.04992"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Seong Jin Cho, Sunghun Kang, Chang D. Yoo", "title": "A Resizable Mini-batch Gradient Descent based on a Randomized Weighted Majority. (arXiv:1711.06424v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.06424", "type": "text/html"}], "timestampUsec": "1512450843840930", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45051429d1b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45051429d1b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Determining the appropriate batch size for mini-batch gradient descent is \nalways time consuming as it often relies on grid search. This paper considers a \nresizable mini-batch gradient descent (RMGD) algorithm-inspired by the \nrandomized weighted majority algorithm-for achieving best performance in grid \nsearch by selecting an appropriate batch size at each epoch with a probability \ndefined as a function of its previous success/failure and the validation error. \nThis probability encourages exploration of different batch size and then later \nexploitation of batch size with history of success. At each epoch, the RMGD \nsamples a batch size from its probability distribution, then uses the selected \nbatch size for mini-batch gradient descent. After obtaining the validation \nerror at each epoch, the probability distribution is updated to incorporate the \neffectiveness of the sampled batch size. The RMGD essentially assists the \nlearning process to explore the possible domain of the batch size and exploit \nsuccessful batch size. Experimental results show that the RMGD achieves \nperformance better than the best performing single batch size. Furthermore, it \nattains this performance in a shorter amount of time than that of the best \nperforming. It is surprising that the RMGD achieves better performance than \ngrid search. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763dd0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.06424"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Carlos X. Hern&#xe1;ndez, Hannah K. Wayment-Steele, Mohammad M. Sultan, Brooke E. Husic, Vijay S. Pande", "title": "Variational Encoding of Complex Dynamics. (arXiv:1711.08576v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.08576", "type": "text/html"}], "timestampUsec": "1512450843840929", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45051483fb5\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45051483fb5&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Often the analysis of time-dependent chemical and biophysical systems \nproduces high-dimensional time-series data for which it can be difficult to \ninterpret which individual features are most salient. While recent work from \nour group and others has demonstrated the utility of time-lagged co-variate \nmodels to study such systems, linearity assumptions can limit the compression \nof inherently nonlinear dynamics into just a few characteristic components. \nRecent work in the field of deep learning has led to the development of \nvariational autoencoders (VAE), which are able to compress complex datasets \ninto simpler manifolds. We present the use of a time-lagged VAE, or variational \ndynamics encoder (VDE), to reduce complex, nonlinear processes to a single \nembedding with high fidelity to the underlying dynamics. We demonstrate how the \nVDE is able to capture nontrivial dynamics in a variety of examples, including \nBrownian dynamics and atomistic protein folding. Additionally, we demonstrate a \nmethod for analyzing the VDE model, inspired by saliency mapping, to determine \nwhat features are selected by the VDE model to describe dynamics. The VDE \npresents an important step in applying techniques from deep learning to more \naccurately model and interpret complex biophysics. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763ddf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.08576"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiyu Yu, Tongliang Liu, Mingming Gong, Dacheng Tao", "title": "Learning with Biased Complementary Labels. (arXiv:1711.09535v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09535", "type": "text/html"}], "timestampUsec": "1512450843840928", "comments": [], "summary": {"content": "<p>In this paper we study the classification problem in which we have access to \neasily obtainable surrogate for the true labels, namely complementary labels, \nwhich specify classes that observations do \\textbf{not} belong to. For example, \nif one is familiar with monkeys but not meerkats, a meerkat is easily \nidentified as not a monkey, so \"monkey\" is annotated to the meerkat as a \ncomplementary label. Specifically, let $Y$ and $\\bar{Y}$ be the true and \ncomplementary labels, respectively. We first model the annotation of \ncomplementary labels via the transition probabilities $P(\\bar{Y}=i|Y=j), i\\neq \nj\\in\\{1,\\cdots,c\\}$, where $c$ is the number of classes. All the previous \nmethods implicitly assume that the transition probabilities $P(\\bar{Y}=i|Y=j)$ \nare identical, which is far from true in practice because humans are biased \ntoward their own experience. For example, if a person is more familiar with \nmonkey than prairie dog when providing complementary labels for meerkats, \nhe/she is more likely to employ \"monkey\" as a complementary label. We therefore \nreason that the transition probabilities will be different. In this paper, we \naddress three fundamental problems raised by learning with biased complementary \nlabels. (1) How to estimate the transition probabilities? (2) How to modify the \ntraditional loss functions and extend standard deep neural network classifiers \nto learn with biased complementary labels? (3) Does the classifier learned from \nexamples with complementary labels by our proposed method converge to the \noptimal one learned from examples with true labels? Comprehensive experiments \non MNIST, CIFAR10, CIFAR100, and Tiny ImageNet empirically validate the \nsuperiority of the proposed method to the current state-of-the-art methods with \naccuracy gains of over 10\\%. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512450843841", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/0000000340763df3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09535"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Santiago Hern&#xe1;ndez-Orozco, Narsis A. Kiani, Hector Zenil", "title": "Algorithmically probable mutations reproduce aspects of evolution such as convergence rate, genetic memory, modularity, diversity explosions, and mass extinction. (arXiv:1709.00268v5 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.00268", "type": "text/html"}], "timestampUsec": "1512364308369392", "comments": [], "summary": {"content": "<p>Natural selection explains how life has evolved over millions of years from \nmore primitive forms. The speed at which this happens, however, has sometimes \ndefied explanations based on random (uniformly distributed) mutations. Here we \ninvestigate the application of algorithmic mutations (no recombination) to \nbinary matrices drawn from numerical approximations to algorithmic probability \nin order to compare evolutionary convergence rates against the null hypothesis \n(uniformly distributed mutations). Results both on synthetic and a small \nbiological examples lead to an accelerated rate of convergence when using the \nalgorithmic probability. We also show that algorithmically evolved modularity \nprovides an advantage that produces a genetic memory. We demonstrate that \nregular structures are preserved and carried on when they first occur and can \nlead to an accelerated production of diversity and extinction, possibly \nexplaining naturally occurring phenomena such as diversity explosions (e.g. the \nCambrian) and massive extinctions (e.g. the End Triassic) whose causes have \neluded researchers and are a cause for debate. The approach introduced here \nappears to be a better approximation to biological evolution than models based \nexclusively upon random uniform mutations, and it also approaches better a \nformal version of open-ended evolution based on previous results. The results \nvalidate the motivations and results of Chaitin's Metabiology programme and \nprevious suggestions that computation may be an equally important driver of \nevolution together, and even before, the action and result of natural \nselection. We also show that inducing the method on problems of optimization, \nsuch as genetic algorithms, has the potential to significantly accelerate \nconvergence of artificial evolutionary algorithms. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb640d9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.00268"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiaojie Jin, Yingzhen Yang, Ning Xu, Jianchao Yang, Jiashi Feng, Shuicheng Yan", "title": "WSNet: Compact and Efficient Networks with Weight Sampling. (arXiv:1711.10067v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10067", "type": "text/html"}], "timestampUsec": "1512364308369391", "comments": [], "summary": {"content": "<p>We present a new approach and a novel architecture, termed WSNet, for \nlearning compact and efficient deep neural networks. Existing approaches \nconventionally learn full model parameters independently at first and then \ncompress them via ad hoc processing like model pruning or filter factorization. \nDifferent from them, WSNet proposes learning model parameters by sampling from \na compact set of learnable parameters, which naturally enforces parameter \nsharing throughout the learning process. We show that such novel weight \nsampling approach (and induced WSNet) promotes both weights and computation \nsharing favorably. It can more efficiently learn much smaller networks with \ncompetitive performance, compared to baseline networks with equal number of \nconvolution filters. Specifically, we consider learning compact and efficient \n1D convolutional neural networks for audio classification. Extensive \nexperiments on multiple audio classification datasets verify the effectiveness \nof WSNet. Combined with weight quantization, the resulted models are up to 180x \nsmaller and theoretically up to 16x faster than the well-established baselines, \nwithout noticeable performance drop. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64101", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10067"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Seungkyun Hong, Seongchan Kim, Minsu Joh, Sa-kwang Song", "title": "PSIque: Next Sequence Prediction of Satellite Images using a Convolutional Sequence-to-Sequence Network. (arXiv:1711.10644v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10644", "type": "text/html"}], "timestampUsec": "1512364308369390", "comments": [], "summary": {"content": "<p>Predicting unseen weather phenomena is an important issue for disaster \nmanagement. In this paper, we suggest a model for a convolutional \nsequence-to-sequence autoencoder for predicting undiscovered weather situations \nfrom previous satellite images. We also propose a symmetric skip connection \nbetween encoder and decoder modules to produce more comprehensive image \npredictions. To examine our model performance, we conducted experiments for \neach suggested model to predict future satellite images from historical \nsatellite images. A specific combination of skip connection and \nsequence-to-sequence autoencoder was able to generate closest prediction from \nthe ground truth image. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64116", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10644"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Martin Klissarov, Pierre-Luc Bacon, Jean Harb, Doina Precup", "title": "Learnings Options End-to-End for Continuous Action Tasks. (arXiv:1712.00004v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00004", "type": "text/html"}], "timestampUsec": "1512364308369389", "comments": [], "summary": {"content": "<p>We present new results on learning temporally extended actions for \ncontinuoustasks, using the options framework (Suttonet al.[1999b], Precup \n[2000]). In orderto achieve this goal we work with the option-critic \narchitecture (Baconet al.[2017])using a deliberation cost and train it with \nproximal policy optimization (Schulmanet al.[2017]) instead of vanilla policy \ngradient. Results on Mujoco domains arepromising, but lead to interesting \nquestions aboutwhena given option should beused, an issue directly connected to \nthe use of initiation sets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64134", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00004"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shangtong Zhang, Osmar R. Zaiane", "title": "Comparing Deep Reinforcement Learning and Evolutionary Methods in Continuous Control. (arXiv:1712.00006v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00006", "type": "text/html"}], "timestampUsec": "1512364308369388", "comments": [], "summary": {"content": "<p>Reinforcement learning and evolutionary strategy are two major approaches in \naddressing complicated control problems. Both have strong biological basis and \nthere have been recently many advanced techniques in both domains. In this \npaper, we present a thorough comparison between the state of the art techniques \nin both domains in complex continuous control tasks. We also formulate the \nparallelized version of the Proximal Policy Optimization method and the Deep \nDeterministic Policy Gradient method. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64164", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00006"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jason Bernard, Ian McQuillan", "title": "New Techniques for Inferring L-Systems Using Genetic Algorithm. (arXiv:1712.00180v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00180", "type": "text/html"}], "timestampUsec": "1512364308369387", "comments": [], "summary": {"content": "<p>Lindenmayer systems (L-systems) are a formal grammar system that iteratively \nrewrites all symbols of a string, in parallel. When visualized with a graphical \ninterpretation, the images have self-similar shapes that appear frequently in \nnature, and they have been particularly successful as a concise, reusable \ntechnique for simulating plants. The L-system inference problem is to find an \nL-system to simulate a given plant. This is currently done mainly by experts, \nbut this process is limited by the availability of experts, the complexity that \nmay be solved by humans, and time. This paper introduces the Plant Model \nInference Tool (PMIT) that infers deterministic context-free L-systems from an \ninitial sequence of strings generated by the system using a genetic algorithm. \nPMIT is able to infer more complex systems than existing approaches. Indeed, \nwhile existing approaches are limited to L-systems with a total sum of 20 \ncombined symbols in the productions, PMIT can infer almost all L-systems tested \nwhere the total sum is 140 symbols. This was validated using a test bed of 28 \npreviously developed L-system models, in addition to models created \nartificially by bootstrapping larger models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6418d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00180"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hee Jung Ryu, Margaret Mitchell, Hartwig Adam", "title": "Improving Smiling Detection with Race and Gender Diversity. (arXiv:1712.00193v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.00193", "type": "text/html"}], "timestampUsec": "1512364308369386", "comments": [], "summary": {"content": "<p>Recent progress in deep learning has been accompanied by a growing concern \nfor whether models are fair for users, with equally good performance across \ndifferent demographics. In computer vision research, such questions are \nrelevant to face detection and the related task of face attribute detection, \namong others. We measure race and gender inclusion in the context of smiling \ndetection, and introduce a method for improving smiling detection across \ndemographic groups. Our method introduces several modifications over existing \ndetection methods, leveraging twofold transfer learning to better model facial \ndiversity. Results show that this technique improves accuracy against strong \nbaselines for most demographic groups as well as overall. Our best-performing \nmodel defines a new state-of-the-art for smiling detection, reaching 91% on the \nFaces of the World dataset. The accompanying multi-head diversity classifier \nalso defines a new state-of-the-art for gender classification, reaching 93.87% \non the Faces of the World dataset. This research demonstrates the utility of \nmodeling race and gender to improve a face attribute detection task, using a \ntwofold transfer learning framework that allows for privacy towards individuals \nin a target dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb641ac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00193"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chong Di", "title": "A double competitive strategy based learning automata algorithm. (arXiv:1712.00222v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00222", "type": "text/html"}], "timestampUsec": "1512364308369385", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45051484359\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45051484359&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Learning Automata (LA) are considered as one of the most powerful tools in \nthe field of reinforcement learning. The family of estimator algorithms is \nproposed to improve the convergence rate of LA and has made great achievements. \nHowever, the estimators perform poorly on estimating the reward probabilities \nof actions in the initial stage of the learning process of LA. In this \nsituation, a lot of rewards would be added to the probabilities of non-optimal \nactions. Thus, a large number of extra iterations are needed to compensate for \nthese wrong rewards. In order to improve the speed of convergence, we propose a \nnew P-model absorbing learning automaton by utilizing a double competitive \nstrategy which is designed for updating the action probability vector. In this \nway, the wrong rewards can be corrected instantly. Hence, the proposed Double \nCompetitive Algorithm overcomes the drawbacks of existing estimator algorithms. \nA refined analysis is presented to show the $\\epsilon-optimality$ of the \nproposed scheme. The extensive experimental results in benchmark environments \ndemonstrate that our proposed learning automata perform more efficiently than \nthe most classic LA $SE_{RI}$ and the current fastest LA $DGCPA^{*}$. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb641ba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00222"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Aishwarya Agrawal, Dhruv Batra, Devi Parikh, Aniruddha Kembhavi", "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering. (arXiv:1712.00377v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.00377", "type": "text/html"}], "timestampUsec": "1512364308369384", "comments": [], "summary": {"content": "<p>A number of studies have found that today's Visual Question Answering (VQA) \nmodels are heavily driven by superficial correlations in the training data and \nlack sufficient image grounding. To encourage development of models geared \ntowards the latter, we propose a new setting for VQA where for every question \ntype, train and test sets have different prior distributions of answers. \nSpecifically, we present new splits of the VQA v1 and VQA v2 datasets, which we \ncall Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 \nrespectively). First, we evaluate several existing VQA models under this new \nsetting and show that their performance degrades significantly compared to the \noriginal VQA setting. Second, we propose a novel Grounded Visual Question \nAnswering model (GVQA) that contains inductive biases and restrictions in the \narchitecture specifically designed to prevent the model from 'cheating' by \nprimarily relying on priors in the training data. Specifically, GVQA explicitly \ndisentangles the recognition of visual concepts present in the image from the \nidentification of plausible answer space for a given question, enabling the \nmodel to more robustly generalize across different distributions of answers. \nGVQA is built off an existing VQA model -- Stacked Attention Networks (SAN). \nOur experiments demonstrate that GVQA significantly outperforms SAN on both \nVQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more \npowerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in \nseveral cases. GVQA offers strengths complementary to SAN when trained and \nevaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more \ntransparent and interpretable than existing VQA models. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb641d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00377"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Oliver Bent, Sekou L. Remy, Stephen Roberts, Aisha Walcott-Bryant", "title": "Novel Exploration Techniques (NETs) for Malaria Policy Interventions. (arXiv:1712.00428v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1712.00428", "type": "text/html"}], "timestampUsec": "1512364308369383", "comments": [], "summary": {"content": "<p>The task of decision-making under uncertainty is daunting, especially for \nproblems which have significant complexity. Healthcare policy makers across the \nglobe are facing problems under challenging constraints, with limited tools to \nhelp them make data driven decisions. In this work we frame the process of \nfinding an optimal malaria policy as a stochastic multi-armed bandit problem, \nand implement three agent based strategies to explore the policy space. We \napply a Gaussian Process regression to the findings of each agent, both for \ncomparison and to account for stochastic results from simulating the spread of \nmalaria in a fixed population. The generated policy spaces are compared with \npublished results to give a direct reference with human expert decisions for \nthe same simulated population. Our novel approach provides a powerful resource \nfor policy makers, and a platform which can be readily extended to capture \nfuture more nuanced policy spaces. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb641e7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00428"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Marcello Balduccini, Yuliya Lierler", "title": "Constraint Answer Set Solver EZCSP and Why Integration Schemas Matter. (arXiv:1702.04047v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1702.04047", "type": "text/html"}], "timestampUsec": "1512364308369381", "comments": [], "summary": {"content": "<p>Researchers in answer set programming and constraint programming have spent \nsignificant efforts in the development of hybrid languages and solving \nalgorithms combining the strengths of these traditionally separate fields. \nThese efforts resulted in a new research area: constraint answer set \nprogramming. Constraint answer set programming languages and systems proved to \nbe successful at providing declarative, yet efficient solutions to problems \ninvolving hybrid reasoning tasks. One of the main contributions of this paper \nis the first comprehensive account of the constraint answer set language and \nsolver EZCSP, a mainstream representative of this research area that has been \nused in various successful applications. We also develop an extension of the \ntransition systems proposed by Nieuwenhuis et al. in 2006 to capture Boolean \nsatisfiability solvers. We use this extension to describe the EZCSP algorithm \nand prove formal claims about it. The design and algorithmic details behind \nEZCSP clearly demonstrate that the development of the hybrid systems of this \nkind is challenging. Many questions arise when one faces various design choices \nin an attempt to maximize system's benefits. One of the key decisions that a \ndeveloper of a hybrid solver makes is settling on a particular integration \nschema within its implementation. Thus, another important contribution of this \npaper is a thorough case study based on EZCSP, focused on the various \nintegration schemas that it provides. \n</p> \n<p>Under consideration in Theory and Practice of Logic Programming (TPLP). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64205", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1702.04047"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sirui Yao, Bert Huang", "title": "Beyond Parity: Fairness Objectives for Collaborative Filtering. (arXiv:1705.08804v2 [cs.IR] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1705.08804", "type": "text/html"}], "timestampUsec": "1512364308369380", "comments": [], "summary": {"content": "<p>We study fairness in collaborative-filtering recommender systems, which are \nsensitive to discrimination that exists in historical data. Biased data can \nlead collaborative-filtering methods to make unfair predictions for users from \nminority groups. We identify the insufficiency of existing fairness metrics and \npropose four new metrics that address different forms of unfairness. These \nfairness metrics can be optimized by adding fairness terms to the learning \nobjective. Experiments on synthetic and real data show that our new metrics can \nbetter measure fairness than the baseline, and that the fairness objectives \neffectively help reduce unfairness. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64241", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1705.08804"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Joohyung Lee, Samidh Talsania, Yi Wang", "title": "Computing LPMLN Using ASP and MLN Solvers. (arXiv:1707.06325v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.06325", "type": "text/html"}], "timestampUsec": "1512364308369379", "comments": [], "summary": {"content": "<p>LPMLN is a recent addition to probabilistic logic programming languages. Its \nmain idea is to overcome the rigid nature of the stable model semantics by \nassigning a weight to each rule in a way similar to Markov Logic is defined. We \npresent two implementations of LPMLN, $\\text{LPMLN2ASP}$ and \n$\\text{LPMLN2MLN}$. System $\\text{LPMLN2ASP}$ translates LPMLN programs into \nthe input language of answer set solver $\\text{CLINGO}$, and using weak \nconstraints and stable model enumeration, it can compute most probable stable \nmodels as well as exact conditional and marginal probabilities. System \n$\\text{LPMLN2MLN}$ translates LPMLN programs into the input language of Markov \nLogic solvers, such as $\\text{ALCHEMY}$, $\\text{TUFFY}$, and $\\text{ROCKIT}$, \nand allows for performing approximate probabilistic inference on LPMLN \nprograms. We also demonstrate the usefulness of the LPMLN systems for computing \nother languages, such as ProbLog and Pearl's Causal Models, that are shown to \nbe translatable into LPMLN. (Under consideration for acceptance in TPLP) \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6425b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.06325"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Gregory Kahn, Adam Villaflor, Bosen Ding, Pieter Abbeel, Sergey Levine", "title": "Self-supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation. (arXiv:1709.10489v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.10489", "type": "text/html"}], "timestampUsec": "1512364308369378", "comments": [], "summary": {"content": "<p>Enabling robots to autonomously navigate complex environments is essential \nfor real-world deployment. Prior methods approach this problem by having the \nrobot maintain an internal map of the world, and then use a localization and \nplanning method to navigate through the internal map. However, these approaches \noften include a variety of assumptions, are computationally intensive, and do \nnot learn from failures. In contrast, learning-based methods improve as the \nrobot acts in the environment, but are difficult to deploy in the real-world \ndue to their high sample complexity. To address the need to learn complex \npolicies with few samples, we propose a generalized computation graph that \nsubsumes value-based model-free methods and model-based methods, with specific \ninstantiations interpolating between model-free and model-based. We then \ninstantiate this graph to form a navigation model that learns from raw images \nand is sample efficient. Our simulated car experiments explore the design \ndecisions of our navigation model, and show our approach outperforms \nsingle-step and $N$-step double Q-learning. We also evaluate our approach on a \nreal-world RC car and show it can learn to navigate through a complex indoor \nenvironment with a few hours of fully autonomous, self-supervised training. \nVideos of the experiments and code can be found at github.com/gkahn13/gcg \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64271", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.10489"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mikael Henaff, Junbo Zhao, Yann LeCun", "title": "Prediction Under Uncertainty with Error-Encoding Networks. (arXiv:1711.04994v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.04994", "type": "text/html"}], "timestampUsec": "1512364308369377", "comments": [], "summary": {"content": "<p>In this work we introduce a new framework for performing temporal predictions \nin the presence of uncertainty. It is based on a simple idea of disentangling \ncomponents of the future state which are predictable from those which are \ninherently unpredictable, and encoding the unpredictable components into a \nlow-dimensional latent variable which is fed into a forward model. Our method \nuses a supervised training objective which is fast and easy to train. We \nevaluate it in the context of video prediction on multiple datasets and show \nthat it is able to consistently generate diverse predictions without the need \nfor alternating minimization over a latent space or adversarial training. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64282", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.04994"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Long Ouyang, Michael C. Frank", "title": "Pedagogical learning. (arXiv:1711.09401v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09401", "type": "text/html"}], "timestampUsec": "1512364308369376", "comments": [], "summary": {"content": "<p>A common assumption in machine learning is that training data are i.i.d. \nsamples from some distribution. Processes that generate i.i.d. samples are, in \na sense, uninformative---they produce data without regard to how good this data \nis for learning. By contrast, cognitive science research has shown that when \npeople generate training data for others (i.e., teaching), they deliberately \nselect examples that are helpful for learning. Because the data is more \ninformative, learning can require less data. Interestingly, such examples are \nmost effective when learners know that the data were pedagogically generated \n(as opposed to randomly generated). We call this pedagogical learning---when a \nlearner assumes that evidence comes from a helpful teacher. In this work, we \nask how pedagogical learning might work for machine learning algorithms. \nStudying this question requires understanding how people actually teach complex \nconcepts with examples, so we conducted a behavioral study examining how people \nteach regular expressions using example strings. We found that teachers' \nexamples contain powerful clustering structure that can greatly facilitate \nlearning. We then develop a model of teaching and show a proof of concept that \nusing this model inside of a learner can improve performance. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64291", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09401"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ahmad Chaddad, Behnaz Naisiri, Marco Pedersoli, Eric Granger, Christian Desrosiers, Matthew Toews", "title": "Modeling Information Flow Through Deep Neural Networks. (arXiv:1712.00003v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00003", "type": "text/html"}], "timestampUsec": "1512364308369374", "comments": [], "summary": {"content": "<p>This paper proposes a principled information theoretic analysis of \nclassification for deep neural network structures, e.g. convolutional neural \nnetworks (CNN). The output of convolutional filters is modeled as a random \nvariable Y conditioned on the object class C and network filter bank F. The \nconditional entropy (CENT) H(Y |C,F) is shown in theory and experiments to be a \nhighly compact and class-informative code, that can be computed from the filter \noutputs throughout an existing CNN and used to obtain higher classification \nresults than the original CNN itself. Experiments demonstrate the effectiveness \nof CENT feature analysis in two separate CNN classification contexts. 1) In the \nclassification of neurodegeneration due to Alzheimer's disease (AD) and natural \naging from 3D magnetic resonance image (MRI) volumes, 3 CENT features result in \nan AUC=94.6% for whole-brain AD classification, the highest reported accuracy \non the public OASIS dataset used and 12% higher than the softmax output of the \noriginal CNN trained for the task. 2) In the context of visual object \nclassification from 2D photographs, transfer learning based on a small set of \nCENT features identified throughout an existing CNN leads to AUC values \ncomparable to the 1000-feature softmax output of the original network when \nclassifying previously unseen object categories. The general information \ntheoretical analysis explains various recent CNN design successes, e.g. densely \nconnected CNN architectures, and provides insights for future research \ndirections in deep learning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb642b4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00003"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "You Jin Kim (1), Yun-Geun Lee (1), Jeong Whun Kim (2), Jin Joo Park (2), Borim Ryu (2), Jung-Woo Ha (1) ((1) Clova AI Research, NAVER Corp., (2) Seoul National University Bundang Hospital)", "title": "Highrisk Prediction from Electronic Medical Records via Deep Attention Networks. (arXiv:1712.00010v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00010", "type": "text/html"}], "timestampUsec": "1512364308369373", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450514846e2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450514846e2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Predicting highrisk vascular diseases is a significant issue in the medical \ndomain. Most predicting methods predict the prognosis of patients from \npathological and radiological measurements, which are expensive and require \nmuch time to be analyzed. Here we propose deep attention models that predict \nthe onset of the high risky vascular disease from symbolic medical histories \nsequence of hypertension patients such as ICD-10 and pharmacy codes only, \nMedical History-based Prediction using Attention Network (MeHPAN). We \ndemonstrate two types of attention models based on 1) bidirectional gated \nrecurrent unit (R-MeHPAN) and 2) 1D convolutional multilayer model (C-MeHPAN). \nTwo MeHPAN models are evaluated on approximately 50,000 hypertension patients \nwith respect to precision, recall, f1-measure and area under the curve (AUC). \nExperimental results show that our MeHPAN methods outperform standard \nclassification models. Comparing two MeHPANs, R-MeHPAN provides more better \ndiscriminative capability with respect to all metrics while C-MeHPAN presents \nmuch shorter training time with competitive accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb642c0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00010"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Genevieve Flaspohler, Nicholas Roy, Yogesh Girdhar", "title": "Feature discovery and visualization of robot mission data using convolutional autoencoders and Bayesian nonparametric topic models. (arXiv:1712.00028v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00028", "type": "text/html"}], "timestampUsec": "1512364308369372", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450514d99a6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450514d99a6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The gap between our ability to collect interesting data and our ability to \nanalyze these data is growing at an unprecedented rate. Recent algorithmic \nattempts to fill this gap have employed unsupervised tools to discover \nstructure in data. Some of the most successful approaches have used \nprobabilistic models to uncover latent thematic structure in discrete data. \nDespite the success of these models on textual data, they have not generalized \nas well to image data, in part because of the spatial and temporal structure \nthat may exist in an image stream. \n</p> \n<p>We introduce a novel unsupervised machine learning framework that \nincorporates the ability of convolutional autoencoders to discover features \nfrom images that directly encode spatial information, within a Bayesian \nnonparametric topic model that discovers meaningful latent patterns within \ndiscrete data. By using this hybrid framework, we overcome the fundamental \ndependency of traditional topic models on rigidly hand-coded data \nrepresentations, while simultaneously encoding spatial dependency in our topics \nwithout adding model complexity. We apply this model to the motivating \napplication of high-level scene understanding and mission summarization for \nexploratory marine robots. Our experiments on a seafloor dataset collected by a \nmarine robot show that the proposed hybrid framework outperforms current \nstate-of-the-art approaches on the task of unsupervised seafloor terrain \ncharacterization. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb642cf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00028"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xavier Roynard, Jean-Emmanuel Deschaud, Fran&#xe7;ois Goulette", "title": "Paris-Lille-3D: a large and high-quality ground truth urban point cloud dataset for automatic segmentation and classification. (arXiv:1712.00032v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00032", "type": "text/html"}], "timestampUsec": "1512364308369371", "comments": [], "summary": {"content": "<p>This paper introduces a new Urban Point Cloud Dataset for Automatic \nSegmentation and Classification acquired by Mobile Laser Scanning (MLS). We \ndescribe how the dataset is obtained from acquisition to post-processing and \nlabeling. This dataset can be used to learn classification algorithm, however, \ngiven that a great attention has been paid to the split between the different \nobjects, this dataset can also be used to learn the segmentation. The dataset \nconsists of around 2km of MLS point cloud acquired in two cities. The number of \npoints and range of classes make us consider that it can be used to train \nDeep-Learning methods. Besides we show some results of automatic segmentation \nand classification. The dataset is available at: \n<a href=\"http://caor-mines-paristech.fr/fr/paris-lille-3d-dataset/\">this http URL</a> \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb642fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00032"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "I&#xf1;igo Urteaga, David J. Albers, Marija Vlajic Wheeler, Anna Druet, Hans Raffauf, No&#xe9;mie Elhadad", "title": "Towards Personalized Modeling of the Female Hormonal Cycle: Experiments with Mechanistic Models and Gaussian Processes. (arXiv:1712.00117v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00117", "type": "text/html"}], "timestampUsec": "1512364308369370", "comments": [], "summary": {"content": "<p>In this paper, we introduce a novel task for machine learning in healthcare, \nnamely personalized modeling of the female hormonal cycle. The motivation for \nthis work is to model the hormonal cycle and predict its phases in time, both \nfor healthy individuals and for those with disorders of the reproductive \nsystem. Because there are individual differences in the menstrual cycle, we are \nparticularly interested in personalized models that can account for individual \nidiosyncracies, towards identifying phenotypes of menstrual cycles. As a first \nstep, we consider the hormonal cycle as a set of observations through time. We \nuse a previously validated mechanistic model to generate realistic hormonal \npatterns, and experiment with Gaussian process regression to estimate their \nvalues over time. Specifically, we are interested in the feasibility of \npredicting menstrual cycle phases under varying learning conditions: number of \ncycles used for training, hormonal measurement noise and sampling rates, and \ninformed vs. agnostic sampling of hormonal measurements. Our results indicate \nthat Gaussian processes can help model the female menstrual cycle. We discuss \nthe implications of our experiments in the context of modeling the female \nmenstrual cycle. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6430e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00117"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Zelun Luo, Yuliang Zou, Judy Hoffman, Li Fei-Fei", "title": "Label Efficient Learning of Transferable Representations across Domains and Tasks. (arXiv:1712.00123v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00123", "type": "text/html"}], "timestampUsec": "1512364308369369", "comments": [], "summary": {"content": "<p>We propose a framework that learns a representation transferable across \ndifferent domains and tasks in a label efficient manner. Our approach battles \ndomain shift with a domain adversarial loss, and generalizes the embedding to \nnovel task using a metric learning-based approach. Our model is simultaneously \noptimized on labeled source data and unlabeled or sparsely labeled data in the \ntarget domain. Our method shows compelling results on novel classes within a \nnew domain even when only a few labeled examples per class are available, \noutperforming the prevalent fine-tuning approach. In addition, we demonstrate \nthe effectiveness of our framework on the transfer learning task from image \nobject recognition to video action recognition. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6432a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00123"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512450854, "author": "Tammo Rukat, Dustin Lange, C&#xe9;dric Archambeau", "title": "An interpretable latent variable model for attribute applicability in the Amazon catalogue. (arXiv:1712.00126v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00126", "type": "text/html"}], "timestampUsec": "1512364308369368", "comments": [], "summary": {"content": "<p>Learning attribute applicability of products in the Amazon catalog (e.g., \npredicting that a shoe should have a value for size, but not for battery-type \nat scale is a challenge. The need for an interpretable model is contingent on \n(1) the lack of ground truth training data, (2) the need to utilise prior \ninformation about the underlying latent space and (3) the ability to understand \nthe quality of predictions on new, unseen data. To this end, we develop the \nMaxMachine, a probabilistic latent variable model that learns distributed \nbinary representations, associated to sets of features that are likely to \nco-occur in the data. Layers of MaxMachines can be stacked such that higher \nlayers encode more abstract information. Any set of variables can be clamped to \nencode prior information. We develop fast sampling based posterior inference. \nPreliminary results show that the model improves over the baseline in 17 out of \n19 product groups and provides qualitatively reasonable predictions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512450844, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6434b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00126"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Muneki Yasuda, Kazuyuki Tanaka", "title": "Susceptibility Propagation by Using Diagonal Consistency. (arXiv:1712.00155v1 [cond-mat.stat-mech])", "alternate": [{"href": "http://arxiv.org/abs/1712.00155", "type": "text/html"}], "timestampUsec": "1512364308369367", "comments": [], "summary": {"content": "<p>A susceptibility propagation that is constructed by combining a belief \npropagation and a linear response method is used for approximate computation \nfor Markov random fields. Herein, we formulate a new, improved susceptibility \npropagation by using the concept of a diagonal matching method that is based on \nmean-field approaches to inverse Ising problems. The proposed susceptibility \npropagation is robust for various network structures, and it is reduced to the \nordinary susceptibility propagation and to the adaptive \nThouless-Anderson-Palmer equation in special cases. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64371", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00155"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexandre Yahi, Rami Vanguri, No&#xe9;mie Elhadad, Nicholas P. Tatonetti", "title": "Generative Adversarial Networks for Electronic Health Records: A Framework for Exploring and Evaluating Methods for Predicting Drug-Induced Laboratory Test Trajectories. (arXiv:1712.00164v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00164", "type": "text/html"}], "timestampUsec": "1512364308369366", "comments": [], "summary": {"content": "<p>Generative Adversarial Networks (GANs) represent a promising class of \ngenerative networks that combine neural networks with game theory. From \ngenerating realistic images and videos to assisting musical creation, GANs are \ntransforming many fields of arts and sciences. However, their application to \nhealthcare has not been fully realized, more specifically in generating \nelectronic health records (EHR) data. In this paper, we propose a framework for \nexploring the value of GANs in the context of continuous laboratory time series \ndata. We devise an unsupervised evaluation method that measures the predictive \npower of synthetic laboratory test time series. Further, we show that when it \ncomes to predicting the impact of drug exposure on laboratory test data, \nincorporating representation learning of the training cohorts prior to training \nGAN models is beneficial. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb643cd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00164"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wenbo Zhao, Yang Gao, Rita Singh, Ming Li", "title": "Speaker identification from the sound of the human breath. (arXiv:1712.00171v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1712.00171", "type": "text/html"}], "timestampUsec": "1512364308369365", "comments": [], "summary": {"content": "<p>This paper examines the speaker identification potential of breath sounds in \ncontinuous speech. Speech is largely produced during exhalation. In order to \nreplenish air in the lungs, speakers must periodically inhale. When inhalation \noccurs in the midst of continuous speech, it is generally through the mouth. \nIntra-speech breathing behavior has been the subject of much study, including \nthe patterns, cadence, and variations in energy levels. However, an often \nignored characteristic is the {\\em sound} produced during the inhalation phase \nof this cycle. Intra-speech inhalation is rapid and energetic, performed with \nopen mouth and glottis, effectively exposing the entire vocal tract to enable \nmaximum intake of air. This results in vocal tract resonances evoked by \nturbulence that are characteristic of the speaker's speech-producing apparatus. \nConsequently, the sounds of inhalation are expected to carry information about \nthe speaker's identity. Moreover, unlike other spoken sounds which are subject \nto active control, inhalation sounds are generally more natural and less \naffected by voluntary influences. The goal of this paper is to demonstrate that \nbreath sounds are indeed bio-signatures that can be used to identify speakers. \nWe show that these sounds by themselves can yield remarkably accurate speaker \nrecognition with appropriate feature representations and classification \nframeworks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb643f1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00171"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chris Wu, Tanay Tandon", "title": "Rapid point-of-care Hemoglobin measurement through low-cost optics and Convolutional Neural Network based validation. (arXiv:1712.00174v1 [physics.med-ph])", "alternate": [{"href": "http://arxiv.org/abs/1712.00174", "type": "text/html"}], "timestampUsec": "1512364308369364", "comments": [], "summary": {"content": "<p>A low-cost, robust, and simple mechanism to measure hemoglobin would play a \ncritical role in the modern health infrastructure. Consistent sample \nacquisition has been a long-standing technical hurdle for photometer-based \nportable hemoglobin detectors which rely on micro cuvettes and dry chemistry. \nAny particulates (e.g. intact red blood cells (RBCs), microbubbles, etc.) in a \ncuvette's sensing area drastically impact optical absorption profile, and \ncommercial hemoglobinometers lack the ability to automatically detect faulty \nsamples. We present the ground-up development of a portable, low-cost and open \nplatform with equivalent accuracy to medical-grade devices, with the addition \nof CNN-based image processing for rapid sample viability prechecks. The \ndeveloped platform has demonstrated precision to the nearest $0.18[g/dL]$ of \nhemoglobin, an R^2 = 0.945 correlation to hemoglobin absorption curves reported \nin literature, and a 97% detection accuracy of poorly-prepared samples. We see \nthe developed hemoglobin device/ML platform having massive implications in \nrural medicine, and consider it an excellent springboard for robust deep \nlearning optical spectroscopy: a currently untapped source of data for \ndetection of countless analytes. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64419", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00174"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kelly Peterson, Ognjen (Oggi) Rudovic, Ricardo Guerrero, Rosalind W. Picard", "title": "Personalized Gaussian Processes for Future Prediction of Alzheimer's Disease Progression. (arXiv:1712.00181v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00181", "type": "text/html"}], "timestampUsec": "1512364308369363", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450514d9fcb\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450514d9fcb&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we introduce the use of a personalized Gaussian Process model \n(pGP) to predict the key metrics of Alzheimer's Disease progression (MMSE, \nADAS-Cog13, CDRSB and CS) based on each patient's previous visits. We start by \nlearning a population-level model using multi-modal data from previously seen \npatients using the base Gaussian Process (GP) regression. Then, this model is \nadapted sequentially over time to a new patient using domain adaptive GPs to \nform the patient's pGP. We show that this new approach, together with an \nauto-regressive formulation, leads to significant improvements in forecasting \nfuture clinical status and cognitive scores for target patients when compared \nto modeling the population with traditional GPs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6442e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00181"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nikos Kargas, Nicholas D. Sidiropoulos, Xiao Fu", "title": "Tensors, Learning, and 'Kolmogorov Extension' for Finite-alphabet Random Vectors. (arXiv:1712.00205v1 [eess.SP])", "alternate": [{"href": "http://arxiv.org/abs/1712.00205", "type": "text/html"}], "timestampUsec": "1512364308369362", "comments": [], "summary": {"content": "<p>Estimating the joint probability mass function (PMF) of a set of random \nvariables lies at the heart of statistical learning and signal processing. \nWithout structural assumptions, such as modeling the variables as a Markov \nchain, tree, or other graphical model, joint PMF estimation is often considered \nmission impossible - the number of unknowns grows exponentially with the number \nof variables. But who gives us the structural model? Is there a generic, \n'non-parametric' way to control joint PMF complexity without relying on a \npriori structural assumptions regarding the underlying probability model? Is it \npossible to discover the operational structure without biasing the analysis up \nfront? What if we only observe random subsets of the variables, can we still \nreliably estimate the joint PMF of all? This paper shows, perhaps surprisingly, \nthat if the joint PMF of any three variables can be estimated, then the joint \nPMF of all the variables can be provably recovered under relatively mild \nconditions. The result is reminiscent of Kolmogorov's extension theorem - \nconsistent specification of lower-order distributions induces a unique \nprobability measure for the entire process. The difference is that for \nprocesses of limited complexity (rank of the high-order PMF) it is possible to \nobtain complete characterization from only third-order distributions. In fact \nnot all third order PMFs are needed; and under more stringent conditions even \nsecond-order will do. Exploiting multilinear (tensor) algebra, this paper \nproves that such higher-order PMF completion can be guaranteed - several \npertinent identifiability results are derived. It also provides a practical and \nefficient algorithm to carry out the recovery task. Judiciously designed \nsimulations and real-data experiments on movie recommendation and data \nclassification are presented to showcase the effectiveness of the approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64443", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00205"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "C&#xe9;sar A. Uribe, Soomin Lee, Alexander Gasnikov, Angelia Nedi&#x107;", "title": "Optimal Algorithms for Distributed Optimization. (arXiv:1712.00232v1 [math.OC])", "alternate": [{"href": "http://arxiv.org/abs/1712.00232", "type": "text/html"}], "timestampUsec": "1512364308369361", "comments": [], "summary": {"content": "<p>In this paper, we study the optimal convergence rate for distributed convex \noptimization problems in networks. We model the communication restrictions \nimposed by the network as a set of affine constraints and provide optimal \ncomplexity bounds for four different setups, namely: the function $F(\\xb) \n\\triangleq \\sum_{i=1}^{m}f_i(\\xb)$ is strongly convex and smooth, either \nstrongly convex or smooth or just convex. Our results show that Nesterov's \naccelerated gradient descent on the dual problem can be executed in a \ndistributed manner and obtains the same optimal rates as in the centralized \nversion of the problem (up to constant or logarithmic factors) with an \nadditional cost related to the spectral gap of the interaction matrix. Finally, \nwe discuss some extensions to the proposed setup such as proximal friendly \nfunctions, time-varying graphs, improvement of the condition numbers. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64465", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00232"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tycho Max Sylvester Tax, Jose Luis Diez Antich, Hendrik Purwins, Lars Maal&#xf8;e", "title": "Utilizing Domain Knowledge in End-to-End Audio Processing. (arXiv:1712.00254v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1712.00254", "type": "text/html"}], "timestampUsec": "1512364308369360", "comments": [], "summary": {"content": "<p>End-to-end neural network based approaches to audio modelling are generally \noutperformed by models trained on high-level data representations. In this \npaper we present preliminary work that shows the feasibility of training the \nfirst layers of a deep convolutional neural network (CNN) model to learn the \ncommonly-used log-scaled mel-spectrogram transformation. Secondly, we \ndemonstrate that upon initializing the first layers of an end-to-end CNN \nclassifier with the learned transformation, convergence and performance on the \nESC-50 environmental sound classification dataset are similar to a CNN-based \nmodel trained on the highly pre-processed log-scaled mel-spectrogram features. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64476", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00254"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Nikolay Jetchev, Urs Bergmann, Calvin Seward", "title": "GANosaic: Mosaic Creation with Generative Texture Manifolds. (arXiv:1712.00269v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.00269", "type": "text/html"}], "timestampUsec": "1512364308369359", "comments": [], "summary": {"content": "<p>This paper presents a novel framework for generating texture mosaics with \nconvolutional neural networks. Our method is called GANosaic and performs \noptimization in the latent noise space of a generative texture model, which \nallows the transformation of a content image into a mosaic exhibiting the \nvisual properties of the underlying texture manifold. To represent that \nmanifold, we use a state-of-the-art generative adversarial method for texture \nsynthesis, which can learn expressive texture representations from data and \nproduce mosaic images with very high resolution. This fully convolutional model \ngenerates smooth (without any visible borders) mosaic images which morph and \nblend different textures locally. In addition, we develop a new type of \ndifferentiable statistical regularization appropriate for optimization over the \nprior noise space of the PSGAN model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64488", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00269"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Stefan Webb, Adam Golinski, Robert Zinkov, N. Siddharth, Yee Whye Teh, Frank Wood", "title": "Faithful Model Inversion Substantially Improves Auto-encoding Variational Inference. (arXiv:1712.00287v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00287", "type": "text/html"}], "timestampUsec": "1512364308369358", "comments": [], "summary": {"content": "<p>In learning deep generative models, the encoder for variational inference is \ntypically formed in an ad hoc manner with a structure and parametrization \nanalogous to the forward model. Our chief insight is that this results in \ncoarse approximations to the posterior, and that the d-separation properties of \nthe BN structure of the forward model should be used, in a principled way, to \nproduce ones that are faithful to the posterior, for which we introduce the \nnovel Compact Minimal I-map (CoMI) algorithm. Applying our method to common \nmodels reveals that standard encoder design choices lack many important edges, \nand through experiments we demonstrate that modelling these edges is important \nfor optimal learning. We show how using a faithful encoder is crucial when \nmodelling with continuous relaxations of categorical distributions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64494", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00287"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Thomas Brouwer, Pietro Lio&#x27;", "title": "Prior and Likelihood Choices for Bayesian Matrix Factorisation on Small Datasets. (arXiv:1712.00288v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00288", "type": "text/html"}], "timestampUsec": "1512364308369357", "comments": [], "summary": {"content": "<p>In this paper, we study the effects of different prior and likelihood choices \nfor Bayesian matrix factorisation, focusing on small datasets. These choices \ncan greatly influence the predictive performance of the methods. We identify \nfour groups of approaches: Gaussian-likelihood with real-valued priors, \nnonnegative priors, semi-nonnegative models, and finally Poisson-likelihood \napproaches. For each group we review several models from the literature, \nconsidering sixteen in total, and discuss the relations between different \npriors and matrix norms. We extensively compare these methods on eight \nreal-world datasets across three application areas, giving both inter- and \nintra-group comparisons. We measure convergence runtime speed, cross-validation \nperformance, sparse and noisy prediction performance, and model selection \nrobustness. We offer several insights into the trade-offs between prior and \nlikelihood choices for Bayesian matrix factorisation on small datasets - such \nas that Poisson models give poor predictions, and that nonnegative models are \nmore constrained than real-valued ones. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644a9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00288"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512537806, "author": "Jakub M. Tomczak, Maximilian Ilse, Max Welling", "title": "Deep Learning with Permutation-invariant Operator for Multi-instance Histopathology Classification. (arXiv:1712.00310v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1712.00310", "type": "text/html"}], "timestampUsec": "1512364308369356", "comments": [], "summary": {"content": "<p>The computer-aided analysis of medical scans is a longstanding goal in the \nmedical imaging field. Currently, deep learning has became a dominant \nmethodology for supporting pathologists and radiologist. Deep learning \nalgorithms have been successfully applied to digital pathology and radiology, \nnevertheless, there are still practical issues that prevent these tools to be \nwidely used in practice. The main obstacles are low number of available cases \nand large size of images (a.k.a. the small n, large p problem in machine \nlearning), and a very limited access to annotation at a pixel level that can \nlead to severe overfitting and large computational requirements. We propose to \nhandle these issues by introducing a framework that processes a medical image \nas a collection of small patches using a single, shared neural network. The \nfinal diagnosis is provided by combining scores of individual patches using a \npermutation-invariant operator (combination). In machine learning community \nsuch approach is called a multi-instance learning (MIL). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512537802, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644b3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00310"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Marc Oliu, Javier Selva, Sergio Escalera", "title": "Folded Recurrent Neural Networks for Future Video Prediction. (arXiv:1712.00311v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.00311", "type": "text/html"}], "timestampUsec": "1512364308369355", "comments": [], "summary": {"content": "<p>Future video prediction is an ill-posed Computer Vision problem that recently \nreceived much attention. Its main challenges are the high variability in video \ncontent, the propagation of errors through time, and the non-specificity of the \nfuture frames: given a sequence of past frames there is a continuous \ndistribution of possible futures. This work introduces bijective Gated \nRecurrent Units, a double mapping between the input and output of a GRU layer. \nThis allows for recurrent auto-encoders with state sharing between encoder and \ndecoder, stratifying the sequence representation and helping to prevent \ncapacity problems. We show how with this topology only the encoder or decoder \nneeds to be applied for input encoding and prediction, respectively. This \nreduces the computational cost and avoids re-encoding the predictions when \ngenerating a sequence of frames, mitigating the propagation of errors. \nFurthermore, it is possible to remove layers from an already trained model, \ngiving an insight to the role performed by each layer and making the model more \nexplainable. We evaluate our approach on three video datasets, outperforming \nstate of the art prediction results on MMNIST and UCF101, and obtaining \ncompetitive results on KTH with 2 and 3 times less memory usage and \ncomputational cost than the best scored approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644bf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00311"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hongbin Pei, Bo Yang, Jiming Liu, Lei Dong", "title": "Group Sparse Bayesian Learning for Active Surveillance on Epidemic Dynamics. (arXiv:1712.00328v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00328", "type": "text/html"}], "timestampUsec": "1512364308369354", "comments": [], "summary": {"content": "<p>Predicting epidemic dynamics is of great value in understanding and \ncontrolling diffusion processes, such as infectious disease spread and \ninformation propagation. This task is intractable, especially when surveillance \nresources are very limited. To address the challenge, we study the problem of \nactive surveillance, i.e., how to identify a small portion of system components \nas sentinels to effect monitoring, such that the epidemic dynamics of an entire \nsystem can be readily predicted from the partial data collected by such \nsentinels. We propose a novel measure, the gamma value, to identify the \nsentinels by modeling a sentinel network with row sparsity structure. We design \na flexible group sparse Bayesian learning algorithm to mine the sentinel \nnetwork suitable for handling both linear and non-linear dynamical systems by \nusing the expectation maximization method and variational approximation. The \nefficacy of the proposed algorithm is theoretically analyzed and empirically \nvalidated using both synthetic and real-world data. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644c5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00328"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Adrien Lagrange, Mathieu Fauvel, St&#xe9;phane May, Nicolas Dobigeon", "title": "Hierarchical Bayesian image analysis: from low-level modeling to robust supervised learning. (arXiv:1712.00368v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1712.00368", "type": "text/html"}], "timestampUsec": "1512364308369353", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450514da3b2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450514da3b2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Within a supervised classification framework, labeled data are used to learn \nclassifier parameters. Prior to that, it is generally required to perform \ndimensionality reduction via feature extraction. These preprocessing steps have \nmotivated numerous research works aiming at recovering latent variables in an \nunsupervised context. This paper proposes a unified framework to perform \nclassification and low-level modeling jointly. The main objective is to use the \nestimated latent variables as features for classification and to incorporate \nsimultaneously supervised information to help latent variable extraction. The \nproposed hierarchical Bayesian model is divided into three stages: a first \nlow-level modeling stage to estimate latent variables, a second stage \nclustering these features into statistically homogeneous groups and a last \nclassification stage exploiting the (possibly badly) labeled data. Performance \nof the model is assessed in the specific context of hyperspectral image \ninterpretation, unifying two standard analysis techniques, namely unmixing and \nclassification. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644ca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00368"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, Yanqi Zhou", "title": "Deep Learning Scaling is Predictable, Empirically. (arXiv:1712.00409v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00409", "type": "text/html"}], "timestampUsec": "1512364308369352", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505153bd73\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505153bd73&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep learning (DL) creates impactful advances following a virtuous recipe: \nmodel architecture search, creating large training data sets, and scaling \ncomputation. It is widely believed that growing training sets and models should \nimprove accuracy and result in better products. As DL application domains grow, \nwe would like a deeper understanding of the relationships between training set \nsize, computational scale, and model accuracy improvements to advance the \nstate-of-the-art. \n</p> \n<p>This paper presents a large scale empirical characterization of \ngeneralization error and model size growth as training sets grow. We introduce \na methodology for this measurement and test four machine learning domains: \nmachine translation, language modeling, image processing, and speech \nrecognition. Our empirical results show power-law generalization error scaling \nacross a breadth of factors, resulting in power-law exponents---the \"steepness\" \nof the learning curve---yet to be explained by theoretical work. Further, model \nimprovements only shift the error but do not appear to affect the power-law \nexponent. We also show that model size scales sublinearly with data size. These \nscaling relationships have significant implications on deep learning research, \npractice, and systems. They can assist model debugging, setting accuracy \ntargets, and decisions about data set growth. They can also guide computing \nsystem design and underscore the importance of continued computational scaling. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00409"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "James T. Wilson, Riccardo Moriconi, Frank Hutter, Marc Peter Deisenroth", "title": "The reparameterization trick for acquisition functions. (arXiv:1712.00424v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1712.00424", "type": "text/html"}], "timestampUsec": "1512364308369351", "comments": [], "summary": {"content": "<p>Bayesian optimization is a sample-efficient approach to solving global \noptimization problems. Along with a surrogate model, this approach relies on \ntheoretically motivated value heuristics (acquisition functions) to guide the \nsearch process. Maximizing acquisition functions yields the best performance; \nunfortunately, this ideal is difficult to achieve since optimizing acquisition \nfunctions per se is frequently non-trivial. This statement is especially true \nin the parallel setting, where acquisition functions are routinely non-convex, \nhigh-dimensional, and intractable. Here, we demonstrate how many popular \nacquisition functions can be formulated as Gaussian integrals amenable to the \nreparameterization trick and, ensuingly, gradient-based optimization. Further, \nwe use this reparameterized representation to derive an efficient Monte Carlo \nestimator for the upper confidence bound acquisition function in the context of \nparallel selection. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644db", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00424"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiaoyu Liu, Diyu Yang, Aly El Gamal", "title": "Deep Neural Network Architectures for Modulation Classification. (arXiv:1712.00443v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1712.00443", "type": "text/html"}], "timestampUsec": "1512364308369350", "comments": [], "summary": {"content": "<p>In this work, we investigate the value of employing deep learning for the \ntask of wireless signal modulation recognition. Recently in [1], a framework \nhas been introduced by generating a dataset using GNU radio that mimics the \nimperfections in a real wireless channel, and uses 11 different modulation \ntypes. Further, a convolutional neural network (CNN) architecture was developed \nand shown to deliver performance that exceeds that of expert-based approaches. \nHere, we follow the framework of [1] and find deep neural network architectures \nthat deliver higher accuracy than the state of the art. We tested the \narchitecture of [1] and found it to achieve an accuracy of approximately 75% of \ncorrectly recognizing the modulation type. We first tune the CNN architecture \nof [1] and find a design with four convolutional layers and two dense layers \nthat gives an accuracy of approximately 83.8% at high SNR. We then develop \narchitectures based on the recently introduced ideas of Residual Networks \n(ResNet [2]) and Densely Connected Networks (DenseNet [3]) to achieve high SNR \naccuracies of approximately 83.5% and 86.6%, respectively. Finally, we \nintroduce a Convolutional Long Short-term Deep Neural Network (CLDNN [4]) to \nachieve an accuracy of approximately 88.5% at high SNR. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644e5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1712.00443"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Michael Riis Andersen, Aki Vehtari, Ole Winther, Lars Kai Hansen", "title": "Bayesian inference for spatio-temporal spike-and-slab priors. (arXiv:1509.04752v3 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1509.04752", "type": "text/html"}], "timestampUsec": "1512364308369349", "comments": [], "summary": {"content": "<p>In this work, we address the problem of solving a series of underdetermined \nlinear inverse problems subject to a sparsity constraint. We generalize the \nspike-and-slab prior distribution to encode a priori correlation of the support \nof the solution in both space and time by imposing a transformed Gaussian \nprocess on the spike-and-slab probabilities. An expectation propagation (EP) \nalgorithm for posterior inference under the proposed model is derived. For \nlarge scale problems, the standard EP algorithm can be prohibitively slow. We \ntherefore introduce three different approximation schemes to reduce the \ncomputational complexity. Finally, we demonstrate the proposed model using \nnumerical experiments based on both synthetic and real data sets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644eb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1509.04752"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiaohan Yan, Jacob Bien", "title": "Hierarchical Sparse Modeling: A Choice of Two Group Lasso Formulations. (arXiv:1512.01631v4 [stat.ME] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1512.01631", "type": "text/html"}], "timestampUsec": "1512364308369348", "comments": [], "summary": {"content": "<p>Demanding sparsity in estimated models has become a routine practice in \nstatistics. In many situations, we wish to require that the sparsity patterns \nattained honor certain problem-specific constraints. Hierarchical sparse \nmodeling (HSM) refers to situations in which these constraints specify that one \nset of parameters be set to zero whenever another is set to zero. In recent \nyears, numerous papers have developed convex regularizers for this form of \nsparsity structure, which arises in many areas of statistics including \ninteraction modeling, time series analysis, and covariance estimation. In this \npaper, we observe that these methods fall into two frameworks, the group lasso \n(GL) and latent overlapping group lasso (LOG), which have not been \nsystematically compared in the context of HSM. The purpose of this paper is to \nprovide a side-by-side comparison of these two frameworks for HSM in terms of \ntheir statistical properties and computational efficiency. We call special \nattention to GL's more aggressive shrinkage of parameters deep in the \nhierarchy, a property not shared by LOG. In terms of computation, we introduce \na finite-step algorithm that exactly solves the proximal operator of LOG for a \ncertain simple HSM structure; we later exploit this to develop a novel \npath-based block coordinate descent scheme for general HSM structures. Both \nalgorithms greatly improve the computational performance of LOG. Finally, we \ncompare the two methods in the context of covariance estimation, where we \nintroduce a new sparsely-banded estimator using LOG, which we show achieves the \nstatistical advantages of an existing GL-based method but is simpler to express \nand more efficient to compute. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb644fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1512.01631"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ravi Kiran Raman, Lav R. Varshney", "title": "Universal Joint Image Clustering and Registration using Partition Information. (arXiv:1701.02776v2 [cs.IT] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1701.02776", "type": "text/html"}], "timestampUsec": "1512364308369347", "comments": [], "summary": {"content": "<p>We consider the problem of universal joint clustering and registration of \nimages and define algorithms using multivariate information functionals. We \nfirst study registering two images using maximum mutual information and prove \nits asymptotic optimality. We then show the shortcomings of pairwise \nregistration in multi-image registration, and design an asymptotically optimal \nalgorithm based on multiinformation. Further, we define a novel multivariate \ninformation functional to perform joint clustering and registration of images, \nand prove consistency of the algorithm. Finally, we consider registration and \nclustering of numerous limited-resolution images, defining algorithms that are \norder-optimal in scaling of number of pixels in each image with the number of \nimages. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6451a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1701.02776"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jen-Cheng Hou, Syu-Siang Wang, Ying-Hui Lai, Yu Tsao, Hsiu-Wen Chang, Hsin-Min Wang", "title": "Audio-Visual Speech Enhancement based on Multimodal Deep Convolutional Neural Networks. (arXiv:1703.10893v4 [cs.SD] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.10893", "type": "text/html"}], "timestampUsec": "1512364308369346", "comments": [], "summary": {"content": "<p>Speech enhancement (SE) aims to reduce noise in speech signals. Most SE \ntechniques focus only on addressing audio information. In this work, inspired \nby multimodal learning, which utilizes data from different modalities, and the \nrecent success of convolutional neural networks (CNNs) in SE, we propose an \naudio-visual deep CNNs (AVDCNN) SE model, which incorporates audio and visual \nstreams into a unified network model. We also propose a multi-task learning \nframework for reconstructing audio and visual signals at the output layer. \nPrecisely speaking, the proposed AVDCNN model is structured as an audio-visual \nencoder-decoder network, in which audio and visual data are first processed \nusing individual CNNs, and then fused into a joint network to generate enhanced \nspeech (the primary task) and reconstructed images (the secondary task) at the \noutput layer. The model is trained in an end-to-end manner, and parameters are \njointly learned through back-propagation. We evaluate enhanced speech using \nfive instrumental criteria. Results show that the AVDCNN model yields a notably \nsuperior performance compared with an audio-only CNN-based SE model and two \nconventional SE approaches, confirming the effectiveness of integrating visual \ninformation into the SE process. In addition, the AVDCNN model also outperforms \nan existing audio-visual SE model, confirming its capability of effectively \ncombining audio and visual information in SE. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64538", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.10893"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, Cynthia Rudin", "title": "Learning Certifiably Optimal Rule Lists for Categorical Data. (arXiv:1704.01701v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.01701", "type": "text/html"}], "timestampUsec": "1512364308369345", "comments": [], "summary": {"content": "<p>We present the design and implementation of a custom discrete optimization \ntechnique for building rule lists over a categorical feature space. Our \nalgorithm produces rule lists with optimal training performance, according to \nthe regularized empirical risk, with a certificate of optimality. By leveraging \nalgorithmic bounds, efficient data structures, and computational reuse, we \nachieve several orders of magnitude speedup in time and a massive reduction of \nmemory consumption. We demonstrate that our approach produces optimal rule \nlists on practical problems in seconds. Our results indicate that it is \npossible to construct optimal sparse rule lists that are approximately as \naccurate as the COMPAS proprietary risk prediction tool on data from Broward \nCounty, Florida, but that are completely interpretable. This framework is a \nnovel alternative to CART and other decision tree methods for interpretable \nmodeling. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6455b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.01701"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kamil Ciosek, Shimon Whiteson", "title": "Expected Policy Gradients. (arXiv:1706.05374v4 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.05374", "type": "text/html"}], "timestampUsec": "1512364308369344", "comments": [], "summary": {"content": "<p>We propose expected policy gradients (EPG), which unify stochastic policy \ngradients (SPG) and deterministic policy gradients (DPG) for reinforcement \nlearning. Inspired by expected sarsa, EPG integrates across the action when \nestimating the gradient, instead of relying only on the action in the sampled \ntrajectory. We establish a new general policy gradient theorem, of which the \nstochastic and deterministic policy gradient theorems are special cases. We \nalso prove that EPG reduces the variance of the gradient estimates without \nrequiring deterministic policies and, for the Gaussian case, with no \ncomputational overhead. Finally, we show that it is optimal in a certain sense \nto explore with a Gaussian policy such that the covariance is proportional to \nthe exponential of the scaled Hessian of the critic with respect to the \nactions. We present empirical results confirming that this new form of \nexploration substantially outperforms DPG with the Ornstein-Uhlenbeck heuristic \nin four challenging MuJoCo domains. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6456e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.05374"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le", "title": "Learning Transferable Architectures for Scalable Image Recognition. (arXiv:1707.07012v3 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.07012", "type": "text/html"}], "timestampUsec": "1512364308369343", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505153c00d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505153c00d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Developing neural network image classification models often requires \nsignificant architecture engineering. In this paper, we attempt to automate \nthis engineering process by learning the model architectures directly on the \ndataset of interest. As this approach is expensive when the dataset is large, \nwe propose to search for an architectural building block on a small dataset and \nthen transfer the block to a larger dataset. Our key contribution is the design \nof a new search space which enables transferability. In our experiments, we \nsearch for the best convolutional layer (or \"cell\") on the CIFAR-10 dataset and \nthen apply this cell to the ImageNet dataset by stacking together more copies \nof this cell, each with their own parameters. Although the cell is not searched \nfor directly on ImageNet, an architecture constructed from the best cell \nachieves, among the published works, state-of-the-art accuracy of 82.7% top-1 \nand 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than \nthe best human-invented architectures while having 9 billion fewer FLOPS -- a \nreduction of 28% in computational demand from the previous state-of-the-art \nmodel. When evaluated at different levels of computational cost, accuracies of \nour models exceed those of the state-of-the-art human-designed models. For \ninstance, a smaller network constructed from the best cell also achieves 74% \ntop-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art \nmodels for mobile platforms. On CIFAR-10, an architecture constructed from the \nbest cell achieves 2.4% error rate, which is also state-of-the-art. Finally, \nthe image features learned from image classification can also be transferred to \nother computer vision problems. On the task of object detection, the learned \nfeatures used with the Faster-RCNN framework surpass state-of-the-art by 4.0% \nachieving 43.1% mAP on the COCO dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb64586", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.07012"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christian Donner, Manfred Opper", "title": "Inverse Ising problem in continuous time: A latent variable approach. (arXiv:1709.04495v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1709.04495", "type": "text/html"}], "timestampUsec": "1512364308369342", "comments": [], "summary": {"content": "<p>We consider the inverse Ising problem, i.e. the inference of network \ncouplings from observed spin trajectories for a model with continuous time \nGlauber dynamics. By introducing two sets of auxiliary latent random variables \nwe render the likelihood into a form, which allows for simple iterative \ninference algorithms with analytical updates. The variables are: (1) Poisson \nvariables to linearise an exponential term which is typical for point process \nlikelihoods and (2) P\\'olya-Gamma variables, which make the likelihood \nquadratic in the coupling parameters. Using the augmented likelihood, we derive \nan expectation-maximization (EM) algorithm to obtain the maximum likelihood \nestimate of network parameters. Using a third set of latent variables we extend \nthe EM algorithm to sparse couplings via L1 regularization. Finally, we develop \nan efficient approximate Bayesian inference algorithm using a variational \napproach. We demonstrate the performance of our algorithms on data simulated \nfrom an Ising model. For data which are simulated from a more biologically \nplausible network with spiking neurons, we show that the Ising model captures \nwell the low order statistics of the data and how the Ising couplings are \nrelated to the underlying synaptic structure of the simulated network. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb6459d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1709.04495"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Iqbal H. Sarker, Muhammad Ashad Kabir, Alan Colman, Jun Han", "title": "An Improved Naive Bayes Classifier-based Noise Detection Technique for Classifying User Phone Call Behavior. (arXiv:1710.04461v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.04461", "type": "text/html"}], "timestampUsec": "1512364308369341", "comments": [], "summary": {"content": "<p>The presence of noisy instances in mobile phone data is a fundamental issue \nfor classifying user phone call behavior (i.e., accept, reject, missed and \noutgoing), with many potential negative consequences. The classification \naccuracy may decrease and the complexity of the classifiers may increase due to \nthe number of redundant training samples. To detect such noisy instances from a \ntraining dataset, researchers use naive Bayes classifier (NBC) as it identifies \nmisclassified instances by taking into account independence assumption and \nconditional probabilities of the attributes. However, some of these \nmisclassified instances might indicate usages behavioral patterns of individual \nmobile phone users. Existing naive Bayes classifier based noise detection \ntechniques have not considered this issue and, thus, are lacking in \nclassification accuracy. In this paper, we propose an improved noise detection \ntechnique based on naive Bayes classifier for effectively classifying users' \nphone call behaviors. In order to improve the classification accuracy, we \neffectively identify noisy instances from the training dataset by analyzing the \nbehavioral patterns of individuals. We dynamically determine a noise threshold \naccording to individual's unique behavioral patterns by using both the naive \nBayes classifier and Laplace estimator. We use this noise threshold to identify \nnoisy instances. To measure the effectiveness of our technique in classifying \nuser phone call behavior, we employ the most popular classification algorithm \n(e.g., decision tree). Experimental results on the real phone call log dataset \nshow that our proposed technique more accurately identifies the noisy instances \nfrom the training datasets that leads to better classification accuracy. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb645b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.04461"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Kimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin", "title": "Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples. (arXiv:1711.09325v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.09325", "type": "text/html"}], "timestampUsec": "1512364308369340", "comments": [], "summary": {"content": "<p>The problem of detecting whether a test sample is from in-distribution (i.e., \ntraining distribution by a classifier) or out-of-distribution sufficiently \ndifferent from it arises in many real-world machine learning applications. \nHowever, the state-of-art deep neural networks are known to be highly \noverconfident in their predictions, i.e., do not distinguish in- and \nout-of-distributions. Recently, to handle this issue, several threshold-based \ndetectors have been proposed given pre-trained neural classifiers. However, the \nperformance of prior works highly depends on how to train the classifiers since \nthey only focus on improving inference procedures. In this paper, we develop a \nnovel training method for classifiers so that such inference algorithms can \nwork better. In particular, we suggest two additional terms added to the \noriginal loss (e.g., cross entropy). The first one forces samples from \nout-of-distribution less confident by the classifier and the second one is for \n(implicitly) generating most effective training samples for the first one. In \nessence, our method jointly trains both classification and generative neural \nnetworks for out-of-distribution. We demonstrate its effectiveness using deep \nconvolutional neural networks on various popular image datasets. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb645cd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.09325"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Harish S. Bhat, Sidra J. Goldman-Mellor", "title": "Predicting Adolescent Suicide Attempts with Neural Networks. (arXiv:1711.10057v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.10057", "type": "text/html"}], "timestampUsec": "1512364308369339", "comments": [], "summary": {"content": "<p>Though suicide is a major public health problem in the US, machine learning \nmethods are not commonly used to predict an individual's risk of \nattempting/committing suicide. In the present work, starting with an anonymized \ncollection of electronic health records for 522,056 unique, California-resident \nadolescents, we develop neural network models to predict suicide attempts. We \nframe the problem as a binary classification problem in which we use a \npatient's data from 2006-2009 to predict either the presence (1) or absence (0) \nof a suicide attempt in 2010. After addressing issues such as severely \nimbalanced classes and the variable length of a patient's history, we build \nneural networks with depths varying from two to eight hidden layers. For test \nset observations where we have at least five ED/hospital visits' worth of data \non a patient, our depth-4 model achieves a sensitivity of 0.703, specificity of \n0.980, and AUC of 0.958. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512364308369", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033fb645da", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.10057"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sebastian Urban, Marcus Basalla, Patrick van der Smagt", "title": "Gaussian Process Neurons Learn Stochastic Activation Functions. (arXiv:1711.11059v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11059", "type": "text/html"}], "timestampUsec": "1512136966936910", "comments": [], "summary": {"content": "<p>We propose stochastic, non-parametric activation functions that are fully \nlearnable and individual to each neuron. Complexity and the risk of overfitting \nare controlled by placing a Gaussian process prior over these functions. The \nresult is the Gaussian process neuron, a probabilistic unit that can be used as \nthe basic building block for probabilistic graphical models that resemble the \nstructure of neural networks. The proposed model can intrinsically handle \nuncertainties in its inputs and self-estimate the confidence of its \npredictions. Using variational Bayesian inference and the central limit \ntheorem, a fully deterministic loss function is derived, allowing it to be \ntrained as efficiently as a conventional neural network using mini-batch \ngradient descent. The posterior distribution of activation functions is \ninferred from the training data alongside the weights of the network. \n</p> \n<p>The proposed model favorably compares to deep Gaussian processes, both in \nmodel complexity and efficiency of inference. It can be directly applied to \nrecurrent or convolutional network structures, allowing its use in audio and \nimage processing tasks. \n</p> \n<p>As an preliminary empirical evaluation we present experiments on regression \nand classification tasks, in which our model achieves performance comparable to \nor better than a Dropout regularized neural network with a fixed activation \nfunction. Experiments are ongoing and results will be added as they become \navailable. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2c9fea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11059"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yudong Cao, Gian Giacomo Guerreschi, Al&#xe1;n Aspuru-Guzik", "title": "Quantum Neuron: an elementary building block for machine learning on quantum computers. (arXiv:1711.11240v1 [quant-ph])", "alternate": [{"href": "http://arxiv.org/abs/1711.11240", "type": "text/html"}], "timestampUsec": "1512136966936909", "comments": [], "summary": {"content": "<p>Even the most sophisticated artificial neural networks are built by \naggregating substantially identical units called neurons. A neuron receives \nmultiple signals, internally combines them, and applies a non-linear function \nto the resulting weighted sum. Several attempts to generalize neurons to the \nquantum regime have been proposed, but all proposals collided with the \ndifficulty of implementing non-linear activation functions, which is essential \nfor classical neurons, due to the linear nature of quantum mechanics. Here we \npropose a solution to this roadblock in the form of a small quantum circuit \nthat naturally simulates neurons with threshold activation. Our quantum circuit \ndefines a building block, the \"quantum neuron\", that can reproduce a variety of \nclassical neural network constructions while maintaining the ability to process \nsuperpositions of inputs and preserve quantum coherence and entanglement. In \nthe construction of feedforward networks of quantum neurons, we provide \nnumerical evidence that the network not only can learn a function when trained \nwith superposition of inputs and the corresponding output, but that this \ntraining suffices to learn the function on all individual inputs separately. \nWhen arranged to mimic Hopfield networks, quantum neural networks exhibit \nproperties of associative memory. Patterns are encoded using the simple Hebbian \nrule for the weights and we demonstrate attractor dynamics from corrupted \ninputs. Finally, the fact that our quantum model closely captures (traditional) \nneural network dynamics implies that the vast body of literature and results on \nneural networks becomes directly relevant in the context of quantum machine \nlearning. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2c9ff5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11240"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Christopher Tegho, Pawe&#x142; Budzianowski, Milica Ga&#x161;i&#x107;", "title": "Uncertainty Estimates for Efficient Neural Network-based Dialogue Policy Optimisation. (arXiv:1711.11486v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11486", "type": "text/html"}], "timestampUsec": "1512136966936908", "comments": [], "summary": {"content": "<p>In statistical dialogue management, the dialogue manager learns a policy that \nmaps a belief state to an action for the system to perform. Efficient \nexploration is key to successful policy optimisation. Current deep \nreinforcement learning methods are very promising but rely on epsilon-greedy \nexploration, thus subjecting the user to a random choice of action during \nlearning. Alternative approaches such as Gaussian Process SARSA (GPSARSA) \nestimate uncertainties and are sample efficient, leading to better user \nexperience, but on the expense of a greater computational complexity. This \npaper examines approaches to extract uncertainty estimates from deep Q-networks \n(DQN) in the context of dialogue management. We perform an extensive benchmark \nof deep Bayesian methods to extract uncertainty estimates, namely \nBayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble and \nalpha-divergences, combining it with DQN algorithm. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca004", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11486"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513669230, "author": "Antti Tarvainen, Harri Valpola", "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. (arXiv:1703.01780v4 [cs.NE] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.01780", "type": "text/html"}], "timestampUsec": "1512136966936907", "comments": [], "summary": {"content": "<p>The recently proposed Temporal Ensembling has achieved state-of-the-art \nresults in several semi-supervised learning benchmarks. It maintains an \nexponential moving average of label predictions on each training example, and \npenalizes predictions that are inconsistent with this target. However, because \nthe targets change only once per epoch, Temporal Ensembling becomes unwieldy \nwhen learning large datasets. To overcome this problem, we propose Mean \nTeacher, a method that averages model weights instead of label predictions. As \nan additional benefit, Mean Teacher improves test accuracy and enables training \nwith fewer labels than Temporal Ensembling. Without changing the network \narchitecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250 \nlabels, outperforming Temporal Ensembling trained with 1000 labels. We also \nshow that a good network architecture is crucial to performance. Combining Mean \nTeacher and Residual Networks, we improve the state of the art on CIFAR-10 with \n4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels \nfrom 35.24% to 9.11%. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1513669230, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca010", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.01780"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513055251, "author": "Rafa&#x142; Muszy&#x144;ski, Jun Wang", "title": "Happiness Pursuit: Personality Learning in a Society of Agents. (arXiv:1711.11068v2 [cs.MA] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.11068", "type": "text/html"}], "timestampUsec": "1512136966936906", "comments": [], "summary": {"content": "<p>Modeling personality is a challenging problem with applications spanning \ncomputer games, virtual assistants, online shopping and education. Many \ntechniques have been tried, ranging from neural networks to computational \ncognitive architectures. However, most approaches rely on examples with \nhand-crafted features and scenarios. Here, we approach learning a personality \nby training agents using a Deep Q-Network (DQN) model on rewards based on \npsychoanalysis, against hand-coded AI in the game of Pong. As a result, we \nobtain 4 agents, each with its own personality. Then, we define happiness of an \nagent, which can be seen as a measure of alignment with agent's objective \nfunction, and study it when agents play both against hand-coded AI, and against \neach other. We find that the agents that achieve higher happiness during \ntesting against hand-coded AI, have lower happiness when competing against each \nother. This suggests that higher happiness in testing is a sign of overfitting \nin learning to interact with hand-coded AI, and leads to worse performance \nagainst agents with different personalities. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca01f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11068"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Adit Krishnan, Ashish Sharma, Hari Sundaram", "title": "Improving Latent User Models in Online Social Media. (arXiv:1711.11124v1 [cs.SI])", "alternate": [{"href": "http://arxiv.org/abs/1711.11124", "type": "text/html"}], "timestampUsec": "1512136966936905", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a4505153c206\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a4505153c206&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Modern social platforms are characterized by the presence of rich \nuser-behavior data associated with the publication, sharing and consumption of \ntextual content. Users interact with content and with each other in a complex \nand dynamic social environment while simultaneously evolving over time. In \norder to effectively characterize users and predict their future behavior in \nsuch a setting, it is necessary to overcome several challenges. Content \nheterogeneity and temporal inconsistency of behavior data result in severe \nsparsity at the user level. In this paper, we propose a novel \nmutual-enhancement framework to simultaneously partition and learn latent \nactivity profiles of users. We propose a flexible user partitioning approach to \neffectively discover rare behaviors and tackle user-level sparsity. We \nextensively evaluate the proposed framework on massive datasets from real-world \nplatforms including Q&amp;A networks and interactive online courses (MOOCs). Our \nresults indicate significant gains over state-of-the-art behavior models ( 15% \navg ) in a varied range of tasks and our gains are further magnified for users \nwith limited interaction data. The proposed algorithms are amenable to \nparallelization, scale linearly in the size of datasets, and provide \nflexibility to model diverse facets of user behavior. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca029", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11124"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang, William Yang Wang", "title": "Video Captioning via Hierarchical Reinforcement Learning. (arXiv:1711.11135v1 [cs.CV])", "alternate": [{"href": "http://arxiv.org/abs/1711.11135", "type": "text/html"}], "timestampUsec": "1512136966936904", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a45051591bea\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a45051591bea&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Video captioning is the task of automatically generating a textual \ndescription of the actions in a video. Although previous work (e.g. \nsequence-to-sequence model) has shown promising results in abstracting a coarse \ndescription of a short video, it is still very challenging to caption a video \ncontaining multiple fine-grained actions with a detailed description. This \npaper aims to address the challenge by proposing a novel hierarchical \nreinforcement learning framework for video captioning, where a high-level \nManager module learns to design sub-goals and a low-level Worker module \nrecognizes the primitive actions to fulfill the sub-goal. With this \ncompositional framework to reinforce video captioning at different levels, our \napproach significantly outperforms all the baseline methods on a newly \nintroduced large-scale dataset for fine-grained video captioning. Furthermore, \nour non-ensemble model has already achieved the state-of-the-art results on the \nwidely-used MSR-VTT dataset. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca032", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11135"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, Guy Van den Broeck", "title": "A Semantic Loss Function for Deep Learning with Symbolic Knowledge. (arXiv:1711.11157v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1711.11157", "type": "text/html"}], "timestampUsec": "1512136966936903", "comments": [], "summary": {"content": "<p>This paper develops a novel methodology for using symbolic knowledge in deep \nlearning. From first principles, we derive a semantic loss function that \nbridges between neural output vectors and logical constraints. This loss \nfunction captures how close the neural network is to satisfying the constraints \non its output. An experimental evaluation shows that our semantic loss function \neffectively guides the learner to achieve (near-)state-of-the-art results on \nsemi-supervised multi-class classification. Moreover, it significantly \nincreases the ability of the neural network to predict structured objects, such \nas rankings and paths. These discrete concepts are tremendously difficult to \nlearn, and benefit from a tight integration of deep learning and symbolic \nreasoning methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca03d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11157"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Sahin Cem Geyik, Jianqiang Shen, Shahriar Shariat, Ali Dasdan, Santanu Kolay", "title": "Towards Data Quality Assessment in Online Advertising. (arXiv:1711.11175v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1711.11175", "type": "text/html"}], "timestampUsec": "1512136966936902", "comments": [], "summary": {"content": "<p>In online advertising, our aim is to match the advertisers with the most \nrelevant users to optimize the campaign performance. In the pursuit of \nachieving this goal, multiple data sources provided by the advertisers or \nthird-party data providers are utilized to choose the set of users according to \nthe advertisers' targeting criteria. In this paper, we present a framework that \ncan be applied to assess the quality of such data sources in large scale. This \nframework efficiently evaluates the similarity of a specific data source \ncategorization to that of the ground truth, especially for those cases when the \nground truth is accessible only in aggregate, and the user-level information is \nanonymized or unavailable due to privacy reasons. We propose multiple \nmethodologies within this framework, present some preliminary assessment \nresults, and evaluate how the methodologies compare to each other. We also \npresent two use cases where we can utilize the data quality assessment results: \nthe first use case is targeting specific user categories, and the second one is \nforecasting the desirable audiences we can reach for an online advertising \ncampaign with pre-set targeting criteria. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca04e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11175"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Dhaval Adjodah, Dan Calacci, Yan Leng, Peter Krafft, Esteban Moro, Alex Pentland", "title": "Improved Learning in Evolution Strategies via Sparser Inter-Agent Network Topologies. (arXiv:1711.11180v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1711.11180", "type": "text/html"}], "timestampUsec": "1512136966936901", "comments": [], "summary": {"content": "<p>We draw upon a previously largely untapped literature on human collective \nintelligence as a source of inspiration for improving deep learning. Implicit \nin many algorithms that attempt to solve Deep Reinforcement Learning (DRL) \ntasks is the network of processors along which parameter values are shared. So \nfar, existing approaches have implicitly utilized fully-connected networks, in \nwhich all processors are connected. However, the scientific literature on human \ncollective intelligence suggests that complete networks may not always be the \nmost effective information network structures for distributed search through \ncomplex spaces. Here we show that alternative topologies can improve deep \nneural network training: we find that sparser networks learn higher rewards \nfaster, leading to learning improvements at lower communication costs. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca060", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11180"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Hyunwoo Lee, Jooyoung Kim, Dojun Yang, Joon-Ho Kim", "title": "Embedded Real-Time Fall Detection Using Deep Learning For Elderly Care. (arXiv:1711.11200v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11200", "type": "text/html"}], "timestampUsec": "1512136966936900", "comments": [], "summary": {"content": "<p>This paper proposes a real-time embedded fall detection system using a \nDVS(Dynamic Vision Sensor) that has never been used for traditional fall \ndetection, a dataset for fall detection using that, and a DVS-TN(DVS-Temporal \nNetwork). The first contribution is building a DVS Falls Dataset, which made \nour network to recognize a much greater variety of falls than the existing \ndatasets that existed before and solved privacy issues using the DVS. Secondly, \nwe introduce the DVS-TN : optimized deep learning network to detect falls using \nDVS. Finally, we implemented a fall detection system which can run on \nlow-computing H/W with real-time, and tested on DVS Falls Dataset that takes \ninto account various falls situations. Our approach achieved 95.5% on the \nF1-score and operates at 31.25 FPS on NVIDIA Jetson TX1 board. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca07b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11200"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yunhao Tang, Alp Kucukelbir", "title": "Variational Deep Q Network. (arXiv:1711.11225v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1711.11225", "type": "text/html"}], "timestampUsec": "1512136966936899", "comments": [], "summary": {"content": "<p>We propose a framework that directly tackles the probability distribution of \nthe value function parameters in Deep Q Network (DQN), with powerful \nvariational inference subroutines to approximate the posterior of the \nparameters. We will establish the equivalence between our proposed surrogate \nobjective and variational inference loss. Our new algorithm achieves efficient \nexploration and performs well on large scale chain Markov Decision Process \n(MDP). \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca094", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11225"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Shu Guo, Quan Wang, Lihong Wang, Bin Wang, Li Guo", "title": "Knowledge Graph Embedding with Iterative Guidance from Soft Rules. (arXiv:1711.11231v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1711.11231", "type": "text/html"}], "timestampUsec": "1512136966936898", "comments": [], "summary": {"content": "<p>Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of \ncurrent research. Combining such an embedding model with logic rules has \nrecently attracted increasing attention. Most previous attempts made a one-time \ninjection of logic rules, ignoring the interactive nature between embedding \nlearning and logical inference. And they focused only on hard rules, which \nalways hold with no exception and usually require extensive manual effort to \ncreate or validate. In this paper, we propose Rule-Guided Embedding (RUGE), a \nnovel paradigm of KG embedding with iterative guidance from soft rules. RUGE \nenables an embedding model to learn simultaneously from 1) labeled triples that \nhave been directly observed in a given KG, 2) unlabeled triples whose labels \nare going to be predicted iteratively, and 3) soft rules with various \nconfidence levels extracted automatically from the KG. In the learning process, \nRUGE iteratively queries rules to obtain soft labels for unlabeled triples, and \nintegrates such newly labeled triples to update the embedding model. Through \nthis iterative procedure, knowledge embodied in logic rules may be better \ntransferred into the learned embeddings. We evaluate RUGE in link prediction on \nFreebase and YAGO. Experimental results show that: 1) with rule knowledge \ninjected iteratively, RUGE achieves significant and consistent improvements \nover state-of-the-art baselines; and 2) despite their uncertainties, \nautomatically extracted soft rules are highly beneficial to KG embedding, even \nthose with moderate confidence levels. The code and data used for this paper \ncan be obtained from https://github.com/iieir-km/RUGE. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca0a9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11231"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Himanshu Sahni, Saurabh Kumar, Farhan Tejani, Charles Isbell", "title": "Learning to Compose Skills. (arXiv:1711.11289v1 [cs.AI])", "alternate": [{"href": "http://arxiv.org/abs/1711.11289", "type": "text/html"}], "timestampUsec": "1512136966936897", "comments": [], "summary": {"content": "<p>We present a differentiable framework capable of learning a wide variety of \ncompositions of simple policies that we call skills. By recursively composing \nskills with themselves, we can create hierarchies that display complex \nbehavior. Skill networks are trained to generate skill-state embeddings that \nare provided as inputs to a trainable composition function, which in turn \noutputs a policy for the overall task. Our experiments on an environment \nconsisting of multiple collect and evade tasks show that this architecture is \nable to quickly build complex skills from simpler ones. Furthermore, the \nlearned composition function displays some transfer to unseen combinations of \nskills, allowing for zero-shot generalizations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca0bf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11289"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, Jaap Kamps", "title": "Learning to Learn from Weak Supervision by Full Supervision. (arXiv:1711.11383v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11383", "type": "text/html"}], "timestampUsec": "1512136966936896", "comments": [], "summary": {"content": "<p>In this paper, we propose a method for training neural networks when we have \na large set of data with weak labels and a small amount of data with true \nlabels. In our proposed model, we train two neural networks: a target network, \nthe learner and a confidence network, the meta-learner. The target network is \noptimized to perform a given task and is trained using a large set of unlabeled \ndata that are weakly annotated. We propose to control the magnitude of the \ngradient updates to the target network using the scores provided by the second \nconfidence network, which is trained on a small amount of supervised data. Thus \nwe avoid that the weight updates computed from noisy labels harm the quality of \nthe target network model. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca0d2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11383"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pierre Stock, Moustapha Cisse", "title": "ConvNets and ImageNet Beyond Accuracy: Explanations, Bias Detection, Adversarial Examples and Model Criticism. (arXiv:1711.11443v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1711.11443", "type": "text/html"}], "timestampUsec": "1512136966936895", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450515920f6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450515920f6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>ConvNets and Imagenet have driven the recent success of deep learning for \nimage classification. However, the marked slowdown in performance improvement, \nthe recent studies on the lack of robustness of neural networks to adversarial \nexamples and their tendency to exhibit undesirable biases (e.g racial biases) \nquestioned the reliability and the sustained development of these methods. This \nwork investigates these questions from the perspective of the end-user by using \nhuman subject studies and explanations. We experimentally demonstrate that the \naccuracy and robustness of ConvNets measured on Imagenet are underestimated. We \nshow that explanations can mitigate the impact of misclassified adversarial \nexamples from the perspective of the end-user and we introduce a novel tool for \nuncovering the undesirable biases learned by a model. These contributions also \nshow that explanations are a promising tool for improving our understanding of \nConvNets' predictions and for designing more reliable models \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca0e2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11443"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ming Liu, Bo Lang, Zepeng Gu", "title": "Calculating Semantic Similarity between Academic Articles using Topic Event and Ontology. (arXiv:1711.11508v1 [cs.CL])", "alternate": [{"href": "http://arxiv.org/abs/1711.11508", "type": "text/html"}], "timestampUsec": "1512136966936894", "comments": [], "summary": {"content": "<p>Determining semantic similarity between academic documents is crucial to many \ntasks such as plagiarism detection, automatic technical survey and semantic \nsearch. Current studies mostly focus on semantic similarity between concepts, \nsentences and short text fragments. However, document-level semantic matching \nis still based on statistical information in surface level, neglecting article \nstructures and global semantic meanings, which may cause the deviation in \ndocument understanding. In this paper, we focus on the document-level semantic \nsimilarity issue for academic literatures with a novel method. We represent \nacademic articles with topic events that utilize multiple information profiles, \nsuch as research purposes, methodologies and domains to integrally describe the \nresearch work, and calculate the similarity between topic events based on the \ndomain ontology to acquire the semantic similarity between articles. \nExperiments show that our approach achieves significant performance compared to \nstate-of-the-art methods. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca0fc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11508"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512364312, "author": "Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra", "title": "Embodied Question Answering. (arXiv:1711.11543v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.11543", "type": "text/html"}], "timestampUsec": "1512136966936893", "comments": [], "summary": {"content": "<p>We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where \nan agent is spawned at a random location in a 3D environment and asked a \nquestion (\"What color is the car?\"). In order to answer, the agent must first \nintelligently navigate to explore the environment, gather information through \nfirst-person (egocentric) vision, and then answer the question (\"orange\"). \n</p> \n<p>This challenging task requires a range of AI skills -- active perception, \nlanguage understanding, goal-driven navigation, commonsense reasoning, and \ngrounding of language into actions. In this work, we develop the environments, \nend-to-end-trained reinforcement learning agents, and evaluation protocols for \nEmbodiedQA. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512364308, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca11a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11543"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Weipeng He, Petr Motlicek, Jean-Marc Odobez", "title": "Deep Neural Networks for Multiple Speaker Detection and Localization. (arXiv:1711.11565v1 [cs.SD])", "alternate": [{"href": "http://arxiv.org/abs/1711.11565", "type": "text/html"}], "timestampUsec": "1512136966936892", "comments": [], "summary": {"content": "<p>We propose to use neural networks (NNs) for simultaneous detection and \nlocalization of multiple sound sources in Human-Robot Interaction (HRI). Unlike \nconventional signal processing techniques, NN-based Sound Source Localization \n(SSL) methods are relatively straightforward and require no or fewer \nassumptions that hardly hold in real HRI scenarios. Previously, NN-based \nmethods have been successfully applied to single SSL problems, which do not \nextend to multiple sources in terms of detection and localization. In this \npaper, we thus propose a likelihood-based encoding of the network output, which \nnaturally allows the detection of an arbitrary number of sources. In addition, \nwe investigate the use of sub-band cross-correlation information as features \nfor better localization in sound mixtures, as well as three different NN \narchitectures based on different processing motivations. Experiments on real \ndata recorded from the robot show that our NN-based methods significantly \noutperform the popular spatial spectrum-based approaches. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca127", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11565"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Han Xiao, Minlie Huang, Xiaoyan Zhu", "title": "KSR: A Semantic Representation of Knowledge Graph within a Novel Unsupervised Paradigm. (arXiv:1608.07685v4 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1608.07685", "type": "text/html"}], "timestampUsec": "1512136966936891", "comments": [], "summary": {"content": "<p>Knowledge representation is a long-history topic in AI, which is very \nimportant. A variety of models have been proposed for knowledge graph \nembedding, which projects symbolic entities and relations into continuous \nvector space. However, most related methods merely focus on the data-fitting of \nknowledge graph, and ignore the interpretable semantic expression. Thus, \ntraditional embedding methods are not friendly for applications that require \nsemantic analysis, such as question answering and entity retrieval. To this \nend, this paper proposes a semantic representation method for knowledge graph \n\\textbf{(KSR)}, which imposes a two-level hierarchical generative process that \nglobally extracts many aspects and then locally assigns a specific category in \neach aspect for every triple. Since both aspects and categories are \nsemantics-relevant, the collection of categories in each aspect is treated as \nthe semantic representation of this triple. Extensive experiments show that our \nmodel outperforms other state-of-the-art baselines substantially. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca141", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1608.07685"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ke Li, Jitendra Malik", "title": "Learning to Optimize Neural Nets. (arXiv:1703.00441v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1703.00441", "type": "text/html"}], "timestampUsec": "1512136966936890", "comments": [], "summary": {"content": "<p>Learning to Optimize is a recently proposed framework for learning \noptimization algorithms using reinforcement learning. In this paper, we explore \nlearning an optimization algorithm for training shallow neural nets. Such \nhigh-dimensional stochastic optimization problems present interesting \nchallenges for existing reinforcement learning algorithms. We develop an \nextension that is suited to learning optimization algorithms in this setting \nand demonstrate that the learned optimization algorithm consistently \noutperforms other known optimization algorithms even on unseen tasks and is \nrobust to changes in stochasticity of gradients and the neural net \narchitecture. More specifically, we show that an optimization algorithm trained \nwith the proposed method on the problem of training a neural net on MNIST \ngeneralizes to the problems of training neural nets on the Toronto Faces \nDataset, CIFAR-10 and CIFAR-100. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca155", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1703.00441"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513055251, "author": "Gal Dalal, Bal&#xe1;zs Sz&#xf6;r&#xe9;nyi, Gugan Thoppe, Shie Mannor", "title": "Finite Sample Analyses for TD(0) with Function Approximation. (arXiv:1704.01161v4 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1704.01161", "type": "text/html"}], "timestampUsec": "1512136966936889", "comments": [], "summary": {"content": "<p>TD(0) is one of the most commonly used algorithms in reinforcement learning. \nDespite this, there is no existing finite sample analysis for TD(0) with \nfunction approximation, even for the linear case. Our work is the first to \nprovide such results. Existing convergence rates for Temporal Difference (TD) \nmethods apply only to somewhat modified versions, e.g., projected variants or \nones where stepsizes depend on unknown problem parameters. Our analyses obviate \nthese artificial alterations by exploiting strong properties of TD(0). We \nprovide convergence rates both in expectation and with high-probability. The \ntwo are obtained via different approaches that use relatively unknown, recently \ndeveloped stochastic approximation techniques. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1513055249, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca15e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1704.01161"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jo&#xe3;o M. Cunha, Jo&#xe3;o Gon&#xe7;alves, Pedro Martins, Penousal Machado, Am&#xed;lcar Cardoso", "title": "A Pig, an Angel and a Cactus Walk Into a Blender: A Descriptive Approach to Visual Blending. (arXiv:1706.09076v2 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1706.09076", "type": "text/html"}], "timestampUsec": "1512136966936888", "comments": [], "summary": {"content": "<p>A descriptive approach for automatic generation of visual blends is \npresented. The implemented system, the Blender, is composed of two components: \nthe Mapper and the Visual Blender. The approach uses structured visual \nrepresentations along with sets of visual relations which describe how the \nelements (in which the visual representation can be decomposed) relate among \neach other. Our system is a hybrid blender, as the blending process starts at \nthe Mapper (conceptual level) and ends at the Visual Blender (visual \nrepresentation level). The experimental results show that the Blender is able \nto create analogies from input mental spaces and produce well-composed blends, \nwhich follow the rules imposed by its base-analogy and its relations. The \nresulting blends are visually interesting and some can be considered as \nunexpected. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca174", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1706.09076"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Tambet Matiisen, Avital Oliver, Taco Cohen, John Schulman", "title": "Teacher-Student Curriculum Learning. (arXiv:1707.00183v2 [cs.LG] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.00183", "type": "text/html"}], "timestampUsec": "1512136966936887", "comments": [], "summary": {"content": "<p>We propose Teacher-Student Curriculum Learning (TSCL), a framework for \nautomatic curriculum learning, where the Student tries to learn a complex task \nand the Teacher automatically chooses subtasks from a given set for the Student \nto train on. We describe a family of Teacher algorithms that rely on the \nintuition that the Student should practice more those tasks on which it makes \nthe fastest progress, i.e. where the slope of the learning curve is highest. In \naddition, the Teacher algorithms address the problem of forgetting by also \nchoosing tasks where the Student's performance is getting worse. We demonstrate \nthat TSCL matches or surpasses the results of carefully hand-crafted curricula \nin two tasks: addition of decimal numbers with LSTM and navigation in \nMinecraft. Using our automatically generated curriculum enabled to solve a \nMinecraft maze that could not be solved at all when training directly on \nsolving the maze, and the learning was an order of magnitude faster than \nuniform sampling of subtasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca17f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.00183"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Felix Burget, Lukas Dominique Josef Fiederer, Daniel Kuhner, Martin V&#xf6;lker, Johannes Aldinger, Robin Tibor Schirrmeister, Chau Do, Joschka Boedecker, Bernhard Nebel, Tonio Ball, Wolfram Burgard", "title": "Acting Thoughts: Towards a Mobile Robotic Service Assistant for Users with Limited Communication Skills. (arXiv:1707.06633v3 [cs.AI] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.06633", "type": "text/html"}], "timestampUsec": "1512136966936886", "comments": [], "summary": {"content": "<p>As autonomous service robots become more affordable and thus available also \nfor the general public, there is a growing need for user friendly interfaces to \ncontrol the robotic system. Currently available control modalities typically \nexpect users to be able to express their desire through either touch, speech or \ngesture commands. While this requirement is fulfilled for the majority of \nusers, paralyzed users may not be able to use such systems. In this paper, we \npresent a novel framework, that allows these users to interact with a robotic \nservice assistant in a closed-loop fashion, using only thoughts. The \nbrain-computer interface (BCI) system is composed of several interacting \ncomponents, i.e., non-invasive neuronal signal recording and decoding, \nhigh-level task planning, motion and manipulation planning as well as \nenvironment perception. In various experiments, we demonstrate its \napplicability and robustness in real world scenarios, considering \nfetch-and-carry tasks and tasks involving human-robot interaction. As our \nresults demonstrate, our system is capable of adapting to frequent changes in \nthe environment and reliably completing given tasks within a reasonable amount \nof time. Combined with high-level planning and autonomous robotic systems, \ninteresting new perspectives open up for non-invasive BCI-based human-robot \ninteractions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca18c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.06633"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rocco Tripodi, Stefano Li Pira", "title": "Analysis of Italian Word Embeddings. (arXiv:1707.08783v4 [cs.CL] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1707.08783", "type": "text/html"}], "timestampUsec": "1512136966936885", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450515924c1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450515924c1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this work we analyze the performances of two of the most used word \nembeddings algorithms, skip-gram and continuous bag of words on Italian \nlanguage. These algorithms have many hyper-parameter that have to be carefully \ntuned in order to obtain accurate word representation in vectorial space. We \nprovide an accurate analysis and an evaluation, showing what are the best \nconfiguration of parameters for specific tasks. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca1a1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1707.08783"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman, John Miller", "title": "Deep Voice 3: 2000-Speaker Neural Text-to-Speech. (arXiv:1710.07654v2 [cs.SD] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1710.07654", "type": "text/html"}], "timestampUsec": "1512136966936884", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450515e8a8b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450515e8a8b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present Deep Voice 3, a fully-convolutional attention-based neural \ntext-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural \nspeech synthesis systems in naturalness while training ten times faster. We \nscale Deep Voice 3 to data set sizes unprecedented for TTS, training on more \nthan eight hundred hours of audio from over two thousand speakers. In addition, \nwe identify common error modes of attention-based speech synthesis networks, \ndemonstrate how to mitigate them, and compare several different waveform \nsynthesis methods. We also describe how to scale inference to ten million \nqueries per day on one single-GPU server. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca1b4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1710.07654"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xin Wang, Geoffrey Oxholm, Da Zhang, Yuan-Fang Wang", "title": "Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer. (arXiv:1612.01895v2 [cs.CV] CROSS LISTED)", "alternate": [{"href": "http://arxiv.org/abs/1612.01895", "type": "text/html"}], "timestampUsec": "1512136966936883", "comments": [], "summary": {"content": "<p>Transferring artistic styles onto everyday photographs has become an \nextremely popular task in both academia and industry. Recently, offline \ntraining has replaced on-line iterative optimization, enabling nearly real-time \nstylization. When those stylization networks are applied directly to \nhigh-resolution images, however, the style of localized regions often appears \nless similar to the desired artistic style. This is because the transfer \nprocess fails to capture small, intricate textures and maintain correct texture \nscales of the artworks. Here we propose a multimodal convolutional neural \nnetwork that takes into consideration faithful representations of both color \nand luminance channels, and performs stylization hierarchically with multiple \nlosses of increasing scales. Compared to state-of-the-art networks, our network \ncan also perform style transfer in nearly real-time by conducting much more \nsophisticated training offline. By properly handling style and texture cues at \nmultiple scales using several modalities, we can transfer not just large-scale, \nobvious style cues but also subtle, exquisite ones. That is, our scheme can \ngenerate results that are visually pleasing and more similar to multiple \ndesired artistic styles with color and texture cues at multiple scales. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca1c9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1612.01895"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Lingfei Wang, Tom Michoel", "title": "Wisdom of the crowd from unsupervised dimension reduction. (arXiv:1711.11034v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11034", "type": "text/html"}], "timestampUsec": "1512136966936882", "comments": [], "summary": {"content": "<p>Wisdom of the crowd, the collective intelligence derived from responses of \nmultiple human or machine individuals to the same questions, can be more \naccurate than each individual, and improve social decision-making and \nprediction accuracy. This can also integrate multiple programs or datasets, \neach as an individual, for the same predictive questions. Crowd wisdom \nestimates each individual's independent error level arising from their limited \nknowledge, and finds the crowd consensus that minimizes the overall error. \nHowever, previous studies have merely built isolated, problem-specific models \nwith limited generalizability, and mainly for binary (yes/no) responses. Here \nwe show with simulation and real-world data that the crowd wisdom problem is \nanalogous to one-dimensional unsupervised dimension reduction in machine \nlearning. This provides a natural class of crowd wisdom solutions, such as \nprincipal component analysis and Isomap, which can handle binary and also \ncontinuous responses, like confidence levels, and consequently can be more \naccurate than existing solutions. They can even outperform \nsupervised-learning-based collective intelligence that is calibrated on \nhistorical performance of individuals, e.g. penalized linear regression and \nrandom forest. This study unifies crowd wisdom and unsupervised dimension \nreduction, and thereupon introduces a broad range of highly-performing and \nwidely-applicable crowd wisdom methods. As the costs for data acquisition and \nprocessing rapidly decrease, this study will promote and guide crowd wisdom \napplications in the social and natural sciences, including data fusion, \nmeta-analysis, crowd-sourcing, and committee decision making. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca1d6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11034"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy", "title": "A Multi-Horizon Quantile Recurrent Forecaster. (arXiv:1711.11053v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11053", "type": "text/html"}], "timestampUsec": "1512136966936881", "comments": [], "summary": {"content": "<p>We propose a framework for general probabilistic multi-step time series \nregression. Specifically, we exploit the expressiveness and temporal nature of \nRecurrent Neural Networks, the nonparametric nature of Quantile Regression and \nthe efficiency of Direct Multi-Horizon Forecasting. A new training scheme for \nrecurrent nets is designed to boost stability and performance. We show that the \napproach accommodates both temporal and static covariates, learning across \nmultiple related series, shifting seasonality, future planned event spikes and \ncold-starts in real life large-scale forecasting. The performance of the \nframework is demonstrated in an application to predict the future demand of \nitems sold on Amazon.com, and in a public probabilistic forecasting competition \nto predict electricity price and load. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca1ed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11053"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Yen-Chi Chen, Y. Samuel Wang, Elena A. Erosheva", "title": "On the use of bootstrap with variational inference: Theory, interpretation, and a two-sample test example. (arXiv:1711.11057v1 [stat.ME])", "alternate": [{"href": "http://arxiv.org/abs/1711.11057", "type": "text/html"}], "timestampUsec": "1512136966936880", "comments": [], "summary": {"content": "<p>Variational inference is a general approach for approximating complex density \nfunctions, such as those arising in latent variable models, popular in machine \nlearning. It has been applied to approximate the maximum likelihood estimator \nand to carry out Bayesian inference, however, quantification of uncertainty \nwith variational inference remains challenging from both theoretical and \npractical perspectives. This paper is concerned with developing uncertainty \nmeasures for variational inference by using bootstrap procedures. We first \ndevelop two general bootstrap approaches for assessing the uncertainty of a \nvariational estimate and the study the underlying bootstrap theory in both \nfixed- and increasing-dimension settings. We then use the bootstrap approach \nand our theoretical results in the context of mixed membership modeling with \nmultivariate binary data on functional disability from the National Long Term \nCare Survey. We carry out a two-sample approach to test for changes in the \nrepeated measures of functional disability for the subset of individuals \npresent in 1984 and 1994 waves. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca1f9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11057"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Vinay Jethava, Devdatt Dubhashi", "title": "GANs for LIFE: Generative Adversarial Networks for Likelihood Free Inference. (arXiv:1711.11139v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1711.11139", "type": "text/html"}], "timestampUsec": "1512136966936879", "comments": [], "summary": {"content": "<p>We introduce a framework using Generative Adversarial Networks (GANs) for \nlikelihood--free inference (LFI) and Approximate Bayesian Computation (ABC). \nOur approach addresses both the key problems in likelihood--free inference, \nnamely how to compare distributions and how to efficiently explore the \nparameter space. Our framework allows one to use the simulator model as a black \nbox and leverage the power of deep networks to generate a rich set of features \nin a data driven fashion (as opposed to previous ad hoc approaches). Thereby it \nis a step towards a powerful alternative approach to LFI and ABC. On benchmark \ndata sets, our approach improves on others with respect to scalability, ability \nto handle high dimensional data and complex probability distributions. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca1ff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11139"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chao Gao", "title": "Phase Transitions in Approximate Ranking. (arXiv:1711.11189v1 [math.ST])", "alternate": [{"href": "http://arxiv.org/abs/1711.11189", "type": "text/html"}], "timestampUsec": "1512136966936878", "comments": [], "summary": {"content": "<p>We study the problem of approximate ranking from observations of pairwise \ninteractions. The goal is to estimate the underlying ranks of $n$ objects from \ndata through interactions of comparison or collaboration. Under a general \nframework of approximate ranking models, we characterize the exact optimal \nstatistical error rates of estimating the underlying ranks. We discover \nimportant phase transition boundaries of the optimal error rates. Depending on \nthe value of the signal-to-noise ratio (SNR) parameter, the optimal rate, as a \nfunction of SNR, is either trivial, polynomial, exponential or zero. The four \ncorresponding regimes thus have completely different error behaviors. To the \nbest of our knowledge, this phenomenon, especially the phase transition between \nthe polynomial and the exponential rates, has not been discovered before. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca209", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11189"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Chang Liu, Jun Zhu", "title": "Riemannian Stein Variational Gradient Descent for Bayesian Inference. (arXiv:1711.11216v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11216", "type": "text/html"}], "timestampUsec": "1512136966936877", "comments": [], "summary": {"content": "<p>We develop Riemannian Stein Variational Gradient Descent (RSVGD), a Bayesian \ninference method that generalizes Stein Variational Gradient Descent (SVGD) to \nRiemann manifold. The benefits are two-folds: (i) for inference tasks in \nEuclidean spaces, RSVGD has the advantage over SVGD of utilizing information \ngeometry, and (ii) for inference tasks on Riemann manifolds, RSVGD brings the \nunique advantages of SVGD to the Riemannian world. To appropriately transfer to \nRiemann manifolds, we conceive novel and non-trivial techniques for RSVGD, \nwhich are required by the intrinsically different characteristics of general \nRiemann manifolds from Euclidean spaces. We also discover Riemannian Stein's \nIdentity and Riemannian Kernelized Stein Discrepancy. Experimental results show \nthe advantages over SVGD of exploring distribution geometry and the advantages \nof particle-efficiency, iteration-effectiveness and approximation flexibility \nover other inference methods on Riemann manifolds. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca213", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11216"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Been Kim, Justin Gilmer, Fernanda Viegas, Ulfar Erlingsson, Martin Wattenberg", "title": "TCAV: Relative concept importance testing with Linear Concept Activation Vectors. (arXiv:1711.11279v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11279", "type": "text/html"}], "timestampUsec": "1512136966936876", "comments": [], "summary": {"content": "<p>Neural networks commonly offer high utility but remain difficult to \ninterpret. Developing methods to explain their decisions is challenging due to \ntheir large size, complex structure, and inscrutable internal representations. \nThis work argues that the language of explanations should be expanded from that \nof input features (e.g., assigning importance weightings to pixels) to include \nthat of higher-level, human-friendly concepts. For example, an understandable \nexplanation of why an image classifier outputs the label \"zebra\" would ideally \nrelate to concepts such as \"stripes\" rather than a set of particular pixel \nvalues. This paper introduces the \"concept activation vector\" (CAV) which \nallows quantitative analysis of a concept's relative importance to \nclassification, with a user-provided set of input data examples defining the \nconcept. CAVs may be easily used by non-experts, who need only provide \nexamples, and with CAVs the high-dimensional structure of neural networks turns \ninto an aid to interpretation, rather than an obstacle. Using the domain of \nimage classification as a testing ground, we describe how CAVs may be used to \ntest hypotheses about classifiers and also generate insights into the \ndeficiencies and correlations in training data. CAVs also provide us a directed \napproach to choose the combinations of neurons to visualize with the DeepDream \ntechnique, which traditionally has chosen neurons or linear combinations of \nneurons at random to visualize. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca21d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11279"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Xiaofan Lin, Cong Zhao, Wei Pan", "title": "Towards Accurate Binary Convolutional Neural Network. (arXiv:1711.11294v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1711.11294", "type": "text/html"}], "timestampUsec": "1512136966936875", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450515e8cf2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450515e8cf2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We introduce a novel scheme to train binary convolutional neural networks \n(CNNs) -- CNNs with weights and activations constrained to {-1,+1} at run-time. \nIt has been known that using binary weights and activations drastically reduce \nmemory size and accesses, and can replace arithmetic operations with more \nefficient bitwise operations, leading to much faster test-time inference and \nlower power consumption. However, previous works on binarizing CNNs usually \nresult in severe prediction accuracy degradation. In this paper, we address \nthis issue with two major innovations: (1) approximating full-precision weights \nwith the linear combination of multiple binary weight bases; (2) employing \nmultiple binary activations to alleviate information loss. The implementation \nof the resulting binary CNN, denoted as ABC-Net, is shown to achieve much \ncloser performance to its full-precision counterpart, and even reach the \ncomparable prediction accuracy on ImageNet and forest trail datasets, given \nadequate binary weight bases and activations. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca228", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11294"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1512364319, "author": "Kerem C. Tezcan, Christian F. Baumgartner, Ender Konukoglu", "title": "MR image reconstruction using the learned data distribution as prior. (arXiv:1711.11386v2 [cs.CV] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.11386", "type": "text/html"}], "timestampUsec": "1512136966936874", "comments": [], "summary": {"content": "<p>MR image reconstruction from undersampled data exploits priors to compensate \nfor missing k-space data. This has previously been achieved by using \nregularization methods, such as TV and wavelets, or data adaptive methods, such \nas dictionary learning. We propose to explicitly learn the probability \ndistribution of MR image patches and to constrain patches to have a high \nprobability according to this distribution in reconstruction, effectively \nemploying it as the prior. \n</p> \n<p>We use variational autoencoders (VAE) to learn the distribution of MR image \npatches. This high dimensional distribution is modelled by a latent parameter \nmodel of lower dimensions in a non-linear fashion. We develop a reconstruction \nalgorithm that uses the learned prior in a Maximum-A-Posteriori estimation \nformulation. We evaluate the proposed method with T1 weighted images and \ncompare it to existing alternatives. We also apply our method on images with \nwhite matter lesions. \n</p> \n<p>Visual evaluation of the samples drawn from the learned model showed that the \nVAE algorithm was able to approximate the distribution of MR image patches. \nFurthermore, the reconstruction algorithm using the approximate distribution \nproduced qualitatively better results. The proposed technique achieved RMSE, \nCNR and CN values of 2.77%, 0.43, 0.11 and 4.29%, 0.43, 0.11 for undersampling \nratios of 2 and 3, respectively. It outperformed other evaluated methods in \nterms of used metrics. In the experiments on images with white matter lesions, \nthe method faithfully reconstructed the lesions. \n</p> \n<p>We introduced a novel method for MR reconstruction, which takes a new \nperspective on regularization by learning priors. Results suggest the method \ncompares favorably against TV and dictionary based methods as well as the \nneural-network based ADMM-Net in terms of the RMSE, CNR and CN and perceptual \nimage quality and can reconstruct lesions as well. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512364309, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca231", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11386"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Burim Ramosaj, Markus Pauly", "title": "Who wins the Miss Contest for Imputation Methods? Our Vote for Miss BooPF. (arXiv:1711.11394v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11394", "type": "text/html"}], "timestampUsec": "1512136966936873", "comments": [], "summary": {"content": "<p>Missing data is an expected issue when large amounts of data is collected, \nand several imputation techniques have been proposed to tackle this problem. \nBeneath classical approaches such as MICE, the application of Machine Learning \ntechniques is tempting. Here, the recently proposed missForest imputation \nmethod has shown high imputation accuracy under the Missing (Completely) at \nRandom scheme with various missing rates. In its core, it is based on a random \nforest for classification and regression, respectively. In this paper we study \nwhether this approach can even be enhanced by other methods such as the \nstochastic gradient tree boosting method, the C5.0 algorithm or modified random \nforest procedures. In particular, other resampling strategies within the random \nforest protocol are suggested. In an extensive simulation study, we analyze \ntheir performances for continuous, categorical as well as mixed-type data. \nTherein, MissBooPF, a combination of the stochastic gradient tree boosting \nmethod together with the parametrically bootstrapped random forest method, \nappeared to be promising. Finally, an empirical analysis focusing on credit \ninformation and Facebook data is conducted. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca240", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11394"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Daniel Chicharro, Giuseppe Pica, Stefano Panzeri", "title": "The identity of information: how deterministic dependencies constrain information synergy and redundancy. (arXiv:1711.11408v1 [q-bio.NC])", "alternate": [{"href": "http://arxiv.org/abs/1711.11408", "type": "text/html"}], "timestampUsec": "1512136966936872", "comments": [], "summary": {"content": "<p>Understanding how different information sources together transmit information \nis crucial in many domains. For example, understanding the neural code requires \ncharacterizing how different neurons contribute unique, redundant, or \nsynergistic pieces of information about sensory or behavioral variables. \nWilliams and Beer (2010) proposed a partial information decomposition (PID) \nwhich separates the mutual information that a set of sources contains about a \nset of targets into nonnegative terms interpretable as these pieces. \nQuantifying redundancy requires assigning an identity to different information \npieces, to assess when information is common across sources. Harder et al. \n(2013) proposed an identity axiom stating that there cannot be redundancy \nbetween two independent sources about a copy of themselves. However, \nBertschinger et al. (2012) showed that with a deterministically related \nsources-target copy this axiom is incompatible with ensuring PID nonnegativity. \nHere we study systematically the effect of deterministic target-sources \ndependencies. We introduce two synergy stochasticity axioms that generalize the \nidentity axiom, and we derive general expressions separating stochastic and \ndeterministic PID components. Our analysis identifies how negative terms can \noriginate from deterministic dependencies and shows how different assumptions \non information identity, implicit in the stochasticity and identity axioms, \ndetermine the PID structure. The implications for studying neural coding are \ndiscussed. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca24d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11408"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Ibrahim El Khalil Harrane, R&#xe9;mi Flamary, C&#xe9;dric Richard", "title": "On reducing the communication cost of the diffusion LMS algorithm. (arXiv:1711.11423v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11423", "type": "text/html"}], "timestampUsec": "1512136966936871", "comments": [], "summary": {"content": "<p>The rise of digital and mobile communications has recently made the world \nmore connected and networked, resulting in an unprecedented volume of data \nflowing between sources, data centers, or processes. While these data may be \nprocessed in a centralized manner, it is often more suitable to consider \ndistributed strategies such as diffusion as they are scalable and can handle \nlarge amounts of data by distributing tasks over networked agents. Although it \nis relatively simple to implement diffusion strategies over a cluster, it \nappears to be challenging to deploy them in an ad-hoc network with limited \nenergy budget for communication. In this paper, we introduce a diffusion LMS \nstrategy that significantly reduces communication costs without compromising \nthe performance. Then, we analyze the proposed algorithm in the mean and \nmean-square sense. Next, we conduct numerical experiments to confirm the \ntheoretical findings. Finally, we perform large scale simulations to test the \nalgorithm efficiency in a scenario where energy is limited. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca25a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11423"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Francisco J. Valverde-Albacete, Carmen Pel&#xe1;ez-Moreno", "title": "The Channel Multivariate Entropy Triangle and Balance Equation. (arXiv:1711.11510v1 [cs.IT])", "alternate": [{"href": "http://arxiv.org/abs/1711.11510", "type": "text/html"}], "timestampUsec": "1512136966936870", "comments": [], "summary": {"content": "<p>In this paper we use information-theoretic measures to provide a theory and \ntools to analyze the flow of information from a discrete, multivariate source \nof information $\\overline X$ to a discrete, multivariate sink of information \n$\\overline Y$ joined by a distribution $P_{\\overline X \\overline Y}$. The first \ncontribution is a decomposition of the maximal potential entropy of $(\\overline \nX, \\overline Y)$ that we call a balance equation, that can also be split into \ndecompositions for the entropies of $\\overline X$ and $\\overline Y$ \nrespectively. Such balance equations accept normalizations that allow them to \nbe represented in de Finetti entropy diagrams, our second contribution. The \nmost important of these, the aggregate Channel Multivariate Entropy Triangle \nCMET is an exploratory tool to assess the efficiency of multivariate channels. \nWe also present a practical contribution in the application of these balance \nequations and diagrams to the assessment of information transfer efficiency for \nPCA and ICA as feature transformation and selection procedures in machine \nlearning applications. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca26d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11510"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Rui Luo, Yaodong Yang, Jun Wang, Yuanyuan Liu", "title": "Thermostat-assisted Continuous-tempered Hamiltonian Monte Carlo for Multimodal Posterior Sampling on Large Datasets. (arXiv:1711.11511v1 [stat.ML])", "alternate": [{"href": "http://arxiv.org/abs/1711.11511", "type": "text/html"}], "timestampUsec": "1512136966936869", "comments": [], "summary": {"content": "<p>In this paper, we propose a new sampling method named as the \nthermostat-assisted continuous-tempered Hamiltonian Monte Carlo for multimodal \nposterior sampling on large datasets. It simulates a noisy system, which is \naugmented by a coupling tempering variable as well as a set of Nos\\'e-Hoover \nthermostats. This augmentation is devised to address two main issues of \nconcern: the first is to effectively generate i.i.d. samples from complex \nmultimodal posterior distributions; the second is to adaptively control the \nsystem dynamics in the presence of unknown noise that arises from the use of \nmini-batches. The experiment on synthetic distributions has been performed; the \nresult demonstrates the effectiveness of the proposed method. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca279", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11511"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 1513315291, "author": "Kshiteej Sheth, Dinesh Garg, Anirban Dasgupta", "title": "Improved Linear Embeddings via Lagrange Duality. (arXiv:1711.11527v2 [stat.ML] UPDATED)", "alternate": [{"href": "http://arxiv.org/abs/1711.11527", "type": "text/html"}], "timestampUsec": "1512136966936868", "comments": [], "summary": {"content": "<p>Near isometric orthogonal embeddings to lower dimensions are a fundamental \ntool in data science and machine learning. In this paper, we present the \nconstruction of such embeddings that minimizes the maximum distortion for a \ngiven set of points. We formulate the problem as a non convex constrained \noptimization problem. We first construct a primal relaxation and then use the \ntheory of Lagrange duality to create dual relaxation. We also suggest a \npolynomial time algorithm based on the theory of convex optimization to solve \nthe dual relaxation provably. We provide a theoretical upper bound on the \napproximation guarantees for our algorithm, which depends only on the spectral \nproperties of the dataset. We experimentally demonstrate the superiority of our \nalgorithm compared to baselines in terms of the scalability and the ability to \nachieve lower distortion. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1513315288, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca286", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11527"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Alexander G. Ororbia II, Patrick Haffner, David Reitter, C. Lee Giles", "title": "Learning to Adapt by Minimizing Discrepancy. (arXiv:1711.11542v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1711.11542", "type": "text/html"}], "timestampUsec": "1512136966936867", "comments": [], "summary": {"content": "<p>We explore whether useful temporal neural generative models can be learned \nfrom sequential data without back-propagation through time. We investigate the \nviability of a more neurocognitively-grounded approach in the context of \nunsupervised generative modeling of sequences. Specifically, we build on the \nconcept of predictive coding, which has gained influence in cognitive science, \nin a neural framework. To do so we develop a novel architecture, the Temporal \nNeural Coding Network, and its learning algorithm, Discrepancy Reduction. The \nunderlying directed generative model is fully recurrent, meaning that it \nemploys structural feedback connections and temporal feedback connections, \nyielding information propagation cycles that create local learning signals. \nThis facilitates a unified bottom-up and top-down approach for information \ntransfer inside the architecture. Our proposed algorithm shows promise on the \nbouncing balls generative modeling problem. Further experiments could be \nconducted to explore the strengths and weaknesses of our approach. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca28e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11542"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Jason Jo, Yoshua Bengio", "title": "Measuring the tendency of CNNs to Learn Surface Statistical Regularities. (arXiv:1711.11561v1 [cs.LG])", "alternate": [{"href": "http://arxiv.org/abs/1711.11561", "type": "text/html"}], "timestampUsec": "1512136966936866", "comments": [], "summary": {"content": "<p>Deep CNNs are known to exhibit the following peculiarity: on the one hand \nthey generalize extremely well to a test set, while on the other hand they are \nextremely sensitive to so-called adversarial perturbations. The extreme \nsensitivity of high performance CNNs to adversarial examples casts serious \ndoubt that these networks are learning high level abstractions in the dataset. \nWe are concerned with the following question: How can a deep CNN that does not \nlearn any high level semantics of the dataset manage to generalize so well? The \ngoal of this article is to measure the tendency of CNNs to learn surface \nstatistical regularities of the dataset. To this end, we use Fourier filtering \nto construct datasets which share the exact same high level abstractions but \nexhibit qualitatively different surface statistical regularities. For the SVHN \nand CIFAR-10 datasets, we present two Fourier filtered variants: a low \nfrequency variant and a randomly filtered variant. Each of the Fourier \nfiltering schemes is tuned to preserve the recognizability of the objects. Our \nmain finding is that CNNs exhibit a tendency to latch onto the Fourier image \nstatistics of the training dataset, sometimes exhibiting up to a 28% \ngeneralization gap across the various test sets. Moreover, we observe that \nsignificantly increasing the depth of a network has a very marginal impact on \nclosing the aforementioned generalization gap. Thus we provide quantitative \nevidence supporting the hypothesis that deep CNNs tend to learn surface \nstatistical regularities in the dataset rather than higher-level abstract \nconcepts. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca29c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11561"}]}, {"commentsNum": -1, "origin": {"title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/", "streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml"}, "updated": 0, "author": "Pravesh K. Kothari, David Steurer", "title": "Outlier-robust moment-estimation via sum-of-squares. (arXiv:1711.11581v1 [cs.DS])", "alternate": [{"href": "http://arxiv.org/abs/1711.11581", "type": "text/html"}], "timestampUsec": "1512136966936865", "comments": [], "summary": {"content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a450515e8f26\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a450515e8f26&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We develop efficient algorithms for estimating low-degree moments of unknown \ndistributions in the presence of adversarial outliers. The guarantees of our \nalgorithms improve in many cases significantly over the best previous ones, \nobtained in recent works of Diakonikolas et al, Lai et al, and Charikar et al. \nWe also show that the guarantees of our algorithms match information-theoretic \nlower-bounds for the class of distributions we consider. These improved \nguarantees allow us to give improved algorithms for independent component \nanalysis and learning mixtures of Gaussians in the presence of outliers. \n</p> \n<p>Our algorithms are based on a standard sum-of-squares relaxation of the \nfollowing conceptually-simple optimization problem: Among all distributions \nwhose moments are bounded in the same way as for the unknown distribution, find \nthe one that is closest in statistical distance to the empirical distribution \nof the adversarially-corrupted sample. \n</p>", "direction": "ltr"}, "crawlTimeMsec": "1512136966937", "annotations": [], "published": 1512136967, "likingUsers": [], "id": "tag:google.com,2005:reader/item/000000033e2ca2a9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "canonical": [{"href": "http://arxiv.org/abs/1711.11581"}]}]}